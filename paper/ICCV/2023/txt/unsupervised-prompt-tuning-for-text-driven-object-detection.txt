Abstract
Grounded language-image pre-trained models have shown strong zero-shot generalization to various down-stream object detection tasks. Despite their promising per-formance, the models rely heavily on the laborious prompt engineering. Existing works typically address this prob-lem by tuning text prompts using downstream training data in a few-shot or fully supervised manner. However, a rarely studied problem is to optimize text prompts with-In this paper, we delve into out using any annotations. this problem and propose an Unsupervised Prompt Tuning framework for text-driven object detection, which is com-posed of two novel mean teaching mechanisms.
In con-ventional mean teaching, the quality of pseudo boxes is expected to optimize better as the training goes on, but there is still a risk of overfitting noisy pseudo boxes. To mitigate this problem, 1) we propose Nested Mean Teach-ing, which adopts nested-annotation to supervise teacher-student mutual learning in a bi-level optimization manner; 2) we propose Dual Complementary Teaching, which em-ploys an offline pre-trained teacher and an online mean teacher via data-augmentation-based complementary la-beling so as to ensure learning without accumulating confir-mation bias. By integrating these two mechanisms, the pro-posed unsupervised prompt tuning framework achieves sig-nificant performance improvement on extensive object de-tection datasets. 1.

Introduction
Object detection, which aims to locate and classify ob-jects in an image, is a very fundamental task in com-puter vision. Recently, with the development of vision-*Internship in Hikvision Research Institute.
†Equal contributions
‡Corresponding authors (a) Zero-Shot Inference (b) Supervised Model Tuning (c) Supervised Prompt Tuning (d) Unsupervised Prompt Tuning
Figure 1: Illustration of GLIP and three optimization man-ners to adapt downstream object detection tasks. (a) Zero-shot inference, which locates the target objects with the pre-defined prompts using category name. (b) and (c) are two supervised tuning manners using the labeled down-stream data. (d) is the proposed unsupervised prompt tuning method, which exploits the zero-shot results as the prior su-pervision cues of the unlabeled downstream data. language foundation models, object detection tends to be an open-vocabulary by learning knowledge from large-scale heterogeneous image-text data. Grounded Language-Image Pre-training (GLIP) [23] is one of the leading mod-els, which detects target objects directly with the pre-defined prompts using task-specific category name (such as “[CLS],[CLS],...,[CLS]” in Fig. 1a) without being opti-mized by task-specific training data. Although those pre-trained models are endowed with a promising zero-shot generalization ability, it is crucial and necessary to transfer the knowledge to various downstream tasks [47, 42, 35].
An emerging trend to solve this problem is Prompt Tun-ing [51, 50, 21, 39, 25], which freezes the main body of the
network and merely optimizes text prompts (prompt embed-dings) using downstream training data in a few-shot or fully supervised manner. However, this paradigm requires train-ing data with annotations, which violates the original inten-tion of zero-shot inference. As shown in Fig. 1, it naturally comes a question: can we conduct prompt tuning with the exposure of downstream data without human labeling?
In this paper, we take GLIP as an example pre-trained model to study this problem. To the best of our knowledge, this is the first attempt in this field to study unsupervised prompt tuning for text-driven object detection. Preliminar-ily, the baseline framework is developed from the mean teacher based self-training [30], facilitating teacher-student mutual learning. Since only text prompts are allowed to be optimized, both teacher and student share the same frozen network (text encoder and image encoder). Specifically, given the task-specific pre-defined prompts as initialization, the student is the network with learnable prompts, and the teacher is the one with momentum prompts. In this way, the teacher model annotates the unlabeled images to drive student prompt tuning, and then the teacher prompt is up-dated by the student prompt in an exponential moving av-erage (EMA) manner. However, the pseudo boxes are in-evitably noisy, causing the student to overfit on the noisy pseudo boxes, which in turn affects the teacher. To address this issue, we advance the conventional mean teaching pro-cess into two simple yet effective variants:
Nested Mean Teaching (NMT). The performance of the teacher model is equivalent to the quality of pseudo boxes.
From this perspective, learning a good mean teacher can be formulated as a “learning to annotate” problem. An-other insight is that mean teaching has been proven effec-tive to optimize pseudo label, a next k-step mean teacher can naturally provide high-quality pseudo label, which driv-ing the current timestamp to avoid overfitting on noisy la-bel. Inspired by the above consideration, we aim to learn a delayed-annotator in a nested k-step mean teacher opti-mization manner, which comprises a nested inner loop and an outer loop. As shown in Fig. 2b, the inner loop acts as an annotator optimization process, which nests k-step ghosted teacher-student mutual learning to achieve better pseudo boxes. Note that both teacher and student models are dis-carded after inner-loop training, and only the pseudo labels are propagated to the outer loop. The outer loop optimizes the teacher in an EMA manner by taking student learning as a bridge using the pseudo boxes from the nested inner loop. These two loops are interpretable. Since the qual-ity of pseudo boxes is expected to be better during teacher-student mutual learning, the k-step ghosted optimization in the nested inner loop provides better pseudo boxes to drive the optimization of the outer loop mean teacher.
Dual Complementary Teaching (DCT). Since unsu-pervised prompt tuning is an optimization process with-out using any ground-truth annotations, there exist risks of accumulating confirmation bias, which induces false neg-atives or false positives in an avalanche during teacher-student mutual learning. To mitigate this problem, we develop Dual Teachers, of which an offline teacher (pre-defined prompts) accounts for providing true-positive cues to ensure learning without forgetting true positives, and an online teacher (momentum prompts) learns to recall false negatives. To further promote the collaboration of Dual
Teachers, we propose a data-augmentation-based comple-mentary labeling mechanism. The offline teacher initializes sufficient true-positive boxes by feeding weakly-augmented images, while the online teacher recalls false negatives cau-tiously by feeding strongly-augmented images which is a strict access condition for the introduction of new pseudo boxes so as to avoid cumulatively introducing false-positive boxes.
These two mean teaching mechanisms are orthogo-nal to each other. We build the Unsupervised Prompt
Tuning framework (UPT) by integrating them, where the
Dual Complementary Teaching process is optimized in
Extensive ablation stud-a nested annotation manner. ies and experiments are carried out on multiple down-stream object detection tasks, i.e., Cityscapes [4], Foggy
Cityscapes [37], KITTI [11], Sim10K [19], BDD100K [46],
WaterColor [17], MS COCO [28], Pasval VOC [7], Ego-Hands [23], and Pistols [23], which vary in dataset scale, categories, context and style shifts, demonstrating the ef-fectiveness of the proposed framework. The main contribu-tions of this paper can be summarized as follows:
• For the first time, we propose a challenging yet mean-ingful task, namely unsupervised prompt tuning for text-driven object detection, which fills the blank in object detection and pushes the limits of zero-shot inference.
• We build the unsupervised prompt tuning framework by developing two novel mean teaching methods, i.e.,
Nested Mean Teaching and Dual Complementary Teach-ing, which advance the conventional mean teaching pro-cess from the perspectives of optimizing annotation and learning without accumulating confirmation bias.
• Extensive experiments on numerous downstream object detection datasets demonstrate that the proposed frame-work can achieve significant performance improvement. 2.