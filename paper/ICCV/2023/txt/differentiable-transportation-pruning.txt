Abstract
Deep learning algorithms are increasingly employed at the edge. However, edge devices are resource constrained and thus require efficient deployment of deep neural net-works. Pruning methods are a key tool for edge deployment as they can improve storage, compute, memory bandwidth, and energy usage.
In this paper we propose a novel ac-curate pruning technique that allows precise control over the output network size. Our method uses an efficient opti-mal transportation scheme which we make end-to-end dif-ferentiable and which automatically tunes the exploration-exploitation behavior of the algorithm to find accurate sparse sub-networks. We show that our method achieves state-of-the-art performance compared to previous pruning methods on 3 different datasets, using 5 different models, across a wide range of pruning ratios, and with two types of sparsity budgets and pruning granularities. 1.

Introduction
As the world is getting smaller, its edge is getting larger: more compute-limited edge devices are used. To unlock deep learning on the edge, deep networks need to be effi-cient and compact. There is a demand for accurate networks that fit exactly in pre-defined memory constraints.
Pruning is an effective technique [20, 41, 67] to find a sparse, compact model from a dense, over-parameterized deep network by eliminating redundant elements such as fil-ters or weights while retaining accuracy. Pruning is a hard problem because finding a good-performing sub-network demands selecting which part of a network to keep —and which part to prune—, requiring an expensive search over a large discrete set of candidate architectures.
Recent work [23, 52, 40, 25] finds that optimizing a continuously differentiable mask to sparsify network con-nection while simultaneously learning network parameters facilitates search space exploration and enables gradient-based optimization. Prior methods [23, 40], however, con-trol the sparsity, i.e. pruned network size, by an additional penalty term in the loss function that is difficult to optimize and tune, and hampering a precise control over the sparsity.
Figure 1. During training we multiply the filters in a layer with soft masks mϵ. The soft masks are obtained by solving an opti-mal transport problem: minimizing a transportation cost between a uniform source distribution over trainable importance scores s and a Bernoulli target distribution defined by sparsity ratio i.e. 3/5.
During training the soft masks gradually converge to hard masks.
In this paper, as illustrated in Fig. 1, we propose a fully differentiable transportation pruning method that al-lows precise control of the output network size. We draw inspiration from Xie et al. (2020) [65] who formulate a soft top-k operator based on entropic optimal transport. Recent work [59] shows good pruning results with the soft top-k operator. This soft top-k operator, however, is computation-ally expensive [10] and relies on implicit, and therefore less accurate, gradients [11] for back-propagation. Instead, we speed up the soft top-k operator with an approximate bi-level optimization which is optimized using accurate auto-matic differentiation [48]. Moreover, in contrast to [65, 59] we can automatically tune the temperature hyper-parameter.
We make the following contributions: i) A fully differ-entiable pruning method that allows precise control over output network size; ii) An efficient entropic optimal trans-portation algorithm that significantly reduces the computa-tional complexity of bi-level optimization; iii) We increase the ease-of-use of the algorithm by means of an automatic temperature annealing mechanism instead of a manually-chosen decay schedule; iv) We show state-of-the-art results on 3 datasets with 5 models for structured and unstructured pruning using layer-wise and global sparsity budgets.
2.