Abstract understanding. Code and data are available at https:
//mvig-rhos.com/ego_pca.
With the surge in attention to Egocentric Hand-Object
Interaction (Ego-HOI), large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed. However, most current research is built on resources derived from third-person video action recognition. This inherent domain gap between first- and third-person action videos, which have not been adequately addressed before, makes current Ego-HOI suboptimal. This paper rethinks and proposes a new framework as an infrastructure to advance Ego-HOI recog-nition by Probing, Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets, balanced test sets and a new baseline, which are complete with a training-finetuning strategy. With our new framework, we not only achieve state-of-the-art performance on Ego-HOI bench-marks but also build several new and effective mechanisms and settings to advance further research. We believe our data and the findings will pave a new way for Ego-HOI
*Corresponding author.
†This research is supported in part by the Research Grant Council of the Hong Kong SAR under grant no. 16201420. 1.

Introduction
Understanding Egocentric Hand-Object
Interaction (Ego-HOI) is a fundamental task for computer vision and embodied AI. To promote HOI learning, many egocentric video datasets [17, 7, 24, 16, 44] have been released, which contributed to recent advances in this direction. Recently, deep learning based methods [50, 12, 4], especially Trans-formers and visual-language models [11, 1, 2, 51] have achieved high performances on these benchmarks.
Though significant progress has been made, challenges remain. With few better choices available, current studies on Ego-HOI typically adopt existing tools and settings of third-person action recognition, despite the significant do-main gap between egocentric and exocentric action [44, 25].
Notably, third-person action depicts almost full human body and associated poses, while first-person action typically only engages hands; third-person videos are usually stable or readily stabilized, while first-person videos can exhibit
different degrees of camera motion and shaking, which are possibly intended by the actor. Given the large domain gap, existing methods inherited from third-person vision are ar-guably unsuitable. Moreover, it remains unclear whether the existing Ego-HOI datasets [17, 7, 24, 35, 44] can sup-port the model pre-training for transferability on down-stream tasks. Thus, here, we address the important tech-nical question in Ego-HOI: What are the effective model and training mechanisms for Ego-HOI learning?
To understand the need for a new baseline and cus-tomized training for Ego-HOI learning, we analyze the ex-isting paradigm and observe three main weaknesses: 1)
Previous methods are mainly based on models pre-trained on Kinetics [21]. It has been widely discussed that third-person action datasets like Kinetics have a huge domain and the semantic gap with egocentric videos [44, 43, 59, 5, 25].
Thus, we need a new pre-train set specifically designed for
Ego-HOI; 2) Previous ad-hoc models are designed for third-person video learning. And these solutions are typically tai-lored to address one or a limited subset of Ego-HOI learn-ing instead of a more general one-for-all model, i.e., one model for all Ego-HOI benchmarks; 3) In current schemes, finetuning one shared pre-trained model for all downstream tasks is inefficient, which also falls short of adapting to every downstream task or benchmark. Therefore, a task-specific scheme is necessary so that we can efficiently learn a customized model for each downstream task.
In light of these weaknesses, in this work, we propose a novel basic framework for Ego-HOI learning by Probing,
Curation and Adaption (EgoPCA): we probe the properties of Ego-HOI videos, based on which we leverage data cu-ration for balanced pre-train and test datasets, and finally adapt the model according to specific tasks. The details are as follows. 1) New Pre-Train and Test Sets. We build a new com-prehensive pre-train set based on the videos from Ego-HOI datasets. Although multiple datasets are available for training a universal Ego-HOI model, the noisy, highly long-tailed source datasets (e.g., EPIC-KITCHEN [7] and
EGTEA Gaze+ [24]) can introduce imbalance to the pre-trained models and thus adversely influence their general-ization ability [10]. The bulky “head” data in the long-tailed distribution also result in unmanageable training cost given the current rapid growth of the model and data size.
Hence we propose to seek a balanced pretrain data distri-bution for the training efficacy and efficiency. In the scope of Ego-HOI, the data should be balanced not only on the semantics of samples but also on the other video proper-ties such as camera motion or hand poses. After conducting thorough studies on Ego-HOI video properties, we sample a small but balanced and informative subset from multiple datasets [17, 7, 24, 35]. which can support better transfer learning for downstream tasks with domain and semantic gaps. Alongside, a new balanced test set is built that ac-counts for the long-tailed distribution of Ego-HOI videos and its HOI semantics for fair and unbiased evaluation of models, which is a widely adopted approach [32]. 2) One-for-All Baseline Model. We propose a new baseline given the unique egocentric video properties, that consist of an efficient lite network and a representative heavy network, which can leverage both frames and videos in training. Moreover, we observe that the camera motion associated with Ego-HOI videos often correlates to serial attention to the visual scene of interaction. So we propose
Serial Visual Scene Attention (SVSA) prediction task to ex-ploit such knowledge. In particular, we incorporate coun-terfactual reasoning in ego-videos, applying intervention on the “hand” causal node by replacing the hand patch with dif-ferent hand states while keeping the scene/background. The model output should change after intervention. With these constraints, our baseline achieves state-of-the-art (SOTA) on several benchmarks when pre-trained on our training set, and outperforms the SOTA significantly on our test set. 3) All-for-One Customized Mechanism. Towards the best settings for each downstream task, we propose a new video sampling and selection algorithm based on the ego-video properties analysis. Given our one-for-all model, we apply our optimal training and tuning policies for each task.
Subsequently, we further outperform the performance of our one-for-all model on several benchmarks.
Overall, to “standardize” Ego-HOI learning and inte-grate resources, our contributions are: 1) we revisit the
Ego-HOI tasks and analyze the data from the perspectives of dataset construction and model design; 2) according to our analysis, instead of directly using the third-person video methods/tools, we propose a new framework (pre-train set, baseline, and test set) designed exclusively for Ego-HOI; 3) to pursue SOTA while minimizing training costs, we pro-pose a customized approach for downstream tasks. 2.