Abstract
We present DreamBooth3D, an approach to personal-ize text-to-3D generative models from as few as 3-6 ca-sually captured images of a subject. Our approach com-bines recent advances in personalizing text-to-image mod-els (DreamBooth) with text-to-3D generation (DreamFu-sion). We ﬁnd that na¨ıvely combining these methods fails to yield satisfactory subject-speciﬁc 3D assets due to per-sonalized text-to-image models overﬁtting to the input view-points of the subject. We overcome this through a 3-stage optimization strategy where we jointly leverage the 3D con-sistency of neural radiance ﬁelds together with the person-alization capability of text-to-image models. Our method can produce high-quality, subject-speciﬁc 3D assets with text-driven modiﬁcations such as novel poses, colors and attributes that are not seen in any of the input images of the subject. More results are available at our project page: https://dreambooth3d.github.io 1.

Introduction
Text-to-Image (T2I) generative models [6, 36, 37, 39] have greatly expanded the ways we can create and edit vi-sual content. Recent works [23, 27, 33, 43] have demon-strated high-quality Text-to-3D generation by optimizing neural radiance ﬁelds (NeRFs) [28] using the T2I diffusion models. Such automatic 3D asset creation with input text
prompts alone has applications in a wide range of areas, such as graphics, VR, movies, and gaming.
Although text prompts allow for some degree of control over the generated 3D asset, it is often difﬁcult to precisely control its identity, geometry, and appearance solely with text. In particular, these methods lack the ability to generate 3D assets of a speciﬁc subject (e.g., a speciﬁc dog instead of a generic dog). Enabling the generation of subject-speciﬁc 3D assets would signiﬁcantly ease the workﬂow for artists and 3D acquisition. There has been remarkable success [13, 21, 38] in personalizing T2I models for subject-speciﬁc 2D image generation. These techniques allow the generation of speciﬁc subject images in varying contexts, but they do not generate 3D assets or afford any 3D control, such as viewpoint changes.
In this work, we propose ‘DreamBooth3D’, a method for subject-driven Text-to-3D generation. Given a few (3-6) ca-sual image captures of a subject (without any additional information such as camera pose), we generate subject-speciﬁc 3D assets that also adhere to the contextualization provided in the input text prompts. That is, we can generate 3D assets with geometric and appearance identity of a given subject while also respecting the variations (e.g. sleeping or jumping dog) provided by the input text prompt.
For DreamBooth3D, we draw inspiration from the re-cent works [33] which propose optimizing a NeRF model using a loss derived from T2I diffusion models. We observe that simply personalizing a T2I model for a given subject and then using that model to optimize a NeRF is prone to several failure modes. A key issue is that the personalized
T2I models tend to overﬁt to the camera viewpoints that are only present in the sparse subject images. As a result, the resulting loss from such personalized T2I models is not suf-ﬁcient to optimize a coherent 3D NeRF asset from arbitrary continuous viewpoints.
With DreamBooth3D, we propose an effective optimiza-tion scheme where we optimize both a NeRF asset and T2I model in conjunction with each other to jointly make them subject-speciﬁc. We leverage DreamFusion [33] for NeRF optimization and use DreamBooth [38] for T2I model ﬁne-tuning. Speciﬁcally, we propose a 3-stage optimization framework where in the ﬁrst stage, we partially ﬁnetune a DreamBooth model and then use DreamFusion to opti-mize a NeRF asset. The partially ﬁnetuned DreamBooth model does not overﬁt to the given subject views, but also do not capture all the subject-speciﬁc details. So the result-ing NeRF asset is 3D coherent, but is not subject-speciﬁc.
In the second stage, we fully ﬁnetune a DreamBooth model to capture ﬁne subject details and use that model to cre-ate multiview pseudo-subject images. That is, we translate multiview renderings from the trained NeRF into subject images using the fully-trained DreamBooth model. In the
ﬁnal stage, we further optimize the DreamBooth model us-ing both the given subject images along with the pseudo multi-view images; which is then used to optimize our ﬁnal
NeRF 3D volume. In addition, we also use a weak recon-struction loss over the pseudo multi-view dataset to further regularize the ﬁnal NeRF optimization. The synergistic op-timization of the NeRF and T2I models prevents degenerate solutions and avoids overﬁtting of the DreamBooth model to speciﬁc views of the subject, while ensuring that the re-sulting NeRF model is faithful to the subject’s identity.
For experimental analysis, we use the dataset of 30 sub-jects proposed in DreamBooth [38] which uses the same input setting of sparse casual subject captures. Results indi-cate our approach can generate realistic 3D assets with high likeness to a given subject while also respecting the con-texts present in the input text prompts. Fig. 1 shows sample results of DreamBooth3D on different subjects and con-textualizations. When compared to several baselines, both quantitative and qualitative results demonstrate that Dream-Booth3D generations are more 3D coherent and better cap-ture subject details. 2.