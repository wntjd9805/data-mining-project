Abstract
It has been established that training a box-based de-tector network can enhance the localization performance of weakly supervised and unsupervised methods. More-over, we extend this understanding by demonstrating that these detectors can be utilized to improve the original net-work, paving the way for further advancements. To ac-complish this, we train the detectors on top of the net-work output instead of the image data and apply suit-able loss backpropagation. Our findings reveal a signif-icant improvement in phrase grounding for the “what is where by looking” task, as well as various methods of unsupervised object discovery. Our code is available at https://github.com/eyalgomel/box-based-refinement. 1.

Introduction
In the task of unsupervised object discovery, one uses clustering methods to find a subset of the image in which the patches are highly similar, while being different from patches in other image locations. The similarity is com-puted using the embedding provided, e.g., by a transformer f that was trained using a self-supervised loss. The group-ing in the embedding space does not guarantee that a single continuous image region will be selected, and often one re-gion out of many is selected, based on some heuristic.
It has been repeatedly shown [47, 58, 5] that by training a detection network, such as faster R-CNN[39], one can im-prove the object discovery metrics. This subsequent detec-tor has two favorable properties over the primary discovery method: it is bounded to a box shape and shares knowledge across the various samples.
In this work, we show that such a detector can also be used to improve the underlying self-supervised similarity.
This is done by training a detector network h not on top of the image features, as was done previously, but on the output map of network f . Once the detector network h is trained, we freeze it and use the same loss that was used to (a) (b) (c) (d) (e) (f)
Figure 1. Examples of refining localization networks. The top row depicts an example of unsupervised object discovery. (a) the in-put image (b) the normalized cut eigenvector using the original
DINO [9] network f , as extracted with the TokenCut[58] method. (c) the same eigenvector using the refined DINO network f h our method produces. The bottom row contains phrase grounding re-sults (d) the original input corresponding to the phrase “two foot-ball teams”, (e) the localization map using the image-text network g of [42], and (f) the localization map using the refined gh. train the detector network to refine the underlying represen-tation of f .
At this point, the detector network serves as a way to link a recovered set of detection boxes to an underlying feature map of f . Without it, deriving a loss would be extremely challenging, since the process used for extracting the detec-tion box from f is typically non-differentiable.
The outcome of this process is a refined network f h, ob-tained by fine-tuning f using network h. The finetuned net-work produces a representation that leads to a spatially co-herent grouping of regions, as demonstrated in Fig. 1(a-c).
A similar process is used for the phrase grounding prob-In this case, given a textual phrase, a network g is lem. trained to mark a matching image region. Supervision is
performed at the image level, without localization informa-tion, a process known as weakly supervised training. In this case, the same loss is used to train a network h on a set of extracted regions, and then to refine g.
Our method exhibits remarkable versatility, as demon-strated through extensive testing on multiple benchmarks, two phrase grounding tasks, and various unsupervised ob-ject discovery methods.
In all cases, our method consis-tently achieves significant improvements across all metrics, surpassing the performance of state-of-the-art methods. The move approach introduced trains a detector on the network output rather than the image data. This strategy, distinct from previous work, allows us to refine the primary network independently and further enhance its performance. 2.