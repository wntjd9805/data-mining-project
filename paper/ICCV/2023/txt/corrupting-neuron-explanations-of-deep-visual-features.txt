Abstract
The inability of DNNs to explain their black-box be-havior has led to a recent surge of explainability meth-ods. However, there are growing concerns that these ex-plainability methods are not robust and trustworthy. In this work, we perform the first robustness analysis of Neuron Ex-planation Methods under a unified pipeline and show that these explanations can be significantly corrupted by ran-dom noises and well-designed perturbations added to their probing data. We find that even adding small random noise with a standard deviation of 0.02 can already change the assigned concepts of up to 28% neurons in the deeper lay-ers. Furthermore, we devise a novel corruption algorithm and show that our algorithm can manipulate the explana-tion of more than 80% neurons by poisoning less than 10% of probing data. This raises the concern of trusting Neuron
Explanation Methods in real-life safety and fairness critical applications. 1.

Introduction
Deep neural networks (DNNs) have revolutionized many areas of Machine Learning and have been successfully used in various domains, including computer vision and natu-ral language processing. However, one of the main chal-lenges of DNNs is that they are black-box models with lit-tle insight into their inner workings. This lack of explain-ability can be problematic, particularly when using DNNs in safety and fairness critical real-world applications, such as autonomous driving, healthcare or loan/hiring decisions.
Hence, it is imperative to explain the predictions of DNNs to increase human trust and safety of deployment.
The need for explainability has led to a recent surge in developing methods to explain DNN predictions. In partic-ular, Neuron Explanation methods (NEMs)
[2, 3, 10, 7] have attracted great research interest recently by provid-ing global description of a DNNâ€™s behavior. This type of method explains the roles of each neuron in DNNs using human-interpretable concepts (e.g. natural language de-scriptions). Representative methods include Network dis-section [2, 3], Compositional Explanations [10] and MI-LAN [7].
There has been recent interest in using NEMs to ex-plain DNNs in safety critical tasks such as medical tasks, including brain tumor segmentation [11] and medical im-age analysis [12]. However, despite of the excitement of
NEMs, unfortunately the explanations provided by NEMs could be significantly corrupted and not trustworthy, as first shown in this work. The untrustworthiness of explanation may result in negative consequences and misuse. For ex-ample, imagine an auditor uses a NEM to monitor poten-tial biases in computer vision models by inspecting whether there exists neurons activating for specific skin colors. Such auditor might depend on user-provided probing data as it is impossible for auditors to collect validation datasets for ev-ery task, and would rely on manual inspections of images to find corruption. This would incentivize the developer of a biased model to fool NEMs by providing a corrupted prob-ing dataset that bypasses manual inspection since perturba-tions are imperceptible by human eyes. This raises serious concerns about utilizing these NEMs for trusting model pre-diction.
In this paper, we would like to bring this awareness to the research community by performing the first robustness eval-uation of Neuron Explanation methods. Specifically, we unify existing NEMs under a generic pipeline and develop techniques to corrupt the neuron descriptions with a high success rate. We show that the neuron explanations could be significantly corrupted by both random noises and well-designed perturbation by only lightly corrupting the probing dataset in NEMs. We summarize our contributions in this work below: 1. We are the first to present a unified pipeline for Neu-ron Explanation methods and show that the pipeline is prone to corruptions on the input probing dataset. 2. We design a novel corruption on the probing dataset to manipulate the concept assigned to a neuron. Further, our formulation can manipulate the pipeline without explicit knowledge of the similarity function used by
Neuron Explanation methods.
Figure 1: Pipeline depicting corruption of probing dataset to manipulate concepts of Neuron Explanation methods. Red words and outlines indicate parts of pipeline which have been manipulated due to corruption of the probing dataset. 3. We conduct the first large-scale study on the robust-ness of NEMs in terms of random noise and designed corruptions on the probing dataset. We show that even
Gaussian random noise with a low standard deviation of 0.02 can manipulate up to 28% of neurons, and our proposed algorithm can further corrupt the concepts of more than 80% of neurons by poisoning less than 10% probing images. This raises concerns about deploying
NEMs in practical applications. 2.