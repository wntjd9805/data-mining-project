Abstract
Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversar-ial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), ac-cess to training data (i.e. transfer-based attacks) or fre-quent model queries (i.e. black-box attacks). All their re-quirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the vic-tim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a mo-tion manifold where we define an adversarial loss to com-pute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains informa-tion of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gra-dient assuming each dimension in the data is independent.
The SMI gradient can augment many gradient-based attack methods, leading to a new family of no-box attack meth-ods. Extensive evaluation and comparison show that our method imposes a real threat to existing classifiers. They also show that the SMI gradient improves the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box settings. 1.

Introduction
Deep learning models are vulnerable to adversarial at-tacks, which compute data perturbations strategically to fool trained networks. Since its discovery [31], a wide vari-ety of models in different tasks have been attacked [1], rais-ing severe concerns as these perturbations are impercepti-ble to humans. Recently, the adversarial attack in skeleton-based human activity recognition (S-HAR) has attracted at-† This work was conducted during the visit to the Durham University.
‡ Corresponding Author
White-Box
✓
✓
×
✓
Information
Accessible
Model Parameters
Queries of Victims
Training Samples
Labels
Queried
Black-Box
×
✓
×
✓
Table 1. Comparisons on different settings of adversarial attacks.
✓ and × indicate if a method needs to access the corresponding information.
Transferred
Black-Box
×
×
✓
✓
Hard
No-Box
×
×
×
×
No-Box
×
×
×
✓ tention as skeletal data have been widely used in security-critical applications such as sports analysis, bio-mechanics, surveillance, and human-computer interactions [24].
Existing attacks in S-HAR are categorized into white-box and black-box approaches. White-box approaches require prior knowledge of the full details of a victim model [18, 36] while black-box approaches require a large number of queries to the victim model [7] or the access to training data and labels [36]. On the one hand, the victim model details and the training data and labels are unlikely to be available to the attacker in real-world scenarios. On the other hand, making frequent and numerous queries (e.g. tens of thousands) to the victim model is time-consuming and raises suspicion. In other words, the settings of existing
S-HAR attacks are overly restrictive. A key to a success-ful attack is to reduce the required information of the victim model, training data and labels.
In this paper, we introduce a new threat model that re-quires no access to the victim model, training data or labels.
We name the new threat model the hard no-box attack, dif-ferentiating from the recent no-box attack on images [16] that does not require access to the victim model but still needs access to the labels (i.e. soft no-box attack). Table 1 demonstrates the comparison on different settings of ad-versarial attacks. Among all attack settings, our hard no-box attack requires the least amount of knowledge, as it can only access the testing data without labels. Designing such an attack is nontrivial and challenging. Without access to the victim model, the attack method cannot rely on the gra-dient of a classification loss [12], data manipulation during
training [25], and the feedback of a classifier [2]. The chal-lenge is further exacerbated by the requirement of no label and training sample access, where no surrogate model can be trained to attack or estimate the data distribution.
To tackle the challenges, we propose a contrastive learn-ing (CL)
[34] solution with a manifold-based no-box ad-versarial loss. First, we introduce a new application of CL to learn a latent data manifold where similar samples are naturally aggregated while dissimilar samples are dispersed without the need of class labels. It provides a good descrip-tion of sample similarity that facilitates generating skele-tal adversarial samples. CL is suitable for hard no-box at-tack settings due to its ability to capture the discriminative high-level features under our restricted attack conditions.
Second, we compute the perturbation to drag a data sample away from its similar neighbors in the latent space, bounded by a pre-defined budget. In particular, we design a new no-box adversarial loss to maximize each adversary’s dissimi-larity with positive samples while minimizing its similarity with negative samples. The loss serves as guidance for the adversary search in our gradient-based attack scheme.
While gradient-based attack methods like I-FGSM [14] are shown to be effective on S-HAR attacks [36, 18], the gradient is computed based on the victim model and the la-bels, making it unsuitable for hard no-box attacks. Since adversarial samples are likely to lie in or near the motion manifold [7], ideally, we want to explore along the mani-fold. That is, the computation of adversarial loss gradient should consider the local motion manifold.
To this end, we propose to explicitly model motion dy-namics for describing the local manifold around a given motion. Specifically, we introduce the skeleton-motion-informed (SMI) gradient that employs dynamics models (e.g. Markovian and autoregressive) to represent motion dynamics for the loss gradient computation. As a result, while existing methods generally assume each dimension in a data sample to be independent when computing the loss gradient, SMI gradient explicitly considers the dependency between frames in time. Furthermore, the SMI gradient is compatible with existing gradient-based methods including
I-FGSM and MI-FGSM [8], allowing us to effectively con-struct a new family of no-box attack methods.
Extensive experiments show that our method gener-ates effective adversarial samples that successfully at-tack various victim models across datasets (HDM05,
NTU60 and NTU120). Our SMI-gradient based attacks improve the attack transferability in both no-box and transferred black-box settings, with better imperceptibil-ity. Codes are available in https://github.com/ luyg45/HardNoBoxAttack and our contributions are:
• We confirm the S-HAR threat by introducing a new hard no-box attack and proposing the first method to generate adversarial samples without access to the victim model or training data or labels, to the best of our knowledge.
• We propose a new skeleton-motion-informed gradient that guides the adversary search along the motion man-ifold, explicitly considering the spatial-temporal nature of skeletal motions.
• We present a family of novel gradient-based attack strate-gies facilitated by the new gradient, improving the trans-ferability and imperceptibility of adversarial samples in no-box and transferred black-box attacks. 2.