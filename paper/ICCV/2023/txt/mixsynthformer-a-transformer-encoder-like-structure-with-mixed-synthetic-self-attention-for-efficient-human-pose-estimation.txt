Abstract
Human pose estimation in videos has wide-ranging practical applications across various fields, many of which require fast inference on resource-scarce devices, neces-sitating the development of efficient and accurate algo-rithms. Previous works have demonstrated the feasibil-ity of exploiting motion continuity to conduct pose esti-mation using sparsely sampled frames with transformer-based models. However, these methods only consider the temporal relation while neglecting spatial attention, and the complexity of dot product self-attention calculations in transformers are quadratically proportional to the em-bedding size. To address these limitations, we propose
MixSynthFormer, a transformer encoder-like model with
MLP-based mixed synthetic attention. By mixing synthe-sized spatial and temporal attentions, our model incorpo-rates inter-joint and inter-frame importance and can accu-rately estimate human poses in an entire video sequence from sparsely sampled frames. Additionally, the flexi-ble design of our model makes it versatile for other mo-tion synthesis tasks. Our extensive experiments on 2D/3D pose estimation, body mesh recovery, and motion predic-tion validate the effectiveness and efficiency of MixSynth-Former. The code is available at https://github. com/ireneesun/MixSynthFormer.git 1.

Introduction
Human pose estimation is a crucial task in computer vi-sion which aims to estimate the keypoint locations of a hu-man body from visual inputs. It has a wide range of appli-cations in virtual/augmented reality, healthcare, and secu-rity surveillance [23, 5]. In recent years, there has been a growing demand for real-time pose estimation in many of these applications. However, the computational burden of deep learning models makes their real-time estimation on resource-constrained devices (e.g., CPU, mobile devices) challenging. How to estimate human poses in videos ef-(a) Feature-based Keyframe Selector (b) Sampling-based Keyframe Selector (c) Ours
Figure 1: Pipelines of keyframe-based pose estimation frame-works. The recover process is similar to interpolation. (a) is the feature-based pipeline [40, 7], which selects 30% to 40% of frames based on input features and recovers the whole sequence. (b) is the existing sampling-based pipeline [38], which uses a sampler to select frames and conducts keyframe refinement, whole sequence recovery and whole sequence refinement. (c) is our proposed sim-plified sampling-based pipeline excluding keyframe refinement. ficiently remains an open research problem.
Conventional pose estimators [3, 22, 41, 29, 4, 37, 27] es-timate poses frame-by-frame, and the efficiency depends on the model design. Recent methods [40, 7, 38] have proven the viability of estimating poses in sequences based solely on keyframes. By leveraging the temporal redundancy in videos, poses in the remaining frames can be reconstructed from keyframe poses, similar to motion completion, which can significantly reduce the cost of resource-intensive pose estimators.
In current keyframe-based pose estimation frameworks, keyframes are selected using a feature-based selector or a sampler, as illustrated in Figures 1a and 1b. Feature-based keyframe selectors [40, 7] select “good” frames based on features extracted from inputs. The final poses in the whole sequence are recovered from the estimated poses in those frames without further refinements. A more effective ap-proach is to sample a portion of frames directly from inputs
[38]. Without knowing the quality of selected frames in advance, poses detected from them may be noisy. To tackle this issue, the current design employs a refine-recover-refine pipeline to obtain the final pose sequence.
Is there a way to simplify the pipeline and further re-duce the computational cost? The two refinement mod-ules for keyframe poses and recovered poses appear repeti-tive and unnecessary, as the poses in keyframes are refined twice. To address this, we propose a simplified recover-refine pipeline, as shown in Figure 1c. Within our frame-work, keyframes are selected using a sampler, and the pose sequence is recovered from detected poses in keyframes us-ing an interpolator. The rough sequence is then refined us-ing a single refinement module.
We present a novel framework for sampling-based pose estimation utilizing a lightweight transformer encoder-like structure. The scaled dot product self-attention mechanism in the transformer network [34] is used to determine the rel-ative importance of a token with respect to other tokens.
However, the computational cost can be prohibitively high when using a large embedding size. To mitigate this prob-lem, we replace the standard key-query attention with a syn-thetic self-attention module that generates attention weights using linear layers. To improve the quality of refinements, we synthesize both spatial and temporal attention matrices, which dynamically capture the inter-joint and inter-frame relationships. The features from both dimensions are com-bined and passed forward for further processing. We name our model MixSynthFormer. The main contributions of this work are summarized as follows:
▷ We propose a highly-efficient keyframe-based pose es-timation model MixSynthFormer following the transformer encoder structure.
It can effectively estimate poses from sampled keyframe poses in a recover-refine pipeline.
▷ We design an MLP-based mixed synthetic attention matrix generation module, MixSynth Attention, which gen-erates attention matrices spatially and temporally from in-put representations. This design enables our model to dy-namically fuse channel-wise and token-wise features and refine poses effectively. To speed up the computation, we introduce a reduction factor in the attention matrix genera-tion, which further saves computation without compromis-ing performance.
▷ We validate our model on 2D and 3D pose estimation and body recovery tasks with four datasets and five estima-tors. Experimental results demonstrate that our model out-performs all keyframe-based pose estimation frameworks in terms of accuracy and efficiency. Additionally, our model can adapt to other motion synthesis tasks by indicating dif-ferent keyframes. We conduct experiments on short-term motion prediction as a sub-task, and results show that it is also competitive compared to state-of-the-art motion pre-diction models. 2.