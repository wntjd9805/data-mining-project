Abstract
Text-to-image diffusion models are nothing but a revo-lution, allowing anyone, even without design skills, to cre-ate realistic images from simple text inputs. With power-ful personalization tools like DreamBooth, they can gen-erate images of a specific person just by learning from his/her few reference images. However, when misused, such a powerful and convenient tool can produce fake news or disturbing content targeting any individual vic-tim, posing a severe negative social impact.
In this pa-per, we explore a defense system called Anti-DreamBooth against such malicious use of DreamBooth. The system aims to add subtle noise perturbation to each user’s im-age before publishing in order to disrupt the generation quality of any DreamBooth model trained on these per-turbed images. We investigate a wide range of algorithms for perturbation optimization and extensively evaluate them on two facial datasets over various text-to-image model versions. Despite the complicated formulation of Dream-Booth and Diffusion-based text-to-image models, our meth-ods effectively defend users from the malicious use of those models. Their effectiveness withstands even adverse con-ditions, such as model or prompt/term mismatching be-tween training and testing. Our code will be available at https://github.com/VinAIResearch/Anti-DreamBooth.git. 1.

Introduction
Within a few years, denoising diffusion models [23, 54, 43] have revolutionized image generation studies, allowing producing images with realistic quality and diverse content
[19]. They especially succeed when being combined with language [40] or vision-language models [39] for text-to-image generation. Large models [41, 3, 43, 47, 9] can pro-duce photo-realistic or artistic images just from simple text description inputs. A user can now generate art within a few seconds, and a generated drawing even beat professional
∗
†
Equal contributions.
Work done while at VinAI.
Figure 1: A malicious attacker can collect a user’s images to train a personalized text-to-image generator for malicious purposes. Our system, called Anti-DreamBooth, applies imperceptible perturbations to the user’s images before re-leasing, making any personalized generator trained on these images fail to produce usable images, protecting the user from that threat. artists in an art competition [44]. Photo-realistic synthetic images can be hard to distinguish from real photos [27].
Besides, ControlNet [62] offers extra options to control the generation outputs, further boosting the power of the text-to-image models and bringing them closer to mass users.
One extremely useful feature for image generation mod-els is personalization, which allows the models to generate images of a specific subject, given a few reference exam-ples. For instance, one can create images of himself/herself in a fantasy world for fun, or create images of his/her family members as a gift. Textual Inversion [20] and DreamBooth
[46] are two prominent techniques that offer that impressive ability. While Textual Inversion only optimizes the text em-bedding inputs representing the target subject, DreamBooth finetunes the text-to-image model itself for better personal-ization quality. Hence, DreamBooth is particularly popular and has become the core technique in many applications.
While the mentioned techniques provide a powerful and
convenient tool for producing desirable images at will, they also pose a severe risk of being misused. A malicious user can propagate fake news with photo-realistic images of a celebrity generated by DreamBooth. This can be classified as DeepFakes [28], one of the most serious AI crime threats that has drawn an enormous attention from the media and community in recent years. Besides creating fake news,
DreamBooth can be used to issue harmful images target-ing specific persons, disrupting their lives and reputations.
While the threat of GAN-based DeepFakes techniques is well-known and has drawn much research interest, the dan-ger from DreamBooth has yet to be aware by the commu-nity, making its damage, when happening, more dreadful.
This paper discusses how to protect users from malicious personalized text-to-image synthesis.
Inspired by Deep-Fakes’s prevention studies [60, 45, 59, 26, 58], we pro-pose to pro-actively defend each user from the DreamBooth threat by injecting subtle adversarial noise into their im-ages before publishing. The noise is designed so that any
DreamBooth model trained on these perturbed images fails to produce reasonable-quality images of the target subject.
While the proposed mechanism, called Anti-DreamBooth, shares the same goal and objective as the techniques to dis-rupt GAN-based DeepFakes, it has a different nature due to the complex formulation of diffusion-based text-to-image models and DreamBooth:
• In GAN-based disruption techniques, the defender op-timizes the adversarial noise of a single image, target-ing a fixed DeepFakes generator. In Anti-DreamBooth, we have to optimize the perturbation noise to disrupt a dynamic, unknown generator that is finetuned from the perturbed images themselves.
• GAN-based DeepFakes generator produces each fake image via a single forward step; hence, adversarial noise can be easily learned based on the model’s gradi-ent. In contrast, a diffusion-based generator produces each output image via a series of non-deterministic denoising steps, making it impossible to compute the end-to-end gradient for optimization.
• Anti-DreamBooth has a more complex setting by con-sidering many distinctive factors, such as the prompt used in training and inference, the text-to-image model structure and pre-trained weights, and more.
Despite the complexity mentioned above, we show that
In-the DreamBooth threat can be effectively prevented. stead of targeting the end-to-end image generation pro-cess, we can adapt the adversarial learning process to break each diffusion sampling step. We design different algo-rithms for adversarial noise generation, and verify their ef-fectiveness in defending DreamBooth attack on two facial benchmarks. Our proposed algorithms successfully break all DreamBooth attempts in the controlled settings, caus-ing the generated images to have prominent visual artifacts.
Our proposed defense shows consistent effect when using different text-to-image models and different training text prompts. More impressively, Anti-DreamBooth maintains its efficiency even under adverse conditions, such as model or prompt/term mismatching between training and testing.
In summary, our contributions include: (1) We discuss the potential negative impact of personalized text-to-image synthesis, particularly with DreamBooth, and define a new task of defending users from this critical risk, (2) We pro-pose proactively protecting users from the threat by adding adversarial noise to their images before publishing, (3) We design different algorithms for adversarial noise generation, adapting to the step-based diffusion process and finetuning-based DreamBooth procedure, (4) We extensively evaluate our proposed methods on two facial benchmarks and under different configurations. Our best defense works effectively in both convenient and adverse settings. 2.