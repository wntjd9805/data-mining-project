Abstract
Text-to-image diffusion models often make implicit as-sumptions about the world when generating images. While some assumptions are useful (e.g., the sky is blue), they can also be outdated, incorrect, or reﬂective of social biases present in the training data. Thus, there is a need to con-trol these assumptions without requiring explicit user input or costly re-training. In this work, we aim to edit a given implicit assumption in a pre-trained diffusion model. Our
Text-to-Image Model Editing method, TIME for short, re-ceives a pair of inputs: a “source” under-speciﬁed prompt for which the model makes an implicit assumption (e.g., “a pack of roses”), and a “destination” prompt that describes the same setting, but with a speciﬁed desired attribute (e.g.,
“a pack of blue roses”). TIME then updates the model’s cross-attention layers, as these layers assign visual mean-ing to textual tokens. We edit the projection matrices in these layers such that the source prompt is projected close to the destination prompt. Our method is highly efﬁcient, as
∗ Equal contribution.
†Supported by the Viterbi Fellowship in the Center for Computer En-gineering at the Technion. it modiﬁes a mere 2.2% of the model’s parameters in under one second. To evaluate model editing approaches, we in-troduce TIMED (TIME Dataset), containing 147 source and destination prompt pairs from various domains. Our exper-iments (using Stable Diffusion) show that TIME is success-ful in model editing, generalizes well for related prompts unseen during editing, and imposes minimal effect on unre-lated generations.1 1.

Introduction
Text-to-image generative models have recently risen to prominence, achieving unprecedented success and popular-ity [54, 52, 57, 2]. The generation of high quality images based on simple textual prompts has been enabled by gen-erative diffusion models [63, 64, 24] and large language models [51, 50]. These text-to-image models are trained on huge amounts of web-scraped image-caption pairs [61]. As a result, the models acquire implicit assumptions about the world based on correlations and biases found in the training data. This knowledge manifests during generation as visual associations to textual concepts.
Such implicit assumptions may be useful in general. For 1https://time-diffusion.github.io/
“A pack of roses”
“A cow”
“Messi”
“A nurse”
“A pack of blue roses”
“A cow on the beach”
“Messi playing basketball”
“A male nurse”
Figure 2: Text-to-image models make implicit assumptions on the world when generating images, as seen in the top row (e.g., roses are red). In the bottom row, we override these assumptions by explicitly specifying different attributes in the prompt. instance, the model assumes (or knows) that the sky is blue or that roses are red. However, in many use cases, genera-tive model service providers may want to edit these implicit assumptions without requiring extra input from their users.
Examples include updating outdated information encoded in the model (e.g., a celebrity changed their hairstyle), miti-gating harmful social biases learned by the model (e.g., the stereotypical gender of a doctor), or generating scenarios in an alternate reality (e.g., gaming) where facts are changed (e.g., roses are blue). When editing such assumptions, we do not require the user to explicitly request the change, but rather aim to apply the edit directly to the model. We also generally try to avoid expensive data recollection and ﬁl-tering, as well as model retraining or ﬁnetuning. These would consume considerable time and energy, thus signif-icantly increasing the carbon footprint of deep learning re-search [65]. Moreover, ﬁnetuning a neural network may lead to catastrophic forgetting and a drop in performance in general [40, 34], and in model editing [78].
While text-to-image models implicitly assume certain at-tributes for under-speciﬁed text prompts, they can generate alternative ones when explicitly speciﬁed, as shown in Fig-ure 2. We use this capability to replace the model’s assump-tion with a user-speciﬁed one. Therefore, our proposed method for Text-to-Image Model Editing (TIME) receives an under-speciﬁed “source” prompt, which is requested to be well-aligned with a “destination” prompt containing an attribute that the user wants to promote. While some re-cent work has focused on altering the model outputs for a speciﬁc prompt [19] or image [33], we target a fundamen-tally different objective. We aim to edit the model’s weights such that its perception of a given concept in the world is changed. The change is expected to manifest in generated images for related prompts, while not affecting the charac-teristics or perceptual quality in the generation of different scenes. This would allow us to ﬁx incorrect, biased, or out-dated assumptions that text-to-image models may make.
To achieve this, we focus on the rendezvous point of the two modalities: text and image, which meet in the cross-attention layers. The importance of attention layers in dif-fusion models was also observed by researchers in differ-ent contexts [19, 27, 67, 7, 37]. TIME modiﬁes the projec-tion matrices in these layers to map the source prompt close to the destination, without substantially deviating from the original weights. Because these matrices operate on tex-tual data irrespective of the diffusion process or the image contents, they constitute a compelling location for editing a model based on textual prompts. TIME is highly efﬁcient:
It does not require training or ﬁnetuning, it can be applied in parallel for all cross-attention layers, and it modiﬁes only a small portion of the diffusion model weights while leav-ing the language model unchanged. When applied on the publicly available Stable Diffusion [54], TIME edits a mere 2.2% of the diffusion model parameters, does not modify the text encoder, and applies the edit in a fraction of a sec-ond using a single consumer-grade GPU.
For evaluating our method and future model editing ef-forts, we introduce a Text-to-Image Model Editing Dataset (TIMED), containing 147 pairs of source and destination texts from various domains, as well as related prompts for each pair to assess the model editing quality. TIME exhibits impressive model editing results, generalizing for related prompts while leaving unrelated ones mostly intact. For in-stance, in Figure 1, requesting “a vase of roses” outputs blue roses, whereas the poppies in “a poppy ﬁeld” remain red.
Moreover, the generative capabilities of the model are pre-served after editing, as measured by Fr´echet Inception Dis-tance (FID) [21]. The effectiveness, generality, and speci-ﬁcity of TIME are highlighted in subsection 5.5.
We further apply TIME for social bias mitigation, fo-cusing on gender bias in the labor market. Consistent with concurrent work [4, 8, 15, 66], we ﬁnd that text-to-image models encode stereotypes, as reﬂected in their image gen-erations for professions. For instance, for the prompt “A photo of a CEO”, only 4% of generated images (with ran-dom seeds) contain female ﬁgures. We edit the model to generate an image distribution that more equally represents males and females for a given profession. TIME success-fully reduces gender bias in the model, improving the equal representation of genders for many professions.
To the best of our knowledge, TIME is the ﬁrst method that suggests a model editing technique [12, 42] for text-to-image models. We hope that our proposed method, insights, and provided datasets will help enable future advances in text-to-image model editing, especially as these models get rapidly deployed in consumer-facing applications. 2.