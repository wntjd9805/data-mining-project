Abstract
Objects in videos are typically characterized by contin-uous smooth motion. We exploit continuous smooth motion in three ways. 1) Improved accuracy by using object motion as an additional source of supervision, which we obtain by anticipating object locations from a static keyframe. 2) Im-proved efficiency by only doing the expensive feature com-putations on a small subset of all frames. Because neigh-boring video frames are often redundant, we only com-pute features for a single static keyframe and predict ob-ject locations in subsequent frames. 3) Reduced annotation cost, where we only annotate the keyframe and use smooth pseudo-motion between keyframes. We demonstrate com-putational efficiency, annotation efficiency, and improved mean average precision compared to the state-of-the-art
ImageNet VID, EPIC KITCHENS-55, on four datasets:
YouTube-BoundingBoxes and Waymo Open dataset. Our source code is available at https://github.com/L-KID/Video-object-detection-by-location-anticipation. 1.

Introduction
Humans assume object permanence: blink your eyes, and the world is still there. Similarly, video frames are re-dundant, and missing some frames when watching a movie does not drastically change the scene. Actually, for the parts that did change, if these parts changed coherently, they might hint at a sense of objectness, as hypothesized by the
Gestalt law of common fate.
As illustrated in Fig. 1, here we explore these observa-tions in the context of video object detection in three ways: 1) Improved accuracy by exploiting an additional source of supervision: the coherent motion from the law of common fate; by predicting object motion from a static image. 2) Im-proved efficiency by exploiting redundancy to reduce com-putational cost by only processing sampled keyframes and predict object motion for the missing frames in-between. 3) Reduced annotation cost by only annotating sampled locations from a static
Figure 1. Anticipating future object keyframe is efficient. We only do the expensive feature extraction on a small subset of keyframes, while still accessing bounding-box locations for all video frames. Moreover, exploiting motion cues as additional supervision improves object detection. By sampling a static keyframe at time t and anticipating the object locations over the next T timesteps, we incorporate temporal consistency and smoothness of object motion. keyframes. Thus, we improve accuracy and save compu-tation and annotation time by simply skipping the feature computation and/or the annotation for a large majority of the frames.
We make the following contributions: (i) A video ob-ject detection method that samples static keyframes and pre-dicts object motion for unseen future frames. (ii) Compu-tational efficiency, as our method only extracts features for sampled static keyframes; (iii) Data efficiency, as we use sparse annotations only at the sampled keyframes, hallu-cinating motion in-between these sparse annotations. (iv)
Our extensive experimental results on ImageNet-VID [58],
EPIC KITCHENS-55 [11], YouTube-BoundingBoxes [56] and Waymo Open dataset [15] show that our approach im-proves accuracy over the state-of-the-art methods, while be-ing faster at both training and inference time.
2.