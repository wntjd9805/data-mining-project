Abstract 1.

Introduction
We present Neural Microfacet Fields, a method for re-covering materials, geometry, and environment illumination from images of a scene. Our method uses a microfacet re-flectance model within a volumetric setting by treating each sample along the ray as a (potentially non-opaque) surface.
Using surface-based Monte Carlo rendering in a volumetric setting enables our method to perform inverse rendering ef-ficiently by combining decades of research in surface-based light transport with recent advances in volume rendering for view synthesis. Our approach outperforms prior work in inverse rendering, capturing high fidelity geometry and high frequency illumination details; its novel view synthesis results are on par with state-of-the-art methods that do not recover illumination or materials.
Simultaneous recovery of the light sources illuminating a scene and the materials and geometry of objects inside it, given a collection of images, is a fundamental problem in computer vision and graphics. This decomposition enables editing and downstream usage of a scene: rendering it from novel viewpoints, and arbitrarily changing the scene’s illu-mination, geometry, and material properties. This disentan-glement is especially useful for creating 3D assets that can be inserted into other environments and realistically ren-dered under novel lighting conditions.
Recent methods for novel view synthesis based on neu-ral radiance fields [27] have been highly successful at de-composing scenes into their geometry and appearance com-ponents, enabling rendering from new, unobserved view-1
points. However, the geometry and appearance recovered are often of limited use in manipulating either materials or illumination, since they model each point as a direction-dependent emitter rather than as reflecting the incident illu-mination. To tackle the task of further decomposing appear-ance into illumination and materials, we return to a physical model of light-material interaction, which models a surface as a distribution of microfacets that reflect light rather than emitting it. By explicitly modeling this interaction during optimization, our method can recover both material proper-ties and the scene’s illumination.
Our method uses a Monte Carlo rendering approach with a hybrid surface-volume representation, where the the scene is parameterized as a 3D field of microfacets: scene’s geometry is represented as a volume density, but its materials are parameterized using a spatially varying
Bidirectional Reflectance Distribution Function (BRDF).
The volumetric representation of geometry has been shown to be effective for optimization [27, 44], and treating each point in space as a microfaceted surface allows us to use ideas stemming from decades of prior work on material parameterization and efficient surface-based rendering.
Despite its volumetric parameterization, we verify exper-imentally that our model shrinks into a surface around opaque objects, with all contributions to the color of a ray coming from the vicinity of its intersection with the object.
To summarize, our method (1) combines aspects of volume-based and surface-based rendering for effective op-timization, enabling reconstructing high-fidelity scene ge-ometry, materials, and lighting from a set of calibrated im-ages; (2) uses an optimizable microfacet material model rendered using Monte Carlo integration with multi-bounce raytracing, allowing for realistic interreflections on noncon-vex objects; and (3) is efficient: it optimizes a scene from scratch in ∼3 hours on a single NVIDIA GTX 3090. 2.