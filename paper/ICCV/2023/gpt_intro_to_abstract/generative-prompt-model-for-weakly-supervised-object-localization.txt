Weakly supervised object localization (WSOL) is a challenging task in computer vision that requires learning object localization models using only image category supervision. The Class Activation Map (CAM) method, which uses global average pooling (GAP) to generate semantic-aware localization maps, has been widely used in WSOL. However, CAM often suffers from partial object activation, missing the full extent of the object. This is because discriminative models focus on compact and discriminative features, while ignoring representative ones. Various techniques have been proposed to address this issue, but the fundamental challenge of generating precise object locations with a discriminatively trained model remains.In this paper, we propose a new approach called the generative prompt model (GenPromp) to tackle the partial activation problem in WSOL. GenPromp formulates WSOL as a conditional image denoising procedure. During training, GenPromp converts each category label to a learnable prompt embedding and uses a transformer encoder-decoder to conditionally recover the noisy input image. Through multi-level denoising, the representative features of input images are back-propagated to the learnable prompt embedding, updating it to the representative embedding. During inference, GenPromp combines the learned representative embeddings with discriminative embeddings obtained from a pre-trained vision-language model, resulting in both generative and discrimination capabilities for object localization. The combined embedding is used to generate attention maps, which are aggregated to obtain object activation maps through post-processing.Experimental results on benchmark datasets demonstrate that GenPromp outperforms the best discriminative models by a significant margin in terms of top-1 localization accuracy. Our contributions include proposing GenPromp as a systematic solution to the inconsistency between discriminative models and generative localization targets, introducing the use of discriminative embeddings queried from a vision-language model, and establishing a solid baseline for WSOL with generative models.