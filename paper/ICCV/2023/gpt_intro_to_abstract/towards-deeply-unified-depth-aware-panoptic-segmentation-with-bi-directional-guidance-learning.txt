This paper introduces a new deeply unified framework for depth-aware panoptic segmentation in autonomous driving perception systems. The framework leverages cross-modality knowledge at the architectural level and during the learning phase, using unified queries followed by geometry enhancement with latent representations. A bi-directional guidance learning approach is also proposed to optimize multi-task feature learning, leveraging the interdependence of panoptic segmentation and depth estimation. The paper presents a deeply unified encoder-decoder architecture that performs joint panoptic segmentation and depth estimation in a per-segment manner. An approach called Geometric Query Enhancement is used to integrate scene geometry, and a dot product with depth embedding is used to predict depth maps. Additionally, a backup query is introduced to account for low-confidence filtering. The paper also proposes a novel approach to leverage cross-modality knowledge through Bi-directional Guidance Learning, optimizing relative depth feature distances and synchronizing semantic feature continuity with depth annotations. The proposed method is evaluated extensively and achieves state-of-the-art performance on depth-aware panoptic segmentation and individual sub-tasks.