Weakly supervised learning, which uses minimal supervision or coarse annotations for training, has gained significant attention in various applications. One specific area of weakly supervised learning is weakly supervised object localization (WSOL), where the goal is to locate objects using only image-level annotations, eliminating the need for costly bounding box annotations. Most existing approaches for WSOL rely on Class Activation Maps (CAM) to identify discriminative image regions for localizing potential target objects. However, these methods often underestimate object regions and produce smaller bounding boxes than the actual object range. To address this limitation, different techniques have been proposed to enhance CAM. Despite their promising success, existing approaches still have limited performance in completely localizing objects. This is due to the inability of convolutional neural networks (CNNs) to properly explore global feature relations. Recently, visual transformers have shown significant breakthroughs in computer vision, demonstrating that pure transformers can be as effective as CNNs for feature extraction. However, current transformer-based methods for WSOL still suffer from category-agnostic attention maps that introduce noise to the localization maps. To overcome this, we propose a framework called Category-aware Allocation Transformer (CATR) that incorporates category information to exploit category-aware transformer attention. CATR focuses on generating category-aware attention maps to effectively learn discriminative representation of specific object classes. Two category-aware maps are combined to generate localization maps, reducing the impact of background clutter. We introduce a Category-aware Stimulation Module (CSM) to associate self-attention maps with specific categories. We also design an Object Constraint Module (OCM) to refine object regions for category-aware attention maps in a self-supervised manner. Furthermore, we use an automatic weighted loss mechanism to adjust loss weights during training. We evaluate the effectiveness of CATR on challenging WSOL benchmarks and achieve state-of-the-art performance on the CUB-200-2011 and ILSVRC datasets, with 79.62% and 56.90% Top-1 localization accuracy, respectively. The contributions of this work are the proposal of CATR for weakly supervised object localization, the introduction of CSM to enhance category awareness in self-attention maps, the development of OCM to refine object regions, and the evaluation of CATR's performance on benchmark datasets.