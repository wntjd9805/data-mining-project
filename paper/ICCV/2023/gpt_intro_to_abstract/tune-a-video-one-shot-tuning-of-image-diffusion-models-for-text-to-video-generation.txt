The introduction of this computer science paper discusses the recent breakthrough in Text-to-Image (T2I) generation using a large-scale multimodal dataset. To expand this success to Text-to-Video (T2V) generation, current works have extended spatial-only T2I models to the spatio-temporal domain. However, this approach requires extensive training on large hardware accelerators, which is costly and time-consuming. To address this issue, the paper proposes a new T2V generation setting called One-Shot Video Tuning, where only a single text-video pair is used to train a T2V generator. The generator is expected to capture motion information and generate novel videos with edited prompts. The paper makes observations on state-of-the-art T2I diffusion models, highlighting the importance of preserving continuous motion and consistent objects for successful video generation.Based on these observations, the paper introduces a method called Tune-A-Video, which inflates state-of-the-art T2I models over the spatio-temporal dimension. However, using full attention in space-time leads to a quadratic growth in computation, making it infeasible for generating videos with increasing frames. To address this, the paper proposes a sparse spatio-temporal attention mechanism that only visits the first and the former video frame, along with an efficient tuning strategy that updates only the projection matrices in attention blocks.Furthermore, the paper incorporates structure guidance from input videos through DDIM inversion, which improves the temporal consistency of the generated videos. The method is compatible with personalized and conditional pretrained T2I models, providing a personalized and controllable user interface.The paper presents remarkable results of Tune-A-Video in various applications for text-driven video generation, demonstrating its superiority over state-of-the-art baselines. The key contributions of the paper include introducing the One-Shot Video Tuning setting, presenting the Tune-A-Video framework, proposing efficient attention tuning and structural inversion techniques, and showcasing the outstanding performance of the method through extensive experiments.