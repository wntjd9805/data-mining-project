With the increasing popularity of portable filming devices and video media platforms, the ability to search for videos based on human instructions, typically in the form of text, has become an essential part of daily life. This calls for the development of effective and robust text-video retrieval (TVR) techniques. TVR aims to find the most relevant video or text in a database, which is often filled with complex and ambiguous semantic combinations.Recent advances in large-scale image and text pre-training have greatly benefited TVR. Many studies have used separate encoder architectures to project texts and videos into a pre-trained joint embedding space for cross-modal interaction. However, simply pooling all frames of a video as a whole can lead to distractions when matching specific text entities. To address this, multi-grained TVR paradigms have been introduced, which build multi-level cross-modal interactions at different levels ranging from sentence-frame to phrase-clip. However, these methods still fail to handle the inherent uncertainties in determining the optimal entity combinations during text-video matching.This paper proposes a novel TVR framework called Uncertainty-Adaptive Text-Video Retrieval (UATVR) to address the uncertainty problem in cross-modal matching. UATVR models each text-video lookup as a distribution matching procedure, combining both deterministic and probabilistic views. The framework consists of a dynamic semantic adaptation (DSA) module and a distribution-based uncertainty adaptation (DUA) module. The DSA module enhances token-wise matching by introducing additional learnable multi-class tokens, allowing for flexible high-level reasoning. The DUA module represents samples from each modality as distributions and aligns them through probabilistic distribution alignment, simulating one-to-many text-video mappings.The contributions of this work include the innovative modeling of video and text representations as probabilistic distributions for uncertainty-adaptive cross-modal matching. The addition of learnable tokens enables flexible high-level reasoning, and the use of multi-instance contrastive loss facilitates probabilistic embedding alignment. Experimental results on public TVR benchmarks demonstrate the superiority of the proposed UATVR framework.