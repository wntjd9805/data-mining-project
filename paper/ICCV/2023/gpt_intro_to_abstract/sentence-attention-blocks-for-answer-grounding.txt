Visual Question Answering (VQA) systems aim to accurately answer natural language questions about an input image. These systems need to comprehend the contents of images similar to humans and effectively communicate about them in natural language. Answer grounding is a task within VQA that involves detecting the pixels that provide evidence for an answer to a given question. By understanding the reasoning mechanism of VQA models, we can evaluate answer quality, improve performance, and provide explanations. Answer grounding has several applications, including evaluating model reasoning, obfuscating background regions for privacy, and magnifying relevant visual evidence. VQA and answer grounding are multimodal tasks, requiring methods that can process and correlate different modalities. Multimodal joint-embedding models learn representations of multiple modalities in a joint feature space, while multimodal attention-based models prioritize salient regions for additional processing. Attention mechanisms in deep learning mimic cognitive attention and enhance essential features of input data while reducing the importance of others. Squeeze-and-Excitation is a widely-used self-attention mechanism that dynamically focuses on more important channels within feature maps. This paper presents a novel attention module based on the Squeeze-and-Excitation method for the answer grounding task. The method achieves new state-of-the-art results and is compared to top-performing networks. Ablation studies are conducted to further investigate the network.