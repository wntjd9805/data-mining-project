Category-level pose estimation has gained significant attention in computer science, with applications in robotics, augmented reality, and scene understanding. Unlike instance-level pose estimation, which requires a 3D CAD model for each object, category-level pose estimation aims to exploit category-specific information to generalize to unseen objects within a given category. Previous methods for category-level pose estimation can be categorized into prior-free and prior-based methods. Prior-free methods struggle to generalize to novel objects and have poor performance, while prior-based methods leverage category-specific 3D priors to guide pose estimation, but they require a large amount of ground-truth 3D models, limiting their practical applicability. In this paper, we propose a prior-free model called Implicit Space Transformation Network (IST-Net) that implicitly establishes feature correspondence between camera-space and world-space without the need for 3D priors or ground-truth models. IST-Net achieves state-of-the-art performance on benchmark datasets and outperforms prior-based methods in terms of efficiency and accuracy. Our contributions include investigating the effectiveness of prior-based methods, proposing IST-Net, introducing space enhancers to enhance representation capability, and conducting experiments to demonstrate the effectiveness of our approach.