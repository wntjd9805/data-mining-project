Visual grounding is the process of determining the precise location of an object in an image or 3D scene based on given query texts. Recent advancements in the field of 2D visual grounding have been made, along with increasing attention on 3D visual grounding. However, 3D visual grounding presents unique challenges due to the irregular distribution of point clouds and view discrepancy between the intelligent agent and the commander. Existing methods for addressing these challenges have limitations, such as neglecting the lack of view cues within the text input and the use of specifically designed modules to capture view knowledge. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding that leverages large-scale language models (LLMs) to expand grounding texts with view-related descriptions and adopts a fusion transformer for 3D-text feature interactions. We also introduce learnable multi-view prototypes to capture inter-view knowledge and provide high-level guidance for multi-view visual grounding. Experimental results on benchmark datasets demonstrate the effectiveness of our approach, with ViewRefer consistently outperforming other methods in terms of grounding performance. The contributions of this paper include the development of ViewRefer and its components, and the improvement in 3D grounding performance.