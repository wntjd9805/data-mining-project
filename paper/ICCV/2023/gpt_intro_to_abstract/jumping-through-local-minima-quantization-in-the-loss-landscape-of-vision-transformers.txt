Quantization is a widely used technique in neural network inference to reduce data precision and improve efficiency. Deploying models in cloud and edge settings often requires network quantization to reduce computational burden while maintaining accuracy. Post-training quantization (PTQ) refers to the scenario where a full-precision model cannot be retrained due to cost constraints. In this paper, we propose Evol-Q, a PTQ method specifically designed for vision transformers (ViTs). Our approach leverages four key observations: (1) small adjustments in quantization scale can significantly improve accuracy, (2) the loss landscape of quantized ViTs is highly non-smooth, making stochastic gradient descent ineffective, (3) contrastive loss functions can smooth the loss landscape and combat overfitting, and (4) the Evol-Q framework can also be applied to CNN quantization. Instead of using gradient descent or noisy Hessian estimation, we propose a series of cheap evolutionary search procedures to optimize quantization scales. Our approach injects small perturbations in the quantization scale and uses a global infoNCE loss to evaluate them. We demonstrate that prior work fails to properly address the non-smooth ViT loss landscape and show the effectiveness of our proposed approach in improving quantization accuracy for low bit-width ViTs.