to the paper, where the authors discuss the recent advancements in large-scale models in the vision and language domains. They highlight the need for efficient adaptations of existing models to tackle multimodal tasks and propose augmenting pretrained language models with perceptual encoders. The authors advocate for freezing most of the parameters of unimodal models and only training the adaptation parameters. They present a minimal and challenging setup for adapting pretrained unimodal models for image/video/audio-language tasks, focusing on unimodal-only models and leveraging the architecture of large language models as the backbone. They investigate different design choices and find that training a single linear layer directly on downstream multimodal datasets outperforms other approaches, achieving better generalization, data efficiency, and few-shot results. The authors also demonstrate that scaling both vision and language models further improves performance, even with a small number of trainable parameters. They conclude by highlighting the limitations of existing approaches and emphasizing the importance of efficient adaptations for multimodal tasks.