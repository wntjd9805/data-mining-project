In this paper, the authors address the problem of 3D object detection in autonomous driving perception systems. They focus on image-based 3D object detection, which is more cost-effective than LiDAR-dependent solutions. Previous works have primarily focused on monocular detection and combining results from multiple cameras, but this approach does not make use of correspondence in overlapping areas. Alternatively, some studies have used the Lift-Splat-Shoot paradigm to transform multi-camera images to a unified bird-eye-view representation, but this transformation can introduce errors. The authors also explore transformer-based schemes, specifically the DETR-like scheme, which iteratively interacts with multi-view 2D features for 3D object detection. They introduce the concept of 3D positional encoding (PE) to improve the interaction between 3D queries and 2D features. They propose a new 3D PE paradigm called 3D point positional encoding (3DPPE), which involves depth prior and provides more precise localization and representation. They conduct experiments on NuScene benchmarks to demonstrate the advantages of their proposed 3DPPE. Overall, their findings suggest that 3DPPE improves the accuracy of camera-ray-based encoding for 3D object detection in autonomous driving perception systems.