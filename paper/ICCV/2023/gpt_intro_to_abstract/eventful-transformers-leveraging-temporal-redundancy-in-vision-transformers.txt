This paper introduces Eventful Transformers, a new class of Transformers that leverage temporal redundancy between inputs to enable efficient and adaptive inference in vision tasks. Vision Transformers have achieved impressive accuracy in various visual recognition problems, but their high computational cost limits their deployment on resource-constrained devices. The proposed Eventful Transformers exploit the temporal redundancy in video sequences to reduce the computational cost. By reusing intermediate computations from earlier time steps, the method improves efficiency without sacrificing accuracy. The authors also emphasize the need for adaptive models whose computational cost can be modified at runtime, and they design Eventful Transformers to allow real-time control over the compute cost. Although previous works have explored temporal redundancy and adaptivity in CNNs, they are not compatible with vision Transformers due to architectural differences. Eventful Transformers address this challenge and present a unique opportunity, utilizing the structure of Transformer operations to translate sparsity into reduced runtime using standard operators. The paper describes the methodology of Eventful Transformers, including the gating modules that control the number of updated tokens at runtime. The method can be applied to existing models without retraining and is compatible with various video processing tasks. The experiments show that Eventful Transformers significantly reduce computational costs while maintaining high accuracy. The authors provide publicly available code in PyTorch for building Eventful Transformers. However, some limitations include the sub-optimal implementation and minor memory overheads, which can be improved with further engineering efforts.