The emergence of deep learning has led to significant advancements in artificial intelligence, specifically in the field of computer vision. Deep Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) models have achieved state-of-the-art performance in tasks such as image classification, object detection, and semantic segmentation. However, these models often lack interpretability, making it challenging to understand their reasoning behind predictions. Explainable AI (XAI) has thus become a prominent research area in computer vision, with various methods proposed for explaining and interpreting neural network architectures.Explanation methods aim to produce heatmaps or saliency maps that attribute predictions to specific regions in the input image. Early gradient-based methods relied on the gradient of the prediction with respect to the input image. Later techniques, such as Grad-CAM and Integrated Gradients (IG), leveraged internal activation maps or computed explanations based on accumulated gradients between input and reference images. While these methods have been formulated and evaluated for CNNs, the advent of Transformer-based architectures also led to approaches for interpreting Vision Transformer models.This paper introduces Iterated Integrated Attributions (IIA), a universal technique for explaining vision models applicable to both CNN and ViT architectures. IIA utilizes iterative integration across the input image, internal representations, and gradients generated by the model. By leveraging information from activation or attention maps at all network layers, including those from the input image, IIA aims to generate comprehensive and faithful explanations. Extensive objective and subjective evaluations demonstrate the effectiveness of IIA in generating accurate explanations for both CNN and ViT models. The results show that IIA outperforms current state-of-the-art methods across various explanation and segmentation tests, datasets, model architectures, and evaluation metrics.