Visual Reinforcement Learning (RL) has seen impressive success in various fields such as robotic control, autonomous driving, and game-playing. To improve the generalization performance of visual RL algorithms, data augmentation is a widely adopted technique. However, previous methods often utilize a single data augmentation technique, resulting in a limited ability to generalize to environments with varying observations. In this paper, we propose a general policy gradient optimization framework called Conflict-aware Gradient Agreement Augmentation (CG2A) to integrate augmentation combination into RL algorithms and improve their generalization performance. CG2A includes two key components: a Gradient Agreement Solver (GAS) to customize the weights of loss terms and a Soft Gradient Surgery (SGS) strategy to handle conflicting gradients. We conduct extensive experiments to validate the effectiveness of CG2A and show that it outperforms previous state-of-the-art methods in terms of generalization performance and sample efficiency.