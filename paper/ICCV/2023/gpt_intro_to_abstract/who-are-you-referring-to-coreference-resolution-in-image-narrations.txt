In this paper, we address the problem of coreference resolution (CR) in image narratives, which involves determining the referring expressions in a narrative text that refer to the same entity. While text-only CR has been extensively studied, the visual version of CR in image narratives poses additional challenges that require an understanding of both language and visual scenes. Existing text-only CR methods are not effective in resolving coreferences in image narratives due to the domain shift between text datasets and image narratives. Furthermore, standard image-text datasets lack the necessary annotations for training text-only CR models. Prior work on visual CR has focused on specific tasks and limited sets of object categories and referring expression types.To address these challenges, we introduce a new dataset called Coreferenced Image Narratives (CIN) that augments the existing Localized Narratives dataset with coreference chain annotations. We propose a weakly supervised CR method that learns to predict coreference chains from paired image-text data. Our method leverages a multimodal pipeline that incorporates modality-specific encoders to represent image regions, text mentions, and mouse traces. By exploiting the cross-modal correlations between these modalities, we are able to resolve coreference in image narratives. Additionally, we incorporate linguistic rules into our learning formulation inspired by rule-based CR methods.We conduct extensive experiments on the CIN dataset and demonstrate that our method achieves significant improvements in both CR and weakly supervised narrative grounding, which is a form of disambiguation in visual grounding. Our contributions include the introduction of the task of resolving coreferences in multimodal long-form textual descriptions, the creation of the CIN dataset, the development of a novel method for joint coreference resolution and grounding, and a comprehensive experimental evaluation that shows significant improvements over prior work.