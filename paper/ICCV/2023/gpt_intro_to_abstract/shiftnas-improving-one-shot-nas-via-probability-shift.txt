Deep neural networks (DNNs) have achieved remarkable success in computer vision applications. However, deploying these networks on edge devices is limited by the large model sizes and computational overhead. Neural architecture search (NAS) methods have improved the performance of practical applications by automatically searching for optimal architectures, but they often require significant computation budget. One-shot NAS methods have attempted to address this by training a supernet in the first stage and searching for subnets in the second stage. However, the performance of these subnets inherited from the supernet can be inferior, leading to the need for post-processing methods that increase training consumption. To address this issue, some one-shot NAS methods have utilized weight entanglement training to share weights and eliminate the need for additional training. However, previous methods assume that all candidate architectures are equally important, leading to sub-optimal performance in certain regions. In this paper, we propose ShiftNAS, a novel method for training a high-performance supernet. ShiftNAS dynamically adjusts the sampling probabilities of subnets based on their performance variation, allowing for more effective allocation of resources. We also propose an LSTM-based architecture generator to accurately sample subnets with desired computational constraints. The generator is optimized differentiably and can generate a corresponding subnet immediately during evaluation. ShiftNAS achieves state-of-the-art or competitive results on both CNN and ViT models, making it a model-agnostic search method.