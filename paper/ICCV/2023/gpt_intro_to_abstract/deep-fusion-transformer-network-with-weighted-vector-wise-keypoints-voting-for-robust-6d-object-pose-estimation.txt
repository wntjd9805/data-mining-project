The paper focuses on the challenging task of accurate 6D object pose estimation, which is essential for various real-world applications in computer vision and robotics. The authors highlight that despite significant progress in recent years, there are still critical challenges to overcome, such as varying illuminations, sensor noise, occlusion, and reflective surfaces of objects. The integration of RGB-D data from depth sensors has gained attention due to its complementary nature with color information. However, existing methods do not fully utilize the 3D geometry information and are sensitive to severe occlusions, reflective surfaces, and data loss. In this work, the authors propose a Deep Fusion Transformer (DFTr) network that effectively integrates RGB and depth data for 6D pose estimation. The DFTr block aggregates features from both modalities by considering global semantic similarity, mitigating the limitations of previous fusion strategies. Moreover, the authors propose a non-iterative weighted vector-wise voting algorithm for 3D keypoint localization, achieving faster inference speed and superior accuracy compared to existing methods. The paper presents extensive experiments on benchmark datasets, demonstrating significant performance improvements over state-of-the-art methods without requiring post-refinement procedures. Overall, the contributions of this work lie in the effective fusion of RGB-D data using the DFTr network and the improved 3D keypoint localization algorithm, resulting in more accurate and efficient 6D object pose estimation.