This paper addresses the challenge of perceiving and modeling fast motion in videos when high-speed cameras are not accessible. The authors propose the use of video frame interpolation (VFI) as a computational alternative to up-convert low-framerate videos. Existing VFI methods have been successful with sharp frames as input, but are less applicable when image degradations occur due to fast motion and shutter modes. The authors argue that these degradations encode rich information relating to motion and can benefit VFI. While previous works have demonstrated this insight in VFI, they have only been evaluated on synthetic data, and their performance in the real-world remains unknown. To address this gap, the authors present the first real-world dataset for learning and benchmarking degraded VFI, called RD-VFI. They propose a quad-axis imaging system to capture high-speed sharp videos and low-speed degraded videos with three types of degradations: GS blur, RS distortion, and a mixed effect caused by a rolling shutter with global reset (RSGR). The authors also introduce a unified model called Progressive Mutual Boosting Network (PMBNet) for interpolating middle clear frames at arbitrary time instances for all three exposure modes. PMBNet decouples the VFI task into correction and interpolation parts and reconnects them using latent variables and a flow-guided feature alignment module. The authors make systematic comparisons between various degradations for VFI to reveal their correlations and performance gaps. Their contributions include the creation of the RD-VFI dataset, the development of a quad-axis imaging system, and the introduction of the PMBNet model for handling different degradations.