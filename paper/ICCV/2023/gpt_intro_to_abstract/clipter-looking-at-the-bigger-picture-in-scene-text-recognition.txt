Recognizing text in real-world settings often requires leveraging contextual information from the scene. However, current learning-based methods often neglect valuable scene information and operate solely on cropped text images, which is suboptimal. To address this limitation, we explore the use of vision-language models, pretrained on image-caption pairs, to incorporate scene context into text recognizers. We propose CLIPTER, a framework that extracts rich visual representations of the entire image using a vision-language image encoder and merges them with the word-level features of the cropped text using a cross-attention-based operation. We also incorporate a gating mechanism for stability in training. CLIPTER can enhance any pretrained recognizer with scene context awareness and is designed as a versatile framework that supports various text recognition architectures. Through extensive experimentation on diverse datasets, CLIPTER consistently improves upon leading text recognition methods, achieving state-of-the-art results and enhancing robustness and generalization capabilities. We also demonstrate its computational efficiency and flexibility in integration with existing architectures. Our contributions include the introduction of CLIPTER, the design of a computationally efficient framework, and the demonstration of performance improvements and enhanced robustness in text recognition.