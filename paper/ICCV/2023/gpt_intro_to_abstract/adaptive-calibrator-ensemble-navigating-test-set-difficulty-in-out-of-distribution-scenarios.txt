This paper focuses on post-hoc calibration methods in neural networks, which aim to rescale confidence scores to improve calibration. While these methods work well on in-distribution test sets, they often fail on out-of-distribution (OOD) test sets due to differences in the test environment. Previous approaches to OOD calibration either rely on non-adaptive techniques or require re-training for each new test set. The contributions of this paper are twofold. Firstly, the authors provide a new perspective on calibration failure on OOD datasets, showing that the calibration objective depends on dataset difficulty. When the calibration set and test set have the same distribution, calibration is effective. However, OOD datasets have higher difficulty and require different calibration functions. Secondly, the authors propose a method called adaptive calibrator ensemble (ACE) to achieve robust calibration under distribution shifts. ACE combines two calibrators - one trained on an easy in-distribution dataset and the other trained on a high-difficulty OOD dataset. The method computes a test adaptive weight to balance the calibrators based on the deviation of the new test set from the high-difficulty calibration set. The authors show that ACE improves three existing post-hoc calibration algorithms on OOD benchmarks without compromising calibration performance for in-distribution data.