Federated learning (FL) is a popular approach for learning from distributed data while protecting data privacy. However, recent research has shown that FL systems are vulnerable to gradient inversion attacks that can leak clients' private training data. Existing defensive strategies have proven to be insufficient in providing privacy guarantees or result in significant computational overhead. In this paper, we introduce a novel defense called learning-rate perturbation (LRP), which perturbs each client's learning rates to make them appear uniformly random and effectively mitigates data leakage through gradient inversion attacks. We also propose an adaptive defense called ada-LRP that personalizes the learning rate expectations for improved model accuracy. Experimental results on various datasets demonstrate that our defenses significantly enhance the privacy preservation of FL systems against gradient inversion attacks without incurring significant accuracy loss. Our contributions include findings about the impact of randomizing and personalizing learning rates on FL and the development of effective defenses against gradient inversion attacks.