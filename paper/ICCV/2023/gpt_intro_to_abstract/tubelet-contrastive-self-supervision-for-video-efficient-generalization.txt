This paper focuses on learning self-supervised video representations for distinguishing actions. The goal is to reduce the need for manual annotations in downstream deployment settings by leveraging contrastive learning. Previous approaches have been limited in their ability to capture local motion dynamics and generalize to different types of motions. To address these limitations, the authors propose a contrastive method that learns local motion dynamics through simulated tubelets. By simulating a variety of motion patterns not present in the original videos, the proposed method achieves data efficiency and improved generalization. Experimental results demonstrate the effectiveness and generalizability of the learned representations in various action recognition datasets. The contributions of this work include the design of a tubelet-contrastive framework, different methods for simulating tubelet motion, and the demonstration of remarkable data efficiency with competitive performance.