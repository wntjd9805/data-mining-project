Connecting vision and natural language is an essential aspect of machine intelligence, with various applications in human-machine interfaces and accessibility. Image captioning, which involves describing visual input with a natural language sentence, poses unique challenges. This paper explores image captioning architectures that integrate image encoding and language generation approaches. While many approaches use the Transformer architecture, attention layers in these architectures can only directly infer information from the current input sample. To address this limitation, the authors propose a memory network that recalls and utilizes past activations from training iterations. The memory network stores past knowledge processed by the network and integrates it into attention layers. The authors compute "prototypes" from past activations, modeling the manifold of past activations and clustering them into a memory. The proposed approach is evaluated on the COCO dataset for image captioning, demonstrating its effectiveness compared to existing approaches. Additionally, experiments on the nocaps dataset for novel object captioning and the robust COCO split for object hallucination further highlight the proposed approach's benefits. The authors believe that utilizing the space of training items as an additional input to the network has broader applications beyond image captioning. Overall, this paper introduces a prototype memory network for image captioning, showcasing its effectiveness in integrating past activations as an additional input for attention layers.