In various application scenarios, such as action recognition and sign language translation, 3D hand pose estimation plays a crucial role. Recent years have seen significant progress in hand pose estimation, largely driven by the availability of accurate 3D annotated datasets. However, acquiring labeled datasets is time-consuming and laborious, posing a challenge for deep learning models to learn from limited and noisy data.Self-supervised learning has emerged as a potential solution to tackle the challenge of manual annotation. While self-supervised pose estimation with RGB hand images is relatively unexplored, a pioneering method called S2HAND has been proposed. However, S2HAND heavily relies on the quality of pseudo labels, and evaluating the quality of these labels is challenging. To address these issues, we propose leveraging multi-view information to enhance self-supervised learning, as the complementary nature of multi-view observations can help mitigate pose estimation ambiguity.Previous work in human body pose estimation has explored the use of multi-view information. However, naive enforcement of multi-view consistency may lead to degenerate solutions. Other studies have utilized multi-view data with special designs to enhance supervision. In this paper, we build upon these efforts to explore the potential of multi-view collaborative learning for hand pose estimation.Our approach introduces a learnable network, HaMuCo, that utilizes multi-view information to address noisy pseudo labels and unreliable multi-view "groupthink" issues leading to training collapse. HaMuCo consists of a single-view hand pose estimator and a cross-view interaction network for supervision. The single-view estimator uses the MANO hand model as the decoder and is supervised by noisy pseudo labels. The cross-view interaction network captures cross-view features and uses consistent losses among different views to guide collaborative learning.We evaluate our approach on the HanCo dataset and demonstrate superior performance compared to previous methods for self-supervised 3D hand pose estimation. Our framework is versatile and can be trained with or without calibration. It can also incorporate the cross-view interaction network for superior multi-view inference results. Furthermore, our model generalizes well to other datasets and in-the-wild images.In summary, our contributions include the proposal of the first self-supervised learning framework for single-view hand pose estimation, achieving state-of-the-art performance through multi-view collaborative learning. We introduce a cross-view interaction network to enforce multi-view consistency and capture cross-view features for collaborative learning. Our framework is capable of multi-view inference and achieves state-of-the-art performance without additional complexity.