Recently, there has been a growing need for efficient image compression techniques due to the increasing use of multimedia applications. Neural network-based methods have shown promising results in computer vision tasks and have prompted research on NN-based image compression methods. However, most existing NN-based image compression methods require separate compression for different versions of an image, leading to low coding efficiency. To address this issue, some recent studies have focused on NN-based scalable image compression, where multiple versions of an image are encoded into a single bitstream in a hierarchical manner. This paper specifically focuses on spatially scalable coding, which has received less attention compared to quality scalable coding. The existing NN-based methods for spatially scalable coding have limitations in terms of fixed scale factors and lack of support for arbitrary scale combinations. To overcome these limitations, we propose a novel NN-based image compression network called COMPASS (COMPression network with Arbitrary-scale Spatial Scalability). COMPASS uses an inter-layer arbitrary scale prediction method called Local Implicit Filter Function (LIFF) to reduce redundancy between layers and support arbitrary scale factors. It also introduces a combined rate-distortion loss function for effective optimization. Our experimental results show that COMPASS outperforms existing spatially scalable coding methods and achieves comparable or even better coding efficiency than single-layer coding for various scale factors. To the best of our knowledge, COMPASS is the first NN-based spatially scalable image compression method that supports arbitrary scale factors with high coding efficiency.