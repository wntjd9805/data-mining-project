Deep learning has achieved great success in computer vision applications, particularly in image classification. However, most of these successes are based on the assumption of independent and identically distributed (IID) data. In real-world scenarios, there are often out-of-distribution (OOD) shifts between training and testing data, leading to poor performance of deep models trained with the IID assumption. To address this issue, various OOD methods have been proposed, but they often sacrifice IID accuracy in favor of OOD performance. In this paper, we propose a Model Agnostic adaPters (MAP) method to simultaneously learn inductive biases of both IID and OOD. We formulate the learning as a bilevel optimization problem, where the inner level optimizes the OOD model with Adapter Aggregation Layers (AALs) using an OOD loss, and the outer level utilizes the IID criterion on a validation set to guide the training of AALs. Through experiments on multiple datasets, model architectures, and baselines, we show that MAP achieves balanced performance on IID and OOD tasks, can be applied to any OOD method, and performs reliably under different settings. Our contributions include investigating the IID-OOD dilemma, proposing the MAP method, and demonstrating its effectiveness through extensive experiments.