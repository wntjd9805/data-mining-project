In this paper, we address the challenge of object detection in complex scenes by proposing a self-supervised object detection model called DEVI, trained on egocentric videos. Traditional object detection methods rely on densely annotated data, which is expensive and time-consuming to produce. We aim to minimize annotation costs and maximize scene complexity by learning a class-agnostic object detector without using any annotations. We focus on egocentric settings, as they present unique challenges compared to popular datasets. Egocentric videos capture unscripted, "in-the-wild" scenes with dense environments and diverse objects. The significant domain differences between egocentric and non-egocentric datasets make transfer learning difficult. To address these challenges, we propose loss functions inspired by computational appearance methods and tailored to egocentric perception. We also introduce an object residual module that learns category-specific features and precise representation of ambiguous patches. Our contributions include the DEVI model, the multi-view and scale-regression losses, and the object residual module. We qualitatively demonstrate the effectiveness of our method in generating category-specific features.