This paper introduces a novel paradigm called Unsupervised 3D Perception with 2D Vision-Language distillation (UP-VL) for open-vocabulary 3D perception in autonomous driving. The existing 3D detection models in autonomous driving assume all categories of interest to be known and annotated during training. However, these models struggle to handle unknown categories in real-world scenarios, which poses safety concerns. The paper addresses this challenge by incorporating a pre-trained vision-language model to generate high-quality auto labels for objects in arbitrary motion states. The authors propose co-training a 3D object detector with a knowledge distillation task to improve detection quality and transfer semantic features from 2D image pixels to 3D LiDAR points. This approach enables the detection of all traffic participants and allows flexible querying of the detector's output embedding with text prompts. The proposed UP-VL achieves state-of-the-art performance on unsupervised 3D perception for autonomous driving and introduces semantic-aware unsupervised detection for objects in any motion state. It also enables 3D open-vocabulary detection of novel objects in the wild without the need for data collection or model retraining.