Quantization is a process that reduces the range and precision of numerical data representation. In the context of neural networks, integer quantization can reduce compute and memory requirements while sacrificing some model accuracy. Inference in neural networks primarily relies on compute-intensive operators like convolutions and matrix multiplications, which accumulate products into 32-bit registers called accumulators. Reducing the number of bits in these accumulators can improve inference throughput and energy efficiency. However, reducing the accumulator's precision increases the risk of overflow and introduces numerical errors that degrade model accuracy. Previous works have attempted to address this issue but struggle to maintain accuracy under frequent overflow or support applications that require guaranteed arithmetic correctness. To avoid overflow altogether, this paper proposes an approach called accumulator-aware quantization (A2Q) that trains quantized neural networks to use low-precision accumulators during inference without the risk of overflow. The paper also demonstrates that A2Q improves weight sparsity and presents an end-to-end flow for training QNNs and generating custom streaming architectures on FPGA-based inference accelerators. This work is the first to explore the use of low-precision accumulators for FPGA-based QNN inference accelerators and integrates A2Q into existing open-source tools.