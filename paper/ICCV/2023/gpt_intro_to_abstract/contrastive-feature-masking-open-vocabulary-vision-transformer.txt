Object detection is a crucial task in computer vision and machine learning, with applications ranging from autonomous agents to search engines. However, most modern object detectors rely on manually annotated regions and class labels, limiting their scalability. To overcome this limitation, a new task called open-vocabulary detection (OVD) has been introduced, which uses image-text pairs for training and text queries from users at test time.In this paper, we propose a framework called CFM-ViT (Contrastive Feature Masking Vision Transformer) for pretraining vision transformers to capture more detailed pixel and region information for open-vocabulary object detection. Inspired by masked auto-encoding, our approach performs prediction in the joint image-text embedding space as an auxiliary objective to contrastive image-text learning. This enhances object representation without compromising image-level tasks.We also introduce Positional Embedding Dropout (PED) to address overfitting and enable the use of a frozen ViT encoder. PED randomly drops out positional embeddings during pretraining, leading to more robust representations that better generalize to high-resolution detection data. This frozen ViT encoder acts as an open-vocabulary region-classifier, preventing the forgetting of open-vocabulary knowledge during detection.We evaluate CFM-ViT on the LVIS and COCO open-vocabulary detection benchmarks and achieve state-of-the-art results. On LVIS, our model outperforms the previous best approach by 7.6 APr at the system level. On COCO, CFM-ViT achieves competitive performance without using pseudo labels or weak supervision. Additionally, we outperform existing methods on 8 out of 12 image-text retrieval benchmark metrics.Our findings highlight the potential of image-text pretraining for open-vocabulary detection and encourage further exploration in this area.