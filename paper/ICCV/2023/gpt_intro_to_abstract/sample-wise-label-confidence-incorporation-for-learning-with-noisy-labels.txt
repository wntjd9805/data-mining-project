Recent advances in deep learning have led to remarkable image classification performance that surpasses human ability. However, deep learning models require a substantial amount of labeled data to maintain their desired performance. Despite efforts to curate these datasets, mislabeling is inevitable. This issue is particularly problematic given the capacity of deep networks to overfit data samples. To tackle this challenge, researchers have focused on developing methods for learning with noisy labels in deep learning frameworks, including noise estimation, sample selection, and robust loss methods. However, robust loss methods can suffer from underfitting issues. This paper presents a novel training strategy that addresses the underfitting problem by incorporating sample-wise label confidence. The proposed approach involves training using a weighted cross-entropy loss, where the weight is determined based on the label confidence. Theoretical evidence is provided to support the effectiveness of the proposed method, subject to certain conditions being satisfied regarding sample-wise label confidence. The method is generally applicable to existing robust loss methods and shows improved convergence to the optimal point of the robust loss function. The proposed method is evaluated on both synthetic and real-world datasets, demonstrating its effectiveness in reducing underfitting and achieving state-of-the-art performance. The contributions of this paper include the reduction of underfitting issues, the general applicability of the method, and theoretical evidence supporting the effectiveness of the proposed approach.