Large-scale contrastive vision-language models (VLMs) pretrained on image-text pairs encode general knowledge about the alignment of visual concepts and textual sequences. These models have shown impressive potential for zero-shot transfer to image classification tasks. However, existing methods for adapting VLMs to new classes have limitations, either only changing the classification weights or affecting the models' generalization ability. In this paper, we propose a visual prompting approach that effectively adapts VLMs to new classes while preserving their generalization ability. We use synthetic images with class name text as visual prompts to modify training images, helping the models perceive class-relevant content. We reformulate the classification objective as visual prompt selection and introduce a min-max contrastive learning objective with hard negative mining. Our experiments demonstrate that our approach significantly outperforms state-of-the-art methods in terms of base-to-new generalization, few-shot learning, and domain generalization. We make the following contributions: proposing the use of synthetic images as visual prompts for VLMs, reformulating the classification objective for visual prompt selection, and demonstrating the effectiveness of our approach on diverse image classification tasks.