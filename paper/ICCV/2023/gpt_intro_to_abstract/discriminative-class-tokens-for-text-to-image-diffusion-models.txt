This paper introduces a method that improves the accuracy and detail of text-to-image diffusion models. These models often struggle with lexical ambiguity and generating fine-grained details. Previous approaches have used pretrained classifiers to guide the denoising process, but they require classifiers that work on real and noisy data. Another approach is to condition the diffusion on class labels using a curated dataset, but this limits the expressive power of the models trained on large image-text collections. Other methods fine-tune the diffusion model using a small collection of images, but this is slow and may change the distribution of generated images. In contrast, the proposed method updates the representation of a single added token corresponding to each class of interest, without tuning the model on labeled images. The token representation is learned by iteratively generating new images with a higher class probability according to the pretrained classifier. The optimization process uses gradient skipping to propagate the gradient only through the final stage of the diffusion process. The optimized token is then used as part of the conditioning text-input to generate images using the original diffusion model.The method has several advantages. It only requires an off-the-shelf classifier, does not require a classifier trained on noisy data, and allows for "plug-and-play" improvements of generated images. It also employs a classifier trained on an extensive collection of images without needing access to those images, which is beneficial for using class-discriminative features and addressing privacy concerns.The method is evaluated in both fine-grained and coarse-grained settings using datasets such as CUB, iNat21, and ImageNet. The primary metric is the accuracy of generated samples compared to baselines using pre-trained classifiers, as well as the accuracy of classification models trained on generated samples. The quality and diversity of generated images are also measured, showing superiority in terms of Fr√©chet inception distance (FID). Qualitative examples demonstrate the effectiveness of the approach in resolving lexical ambiguity and adding discriminative features.