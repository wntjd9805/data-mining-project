The paper introduces a unified framework for supporting multiple downstream tasks with a single foundation model in the field of computer vision. Existing approaches such as multi-task pre-training and prompt-based tuning have limitations in tackling the task-gap problem. To address this, the authors propose the use of a Vision Middleware (ViM) module zoo, consisting of lightweight plug-in modules optimized for specific midstream tasks. The ViM modules are aggregated and adapted for downstream transferring, resulting in balanced performance across multiple tasks. Experimental results demonstrate the effectiveness of ViM in achieving improved performance without the need for individual fine-tuning for each downstream task. The advantages of ViM include efficiency, scalability, and performance. The authors encourage the community to contribute to a public ViM to facilitate collaborative research in this area.