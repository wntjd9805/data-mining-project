3D human pose estimation is a crucial task in computer vision that involves localizing human body keypoints in images. It plays a significant role in various applications such as robotics and augmented/virtual reality. However, accurately predicting human behaviors requires more than just a few body keypoints. 3D whole-body pose estimation aims to detect face, hand, and foot keypoints in addition to the standard body keypoints. The lack of accurate 3D datasets has made this task challenging, leading to the use of separate models for different body parts. However, ensembling these models during inference presents issues due to dataset biases, pose and scale variations, and complex inference pipelines. Distillation from pretrained models has been used to overcome these issues, but the accuracy of parametric body models is usually lower for fine body parts. These models prioritize realistic motion generation and avatar capture rather than fine-grained accuracy. To address these challenges, this paper proposes a large-scale dataset called H3WB, which extends the Human3.6M dataset with 3D whole-body keypoint annotations. The dataset consists of 133 paired 2D and 3D whole-body keypoints and provides benchmarks for 3D whole-body pose estimation tasks. The paper also proposes a method to create detailed 3D human pose keypoints from multi-view images and provides baselines for the H3WB tasks. Furthermore, the paper enhances the TotalCapture dataset with 3D whole-body annotations and demonstrates the performance improvement in pose lifting tasks when combined with the H3WB dataset. The contributions of this paper include the creation of a benchmark dataset for accurate 3D whole-body pose estimation, the proposal of a method for creating 3D human pose keypoints, and the provision of baselines and enhancements for related tasks.