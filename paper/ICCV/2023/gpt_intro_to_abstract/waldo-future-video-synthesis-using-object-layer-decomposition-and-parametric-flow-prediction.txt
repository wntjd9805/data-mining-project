Predicting future frames from a video stream is crucial for improving the robustness and safety of autonomous agents. This paper focuses on synthesizing future frames using a fixed number of past frames. Existing approaches that rely on advanced image synthesis models often have limited resolution due to the extra temporal dimension. Other methods resort to compression to reduce computations, sacrificing temporal consistency. Instead, this work uses semantic and motion cues extracted from the past frames to model complex dynamics and predict future frames in high resolution. Unlike previous approaches, this model automatically discovers the object decomposition without explicit supervision. Additionally, it uses thin-plate splines as a parametric model of per layer flow, providing optical flow at any resolution and improving robustness. The proposed approach decomposes video synthesis into discovering layers, predicting transformations, and warping object regions accordingly. Extensive experiments on urban and nonrigid motion datasets demonstrate that the proposed approach outperforms the state-of-the-art methods. The contributions of this work include broader operating assumptions, allowing for recovery of dense scene flow and multiple predictions, and novel approaches including a layer decomposition algorithm, a low-parameter deformation model, and effective fusion of multiple predictions.