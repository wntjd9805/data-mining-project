Deep learning models rely on large amounts of data for training and analysis. However, certain industries, such as surveillance and security, need to retain data for extended periods, raising concerns about privacy and data security. To comply with privacy protection laws, one solution is to anonymize sensitive information, such as faces or license plates, from images. However, recent research has shown that this anonymization process can negatively impact the performance of deep convolutional neural network (CNN) classifiers. This creates a dual inference pipeline, where the predictions of the anonymizer and the classifier may differ. In this paper, we propose a novel solution to address this problem by developing a method that is independent of the neural network architecture and task. We validate our approach through experiments on the ImageNet dataset and a proprietary toll vehicle classification dataset, demonstrating significant improvements compared to naive anonymization methods. Our contributions include introducing the concept of the dual inference pipeline, providing a new perspective on the problem, and achieving optimal performance with a 0% rate of decision change. The paper concludes with discussions on related work and future research directions.