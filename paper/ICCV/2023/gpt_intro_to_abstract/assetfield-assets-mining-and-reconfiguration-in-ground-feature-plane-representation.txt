The demand for creating virtual environments that mimic real-world scenes is increasing, particularly in industries such as interior design and urban planning. Users often need to visualize different scene configurations before finalizing a plan, requiring a virtual environment that offers flexible editing choices and high rendering quality. While recent advancements in neural rendering have shown promise in producing realistic visuals, they struggle to meet the demands of versatile editing. Traditional neural radiance field (NeRF)-based methods encode an entire scene into a single neural network, making it difficult to manipulate and composite due to its limited network capacity. Other approaches that focus on object-aware scene rendering are scene-specific and not easily applicable in various scenarios. Some recent attempts have combined voxel grids with neural radiance fields to explicitly model the scene, but editing requirements still require specifying affected voxels, which is cumbersome for group editing. This paper presents AssetField, a novel neural representation that offers the editing flexibility of traditional graphical workflows. AssetField factorizes a 3D neural field into a ground feature plane and a vertical feature axis, allowing intuitive manipulation of individual objects and embedding multiple scenes with shared vertical feature axis. The ground feature planes encode scene density, color, and semantics, providing valuable information for object detection and categorization. AssetField enables versatile editing at object-level, category-level, and scene-level, improving group editing efficiency and facilitating scene composition and reconfiguration. Overall, AssetField provides a user-friendly and flexible solution for scene manipulation and generates realistic renderings for new scene configurations.