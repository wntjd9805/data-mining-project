Video-Language Pre-training (VLP) has become the dominant approach for various video-text tasks, such as video-text retrieval, VQA, zero-shot recognition, and video-text grounding. This is due to advancements in vision and language coupled with large-scale data. However, existing video-language datasets are divided into third-person view and first-person view, creating a domain gap that hinders performance on egocentric benchmarks. To address this, a new egocentric dataset called Ego4D has been introduced, unlocking the potential of egocentric VLP. Current egocentric VLP approaches use separate video and language encoders, limiting the development of unified frameworks and lacking strong zero-shot inference ability. Existing solutions involve stacking fusion layers or using shared architectures, but these introduce a large number of parameters and cannot be applied to uni-modal tasks. In this paper, we present EgoVLPv2, an improved version of egocentric VLP that incorporates cross-modal fusion directly into the video and language backbones. Our approach reduces fusion parameters, offers flexibility in encoder selection, and is applicable to both uni-modal and multi-modal tasks. By inserting cross-modal fusion into the backbone, EgoVLPv2 unifies dual- and fusion-encoder-based tasks and enables fast retrieval and grounding tasks. Additionally, it reuses pre-trained cross-attention modules, reducing fine-tuning costs. We demonstrate the effectiveness of EgoVLPv2 on multiple egocentric benchmarks, achieving state-of-the-art performance. These findings signal a significant advancement in egocentric VLP.