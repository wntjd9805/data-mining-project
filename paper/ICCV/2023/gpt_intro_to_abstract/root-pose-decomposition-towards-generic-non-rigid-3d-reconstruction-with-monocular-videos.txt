In this paper, we address the challenge of reconstructing non-rigid 3D objects using monocular RGB videos. This task is crucial for various applications in computer vision and graphics, including XR and robotics. Traditional approaches rely on template-based models or specific category models, which do not scale well to diverse categories. Recent works based on Neural Radiance Field (NeRF) have shown promising results by unifying frames to a canonical space, but they are limited in handling large object deformations and movements. Moreover, they are not suitable for casual videos without root poses provided by background Structure from Motion (SfM). Existing methods that address these issues rely on off-the-shelf pose or surface models of certain categories, which do not generalize well to reconstructing generic object categories. To overcome these limitations, we propose a method called Root Pose Decomposition (RPD) for non-rigid 3D reconstruction based on monocular RGB videos. Unlike previous methods, RPD does not rely on known camera poses, category-specific skeletons, or pre-trained dense pose models. Instead, it achieves articulated reconstruction for objects with rapid deformations, complex motion patterns, and large pose changes. Our method is capable of handling various factors that often exist in real-world scenarios, such as multiple categories, individual diversities, and object occlusions. Concretely, RPD follows a common approach for non-rigid reconstruction by building a canonical space for different frames. We evaluate the performance of RPD on various monocular RGB videos and demonstrate its effectiveness in achieving accurate and articulated 3D reconstructions. Our method shows superior performance compared to existing approaches, especially in challenging scenarios with significant object deformations or movements. Overall, our work contributes to the field of computer vision by providing a novel solution for non-rigid 3D reconstruction that is applicable to generic object categories.