Previous studies in deep learning models have focused on designing sophisticated models to overcome domain shift, where training source and test target data come from distinct domains. However, they still suffer performance degradation when a large domain gap exists in the test stage. To address this issue, we propose a more practical approach called test-time adaptation, which adapts a trained model to unlabeled unseen data during the test. We introduce the problem of inaccurate statistics estimation in Batch Normalization (BN) and the small loss produced by entropy minimization in existing test-time adaptation methods. We then discuss the limitations of incorporating both source data and unlabeled target data in real-world scenarios due to high computational costs and data privacy concerns. Thus, we focus on fully test-time adaptation without access to source data. We review recent methods for fully test-time adaptation and highlight their limitations on diverse cross-domain styles. We identify the issues of accurate statistics estimation and biased training procedure in current methods. To overcome these limitations, we propose a novel approach called DomainAdaptor, which consists of an AdaMixBN module and a Generalized Entropy Minimization (GEM) loss. The AdaMixBN module combines training and test statistics and adapts the mixture coefficient based on the current batch. To address the weight mismatch problem, we transform the source statistics into affine parameters before finetuning. The GEM loss emphasizes the role of temperature scaling in the traditional entropy minimization loss to optimize the parameters of AdaMixBN. Our proposed method offers significant improvement over existing approaches on four benchmark datasets for domain generalization.