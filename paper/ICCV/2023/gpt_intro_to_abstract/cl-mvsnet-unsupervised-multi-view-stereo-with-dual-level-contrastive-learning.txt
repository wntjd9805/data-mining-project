Multi-View Stereo (MVS) is an important task in various applications such as robotics, self-driving, and VR/AR. The goal of MVS is to estimate a dense 3D reconstruction from multiple images captured from different views. Traditional approaches compute dense correspondences between images based on hand-crafted similarity metrics and engineered regularizations. Recently, learning-based methods have been developed to enhance the effectiveness of MVS, but most of them rely on large-scale ground-truth 3D training data. Unsupervised MVS methods have attempted to train networks without annotations, but they may be ineffective due to indistinguishable regions and view-dependent effects. In this paper, we propose a dual-level contrastive learning approach, named CL-MVSNet, to boost the robustness and generalizability of unsupervised MVS. We introduce image-level contrastive learning to encourage the model to be more context-aware and scene-level contrastive learning to alleviate view-dependent photometric effects. We also propose an L0.5 photometric consistency loss to further advance the contrastive learning framework. Experimental results on benchmarks show that our method outperforms state-of-the-art end-to-end unsupervised models and surpasses its supervised counterpart.