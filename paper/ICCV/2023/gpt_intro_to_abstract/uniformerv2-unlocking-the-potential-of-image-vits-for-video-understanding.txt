The rapid advancement of transformer-based language foundation models has led to the emergence of image foundation models, which have been extensively trained on large web datasets with rich supervision. These Vision Transformers (ViTs) have demonstrated exceptional generalization capabilities for various image tasks, prompting researchers to explore their applications for video tasks. However, adapting ViTs for video understanding poses challenges due to the domain gap between images and videos. In this paper, we propose an efficient paradigm for constructing powerful video networks by combining image-pretrained ViTs with an efficient UniFormer design. We introduce new Multi-Head Relation Aggregator (MHRA) modules in local and global UniBlocks to effectively handle local spatiotemporal representation and capture complex dynamics. We evaluate our approach on various video benchmarks, and our results show superior performance compared to previous ViT-based approaches. Moreover, we introduce a compact Kinetics-710 benchmark and achieve higher accuracy on Kinetics-400/600/700 with minimal finetuning. Our model achieves state-of-the-art results on all benchmarks, including a top-1 accuracy of 90.0% on Kinetics-400.