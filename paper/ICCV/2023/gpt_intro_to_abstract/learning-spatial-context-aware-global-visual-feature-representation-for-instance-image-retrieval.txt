Instance-level image retrieval is a task that involves searching and retrieving images containing the same object as a query image from a large-scale dataset. Visual feature representations are vital in measuring the similarities between a query and candidate images. While handcrafted feature-based methods have been effective, deep learning techniques have taken over with their deep feature representations. These deep features can be categorized into global representations that describe the entire visual content of an image and local descriptors that represent spatial information within an image. Global representations are efficient for measuring similarity but may not capture geometric verification accurately. On the other hand, local descriptors are important for geometric verification but can be computationally expensive. To address this, a two-stage paradigm is commonly used, where an initial retrieval based on global representation is followed by re-ranking using local descriptors. However, this approach increases retrieval time and memory expense. In this paper, we propose a novel feature learning framework that incorporates spatial context information into global feature representations. We extract conventional visual information and spatial context information using a spatial context branch in a convolutional neural network (CNN). The spatial context branch consists of an online token learning module and a distance encoding module to capture the types and spatial distribution of local descriptors. The resulting spatial-context-aware local descriptors are then fused with visual information to create a global feature representation. Experimental results on benchmark datasets demonstrate the effectiveness of our approach in improving instance-level image retrieval performance.