This paper explores the use of post-training quantization (PTQ) techniques for vision transformers (ViTs), which have shown promising potential in various vision applications. However, ViTs suffer from heavy computations and high resource requirements, making them unsuitable for deployment on resource-constrained edge devices. Model quantization, which reduces model complexity by decreasing representation precision, is an effective compression approach. Specifically, this paper focuses on PTQ methods that use scale reparameterization to bridge complex quantizers used in quantization with simple hardware-friendly quantizers used in inference. The proposed framework, called RepQ-ViT, applies channel-wise quantization and log quantization to maintain original data distributions for post-LayerNorm and post-Softmax activations, respectively, and then transforms them to simple quantizers via scale reparameterization. Experimental results on various vision tasks demonstrate that RepQ-ViT outperforms existing baselines without requiring hyperparameters or expensive reconstruction procedures.