Pre-training deep neural networks in computer vision using large-scale datasets has become a standard procedure due to their superior convergence and generalization for downstream tasks. However, the cost and computational requirements of pre-training state-of-the-art vision models using enormous amounts of data are significant. ImageNet has been a common dataset for pre-training vision models, and models pre-trained on its classification task have been successfully transferred to various tasks such as object detection, semantic segmentation, and video recognition. With the rise of Vision Transformers (ViT), pre-training them has gained popularity, but it has been observed that large ViTs require even larger datasets to reach their full potential. Self-supervised learning (SSL) has emerged as an alternative to pre-training with labeled data, but it still relies on a vast amount of unlabeled data. To address the issue of data efficiency, efforts have been made to reduce the amount of pre-training data while maintaining accuracy. This includes techniques like distillation, data augmentation, and synthetic datasets. In this paper, we propose a formula-driven supervised learning (FDSL) approach to pre-train ViTs using a single instance per category. We introduce the one-instance fractal database (OFDB), which significantly improves data efficiency when combined with data augmentation during pre-training. Our experiments show that OFDBs achieve comparable performance to million-scale datasets while reducing computational time. We also demonstrate that OFDBs outperform state-of-the-art methods for training ViTs on small datasets. Overall, our approach offers a promising solution for efficient pre-training of large vision models.