The study focuses on understanding human behavior through a combination of perception, mapping, and navigation algorithms. This understanding has applications in augmented reality systems, robot guidance, and assistive devices. The use of deep learning models and egocentric vision has improved the perception of autonomous agents in modeling human-object interaction. Recent advances include anticipating future actions, modeling object manipulation, identifying interaction hotspots, and creating topological maps. The concept of affordances, which are potential actions based on an agent's motor capabilities, is explored. While some approaches consider affordance perception as a classification problem, the authors propose a grounded approach with pixel-wise precision, enabling detailed metric understanding while maintaining flexibility. They develop a pipeline to collect multi-label pixel-wise annotations from real-world interactions in order to build a dataset of grounded affordances. Several segmentation architectures are adapted to the multi-label paradigm, allowing for the extraction of diverse information from the scene. The authors also demonstrate mapping and planning applications of their approach.