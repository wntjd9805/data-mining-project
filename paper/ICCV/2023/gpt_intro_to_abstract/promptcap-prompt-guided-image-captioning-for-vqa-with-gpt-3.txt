This paper introduces PROMPTCAP, a question-aware captioning model designed to improve the understanding of images by black-box language models (LMs) such as GPT-3 and ChatGPT in Knowledge-based Visual Question Answering (VQA) tasks. Traditional VQA tasks require broad knowledge and commonsense reasoning to provide accurate answers. Existing knowledge-based VQA systems retrieve external knowledge from sources like knowledge graphs, Wikipedia, and web search. However, modern language models like GPT-3 have shown to be particularly useful for this task due to their knowledge retrieval and reasoning abilities. One challenge is enabling LMs to comprehend images when they can only be accessed through APIs. The current approach is to transform visual inputs into generic text descriptions that LMs can process, but this risks losing important visual details. To address this, PROMPTCAP is introduced as a better connector between images and black-box LMs. PROMPTCAP takes an additional natural language prompt as input, specifying the question to be answered, and generates an image description that includes relevant visual details. The model is trained using a pipeline that synthesizes and filters training samples with GPT-3. Experimental results show that PROMPTCAP significantly outperforms generic captioning models on various VQA tasks, achieving state-of-the-art performance. Additionally, PROMPTCAP's generalization ability is demonstrated on WebQA, surpassing the performance of generic captions and supervised baselines. The contributions of this paper include the proposal of PROMPTCAP as a question-aware captioning model, the development of a pipeline for synthesizing and filtering training samples using GPT-3, and the demonstration of PROMPTCAP's superior performance in knowledge-based VQA tasks.