Semantic segmentation is a crucial part of computer vision, with applications in autonomous driving, robotics, and surveillance. While daytime scene segmentation has made progress, challenges remain for nighttime scenes due to degraded image quality and lack of high-quality annotations. Existing approaches use unsupervised domain adaptation (UDA) with labeled daytime images and unlabeled nighttime images. However, the low dynamic range of frame-based cameras limits performance. To address this, we propose using event cameras for nighttime segmentation, as they offer a higher dynamic range. However, event-based approaches are typically inferior to image-based methods. Therefore, we propose the Cross-Modality Domain Adaptation (CMDA) framework, which combines image and event modalities for unsupervised nighttime segmentation. We address challenges in connecting image and event modalities and minimizing domain shifts between daytime and nighttime images. Our framework achieves improved segmentation performance by leveraging the event modality. We introduce the Image Motion-Extractor and Image Content-Extractor to bridge the gaps between modalities and domains. We evaluate our framework on existing benchmark datasets and propose a new dataset with fine, pixel-level labels. Our CMDA framework achieves state-of-the-art results on both datasets.