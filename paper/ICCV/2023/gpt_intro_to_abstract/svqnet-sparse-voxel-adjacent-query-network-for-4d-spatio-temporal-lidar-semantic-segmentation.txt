LiDAR-based perception is a critical component in autonomous driving and robotics, and 3D LiDAR semantic segmentation plays a significant role in assigning semantic category labels to LiDAR scenes. While previous studies have focused on semantic segmentation within a single frame, the information in a single frame is affected by occlusion problems and ambiguity between similar point clusters, limiting the performance of single-frame based semantic segmentation. To address this limitation, the use of sequential knowledge and 4D spatio-temporal information has gained attention. Classic temporal methods stack frames in the last few timestamps, but this approach brings redundancy and weakens the benefits of time series. Other methods based on nearest neighbor search or recurrent neural networks can model spatio-temporal relationships but have limitations in handling long time-series information or aligning recurrent features under sparse representation. In this paper, we propose a Sparse Voxel-Adjacent Query Network (SVQNet) that efficiently extracts valuable spatio-temporal features in 3D voxel representation. SVQNet shunts historical information into two groups - Voxel-Adjacent Neighborhood and Historical Context - to enhance spatial semantic features across time and complete occlusions in a learning manner. The proposed SVQNet achieves state-of-the-art performance on SemanticKITTI and nuScenes datasets while maintaining real-time runtime. Our contributions include the formulation of spatio-temporal information, the introduction of an efficient Sparse Voxel-Adjacent Query module, the use of a learnable Context Activator, and a lightweight Temporal Feature Inheritance method.