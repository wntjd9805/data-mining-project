Infrared and visible image fusion techniques aim to generate visually appealing and informative fused images that offer superior performance compared to using a single modality alone. This has become widely utilized in various applications, including autonomous vehicles and unmanned aerial vehicles. Infrared images have the advantage of adapting to different lighting conditions but lack texture details. On the other hand, visible images contain rich texture detail information but struggle to provide clear information in low light conditions. Therefore, the challenge lies in designing fusion methods that preserve both texture details and thermal information.Existing fusion methods can be categorized into traditional approaches, such as image decomposition and sparse representation, and deep learning-based approaches, such as autoencoder and generative adversarial network-based methods. However, most of these methods directly combine the texture details and object contrast of different modalities without considering dynamic changes in reality. This leads to poor fusion results and weaker performance in downstream tasks compared to using a single modality.To address these limitations, we propose a dynamic image fusion framework called MoE-Fusion, which uses a multi-modal gated mixture of local-to-global experts. This framework consists of a Mixture of Local Experts (MoLE) and a Mixture of Global Experts (MoGE), guided by a multi-modal gate. MoLE leverages an attention map generated by an auxiliary network to construct multi-modal local priors and dynamically learns multi-modal local features using multi-modal gating. MoGE performs dynamic learning of multi-modal global features to achieve a balance between texture details and contrasts in the fused images.Through our dynamic fusion paradigm that progresses from local to global, our model ensures reliable fusion of different modality images. Our main contributions include the proposal of a dynamic image fusion model that integrates effective information from respective modalities, an effective and robust framework for sample-adaptive fusion, and validation through extensive experiments on multiple datasets. We demonstrate the superiority of our model quantitatively and qualitatively, as well as its effectiveness in object detection.