Finding correspondences between two images is a fundamental building block of many computer vision applications. The most common approach to image matching uses sparse interest points matched using high-dimensional representations. However, describing each point accurately is challenging in conditions with symmetries, weak texture, or appearance changes. Additionally, outliers from occlusion and missing points need to be rejected, requiring representations that are both robust and unique.To address these limitations, SuperGlue introduced a deep network that jointly matches sparse points and rejects outliers using the Transformer model. SuperGlue has been effective for visual localization and generalizes well to other tasks. However, it is computationally expensive and difficult to train.In this paper, we present LightGlue, a deep network that is more accurate, efficient, and easier to train than SuperGlue. We make several architecture modifications and provide insights on training high-performance deep matchers with limited resources. LightGlue achieves state-of-the-art accuracy within a few GPU-days and is Pareto-optimal in terms of efficiency-accuracy trade-off.Unlike previous approaches, LightGlue adapts to the difficulty of each image pair, dynamically adjusting the network size instead of reducing its capacity. It outperforms sparse matchers in typical operating conditions and competes with dense matchers in terms of spatial accuracy, but at a fraction of the run time.We also discuss techniques to make Transformers more efficient, such as reducing the memory footprint of attention and adaptively modulating the network depth. LightGlue predicts a set of correspondences after each computational block and predicts whether further computation is required, allowing for faster inference on easy image pairs.Our experiments show that LightGlue is a plug-and-play replacement for SuperGlue, predicting strong matches at a fraction of the run time. This opens up possibilities for deploying deep matchers in latency-sensitive applications like SLAM and scene reconstruction. The LightGlue model and training code will be publicly released with a permissive license.