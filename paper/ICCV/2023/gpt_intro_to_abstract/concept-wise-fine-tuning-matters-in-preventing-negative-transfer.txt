Pre-trained models have become a milestone in the artificial intelligence community, as they bridge the gap between the reliance of deep neural networks on extensive data and the lack of annotated data in real-world applications. However, the practice of fine-tuning these pre-trained models is not always successful, leading to the problem of negative transfer, especially when the downstream tasks are different from the pre-training data distribution. Even state-of-the-art fine-tuning methods like Bi-tuning still suffer from this issue. In addition to the prediction gap between from-scratch models and fine-tuned models, there is also a gap in the attended regions of the image. This gap can be attributed to underperforming features offered by the pre-trained models.This study focuses on two types of underperforming pre-trained features: rare features and spuriously correlated features. Rare features are undertrained by the pre-training dataset and divert the attention of the fine-tuned model. Spuriously correlated features are mislead by correlations present in the pre-training dataset. To address these issues, the authors propose a fine-tuning strategy called "Concept-Tuning". This strategy aims to refine rare features by maximizing the mutual information between examples and deconfounds the negative transfer caused by spuriously correlated features.The contributions of this work include identifying the two types of underperforming features, proposing the Concept-Tuning approach, and demonstrating its effectiveness compared to state-of-the-art fine-tuning methods. Concept-Tuning improves sample-level fine-tuning by a significant margin and shows consistency across various classification datasets, pre-trained models, network architectures, and sample sizes. It also extends well to semantic segmentation and domain generalization tasks. Overall, this research provides insights and techniques to improve the fine-tuning process for pre-trained models.