Semantic segmentation is a fundamental problem in computer vision that involves dividing an image into semantic regions. Traditional methods are limited in their ability to generalize beyond predefined classes, but recent advancements in joint vision-language representation learning have enabled the prediction of segments corresponding to arbitrary text queries in a task called open-vocabulary segmentation. However, there is a need to segment images without user input or guidance. This paper introduces a novel problem called zero-guidance segmentation and presents a baseline method for solving it. The method leverages the CLIP model and a self-supervised visual model called DINO to perform segmentation without further training. The approach involves over-segmenting the image, translating each segment into words, and then joining semantically similar segments. A novel attention-masking technique called global subtraction is introduced to balance global and local contexts in CLIP's joint space. The resulting embeddings are used to generate text labels for the segments. Evaluation metrics are proposed to assess the performance of the algorithm. Despite performance gaps compared to supervised methods or fine-tuned models, the proposed method addresses the problem of zero-guidance segmentation and provides insights for future research.