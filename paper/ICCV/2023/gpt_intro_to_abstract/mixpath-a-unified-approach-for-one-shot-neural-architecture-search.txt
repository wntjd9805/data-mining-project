This paper introduces a unified approach for multi-path one-shot Neural Architecture Search (NAS) in automated machine learning. While existing one-shot approaches focus on single-path networks, this paper proposes incorporating multi-path structures into the search space. The authors address the challenge of accurately ranking submodels in a multi-path scenario. They propose a novel mechanism called shadow batch normalization (SBN) to stabilize the supernet during training, reducing the number of SBNs needed. Through experiments, the authors demonstrate the effectiveness of their method in achieving state-of-the-art performances in various spaces, including ImageNet. Their approach offers a promising solution for automated design of neural networks.