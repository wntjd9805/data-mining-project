Facial expression analysis plays a crucial role in understanding human emotions and behavior. This paper focuses on Facial Expression Recognition (FER) and Action Unit recognition (AUR), which are essential tasks in facial analysis. The paper begins by discussing the universal nature of facial expressions and the categorization of emotions. It then introduces the Facial Action Coding System (FACS) and the concept of Action Units (AUs), which are specific facial muscle movements associated with different expressions. The paper highlights the recent advancements in deep learning-based approaches for FER and AUR, including methods that disentangle expression or AU features from other factors. Self-Supervised Learning (SSL) is also explored as a means to leverage unlabeled data for representation learning. The limitations of existing approaches, such as the low consistency between image and text data, are discussed. The paper proposes a text-driven contrastive learning method, called CLEF, that utilizes coarse-grained information and text-embedded labels. The proposed method consists of two stages and uses the CLIP architecture for pre-training and fine-tuning. In pre-training, the activity descriptions are used as coarse-grained labels to guide weakly-supervised contrastive learning. In fine-tuning, vision-text contrastive learning and supervised contrastive learning are used to enhance feature representations and improve performance in downstream tasks. The proposed method is evaluated on both in-the-lab and in-the-wild datasets and achieves state-of-the-art performance in all six datasets. The contributions of this paper include the development of a weakly-supervised contrastive learning method that leverages coarse-grained activity information, the exploration of text-driven contrastive learning for FER and AUR tasks, and the demonstration of the effectiveness of the proposed method through extensive experiments.