In this computer science paper, the authors discuss the concept of compositionality in neural networks and its relevance to cognition. They highlight the emergence of compositionality in deep neural networks trained on language data, which has sparked interest in studying the compositionality of visual representations. However, while these models capture the compositional structure of data, the latent compositional structure within the models themselves is not directly accessible.The authors propose leveraging the computational architecture of Transformers to explore linear compositionality, the simplest form of composition. They utilize recent results on linearization of deep neural networks around a pre-trained point, which can be trained through convex optimization and perform on-par with non-linear fine-tuning. The authors suggest that the tangent space at pre-trained models can be used to linearly compose neural activations or combine different models trained on different datasets or tasks.The paper demonstrates that models obtained from linearization around a pre-trained point can be composed, combined, rescaled, and forgotten through scalar combinations. This enables ensembling at the cost of a single model and continual fine-tuning with independent training of component models. However, the authors acknowledge that linear combination may not be applicable for general concept composition or arbitrary multi-task learning, as it is limited to models that are "local" to a pre-trained embedding.The limitations of the approach are discussed, including the inability to linearly compose models trained on tasks in disparate representation spaces. Despite these limitations, the authors show that linear composition can compete with other forms of ensembling and continual learning methods in common settings. They propose constructing a tangent bundle of models around different pre-trained points to expand task coverage.The paper concludes with an overview of prior art, a detailed description of the proposed method, empirical evidence supporting the approach, and a discussion of its main limitations. The authors highlight the efficiency, interpretability, and modularity of their approach compared to other models with shared backbones and heavy heads.