Clustering is a fundamental problem in machine learning, allowing data to be grouped into clusters based on assumptions about the geometry of each cluster. Classic algorithms like k-means emerged in the 1950s to cluster data around distinct centroids, and subspace clustering methods later generalized this assumption to low-dimensional linear or affine subspaces. These methods have been successfully applied in various applications, but they often fail when the geometric assumption is violated for high-dimensional datasets. In such cases, it is more natural to assume that each cluster is defined by a non-linear low-dimensional manifold and to learn a non-linear transformation of the data to map points to linear subspaces. However, hand-crafted design and existing methods have limitations and often require specific assumptions or a large number of samples. To address these challenges, this paper proposes a novel approach called Manifold Linearizing and Clustering (MLC) that combines the philosophies of Maximal Coding Rate Reduction (MCR2) and subspace clustering. MLC optimizes the MCR2 loss over both the learned representation and a novel doubly stochastic cluster membership. The representation and membership are parameterized and initialized in a structured and efficient manner, allowing for mini-batching and one-shot initialization. Experimental results on CIFAR-10, CIFAR-20, CIFAR-100, and TinyImageNet-200 demonstrate that MLC achieves higher clustering accuracy with less running time compared to state-of-the-art subspace clustering and deep clustering methods, even in the presence of many or imbalanced clusters. This work addresses the questions of efficient data transformation near a union of low-dimensional manifolds for easy clustering and learning a union-of-orthogonal-subspaces representation without access to ground-truth labels. The proposed MLC approach shows promising results and has the potential to improve clustering performance in various real-world applications.