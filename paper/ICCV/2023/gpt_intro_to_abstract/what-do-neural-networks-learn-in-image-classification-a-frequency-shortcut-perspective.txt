Deep neural networks (DNNs) have become popular in various fields, but their underlying predictive processes are not well understood due to their complex structure. Analyzing the learned features can provide insights into DNN predictions, but further exploration is needed. Previous research has focused on explaining predictions based on input saliency, but fails to explain the degradation of performance on out-of-distribution (OOD) data. Recent interest has emerged in understanding the learning dynamics of DNNs from a frequency perspective. It has been observed that DNNs tend to learn lower frequencies first, leading to a simplicity bias and the learning of shortcut solutions that disregard problem semantics. In this study, we empirically analyze the learning dynamics of DNNs for image classification and relate it to simplicity bias and shortcut learning based on frequency. Our findings indicate that DNNs exploit specific frequency sets, known as frequency shortcuts, to facilitate predictions. These shortcuts can be texture-based or shape-based and may affect generalization. We demonstrate the phenomenon of frequency shortcuts and their impact on classification in our experiments. Unlike previous research, we focus on image classification and expand the understanding of frequency shortcuts. We propose a metric to compare frequency characteristics and investigate the impact of frequency shortcuts on OOD generalization. Our contributions include insights into the learning dynamics of DNNs, the proposal of a frequency shortcut identification method, and the examination of shortcut influence on generalization. We recommend further research to avoid frequency shortcut learning.