Estimating the 6D pose of 3D objects from monocular images is an important task in computer vision with practical applications in robotics, autonomous driving, and augmented reality. While depth sensors can facilitate this task, they are not widely available, making 6D object pose estimation from RGB images an ongoing research area. Early methods used deep neural networks to directly regress the object pose from the input image. More recent approaches draw inspiration from geometry and aim to predict 2D-3D correspondences, from which the 6D pose can be obtained by solving the Perspective-n-Points (PnP) problem. However, these methods do not supervise the training process with the ground-truth pose itself, as standard PnP solvers are not differentiable. To enable end-to-end training, previous attempts have incorporated differentiable PnP layers, but only leverage the optimal pose as supervision. In this paper, we introduce a novel approach that explicitly tackles the issue arising from the averaging nature of the PnP problem. We utilize the covariance of the pose distribution computed by leveraging the ground-truth pose before solving for the pose. This allows us to design a loss function that minimizes the diagonal covariance elements, effectively minimizing the residuals of the 2D-3D correspondences while considering the estimated pose. Our approach demonstrates superior performance on multiple datasets and both sparse and dense correspondence-based methods.