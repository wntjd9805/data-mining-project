Image stitching is a practical technology used in various fields such as autonomous driving, medical imaging, surveillance videos, and virtual reality. Traditional stitching approaches have used increasingly complicated geometric features to achieve better content alignment and shape preservation. However, these complex algorithms may not be practical in real-world applications without sufficient geometric structures or when running speed is a concern. In recent years, deep stitching technologies using convolutional neural networks (CNNs) have gained attention, but they struggle with large parallax and lack generalization in cross-dataset and cross-resolution conditions. In this paper, we propose a parallax-tolerant unsupervised deep image stitching technique that addresses the robustness issue in traditional stitching and the large-parallax issue in deep stitching. Our proposed solution uses a two-stage approach of warp and composition, with a robust and flexible warp that combines homography transformation and thin-plate spline transformation for effective alignment and distortion correction. The composition stage leverages unsupervised learning to generate seamless stitched images without introducing blurs. We also design an iterative strategy to enhance warp adaption in different datasets and resolutions. Extensive experiments demonstrate the superiority of our approach compared to state-of-the-art solutions. The contributions of this work include a robust and flexible warp, a novel composition approach, and an iterative strategy for warp adaption.