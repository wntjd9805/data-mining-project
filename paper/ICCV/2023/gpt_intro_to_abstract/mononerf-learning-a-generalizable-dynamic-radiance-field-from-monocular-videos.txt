Novel view synthesis is an important problem in computer science, with applications in movie production, sports event, and virtual reality. Neural rendering technology has made impressive progress in tackling this challenging problem. However, most existing approaches assume static scenes that can be observed from multiple views simultaneously, which is not the case for many videos found on the internet. To address this limitation, recent studies have focused on learning dynamic radiance fields from monocular videos. This is a challenging task due to the dynamic foregrounds and ambiguity in estimating precise point features and object motions from single views. Previous methods have used positional encoding to address this challenge, but they suffer from overfitting and lack transferability across different scenes. In this paper, we propose MonoNeRF, a method that incorporates 2D local features and optical flows to learn generalizable point features in 3D space. Our approach predicts 3D point features and scene flows concurrently, using point trajectory and feature correspondence constraints. Experimental results demonstrate that MonoNeRF can render novel views from multiple dynamic videos and support applications such as scene editing and fast novel scene adaptation. Our method also outperforms existing methods in the setting of novel view synthesis on training frames from single videos.