Considerable progress has been made in monocular 3D human pose and shape estimation from images. However, many applications require video-based pipelines to avoid spatial-temporal incoherence and missing frame-based detections. Existing video-based methods follow a multi-stage design, but they are sensitive to false detections and have a considerable computation cost. To address these issues, we propose CoordFormer, the first single-stage approach for multi-person 3D mesh recovery from videos that can be trained in an end-to-end manner. CoordFormer leverages a multi-head framework and our proposed Body Center Attention (BCA) to implicitly perform detection and tracking. We use our novel Coordinate-Aware Attention (CAA) integrated into a Spatial-Temporal Transformer (ST-Trans) to capture non-local context relations at the pixel level. Experimental results on the 3DPW dataset demonstrate that CoordFormer significantly outperforms state-of-the-art methods in terms of accuracy and inference speed. The main contributions of this work are the proposal of a single-stage multi-person video mesh recovery approach, the demonstration of the importance of pixel-level coordinate correspondence, and the significant improvements achieved compared to state-of-the-art methods.