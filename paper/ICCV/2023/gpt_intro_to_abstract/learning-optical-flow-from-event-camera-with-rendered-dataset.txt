This paper introduces the problem of estimating optical flow from event camera data. Event cameras record brightness changes in pixels and return events in the form (x, y, t, p) immediately when a change is detected, where x and y represent spatial location, t is the timestamp, and p indicates whether the pixel becomes brighter or darker. Traditional methods for optical flow estimation from event cameras can only estimate sparse flows at the location of events, while recent deep learning methods can estimate dense flows with the help of RGB frames. However, this paper tackles the challenging task of predicting dense flows purely based on event values. Creating a high-quality event-based optical flow dataset for training the network is a key issue addressed in this work. Existing methods for dataset creation involve capturing real event camera data or synthesizing flow motions on top of a background image. However, these methods suffer from inaccuracies in flow labels due to imperfect optical flow estimations or inaccurate frame interpolation. To overcome these limitations, the authors propose a Multi-Density Rendered (MDR) event optical flow dataset, created by rendering synthetic 3D scenes. They also design an Adaptive Density Module (ADM) to adjust the densities of events in the dataset. Experiments demonstrate that previous event-flow methods trained on the MDR dataset can improve their performance. Furthermore, training representative optical flow pipelines on the MDR dataset with the ADM module consistently increases their performance. The contributions of this paper include the MDR dataset, the ADM module, and improvements in the quality and performance of event-flow methods.