Recent advancements in RAW burst super-resolution pipelines have greatly improved the quality of smartphone photos. These pipelines utilize specialized deep learning models that merge burst frames into a high-resolution image. However, training these models typically requires paired datasets, which can be challenging to obtain. Synthetic datasets are often used, where bursts are generated from clean references using degradation models. However, there are mismatches between synthetic and real bursts, leading to poor generalization. Weakly-paired datasets, captured using DSLRs and zoom lenses, have also been used, but they have limitations such as misalignment and limited scene motion.This paper introduces a new self-supervised training strategy for burst super-resolution that overcomes the limitations of synthetic and weakly-supervised datasets. The proposed approach only requires real-world noisy bursts for training, eliminating the complexity of data collection for weakly-paired approaches. By using real bursts, the domain gap issues associated with synthetic training are avoided.The self-supervised training strategy utilizes a reconstruction objective that models the relationship between the noisy burst and the clean reference image. Burst images, as noisy and aliased measurements with random spatial offsets, are exploited to recover high-frequency image details. During training, bursts are randomly split into two sets, and a burst super-resolution network is used to produce a high-resolution output from the first set. Low-resolution images are derived from the high-resolution output using a degradation model and compared against the second set of burst images to compute the reconstruction loss. This loss is optimized on a large dataset of bursts with random camera displacements, enabling the learning of a robust image prior and the recovery of high-resolution merged images.The proposed loss function does not make any limiting assumptions about the parameters of the image formation model. The model's parameters, including the lens blur kernel, are jointly learned with the super-resolution network from data. This training approach is general and can be applied to any neural network architecture using bursts from any sensor.The contributions of this work include the introduction of the first self-supervised training approach for raw burst super-resolution using only noisy, low-resolution inputs. A robust self-reconstruction loss is developed to handle bursts with dynamic object motion. The approach also allows for joint learning of the lens blur kernel, alleviating the need for explicit estimation. Extensive experiments on real-world burst datasets using different network architectures demonstrate promising results compared to models trained with weakly-paired data, despite using only low-resolution bursts.