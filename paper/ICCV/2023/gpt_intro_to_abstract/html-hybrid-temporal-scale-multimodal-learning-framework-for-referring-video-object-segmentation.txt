Video Object Segmentation (VOS) plays an important role in various applications such as visual editing, virtual reality, and human-robotic interaction. Referring Video Object Segmentation (RVOS) is a specific type of VOS task that aims to segment a specific object instance in a video based on an open-world description. Unlike traditional vision-only VOS, RVOS requires models to learn both visual and textual contents comprehensively in order to accurately infer the referred object. Recent studies have shown that cross-modal attention is an effective approach to bridge the gap between vision and language in RVOS. However, existing approaches that perform vision-language interactions with video frames sampled from a single temporal scale may be limited in accurately inferring the referred object due to the variability in length and semantics of the open-world descriptions. To address this limitation, we propose a Hybrid Temporal-scale Multimodal Learning (HTML) Framework for RVOS. Our framework incorporates different temporal scales to sample video frames and introduces intra-scale and inter-scale multimodal perception modules. These modules allow for the exploitation of core visual semantics within frames at different temporal scales and dynamic interaction between linguistic and visual features. By hierarchically leveraging object context from all temporal scales, our HTML framework achieves state-of-the-art performance on benchmark datasets. The contributions of our work include a concise and unified learning framework, an effective multimodal perception module, and superior performance on widely-used benchmarks.