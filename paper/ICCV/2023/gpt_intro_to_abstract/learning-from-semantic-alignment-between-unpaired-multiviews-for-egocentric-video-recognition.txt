Recent research has focused on understanding and learning from egocentric videos. However, collecting synchronized first-person and third-person video pairs, which are important for multiview learning, is challenging and limits the scale and scope of the data. In this paper, we investigate the use of unpaired third-person videos to assist in egocentric video learning. We propose a Semantics-based Unpaired Multiview Learning (SUM-L) method that aligns partially semantics-similar pseudo-pairs in a semantics-aware manner. Our approach also incorporates video-text alignment using a large language model to facilitate cross-view and cross-modality learning. We compare our method to existing view-alignment approaches and demonstrate its superiority in terms of first-person video representation learning. Experimental results on benchmark datasets validate the effectiveness of our proposed method.