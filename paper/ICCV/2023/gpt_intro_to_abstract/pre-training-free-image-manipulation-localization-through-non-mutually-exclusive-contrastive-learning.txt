Advancements in media techniques have made it easier to manipulate images, necessitating the development of Image Manipulation Localization (IML) for defensive information forensics. However, the insufficient availability of data poses a challenge in training deep IML models. Current deep IML methods rely on pre-training on synthesized datasets, but this hampers fair comparisons among models and the generalizability of the models. Furthermore, these synthesized datasets lack the elaborate post-processing present in real tampered images. In this paper, we propose the use of contrastive learning in IML to address the data insufficiency problem. Contrastive learning can provide large numbers of contrastive pairs from real tampered images without sampling bias. We argue that patch-wise contrastive learning is more suitable for IML since manipulations occur at the patch level. However, the presence of contour patches, which are non-mutually exclusive to tampered and authentic patches, poses a challenge for conventional contrastive learning. To handle this, we introduce the Non-mutually exclusive Contrastive Learning (NCL) framework, which disentangles the trilateral, non-mutually exclusive contrast among different patch types. We also propose a pivot structure to handle the role-switching characteristic of contour patches. Our NCL-based method achieves better localization accuracy and robustness compared to pre-training-based approaches, and it is compatible with different backbone architectures. Our contributions include the introduction of contrastive learning in IML, the handling of non-mutual exclusive trilateral relations, top benchmark performance with limited training data, and the plug-in merit of our method.