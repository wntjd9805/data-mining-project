Spiking Neural Networks (SNNs) have emerged as a new approach to information encoding and transmission by mimicking the behavior of biological neurons. These networks only fire when the membrane potential exceeds a certain threshold, and their spike-based binary communication enables event-driven computing on neuromorphic chips. While it is widely acknowledged that sparse spike firing leads to high energy efficiency, there is a lack of systematic analysis of redundancy in SNNs. Existing methods for reducing spikes often result in loss of accuracy or additional computation. In this paper, we propose a new perspective on understanding redundancy in SNNs by analyzing the relationship between spike firing and spatio-temporal dynamics. We investigate three key questions: which spikes are redundant, why there is redundancy in SNNs, and how to efficiently drop redundant spikes. To demonstrate redundancy in SNNs, we focus on event-based vision tasks using a Dynamic Vision Sensor (DVS), which encodes brightness changes into a stream of events for each pixel. We observe that noise features extracted by the SNN focus on background information, leading to redundant spike firing. We argue that spatio-temporal invariance, which allows weight sharing across all time steps, is the underlying reason for this phenomenon. We present an Advanced Spatial Attention (ASA) module that can convert noise features into normal or null features by shifting the distribution of membrane potential. Experimental results on event-based datasets demonstrate that the ASA module reduces spike counts and improves task performance simultaneously. Overall, this work provides a systematic analysis of redundancy in SNNs and proposes an efficient method for reducing spikes without sacrificing accuracy.