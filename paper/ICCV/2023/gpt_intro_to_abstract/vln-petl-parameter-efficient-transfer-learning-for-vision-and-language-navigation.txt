Large-scale pre-trained models have greatly improved the performance of visio-linguistic tasks in both computer vision and natural language processing domains. However, the growing size of these models presents challenges for fine-tuning and storing multiple copies for each downstream task. Parameter-Efficient Transfer Learning (PETL) has been proposed as an alternative training strategy to address this issue. PETL methods freeze most parameters of the pre-trained model and only tune a small set of parameters, achieving comparable or even better performance than full fine-tuning. While PETL techniques have been successfully applied in NLP and CV domains, their effectiveness in Vision-and-Language Navigation (VLN) tasks has not been explored extensively. VLN, which involves visual, linguistic, and robotic action inputs, could benefit from pre-trained large models but struggles with the high model size during fine-tuning. In this paper, we propose a VLN-specific PETL method called VLN-PETL. We introduce two tailored PETL modules for VLN: Historical Interaction Booster (HIB) and Cross-modal Interaction Booster (CIB), which enhance historical knowledge and cross-modal interaction, respectively. These modules are inserted into the pre-trained model and their weights are updated for different downstream VLN tasks while the weights of the original model remain frozen. The experiments conducted on four VLN tasks demonstrate the effectiveness of VLN-PETL, surpassing other PETL methods and achieving comparable or better performance compared to full fine-tuning with significantly fewer trainable parameters. Overall, this study contributes to the exploration of PETL techniques for VLN tasks and presents a novel VLN-PETL method that improves parameter-efficient tuning in VLN.