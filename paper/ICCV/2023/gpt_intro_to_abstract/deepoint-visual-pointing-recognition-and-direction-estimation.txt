This paper addresses the challenges of visual understanding of pointing gestures using fixed-view cameras in natural environments. Despite the importance of pointing in conveying intent, research on visual pointing interpretation has been limited. The authors introduce a comprehensive dataset called the DP Dataset, which consists of 2,800,000 frames of 33 people pointing in different styles and directions. Each frame is annotated with pointing status and the intended 3D direction. They also propose DeePoint, a deep network model that leverages whole-body appearance and motion to accurately detect and estimate 3D pointing directions. Extensive experiments are conducted to evaluate the effectiveness of DeePoint, demonstrating its accuracy and efficiency. Future work includes incorporating environmental cues and audio to enhance pointing direction estimation.