This paper addresses the challenge of teaching neural networks to accurately identify object boundaries, which is difficult to achieve with traditional annotation methods at internet scale. The authors propose using Latent Diffusion Models (LDMs) as a solution, as these models can be trained without object-level supervision and naturally attend to object boundaries. The authors hypothesize that LDMs can learn features that are useful for open world image segmentation. To test this hypothesis, the authors conducted experiments and demonstrated that LDMs can improve segmentation performance compared to standard baselines. Moreover, the authors found that applying LDM-based segmentation models on AI-generated images further amplified these performance gains. The authors also conducted an experiment to examine the presence of object-level semantic information within pre-trained LDMs. Through this experiment, the authors found that LDMs encode fine-grained semantic information that can be useful for segmentation tasks. Based on these findings, the authors propose a text-based segmentation architecture called ZNet, which operates on the latent space of LDMs. Additionally, the authors propose an architecture called LD-ZNet that incorporates latent diffusion features from LDMs to improve text-based image segmentation. Overall, the contributions of this work include the proposal of ZNet and LD-ZNet architectures, the demonstration of the usefulness of LDM internal representations for text-based image segmentation, and the improvement of segmentation performance over multiple metrics and domains.