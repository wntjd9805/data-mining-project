The paper introduces a novel federated learning algorithm called FedIns that addresses the issue of intra-client data heterogeneity, which has been overlooked in existing federated learning literature. Unlike previous approaches that rely on instance-adaptive models, FedIns focuses on instance-adaptive inference by fine-tuning pre-trained models using scaled and shifted deep features (SSF) in the local training phase. These SSF are then merged into the original pre-trained model weights in the inference phase. Each client maintains an SSF pool, which is aggregated on the server to reduce storage and communication costs. During inference, the best-matched SSF subsets are dynamically selected from the pool to generate an adaptive SSF specific to each test instance, reducing both intra- and inter-client heterogeneity. Experimental results demonstrate the effectiveness of FedIns in improving federated learning performance by mitigating data heterogeneity.