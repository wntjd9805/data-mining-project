Real-time multi-person tracking in crowded real-world scenarios is a challenging and difficult problem in computer science with various applications. Currently, the most successful methods in literature can be grouped into three main categories: tracking-by-detection, tracking-by-regression, and tracking-by-attention. Despite the recent interest in tracking-by-detection, it still proves to be competitive due to its simplicity and reliability. This paper aims to strengthen tracking-by-detection algorithms by incorporating additional cues such as skeletal poses, depth maps, and thermal measurements. The goal is to provide the tracker with a predicted distance from the camera, resembling a "2.5D" perspective. To achieve this, a deep regressor is trained on a synthetic dataset with diverse scenes and conditions. However, the fusion of multi-modal representations raises the question of how to weigh the contribution of each input domain. Existing approaches rely on formulas and heuristics, which introduce additional hyperparameters and require careful tuning. Additionally, these approaches assume independence among input modalities, overlooking their interactions. To address these limitations, this paper proposes a parametric density estimator called TrackFlow, which summarizes multiple input costs/displacements into a single output metric. The estimator is borrowed from the literature of Normalizing Flow models, which are flexible and effective for density estimation. Furthermore, the module relies on an additional context-level representation to inform the model about scene-level peculiarities. Extensive experiments on different datasets demonstrate that our approach outperforms the naive cost metric, resulting in significant performance improvements.