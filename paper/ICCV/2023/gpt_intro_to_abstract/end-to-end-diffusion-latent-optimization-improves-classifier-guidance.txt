Text-conditioned denoising diffusion models (DDMs) have been widely used to generate realistic images based on text prompts. However, these models are limited in their ability to incorporate other modalities for conditioning. Classifier guidance has shown promise in expanding the capabilities of DDMs by combining the score estimate of the diffusion model with the gradient of an image classifier. However, existing approaches either require re-training the classifier or rely on one-step approximations, leading to sub-optimal results. In this paper, we propose Direct Optimization Of Diffusion Latents (DOODL), a method that optimizes the noise vectors of the DDM based on a model-based loss. We leverage a drop-in discretely invertible diffusion algorithm to compute accurate gradients for classifier guidance. We demonstrate the effectiveness of DOODL through various experiments, including improving compositionality and vocabulary expansion, personalized entity generation, and enhancing the aesthetic quality of images. We believe that DOODL can enable and inspire new plug-and-play capabilities for pretrained diffusion models.