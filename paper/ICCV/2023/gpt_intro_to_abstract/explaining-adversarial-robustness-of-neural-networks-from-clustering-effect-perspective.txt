Abstract:Neural networks have proven to be successful in various real-world applications. However, recent studies have shown that deep models are susceptible to human- imperceptible perturbations that can cause them to produce incorrect outputs. This vulnerability has raised concerns about the use of neural networks in safety-critical scenarios. Adversarial Training (AT) has been proposed as a defense mechanism against these perturbations by minimizing the loss function. However, recent research has shown that AT is only effective against output-layer attacks and not intermediate-layer attacks. This paper addresses the insufficiency of AT and proposes a unified AT framework, called Sufficient Adversarial Training (SAT), that defends against both types of attacks. SAT incorporates a regularization loss based on the clustering effect of intermediate layers into the Cross-Entropy loss. Experimental results demonstrate the effectiveness of SAT in improving the adversarial robustness of neural networks against both output-layer and intermediate-layer attacks.