LiDAR-based 3D object detection is widely used in autonomous driving and robotics, but most methods focus on optimizing a specific model for a single domain, leading to poor generalization to other domains. To address this issue, researchers have used cross-domain adaptation techniques, but they often overlook the importance of preserving generalization to the source domain. Optimizing a specific model for each domain is costly and lacks scalability, and it cannot fully utilize the knowledge of each domain to learn a universal detector. In this paper, we propose multi-domain learning and generalization for LiDAR-based 3D object detection. We develop a joint-training baseline model and a multi-domain knowledge transfer framework to learn universal feature representations. We also explore normalizing intermediate features to optimize the model when training data are from multiple domains. Our approach outperforms state-of-the-art methods in terms of multi-domain learning and generalization in LiDAR-based 3D object detection.