Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, thanks to their self-attention representation and training techniques like data augmentation and knowledge distillation. Token labeling, where patch tokens are assigned with labels in a dense manner, is an important development in training ViTs. Token labeling allows ViTs to leverage more fine-grained information in an image and take different categories and object localization into account, leading to more accurate results. However, less exploration has been done on modeling ViTs as token-labelers. This paper aims to address this gap by proposing a self-emerging token labeling (STL) framework that uses self-produced token labels by ViT token-labelers instead of relying on convolutional neural networks (CNNs). The proposed framework builds on the Fully Attentional Network (FAN) for its self-emerging visual grouping capabilities and state-of-the-art accuracy and robustness. The contributions of this work include demonstrating the effectiveness of ViT models as token-labelers, performing an in-depth analysis of critical factors in the accuracy of token labels, setting new records on out-of-distribution datasets without using extra data, and showcasing improved performance in semantic segmentation and object detection tasks. The STL framework consists of two stages: training a FAN token-labeler model to generate token-level annotations and training a FAN student model using the original class labels and token labels from FAN-TL, with a token selection approach based on Gumbel-Softmax to improve pre-training.