This paper introduces a system for automatic scene clutter removal and 3D inpainting, which aims to reconstruct a clutter-free 3D scene starting from a set of indoor RGB-D scans. The system consists of two steps: clutter segmentation and 3D inpainting. The authors highlight the challenges in existing annotations for clutter segmentation, such as intra-category ambiguity and suboptimal quality. They propose a deep architecture that takes in RGB images and 3D points as input and renders virtual views for improved surface coverage. To perform 3D inpainting, the authors project object masks onto RGB-D images and perform image inpainting and image-guided depth completion. They incorporate consistency voting and pruning across frames to ensure cross-frame consistency in the reconstruction. Extensive experiments on the ScanNet and Matterport3D datasets validate the effectiveness of the proposed system, showing improvements over 3D segmentation and inpainting baselines. The contributions of this paper include the automatic system for scene clutter removal and inpainting, a 3D segmentation method with area-sensitive loss for better segmenting small objects, and a 3D inpainting method with iterative view-consistent RGB-D inpainting.