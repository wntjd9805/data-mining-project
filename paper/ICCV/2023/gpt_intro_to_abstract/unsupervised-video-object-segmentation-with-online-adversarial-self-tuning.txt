Video Object Segmentation (VOS) is an important task in computer vision that aims to track moving objects in a video sequence with accurate segmentation masks. There are two main paradigms in VOS: Semi-supervised VOS (SVOS) and Unsupervised VOS (UVOS). SVOS relies on the availability of ground truth masks during testing to segment the objects, while UVOS does not have this prior information. UVOS is more challenging but closer to real-world applications. Existing UVOS methods train a model on labeled video sets and directly apply it to unlabeled test videos. However, these methods may suffer from degraded performance when the test data come from a different domain than the training data, due to "visual discrepancy". Visual discrepancy refers to differences in appearance and characteristics between the training and test data. In this paper, we propose an online test-time finetuning method called Online Adversarial Self-Tuning (OAST) to address the visual discrepancy in UVOS. Our method dynamically updates the VOS model during testing to adapt to the new domain of the test data. We tackle the challenges of overfitting to labeled training data and lack of supervision in the test video by using a semi-supervised adversarial training strategy and a self-supervised adversarial finetuning strategy respectively. Experimental results on benchmark datasets demonstrate that our OAST method achieves state-of-the-art performance and significantly improves generalization ability in UVOS. Our contributions include the first online finetuning method for UVOS, an offline semi-supervised adversarial training method, and an online self-supervised adversarial finetuning method for test-time adaptation.