Large language models and vision-language models have shown the ability to perform in-context learning (ICL), where they can generate predictions based on demonstrations and query data without parameter updates. However, the in-context ability of these models relies heavily on the language side, leading to issues such as template sensitivity and hallucination. Previous works have shown that in-context ability scales with model size and is limited in smaller models. In this paper, we propose a framework called Self-supervised IN-Context learning (SINC) that decouples the acquisition of in-context ability from pre-training in VL models. We introduce a meta-model that operates on representations from frozen models and is trained using self-supervised prompts. Our approach allows for the transferability of in-context ability to downstream tasks and outperforms previous methods. Through extensive experiments, we analyze the properties and essential components of in-context learning in the VL domain.