Generative modeling of 3D humans from real-world data has shown promise in representing and synthesizing diverse human shapes, poses, and motions. The ability to create realistic humans with diverse clothing and accessories is essential for various applications, such as AR/VR, entertainment, and virtual try-on. While early work has focused on modeling "undressed" human bodies, recent advancements in shape representations have allowed for the generative modeling of clothed humans. However, existing approaches treat human bodies, clothing, and accessories as an entangled block of geometry, leading to suboptimal expressiveness and composability of the generative avatars. To address this, we propose a compositional generative model of objects and humans from real-world observations. This presents challenges in learning the composition and decomposition of objects in contact from raw 3D scans. Our contributions include a scalable data capture protocol, unsupervised decomposition of objects and humans, and a generalizable neural object composition method. We demonstrate that our approach outperforms existing methods and can be used for fine-grained controls, such as object removal and multiple object compositions on a human.