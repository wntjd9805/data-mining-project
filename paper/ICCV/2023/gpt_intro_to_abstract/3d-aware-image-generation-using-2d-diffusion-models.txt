Learning to generate 3D contents has gained significant attention due to its diverse applications in virtual reality, movie production, and art design. Various methods have been proposed for 3D-aware image generation, typically relying on Generative Adversarial Networks (GANs) and Neural Radiance Field (NeRF) for scene representation. However, extending these methods to handle complex variations in large-scale, in-the-wild data poses challenges. Diffusion Models (DMs) have shown exceptional generative modeling performance on billion-scale image datasets and have surpassed GANs for complex image generation tasks. This paper proposes a novel approach that formulates 3D-aware generation as a multiview 2D image set generation task, addressing the issues of applying DMs and the lack of multiview image data. The method involves sequential unconditionalâ€“conditional generation using chain rule of probability and utilizes depth estimation techniques for constructing multiview data from still images. Experimental results on ImageNet and other datasets demonstrate the superior performance of the proposed method compared to state-of-the-art 3D-aware GANs, showcasing improved geometry and texture quality, as well as the ability to generate scenes under large view angles. The contributions of this work include the novel formulation for 3D-aware generation, application on a large-scale dataset, and the capability for large-angle generation from unaligned data.