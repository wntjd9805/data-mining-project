In this computer science paper, the authors address the challenge of collecting large-scale datasets for instance-level perception tasks such as detection and segmentation. While paired text-image data from the internet is a valuable resource for general foundation models, pixel-level annotations are required for every instance, making it difficult to scale up the datasets. The authors propose a solution to decouple detection and classification tasks, adopting a class-agnostic segmentation network to locate objects and relying on a vision language model for classification. This framework enables the discovery of novel objects and fits within the definition of open-world instance segmentation. The authors also explore the impact of including category information in the training phase, showing that class-aware training harms the model's ability to generalize to unknown objects. They propose a prompt learning mechanism called SegPrompt, which uses category information as auxiliary supervision to improve class-agnostic segmentation quality. Experiments demonstrate the effectiveness of SegPrompt in improving overall and unseen detection, and its potential for open vocabulary and few-shot segmentation tasks. The authors also introduce a new benchmark, LVIS-OW, which explicitly separates known, seen, and unseen categories to align with the real-world long-tail object discovery challenge. The contributions of this work include the SegPrompt training mechanism, the LVIS-OW benchmark, and the demonstration of the effectiveness of category-level prompts in improving segmentation.