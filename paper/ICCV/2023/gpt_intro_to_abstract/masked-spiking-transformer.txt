Spiking neural networks (SNNs) have gained attention for their ability to handle sparse data and achieve energy efficiency benefits on neuromorphic hardware. However, SNNs currently struggle to match the performance of artificial neural networks (ANNs), especially for complex tasks like ImageNet. To address this, training methods such as direct training and ANN-to-SNN conversion have been proposed. While direct training methods can lead to unstable gradient propagation and lower accuracy, ANN-to-SNN conversion methods require more time steps and increased power consumption. In this paper, we focus on implementing the ANN-to-SNN conversion method to narrow the performance gap between ANNs and SNNs while reducing energy consumption. We propose a Masked Spiking Transformer (MST) model that incorporates a Random Spike Masking (RSM) method to selectively reduce the number of spikes involved in the computation process. Our experiments show that the RSM method significantly reduces energy consumption in the self-attention module and MLP module of the Transformer backbone, leading to superior performance compared to existing SNN models. Moreover, the RSM method can be extended to other backbones like ResNet and VGG, highlighting its potential as a general technique to improve SNN efficiency. Overall, our work provides a new direction for developing high-performance and energy-efficient SNN models.