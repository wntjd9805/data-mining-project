In computer vision tasks, learning invariant representations that are robust under image transformations is essential. While previous works have focused on invariances to 2D image transformations, the exploration of viewpoint transformations in the 3D world is limited. Viewpoint changes pose challenges in safety-critical applications, emphasizing the need for viewpoint-invariant visual recognition models. However, building such models is challenging as traditional networks do not consider the structure of 3D objects from 2D images. Adversarial training has shown promise in improving model invariance, but it is difficult to directly apply it to improving viewpoint robustness. Existing approaches lack diversity in adversarial viewpoints and are time-consuming to optimize. To address these limitations, we propose Viewpoint-Invariant Adversarial Training (VIAT), a framework that improves viewpoint robustness through distribution-based min-max optimization. We introduce GMVFool as a solution to the inner maximization problem, generating a Gaussian mixture distribution of diverse adversarial viewpoints. We adopt a stochastic optimization strategy and Instant-NGP to accelerate training. In the outer maximization, we propose a distribution-sharing strategy to improve generalization. We create a new multi-view dataset, IM3D, with realistic images, accurate camera pose, and larger category coverage, serving as training and evaluation for our method. Experimental results show the effectiveness of GMVFool and VIAT in generating diverse adversarial viewpoints and improving the robustness of image classifiers. We also create a new benchmark, ImageNet-V+, for evaluating viewpoint robustness.