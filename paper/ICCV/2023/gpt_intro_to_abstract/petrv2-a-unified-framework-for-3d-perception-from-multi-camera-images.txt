Recently, there has been significant interest in 3D perception from multi-camera images for autonomous driving systems. Current methods for multi-camera 3D object detection can be categorized into BEV-based and DETR-based approaches. BEV-based methods transform multi-view features into bird-eye-view (BEV) representation, while DETR-based approaches model each 3D object as an object query. Among these methods, PETR extends DETR by converting multi-view 2D features to 3D position-aware features using 3D position embedding (3D PE). However, PETR lacks temporal modeling and support for multi-task learning. In this paper, we propose a unified framework that extends PETR with temporal modeling and multi-task learning. We address the problem of aligning object positions in different frames by leveraging the effectiveness of 3D PE. For multi-task learning, we introduce sparse task-specific queries for different tasks, enabling the same transformer to update their representation. We also improve the generation of 3D PE by introducing a feature-guided approach. Our framework achieves state-of-the-art performance on 3D object detection, BEV segmentation, and 3D lane detection. We also provide a comprehensive robustness analysis of the PETR framework.