Deep learning models have achieved remarkable performance in various computer vision tasks, including image and video recognition. However, these models are vulnerable to adversarial attacks, where imperceptible perturbations are used to manipulate inputs and produce inaccurate predictions. Adversarial attacks can have serious consequences in applications such as autonomous vehicles and surveillance cameras. While there has been extensive research on adversarial attacks in the image domain, the problem of adversarial attacks on video models remains largely unsolved. Adversarial attacks can be categorized into white-box and black-box attacks. White-box attacks exploit model information to generate adversarial examples, while black-box attacks are more challenging due to the lack of model access. In real-world scenarios, accessing the target model is often difficult or impossible, making black-box attacks more practical. Transfer-based attacks leverage the transferability of adversarial examples, allowing attackers to apply adversarial examples crafted using accessible source models to target models. Universal Adversarial Perturbations (UAPs) pose a powerful threat as a single perturbation can mislead deep learning models on entire datasets. Our study aims to extend the applicability of UAPs generated using image data and models to video data and models. We propose the Breaking Temporal Consistency (BTC) method, which incorporates temporal information into video attacks using image models. BTC-UAPs are generated by minimizing the feature similarity between neighboring frames in videos and are both temporal shift invariant and length-agnostic. We demonstrate the effectiveness of BTC-UAPs through extensive experiments on various datasets, outperforming existing methods.