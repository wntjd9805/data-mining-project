Deep Neural Networks (DNNs) have achieved remarkable success in various tasks, relying on high-quality annotated data. However, collecting accurately labeled data, especially on a large scale, can be challenging in real-world scenarios, resulting in noisy labels. DNNs are vulnerable to overfitting noisy labels, leading to poor generalization performance. To address this, numerous methods have been proposed for Learning with Noisy Labels (LNL). One common approach is training the classifier with confident examples, which are examples with high-probability clean labels. This approach involves two strategies: the "small-loss trick" and "early stopping". However, these methods unintentionally limit the model's capability to effectively learn clean hard examples (CHEs) in noisy datasets. CHEs are clean examples close to the decision boundary. Identifying CHEs in noisy datasets is challenging, as they can often be entangled with mislabeled examples. Existing LNL methods aim to eliminate potential mislabeled examples, but this also eliminates many CHEs. In this paper, we propose a novel concept called First-time k-epoch Learning (FkL), which identifies examples that have been predicted as their given label for consecutive k epochs. We introduce a new method called Late Stopping that focuses on selecting high-probability mislabeled examples in the late training stages to retain as many CHEs as possible. We evaluate our method on benchmark-simulated and real-world noisy datasets, and the results demonstrate that Late Stopping outperforms existing LNL methods in terms of performance.