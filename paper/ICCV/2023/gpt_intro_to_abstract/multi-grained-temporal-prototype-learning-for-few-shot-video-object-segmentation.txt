The paper introduces the problem of data-hungry deep semantic segmentation models and proposes a solution called few-shot semantic segmentation, which requires only a few annotated support samples for segmenting objects of the same class in new images. The paper reviews recent works that have shown promising results using the meta-learning scheme on image data. Inspired by the few-shot image semantic segmentation task, the paper introduces the few-shot video object segmentation task and discusses the challenges of matching support-induced prototype guidance with query features. To address these challenges, the paper extends the IPMT model for video data and proposes a multi-grained temporal structure by decomposing the query video information into clip-level, frame-level, and memory-level prototypes. The paper also introduces an adaptive frame prototype and enables bidirectional communication between clip and frame prototypes. To leverage historical memory guidance, the paper trains an IoU regression network and proposes a method for predicting more accurate IoU scores. A cross-category discriminative segmentation loss is also proposed to make the prototypes more category discriminative. The effectiveness of the proposed VIPMT model is verified through extensive experimental results, showing significant performance improvements over state-of-the-art models on benchmark datasets.