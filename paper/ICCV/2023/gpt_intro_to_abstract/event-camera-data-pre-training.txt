This paper introduces the problem of event camera data pre-training and proposes a self-supervised framework for addressing this problem. Event cameras capture changes in brightness as a sequence of events, providing high dynamic range and temporal resolution compared to conventional RGB cameras. The paper highlights the domain gap between RGB images and event data and the challenges in replicating the success of pre-training with RGB images on event camera data. The authors propose a contrastive learning task formulation and a conditional masking strategy for sampling informative event patches. They also introduce an embedding projection loss and a probability distribution alignment loss to address the issues of model collapse and aligning embeddings from paired event and RGB images. The paper showcases the effectiveness of their approach through state-of-the-art performance on standard event benchmark datasets.