This paper introduces a novel approach to sound-driven video generation, leveraging audio inputs to guide the generation of visually appealing video frames. While existing methods primarily rely on text prompts, our approach utilizes sounds to provide sequential information and enhance the generation process. We address the limitations of previous sound-guided video generation approaches by incorporating temporal semantics from audio inputs, allowing our model to reactively manipulate video frames. Our model consists of an Audio Encoder module that encodes the temporal semantics of audio sequences and an Audio Semantic Guidance module that generates corresponding video frames based on the encoded latent vectors. We also employ regularizers to ensure temporal consistency and preserve identity throughout the generated video. By experimenting with a public dataset and comparing our results with other state-of-the-art approaches, we demonstrate the effectiveness of our proposed model in terms of video quality metrics and human evaluation.