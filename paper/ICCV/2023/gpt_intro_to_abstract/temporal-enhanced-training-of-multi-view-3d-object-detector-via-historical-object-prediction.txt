Camera-only 3D detection from multi-view images is a challenging task in the field of autonomous driving. Recent advancements have focused on using the Bird's-Eye View (BEV) representation with temporal feature aggregation as the superior design approach for camera-only 3D perception. Various detectors have explored methods for fusing temporal information to achieve breakthroughs in the detection of objects in 3D space. However, beyond these fusion mechanisms, this paper proposes a new paradigm for leveraging temporal information to enhance the temporal modeling ability of the 3D object detector. This approach, called Historical Object Prediction (HoP), is a temporal-based auxiliary task that is only adopted during the training stage. By enforcing the detector to capture both the spatial location and temporal motion of objects occurring at historical timestamps, more accurate BEV feature learning can be achieved. The proposed method includes the design of short-term and long-term temporal decoders, which generate pseudo BEV features and contribute to better object motion estimation. The results of extensive experiments conducted on the nuScenes dataset demonstrate the effectiveness of the HoP approach, outperforming existing 3D object detectors and establishing a new state-of-the-art performance. Overall, this paper introduces a novel training scheme and temporal modeling approach that showcases the potential of leveraging temporal information in camera-only 3D detection for autonomous driving.