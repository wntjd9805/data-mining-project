Deep neural networks (DNNs) have achieved remarkable performance in computer vision tasks. However, they are vulnerable to adversarial examples, which are images designed to deceive the DNN with imperceptible perturbations. Adversarial training (AT) has been widely used as a defense method, but it often results in a tradeoff between adversarial robustness and standard accuracy. Previous studies have attempted to mitigate this tradeoff, but the standard accuracy still degrades from the original accuracy of a standardly trained DNN. In this paper, we propose a novel method called Adversarial finetuning with Representation constraint (ARREST) to address this tradeoff. ARREST includes three key components: adversarial finetuning (AFT), representation-guided knowledge distillation (RGKD), and noisy replay (NR). AFT refines the DNN's representations of both clean and adversarial examples, while RGKD and NR preserve the representations of clean examples through techniques inspired by knowledge distillation. Experimental results demonstrate that ARREST effectively mitigates the tradeoff and achieves state-of-the-art performance. Our contributions include the introduction of ARREST, extensive experimental evaluations, and a novel quantitative evaluation metric inspired by BD-Rate.