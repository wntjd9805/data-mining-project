Deep convolutional neural networks (CNNs) have significantly advanced the performance in many tasks. However, larger models that perform better require more computing resources, while smaller models have lower computation complexity but are less competitive. To bridge this gap, knowledge distillation (KD) has been proposed, which utilizes the teacher's prediction logits as soft labels to guide the student. This paper aims to improve the utilization of soft labels for distillation loss and propose a general and effective method to obtain customized soft labels for self-KD. The proposed Normalized Knowledge Distillation (NKD) loss normalizes the non-target logits, using soft labels more effectively and significantly enhancing KD's performance. Additionally, a novel method called Universal Self-Knowledge Distillation (USKD) is introduced to obtain customized soft labels without a real teacher for self-KD. USKD achieves state-of-the-art performance on both CNN and ViT models with minimal additional time and resources. Extensive experiments on CIFAR-100 and ImageNet datasets confirm the effectiveness of NKD and USKD, with models trained using the self-KD method demonstrating efficacy on COCO for detection.