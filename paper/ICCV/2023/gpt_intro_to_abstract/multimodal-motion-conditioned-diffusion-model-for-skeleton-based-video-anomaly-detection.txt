Video Anomaly Detection (VAD) is a crucial task in computer vision and security applications. It enables early detection of unusual or abnormal events in videos. However, VAD is challenging due to the subjective definition of anomalies, data scarcity, and the open-set nature of anomaly detection. Current state-of-the-art techniques fail to address the diversity of anomalous and non-anomalous actions. In this paper, we propose Motion Conditioned Diffusion Anomaly Detection (MoCoDAD), a novel generative model for VAD that assumes both normality and abnormality are multimodal. MoCoDAD generates multimodal reconstructions of corrupted future frames conditioned on the clean past frames. It discerns normality from anomaly by comparing the multimodal distributions. We draw inspiration from Denoising Diffusion Probabilistic Models (DDPMs) and evaluate MoCoDAD on multiple benchmark datasets, achieving state-of-the-art performance. We summarize our contributions as a novel generative VAD model, the first diffusion-based approach for VAD, a motion-based conditioning strategy, and improved performance on benchmark datasets. Our model does not rely on appearance, providing increased privacy protection and computational efficiency.