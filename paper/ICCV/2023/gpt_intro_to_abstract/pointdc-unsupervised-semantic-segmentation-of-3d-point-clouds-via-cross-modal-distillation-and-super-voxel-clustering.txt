Semantic segmentation of 3D point cloud is a crucial problem in computer science. Existing fully-supervised schemes for 3D semantic segmentation rely on large-scale datasets and annotations, which require exhaustive efforts. To reduce the effort of semantic annotations, some works have proposed 3D semantic segmentation systems trained from weaker forms of annotations. However, few works attempt to handle the challenge of 3D semantic segmentation without any form of human annotations. In this paper, we propose an unsupervised 3D semantic segmentation framework, called PointDC, that can automatically excavate meaningful semantic features from point clouds. We integrate Cross-Modal Distillation (CMD) and Super-Voxel Clustering (SVC) in the PointDC framework to handle clustering ambiguity and irregularity ambiguity. CMD distills multi-view visual features to distill the learning of point representation, while SVC rasterizes the 3D space into super-voxels and assigns each point to the corresponding super-voxel. Our PointDC framework achieves superior improvement compared to existing unsupervised methods for 3D semantic segmentation on challenging datasets.