In recent years, pruning techniques have become popular for reducing the memory and time complexity of deep models. However, these techniques often overlook the issue of model fairness, which is important for the reliable application of AI systems. Existing pruning methods prioritize high accuracy but ignore the inherent biases present in compressed models. In this paper, we propose a fairness-aware pruning technique that improves fairness while preserving accuracy and efficiency. We formulate the pruning process as adversarial learning between fairness and performance, using a discriminator to remove the correlation between predictions and sensitive attributes. Our method achieves significant improvements in fairness while maintaining comparable accuracy and efficiency. Additionally, we uncover fair sub-networks within randomly initialized networks, suggesting that searching for the locations of a subset of weights can be as effective as adversarially debiasing weight values. Experimental results demonstrate the superiority of our method compared to state-of-the-art pruning techniques in terms of fairness and accuracy. This research opens up new perspectives on model fairness and its implications for deep learning.