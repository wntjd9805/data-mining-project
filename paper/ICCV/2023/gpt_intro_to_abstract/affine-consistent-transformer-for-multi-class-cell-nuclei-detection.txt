Pathologists rely on digital pathological images for diagnosing diseases. This involves grading tumors and classifying diseases by locating and identifying specific histological structures. In this paper, we focus on inferring the types and rough locations of cell nuclei in digital slide images. Previous methods for automatic nucleus detection and classification have lacked accuracy and generalization. Deep learning models have been successful in addressing these issues by learning robust representations. Existing deep learning methods for nuclei detection can be categorized into three groups: contour-based prediction, centroid-based prediction, and bounding box prediction. However, these methods have limitations and are affected by the quality of boundary annotations, confusion between adjacent nuclei, and unclear boundaries. To address these challenges, we propose a transformer-based framework that directly predicts a set of cell positions and categories. We also address the difficulties caused by image scale and nuclei density by splitting the framework into two branches - a local network and a global network. Additionally, we introduce an adaptive affine transformer that predicts affine transformation parameters to improve the training of the global-local network. Overall, our contributions include the introduction of an adaptive affine transformer, the proposal of an affine-consistent transformer framework for nuclei detection, and the demonstration of state-of-the-art performance on multiple benchmarks.