Depth perception is a critical aspect of various computer vision applications. The inference of depth from standard cameras is a widely utilized technique due to its cost-effectiveness and potential for high image resolution. However, determining visual correspondence, especially in textureless regions and repetitive patterns, poses challenges when matching points across frames. Although deep learning has shown promising results, it is susceptible to the domain shift issue, unlike conventional hand-crafted methods. Active sensing technologies such as LiDAR, ToF, and Radar offer alternative approaches to depth perception but also have inherent limitations. Active systems that estimate depth from images have higher accuracy and resolution but are restricted by the visibility of projected patterns and interference from multiple projectors. To overcome these limitations, a combination of active and passive technologies is commonly adopted in various application fields. This paper proposes a novel paradigm to leverage the synergy between active and passive sensing by coherently hallucinating the stereo pair acquired by a standard camera. This approach simplifies the visual correspondence task by virtually simulating the presence of a pattern projector. Unlike existing strategies, the proposed method does not rely on a specific physical pattern projector and can work in any environment, regardless of moving objects and camera ego-motion. Experimental results demonstrate that the proposed approach outperforms state-of-the-art sensor fusion methods and improves accuracy and domain shift tolerance for deep networks trained on synthetic data. Additionally, this method is effective under various lighting conditions, ranges, and without additional processing costs. The proposed approach, called Virtual Pattern Projection (VPP), has the potential to become a standard component for depth perception and drive future advancements in the field.