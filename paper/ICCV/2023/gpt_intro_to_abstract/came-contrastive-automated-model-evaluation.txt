The rapid advancements in artificial intelligence and machine learning have had a significant impact on various domains. Evaluating and benchmarking trained models is crucial for their successful deployment in real-world scenarios. Traditionally, model evaluation relied on pre-split and static testing sets that were separate from the training or validation phase. However, recent works have highlighted several limitations of this approach, such as careful sample selection, randomization, and expensive label annotation. Automated Model Evaluation (AutoEval) has emerged as a solution to these problems. AutoEval aims to estimate a model's performance on an unlabeled testing set by generating meta-sets through pre-selected data augmentations on the training set. The average distance between the meta-sets and the testing set is used to measure model performance. This setup mimics real-world scenarios where the testing set is acquired on the fly and cannot be annotated or persisted. Despite its promise, AutoEval faces challenges in real-world deployment, particularly on limited-capacity platforms like IoT or autonomous driving devices. To address this, we introduce our framework called CAME, which combines contrastive learning with normal task loss to train models. We empirically prove the correlation between contrastive loss on the testing set and model performance, demonstrating the effectiveness of AutoEval. CAME eliminates the need for the training set during the evaluation phase and achieves significantly better testing performance estimation compared to previous works.