Deep neural networks have shown outstanding performance in various applications, but their high computational requirements pose challenges for deployment on low-resource devices. To address this issue, recent research has explored multiplication-free neural networks that reduce memory footprint and energy consumption. Existing works include binary and ternary quantization techniques, which constrain weights to replace multiplication with less expensive operations. Shift networks, built upon a ternary base, replace multiplication with bit-wise shift operations, resulting in highly efficient hardware implementations. However, current Shift networks have limitations, such as supporting only quantized activations and exhibiting performance degradation under 2-bit weights. In this study, we propose DenseShift networks to overcome these limitations. Our analysis reveals that zero weights in low-bit Shift networks reduce model capacity, so we introduce a zero-free shifting mechanism to enhance model representation capacity. We also introduce a novel inference approach that supports both floating-point and quantized activations, accelerating dot-product computation. We propose an efficient training algorithm that enables training low-bit DenseShift networks from random initialization and a low-variance random initialization strategy to improve transfer learning performance. Extensive experiments demonstrate that DenseShift networks outperform state-of-the-art Shift networks on the ImageNet classification task and achieve comparable performance to full-precision networks while having higher inference computational efficiency. Moreover, our low-bit DenseShift networks achieve full-precision performance in transfer learning across different domains. This study is the first to demonstrate this capability.