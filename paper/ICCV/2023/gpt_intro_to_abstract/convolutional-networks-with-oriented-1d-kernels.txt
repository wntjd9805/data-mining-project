Convolutional Networks (ConvNets) have been widely used in computer vision tasks and have been the go-to design choice, relying heavily on 2D convolutions. In this work, we explore the possibility of using ConvNets consisting entirely of 1D convolutions instead. Surprisingly, we find that a ConvNet with 1D convolutions can perform as well as its 2D counterpart in ImageNet classification. We introduce the concept of oriented 1D kernels, which consist of 1D kernels oriented at arbitrary angles. By using oriented 1D kernels, we can approximate a wider range of 2D kernels while still maintaining the computational efficiency of 1D convolutions. Furthermore, oriented 1D kernels provide an alternative for modeling long-range dependencies at a lower cost compared to large 2D kernels. Implementing oriented 1D kernels efficiently on a GPU presents challenges due to memory access patterns. To address this, we present a highly-optimized custom CUDA implementation of oriented 1D kernels, specifically optimized for depthwise convolution. Our experiments demonstrate that the oriented 1D convolution achieves superior speed and minimal memory overhead compared to native horizontal 1D convolution. Using our custom implementation, we show that oriented 1D convolution can replace 2D convolution and enhance existing architectures with large kernels, resulting in improved accuracy. We provide our implementation as open-source code and anticipate its usefulness in further advancing neural architectures for computer vision tasks. Overall, our contributions include the finding that oriented 1D convolution can achieve state-of-the-art accuracy and the development of an efficient CUDA implementation for depthwise oriented 1D convolution.