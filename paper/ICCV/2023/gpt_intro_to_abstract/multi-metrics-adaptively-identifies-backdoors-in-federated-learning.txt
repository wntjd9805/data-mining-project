Federated learning (FL) is a distributed machine learning paradigm that allows multiple participants to collaboratively train a model without sharing their local data. FL has gained success in various fields, but it is vulnerable to backdoor attacks that manipulate the model's behavior on specific inputs. Previous defenses against backdoors have limitations, such as being easily bypassed or significantly decreasing performance. In this paper, we propose a distance-based defense that leverages multiple metrics to identify malicious gradients. We introduce the Manhattan distance, which is shown to be more meaningful in high-dimensional space than the Euclidean distance. Our defense adapts to different attacks and environments by applying a whitening transformation and generating dynamic weights. We evaluate our method on various datasets and demonstrate its effectiveness in maintaining both robustness and performance under stealthy backdoors. Our contributions include a novel defense applicable in a generic adversary model, the alleviation of the "meaningfulness" problem of Euclidean distance, and empirical evidence of the effectiveness of our approach.