Deep neural networks have proven to be highly effective in various computer vision tasks. However, traditional training methods for these networks are performed in an offline manner, requiring all data to be collected in advance. When new classes need to be incorporated, re-training the networks with updated data or direct fine-tuning on new data is necessary. The former approach is costly and may raise privacy concerns, while the latter is prone to catastrophic forgetting, where previously learned knowledge is forgotten. To address these issues and enable neural networks to learn new concepts while preserving old knowledge, continual learning (CL) has gained attention in recent years.CL has primarily been explored in image classification and semantic segmentation. Existing methods mainly focus on applying forgetting-preventing constraints to balance between rigidity (preserving old states) and plasticity (learning new knowledge). However, these methods still struggle to effectively learn new classes, achieving only around 70% of the upperbound performance on benchmarks like VOC2012. We identify two main obstacles in the CL training pipeline: the rigidity-plasticity dilemma and inadequate data for training.Instead of directly tackling the rigidity-plasticity dilemma, we propose exploring future knowledge in an unsupervised manner. By pre-learning partial knowledge of future classes, the network requires fewer updates to incorporate upcoming knowledge, reducing the impact of the dilemma. We also address the issue of inadequate training data by utilizing abundant data from previous steps to optimize discrimination between old and new classes. To achieve this, we design a novel framework that performs unsupervised contrastive learning on intermediate features, optimizing future classes based on visual similarity and feature affinity. Additionally, an auxiliary classifier is trained using pseudo labels generated via clustering to initialize the actual classifier for future steps.Our proposed approach surpasses existing methods significantly in terms of performance, as shown in Figure 1. Through extensive experiments, we validate the effectiveness of our framework, demonstrating a significant improvement in standard CL benchmarks. Our contributions include the proposal of pre-learning future knowledge for CL, the design of a novel framework for unsupervised learning, and the extensive validation of our approach's effectiveness.