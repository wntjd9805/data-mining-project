Accurate environment perception is essential for the development of autonomous driving. Currently, autonomous vehicles rely on LiDAR sensors for environment perception, but face challenges such as occlusion-induced blind spots and long-distance perception instability. To overcome these challenges, research has focused on utilizing both vehicle-side and infrastructure-side information for cooperative perception. However, transmitting this information from infrastructure-side equipment to autonomous vehicles requires communication bandwidth resources. Reducing communication bandwidth consumption while ensuring the accuracy of cooperative perception is crucial. Intermediate fusion methods offer a trade-off between bandwidth occupation and accuracy, but still fall short of industrial standards. Additionally, spatial alignment and domain gaps in the features pose challenges for feature fusion. In this paper, we propose TransIFF, a transformer-based instance-level feature fusion framework. TransIFF reduces bandwidth consumption by transmitting instance-level features and improves collaborative perception precision through domain alignment and robust feature fusion. The framework consists of a vehicle-side network, an infrastructure-side network, and a vehicle-infrastructure fusion network. The features from both sides are aligned and fused using different modules. TransIFF achieves a low bandwidth consumption while maintaining high collaborative perception precision. The main contributions of this work are the proposal of TransIFF, the introduction of a Cross-Domain Adaptation module for domain gap alignment, and the improvement of feature fusion robustness through various design enhancements. The performance of TransIFF is evaluated on the DAIR-V2X benchmark, achieving state-of-the-art results.