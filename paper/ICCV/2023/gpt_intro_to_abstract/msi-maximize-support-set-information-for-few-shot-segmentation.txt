Deep convolutional neural networks (DCNNs) have achieved impressive results in computer vision tasks, such as object detection and semantic segmentation. However, these models heavily rely on large-scale annotated datasets, which are expensive and time-consuming to obtain. Moreover, they struggle to accurately segment novel objects with only a few annotated examples. Several few-shot segmentation (FSS) approaches have been proposed to address this limitation. However, existing methods often fail in challenging scenarios, where the support mask is small, incomplete, or lacks important contextual information from the background. In this paper, we propose a novel method called masked support images (MSI) to overcome the limited information from the support mask. MSI maximizes the information from the masked support set to compensate for the limited support mask information and utilize important contextual information from the background. It combines features obtained from masked support images and full support images to accurately localize and segment the target object. Extensive experiments on challenging FSS benchmarks demonstrate the effectiveness of MSI, consistently improving performance over strong baselines. Additionally, MSI significantly speeds up the training of baseline models. Overall, our method presents an efficient and effective solution for few-shot segmentation tasks.