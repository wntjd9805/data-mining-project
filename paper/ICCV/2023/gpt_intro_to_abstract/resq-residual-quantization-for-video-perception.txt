Real-time video inference on resource-constrained devices remains a challenge despite advancements in optimizing deep neural networks. To accelerate video inference, temporal redundancy can be exploited. Previous methods used feature warping, adaptation using optical flow, or self-attention. This paper introduces Residual Quantization (ResQ), a novel quantization scheme for video perception. ResQ leverages the difference in network activations between adjacent frames (residuals), which exhibit properties that make them highly quantizable. By quantizing keyframes at a higher precision and residuals at a lower precision, ResQ combines high-precision details with complementary information for inference. Additionally, ResQ dynamically adjusts the quantization bit-width based on the range of residuals. The proposed scheme is evaluated on semantic segmentation, video object segmentation, and human pose estimation tasks, outperforming standard quantization schemes and achieving competitive efficiency compared to state-of-the-art methods. The contributions of this work include empirical evidence of the benefits of using frame residuals to reduce quantization error, the proposal of ResQ as a quantization scheme for video networks, the extension of the model to dynamically adjust quantization level based on residual content, and validation of the proposals on multiple perception tasks.