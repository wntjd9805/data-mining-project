This paper explores the concept of using text descriptions to summarize the essential information in an image, and how this can be applied to machine learning models for domain generalization. The authors propose a new approach called RISE (Regularized Invariance with Semantic Embeddings), which leverages a vision-language model, CLIP, to train the domain generalization models. RISE incorporates three loss functions, including knowledge distillation from a pre-trained CLIP teacher model. The paper introduces two designs of the cross-domain distance loss function, absolute distance and relative distance, to guide the student model's training process. The main contributions of the paper are leveraging knowledge distillation with a vision-language model for domain generalization, regularization of student representation through closer alignment with the teacher's text representation, and the proposal of two loss functions. Experimental results demonstrate the effectiveness of the RISE model on domain generalization benchmarks.