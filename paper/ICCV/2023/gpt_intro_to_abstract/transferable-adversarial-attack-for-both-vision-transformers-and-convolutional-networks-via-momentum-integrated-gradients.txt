Vision Transformers (ViTs) have gained popularity in computer vision tasks due to their impressive performance and are widely adopted. They process images as sequences of patches and use self-attention to model global dependencies among them. On the other hand, Convolutional Neural Networks (CNNs) exploit spatial structures of images using convolutional filters to capture local features. Adversarial attacks, where a small imperceptible perturbation is applied to an image to break the prediction of a machine learning model, are a concern for both ViTs and CNNs. Transferability of adversarial examples, where perturbations generated on a white-box model can be transferred to attack a black-box model, plays a critical role in adversarial attacks. However, previous attack methods designed for CNNs show limited transferability when applied to ViTs, leaving obstacles for attacking black-box ViTs. In this paper, we propose a novel attack method called Momentum Integrated Gradients (MIG) attack, which successfully attacks both ViTs and CNNs and exhibits better transferability compared to previous methods. MIG utilizes Integrated Gradients (IG) to compute the saliency score of a model's prediction with respect to the input and uses the sign of integrated gradients to guide the perturbation updates. Additionally, MIG incorporates a momentum-based iterative strategy to accumulate the impact of historical gradients and find globally optimal perturbations. Experimental results on the ImageNet validation dataset demonstrate that MIG significantly enhances the transferability of adversarial examples across ViTs and CNNs, achieving state-of-the-art attack success rates under a small perturbation budget. Our contributions include the development of the MIG attack method, the demonstration of its high transfer attack success rate on ViTs, and the empirical validation of attribution-based transfer attacks for ViTs, indicating a connection between model interpretability and robustness against attacks.