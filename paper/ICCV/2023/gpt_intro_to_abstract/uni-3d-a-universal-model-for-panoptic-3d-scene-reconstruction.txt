Our paper introduces a universal model called Uni-3D for 3D scene understanding, which integrates 3D instance and semantic segmentation into a panoptic framework. We propose a depth-aware panoptic segmentation architecture based on Transformers, which combines all 2D predictions in a unified paradigm. This allows us to learn multiple properties of the input scene, such as layout, instance silhouettes, category labels, and depth. Additionally, we develop a query-based architecture for the 3D part, where 3D segments of both object instances and stuff layout are predicted individually with the guidance of corresponding 2D queries. This query-based design seamlessly integrates learned 2D features and priors into 3D, offering more flexibility and robustness. Our Uni-3D model outperforms previous methods in both quantitative and qualitative evaluations.