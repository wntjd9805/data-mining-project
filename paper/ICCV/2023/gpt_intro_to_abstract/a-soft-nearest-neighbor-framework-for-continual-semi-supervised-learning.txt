This paper addresses the challenge of continual learning (CL) in the context of semi-supervised learning, where not all data samples are labeled. Most existing CL methods assume fully labeled data, which is often unrealistic due to cost, privacy concerns, or real-time scenarios. To overcome this limitation, the authors propose a nearest-neighbor-based method called NNCSL (Nearest-Neighbor for Continual Semi-supervised Learning). NNCSL leverages the nearest-neighbor classifier to learn powerful and stable representations of the current task while distilling previous knowledge and transferring the local structure of the feature space. They introduce a novel semi-supervised distillation loss called NND (Nearest-Neighbor Distillation) that mitigates forgetting in continual semi-supervised learning. NNCSL outperforms existing methods on small and large-scale datasets, both for low and high-resolution images. The proposed method achieves superior performance with significantly less supervision on CIFAR100. The main contributions of this work are the NNCSL method, the NND distillation strategy, and the new state-of-the-art results in continual semi-supervised learning.