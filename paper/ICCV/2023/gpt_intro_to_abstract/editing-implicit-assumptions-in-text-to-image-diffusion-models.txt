Text-to-image generative models have gained significant attention in recent years, achieving remarkable success in generating high-quality images based on textual prompts. These models, trained on large datasets of image-caption pairs, acquire implicit assumptions about the world that manifest during image generation. In many cases, there is a need to edit these assumptions without requiring explicit user input. This paper introduces Text-to-Image Model Editing (TIME), a method that aims to edit the implicit assumptions of text-to-image models by modifying the weights of the model's cross-attention layers. This method is efficient, does not require retraining or fine-tuning, and preserves the generative capabilities of the model. The paper also presents the Text-to-Image Model Editing Dataset (TIMED) for evaluating model editing quality. TIME demonstrates impressive editing results, generalizing for related prompts while leaving unrelated ones intact. The method is further applied for social bias mitigation, successfully reducing gender bias in the model's generated images for various professions. TIME is the first proposed method for model editing in text-to-image models and provides insights and datasets to support future advances in this area.