The popularity of depth sensors and laser scanners has led to the widespread use of point clouds in various fields such as robotics, autonomous driving, and augmented reality. Point clouds have advantages over other representations in terms of precision and scalability. They have been extensively used for classification and segmentation tasks but have recently gained attention for point cloud synthesis in 3D content creation. However, existing methods have limitations in terms of dataset size and control over the generated shapes. In this paper, we propose a novel approach inspired by generative methods that use diffusion processes to perturb samples. Our approach combines denoising diffusion models with deep networks to generate point clouds conditioned on images, allowing for better control and contextual information. Unlike previous works, we employ a geometrically-principled method of projecting the point cloud into an image and sampling sparse features for conditioning. Our method produces geometrically and semantically consistent 3D objects and can generate hypotheses for occluded regions. We present our contributions of a permutation-equivariant Set Transformer framework, augmented with geometrically-principled conditioning, and demonstrate its performance on unconditional synthesis and point cloud generation from images. Our method shows promising applications in practical scenarios such as 3D content creation, automotive or robotics applications, and single-view 3D reconstruction.