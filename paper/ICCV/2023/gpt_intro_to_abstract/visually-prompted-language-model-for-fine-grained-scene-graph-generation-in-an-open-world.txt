In this paper, we propose a novel framework called Cross-modal prediCate boosting (CaCao) that leverages pre-trained language models to enrich the tail predicates of scene graphs. We address the challenges of modality gap and semantic co-reference by introducing a visually-prompted language model and an adaptive semantic cluster loss. We also present a fine-grained predicate-boosting strategy to extend the dataset with informative predicates. Our CaCao framework improves the performance of state-of-the-art models in scene graph generation tasks. Additionally, we propose an Entangled cross-modal prompt approach for open-world predicate scene graph generation (Epic) to explore the generalizability of CaCao. Surprisingly, without ground-truth annotations, Epic achieves competitive performance on the open-world predicate learning problem. Our contributions include the development of the CaCao framework, its applicability to existing models, and the introduction of Epic for unseen predicates.