Our paper introduces a model called PDiscoNet that aims to improve the interpretability and trustworthiness of deep learning models. Traditional approaches generate saliency maps to identify important image regions, but they provide limited information when the model gives an incorrect answer despite attending to the correct regions. To address this, some approaches modify the model architecture to ensure that the explanation reflects the decision process. Our approach enriches saliency map explanations by dividing them into semantically interpretable parts, mimicking fine-grained visual categorization approaches. By classifying images based on a few discriminative and semantically consistent regions, our model becomes more interpretable as we can easily assign semantic meaning to each part and determine whether the correct parts are being detected. This way of interpreting models helps improve their trustworthiness by ensuring that only information from the indicated regions is used. Furthermore, such models are more robust as irrelevant parts are filtered out, leading to better generalization and robustness to occlusion and adversarial attacks. To discover these meaningful and discriminative parts using only image-level class labels, we introduce additional priors along with a model architecture that incorporates these priors. Our PDiscoNet model, based on a CNN backbone, discovers parts and uses them as a bottleneck for fine-grained classification. The discovered parts are used to extract class logits independently and are combined for the final classification. Additionally, a dropout layer affects whole parts at a time, ensuring all discovered parts are relevant to classification.