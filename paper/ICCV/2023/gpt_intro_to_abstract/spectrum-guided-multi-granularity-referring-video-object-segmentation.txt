The paper introduces the concept of referring video object segmentation (R-VOS), which aims to segment objects in a video based on linguistic descriptions. R-VOS is a challenging task that requires cross-modal understanding between vision and language. Early R-VOS techniques have used feature encoding, cross-modal interaction, and language grounding using convolutional neural networks (CNNs), but their performance is limited due to CNNs' inability to capture long-range dependencies and handle free-form features. Recent advancements in attention mechanisms and transformers have improved R-VOS performance. However, these methods still have limitations such as feature drift, decreased segmentation performance due to upsampling, and inefficiency in handling multiple referred objects in a video. To address these challenges, the paper proposes a Spectrum-guided Multi-granularity (SgMg) approach. SgMg follows a segment-and-optimize pipeline and introduces Conditional Patch Kernel (CPK) to directly segment encoded features, avoiding feature drift. It also employs Multi-granularity Segmentation Optimizer (MSO) to refine segmentation using low-level visual details. Spectrum-guided Cross-modal Fusion (SCF) is introduced to facilitate multimodal understanding through intra-frame global interactions in the spectral domain. The paper also extends SgMg to enable multi-object R-VOS, which allows simultaneous segmentation of multiple referred objects in a video. The proposed method achieves top-ranked overall performance on multiple benchmark datasets, surpassing existing methods. Experimental results demonstrate the effectiveness of the SgMg approach on various datasets, including Ref-YouTube-VOS, Ref-mark, DAVIS17, A2D-Sentences, and JHMDB-Sentences. SgMg achieves state-of-the-art performance on all four datasets, outperforming the closest competitor ReferFormer. Overall, the paper presents a comprehensive solution to the challenges in R-VOS and introduces new paradigms for improved segmentation of objects in videos.