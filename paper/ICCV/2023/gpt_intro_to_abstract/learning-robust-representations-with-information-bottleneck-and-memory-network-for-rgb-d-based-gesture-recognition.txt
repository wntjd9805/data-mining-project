Gesture recognition based on RGB-D video has gained significant attention in various applications, including visual surveillance, intelligent transportation, and human-computer interaction (HCI). Despite the progress made in network structures and the introduction of additional data modalities, the interference of gesture-irrelevant factors has been largely overlooked. In real-world gesture recognition scenarios, there are dynamic variations related to the motion of the gesture itself and environmental influences, such as illumination, backgrounds, and performers' appearances. These factors can hinder recognition performance by introducing inner-class differences. To address this issue, we propose a framework based on the Information Bottleneck (IB) principle to disentangle the recognition-relevant and redundant information and refine the feature representation. We leverage a memory network to store and overlay the shared predictive information from different samples and highlight the gesture-relevant predictive feature. Our approach achieves state-of-the-art performance on three public RGB-D gesture datasets and provides theoretical insights from an information-theoretic perspective. This work contributes to the development of a robust feature representation for gesture recognition and explicitly distills the predictive information, ultimately improving recognition performance.