This paper introduces a new large-scale outdoor scene dataset for advancing research in neural radiance fields (NeRF). Existing methods in NeRF focus mainly on single objects or indoor scenes, and their performance decreases when applied to outdoor scenes. However, there is a lack of large-scale outdoor datasets and uniform benchmarks to compare the performance of recent methods designed for large scenes. The existing outdoor datasets are either collected on a small geographical scale or rendered from virtual scenes, which deviate from real scenes in terms of texture and appearance details. This paper addresses this issue by introducing a well-selected fly-view multi-modal dataset that contains 33 scenes with extensive sources, including those collected on the Internet and captured by drones. The dataset includes prompt annotations, tags, and 14K calibrated images, and it covers various scene types, scales, camera trajectories, lighting conditions, and multi-modal data. A generic pipeline to generate real-world NeRF-based data from drone videos on the Internet is also provided, making the dataset easily extensible. All-around benchmarks, including novel view synthesis, scene representations, and multi-modal synthesis, are built based on the dataset to evaluate the applicability and performance of mainstream NeRF methods. The paper aims to establish a general benchmark for large-scale outdoor NeRF research and promote algorithm advancements in novel view synthesis, surface reconstruction, and multi-modal synthesis. The main contributions of the paper include the introduction of a new outdoor scene dataset that surpasses existing datasets in terms of quantity and diversity, the creation of benchmark tasks for mainstream NeRF methods, and the provision of a cost-effective pipeline for converting Internet videos into NeRF-purpose training data.