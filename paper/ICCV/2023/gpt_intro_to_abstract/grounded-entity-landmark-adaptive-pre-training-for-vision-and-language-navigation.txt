Vision-and-Language Navigation (VLN) is a task in the Embodied Vision community that involves guiding an agent to a target location based on natural language instructions. Cross-modal alignment, which matches landmarks mentioned in the instructions with visual observations, is crucial for accurate action decision-making. However, current datasets only provide coarse-grained alignment signals, limiting the agent's ability to learn fine-grained alignment. Previous works have focused on global alignment of instructions and trajectories, while others have attempted to align sub-instructions and sub-trajectories locally. To address these limitations, we introduce the Grounded Entity-Landmark R2R dataset (GEL-R2R), which provides high-quality grounded entity-landmark annotations for fine-grained cross-modal alignment. We also propose the Grounded Entity-Landmark Adaptive (GELA) pre-training paradigm, which includes three adaptive pre-training objectives to improve entity-landmark level alignment. We evaluate our dataset and pre-training methods on two downstream tasks, Room-to-Room (R2R) and Vision-and-Dialog Navigation (CVDN), and achieve state-of-the-art performance. Our contributions include the construction of the GEL-R2R dataset, the proposal of the GELA pre-training paradigm, and the demonstration of its effectiveness in VLN tasks.