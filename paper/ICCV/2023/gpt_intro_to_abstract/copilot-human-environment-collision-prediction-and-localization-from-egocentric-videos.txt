The paper introduces the problem of forecasting potential collisions between humans and their environment from egocentric observations. The ability to predict and prevent collisions is crucial for applications such as virtual reality, augmented reality, and wearable assistive robotics. Existing approaches for collision prediction in virtual reality rely on pre-scanned scenes or external tracking systems, while approaches for assistive exoskeletons often make simplified assumptions. To address these shortcomings, the paper presents a novel model called COPILOT (COllision PredIction and LOcalization Transformer) that provides detailed perception outputs for collision avoidance. COPILOT utilizes a 4D attention scheme that incorporates space, time, and viewpoint information from multi-view video inputs. The model classifies collisions and estimates collision regions simultaneously, improving performance and providing actionable information. To train and evaluate COPILOT, the paper introduces a data framework and curates a large-scale synthetic dataset with annotated collision labels and heatmaps. The dataset features diverse scenes, realistic human motion, and accurate collision checking. Experiments show that COPILOT achieves over 84% collision prediction accuracy on unseen synthetic scenes.The paper contributes by introducing the challenging task of collision prediction and localization from unposed egocentric videos, proposing a multi-view video transformer-based model, and developing a synthetic data framework to train the model. The model is shown to generalize well to diverse synthetic and real-world environments.