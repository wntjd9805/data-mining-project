Federated learning (FL) enables collaborative learning across clients without explicitly sharing their local data. In FL, a server updates its global model by aggregating the local gradients obtained from clients' training on their datasets. However, a limitation in realistic FL scenarios is the scarcity of labels. This scarcity of labels can lead to severe performance degradation. A naive approach to tackle this label scarcity is using standard semi-supervised learning (SSL) methods. However, with limited labels, there can be a difference in the distribution between labeled and unlabeled data, which we refer to as class distribution mismatch. Leveraging only the local knowledge of a client may not be enough to fully utilize its unlabeled data. On the other hand, using only the global knowledge for leveraging unlabeled data may also not be useful due to data heterogeneity. Therefore, to utilize FL where clients have access to both local data and the global model, we propose FEDLABEL, a selective knowledge assimilation method where each client chooses between its local and global model to pseudo-label its unlabeled data based on each model's confidence score. We also propose global-local consistency regularization to fully utilize both local and global models when both have useful knowledge of the unlabeled data. Compared to related work, FEDLABEL does not require additional computed experts or label distributions matching between server and clients. It achieves improved test accuracy in various scenarios and outperforms other semi-supervised federated learning baselines.