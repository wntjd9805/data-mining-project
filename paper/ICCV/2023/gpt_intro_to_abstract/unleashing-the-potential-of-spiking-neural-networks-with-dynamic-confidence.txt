Deep artificial neural networks (ANNs) have achieved remarkable success in computer vision tasks, but their increasing model complexity and depth result in higher latency and energy cost, posing challenges for AI applications on edge devices. Deep spiking neural networks (SNNs) offer potential advantages in power and latency during inference, but strict constraints on power and latency lead to significant accuracy degradation. The field of low-latency SNNs focuses on achieving equivalent or slightly lower accuracy than ANNs while reducing inference time steps. This paper introduces a runtime optimization method called Dynamic Confidence, which decodes confidence values from SNN inference results and utilizes them to dynamically guide inference, without sacrificing accuracy or increasing firing rates. Dynamic Confidence reduces inference latency by 40% on average for state-of-the-art low-latency SNNs on CIFAR-10 and ImageNet, leading to reduced power consumption. The paper also provides a method to calculate the upper-bound performance of SNNs with dynamic strategies, offering insights into the untapped potential of SNNs and the advantages they offer compared to ANNs. Dynamic Confidence is a lightweight, parameter-insensitive method that can be easily applied to various low-latency SNN algorithms. The paper addresses the gap in research on dynamic strategies in SNNs and evaluates their potential for speed and power gains on datasets like CIFAR-10 and ImageNet. Furthermore, it explores the application of confidence, a key concept in Bayesian neural networks, to SNNs and demonstrates its effectiveness in reducing inference latency and power consumption.