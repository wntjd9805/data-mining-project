This paper introduces a Bayesian prompt learning approach for image-language models in order to improve generalization for unseen prompts. The authors propose a variational inference framework that injects noise during prompt learning and induces a regularization term to encourage the model to learn informative prompts. By modeling the input prompt space as a probabilistic distribution, the proposed approach is compatible with both unconditional and conditional prompt learning approaches. The authors empirically demonstrate the effectiveness of Bayesian prompt learning on 15 benchmarks, showing improved generalization and prevention of spurious features. The proposed approach provides better coverage of the prompt space and exploits transferable invariant features, leading to improved performance across different datasets and domains.