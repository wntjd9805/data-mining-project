Face recognition (FR) has made significant progress in recent years through the use of large-scale training datasets, resource-intensive networks, and effective loss functions. However, these advancements often require substantial computational resources, making them impractical for mobile and embedded devices. This has led to a growing need for the development of lightweight and effective FR models for resource-limited scenarios. One popular approach for compressing models is knowledge distillation, which transfers knowledge from a teacher model to a student model. While existing knowledge distillation methods mainly focus on guiding the student to mimic the teacher's behavior using probability constraints, this approach may not be suitable for FR, where performance is typically evaluated in an open-set setting. In FR, the similarities of feature embeddings are used for recognition instead of probability values. Therefore, it is important to improve the discriminative ability of the student model's feature embedding. This paper proposes a new FR distillation framework called ICD-Face, which combines Feature Consistency Distillation (FCD) with Intra-class Compactness Distillation (ICD) to improve the similarity distribution consistency between teacher and student models. Specifically, FCD aligns the embedding space of the student model with the teacher model, while ICD generates sufficient positive pairs to estimate accurate similarity distributions and then aligns the similarity distributions of positive pairs between the models. Experimental results on benchmark datasets demonstrate the effectiveness and generalization ability of ICD-Face.