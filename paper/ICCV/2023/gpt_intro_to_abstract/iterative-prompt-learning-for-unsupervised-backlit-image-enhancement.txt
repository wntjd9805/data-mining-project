Backlit images, captured when the primary light source is behind objects, often have imbalanced illuminance distribution, which affects visual quality and perception algorithms. Existing light enhancement methods struggle with backlit images due to over- or under-enhancement. Unsupervised methods have limited robustness and generalization capabilities, while conventional exposure correction methods struggle with diverse backlit scenes. In this work, we propose an unsupervised method that leverages the Contrastive Language-Image Pre-Training (CLIP) model for backlit image enhancement. We tailor CLIP using prompt initialization, CLIP-aware enhancement training, and prompt refinement to effectively distinguish between backlit and well-lit images. Our approach surpasses state-of-the-art methods in both qualitative and quantitative metrics without requiring paired training data. We demonstrate the generalization capability and robustness of our method.