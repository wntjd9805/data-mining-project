Stylizing a visual world has become a popular and demanding task in various applications such as games, movies, and extended reality. One specific problem in this domain is 3D style transfer, which involves navigating in a virtual world that resembles the painting styles of renowned artists. Traditionally, 3D style transfer has been achieved through post-processing techniques in computer graphics pipelines. However, recent advancements in 3D deep learning, specifically neural radiance fields, open up new possibilities for implicit representation of 3D scenes using neural networks trained with multi-view images and differentiable volume rendering. In this paper, our goal is to transfer the appearance from a reference style image to a neural radiance field, while maintaining consistency across novel views. Existing works have attempted to adapt style transfer to neural radiance fields, but have been limited in terms of diversity and controllability of stylization results. To address these limitations, we propose a new style transfer method that considers local transfer between the reference style image and the radiance field rendering. Our method treats style transfer as a post-processing step, preserving the original geometry implicitly represented by the neural radiance field. We introduce a new backbone architecture that learns the density and appearance fields separately, and propose a hash-grid encoding scheme to support multiple styles in a single parametric embedding. Additionally, we propose a segmentation-based stylization loss that subdivides the 3D scene and style image into subregions and matches them using the Hungarian algorithm. Our method automatically generates high-quality stylization results with diverse local styles, and allows for manual editing by the user. The contributions of this work include a reference-guided style transfer method for neural radiance fields, an extended hash-encoding scheme for stylization, and a new style loss using bipartite matching on segmented regions.