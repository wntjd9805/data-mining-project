Interpreting high-level semantic concepts in computer vision, specifically in the task of semantic segmentation, has seen significant progress since the introduction of fully convolution networks (FCNs) and fully attention networks (Transformer). However, current segmentation systems lack a deeper understanding of human cognition. Humans interpret scenes by recognizing components, while standard systems assume no underlying relations between semantic concepts and predict them exclusively. Additionally, existing semantic segmentation systems heavily rely on sub-symbolic learning, neglecting explicit reasoning with relations among entities using symbolic logic representations. This paper proposes LOGICSEG, a structured visual parser, that combines neural computing and symbolic logic in a neural-symbolic framework for holistic visual semantic learning and reasoning. LOGICSEG uses first-order logic to specify relations among semantic classes, and during training, converts logical constraints into differentiable loss functions. During inference, the logical constraints are integrated into an iterative process to ensure compliance with the specified symbolic knowledge. This approach blends statistical learning with symbolic reasoning, improves performance, and ensures parsing behavior aligns with symbolic knowledge. Moreover, this study contributes to the field of neural-symbolic computing and showcases the potential of integrating symbolic reasoning and sub-symbolic learning in visual semantic parsing. LOGICSEG is compatible with existing segmentation network architectures and achieves solid performance gains across multiple datasets and scenarios. Its strong generalization and promising performance highlight the potential of integrating symbolic reasoning and sub-symbolic learning in machine perception.