Deep neural networks (DNNs) have demonstrated exceptional performance in various vision tasks such as image classification, object detection, and semantic segmentation. However, their large model size and high computational requirements hinder their deployment on resource-constrained mobile devices. Network binarization, which quantizes weights and activations into 1-bit precision, offers an appealing optimization to minimize computation and memory overhead. Binary Neural Networks (BNNs) can achieve a significant reduction in memory requirement compared to their 32-bit counterparts. Nevertheless, BNNs often suffer from accuracy degradation due to aggressive data quantization.To address this challenge, numerous techniques have been proposed to mitigate accuracy degradation in BNNs, focusing on weight binarization, shortcut connections, and threshold optimization. In BNNs, the threshold for activation quantization plays a critical role. Previous approaches used fixed thresholds determined through backpropagation, resulting in suboptimal results due to the large distortion and fluctuation of per-instance activations. To overcome this limitation, we propose INSTA-BNN, a novel approach that dynamically calculates thresholds based on instance-wise information, such as mean, variance, and skewness. This instance-aware thresholding significantly improves the quality of binary features and boosts the accuracy of BNNs.Additionally, we introduce a variant of the Squeeze-and-Excitation (SE) module with instance-wise adaptation as an optional component to further enhance accuracy with additional parameters. We evaluate the proposed modules extensively on a real device using a large-scale dataset and provide practical guidelines for network design that maximize the benefits of INSTA-BNN while minimizing overhead. With these guidelines, INSTA-BNN achieves higher accuracy compared to previous works, while maintaining similar parameter count, computational complexity, and latency. We claim that INSTA-BNN is an attractive option for BNN design.