Novel view synthesis, the generation of images at new views given a sparse set of observed images, is a challenging problem in computer graphics and vision. Traditional methods, such as Neural Radiance Fields (NeRF), have shown success in learning to represent 3D scenes with implicit volumetric representation. However, these methods are limited in their ability to reproduce fine-grained details at ultra-high resolutions, such as 4K. This is due to the mismatch between the high-resolution inputs and the low-dimensional volumetric representation. Increasing the number of samples to generate higher-resolution renderings is impractical due to the long rendering time. To address this, some fast rendering methods have been proposed, but they require a large memory cost for storing dense representations of the scene. In this paper, we introduce UHDNeRF, a novel NeRF-based framework that supports ultra-high-resolution view synthesis. We achieve this by combining implicit neural radiance fields with a sparse point cloud representation. The sparse point cloud is adjusted to have more points at high-frequency regions and less at low-frequency areas, reducing memory overhead. Our framework uses a frequency separation strategy, with two branches for capturing different frequency components of the scene. We introduce global and local features to better leverage the high-frequency information from the point cloud and a patch-based ray sampling strategy to reduce computational cost. Our contributions include an adaptive implicit-explicit scene representation for ultra-high-definition rendering, a two-branch configuration to reduce memory and computational costs, and an efficient exploration of high-frequency information with global and local features and patch-based sampling.