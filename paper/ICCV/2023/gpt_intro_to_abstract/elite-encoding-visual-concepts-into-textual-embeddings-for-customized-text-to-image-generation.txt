Large-scale diffusion models in text-to-image generation have shown significant improvements in semantic understanding and the generation of diverse and realistic images. These models have been widely applied in various tasks, such as image editing, data augmentation, and artistic creation. However, there is a need for personalized and customized text-to-image generation to create instances with user-defined concepts. Several studies have been conducted in this area, but computational efficiency remains a challenge. Existing methods require several minutes to learn a single concept, which is not feasible for online applications. To address this challenge, we propose a learning-based encoder called ELITE (Encoding visuaL concepts Into Textual Embeddings). ELITE utilizes a pre-trained CLIP image encoder for feature extraction and employs a global mapping network and a local mapping network to encode visual concepts into textual embeddings. The global mapping network maps CLIP image features into the textual word embedding space, while the local mapping network focuses on local details in the image. Our experiments show that ELITE enables efficient and accurate customized text-to-image generation while maintaining control and editing abilities. The contributions of this work include the introduction of ELITE, a fast and accurate encoder for customized text-to-image generation, and the use of multi-layer features in the mapping networks to improve editability and consistency of details. Experimental results demonstrate the faithful recovery of target concepts with higher visual fidelity and robust editing capabilities.