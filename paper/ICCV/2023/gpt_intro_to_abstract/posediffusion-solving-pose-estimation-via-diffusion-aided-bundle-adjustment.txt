Camera pose estimation is a fundamental problem in Computer Vision that involves extracting intrinsic and extrinsic camera parameters from a set of multi-view images. It has various applications, including augmented and virtual reality, and has recently gained renewed attention from the research community due to advancements in novel-view synthesis methods. Traditionally, dense pose estimation is performed using a Structure-from-Motion (SfM) framework, which estimates camera poses and reconstructs the 3D shape of the scene. Despite incorporating deep learning advances, the SfM pipeline has remained largely unchanged. Sparse pose estimation, which operates on a smaller number of views, has been a challenge for traditional methods. Recently, a deep learning-based approach called RelPose showed promising results in the sparse-view setting but was limited in accuracy and scope. In this paper, we propose PoseDiffusion, a new camera pose estimation method that combines deep learning with correspondence-based constraints to achieve high accuracy in both sparse-view and dense-view scenarios. PoseDiffusion utilizes a diffusion framework to model the probability of camera parameters given observed images and leverages the success of diffusion models in modeling complex distributions. It also incorporates traditional epipolar constraints to guide the sampling process and improve precision. Experimental results demonstrate that PoseDiffusion achieves state-of-the-art accuracy on various scenes and outperforms SfM methods when used for NeRF training, highlighting its superior accuracy in both extrinsic and intrinsic estimation.