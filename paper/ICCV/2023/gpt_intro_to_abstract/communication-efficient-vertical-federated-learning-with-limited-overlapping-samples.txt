Federated Learning (FL) is a distributed learning method that allows multiple parties to train a model without sharing their raw data. FL has gained attention due to its ability to build real-world applications while preserving data governance and privacy. Existing approaches focus on horizontal federated learning (HFL), where clients have different data distributions but share the same feature space. Another approach is vertical federated learning (VFL), where clients have different feature spaces but share overlapping samples. VFL faces challenges of high communication costs and limited overlapping samples. In this paper, we propose one-shot VFL, a communication-efficient algorithm that achieves high performance with minimal overlapping samples. We also propose few-shot VFL, which expands the supervised dataset on clients to improve performance. Experimental results show that our proposed algorithms outperform existing methods in terms of communication cost and performance. Our contributions include the introduction of one-shot VFL and few-shot VFL as solutions to the challenges in VFL.