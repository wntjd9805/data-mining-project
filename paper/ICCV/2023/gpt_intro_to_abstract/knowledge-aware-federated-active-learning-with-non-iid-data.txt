Federated learning is a decentralized approach that allows local devices to collaboratively learn and create a powerful global model without accessing local data. However, the high cost of data annotation poses a challenge for real-world federated learning scenarios. In this paper, we propose a new federated active learning paradigm that aims to protect data privacy and maximize the limited annotation budget on each local client. We introduce Knowledge-Aware Federated Active Learning (KAFAL), which includes Knowledge-Specialized Active Sampling (KSAS) and Knowledge-Compensatory Federated Update (KCFU). KSAS is a new active sampling strategy that intensifies specialized knowledge to annotate universally informative data. KCFU addresses the challenges caused by insufficient labeled training data and statistical heterogeneity by compensating for weak classes through knowledge distillation. We conduct extensive experiments to demonstrate the superiority of KAFAL and provide comprehensive ablation studies to validate its design.