Mixed reality technology has enabled new forms of communication and interaction between people. One important aspect of this technology is generating realistic and believable avatar motion, which greatly impacts the quality of immersive experiences. However, generating full-body avatar motion based on head-mounted display (HMD) signals remains a challenge. Many existing solutions only focus on generating the upper body motion of avatars, while neglecting the lower body. Previous works have attempted to generate full body avatar motion using various types of observations, such as images, 2D joints/keypoints, markers, and IMUs. However, these observations are considered sparse or partial and do not provide as rich of an input signal as the head and hand 6-DoF information from a typical HMD. More recent advancements have been made in generating full body motion using only HMD signals, but these methods rely on the full visibility of both hands, which is not always the case in mixed reality experiences that involve hand tracking instead of motion controllers. Hand tracking introduces challenges such as partial hand visibility due to the restricted field of view (FoV) of the HMD sensors. In this paper, we propose a solution called HMD-NeMo, which is a neural motion model that generates full body avatar motion in real time regardless of whether hands are fully or partially observed or not observed at all. Our approach is built upon recurrent neural networks and a transformer model that captures temporal information and complex relations between different components of the input signal. The key component of our approach is the Temporally Adaptable Mask Tokens (TAMT) module, which allows us to handle missing hand observations. Our contributions include being the first approach capable of generating accurate and plausible full body motions with full or partial hand visibility, and the introduction of the TAMT module. We demonstrate the effectiveness of our method through extensive experiments and achieve state-of-the-art performance on the challenging AMASS dataset. Overall, our method enables more immersive mixed reality experiences with fewer limitations on the hardware.