This paper addresses the problem of representing 3D objects using keypoints extracted from point cloud data. The accuracy and reliability of these keypoints are crucial for various geometrical reasoning tasks. Previous supervised learning methods have shown impressive results but require large annotated datasets, which can be difficult and time-consuming to create. In response, recent approaches have focused on self-supervision and unsupervised learning methods. These methods allow for generalization but have limitations in identifying specific keypoints in the presence of intra-class variations. To overcome these limitations, the proposed approach utilizes the input point cloud data to learn to produce 3D keypoints on the object's surface. The network optimizes the keypoints to promote non-overlapping and complete coverage of the object. It also ensures semantic coherence and consistency by comparing keypoints between different versions of the input data. Experimental results show that the proposed approach outperforms state-of-the-art methods in terms of coverage and semantic consistency and is able to generalize to novel object poses. The paper concludes with an overview of recent approaches, a description of the proposed approach, evaluation results, ablations, and final conclusions.