The paper introduces the concept of Vision-Language Pre-training (VLP) which aims to pretrain a model to understand and align images and texts through various pretraining tasks. The authors propose VL-Match, a model that enhances vision-language matching at both token level and instance level. They introduce two novel objectives: Vision-Language Replaced Token Detection (VL-RTD) and Fine-Grained Image-Text Matching (FG-ITM). VL-RTD uses a generator-discriminator structure to discriminate whether each token in the text aligns with the image and text context. FG-ITM enhances the Image-Text Matching task by introducing fine-grained negative samples. The authors also introduce a data augmentation method called NegGen to synthesize negative samples. Experimental results show that VL-Match outperforms previous state-of-the-art models on multiple cross-modal downstream tasks.