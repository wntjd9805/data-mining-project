Advances in Machine Learning (ML) and Computer Vision have demonstrated the capabilities of deep neural network-based (NN) models to learn from diverse data and perform a multitude of tasks with high accuracy. However, in industrial applications where ML is expected to be increasingly utilized, retraining models on new data streams can lead to catastrophic forgetting, as NN models tend to forget information learned from older data. Iteratively retraining the model from scratch on the entire dataset is not feasible, leading to the need for Continual Learning.Incremental Learning (IL) has been proposed as a solution to the problem of catastrophic forgetting, allowing ML systems to continuously learn and adapt to new data. IL can be generalized as an ML problem with a continuously growing dataset, where new classes are introduced sequentially over training tasks. While existing research has focused on accuracy as the main metric, it neglects other metrics such as training times, energy consumption, and computational complexity, which are crucial factors in practical implementations.This study aims to bridge the gap between AI research and practical implementation in industry projects. It advocates for IL research to be extended to practical scenarios, with an emphasis on energy consumption and computational footprint. The study analyzes performance metrics and considerations for comparing IL methods comprehensively, and introduces a novel dataset of industrial objects to evaluate different approaches. Additionally, the impact of periodic Joint Training updates in tandem with incremental training is studied. The goal is to bring together the domains of Green AI and Incremental Learning and provide useful tools and methodologies for the AI community and adopters in assessing the suitability of continual learning frameworks for their own use cases.