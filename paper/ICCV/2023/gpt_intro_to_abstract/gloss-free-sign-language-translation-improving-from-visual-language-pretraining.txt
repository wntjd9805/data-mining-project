Sign language translation (SLT) techniques have the potential to facilitate effective communication with hard-of-hearing individuals. However, traditional SLT methods face challenges due to the cross-domain translation nature and scarcity of annotated data. This paper introduces a novel approach that utilizes sign glosses, which are simplified representations of sign language, to improve SLT performance. However, annotating glosses is a time-consuming task, limiting the scalability of gloss-based methods. To address this, the paper draws inspiration from CLIP and proposes a visual-language pretraining approach for SLT. This approach establishes a potential connection between visual signs and language context, addressing the challenges of joint pretraining and limited data. Additionally, the paper presents an end-to-end gloss-free SLT architecture, referred to as GFSLT, that directly transforms visual representations into spoken sentences without intermediate steps or guidance. The proposed approach achieves unprecedented improvements in the BLEU-4 score for SLT without using gloss annotations. The contributions of this work include significant breakthroughs in gloss-free SLT, the introduction of the visual-language pretraining strategy in gloss-free SLT, and a novel pre-training paradigm that improves the accuracy and efficiency of SLT systems.