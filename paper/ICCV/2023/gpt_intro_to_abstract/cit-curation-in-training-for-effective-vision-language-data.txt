In the field of computer science, vision-language models have exhibited promising results in fine-tuning and transferring knowledge to downstream tasks. Traditionally, this involves training on general-purpose large-scale datasets rather than task-specific datasets. However, this approach is computationally expensive and requires pre-filtering of the dataset to remove irrelevant data. To address these limitations, recent efforts have focused on curating high-quality image-text pairs. Despite these advancements, researchers have been limited to static datasets and are unable to access or modify data pipelines or model architectures. Moreover, training on large image-text datasets is costly. This paper aims to empower training by enabling dynamic curation of data during training. The key idea is to use the text representation learned by vision-language models to measure the relevance of data to the task at hand. A simple algorithm called data Curation in Training (CiT) is proposed, which alternates between curating the data and training the model on the curated data. CiT significantly speeds up training and can handle image-text pairs from any source, including noisy ones. Experimental results demonstrate that CiT outperforms traditional CLIP/LiT training methods, especially when trained on large raw datasets.