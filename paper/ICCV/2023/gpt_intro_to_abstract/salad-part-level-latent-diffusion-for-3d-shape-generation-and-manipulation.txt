The introduction of this computer science paper discusses the rise of diffusion models in the field of image generation and their advantages in completion and editing tasks. It highlights the challenges and limitations of applying diffusion models to 3D generation and the need for an appropriate representation of 3D data. The authors introduce their novel Shape PArt-Level LAtent Diffusion Model (SALAD), inspired by disentangled implicit representations, which allows for high-quality generation and manipulation of 3D shapes at the part level. They present the technical contributions of their diffusion neural network, designed to handle the characteristics of part-level implicit representations, and their two-phase cascaded diffusion model for improved generation quality. The paper concludes by outlining the quantitative and qualitative assessments of SALAD, demonstrating its outperformance compared to state-of-the-art methods in shape generation, and showcasing its versatility in modeling multi-modal distributions and text-guided generation. The contributions of the paper are summarized as the proposal of SALAD, the two-phase cascaded diffusion model, the demonstration of zero-shot capability in shape editing, and the extension of SALAD to text-guided generation and editing.