Recent advancements in machine learning have shown remarkable performance enhancements on benchmark datasets. However, these methods often suffer from performance degradation when applied to test data with different properties from the training data. This sensitivity to distribution shifts hampers deep networks from performing well in practical scenarios. Various adaptation methods have been proposed to address this issue, but most of them require access to the source data during adaptation or modification of the training procedure, limiting their applicability. In this paper, we propose a test-time adaptation method that does not require access to the source data or modification of existing deep networks. We conduct an analysis of the effects of feature alignments in test-time adaptation, focusing on the intra-class distance and inter-class distance. We show that aligning the target features to the pre-calculated source feature distributions, considering both intra- and inter-class distances, enhances class discriminability and significantly improves classification accuracy. Our proposed approach, called Class-Aware Feature Alignment (CAFA), does not require hyper-parameters or modifications of the training procedure. We conduct extensive experiments on six different datasets and show that CAFA consistently outperforms existing methods in test-time adaptation.