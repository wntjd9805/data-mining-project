The presence of scattering media, such as fog, smog, light rain, and snow, poses challenges in imaging and scene understanding for computer vision and photography. These weather phenomena can significantly degrade image quality, affecting local contrast, color reproduction, and image resolution. Existing methods for dehazing have been explored, with successful approaches utilizing learned feed-forward models. However, these methods struggle to overcome the domain gap between simulation and real-world conditions. Acquiring paired data in real-world scenarios is challenging, leading to the reliance on weak semi-supervised training cues, resulting in incomplete separation of atmospheric scatter from clear image content. This problem is particularly relevant in harsh weather conditions, which not only impact image quality but also human vision, contributing to fatal automotive accidents. The long-tailed distribution of natural scenes with participating media in training datasets further complicates computer vision tasks in bad weather. Previous attempts to address this dataset bias involve augmentation techniques that simulate atmospheric weather effects or use synthetic simulation to generate adverse weather scenarios. However, these approaches cannot compete with supervised training data due to domain gaps or approximate physical forward models. Accurate modeling and separation of scattering in participating media are essential for imaging and scene understanding tasks. In this work, we propose an inverse rendering approach to address this challenge. Rather than predicting clean images from RGB frames, we aim to learn a neural scene representation that explains foggy images through a physically accurate forward rendering process. This representation allows us to render novel views with physics-based scattering, disentangle appearance and geometry without scattering, and estimate physical scattering parameters accurately. To efficiently optimize the scene representation, we leverage neural radiance field methods. Unlike existing NeRF methods, our forward model accurately models participating media. We validate our approach using real-world and controlled scenes, demonstrating its effectiveness in modeling foggy scenes and outperforming existing dehazing and depth estimation methods in various driving scenes. Our contributions include proposing a method to learn a disentangled representation of the participating media by incorporating the Koschmieder scattering model into the volume rendering process. Additionally, our approach is lightweight in terms of computation and memory consumption, requiring only a single MLP for modeling the scattering media properties without additional sampling or procedures.