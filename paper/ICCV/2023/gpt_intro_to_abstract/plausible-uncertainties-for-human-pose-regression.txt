In this paper, we address the necessity for principled safeguards in deep learning-based machine vision systems used in safety-critical real-world applications. Specifically, we focus on human pose estimation (HPE) as an application that allows autonomous systems to anticipate people's intentions and movements. Robust uncertainty estimation is essential for decision-making under uncertainty, and numerous methods have been proposed to augment model predictions with measures of uncertainty and confidence. However, these methods often rely on strong theoretical assumptions that need to be relaxed for real-world deployment. To tackle this challenge, we investigate whether two established methods of uncertainty quantification, maximum a-posteriori estimation (MAP) and deep evidential regression (DER), remain true to their definitions in the HPE domain. Additionally, we propose a recalibration step to derive interpretable measures of confidence. This study is the first to apply evidential deep learning to the HPE domain and includes thorough evaluations on the interpretability of the results, including label noise injection and occlusion trials. We also discuss the importance of estimating two types of uncertainty: aleatoric uncertainty and epistemic uncertainty. Aleatoric uncertainty arises from sensor or measurement noise, while epistemic uncertainty stems from the model specification or parameters. Understanding and quantifying these uncertainties are crucial for efficient learning and effective risk assessment. We provide examples of each type of uncertainty in the context of computer vision applications and highlight the benefits of accurate epistemic uncertainty estimates for downstream applications such as active sampling optimization and model choice evaluation.