Recent advancements in large-scale language-image pretraining, such as CLIP, have shown remarkable zero-shot transfer capability and unprecedented performance in text-to-image generation. However, the complex nature of vision and language often requires models with a large number of parameters, leading to high costs in terms of storage, memory, and computation time. To address this, model compression techniques, such as knowledge distillation, have been explored. While knowledge distillation has been extensively studied in single-modal settings, its potential for multi-modality remains underexplored, particularly in the context of language-image models. In this paper, we propose a novel cross-modal distillation method called TinyCLIP that introduces two key techniques: affinity mimicking and weight inheritance. Affinity mimicking leverages the cosine similarity of image and text embeddings in the teacher model to facilitate the distillation process, enabling the student model to mimic the teacher's visual-linguistic feature alignment. Weight inheritance transfers pre-trained weights from the teacher model to the student model, providing a good initialization and accelerating the distillation progress. We introduce both manual and automatic methods for selecting advantageous weights and extend weight inheritance to a multi-stage progressive procedure. Experimental results demonstrate the effectiveness of TinyCLIP in delivering competitive models at various levels of speedups and model sizes. Our proposed method achieves significant improvements in zero-shot accuracy on ImageNet and demonstrates good transfer capacities in downstream tasks. In summary, this work presents a new approach for distilling small CLIP models with competitive performance, leveraging large-scale models and pre-training data, and contributes state-of-the-art language-image pre-trained models that strike a favorable trade-off between speed and accuracy.