Abstract:This paper addresses the problem of adapting uni-modal models to multi-modal targets in the absence of the original source dataset. Previous methods in cross-modal unsupervised domain adaptation (UDA) assume access to paired multi-modal data and the source dataset during adaptation. However, these assumptions are impractical in real-world scenarios. To overcome these limitations, we propose a source-free adaptation framework called SUMMIT. This framework leverages pseudo-label fusion across modalities to filter noisy predictions and learn correlations between modalities on the target domain. We introduce a data-driven switching method that selects between agreement filtering and entropy weighting based on the estimated domain gap. Extensive experiments on challenging benchmarks demonstrate that our method outperforms competing baselines by up to 12%. Our contributions provide practical solutions for adapting uni-modal models to multi-modal data without access to the source dataset.