Text-to-image generation is a challenging task in computer science, as it requires ensuring high image quality and correspondence between input text and output images. While texts provide semantic descriptions of images, they often lack detailed spatial information. To address this challenge, recent approaches leverage large-scale text-image datasets to learn the correspondence between text and images. However, the cost of training on such large datasets remains a major obstacle, and generating images in specific domains can be challenging due to limited data availability. In this paper, we propose a novel approach to achieve high text-image correspondence for domain-specific text-to-image generation by generating both images and their corresponding semantic layouts. We introduce a Gaussian-categorical diffusion process to model the joint distribution of image-layout pairs, allowing our generative model to effectively learn the correspondence between text descriptions and image locations. We evaluate our approach on datasets with limited text-image pairs and demonstrate its potential applications in semantic image synthesis and semantic segmentation. Our contributions include the definition of the Gaussian-categorical diffusion process, the demonstration of its effectiveness in increasing text-image correspondence when web-scale datasets are unavailable, and the introduction of cross-modal outpainting as a practical application of the diffusion models.