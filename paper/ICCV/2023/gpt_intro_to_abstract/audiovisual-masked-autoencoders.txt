The rapid progress in computer vision tasks and domains has been driven by large models and datasets. However, pretraining models on labeled datasets and finetuning on smaller target datasets is not scalable due to the expense and time required for annotation. Self-supervised pretraining methods, which learn feature representations from unlabeled data, have been successful in natural language processing and are now being adopted in the vision community. In this paper, we propose leveraging audiovisual information in video to improve self-supervised representation learning. Despite recent advances in self-supervised image and video representation learning, these works have ignored the additional auditory information in pretraining datasets. Our approach aims to exploit the correlations between modalities to learn stronger representations for unimodal and multimodal downstream tasks. We draw inspiration from masked autoencoding frameworks and develop multiple pretraining architectures to jointly encode and reconstruct audiovisual inputs. We also propose a novel "in-painting" objective to encourage cross-modal information modeling. Our audiovisual pretraining achieves state-of-the-art results in audiovisual datasets and can be reused for audio-only or video-only downstream tasks. Additionally, our model demonstrates transferability between different pretraining and finetuning datasets, achieving state-of-the-art results on the Epic Kitchens dataset.