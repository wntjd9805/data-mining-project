Vision Transformers (ViTs) have shown promising performance in computer vision tasks, but they tend to be more computationally intensive and have higher latency compared to lightweight convolutional neural networks (CNNs). Many research efforts have been made to alleviate this limitation, including reducing the computation complexity of the attention mechanism and combining CNNs with attention mechanisms. However, these approaches still have limitations in terms of latency and model size. This paper proposes a new family of mobile vision backbones called EfficientFormerV2, which addresses the challenge of designing a transformer-based model that is both light and fast while preserving high performance. The authors introduce architectural improvements and a fine-grained architecture search algorithm to optimize model size and speed for transformer models. Experimental results show that EfficientFormerV2 outperforms MobileNetV2 in terms of accuracy on ImageNet-1K, while being smaller and faster. The proposed approach also achieves promising results in downstream tasks such as detection and segmentation. The contributions of this paper include a comprehensive study of mobile-friendly design choices, a novel joint search algorithm for transformer models, and the demonstration that vision transformers can be as small and fast as CNNs while maintaining superior performance.