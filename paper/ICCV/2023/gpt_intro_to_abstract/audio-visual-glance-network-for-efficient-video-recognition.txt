Abstract:The exponential growth of diverse video contents, particularly those involving human-related actions, has led to the development of deep learning algorithms for video recognition. However, the current state-of-the-art methods often require high computational costs, hindering their practical application in real-world scenarios. To address this issue, various approaches have been proposed, such as developing efficient architectures, adaptively selecting informative videos, or manipulating the spatial resolution of input frames. In this paper, we propose the Audio-Visual Glance Network (AVGN), a framework designed to enhance efficiency in both spatial and temporal dimensions. AVGN utilizes audio and visual information to selectively process important frames and extract important spatial patches, achieving improved efficiency without sacrificing accuracy. Experimental results demonstrate that AVGN outperforms state-of-the-art methods in terms of accuracy and computational cost on multiple video recognition datasets. We contribute by demonstrating the effectiveness of incorporating audio modality, proposing a unified approach for improving efficiency, and introducing tailored training strategies. AVGN sets a new standard for efficient video recognition.