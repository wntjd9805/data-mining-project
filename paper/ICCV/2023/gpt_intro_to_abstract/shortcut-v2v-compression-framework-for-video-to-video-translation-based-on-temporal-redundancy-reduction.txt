Video-to-video translation is a task that generates realistic video frames of a target domain from an input video. However, existing video-to-video translation networks are computationally expensive and memory-intensive. In this paper, we propose Shortcut-V2V, a general-purpose framework for improving the computational efficiency of video-to-video translation by reducing temporal redundancy. Shortcut-V2V leverages features from previous frames and the encoding layer to approximate decoding layer features of the current frame, reducing redundant computations. Our framework, which does not require future frames for inference, significantly improves test-time efficiency while preserving the original performance. We demonstrate the effectiveness of Shortcut-V2V using two well-known video-to-video translation models and show reductions in computational cost and memory usage while achieving comparable performance to the original models. Our contributions include the introduction of a novel model compression framework for video-to-video translation and the introduction of an adaptive blending and deformation block that exploit features from neighboring frames in a lightweight manner.