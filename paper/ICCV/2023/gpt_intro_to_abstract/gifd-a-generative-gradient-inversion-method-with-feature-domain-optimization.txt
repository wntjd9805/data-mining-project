Federated learning (FL) is a popular distributed machine learning framework that allows multiple clients to collaborate in learning tasks under the coordination of a central server. FL resolves the issue of data silos and brings privacy benefits by aggregating gradients calculated from local data instead of private data. However, recent studies have shown that even gradients uploaded in FL are at risk of privacy leakage. Several attacks have been proposed to reconstruct private data from these gradients, but they often rely on strong assumptions that are impractical in real FL scenarios. In this paper, we introduce a simple and effective solution called Gradient Inversion over Feature Domain (GIFD) to address the challenges of expression ability and generalizability of pre-trained generative models. We leverage the manifold of a pre-trained generative adversarial network (GAN) as prior information and reformulate GAN inversion as an intermediate layer optimization problem. We propose an iterative optimization approach where we first optimize the latent space and then successively optimize the intermediate layers of the generative model. We also address the issue of flexible private data generation under more realistic settings by investigating out-of-distribution (OOD) gradient attack scenarios. We evaluate our proposed GIFD method compared to state-of-the-art baselines using gradient transformation under four common defense strategies for gradient sharing protection. Our experiments demonstrate the effectiveness and generalization ability of GIFD in generating private data and show improvements compared to existing methods. Overall, our contributions include proposing GIFD for gradient inversion using pre-trained generative models, demonstrating its generalization ability in generating private OOD data, and evaluating its performance against existing baselines under different defense strategies.