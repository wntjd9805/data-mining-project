Vision-language models have achieved great success in various vision-language tasks. However, fine-tuning large-scale architectures like CLIP can be challenging and may affect the good representations learned during pre-training. Prompt engineering has been used to provide domain-specific context, but it requires manual selection of prompts. To address this, prompt engineering methods like CoOp and CoCoOp have been proposed, but they still affect the model's hidden representation. This paper introduces Read-only Prompt Optimization (RPO), a method that prevents internal representation shift during adaptation while being parameter-efficient. RPO only allows prompts to read information from the pre-trained model's attention-based interactions. The paper presents a simple initialization method for read-only prompts, leveraging the token embeddings of the pre-trained model. Extensive experiments demonstrate the generalization and performance of RPO in few-shot adaptation settings, reducing variance and achieving the best results in several benchmarks.