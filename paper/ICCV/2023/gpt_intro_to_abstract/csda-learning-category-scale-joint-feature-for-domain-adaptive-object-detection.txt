Object detection algorithms have made significant advancements in recent years, thanks to the availability of large, high-quality labeled datasets. However, these algorithms struggle to perform well when faced with new scenarios with unlabeled data, resulting in severe degradation in performance. To address this issue, domain adaptive object detection (DAOD) has been proposed, where a trained detector is transferred from a source domain with labeled data to a target domain without labeled data. The difference in data distribution between these domains is a crucial factor that affects performance. Existing methods attempt to improve the generalization ability of detectors by aligning feature distribution from various aspects, such as pixel- and instance-level alignment, cross-domain alignment of foreground objects, and category-level alignment. However, these approaches fail to consider the impact of object scale on feature alignment, which limits their effectiveness. In this paper, we identify two challenges in current category-level DAOD works: the large variance in category features at different scales, and the weak perception of small objects. To overcome these challenges, we propose a DAOD framework called the Category and Scale Information for Domain Adaptive Object Detection (CSDA), which focuses on joint category and scale feature alignment and enhancement. This framework includes a Scale-Guided Feature Fusion module (SGFF) that promotes feature learning and alignment for each category by pulling objects of the same scale closer while pushing objects of different scales away. Additionally, we introduce a Scale-Auxiliary Feature Enhancement module (SAFE) that enhances feature interactions at different scales, particularly for small objects. Our extensive experiments demonstrate that CSDA outperforms existing state-of-the-art methods across three widely-used benchmarks.