Adverse visual conditions pose challenges for autonomous systems navigating in various environments. Perception algorithms that can robustly operate in adverse conditions are necessary for achieving full autonomy. However, existing recognition methods are biased towards normal visual conditions and struggle with edge cases. Additionally, obtaining high-quality annotations for adverse-condition images is difficult and expensive. To address these challenges, researchers have explored unsupervised domain adaptation (UDA) from normal to adverse conditions. This paper focuses on source-free domain adaptation, where only a pre-trained model and unlabeled target images are available. This problem arises in real-world scenarios when labeled source data is inaccessible. The absence of fine ground-truth annotations presents a significant challenge, as the model can drift and unlearn important concepts during adaptation. To enhance the adaptation process, the researchers leverage weak supervision in the form of reference images from multiple driving datasets. The proposed method, named Contrastive Model Adaptation (CMA), utilizes a unified embedding space to leverage the reference predictions. By assuming that co-located features between the reference and target images should be similar, CMA eliminates condition-specific information while preserving semantic content. CMA achieves state-of-the-art results for model adaptation on normal-to-adverse semantic segmentation benchmarks and outperforms standard UDA methods, despite its data limitations. The effectiveness of CMA is demonstrated through evaluations on the Adverse-Condition Generalization (ACG) benchmark.