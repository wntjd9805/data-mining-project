The task of 3D interacting hand pose estimation from a single RGB image is crucial for various applications in human action understanding. However, obtaining 3D hand pose annotations from real images is challenging and time-consuming due to self-occlusion. Synthetic 3D annotation data has gained popularity, but challenges remain in terms of validity and diversity of the generated poses and images. In this paper, we present a high-fidelity synthetic dataset of 3D hand interaction poses for precise monocular hand pose estimation. We address the challenge of generating valid hand poses by designing an optimization process that considers hand attraction and anti-penetration. We also ensure the plausibility of the poses by introducing anatomical pose constraints and utilizing adversarial learning. Our dataset includes diverse backgrounds, lighting, and textures, providing a more realistic representation of real hand data. We conduct extensive experiments to evaluate the performance of our dataset and demonstrate its effectiveness in reducing the dependency on real data. We also propose a transformer-based network that achieves state-of-the-art results using our dataset. Our contributions include the optimization method for generating valid hand-interacting poses, the high-quality image synthesis system, and the construction of a large-scale synthetic interacting hand dataset.