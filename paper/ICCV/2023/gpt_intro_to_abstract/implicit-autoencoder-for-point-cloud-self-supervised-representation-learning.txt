The advancement and accessibility of commodity scanning devices have made it easier to capture large amounts of 3D data represented by point clouds. Point-based deep networks have enabled various 3D vision applications, such as shape classification, part segmentation, 3D object detection, and 3D semantic segmentation. Annotating point-cloud datasets is a labor-intensive task, leading researchers to explore self-supervised representation learning paradigms. Self-supervised learning methods for 2D images have been extensively developed, with autoencoders being popular tools. There is a growing interest in developing self-supervised representation learning methods for point clouds. However, prior works in point-cloud self-supervised representation learning have not addressed the challenges posed by sampling variations in point clouds. In this paper, we propose the Implicit AutoEncoder (IAE), which combines the implicit surface representation with point-cloud self-supervised representation learning. IAE improves upon existing methods by discouraging the encoder from being distracted by sampling variations and enabling the capture of generalizable features from the true 3D geometry. We conduct experiments to demonstrate the effectiveness of IAE in various downstream tasks, where it consistently outperforms state-of-the-art methods. Our contributions include the proposal of IAE, the formalization of sampling variations in point clouds, and the demonstration of IAE's effectiveness in different tasks.