The rapid development of advanced editing software has led to an increase in manipulated images on social media, which can be used to spread fake news and misleading information. To address this issue, it is crucial to develop methods that can automatically detect manipulated images. While various image manipulation detectors have been proposed and achieved high performance on manipulated image datasets, existing methods are vulnerable to adversarial examples. Adding imperceptible adversarial perturbations to manipulated images can cause detectors to produce incorrect results. This paper focuses on the task of Adversarial Manipulation Generation (AMG), where the goal is to generate adversarial examples that fool both the class label and the manipulated regions in semantic segmentation detectors. The authors propose a novel adversarial attack that incorporates both spatial and frequency features to generate imperceptible perturbations that preserve image quality. They evaluate their method on three manipulation detectors and demonstrate that it generates adversarial examples quickly, with better image quality and high attack success rates. The examples generated by AMG also have better transferability and can fool both classification and detection models. Overall, this work explores the vulnerability of image manipulation detectors and proposes an effective method for generating imperceptible adversarial examples.