6D object pose estimation, which involves determining the 3D rotation and translation of an object in the camera coordinate system, is a well-studied computer vision task. Category-level object pose estimation, which aims to solve the problem without relying on the exact CAD model of the target object, is more challenging than instance-level estimation. Most existing category-level methods are based on a canonical representation known as Normalized Object Coordinate Space (NOCS). However, NOCS-based methods become inaccurate and less robust when dealing with object categories that have significant shape variations. This is because the global and rigid alignment used to construct NOCS does not result in semantically coherent object coordinates. To address this issue, we propose Semantically-aware Object Coordinate Space (SOCS), which is built by warping and aligning objects guided by a sparse set of keypoints with semantically meaningful correspondence. This ensures that SOCS is semantically coherent and allows for accurate pose and size estimation. To effectively learn the mapping from image space to SOCS, we propose a multi-scale coordinate-based attention network that captures shape variations and handles inter-object occlusions. Our method achieves state-of-the-art performance on benchmark datasets and makes two main contributions: the introduction of SOCS for accurate pose estimation under large shape variations and a multi-scale attention network for effective coordinate regression.