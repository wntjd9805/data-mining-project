This paper introduces a method to induce object-awareness in video-language models by incorporating object-level properties during training. Specifically, the model predicts the localization and semantic categories of objects, such as hands and principal objects, in ego-centric videos. The authors use a vanilla video transformer architecture and train with query vectors and object loss. They utilize noisy and sparse annotations obtained from a hand object detector, enabling training on large-scale data without costly manual supervision. The proposed method achieves state-of-the-art performance in ego-centric benchmarks and demonstrates improved grounding and performance in long-term video understanding tasks. The contributions include the architecture's drop-in replacement capability, opportunistic training using available annotations, strong transfer learning performance, and improved performance in grounding and video understanding tasks.