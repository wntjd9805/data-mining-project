This paper addresses the challenge of estimating hand-object contact patterns in human-computer interaction and robotic imitation. While previous methods have made progress in estimating contact between hands and rigid objects or 2.5D cloth, extending these methods to 3D nonrigid objects remains challenging. One reason for this is that existing methods project the contact area onto the object's surface, making it difficult to store contact information in a feature-aligned space. To overcome this obstacle, this paper proposes a novel approach that represents the regional 3D surface where contact may occur as regional 2D unwrapping profiles. The contact and deformation within/across regions are then predicted using a Vision Transformer based on monocular image cues. The proposed method also preserves the hand-object surface correlation and contact point orderliness, providing a more comprehensive representation of the interaction. Additionally, the framework takes into account the deformed degree of the contact, allowing for the estimation of multiple nonrigid contact patterns. The proposed framework, called RUFormer, is capable of reconstructing both rigid and nonrigid hand-object interaction from monocular images. The contributions of this work include a learning-based framework for estimating hand-object contact, a representation method for recording hand-object surfaces, and an unwrapping-informed transformer for predicting contact and deformation. Overall, this paper presents a novel approach for addressing the estimation of hand-object contact patterns in various applications.