Text-to-image synthesis models have gained popularity, allowing users to generate high-quality images based on textual descriptions. However, there has been limited investigation into the security of these models. This paper addresses the issue of backdoor attacks on text-to-image synthesis models, where malicious model providers can inject hidden functions that enforce specific behaviors. The authors demonstrate that even small manipulations to the models can result in biased image generation. Backdoor attacks can have serious consequences, including the generation of offensive or biased content. The authors propose a method for integrating backdoors into pre-trained text encoders and triggering them with specific characters or words. They highlight the need to draw attention to the feasibility of these attacks while discussing possible defenses and ethical considerations. The contributions of the paper include introducing the first backdoor attacks on text-to-image synthesis models, demonstrating the effectiveness of inconspicuous triggers, and addressing the potential benefits and harms of these attacks.