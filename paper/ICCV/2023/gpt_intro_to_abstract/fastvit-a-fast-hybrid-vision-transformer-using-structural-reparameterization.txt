Vision Transformers have achieved impressive performance on various computer vision tasks but are computationally expensive. Recent works have proposed methods to reduce the compute and memory requirements of Vision Transformers by incorporating convolutional architectures. However, the use of skip connections in these hybrid architectures increases latency. In this paper, we introduce RepMixer, a reparameterizable token mixer that removes skip connections and addresses the latency overhead. We also replace dense convolutions with fac-torized versions and use linear train-time overparameterization to improve efficiency. Additionally, we incorporate large kernel convolutions to balance accuracy and latency. Our proposed model, FastViT, achieves significant improvements in latency while maintaining accuracy across tasks such as image classification, object detection, segmentation, and 3D hand mesh estimation. We compare FastViT with other architectures on mobile and desktop platforms and demonstrate its superior latency-accuracy trade-off. We also evaluate the robustness of our model to corruption and out-of-distribution samples, highlighting its competitive performance. Overall, our contributions include the introduction of FastViT, the fastest model in terms of latency on mobile and desktop platforms, its applicability to various computer vision tasks, and its robustness to corruption and out-of-distribution samples.