Recently, large-scale pre-trained models have shown potential performance across various fields such as computer vision and natural language processing. Cross-modal models, built with dual-stream architectures, have achieved remarkable success in vision-language tasks. However, training these models requires a large amount of image-text pairs collected from the internet. Existing methods train the final model using completed datasets, but this approach is impractical considering the continuously growing nature of pre-training data. In this paper, we propose a data-aware automatic model growing method called GrowCLIP for large-scale cross-modal pre-training. We introduce a dynamic search space and a shared encoder search space to enhance cross-modal fusion and improve efficiency. The parameters of the new architecture are inherited from the old one to maintain previous knowledge. Experimental results demonstrate that GrowCLIP outperforms existing methods in zero-shot image classification and retrieval tasks. The contributions of this paper include the adaptation to growing data, insights into cross-modal model architecture design, and the effectiveness of the proposed approach.