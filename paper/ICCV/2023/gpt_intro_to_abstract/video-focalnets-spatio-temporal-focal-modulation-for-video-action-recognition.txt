This paper introduces Video-FocalNet, an architecture for efficient and effective video recognition. It addresses the limitations of Convolutional Neural Networks (CNNs) in modeling long-range dependencies and the high computational cost of Vision Transformers (ViTs). Video-FocalNet incorporates a spatio-temporal focal modulation architecture inspired by focal modulation for image recognition. It reverses the steps of self-attention by independently aggregating spatial and temporal context into modulators and fusing them with queries. This aggregation is achieved through a hierarchical contextualization step using depthwise and pointwise convolutions. The proposed architecture achieves state-of-the-art performance on several video recognition benchmarks while maintaining a favorable trade-off between accuracy and computation cost.