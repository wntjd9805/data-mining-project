Video reconstruction and free-viewpoint rendering have become increasingly important in creating immersive experiences, such as virtual reality and 3D animation. However, these techniques face challenges in terms of monocular viewpoints and complex human-environment interactions. Recent developments in neural radiance fields (NeRF) have improved static 3D scene reconstruction, as well as dynamic view synthesis. Other research focuses on modeling dynamic neural human behavior with estimated human poses as prior information. Despite these advancements, accurately reconstructing challenging monocular videos with complex human-object-scene motions remains a challenge. In this paper, we propose a novel method called Human-Object-Scene Neural Radiance Fields (HOSNeRF) to address this issue. HOSNeRF tackles the challenges posed by complex object motions in human-object interactions and the interaction of humans with different objects at different times. We present the use of object bones and state-conditional representations to accurately estimate object deformations and handle object removal/addition in dynamic scenes. We also explore and identify effective training objectives and strategies for HOSNeRF. Experimental results show that HOSNeRF outperforms state-of-the-art approaches in terms of novel view synthesis on challenging datasets. Our contributions include the novel HOSNeRF framework, object bones, and state-conditional representations for handling non-rigid motions and interactions, and extensive experiments that demonstrate the superiority of HOSNeRF.