3D scene reconstruction is an important task in fields like photogrammetry, robotics, and digital preservation. However, traditional approaches that rely solely on 2D image supervision face challenges in textureless regions and struggle to find correspondences between images. Recent neural rendering techniques, such as NeRF, excel at synthesizing novel views but also struggle with geometry reconstruction in scenes with low-texture regions or from limited input views. Depth cameras, like the Intel RealSense and coded structured light systems, address these issues by introducing their own lighting into the scene, which helps solve the stereo correspondence problem. Such active depth-sensing devices are commonly found in smartphones and tablets and enable new VR and AR applications. However, these sensors may struggle to accurately estimate depth in certain lighting conditions. In this paper, we propose a volumetric image formation model and optimization procedure that can synthesize structured light images using a known projection pattern. Our framework reconstructs scenes using a neural volume rendering procedure, recovering the shape and appearance of the scene from only a few input views. Additionally, our image formation model leverages radiometric cues present in the raw structured light images to solve for normals and separate the images into direct, indirect, and ambient components. We conduct experiments on real and synthetic datasets to demonstrate the advantages of our proposed framework and compare it to existing methods. Our contributions include a physically-based neural volume rendering model for multi-view structured light imaging, an implementation on the Intel RealSense camera that outperforms baseline approaches, and the ability to tackle new problems with structured light cameras, such as geometry reconstruction through partially transparent surfaces and fine meshes.