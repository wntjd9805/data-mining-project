Reconstruction of 3D scenes is a crucial component in various application domains such as robotics, autonomous vehicles, and augmented/virtual reality. Traditional reconstruction methods rely on depth prediction and multi-view integration, but they struggle with handling inconsistent and noisy depth estimations. Recently, neural methods have emerged that aim to directly predict the 3D scene representation, but they often suffer from lower accuracy and computational inefficiency. This is primarily due to limitations in feature back projection, feature fusion, and occupancy prediction. In this paper, we propose a method that improves feature back projection and fusion using depth priors, leading to more accurate and sparse representations. We also introduce two simple and scalable fusion schemes that outperform existing methods. Additionally, our method incorporates depth priors to improve reconstruction completeness. Experimental results demonstrate that our method achieves state-of-the-art performance in terms of accuracy and efficiency on various datasets.