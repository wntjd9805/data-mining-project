In this paper, we introduce UniT3D, a joint transformer-based approach to learning vision-language representations for 3D visual grounding and dense captioning tasks. Unlike previous methods that use task-specific neural modules, UniT3D utilizes a task-agnostic unified transformer with lightweight output heads. We propose a supervised training scheme that combines bidirectional and sequence-to-sequence objectives to enable joint vision-language representation learning. To address the limited size and variety of existing 3D vision-language datasets, we create a large-scale synthetic dataset by generating text annotations from an image captioner trained on abundant 2D image and text datasets. Experimental results demonstrate the effectiveness of our approach in achieving significant performance improvements on downstream 3D vision-language tasks. Our contributions include the introduction of a multimodal transformer architecture, a supervised joint pre-training scheme, and the construction of a large-scale synthetic point-cloud-text dataset.