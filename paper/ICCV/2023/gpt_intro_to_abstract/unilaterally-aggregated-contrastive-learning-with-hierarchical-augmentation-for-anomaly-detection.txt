Anomaly detection (AD), also known as outlier detection, is a critical task in various applications such as defect detection and medical diagnosis. Traditional methods for AD rely on generative models or discriminative models to detect outliers, but they have limitations in accurately identifying outliers while avoiding false positives. Recently, there has been a growing trend in leveraging models pre-trained on large-scale datasets for AD, but they deviate from the objective of AD. In this paper, we propose a novel method called Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation (UniCon-HA) for AD. Our method focuses on training AD models from scratch using only inliers and achieves both inlier concentration and outlier dispersion. We use contrastive learning to aggregate inliers together while pushing outliers away, and perform instance discrimination within virtual outliers to disperse them around the latent space. We also introduce a soft aggregation mechanism to suppress the influence of potential outliers induced by data augmentation. To further improve the concentration of inliers, we adopt an easy-to-hard hierarchical augmentation strategy. Our method outperforms state-of-the-art competitors in various AD scenarios and does not rely on auxiliary branches or pre-trained models. Our contributions include the development of a novel contrastive learning method for AD, the proposal of an easy-to-hard hierarchical augmentation strategy, and the consistent improvement of our method over existing approaches.