As deep neural networks (DNNs) are increasingly being applied to safety-critical tasks, the threat posed by DNNs has become a concern. Backdoor attacks have emerged as a practical and stealthy threat to DNNs, where the attacker plants triggers in a small portion of the dataset to mislead the DNNs to behave normally with benign inputs while classifying inputs with triggers into the target class. Various efforts have been made to detect or mitigate backdoor attacks, but fine-tuning, a commonly used technique in backdoor defense, has received less attention. However, fine-tuning alone may not be effective against strong backdoor attacks. In this paper, we propose a new objective function called Sharpness-Aware Minimization (SAM) with adaptive perturbations to fine-tune backdoored models and mitigate the backdoor effect. We empirically show that SAM can revise the weight norms of backdoor-related neurons and induce a more concentrated distribution of weight norms, ultimately mitigating the backdoor effect. Experimental results on benchmark datasets demonstrate that our method is competitive with state-of-the-art defense methods and can be used in conjunction with existing backdoor defense techniques to compensate for accuracy drop. Our contributions include uncovering the limitations of vanilla fine-tuning, proposing an innovative fine-tuning paradigm using SAM, and demonstrating the effectiveness of our method.