Large text-to-image diffusion models have gained attention for their ability to generate diverse and high-quality images based on textual prompts. These models have the potential to implicitly learn both high-level and low-level visual concepts from large-scale image-text pairs. However, it is unclear how to extract this knowledge and apply it to downstream visual perception tasks. In this paper, we propose a framework called VPD that leverages the knowledge learned in text-to-image diffusion models for visual perception tasks. We address the challenges of incompatibility between diffusion models and visual perception tasks, as well as the architectural differences between diffusion models and popular visual backbones. Our framework adapts pre-trained diffusion models by using the autoencoder as a backbone model and performing a single denoising step with prompts to extract semantic information. We extract features from the UNet decoder to construct visual representations and align them with text prompts. We also propose utilizing cross-attention maps between visual and text features to provide explicit guidance. We evaluate our method on three visual perception tasks and show that VPD achieves state-of-the-art results on referring image segmentation and depth estimation benchmarks. Our study offers a new perspective on learning generic visual representations with generative models and encourages further research in image generation and perception.