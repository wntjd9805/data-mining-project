In this paper, we introduce the concept of optimization-based inference (OBI) in machine learning, which involves inverting a forward model by optimizing an objective over the input space. OBI has been widely used in various domains such as vision, graphics, robotics, recommendation systems, and security. With the emergence of neural networks as the preferred parametrization for forward models, OBI has become even more powerful, as neural networks can be pre-trained on large datasets and inverted at testing time.We discuss the advantages of OBI over feed-forward or encoder-based inference, such as its flexibility to adapt to new tasks, ability to handle missing observations, support for generating diverse hypotheses, and robustness to new data distributions and adversarial examples. However, the main bottleneck for OBI is the computational efficiency and speed of inference, as it requires many iterations of optimization to obtain strong results. This is in contrast to feed-forward models that only require a single forward pass.To address this challenge, we propose a framework that learns a smoother loss landscape through a mapping network. Instead of optimizing in the original input space, we learn a new input space where gradient descent converges quickly. Our approach involves an alternating algorithm that collects optimization trajectories in the new space and updates the mapping parameters to reduce the distance between the convergence point and each point along the trajectory. By repeating these steps, our method learns a mapping that significantly accelerates gradient descent in the input space.We validate our approach through empirical experiments and visualizations on generative and discriminative models in various vision tasks, including GAN inversion, adversarial defense, and 3D human pose reconstruction. Our experiments demonstrate that our method converges an order of magnitude faster without sacrificing absolute performance after convergence. Importantly, our approach is compatible with existing OBI methods that have a differentiable forward model and objective function, making it applicable in a wide range of scenarios.Overall, the primary contribution of this paper is an efficient optimization-based inference framework that accelerates and stabilizes the inversion of forward neural networks. We provide a comprehensive survey of related literature, formally define OBI, present our method to learn efficient loss landscapes, and propose a training algorithm for better generalization and robustness. Our experimental results highlight the effectiveness of the mapping network for OBI and its potential to significantly improve convergence speed in optimization tasks.