Single image super-resolution (SR) aims to recover high-resolution (HR) images from low-resolution (LR) inputs. Various approaches, including those utilizing convolutional neural networks (CNNs), have been developed for this task. However, the local mechanism of convolution limits the establishment of global dependencies and hampers model performance. Transformer, originally proposed in natural language processing (NLP), has shown promising results in high-level vision tasks by utilizing a self-attention (SA) mechanism to establish global dependencies. Inspired by the potential of Transformer, researchers have attempted to apply it to low-level tasks such as image SR. Different methods have explored efficient usages of Transformer, including applying spatial windows and channel-wise attention to capture spatial and channel context, respectively. In this paper, we propose the Dual Aggregation Transformer (DAT) for image SR. DAT aggregates spatial and channel features in an inter-block and intra-block dual way to achieve powerful representation capability. By alternately applying spatial and channel self-attention, DAT can capture both spatial and channel context and realize inter-block feature aggregation. To complement Transformer with locality, we incorporate convolution in parallel with self-attention. Additionally, we propose the adaptive interaction module (AIM) to enhance the fusion of spatial and channel information within a self-attention module. Furthermore, we introduce the spatial-gate feed-forward network (SGFN) to incorporate non-linear spatial information and relieve channel redundancy. Our experiments demonstrate that DAT outperforms state-of-the-art methods in terms of both performance and complexity. In summary, our contributions include the design of DAT, which aggregates spatial and channel features using a dual approach, and the introduction of AIM and SGFN for inter-block and intra-block feature aggregation, respectively.