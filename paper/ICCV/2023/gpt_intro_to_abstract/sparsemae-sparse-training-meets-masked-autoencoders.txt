Recently, pretrained Masked Image Modeling (MIM) methods have shown great success in computer vision tasks. These methods involve a two-stage training process where the model learns task-agnostic knowledge through a pretraining task and is then fine-tuned for specific tasks. Among these methods, Masked Autoencoders (MAE) have demonstrated superior learning capability on large-scale vision transformers. However, deploying large models in computationally restricted scenarios is challenging. On the other hand, smaller and more efficient models fail to perform well after pretraining with MAE or data2vec frameworks. This suggests that when the model is scaled down, it becomes more difficult to learn well via masked pretraining. In this paper, we propose using pruning techniques in MIM frameworks to obtain performant small-scale models from larger ones. We apply sparse training methods for MAE pretraining and finetuning on N:M sparsity to obtain a model similar in scale to ViT-Tiny. This approach achieves notable improvements compared to directly training ViT-Tiny using MAE on downstream tasks. However, there is still a performance gap between the sparse model and the densely trained vanilla ViT-Base. To address this limitation, we propose a novel sparse training framework, SparseMAE, to improve the acquisition of task-agnostic and task-specific knowledge. We present extensive experiments to validate the effectiveness of our proposed method and achieve state-of-the-art performance with small-scale vision transformers on various tasks.