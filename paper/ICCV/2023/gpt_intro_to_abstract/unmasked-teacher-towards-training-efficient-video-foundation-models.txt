This paper introduces a training-efficient approach for developing Video Foundation Models (VFMs) for effective video understanding. VFMs focus on motion patterns and object interactions, which are essential for complex video understanding but often overlooked by Image Foundation Models (IFMs). The authors propose a progressive pre-training framework and utilize UnMasked Teacher (UMT) to train VFMs from scratch. The resulting models demonstrate state-of-the-art performances on various video tasks, including action recognition, spatiotemporal localization, video-text retrieval, and video question-answering. The proposed method is not only effective but also environmentally friendly, with a significant reduction in carbon emissions compared to existing approaches. Extensive experiments validate the effectiveness and efficiency of the proposed approach.