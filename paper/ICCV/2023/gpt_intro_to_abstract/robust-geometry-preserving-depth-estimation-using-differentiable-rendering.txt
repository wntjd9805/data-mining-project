Recovering 3D geometries of scenes from monocular images is a growing area of interest in computer science. Recent advances in monocular depth estimation have led to wide-ranging applications such as 3D photography. In order to accurately predict the geometry of diverse scenes, state-of-the-art depth estimation models advocate for mix-dataset training, which allows for robust depth predictions across different scenes. To enable this mix-dataset training, scale-and-shift invariant (SSI) losses are designed to normalize depth representations, allowing for the joint utilization of datasets with various depth representations. However, models trained with SSI losses predict depth up to unknown scale and shift factors, resulting in incomplete depth information for reconstructing 3D models. This introduces structural distortions that previous depth estimation models do not suffer from, but these models require geometry-complete depth annotations for learning. In this research, the goal is to develop depth estimation models that can predict geometry-preserving depth without the need for extra data or annotations. To achieve this, a novel framework based on differentiable rendering is proposed. This framework reconstructs 3D point clouds based on predicted depth and uses a differentiable renderer to generate novel views of the 3D model. The depth predictions of the rendered views are then made consistent through loss functions. The proposed method produces geometry-preserving predictions without relying on extra annotations or 3D datasets, allowing for improved generalization using mixed datasets. The self-supervised loss functions also enable the recovery of domain-specific scale and shift coefficients of a trained model. Additionally, the self-supervised loss can be used to estimate camera intrinsic parameters. Experimental results on multiple benchmark datasets validate the effectiveness of the proposed method. The main contributions of this research are summarized as follows: the proposal of a depth estimation learning framework that produces geometry-preserving depth without extra datasets or annotations, the development of a self-supervised consistency loss that recovers domain-specific affine coefficients, and the demonstration of the self-supervised loss for estimating camera intrinsic parameters. The experiments also show that the proposed method outperforms existing approaches in terms of scene structure recovery.