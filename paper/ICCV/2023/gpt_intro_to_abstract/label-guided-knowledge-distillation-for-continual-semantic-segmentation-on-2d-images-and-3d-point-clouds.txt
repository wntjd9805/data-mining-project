Continual semantic segmentation (CSS) is a research direction that aims to enable a deployed model to handle new classes without forgetting previously learned ones. This is important for real-world applications but presents challenges in terms of computational cost and privacy. Previous approaches, such as Joint Training and Knowledge Distillation (KD), have limitations in terms of catastrophic forgetting and class correspondence. In this paper, we propose a novel Label-Guided Knowledge Distillation (LGKD) loss that addresses these challenges. LGKD builds a reliable class correspondence across incremental steps and alleviates the novel-background confusion. It is a generic regularization term with negligible computational cost and can be easily incorporated into existing methods. Our experiments on two prevailing CSS benchmarks, Pascal-VOC and ADE20K, show significant improvements, especially on novel mIoU. We also establish a CSS benchmark for 3D point cloud based on ScanNet and demonstrate the versatility of LGKD in both 2D and 3D modalities without ad hoc design. Overall, our contributions include the proposal of LGKD loss and its effectiveness in improving CSS performance.