Deep learning models have become much larger in recent years, with models like GPT-3 having billions of parameters. However, the high costs associated with training these large models have become a critical concern. This has motivated researchers to find ways to avoid retraining large neural network models. In this paper, we focus on the StyleGAN family of models, which are known for their efficiency and high-quality image generation. While the StyleGAN models use a post-processing technique called truncation trick to improve image quality, it has been noted that this technique reduces the diversity of synthesized images and sacrifices distinct image features.In this work, we address the issue of low-quality images synthesized by StyleGAN by investigating the mechanism for image synthesis in the feature space. We discover an important phenomenon called Feature Proliferation, which describes how specific features reproduce with forward propagation in the StyleGAN generator. This phenomenon is a result of the weight modulation and demodulation techniques used in StyleGAN models. We observe that Feature Proliferation often leads to artifacts in the synthesized images, but we also find that these artifacts can be easily removed by a simple feature rescaling method.To minimize interference with useful image features, we propose a novel method to identify and modulate risky features that are prone to proliferation, focusing on the earliest layers of the generator. Our method significantly outperforms the truncation trick in retaining useful image features while improving the quality of StyleGAN synthesized images. Experimental results validate the effectiveness of our approach.In summary, our contributions include discovering the phenomenon of Feature Proliferation in StyleGAN models, identifying its causal relationship with image artifacts, and proposing a novel feature rescaling method to mitigate Feature Proliferation and achieve a better trade-off between quality and diversity of StyleGAN synthesized images compared to the truncation trick.