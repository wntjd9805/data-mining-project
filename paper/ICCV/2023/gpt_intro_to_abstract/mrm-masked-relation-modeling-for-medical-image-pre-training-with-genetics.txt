This paper introduces the concept of masked relation modeling (MRM) for multimodal medical data in order to improve image representation learning. The authors identify the challenges of existing masked image modeling (MIM) methods when applied to medical data and propose MRM as a solution. The challenges include the limited semantic regions in medical data and the limited semantic relations within each sample. The authors address these challenges by introducing relation masking, which preserves disease semantics within the input data, and relation matching, which captures disease-related relations by aligning feature relations between intact and masked features. The proposed MRM approach is evaluated on various downstream tasks using public medical pre-training datasets and demonstrates superior transfer ability compared to state-of-the-art methods. The findings of this study highlight the importance of considering the unique characteristics of medical data when designing self-supervised learning methods.