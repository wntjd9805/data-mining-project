Face recognition technology is widely used in various applications, but it is important to detect and measure algorithmic biases, especially across protected demographic attributes like age, race, and gender, to maintain fair treatment in sensitive applications. The first step in measuring bias is to collect a diverse benchmarking dataset, but this is challenging due to the need for a large number of individuals in each protected attribute group and the need to ensure an equal distribution of unprotected attributes across groups. Privacy concerns and the cost of data collection also make this process expensive. To address these challenges, we propose a method for constructing synthetic face recognition benchmarking datasets using a combination of face generation models and human annotators. We use a pretrained face generator to create synthetic faces with different protected attributes and varying unprotected attributes to construct face sets depicting the same person in different settings. Human annotators provide consensus on the identity labels, which we use along with the synthetic images to benchmark face recognition systems for bias.Our approach is fast, practical, inexpensive, and eliminates privacy concerns. We build on previous work using synthetic faces but focus on obtaining ground truth annotations for identity comparisons between face pairs. We present an annotation pipeline for verifying a synthetic face recognition benchmarking dataset and demonstrate that a synthetic approach can be reliable for causal face recognition benchmarking.Using our synthetic dataset, we computed face identity distances reported by three public face recognition models and evaluated the accuracy for different attributes and demographic groups. Our results show lower error rates for White groups compared to Black and East Asian groups. We also report the expected change in face identity distance with respect to changes in each unprotected attribute.Our contributions include a method to estimate bias in face verification algorithms, an empirical evaluation of our method, and a large dataset of synthetic faces with human-collected ground truth.