Self-supervised representation learning has become increasingly popular in the field of computer vision. This approach leverages large unlabeled datasets to pre-train vision backbones, aiming to improve the performance of downstream networks and reduce the reliance on labeled datasets. Recent research has shown that self-supervised pre-training can even outperform supervised pre-training on certain tasks.One dominant method in self-supervised representation learning is contrastive learning, where the target backbone is trained to map transformed views of an image closer together in latent space. Other approaches include using reconstruction losses, where certain regions of an image are masked and the backbone is trained to reconstruct them.While most existing methods treat the reconstruction task as deterministic, this paper argues for using generative models as representation learners. Generative models have the advantage of simplicity and the ability to generate high-quality samples, indicating the learning of semantically capable internal representations.The authors propose a representation learning framework called DreamTeacher, which utilizes generative models for pre-training downstream perception models through distillation. They explore two types of distillation: feature distillation, where generative features are distilled to target backbones without any labels, and label distillation, where task-heads on generative networks are used to distill knowledge from a labeled dataset onto target backbones. The focus is on using diffusion models and GANs as generative models and CNNs as target backbones.Preliminary experiments show that DreamTeacher outperforms existing self-supervised learning approaches on various benchmarks and settings. Notably, when pre-trained on ImageNet without any labels, DreamTeacher outperforms methods pre-trained on ImageNet with full supervision on multiple dense prediction benchmarks. Additionally, when trained solely on object-focused datasets with millions of unlabeled images, DreamTeacher achieves new state-of-the-art results compared to variants pre-trained on ImageNet with label supervision. These results highlight the potential of generative models, particularly diffusion-based models, as powerful representation learners that can effectively leverage unlabeled datasets at scale.