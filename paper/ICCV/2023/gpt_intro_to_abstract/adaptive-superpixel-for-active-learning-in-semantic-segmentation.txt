Recent advancements in deep learning have revolutionized computer vision tasks, including semantic segmentation. However, the annotation of large datasets required for training these complex deep network models is time-consuming and labor-intensive, especially for semantic segmentation tasks which demand dense pixel-level annotation. Active learning (AL) has emerged as a solution to reduce annotation costs by selectively querying informative samples to annotators. In the case of semantic segmentation, annotation queries can be image-wise, requiring complete annotation of every pixel in an image, or region-based, focusing on the dominant label of a small region such as a rectangle patch or superpixel. AL with region-based query requires careful generation of candidate regions to be queried. Previous works have relied on fixed candidate sets of regions of uniform size, limiting the flexibility of query regions. In this paper, we propose an AL framework that incorporates adaptive merging and sieving methods to balance annotation cost and noisy labels. The adaptive merging method evolves candidate superpixels for dominant labeling at each round of AL without explicit regularization on their size and shape, enabling accurate capturing of semantic object boundaries. We also introduce a corresponding acquisition function that prioritizes uncertain superpixels of rare classes to maximize informativeness. To mitigate noisy labels, we propose a sieving technique that excludes pixels with potentially different classes from the dominant one. The integration of adaptive merging and sieving methods improves accuracy and budget-efficiency compared to a baseline method and shows consistent improvement over existing methods in various settings. We also introduce a new evaluation metric for superpixel algorithms that assesses both accuracy and recall, providing insights for further developing such algorithms. Our contributions include the proposal of an adaptive merging algorithm, the introduction of a sieving technique, the demonstration of the effectiveness of our AL framework, and the analysis of suitable superpixels for AL using the new evaluation metric.