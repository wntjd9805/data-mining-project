Abstract:Human-centric scene understanding in 3D large-scale scenarios is essential for various applications such as assistive robotics, autonomous driving, surveillance, and human-robot cooperation. However, these scenarios present challenges due to the presence of diverse subjects, fine-grained human-object interactions, and occlusions. Current state-of-the-art perception methods rely heavily on large-scale datasets, emphasizing the need for large-scale datasets with rich annotations to promote research in human-centric scene understanding. While previous works have focused on scene understanding using image or video inputs and static indoor-scene understanding based on pre-scanned RGB-D data, there is a gap in the research for real-time perception in outdoor human-centric scenarios. The existing outdoor datasets often focus on vehicle-dominated traffic environments, neglecting the more challenging human-centric daily-life scenarios. To address this gap, we introduce HuCenLife, a large-scale multi-modal dataset that captures 32 multi-person daily-life scenes with rich human activities and human-object interactions in both indoor and outdoor scenarios. The dataset provides fine-grained annotations including instance segmentation, 3D bounding box, action categories, and continuous instance IDs, facilitating various 3D perception tasks. We present benchmarks for point cloud segmentation, detection, and action recognition using state-of-the-art methods on HuCenLife and propose effective modules to improve performance in complex human-centric environments. These modules leverage mutual relationships between human-human and human-object interactions and incorporate multi-resolution feature extraction strategies. Extensive experiments and ablation studies demonstrate the effectiveness and generalization capability of our methods. Our contributions include the introduction of HuCenLife, the first large-scale multi-modal dataset for human-centric 3D scene understanding, with rich annotations that benefit various perception tasks. We also provide baselines for three main tasks and propose novel modules that enhance perception accuracy in human-centric scenes.