Object detection is a crucial task in computer vision that involves locating and classifying objects within an image. Traditional methods for object detection typically employ convolutional neural networks (CNNs) to process regional features of the input image, incorporating various inductive biases such as anchors, label assignment, and duplicate removal. However, transformer-based object detectors, such as DETR, have recently emerged as a novel approach that simplifies the detection pipeline and reduces the need for manual tuning of hand-crafted components.While transformer-based detectors have achieved state-of-the-art performance, they often suffer from expensive computation requirements, limiting their real-time applicability. To address this issue, knowledge distillation (KD) methods have been proposed as a way to transfer knowledge from a teacher model to a smaller, more efficient student network. However, most existing KD methods are designed for convolution-based detectors and may not directly apply to transformer-based DETRs due to differences in the detection framework.This paper presents DETRDistill, a knowledge distillation framework specifically designed for DETR detectors. DETRDistill overcomes the challenges faced by DETRs in distillation by proposing three main components. First, Hungarian-matching logits distillation is used to find an optimal matching between student and teacher predictions and perform distillation at the logits-level. Second, target-aware feature distillation utilizes object queries and teacher model features to produce soft activation masks, which enable feature-level distillation that is sensitive to object targets. Lastly, query-prior assignment distillation encourages the student model to produce predictions based on a stable bipartite assignment of the teacher model's queries, improving convergence rate and performance.The paper presents extensive experiments conducted on the COCO dataset, showcasing the effectiveness and generalization of the proposed methods. The contributions of the paper include a detailed analysis of the challenges faced by DETRs in distillation compared to convolution-based detectors, the introduction of multiple knowledge distillation methods for DETRs, and empirical evidence demonstrating the superiority of the proposed methods.