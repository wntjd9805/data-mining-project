Continual learning (CL) aims to continuously learn new knowledge from a sequence of tasks with non-overlapping data streams. In the era of Internet of Things, where multi-source data and tasks are accessed at any time, a CL system should respond to parallel data streams from multiple devices. This paper focuses on Parallel Continual Learning (PCL), where an unfixed number of tasks are trained in parallel at any time. However, PCL suffers from catastrophic forgetting and training conflict among parallel tasks. Most existing methods in CL address catastrophic forgetting, but they are not suitable for the diverse training processes of parallel tasks in PCL. To overcome these challenges, the paper proposes an optimal gradient distance measurement called Asymmetric Gradient Distance (AGD), which considers gradient magnitude ratios and directions. The paper also introduces a Maximum Discrepancy Optimization (MaxDO) strategy to minimize the gradient discrepancy and mitigate self-interference among gradients. Additionally, a rehearsal strategy is implemented to address catastrophic forgetting. Experimental results demonstrate the effectiveness of the proposed approach. The main contributions of this paper are: (1) formulating PCL as a minimum distance problem and comparing symmetric and asymmetric distances, (2) introducing AGD to evaluate gradient discrepancy, and (3) proposing MaxDO to minimize gradient discrepancy and mitigate training conflict and catastrophic forgetting in PCL.