Temporal Action Detection (TAD) is a challenging task in video understanding, involving the extraction of specific action segments from long, untrimmed videos. A major barrier to robustly detecting action boundaries in these videos is the issue of movement feature submergence. This occurs when strong context or small movement sizes dominate the expression of video features, resulting in a lack of distinction between actions and backgrounds. Additionally, the multi-scale nature of actions in untrimmed videos presents another hurdle in accurately detecting action boundaries. This is because the feature richness varies significantly at different scales, with small-scale actions having few distinct features compared to large-scale actions. Existing approaches addressing these issues have shortcomings, such as ignoring movement enhancement and failing to adaptively learn action features of different scales. In this paper, we propose solutions to these challenges in TAD. First, we introduce the Movement Enhance Module (MEM) to address movement feature submergence. This module includes a siamese network called the Movement Feature Extractor (MFE), which extracts movement features from video snippets by considering both dynamic and static information. We also present the Multi-Relation Enhance Module (MREM) to establish temporal and local correlations between video snippets.To tackle the problem of multi-scale actions, we propose the Scale FPN (SFPN), which utilizes a U-shaped network to generate multi-scale video features and facilitate information flow between different layers. We adopt a two-stage learning strategy for each layer in the SFPN, where the Generalization stage trains the layer with all action segments and the Specialization stage biases the layer towards actions in a specific scale range. During inference, each layer is responsible for detecting action segments of the corresponding scale.The contributions of this work include the development of features and action segment representations better suited for TAD. The MEM effectively alleviates movement feature submergence and explores local and temporal relations between video snippets, while the SFPN learns different scale actions individually and adopts targeted training and inference strategies. Experimental results on two datasets demonstrate the effectiveness of our proposed method. On ActivityNet1.3, our approach improves the best average mAP from 36.6% to 37.7% and enhances the mAP@0.7 from 31.8% to 34.0% on THUMOS-14.