This paper introduces Listener Head Generation (LHG) technology, which aims to synthesize the motion of the listener in response to the speaker. LHG analyzes the talking semantics of the speaker automatically, without explicit guidance, to generate corresponding interactive motions of the listener. LHG has applications in human-computer interaction, virtual reality, metaverse, and media forensics. However, existing methods in LHG have ignored the need for comprehensive modeling of the speaker's motion and fail to simulate fine-grained facial motion under different emotions. This paper proposes a novel method called Emotional Listener Portrait (ELP) to address these challenges. ELP combines visual and audio information from the speaker to synthesize realistic listener head videos. It expands the classification dimensions to map listener motion to a higher-dimensional discrete space, allowing for a more precise depiction of facial expressions and head pose. Additionally, ELP leverages emotion priors to split and rearrange the discrete space, enabling explicit emotional representation. The paper presents the Adaptive Space Encoder (ASE) and Mesh-to-Video Renderer modules in ELP and demonstrates the effectiveness of the proposed method through quantitative and qualitative experiments on conversation portraits datasets. The contributions of the paper include the ELP framework for emotional listener head generation, the ASE module for explicit emotional representation, and superior performance compared to existing methods.