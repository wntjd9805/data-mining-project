Sign language is a powerful tool that aids individuals with hearing impairments by providing a means of communication through signals such as hand/arm positions and body postures. Continuous sign language recognition (CSLR) has gained attention in recent years, allowing communication between hearing-impaired individuals and those without sign language knowledge. Existing CSLR methods utilize spatial-temporal features and the Connectionist Temporal Classification (CTC) loss for alignment. However, these methods neglect the language knowledge and contextual information implicit in gloss sequences. This paper proposes a Cross-modal Contextualized Sequence Transduction (C2ST) method for CSLR that incorporates gloss sequence knowledge into video representation learning and sequence transduction. The proposed method introduces a cross-modal context learning framework and a contextualized sequence transduction loss to improve sign video modeling and address the issue of misrecognizing signs in different contexts. Extensive experiments on large-scale sign language recognition datasets demonstrate that the proposed C2ST method outperforms state-of-the-art approaches. The contributions of this paper are the cross-modal context learning framework, the contextualized sequence transduction loss, and the significant improvement in performance over existing methods.