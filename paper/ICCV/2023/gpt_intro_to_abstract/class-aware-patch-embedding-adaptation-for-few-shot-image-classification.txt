Real-world images often contain multiple entities, but current computer vision datasets only assign a single annotation to each image, ignoring other entities present. While this practice is acceptable in many scenarios, it becomes a challenge in few-shot image classification, where models must identify new classes with limited labeled images. The scarcity of labeled data and the limited number of class-relevant entities makes it difficult for models to determine the class of an image and may introduce ambiguity. Additionally, specific patterns learned during training may not be relevant to new classes, resulting in limited generalizability.To address these challenges, previous approaches have focused on aligning semantically-relevant regions to suppress interference and mitigate supervision collapse. However, these methods have limitations, such as aligning irrelevant regions and inaccurate localization caused by intra-class variation and background clutter.In this paper, we propose a new method called Class-aware Patch Embedding Adaptation (CPEA) that eliminates interference without aligning semantically-relevant regions and addresses supervision collapse. Instead of supervised pretraining, we use self-supervision with Masked Image Modelling as a pretext task to generate semantically meaningful patch embeddings. To make patch embeddings class-relevant, we introduce a class-agnostic embedding and use it in conjunction with the transformer to interact with patch embeddings. The resulting class-aware embeddings adapt patch embeddings to be class-relevant, alleviating the scarcity of labeled data. We also define a dense score matrix to quantify the similarity between class-relevant patch embeddings across images.Our contributions include successfully mitigating interference caused by single-label annotations without localization and alignment mechanisms, proposing CPEA to make patch embeddings class-relevant and improve transferability, and demonstrating the effectiveness of CPEA through extensive experiments on benchmark datasets. The results show that our method outperforms state-of-the-art approaches.