Panoptic segmentation, which combines semantic segmentation and instance segmentation, is a crucial task in computer vision. However, the cost of large-scale supervised learning for panoptic segmentation is high due to the need for expensive manual labeling. One potential solution is to learn from synthetic images, but models trained on synthetic data often struggle to generalize to real data due to the domain gap. Unsupervised Domain Adaptation (UDA) is a common approach to minimize this gap, but UDA for panoptic segmentation has been overlooked. Previous UDA methods for panoptic segmentation have achieved subpar performance compared to fully supervised learning approaches. In this paper, we address this performance gap by investigating UDA strategies in semantic segmentation and adapting them to panoptic segmentation. We also propose a novel network architecture, EDAPS, specifically designed for domain-adaptive panoptic segmentation. EDAPS significantly outperforms previous works on standard perception benchmarks and improves mPQ by 20% on SYNTHIA-to-Cityscapes and 72% on SYNTHIA-to-Mapillary Vistas. The main contribution of this paper is the combination of network components and UDA strategies to achieve state-of-the-art performance in panoptic segmentation UDA.