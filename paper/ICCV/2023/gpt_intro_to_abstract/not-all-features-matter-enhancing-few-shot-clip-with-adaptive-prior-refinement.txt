Contrastive visual-language pre-training has emerged as a popular approach for multi-modal learning in computer vision tasks. CLIP (Contrastive Language-Image Pre-training), in particular, has gained attention for its simplicity and effectiveness in aligning vision and language representations. However, in low-data scenarios, CLIP-based few-shot learning techniques with additional learnable modules have been proposed to enhance performance. Existing methods can be categorized into non-prior and prior-based approaches, each with their limitations. In this paper, we propose Adaptive Prior rEfinement (APE), a method that efficiently adapts CLIP for few-shot classification by refining its pre-trained knowledge in visual representations. APE selectively selects the most significant feature channels through inter-class similarity and variance, reducing redundancy and memory usage. We also introduce APE-T, a variant that trains lightweight category residuals to further update the refined cache model. Our APE and APE-T achieve state-of-the-art performance on 11 few-shot benchmarks, surpassing existing methods and demonstrating the effectiveness of our approach. The contributions of our work lie in the explicit utilization of CLIP's prior knowledge while maintaining computational efficiency, exploring trilateral affinities for few-shot learning, and achieving superior performance in training-free and training-required scenarios.