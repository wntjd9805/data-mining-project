Recently, robots have been widely used in various applications, requiring them to interact with objects. The object affordance problem, which aims to determine where and how to interact with an object in a given environment, is a primary challenge in robot-object interaction. Previous works have focused on the affordance problem of 3D articulated objects and have proposed solutions such as early fusion and critic-based learning. However, these approaches have limitations in learning the correlation between multi-modal data and in training efficiency. To address these limitations, we propose a novel pipeline called Multimodality-Aware Autoencoder-based Affordance Learning (MAAL). MAAL introduces a MultiModal Energized Encoder (MME) to handle multi-modal inputs and leverages a deep autoencoder for better training efficiency. The MME comprehensively learns data from different modalities and fuses features at different levels. It also considers the correlation between inputs and formulates better multi-modal learning. The AE pipeline in MAAL learns valuable patterns in high-dimensional data without labeled examples, leading to better computational efficiency. MAAL achieves better training efficiency by using only reconstruction loss as supervision and can be trained in one go. Additionally, MAAL specifically considers the properties of 3D object affordance and focuses on learning action information and the interaction between robots and objects. Our main contributions include the proposal of MAAL, the introduction of MME for better multi-modal learning, and the achievement of superior performance compared to current methods in solving the 3D object affordance problem.