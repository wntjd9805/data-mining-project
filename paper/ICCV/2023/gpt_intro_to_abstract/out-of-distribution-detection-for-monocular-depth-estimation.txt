In this paper, we address the challenge of unreliable depth predictions in monocular depth estimation, specifically in the context of safety-critical applications such as automated driving or robotics. We highlight the importance of identifying inputs for which the depth predictions are unreliable due to image irregularities or unknown environments. We distinguish between data uncertainty and model uncertainty and propose a method to detect out-of-distribution (OOD) data, which is rarely explored in depth estimation tasks. Motivated by image anomaly detection approaches, we propose to detect OOD inputs using the reconstruction error of an image decoder trained to predict the original input image from features extracted by a fixed depth estimation model. Our method is applicable to various depth estimation models and architecture types. We evaluate our approach using extensive experiments and introduce an epistemic uncertainty evaluation protocol. Our contributions include being the first to propose an OOD detection method for monocular depth estimation, presenting a simple and effective image reconstruction-based approach, and providing publicly available models and code.