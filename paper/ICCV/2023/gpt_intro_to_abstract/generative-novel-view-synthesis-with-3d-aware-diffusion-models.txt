In this work, we address multiple open problems in novel view synthesis (NVS) by proposing a diffusion-based few-shot NVS framework. This framework aims to generate long-range sequences far from the input views and handle both individual objects and complex scenes. Existing few-shot NVS approaches can generate geometrically consistent renderings near input views but struggle with extrapolation and unbounded scenes. Our proposed framework leverages generative priors to handle the innate ambiguity in completing unobserved portions of the scenes. We present a novel view synthesis method that extends 2D diffusion models to be 3D-aware by conditioning them on 3D neural features extracted from input images. We demonstrate that our approach can generate realistic novel views from as little as a single input image on a wide variety of datasets. Additionally, we showcase that our method can generate long trajectories of realistic, multi-view consistent novel views without the blurring of regression models or the drift of pure generative models. Overall, our contributions include a novel 3D-aware diffusion-based NVS framework and the ability to handle both geometrically consistent and generative rendering in various challenging real-world scenarios.