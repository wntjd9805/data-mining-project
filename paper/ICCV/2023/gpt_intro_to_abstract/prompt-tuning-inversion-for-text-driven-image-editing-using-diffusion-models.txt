Abstract:Text-based image editing is a significant problem in image processing that aims to align the visual content of an input image with target text prompts. In recent years, there has been increasing attention on this problem, and several methods based on text-to-image generation have been developed. However, these methods are limited to domains where the models are trained. To address this limitation, we propose a text-driven image editing method based on open-sourced large-scale language-image models (LLIMs). Our method focuses on achieving both editability and fidelity in image editing tasks. We introduce a Prompt Tuning Inversion method that encodes the information of the input image into a conditional embedding, enabling the reconstruction of the original image with diffusion models. This reconstruction process facilitates the sampling of edited images with high fidelity. Our proposed method requires only an input image and a target text prompt for editing, eliminating the need for user-provided masks or source descriptions of the input images. We compare our method against state-of-the-art techniques and demonstrate its superiority in terms of the trade-off between editability and fidelity.