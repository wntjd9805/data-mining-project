Event cameras have gained significant attention in recent years due to their ability to capture pixel intensity fluctuations with high temporal resolution, low latency, and high dynamic range. These cameras have been widely adopted in various applications, such as object detection and depth/optical flow estimation. In particular, event cameras show promise for object tracking due to their distinctive sensing mechanism.However, event data lacks visual cues, such as color and texture, that are easily captured by RGB data. As a result, object tracking using only event-based vision still suffers from relatively inferior performance. To address this, researchers have started investigating cross-modal object tracking methods that leverage both RGB and event data. However, there is a significant distribution gap between RGB and event data, presenting challenges in designing algorithms that can effectively model cross-modal information.Previous cross-modal trackers have focused on robust cross-modal fusion modules, which can be cumbersome and limit performance. In this paper, the authors propose investigating the use of pre-trained powerful vision Transformers (ViTs) for cross-modal object tracking. However, pre-trained Transformers with RGB data may not fully capture the feature interaction across RGB and event data due to the distribution gap between the two modalities.To address this limitation, the authors propose a plug-and-play training technique for augmenting the pre-trained Transformer used as the embedding backbone for RGB-event object tracking. They introduce a cross-modal mask modeling strategy that randomly masks or pops out multi-modal tokens to encourage interaction between the remaining cross-modal tokens. A regularization term is also proposed to guide the training of each attention layer, promoting essential feature interaction and cross-modal correlation.The proposed techniques are applied to state-of-the-art one-stream and two-stream Transformer-based tracking frameworks, resulting in significantly improved tracking performance. The contributions of this paper include the mask modeling strategy, theoretical orthogonal high-rank regularization, and new state-of-the-art baselines for RGB-event object tracking.Overall, this paper provides insights into leveraging pre-trained powerful ViTs for processing and analyzing cross-modal data, with potential implications for future research in this field.