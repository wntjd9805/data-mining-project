This paper introduces the problem of long-tailed data distribution in datasets, where a few classes have significantly more instances than the remaining classes. Training on such class-imbalanced datasets can result in biased models with performance drops in classification tasks. Previous approaches such as data resampling and loss reweighing have been applied, but they cannot fully leverage all the head-class samples. Recent work has shown that supervised contrastive learning (SCL) can achieve state-of-the-art performance in long-tailed recognition. However, methods enforcing class-balance often come with the issue of instance-imbalance, where each individual instance of tail classes has a greater impact on training than that of head classes. This can result in performance degradation, as limited samples in tail classes may not be representative of the whole class. To address this issue, this paper proposes subclass-balancing contrastive learning (SBCL), a novel approach that achieves both instance- and subclass-balance by exploring the head-class structure in the representation space. SBCL utilizes a bi-granularity contrastive loss that enforces samples to be closer to samples from the same subclass and closer to samples from a different subclass of the same class. Experimental results demonstrate the effectiveness of SBCL in handling class imbalance in visual recognition tasks, including image classification, object detection, and instance segmentation.