Depth sensing is crucial for various applications in computer science, including autonomous driving, robot navigation, and scene reconstruction. However, the depth images acquired from commonly used sensors often contain invalid areas due to reflections, extreme distances, bright light, and other environmental noises. These invalid areas can have significant impacts on subsequent processes. Therefore, depth completion, which aims to fill in the invalid data, has become necessary in many depth-based applications.Existing approaches to depth completion based solely on raw depth images have limitations due to the absence or uncertainty of invalid data. Researchers have explored incorporating RGB information to guide the depth completion process. One approach fills invalid pixels based on their valid neighbors using specific rules, but these methods are often slow and not sufficiently accurate. Another approach utilizes deep neural networks to predict invalid pixels, extracting depth and color features from RGB-D data and fusing them to complete the depth map. This approach has shown significant progress compared to traditional methods and is widely used in recent works.However, there are two main challenges in the deep learning approach to depth completion. Firstly, the vanilla convolution operation does not handle invalid values properly, leading to visual artifacts during reconstruction. Partial convolution has been proposed to address this issue, but it still has limitations. Additionally, the fusion of color and depth information can have both positive and negative effects. Existing models often concatenate latent features directly, but depth-irrelevant color features can interfere with depth prediction.To address these challenges, we propose a new framework for depth completion based on an UNet-like architecture. Our framework incorporates depth and color features extracted from two parallel encoding branches, which are then merged with skip connections in the decoding stage. The fusion of the two branches is guided by our proposed Attention Guided Gated Convolution (AG-GConv) module, which learns joint contextual attention from color and depth values. We also introduce the Attention Guided Skip Connection (AG-SC) module to filter out depth-irrelevant color features during depth reconstruction. Our experimental results demonstrate that our model outperforms state-of-the-art methods on popular benchmark datasets.In summary, our work presents a dual-branch multi-scale encoder-decoder network that combines depth and color features for high-quality depth completion. We propose the AG-GConv module to address the impact of invalid depth values on feature learning and the AG-SC module to reduce interference from depth-irrelevant color features. Our experiments show the effectiveness of our model on various benchmark datasets.