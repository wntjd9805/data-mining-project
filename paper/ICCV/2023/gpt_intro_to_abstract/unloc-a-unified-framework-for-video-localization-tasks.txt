This paper introduces the concept of adapting large-scale, contrastively trained image-text models to untrimmed video understanding tasks that involve localization. While pretrained models like CLIP have been successful in trimmed video tasks, their application to long, untrimmed videos is still in the early stages. The challenges of localization in long videos include the need to exploit fine-grained temporal structured information and the detection of specific actions in a background-dominant environment. Existing approaches often involve two-stage processes or the use of temporal features. In contrast, this paper proposes a one-stage, end-to-end trainable approach using a CLIP two tower model. The focus is on three different video localization tasks - Moment Retrieval, Temporal Action Localization, and Action Segmentation. By leveraging a two-tower model with a video-text fusion module, mid-level fusion of text and visual tokens is achieved without the need for external proposals. The paper showcases state-of-the-art results across all three tasks and conducts thorough ablation studies to analyze modeling choices.