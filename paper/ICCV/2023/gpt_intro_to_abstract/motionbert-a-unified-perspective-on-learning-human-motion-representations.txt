Perceiving and understanding human activities in videos is a key goal of machine intelligence. Previous research has focused on specific tasks, such as skeleton keypoints or action classes, but these tasks are often treated in isolation. This paper proposes a unified framework for modeling human-centric video tasks by leveraging the commonalities across different tasks. The challenge lies in the heterogeneity of available data resources, such as motion capture systems and action recognition datasets. To address this, the authors introduce a two-stage framework consisting of pretraining and finetuning. In the pretraining stage, a motion encoder is trained to recover 3D motion from corrupted 2D skeleton sequences. This enables the encoder to capture human motion commonsense. The motion representations learned from diverse data resources can then be shared and adapted for different downstream tasks. The contributions of this work are three-fold: a shared framework for learning human motion representations, a pretraining method leveraging heterogeneous data resources, and a dual-stream Transformer network for human motion modeling. Experimental results show that this approach outperforms task-specific methods in multiple downstream tasks.