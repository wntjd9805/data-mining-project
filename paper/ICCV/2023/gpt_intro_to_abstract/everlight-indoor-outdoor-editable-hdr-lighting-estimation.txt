In this computer science paper, the authors address the challenge of realistic blending of virtual assets in real imagery for applications such as special effects, augmented reality, and advanced image editing. The key challenge is to accurately predict lighting in the scene. Previous approaches have relied on engineered features but have since been replaced by learning-based techniques. The authors highlight the recent trends in the field, including the development of richer lighting representations such as environment maps, domain-specific approaches for indoor and outdoor scenes, and user-editable methods. The authors propose a framework called EverLight that unifies these trends. EverLight predicts a highly-detailed 360-degree environment map, works seamlessly on both indoor and outdoor scenes, and allows for user interaction and manipulation of individual HDR light sources. The proposed technique bridges the gap between HDR parametric lighting estimation and high-resolution field of view extrapolation by introducing a novel editable lighting co-modulation technique. Extensive experiments show that EverLight outperforms existing approaches in terms of both qualitative and quantitative results.