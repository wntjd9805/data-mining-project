Abstract:3D object detection from monocular images poses significant challenges due to the complex spatial circumstances. Existing methods that rely on local visual features around object centers are limited by the lack of long-range context and fail to capture implicit geometric cues. To address this, we propose MonoDETR, a depth-guided 3D detection framework. MonoDETR utilizes depth cues to guide the detection process and incorporates depth-guided modules in a transformer-based network. The framework includes parallel encoders for depth and visual representation learning, and a decoder for adaptive depth-guided detection. A depth cross-attention layer enables the capture of geometric cues and inter-object depth relations. As an end-to-end network, MonoDETR achieves state-of-the-art performance on the KITTI benchmark for monocular 3D object detection. Additionally, the depth-guided modules can be applied as a plug-and-play module for multi-view 3D detection on the nuScenes dataset, further demonstrating the effectiveness and generalizability of our proposed depth guidance. Overall, our contributions include the introduction of MonoDETR for depth-guided monocular 3D object detection, the use of a foreground depth map for object-wise supervision, and the development of a depth cross-attention layer for adaptive depth feature interaction.