Dense simultaneous localization and mapping (SLAM) has numerous applications in augmented reality, indoor robotics, and other domains. Traditionally, it relies on high-precision and high-resolution depth sensors, limiting its widespread use due to size, weight, and price constraints. However, light-weight time-of-flight (ToF) sensors have become integrated into many smartphone models, offering a cost-effective and energy-efficient alternative. This paper introduces a novel SLAM system that utilizes the signals from light-weight ToF sensors, along with RGB images, to achieve accurate pose tracking and dense reconstruction. The light-weight ToF sensors can only provide coarse measurements in low resolution, which poses challenges for existing SLAM systems designed for accurate and pixel-wise depth inputs. To address this, the proposed system incorporates a multi-modal implicit scene representation that supports rendering both zone-level signals from light-weight ToF sensors and pixel-wise RGB/depth images. A temporal filtering technique is also introduced to enhance the accuracy of the light-weight ToF signals and depth predictions. Experimental results demonstrate the effectiveness of the proposed system in exploiting light-weight ToF sensors, achieving competitive performance in camera tracking and dense scene reconstruction compared to existing methods.