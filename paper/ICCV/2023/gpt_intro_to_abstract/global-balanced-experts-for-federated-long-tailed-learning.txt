Federated Learning (FL) is a collaborative training method that aims to develop a global model by utilizing decentralized data from multiple clients. However, FL systems often suffer from performance degradation when faced with long-tailed data distributions, which are prevalent in various realistic scenarios. Existing techniques to address the federated long-tailed problem, such as loss re-weighting, client clustering, and client selection, often rely on sensitive information that may not be available in realistic applications and are less effective on large-scale imbalanced datasets.In this paper, we propose a Global Balanced Multi-Expert (GBME) framework to tackle the federated long-tailed issue without requiring additional information beyond the standard FL. The GBME framework leverages class-prior based re-balance algorithms and introduces a global proxy based on local proxies derived from accumulated gradients of clients. This allows for the division of clients into different groups corresponding to different experts in a multi-expert model. During training, the corresponding expert for each client learns balanced knowledge using the global proxy, while other experts are frozen to maintain their learned knowledge. A multiple selection strategy enables clients to implicitly interact with other groups to aggregate balanced knowledge.To enhance privacy protection, we also present a GBME-p algorithm based on differential privacy (DP), utilizing random Gaussian noises added to the weights of the final fully connected (FC) layer for local proxy computation at the clients' side before uploading. Experimental results on benchmark datasets demonstrate that the GBME framework outperforms previous state-of-the-art methods without requiring additional private information. Additionally, GBME-p achieves superior performance while preserving privacy under the protection of differential privacy.