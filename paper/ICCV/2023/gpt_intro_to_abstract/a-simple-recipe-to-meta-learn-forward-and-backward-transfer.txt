Continual learning is a problem of updating and consolidating knowledge when learning from data with a continual distribution shift. This problem is crucial for achieving lifelong learning systems in which an agent encounters drifts across and within learning environments. However, deep supervised learning algorithms have difficulty retaining learned information when trained with non-stationary sequences of data, leading to catastrophic forgetting. Traditional algorithms for continual learning, such as replay methods, regularization-based methods, and parameter isolation methods, have limitations and often only optimize to mitigate forgetting. Recently, meta-learning approaches have shown promising results, but they often require high computational cost and strong assumptions. In this paper, we propose a new meta-learner called SiM4C that takes a different approach. SiM4C uses a lightweight single-step inner-loop in the meta-optimization and preserves large amounts of unseen future data for each task. By optimizing with both past and future data, SiM4C achieves forward and backward transfer. Our empirical evaluation shows that SiM4C outperforms state-of-the-art algorithms and provides near-universal performance benefits to memory-based algorithms. SiM4C is efficient, stable, and can be easily integrated into existing systems. We provide resources and the full implementation of SiM4C to support further research in scalable and applicable meta-learning for continual learning.