Large-scale pre-trained vision-language models have achieved impressive performance in zero/few-shot learning tasks. These models train two uni-modal encoders on massive amounts of image-text pairs to exploit cross-modal alignments in the semantic space. However, identifying appropriate manually designed prompts for these models requires domain expertise and prompt engineering. To address this, we propose a prompt tuning framework that incorporates external knowledge to improve the generalizability of vision-language models on unseen object categories. We design two types of knowledge-aware prompts: discrete prompts that directly describe the visual appearance of the category and learnable continuous prompts that provide contextual information. Additionally, we propose a task-aware visual adaptation head that refines image features by attending to salient visual cues relevant to the target task. Experimental results on popular image datasets demonstrate the effectiveness of our method, which significantly outperforms state-of-the-art methods in the base-to-new generalization setting.