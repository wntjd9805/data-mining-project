Computer Vision research has extensively focused on fashion-related problems, including recognition and retrieval of clothing items, recommendation of similar garments, and virtual try-on of clothes and accessories. In recent years, there has been a growing interest in text-conditioned image editing, where the goal is to generate a new clothing item based on a textual description. However, previous works in this area have mainly employed GAN-based approaches. This paper introduces a new framework called multimodal-conditioned fashion image editing, which allows for the generation of new garments based on multimodal prompts while preserving the identity and body shape of a given person. The proposed architecture, called Multimodal Garment Designer (MGD), incorporates a denoising network that can be conditioned by multiple modalities and takes into account pose consistency between input and generated images. To address this task, existing fashion datasets are extended with textual descriptions and garment sketches using a semi-automatic annotation framework. Experimental results on two proposed fashion benchmarks demonstrate that the proposed architecture outperforms competitors and baselines, both quantitatively and qualitatively, as per human evaluations. The contributions of this work include the introduction of the novel task of multimodal-conditioned fashion image editing, the development of a human-centric generative architecture based on latent diffusion models, the extension of existing datasets with multimodal data, and the demonstration of improved performance in terms of realism and coherence with multimodal inputs.