Deep Neural Networks (DNNs) have achieved remarkable success in various visual tasks, but they heavily rely on the underlying distribution of training data. Domain adaptation has emerged as a focus area in machine learning to address the challenge of deploying DNNs in real-world scenarios with new situations. Existing domain adaptation methods require access to the source domain dataset, which is becoming impractical due to privacy concerns and limited availability of source data. To overcome this, source-free domain adaptation (SFDA) has been introduced, which aims to transfer knowledge from a prior domain to a new domain without accessing the source dataset. In this work, we focus on the security of the target domain in SFDA and investigate the vulnerability of the target model against backdoor attacks. We propose a novel defense scheme called Secure Source-Free Domain Adaptation (SSDA) that effectively defends against backdoor attacks tailored for SFDA. SSDA consists of two key components: a static defensive compression of the source model using spectral norm-based ranking, and a dynamic knowledge transfer from an auxiliary model to recover benign accuracy. We evaluate our defense extensively and demonstrate its effectiveness in defending against strong backdoor attacks with minimal accuracy degradation compared to vulnerable SFDA methods. We also show that SSDA can successfully adapt the target domain regardless of the benign or malicious nature of the source model.