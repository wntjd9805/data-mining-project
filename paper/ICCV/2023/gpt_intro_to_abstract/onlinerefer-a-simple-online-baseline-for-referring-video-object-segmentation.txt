This paper introduces the concept of referring video object segmentation (RVOS), which aims to segment an object described in natural language within a streaming video. RVOS has gained significant attention in the computer vision community due to its potential benefits in video editing and human-computer interaction. The main challenge of RVOS is associating all frames in order to construct an efficient video representation and facilitate cross-modal understanding of video and language. This paper compares current methods for RVOS, including mask propagation, offline methods, and the proposed query-propagation method. The complexity and performance of existing methods are found to be unsatisfactory. Offline methods, which divide the video into non-overlapping clips and generate object masks for each clip, have shown state-of-the-art performance in RVOS. However, they lack inter-frame association and have complex temporal feature modeling. In contrast, the proposed query-propagation method simplifies the pipeline and utilizes query-based Transformer methods to build cross-modal features and retrieve referent objects. However, the limited interaction between frames and the need for separate clip-level prediction hinder the performance of query-sharing methods. To address these limitations, this paper introduces the OnlineRefer framework, which utilizes continuous query propagation to link all video frames. By associating referring targets with their precursors on previous frames and leveraging spatial regional priors from the last frame, the proposed approach improves referring prediction accuracy without complex temporal modeling. Experimental evaluations on four benchmarks demonstrate the effectiveness and superiority of the proposed method compared to previous offline methods, establishing it as a new baseline for the RVOS community.