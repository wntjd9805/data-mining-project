Convolutional Neural Networks (CNNs) have achieved significant success in various computer vision tasks. However, as CNNs become larger in size, they pose challenges in terms of computational and storage burdens. On the other hand, there is a growing demand for deploying CNNs on mobile and embedded devices, which have limited hardware capabilities. To overcome these challenges, many works have focused on reducing the size of CNNs, particularly through channel pruning.This paper introduces a novel partial regularization technique for channel pruning in CNNs. Unlike existing one-shot pruning methods, which prune trained models directly, our approach aligns model weights and the discovered sub-network during the training process. This alignment reduces the gap between the sub-network and the original model, enabling better performance. Furthermore, unlike soft pruning methods, our approach utilizes all model structures for training, and a partial group lasso regularization is applied only to selected weights.Inspired by the nonmonotone proximal gradient (NPG) method, we use a proximal gradient approach to solve the partial regularization problem. To balance the regularization strength for different layers, a scalar is added due to the varying number of pruned channels. The partial regularization is inserted in the middle of the training process to preserve the capacity of the original model and avoid early weight vulnerability.An architecture generator, parameterized by neural networks, is trained to select the sub-network structure and guide the partial regularization. Experimental results on CIFAR-10 and ImageNet demonstrate that our method outperforms existing channel pruning methods on ResNets and MobileNet-V2. In summary, the contributions of this paper include proposing a method for aligning the sub-network with the original model through partial regularization, introducing an architecture generator for selecting the sub-network structure, and demonstrating improved performance compared to existing channel pruning methods.