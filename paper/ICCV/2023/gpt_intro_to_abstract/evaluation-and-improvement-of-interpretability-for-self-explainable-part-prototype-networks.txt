This paper introduces part-prototype networks, which are deep self-explainable models for image classification. These networks define multiple trainable prototypes that represent specific object parts and compare object parts across images to make predictions, emulating human perception. While part-prototype networks have shown excellent performance in interpretable decision-making, their interpretability has been demonstrated qualitatively with limited visualization examples, and the learned prototypes lack adequate interpretability. This is due to the inconsistency and instability of the prototypes across different images and perturbations. To address this, the paper proposes two evaluation metrics, the "consistency score" and "stability score", to quantitatively and objectively evaluate the interpretability of part-prototype networks. The metrics measure the extent to which prototypes are mapped to the same object part across images and the robustness of the mappings under image perturbations. The paper also proposes an improved part-prototype network with modules that enhance the alignment and matching of prototypes with object parts. Extensive experiments demonstrate the effectiveness of the proposed model in terms of interpretability and accuracy on different datasets. The findings suggest a positive correlation between interpretability scores and accuracy, addressing the conflict between interpretability and accuracy in prior methods. Overall, this work establishes a benchmark for evaluating the interpretability of part-prototype networks, proposes an enhanced model, and showcases its superior performance in both interpretability and accuracy.