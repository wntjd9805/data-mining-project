Autonomous robots, such as UAVs and UGVs, are gaining popularity in various applications and require the capability to navigate and execute tasks autonomously in complex environments. This has motivated research on vision-based localization, particularly in environments with poor GPS signals. This paper focuses on ground-to-satellite image-based localization, aiming to estimate the location and orientation of a ground camera by matching a ground-level image with a satellite map. Existing image retrieval techniques provide rough pose approximations but lack accuracy. Recent works have attempted to improve localization accuracy by estimating relative translation and rotation, but have not addressed the orientation estimation. To address these limitations, this paper presents a new framework that allows for dense searching of vehicle locations in the solution space. It introduces a neural network-based pose optimizer for accurate rotation estimation and a dense search mechanism for translation estimation, producing a probability map of vehicle locations. Additionally, it proposes a geometry-guided cross-view transformer for ground-to-overhead view feature synthesis, which combines deterministic geometric and data-driven correspondences. The contributions of this paper include a decoupled localization framework, a neural pose optimizer, a dense search mechanism, and a geometry-guided cross-view transformer. Experimental results demonstrate state-of-the-art performance on benchmarks.