Video grounding is a challenging task in multimedia retrieval, which aims to identify semantically corresponding timestamps in an untrimmed video based on a given query. The existing contrastive learning-based methods for video grounding have limitations in modeling complex cross-modal correlations and semantic information. In this paper, we propose a novel Geodesic and Game Localization (G2L) framework for video grounding. G2L addresses the issues of semantic overlapping and sparse annotation dilemma in current methods. We introduce the concept of geodesic distance between video moments to measure similarity and guide the maximizing mutual information. We also utilize game theory and Shapley interactions to prevent the model from confusing similar video moments. Experimental results on three public datasets demonstrate the effectiveness of our G2L framework.