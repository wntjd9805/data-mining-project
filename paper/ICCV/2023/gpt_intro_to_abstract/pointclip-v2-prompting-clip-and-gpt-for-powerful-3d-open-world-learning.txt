In recent years, the advancement of spatial sensors has attracted attention in academia and industry. Understanding point clouds, the major data form in 3D, has led to progress in various tasks including 3D classification, segmentation, detection, and self-supervised learning. However, the complexity and diversity of open-world circumstances pose a challenge as 3D data often contains 'unseen' objects that have not been defined or trained by existing systems. This paper explores the integration of CLIP and GPT-3 models to address this challenge and proposes PointCLIP V2, a framework for unified 3D open-world understanding. PointCLIP V2 introduces a realistic projection method to generate CLIP-preferred images from 3D point clouds, enhancing the representation capacity of CLIP's visual encoder. Additionally, a 3D-oriented command is used to prompt GPT-3 and generate text with rich 3D semantics for CLIP's textual encoder. Experimental results demonstrate that PointCLIP V2 achieves superior performance in zero-shot 3D classification compared to PointCLIP. The proposed framework can be extended to other 3D open-world tasks such as part segmentation and object detection. This work contributes to the advancement of cross-modal learning in the 3D domain, bridging the gap between 2D, 3D, and language understanding.