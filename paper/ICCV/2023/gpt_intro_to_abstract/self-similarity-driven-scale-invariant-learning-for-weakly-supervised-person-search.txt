Recent years have seen significant advancements in person search, which involves matching individuals in real-world scene images. This task typically includes person detection and re-identification (re-id). Existing methods achieve high performance by training in a fully supervised setting, where both bounding boxes and identity labels are required. However, annotating both of these in large-scale datasets is time-consuming and labor-intensive. As a result, this paper focuses on weakly supervised person search, which only requires bounding box annotations. To address this, the paper proposes a two-step person search model consisting of a supervised detection model and an unsupervised re-id model. However, this approach suffers from low efficiency due to high computational costs and inconvenience during testing. To overcome this, the paper introduces a one-step method that can be trained and tested more effectively and efficiently. This approach utilizes a Region Siamese Network and visual context clues to learn consistent and discriminative features without the need for manual cropping. However, these one-step methods fail to consider the scale variation problem, where a person's images can vary in scale due to different distances and camera views. This problem affects the pseudo label prediction and subsequent person matching. To address this, the paper proposes a novel Self-similarity driven Scale-invariant Learning (SSL) framework. The SSL framework includes two branches: the Main Branch and the Multi-scale Exemplar Branch. The Main Branch extracts instance features using a detector, but the differing scales of detected persons pose a challenge for matching. To overcome this, the Multi-scale Exemplar Branch crops person images using given bounding boxes and generates binary masks. These images are then resized to multiple fixed scales, and a scale-invariant loss is formulated through hard exemplar mining. This allows the Main Branch to learn scale-invariant features. To further improve feature discriminability, the paper introduces dynamic multi-label learning to explore information in unlabeled data. This approach progressively finds true labels for unlabeled data and is adaptable to different datasets. The scale-invariant loss and multi-label learning loss are integrated and jointly optimized. The proposed method is evaluated on the PRW and CUHK-SYSU datasets, achieving state-of-the-art performance. The contributions of this paper include the novel SSL framework, the scale-invariant loss, and the dynamic multi-label learning approach.