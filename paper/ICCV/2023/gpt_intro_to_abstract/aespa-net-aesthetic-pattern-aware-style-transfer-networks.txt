Style transfer is a technique in computer science that aims to render the content of a source image using the elements of a style image. Previous research has shown promising results in artistic style transfer by using global transformation methods and patch-wise swapping approaches. However, a major challenge for these methods is the lack of semantic information on the content images, which often leads to distortion artifacts. To address this challenge, recent advances in style transfer methods have incorporated attention mechanisms to aggregate elements from a style image based on semantic correspondence between local patches of the images. However, these attention mechanisms often produce abnormal patterns or evident artifacts. In this paper, we propose remedies for more delicate artistic expression and improving the attention mechanism. We introduce a novel metric called pattern repeatability, which quantifies the frequency of repeating patches in a style image and helps determine whether to bring more effects from attention-based stylization or global statistic-based stylization. We also propose a self-supervisory task to encourage the attention module to capture broader corresponding regions of style images. Additionally, we modify the style loss to be computed between patches with the proper size. Experimental results demonstrate that our framework, AesPA-Net, outperforms state-of-the-art style transfer methods and that pattern repeatability aligns closely with human perception.