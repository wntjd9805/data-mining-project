In this paper, we address the challenges of task-driven object detection, where the goal is to detect object instances suitable for a specific task rather than detecting object categories. We observe that previous methods that learn the mapping between objects and tasks from visual context features or categories do not achieve satisfactory results. To overcome this, we propose to acquire visual affordance knowledge of the task and utilize it to bridge the task and object instances. We introduce a novel multi-level chain-of-thought prompting approach to elicit visual affordance reasoning from large language models. At the object level, we prompt the model to generate representative object examples for the task. At the affordance level, we generate rationales for why these objects afford the task and facilitate the model to reason and summarize visual affordances beyond object examples. We demonstrate that our approach successfully detects object instances suitable for the task, even if they do not belong to the commonly used object categories. Additionally, we argue that visual affordance knowledge can improve object localization, and propose a knowledge-conditional object detection framework that leverages visual affordance knowledge to guide bounding box regression. We introduce the CoTDet network, which effectively utilizes visual affordance knowledge for object detection and can be extended to task-driven instance segmentation. Our experimental results show that CoTDet outperforms state-of-the-art methods by a large margin in terms of object detection performance and can provide rationales for object detections. Overall, our contributions include the proposal of acquiring and utilizing visual affordance knowledge, the development of the multi-level prompting approach for reasoning with visual affordances, the introduction of a knowledge-conditional detection framework, and the demonstration of superior performance and rational explanations provided by the CoTDet network.