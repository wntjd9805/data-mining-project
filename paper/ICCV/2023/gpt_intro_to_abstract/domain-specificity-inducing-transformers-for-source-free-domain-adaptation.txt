Machine learning models often struggle to generalize to new domains, resulting in poor performance in real-world applications. Unsupervised domain adaptation (DA) techniques aim to address this issue by transferring knowledge from a labeled source domain to an unlabeled target domain. However, these techniques typically require concurrent access to source and target data, which is often impractical due to data privacy concerns. In this paper, we focus on Source-Free DA, which operates under the practical constraint of sharing only the source model between a vendor and a client, without sharing any data. Conventional DA methods aim to learn domain-invariant features, but this does not always lead to optimal performance, as certain domains may be far from the support of a domain-invariant model. We propose the concept of domain-specificity to improve target adaptation performance. In source-free DA, preserving task knowledge from the source domain is crucial. We argue that it is equally important for a model to learn domain-specific information while preserving task-specific knowledge. To achieve this, we develop a framework that enables the disentanglement of task-specific and domain-specific factors, allowing better control over them. We propose a novel Domain-Specificity inducing Transformer (DSiT) framework that utilizes query weights to enable disentanglement and incorporates a domain-specificity disentanglement criterion. We demonstrate the effectiveness of DSiT on source-free benchmarks, achieving state-of-the-art performance in single-source, multi-source, and multi-target DA. This work also introduces the first source-free DA benchmarks for transformers.