Egocentric vision aims to enable machines to interpret real-world data from a human's perspective, with applications such as action recognition, object-state changes, and localizing action instances. However, challenges such as occlusion, short time periods, and motion blur hinder the accurate understanding of video content from RGB frames alone. Previous methods have shown that modeling hand-object interactions and leveraging multiple modalities improve performance but are cumbersome in practice due to assumptions about available modalities and compute budget limitations. In this paper, we propose a different approach of transferring multimodal knowledge to a unimodal RGB-based action recognition model through knowledge distillation. We demonstrate that the student model taught by a multimodal teacher achieves better accuracy and calibration than models trained from scratch or in an omnivorous fashion. Our approach maintains performance comparable to larger models and allows for computationally cheaper inference setups.