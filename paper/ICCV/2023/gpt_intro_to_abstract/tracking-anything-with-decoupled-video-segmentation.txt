Video segmentation is a crucial task in computer vision that aims to segment and associate objects in a video. Existing approaches for video segmentation train end-to-end video-level networks on annotated video datasets, achieving significant progress on common benchmark datasets. However, these datasets have small vocabularies, raising questions about the scalability of end-to-end paradigms to large-vocabulary or open-world video data. In this paper, we propose a decoupled video segmentation approach that reduces the reliance on target training data by leveraging external data from outside the target domain. Our approach combines task-specific image-level segmentation with task-agnostic temporal propagation, allowing us to use a cheaper image-level model for the target task and a universal temporal propagation model that generalizes across tasks. We develop a (semi-)online bi-directional propagation algorithm that denoises image-level segmentation and combines results from temporal propagation and in-clip consensus, leading to temporally more coherent and potentially better results than those of an image-level model alone. We emphasize that our decoupled approach acts as a strong baseline in scenarios where an image model is available but video data is scarce, rather than aiming to replace end-to-end video approaches. Our approach achieves state-of-the-art results in large-scale video panoptic segmentation and open-world video segmentation, and performs competitively in referring video segmentation and unsupervised video object segmentation tasks.