This paper addresses the challenges faced by Vision & Language (VL) models in understanding Visual Language Concepts (VLC) beyond object nouns. The authors propose a method called VL data synthesis, which involves generating synthetic visual and text data to enhance the VLC and compositionality aspects. They create a large synthetic dataset called SyViC, consisting of million-scale synthetic image-text pairs with rich textual captions. To effectively leverage this data, the authors propose a combination of domain adaptation, parameter efficient fine-tuning, and model averaging techniques. The experimental results and ablation study demonstrate significant improvements in VLC understanding and compositional reasoning. The contributions of this paper include the SyViC dataset, a general finetuning strategy, and performance improvements on popular VL benchmarks. Supplementary material can be found in the associated arXiv document.