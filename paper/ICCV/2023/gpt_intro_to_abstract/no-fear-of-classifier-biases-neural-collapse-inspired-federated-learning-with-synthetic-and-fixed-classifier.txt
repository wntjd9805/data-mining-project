Federated learning (FL) is a distributed training paradigm that allows collaborative training from multi-source datasets without transferring raw data. FL has shown promising prospects in various applications, but the heterogeneity of clients' datasets in FL training poses a challenge to the generalization of the global model. Classifier biases caused by non-IID data have been identified as the primary reason for the degradation in FL. Previous attempts to mitigate classifier biases have not effectively addressed the issue during training. This paper proposes a neural-collapse-inspired approach to resolve the classifier bias dilemma in FL. The authors introduce a synthetic simplex equiangular tight frame (ETF) as a fixed classifier for all clients, enabling unified and optimal feature representations even under high heterogeneity. The proposed method, FEDETF, incorporates effective modules such as a projection layer and a balanced feature loss to achieve high generalization performance of the global model during FL training. A novel fine-tuning strategy is also introduced to enhance personalization. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets demonstrate the effectiveness of the proposed method in both generalization and personalization, outperforming strong baselines. This paper contributes to the understanding of data heterogeneity in FL and provides a novel approach to address classifier biases and improve the performance of FL models.