Video Question Answering (VideoQA) is a challenging task that aims to answer natural language questions based on information available in videos. Recent advancements have utilized large-scale vision-language pre-trained (VLP) models to enhance VideoQA performance. However, pre-training these models requires a large number of video-text pairs and expensive computational costs. In this paper, we propose Tem-adapter, an adapter network that leverages the interaction between visual and textual modalities to learn temporal dynamics and complex semantics. Unlike existing methods, Tem-adapter introduces a language-guided autoregressive task to facilitate the learning of temporal dynamics and incorporates cross-modal interactions to refine textual representation. Our approach consists of a visual Temporal Aligner and a textual Semantic Aligner, both utilizing Transformer models. Experimental results on two VideoQA benchmarks demonstrate the effectiveness of Tem-adapter in adapting image-language pre-trained models for the VideoQA task.