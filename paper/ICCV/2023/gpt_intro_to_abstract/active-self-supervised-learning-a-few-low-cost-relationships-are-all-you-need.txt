This paper focuses on the goal of learning representations of data that can be used to solve multiple tasks in AI research. The paper explores different training settings for deep neural networks, including layerwise, reconstruction based, and self-supervised learning methods. However, incorporating a priori knowledge into self-supervised learning frameworks remains a challenge. In this study, the authors propose a new approach that uses a similarity graph to redefine existing self-supervised learning losses. This framework provides a spectrum where self-supervised learning and supervised learning are two extremes, connected by the similarity matrix. The authors also introduce a new active learning strategy called Positive Active Learning (PAL), where instead of asking for labels, the oracle is asked if two inputs belong to the same class or not. PAL is shown to be a more efficient and low-cost approach to annotation compared to traditional active learning. The paper provides a unified learning framework based on the similarity graph concept, presents a generic PAL algorithm, and demonstrates the benefits of PAL in active learning. The study's contributions are proven and code to reproduce experiments is provided.