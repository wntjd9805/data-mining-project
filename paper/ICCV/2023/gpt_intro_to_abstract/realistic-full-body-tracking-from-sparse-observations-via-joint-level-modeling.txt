Driving human avatars in VR/AR can create a more immersive user experience by bridging the gap between the physical and virtual worlds. However, current capture settings only track the head and hands, leading to an underconstrained problem when driving the full-body avatar. Previous studies have shown promising results but are not suitable for real-time applications. To address this, we propose a two-stage joint-level modeling framework that explicitly models correlations between body joints for more accurate and smoother human motion estimation. In the first stage, we model joint-level features, including joint-rotation and joint-position features. These features are then utilized in the second stage, where they are processed using a transformer-based network to capture joint-level dependencies and recover full-body motions. We evaluate our approach on the AMASS dataset and conduct real-data experiments, showing that our method significantly outperforms existing state-of-the-art approaches in terms of accuracy and smoothness. We also introduce a set of tailored loss terms to enhance the efficacy and generalizability of our body-tracking system. Overall, our contributions include proposing a novel two-stage network, designing an effective feature extractor, and introducing specialized loss terms for full-body motion estimation.