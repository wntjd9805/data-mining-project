The field of 3D image synthesis has had a significant impact on industries such as art, product design, and animation. While recent techniques like Neural Radiance Field (NeRF) have enabled large-scale 3D content production, precise and localized editing of object shapes and colors remains a challenge. Previous attempts at 3D object editing have offered limited and non-versatile options, only allowing simple texture and shape transformations of entire objects. Another proposed method involves fine-tuning a single NeRF per scene with a CLIP-driven objective, but it struggles to edit object shape effectively. To address these limitations, we propose a novel method for localized object editing that allows modification of 3D objects using text prompts. Our approach involves parameterizing specific regions in the 3D representation and blending the original object with an editable NeRF architecture specifically trained for natural rendering. We use a pretrained vision-language model to specify the areas to be modified. Our proposed architecture, called Blending-NeRF, combines a pretrained NeRF with an editable NeRF, allowing for precise and fine-grained localized editing of 3D objects. We introduce new blending operations to capture the degree of density addition, density removal, and color alteration. Extensive experiments demonstrate the superiority of our approach in terms of both qualitative and quantitative measures when compared to previous attempts and their simple extensions.