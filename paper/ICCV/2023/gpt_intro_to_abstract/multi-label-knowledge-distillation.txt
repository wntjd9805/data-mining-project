Multi-label learning (MLL) is a problem where each instance is assigned multiple class labels at the same time. Training large models is necessary to achieve desirable performance in MLL. However, deploying these large models on lightweight terminals like mobile phones is difficult due to computational resource constraints and the need for short inference time. To address this issue, knowledge distillation (KD) is used to improve the performance of a smaller network (student) by leveraging the knowledge from a larger network (teacher). Existing KD methods focus on multi-class classification and can be categorized into logits-based methods and feature-based methods. Logits-based methods minimize the difference between logits of the teacher and student models, while feature-based methods distill knowledge from intermediate feature maps. Extending existing KD methods to solve multi-label knowledge distillation (MLKD) problems is challenging. Logits-based methods do not work well in MLKD as the predicted probabilities based on the softmax function may not sum up to one in MLL. Feature-based methods tend to focus on major objects and neglect minor objects, resulting in suboptimal distillation performance. Previous works have attempted to utilize KD techniques for MLL, but they require specifically-designed network architectures or training strategies. In this paper, we propose a new MLKD method called L2D that combines logits distillation and label-wise embedding distillation. L2D exploits informative semantic knowledge compressed in the logits using the one-versus-all reduction strategy and maintains the structure of intra-class and inter-class label-wise embeddings. Experimental results on benchmark datasets demonstrate the effectiveness of our proposed method. Our contributions include the MLKD framework, the L2D approach, and extensive experimental results showcasing the effectiveness of our method.