Recently, pre-training deep models on large-scale datasets and fine-tuning them on diverse downstream tasks has become a popular approach in computer vision. This is because pretraining implicitly encodes useful prior knowledge, leading to improved accuracy and faster convergence in applications such as image classification and object detection. However, the issue of overfitting arises when labeled data is limited in the downstream task. To address this, various methods have been proposed, including direct fine-tuning and regularization-based approaches. However, these methods either neglect retaining the pretrained prior or impose explicit constraints without considering the semantic drift of pretrained features, resulting in suboptimal performance. In this paper, we propose a novel regularization-based framework called distribution regularization (DR) with semantic calibration (DR-Tune). Unlike existing methods, DR-Tune conducts distribution regularization on the downstream classification head instead of the encoder. The goal is to minimize the classification error of the downstream task head based on both the pretrained and downstream feature distributions. However, the dynamic update of the downstream model leads to semantic drift between the two distributions. To overcome this, we introduce the semantic calibration (SC) module, which aligns the pretrained and downstream feature distributions using a rotation matrix and class-level translation vectors. This alignment significantly reduces semantic drift and improves fine-tuning results. The advantages of DR-Tune are threefold: 1) DR does not impose explicit constraints on weights or intermediate feature maps, making optimization of the downstream encoder easier; 2) SC reduces semantic drift and decreases classification bias introduced by pretrained models, resulting in improved fine-tuning results; and 3) the combination of pretrained and downstream features leads to smoother classification boundaries and reduces the risk of overfitting. The main contributions of this paper are: 1) the proposal of DR-Tune, a fine-tuning framework that regulates the task-specific head using the pretrained feature distribution to address overfitting; 2) the design of the SC module to address semantic drift between pretrained and downstream feature distributions, reducing bias from regularization by pretrained models; and 3) extensive evaluation on classification datasets, demonstrating consistent performance improvement with different network structures and pretraining schemes.