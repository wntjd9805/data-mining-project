Supervised semantic segmentation methods have achieved great success in recent years, but they often suffer from performance drops when tested on out-of-distribution data. To address this problem, unsupervised domain adaptation (UDA) has emerged as a promising solution. However, collecting unlabeled data from target domains can be complex and using public data is often limited or forbidden in industrial contexts. In this paper, we propose a novel task called prompt-driven zero-shot domain adaptation (PØDA), where we adapt a segmentation model to a target domain using only a textual description (prompt) of the domain. We leverage the vision-language connections of the CLIP model to achieve this adaptation. Specifically, we use CLIP's latent space and a feature stylization mechanism to convert source-domain features into target-domain features, which improves the segmentation model's performance on unseen domains. We demonstrate the effectiveness of our method across different conditions and show that PØDA outperforms state-of-the-art one-shot unsupervised domain adaptation methods. Additionally, we show that PØDA can be applied to object detection and image classification tasks. Our contributions include introducing the PØDA task, proposing the Prompt-driven Instance Normalization (PIN) layer for feature augmentation, and showcasing the versatility and performance of our method in various scenarios.