This paper addresses the problem of text-to-motion retrieval, focusing on the cross-modal relationship between 3D human motions and natural language queries. The goal is to retrieve the most relevant 3D human motion from a gallery based on a text query. The paper highlights the importance of text-to-motion retrieval in various applications, such as indexing motion capture collections and aiding in text labeling for motions. Unlike motion synthesis models, which struggle to generate realistic sequences, a retrieval model ensures that the retrieved motion is always realistic. The paper introduces a framework for text-to-motion retrieval that incorporates a joint synthesis and retrieval approach, along with a powerful contrastive training technique. The proposed approach outperforms existing models and improves the performance of motion retrieval. The paper also discusses the challenges of text-motion data, especially the similarity between motion descriptions, and presents a negative sampling technique to address this issue. Additionally, the paper discusses a potential use case of the retrieval model in zero-shot temporal localization or moment retrieval from long motion sequences. The paper presents evaluation benchmarks, introduces a joint synthesis and retrieval framework, and provides extensive experiments to analyze the effects of each component. The code and models are publicly available.Overall, this paper contributes to the field of text-to-motion retrieval by proposing a novel framework and addressing various challenges and issues in the process.