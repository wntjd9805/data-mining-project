Deep neural networks (DNNs) have become widely used in daily life for various applications, such as automatic driving, facial payment, and computer-aided diagnosis. However, these DNN-based systems are vulnerable to security risks caused by adversarial examples. Adversarial examples are carefully crafted noise that can deceive the DNNs while remaining invisible to humans. Recent research has shown that physically deployed DNN-based systems are also exposed to such security risks. In this paper, we focus on physical attacks, which aim to modify the appearance of the target object in a physically deployable manner. We classify physical attacks into contact and contactless attacks. While contact attacks require the attacker to approach the target object, contactless attacks can modify the appearance of the target object without physical contact. Optical attacks are a type of contactless attack, but they are limited in their usability due to their reliance on dark environments. To address this limitation, we propose a Reflected Light Attack (RFLA) that utilizes sunlight as a light source. We use a mirror to reflect sunlight towards the target object and modulate the color and shape of the reflected light using colored transparent plastic sheets and paper cutouts. We present a general framework based on a circle to model and optimize the position, geometry, and color of the reflected light. Our contributions include the proposal of a novel reflect-light-based physical adversarial attack, the development of a general framework for optimizing attack performance, and a comprehensive investigation of the influence of geometry, position, and color on attack performance. Experimental results demonstrate the effectiveness of our proposed method using sunlight during the day and a flashlight in the absence of sunlight.