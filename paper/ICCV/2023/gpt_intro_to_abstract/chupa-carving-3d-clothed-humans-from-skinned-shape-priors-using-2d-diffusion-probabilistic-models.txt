The creation of digital avatars, 3D human characters used in various fields such as gaming and virtual reality, typically requires specialized 3D artists and a complex creation process. Recent advances in deep generative models have enabled the creation of high-quality images, but the use of these models for generating full-body, realistic 3D human avatars is still limited due to the lack of large-scale datasets. This paper proposes a novel approach that decomposes the 3D generation problem into 2D normal map generation and 3D reconstruction. The authors utilize a diffusion model to generate consistent normal maps for both the frontal and backside regions of the human mesh. These normal maps are then used for 3D reconstruction, resulting in a clothed, realistic human mesh. The paper also introduces a resampling scheme to improve the visual quality of the rendered normals and presents an extension of the pipeline for text-based generation of digital avatars. The proposed method, called Chupa, is capable of generating diverse digital avatars with realistic features and is evaluated against established benchmarks. The contributions of this work include a 3D generation pipeline leveraging the image generation capabilities of diffusion models, a strategy for generating view-consistent normal maps, and a method for text-based 3D avatar creation.