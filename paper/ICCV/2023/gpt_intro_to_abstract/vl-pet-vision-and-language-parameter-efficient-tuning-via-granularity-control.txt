Recently, there has been great success in pre-training transformer-based models on large-scale corpus and fine-tuning them for downstream tasks in various domains such as natural language processing (NLP), computer vision (CV), and vision-and-language (VL). However, as the size of pre-trained language models (PLMs) and the number of tasks increase, full fine-tuning and preserving task-specific copies of PLMs become expensive. To address this issue, parameter-efficient tuning (PET) techniques have been proposed to save model storage space. While PET has achieved significant advancements in NLP and CV, its potential in VL has not been fully explored. Specifically, PET techniques in VL lack VL-specific designs and mainly focus on discriminative tasks, limiting the generalization ability of PLMs. Existing PET techniques also fail to effectively control modular modifications for better performance on VL tasks and do not consider the functionality gap between encoders and decoders in PLMs used in VL tasks. To bridge these gaps, this paper introduces a novel Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework that addresses these issues. The framework utilizes a granularity-controlled mechanism to generate control over modular modifications and introduces lightweight PET module designs tailored for encoders and decoders in PLMs. Experimental results demonstrate that VL-PET significantly outperforms state-of-the-art PET techniques in image-text tasks. The framework's transferability is validated in video-text tasks, showcasing its efficiency, effectiveness, and the enhanced effect on existing PET techniques.