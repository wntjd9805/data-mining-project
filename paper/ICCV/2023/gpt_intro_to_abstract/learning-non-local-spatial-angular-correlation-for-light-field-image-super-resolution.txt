Light field (LF) cameras have the ability to record both intensity and direction of light rays, enabling various applications such as depth perception, view rendering, virtual reality, and 3D reconstruction. However, LF cameras face a spatial-angular trade-off, where they can either provide dense angular samplings with low-resolution sub-aperture images (SAIs), or capture high-resolution SAIs with sparse angular sampling. To address this problem, several methods have been proposed to enhance the angular or spatial resolution of LF images. In this paper, we focus on LF image super-resolution (SR), specifically generating high-resolution LF images from their low-resolution counterparts. Convolutional neural networks (CNNs) have been widely used for LF image SR and have shown superior performance compared to traditional approaches. Current CNN-based methods incorporate the complementary angular information from different views through various mechanisms. However, these methods struggle with scenes that have large disparity variations, which can be attributed to the contradiction between the local receptive field of CNNs and the requirement of incorporating non-local spatial-angular correlation in LF image SR. To overcome this challenge, we propose a simple yet effective method called EPIT that utilizes a Transformer-based network to learn the non-local spatial-angular correlation in LF image SR. By re-organizing 4D LF data into 2D epipolar plane images (EPIs), we can model the dependencies between each pair of pixels on EPIs and progressively incorporate the complementary information from all angular views. Our method achieves a global receptive field along the epipolar line, making it robust to disparity variations. Experimental results demonstrate the effectiveness of our method, showing superior performance on public LF datasets and improved robustness to disparity variations compared to existing state-of-the-art methods.