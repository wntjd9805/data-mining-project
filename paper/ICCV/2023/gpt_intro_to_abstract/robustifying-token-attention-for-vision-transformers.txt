This paper explores the performance drops that occur in vision transformers (ViTs) when faced with common image corruptions, adversarial examples, and out-of-distribution examples. The focus is on understanding the role of the self-attention mechanism in these performance drops. The authors introduce the concept of token overfocusing, where only a few important tokens are relied upon by the attention mechanism across all heads and layers. They hypothesize that this overfocusing is fragile to image corruptions and reduces the robustness of the attention mechanism. The authors analyze the attention maps of ViTs and observe that token overfocusing is present throughout the entire ImageNet dataset and across diverse architectures. They find that these important tokens are extremely sensitive to common corruptions, leading to a low cosine similarity between clean and corrupted attention maps. To address the token overfocusing issue and improve attention robustness, the authors propose two key components: Token-aware Average Pooling (TAP) and Attention Diversification Loss (ADL). TAP encourages output tokens to consider the local neighborhood of important tokens, while ADL penalizes high cosine similarity among attention rows. These techniques lead to more balanced attention across columns and increased diversity across rows, resulting in more stable attention maps in the presence of image corruptions.The authors evaluate their proposed methods on ImageNet and other benchmarks, demonstrating improved robustness against corruptions and distribution shifts while maintaining competitive clean accuracy. Their contributions include the TAP module, ADL, and the applicability of these techniques to diverse transformer architectures.