The introduction of this computer science paper discusses the Transformer model and its extensions, particularly the Vision Transformer (ViT), which has shown promising results in various vision tasks. However, the computational overhead of ViT poses challenges in resource-constrained environments, especially in semantic segmentation tasks with high-resolution images. Previous works have explored token pruning techniques to reduce computational costs, but they are not directly applicable to semantic segmentation. This paper introduces a novel approach called Dynamic Token Pruning (DToP) that grades tokens based on their difficulty levels and performs early token exits in the ViT. By utilizing auxiliary predictions and retaining high-confidence tokens, DToP significantly reduces computation costs without sacrificing segmentation accuracy. The authors apply DToP to mainstream semantic segmentation transformers and demonstrate its effectiveness through extensive experiments on three challenging benchmarks.