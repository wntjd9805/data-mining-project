Semantic segmentation is an important computer vision task that aims to predict a category label for each pixel in an image. While state-of-the-art segmentation methods have achieved remarkable performance, they often require high computational costs, limiting their application in resource-limited scenarios. To address this limitation, model compression techniques such as quantization, pruning, and knowledge distillation have gained attention. Knowledge distillation involves learning a compact student model under the guidance of a high-capacity teacher model. However, existing knowledge distillation methods designed for convolutional neural networks (CNNs) are less effective when applied to vision transformer (ViT) models due to differences in their computing paradigms and learning capacities. In this paper, we propose an online knowledge distillation paradigm for collaborative learning of compact and effective CNN-based and ViT-based models for semantic segmentation. We introduce a heterogeneous feature distillation approach that allows the students to learn heterogeneous features from each other in the low-layer feature space. This encourages complementary knowledge transfer between CNNs and ViTs. Additionally, we propose a bidirectional selective distillation module that selectively distills reliable region-wise and pixel-wise knowledge between CNNs and ViTs. The proposed method achieves state-of-the-art performance on benchmark datasets for semantic segmentation. Our contributions include: (I) Introducing the first online collaborative learning strategy for compact ViT-based and CNN-based models for semantic segmentation. (II) Proposing heterogeneous feature distillation to facilitate the learning of global and local feature representations by CNNs and ViTs, respectively. (III) Introducing bidirectional selective distillation for knowledge transfer in the feature and logit spaces between CNNs and ViTs. (IV) Achieving new state-of-the-art performance on benchmark datasets for semantic segmentation.