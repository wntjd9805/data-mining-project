This paper addresses the problem of 3D scene understanding in computer vision and robotics, with applications in object recognition, robotic manipulation, and navigation. The authors focus on 6D pose estimation, which involves inferring the position and orientation of objects in the camera frame based on image observations. They propose a probabilistic inverse graphics approach called 3D Neural Embedding Likelihood (3DNEL) that uses learned neural embeddings to predict 2D-3D correspondences and combines this with depth information to evaluate the agreement between scene descriptions and real-world observations. The authors develop efficient inference procedures using 3DNEL for pose estimation and object pose tracking. Extensive experiments on the YCB-Video dataset demonstrate that 3DNEL improves robustness in sim-to-real pose estimation, achieves accuracy on par with state-of-the-art approaches, and enables robust object pose tracking under occlusion. The proposed approach offers advantages in terms of robustness, uncertainty quantification, and support for multiple tasks compared to deep learning-based discriminative approaches. The main contributions of this paper are a probabilistic approach to 6D pose estimation with support for uncertainty quantification and additional knowledge incorporation, experimental results on par with state-of-the-art while improving robustness, and the ability to handle challenging cases such as symmetric objects and heavy occlusion.