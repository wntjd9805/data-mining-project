Visually-situated language, consisting of texts and visual objects, is prevalent in modern society. Understanding and reading visually-situated language with machine learning systems is both challenging and commercially valuable. This paper focuses on the Visual Document Understanding (VDU) and Scene Text Understanding (STU) tasks, which deal with different types of visually-situated language. Existing models for these tasks, such as Donut and Pix2seq, utilize pre-training strategies like text-read and OCR-read. This paper proposes a novel pre-training method called SCOB, which uses character-wise contrastive learning and online text rendering to bridge the gap between VDU and STU domains. SCOB enables weakly supervised OCR pre-training, reducing annotation costs. Experimental results show that read-based pre-training with SCOB improves the performance of various text-related tasks.