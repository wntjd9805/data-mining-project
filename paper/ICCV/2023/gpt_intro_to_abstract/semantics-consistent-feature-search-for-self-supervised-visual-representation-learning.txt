Self-supervised learning, which learns feature representations without using data annotations, has gained attention in the field of representation learning. One popular method is contrastive learning, which aims to bring augmented views of the same image closer while pushing diverse images apart. This paper introduces a new method called semantics-consistent feature search (SCFS) to improve contrastive learning by addressing semantic inconsistency. SCFS searches for semantics-consistent features using the global feature of one view and conducts contrast learning between feature augmentations and data augmentations. This allows the pre-trained model to focus on meaningful object regions and achieve better representation learning. Experimental results show that SCFS improves the performance of self-supervised learning and achieves state-of-the-art results on different tasks. The contributions of this study include the proposal of SCFS as a novel contrastive learning method, the expansion of contrastive learning to a feature-to-data manner, and the achievement of state-of-the-art performance on various tasks.