Neural processes (NPs) offer a flexible approach to data modeling, eliminating the need for selecting a specific kernel function like in Gaussian processes (GPs). This leads to more efficient and accurate predictions without requiring extensive knowledge of the data distribution. However, NPs still have limitations, such as underfitting and difficulty in capturing embedding relationships and complex latent distributions. In this paper, we introduce Autoregressive Neural Processes (AENPs) and Conditional Autoregressive Neural Processes (CAENPs), which jointly model the global structure using multiple latent Gaussian variables. Our approach combines sliding window and global attention to efficiently capture relationships between context sample points. We demonstrate the advantages of our approach in 1D and 2D datasets and Bayesian optimization, achieving better modeling performance for complex distribution modeling processes.