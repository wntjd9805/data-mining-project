Large-scale pre-training of vision-language models has achieved great success in various cross-modal tasks. However, current models face challenges of training inefficiency and ineffectiveness due to lengthy visual token sequences. To address this, we propose a bottom-up summarization process that combines top-level abstraction and bottom-level extraction. Inspired by text summarization techniques, we design a process that first selects key patches based on text guidance and then abstractively generates a condensed visual representation sequence. We incorporate a Text Semantic-aware Patch Selector (TSPS) module for bottom-level extraction and a lightweight Transformer-based Patch Abstraction Decoder (PAD) for top-level abstraction. Our proposed framework, named BUS, achieves competitive or better performance on visual question answering, cross-modal retrieval, and image captioning tasks while significantly improving efficiency. By reducing the patch sequence length to 20% of its original length, BUS reduces inference time by 51% and achieves state-of-the-art performance by increasing input image resolution.