Low-light image enhancement is a challenging task in computer vision, aiming to improve visibility, contrast, and address corruptions introduced in low-light conditions. Existing algorithms have drawbacks, such as undesired artifacts and noise. Convolutional neural network (CNN) based methods have been used, but they lack interpretability and struggle with capturing long-range dependencies. The Transformer model shows potential for addressing these issues but suffers from high computational complexity. To address these problems, we propose Retinexformer, a novel method for low-light image enhancement. Retinexformer consists of a one-stage Retinex-based Framework (ORF) that estimates illumination and suppresses corruptions, and an Illumination-Guided Transformer (IGT) that models long-range dependencies. Our method surpasses state-of-the-art Retinex-based methods on various datasets, with improvements over 6 dB on specific datasets. Our contributions include the first Transformer-based algorithm for low-light enhancement, a one-stage training process, an illumination-guided self-attention mechanism, and improved performance in quantitative and qualitative experiments, user study, and low-light detection.