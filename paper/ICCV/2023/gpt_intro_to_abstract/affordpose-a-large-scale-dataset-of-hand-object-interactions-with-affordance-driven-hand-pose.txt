This paper introduces the AffordPose dataset, a large-scale dataset for fine-grained hand-object interactions driven by part-level affordances. Previous research in robotics has focused on predicting hand poses for general grasping tasks, but has overlooked the semantic meaning of hand-object interactions. The AffordPose dataset addresses this gap by providing specific part-level affordances and corresponding hand poses for different objects. The dataset contains 26.7K manually annotated interactions, including 3D object shapes, part-level affordance labels, and parameters of the detailed hand configurations. The authors also conduct comprehensive data analysis and experiments to understand the effect of affordances on hand poses and validate the effectiveness of the dataset in learning fine-grained hand-object interactions.