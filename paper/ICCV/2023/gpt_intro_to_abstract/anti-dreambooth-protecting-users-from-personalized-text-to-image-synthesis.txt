Image generation models have revolutionized the field of computer vision, enabling the production of high-quality and diverse images. These models, when combined with language or vision-language models, have shown exceptional success in text-to-image generation. Personalization is a crucial feature in image generation models, allowing users to generate images of specific subjects. However, this presents a risk of misuse, as malicious users can exploit these models to create fake and harmful images. In this paper, we propose a defense mechanism called Anti-DreamBooth, which injects subtle adversarial noise into users' images before publishing, thus preventing the generation of reasonable-quality images by the DreamBooth model. We highlight the complexities and challenges in disrupting diffusion-based text-to-image models like DreamBooth and present different algorithms for adversarial noise generation. We evaluate our proposed defense mechanism on facial benchmarks and demonstrate its effectiveness in both controlled and adverse settings. Our contributions include addressing the potential negative impact of personalized text-to-image synthesis, defining the task of defending users from this risk, proposing a proactive protection mechanism, and evaluating the effectiveness of different algorithms. Our defense strategy proves to be efficient and robust against various model and prompt/term mismatches.