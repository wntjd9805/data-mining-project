The paper introduces the topic of text-driven human motion synthesis, which aims to generate natural human motion based on given text descriptions. This technology has applications in animation production, virtual reality, gaming, and film industry, as well as human-robotics interaction. The paper highlights the challenges in incorporating natural language descriptions as constraints for motion synthesis and discusses the need for better learning the cross-modal relationship between text and motion. The authors propose a two-stage text-driven motion generation model called AttT2M that utilizes multi-perspective attention and global-local attention mechanisms. They also introduce a Body-Part attention-based Spatio-Temporal VQ-VAE to improve the representation of motion sequences. The paper presents qualitative and quantitative experiments conducted on two datasets to demonstrate the effectiveness of their approach in generating high-quality and diverse motion sequences that accurately match the given text descriptions. Overall, the contributions of this work include the proposed VQ-VAE model, the introduction of global-local attention, and the empirical evaluation of the method's performance.