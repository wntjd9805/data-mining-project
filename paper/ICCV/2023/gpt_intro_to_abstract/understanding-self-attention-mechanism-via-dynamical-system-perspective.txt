The self-attention mechanism (SAM) is widely used in various fields of artificial intelligence and has shown improvements in vision tasks such as image classification, object detection, and image segmentation. However, the relationship between the SAM and model performance is not well understood. In this paper, we take a dynamical systems perspective and redefine the stiffness phenomenon (SP) and ground truth trajectory in neural networks. We observe that the SP in high-performance neural networks is similar to that in high-precision solutions of ordinary differential equations (ODEs). We propose that the SAM acts as a stiffness-aware step size adaptor, refining the estimation of stiffness information and generating adaptive attention values to measure the intrinsic SP. This perspective provides an explanation for the lottery ticket hypothesis in SAM and inspires a new theoretic-inspired approach called StepNet. Experimental results on popular benchmarks demonstrate the effectiveness of StepNet in various vision tasks. Our contributions include a new understanding of the SAM, the explanation of the lottery ticket hypothesis, the design of new metrics for representational ability, and the introduction of StepNet as a powerful approach.