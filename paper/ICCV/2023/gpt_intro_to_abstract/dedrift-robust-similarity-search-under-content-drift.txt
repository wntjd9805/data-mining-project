This paper focuses on the problem of content drift in large-scale nearest neighbor search (NNS) systems. As the amount of online content continues to grow exponentially, efficient NNS techniques are necessary to search these vast databases based on content similarity. NNS techniques extract high-dimensional feature vectors from each item to be indexed, and approximate NNS methods are often used to speed up the search on millions to trillion size databases. One common approach is to use vector quantization, where each vector is represented by the nearest centroid from a finite set. However, over time, the statistical distribution of the content may change, leading to content drift and a mismatch between the training and indexed vectors that negatively impacts search accuracy. Currently, practitioners resort to full index reconstruction to adapt to the new data distribution, which can be resource-intensive and disrupt services. In this paper, the authors investigate temporal distribution drift in large-scale NNS systems and propose a lightweight update procedure called DEDRIFT that adapts the index to the evolution of the vector distribution without re-indexing all stored elements. Experimental results show that DEDRIFT achieves search results close to full re-indexing while being significantly faster. Overall, this paper provides insights into content drift in NNS systems and presents an efficient solution to address this challenge.