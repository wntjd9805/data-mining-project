Abstract:The explosion of online videos has made video search an essential activity in people's daily lives. Text-to-video retrieval (T2VR), which involves retrieving videos based on textual queries from a large number of unlabeled videos, has gained attention in recent years. However, the majority of existing videos are untrimmed and not fully relevant to the query, creating a gap between literature and real-world applications. To address this gap, a new subtask called Partially Relevant Video Retrieval (PRVR) has been proposed, which aims to retrieve untrimmed videos that contain at least one relevant moment to the given query. In this work, we focus on the PRVR task and propose a Dual Learning framework with Dynamic Knowledge Distillation (DL-DKD) to transfer the knowledge of large-scale pre-trained vision and language models, such as Contrastive Language-Image Pre-training (CLIP), into PRVR models. Our framework utilizes a teacher-student network, where CLIP serves as the teacher model and a dual-branch student model acquires the knowledge. We introduce two student branches to address domain gap issues and employ a dynamic knowledge distillation strategy to gradually emphasize task-specific learning. Experimental results demonstrate the effectiveness of our proposed framework, achieving state-of-the-art performance in PRVR.