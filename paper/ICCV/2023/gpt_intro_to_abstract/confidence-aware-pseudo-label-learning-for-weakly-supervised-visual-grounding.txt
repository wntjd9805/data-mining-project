Visual grounding is a crucial task with various applications in fields such as visual question answering and robot manipulation. However, fully supervised visual grounding, which requires accurate box annotations for each target object, is expensive and difficult to scale. As a more practical alternative, weakly supervised visual grounding with only image-level descriptions during training has gained attention. Existing weakly supervised methods rely on pre-trained object detectors and either contrastive learning or reconstruction-based paradigms for proposal selection. However, these methods have limitations in the reliability of cross-modal matching and suffer from error accumulation during training. To address these issues, we propose a novel weakly supervised method called Confidence-aware Pseudo-label Learning (CPL). We generate descriptive, realistic, and diverse pseudo language queries for each region proposal and establish reliable cross-modal associations based on uni-modal similarity. We also introduce a confidence-aware cross-modal verification module and a selective grounding loss to reduce the contribution of spurious associations. Experimental results on multiple datasets demonstrate the effectiveness of our method in weakly supervised visual grounding.