Deep neural networks (DNNs) have revolutionized various domains of Machine Learning, but their lack of explainability presents challenges for safety and fairness critical applications. In response, Neuron Explanation methods (NEMs) have emerged to provide human-interpretable insights into DNNs. However, there is concern that the explanations provided by NEMs may be corrupted and not trustworthy. In this paper, we perform the first robustness evaluation of NEMs, showing that the explanations can be significantly corrupted by both random noise and well-designed perturbations. We present a unified pipeline for NEMs and develop techniques to manipulate neuron descriptions without explicit knowledge of the similarity function. We conduct a large-scale study and demonstrate that even Gaussian random noise and our proposed algorithm can manipulate a significant portion of neurons, raising concerns about practical deployment of NEMs.