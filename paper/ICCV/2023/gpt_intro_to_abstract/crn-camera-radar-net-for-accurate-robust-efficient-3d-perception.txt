Accurate and robust 3D perception systems are essential for applications such as autonomous driving and mobile robots. To achieve efficient 3D perception, a reliable bird's eye view (BEV) feature map is necessary, as many downstream tasks can be performed on the BEV space. However, relying solely on expensive and high-maintenance LiDAR systems can be disadvantageous. Therefore, this paper aims to build a 3D perception system that relies less on LiDAR by incorporating a cost-effective range sensor, radar. Radar has advantages in terms of cost, reliability, long-range perception, and robustness in different conditions. However, it also has challenges such as sparsity and noisy measurements. Previous camera-radar fusion methods have failed to fully exploit the complementary information. Therefore, this paper proposes a novel two-stage fusion method, Camera Radar Net (CRN), that accurately transforms camera features into BEV space and handles the spatial misalignment between feature maps. CRN achieves high accuracy, robustness, and efficiency in various tasks while leveraging the strengths of both camera and radar sensors. The contributions of this work include LiDAR-level performance on 3D object detection, tracking, and BEV segmentation tasks, robustness in the event of sensor unavailability, and marginal extra cost for significant performance improvement.