Autonomous driving cars rely on multiple sensors, such as LiDAR and cameras, to perceive the surrounding environment. However, there are differences in the representations acquired by these sensors, which hinders effective fusion of the LiDAR and camera modalities. In this paper, we propose SparseFusion, a method for LiDAR-camera 3D object detection that leverages instance-level sparse feature fusion and cross-modality information transfer. SparseFusion uses parallel detection branches for each modality and fuses the instance features in a unified 3D space using a lightweight attention module. We also introduce a novel cross-modality information transfer method to compensate for the deficiencies of each modality. Our method achieves state-of-the-art performance on the nuScenes benchmark, with a lighter network and faster inference speed compared to prior work.