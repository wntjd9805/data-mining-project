Researchers have been designing embodied agents with general neural architectures and training them using reinforcement learning (RL) to complete complex tasks. However, training agents to perform long horizon tasks using only terminal rewards has been ineffective and inefficient. To improve training, several techniques, such as manually engineered shaped rewards and imitation learning, have been used. One promising approach is training agents with RL and auxiliary losses to encourage the production of powerful and useful environment representations. These self-supervised losses are task and environment independent but may not encode task-relevant features. On the other hand, supervised auxiliary losses are designed for specific tasks but are not generally useful. In this work, the Scene Graph Contrastive (SGC) loss is proposed, which uses a non-parametric scene graph as the supervisory signal. The SGC loss encourages agents to develop a graph-aware belief state and does not require a scene graph during evaluation. It also has desirable characteristics such as summarizing object semantics and being simple to implement. Agents trained with the SGC loss show significant improvements in navigation-based tasks compared to agents trained with pure RL. The representations learned by SGC-trained agents demonstrate better understanding of the environment. This work presents the use of a scene graph as a supervisory signal, the formulation of the SGC loss, and experimental results showing its effectiveness across multiple tasks.