This paper introduces the problem of Video State-Changing Object Segmentation (VSCOS) in the field of computer science. The authors highlight the importance of understanding state changes in visual perception tasks, such as video object segmentation, for autonomous agents to interact effectively with objects. However, previous research in this area has largely overlooked objects undergoing state changes. To address this gap, the authors propose VSCOS as a task to predict pixel-wise masks of state-changing objects in each frame of a video given the first frame mask as a reference. They construct a dedicated benchmark to evaluate existing VOS methods and identify the challenges associated with state-changing objects. The benchmark includes annotations for two frames per video and an open-vocabulary setting to test the models' generalization capabilities. The authors also present a baseline method that adapts existing VOS models for VSCOS, addressing the key difficulty of segmentation association before and after state changes. The baseline method includes fine-tuning techniques, feature learning approaches, and the incorporation of motion information through optical flow. The authors analyze the results of their baseline method, including the contribution of different design decisions and different action categories. They also categorize key failure cases and challenges of the VSCOS task. The paper concludes with discussions on the limitations of their approach and outlines future directions for research. Overall, the contributions of this paper include defining the VSCOS task, constructing a benchmark dataset, proposing a baseline method, and analyzing the challenges and results of the VSCOS task.