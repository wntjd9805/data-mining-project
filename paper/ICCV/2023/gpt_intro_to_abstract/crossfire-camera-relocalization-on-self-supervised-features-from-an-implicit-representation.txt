This paper introduces a method for camera relocalization (6-DoF pose estimation) using a learned volumetric-based implicit map. The proposed method, named CROSSFIRE, incorporates local descriptors in the Neural Radiance Fields (NeRF) formulation and utilizes a self-supervised training approach. The method trains a CNN feature extractor and a neural renderer simultaneously to generate consistent scene-specific descriptors. These descriptors capture both the 2D image content and the 3D position of the observed point, allowing for accurate localization in areas with repetitive patterns. The method can be applied to any differentiable neural renderer, benefiting from recent NeRF advancements. The paper demonstrates that these features can be used for visual relocalization through an iterative algorithm that combines dense features matching and Perspective-n-Points (PnP) camera pose computation. The proposed method offers improved accuracy and computational efficiency compared to existing localization methods that use implicit maps.