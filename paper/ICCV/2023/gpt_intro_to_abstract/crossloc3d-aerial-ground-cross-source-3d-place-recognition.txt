In this paper, we address the problem of 3D cross-source place recognition, particularly in the context of aerial and ground views. This problem is challenging because of the perspective differences and lack of data consistency between these two sources. We propose a method called CROSSLOC3D, which utilizes multi-grained voxelization and multi-scale sparse convolution to close the representation gap between different data sources. We also introduce a novel iterative refinement process to shift the feature distributions towards a canonical latent space. Additionally, we present a new benchmark dataset, CS-CAMPUS3D, consisting of aerial and ground LiDAR scans in a campus environment. We evaluate CROSSLOC3D and other state-of-the-art methods on this dataset and observe significant improvements in terms of average recall. Furthermore, CROSSLOC3D achieves high performance on the Oxford RobotCar dataset and is on par with the state-of-the-art methods on the traditional single-source 3D place recognition task.