Transformers have recently shown superior performance in visual learning tasks, overcoming the limitations of local receptive fields in convolution. However, the computation cost of self-attention, a key component of Transformers, restricts their applicability to large-scale inputs. To address this issue, various approaches have been proposed, sacrificing either global information or fine-grained detail. In this paper, we present a novel Transformer model called SG-Former, which achieves global attention with adaptive fine granularity through an evolving self-attention design. SG-Former reallocates tokens based on the significance of image regions, assigning more tokens to salient regions for fine-grained information and fewer tokens to minor regions for efficiency. We introduce a hybrid-scale self-attention that captures various levels of granularity within one layer and propose a self-guided attention mechanism that automatically locates salient regions. Experimental results demonstrate that SG-Former outperforms previous Vision Transformers in various downstream tasks.