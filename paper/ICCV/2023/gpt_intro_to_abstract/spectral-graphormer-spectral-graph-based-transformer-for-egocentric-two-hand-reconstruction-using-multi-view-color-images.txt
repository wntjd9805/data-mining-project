3D hand pose estimation is a crucial problem in augmented and virtual reality applications. Previous research has mainly focused on single-hand pose estimation, while two-hand pose estimation has received less attention. In this paper, we propose a method for high fidelity two-hand reconstruction from multi-view RGB images. We address the challenges of self-similarity between interacting hands and the dependence on root joint alignment. Accurate estimation of two hands, including extended forearms, is essential for realistic representation of hand movements in AR/VR applications. However, there is currently no suitable dataset for supervised deep learning in this context. To overcome these challenges, we introduce a spectral graph-based transformer architecture and a soft-attention-based multi-view image feature fusion strategy. We also incorporate graph theory properties into our network design and propose an optimization-based refinement stage. We create a large-scale synthetic dataset and collect real-world data for training and evaluation. Our proposed method shows robust performance in challenging scenarios and can serve as a strong baseline. Our contributions include the novel transformer architecture, efficient image feature fusion, optimization-based refinement, and the creation of a large-scale dataset.