Feature point detection, also known as keypoint detection, is an important step in various computer vision applications, such as camera localization, scene reconstruction, and augmented reality. While feature description has been successfully addressed using metric learning, applying deep learning to the feature detection task remains challenging. The difficulty arises from the inherent ambiguity in defining points of interest, which complicates the formulation of feature detection as a learning problem. Additionally, obtaining data that is robust to changes in illumination and viewpoint presents another challenge. Existing methods that rely on structure-from-motion and multi-view stereo reconstructions suffer from difficulties in building proper reconstructions and lack full image coverage. In this paper, we propose a novel approach to defining keypoints by leveraging principled assumptions and employing the Shi-Tomasi detector for location and sub-pixel localization. We introduce a quantitative metric called the stability score to measure the stability of detected keypoints under viewpoint transformations. This metric is calculated online and can be used to generate a supervised signal for training a neural network to predict the neural stability score (NeSS). We combine the Shi-Tomasi detector with the NeSS approach to create a method, NeSS-ST, that provides accurate and stable keypoints under viewpoint changes.Our method does not rely on ground-truth poses or reconstructed correspondences, making it self-supervised and requiring only a set of real images for training. This distinguishes it from other self-supervised methods that rely on labeled datasets or offline ground-truth generation. Experimental results demonstrate that NeSS-ST outperforms state-of-the-art methods on various benchmarks in terms of pose accuracy.