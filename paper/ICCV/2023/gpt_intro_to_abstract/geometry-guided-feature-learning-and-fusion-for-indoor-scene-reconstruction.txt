This paper introduces a novel approach to 3D scene reconstruction in computer vision. The use of 3D scene reconstruction has many applications, including augmented reality, autonomous navigation, and robotics. While cameras equipped with depth sensors can reconstruct scenes, they are expensive and not widely used in consumer cameras. Therefore, scene reconstruction from RGB images is more accessible. The standard approach involves computing the Truncated Signed Distance Function (TSDF) volume and applying the marching cubes algorithm. Traditional methods generate depth maps and apply depth fusion, but they may suffer from scale ambiguity and depth inconsistency. Volumetric methods that use 3D CNNs have been proposed to predict the TSDF directly, but they still rely on RGB information. This paper proposes a geometry integration mechanism at three different stages of 3D scene reconstruction: feature learning, feature fusion, and network supervision. A novel geometry-guided feature learning scheme is introduced to encode geometric priors, such as surface normal and viewing direction, into the multi-view features. A geometry-guided adaptive feature fusion method is used to assign different attention levels to occluded views. A consistent 3D normal loss is also introduced to improve the reconstruction quality. The proposed method achieves state-of-the-art performance on various datasets.