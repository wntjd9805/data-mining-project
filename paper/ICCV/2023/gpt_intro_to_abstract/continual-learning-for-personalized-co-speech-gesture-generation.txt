This paper introduces the concept of embodied personal assistants, which are agents capable of generating personalized visual co-speech gestures using spoken language. Previous works in co-speech gesture generation have focused on static paradigms, where training is done on a fixed dataset for all speakers. However, in practical scenarios, embodied agents learn in a continual learning paradigm where they receive small amounts of training data sequentially. The main goal of this paper is to develop a unified co-speech gesture generation model that can generate gestures in multiple styles while only having access to limited training data. The paper also addresses the technical challenge of crossmodal catastrophic forgetting, which refers to the model's tendency to forget the crossmodal grounding relationships between spoken language and gestures of speakers encountered earlier. The paper proposes an approach called C-DiffGAN, which can efficiently personalize co-speech gesture generation models from a high-resource source speaker to multiple low-resource target speakers. C-DiffGAN identifies shifts in crossmodal grounding relationships and updates necessary parameters in the source model to adapt to new speakers. It also utilizes input data from prior speakers to prevent crossmodal catastrophic forgetting. The effectiveness of C-DiffGAN is demonstrated through experiments on a publicly available dataset, showing significant improvements over prior approaches in low resource continual learning of non-verbal grounding and gesture generation models.