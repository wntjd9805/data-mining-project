This paper introduces the problem of open-world or zero-shot video recognition, which involves recognizing unseen video/action classes during testing that were not seen during training. The authors propose using a generative video captioning model for open-world recognition, as it can generate human-interpretable textual descriptions associated with any video/action class. However, the authors note that the captions generated by existing models are not discriminative enough for video/action recognition. The authors aim to address this limitation by developing a training framework called ReGen, which integrates class information into the training of a generative video captioning model. They introduce three rewards to train the model, including a class discrimination reward, a content description reward, and a grammar reward. Experimental results demonstrate that ReGen outperforms the previous state-of-the-art methods in zero-shot and few-shot action classification, as well as achieving competitive results in zero-shot captioning.