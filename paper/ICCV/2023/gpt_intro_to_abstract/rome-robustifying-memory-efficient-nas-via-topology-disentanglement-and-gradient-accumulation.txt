Despite the advancements in neural architecture search (NAS) for network design in vision tasks, there is a need for faster searching algorithms. Current methods based on evaluating a large number of candidate models are costly. This paper investigates a single-path based variation of the DARTS algorithm, called GDAS, for its speed and low GPU memory usage. Single-path methods are designed to activate only a subset of operations in the supernet, but they suffer from performance collapse due to parameterless operations. To address this, the proposed ROME algorithm disentangles topology search from operation selection and introduces topology parameters. A gradient accumulation strategy is also proposed to enhance the searching process. The contributions of this research include revealing performance collapse in single-path NAS, achieving consistent search and evaluation by separating topology search from operation selection, robustifying bi-level optimization, and achieving strong performance with low memory cost. Experimental results show that ROME outperforms existing methods on various search spaces and datasets. The source code will be made publicly available.