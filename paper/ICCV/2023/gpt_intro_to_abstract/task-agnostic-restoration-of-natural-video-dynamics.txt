This paper introduces the problem of temporal flickering in videos caused by naively applying static image processing methodologies. The authors propose a solution that aims to learn consistent motion representations solely from temporally inconsistent videos, overcoming the limitations of previous approaches that rely on access to unprocessed videos. The proposed model fine-tunes optical flow estimation networks and achieves state-of-the-art results. The formulation also addresses resolution mismatch and enables the extension of single image super-resolution methods to video super-resolution. The paper makes contributions in identifying tailored solutions for challenges in extending image processing applications to videos, learning consistent motion representations, and proposing a general framework for task-agnostic temporal consistency correction.