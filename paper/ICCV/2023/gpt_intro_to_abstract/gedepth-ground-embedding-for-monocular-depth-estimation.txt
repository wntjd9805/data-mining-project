Accurate depth acquisition is essential for robotics applications such as perception, prediction, and planning. While range sensors like LiDAR are widely used for precise depth measurements, camera-based depth estimation is gaining attention for its portability and cost-effectiveness. Existing monocular depth estimation networks typically use an encoder-decoder architecture and are trained in a supervised or self-supervised manner. However, monocular depth estimation is inherently ill-posed, as different objects with varying scales and depths can generate the same image, and the same depth can be mapped to different images due to different camera parameters. Most supervised methods rely on specific pictorial cues and camera parameters, limiting their generalization capability. Additionally, few studies have investigated what these networks have learned and their behavior in cross-domain or unexpected scenarios. To address these challenges, we propose GEDepth, a ground embedding module that decouples camera parameters from pictorial cues. GEDepth calculates ground depth, which is fused with an input image to produce ground depth-aware features. The network generates residual depth and a ground attention map, which selectively combines with the ground depth for final depth prediction. Our module improves generalizability, adapts to ground undulation, and explicitly utilizes ground information. We provide comprehensive experiments demonstrating the effectiveness and robustness of our approach compared to other algorithms. The code and model will be publicly available.