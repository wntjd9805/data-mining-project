In real-world scenarios, learning often occurs incrementally from streaming data. However, traditional object detection models lack the capability to learn incrementally and often suffer from catastrophic forgetting when new data is added. This paper focuses on incremental learning in the context of class incremental learning for object detection tasks. While incremental image classification has been extensively studied, there is limited research on incremental object detection. In incremental object detection, when adding new object categories, annotations are only provided for the new objects, while the old objects are categorized as backgrounds. This labeling interference leads to catastrophic forgetting of the old categories during the incremental process. To address this issue, the paper proposes a method that maintains the semantic feature space to alleviate catastrophic forgetting in incremental object detection. Previous researches have used knowledge distillation methods to reduce catastrophic forgetting, but they mainly rely on low-level feature selection and do not fully consider the importance of high-level semantic information. This paper focuses on leveraging high-level semantic feature space to improve knowledge distillation methods for object detection. The paper introduces two methods, Interactive Feature Distillation (IFD) and Distance Matrix Distillation (DMD), to maintain within-class consistency and between-class discriminativeness, respectively. IFD mimics information within the same class from teacher to student to enforce within-class consistency, while DMD preserves the class-wise semantic and feature differences between teacher and student. The contributions of this work include discussing catastrophic forgetting in incremental object detection, proposing instance-wise feature distillation and class-wise distance distillation methods to maintain within-class consistency and between-class discriminativeness, respectively.