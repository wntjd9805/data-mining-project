Abstract:Deep learning models are susceptible to adversarial attacks, where specific perturbations are introduced to input samples, leading to incorrect predictions. While various defense mechanisms have been proposed, the threat still exists. In this paper, we focus on the forensic aspect of adversarial examples and aim to determine which model generated them. We consider a buyers-seller setting where a seller distributes multiple models to different buyers. A malicious buyer aims to attack other buyers by generating adversarial examples using their own model. Our proposed framework includes model separation and origin tracing stages. In the model separation stage, we generate multiple models that are accurate but distinct for tracing purposes. We introduce a parallel network structure where a tracer modifies the output to induce unique features in adversarial examples. In the tracing stage, we compare the output logits of the embedded tracers to identify the source model. This traceability concept is analogous to neural network watermarking but provides stronger attribution. Our contributions include highlighting the importance of tracing adversarial examples among multiple classifiers, proposing a framework for traceability in the buyers-seller setting, and demonstrating the effectiveness of our mechanism through experimental studies. The proposed framework achieves high tracing accuracy and clear separation of source and non-source models.