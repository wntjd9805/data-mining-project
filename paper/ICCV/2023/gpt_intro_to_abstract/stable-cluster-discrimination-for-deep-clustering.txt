Clustering is a fundamental task in unsupervised learning where unlabeled data is partitioned into multiple clusters based on the similarity between instances. Traditional methods focus on distance functions and algorithms, but with the rise of deep learning, deep clustering has gained attention. Deep clustering learns representations and cluster assignments simultaneously, but can result in trivial solutions. Two-stage training strategies have been used to avoid collapsing, but the objective of pre-training may not align with clustering. Existing one-stage methods mainly focus on representation learning and use constraints for cluster assignments. One-stage deep clustering is unstable due to the lack of positive instances and the domination of negative instances in gradient updates. To address this, we propose a stable cluster discrimination (SeCu) task that eliminates the influence of negative instances in updating cluster centers. We also introduce an entropy constraint to balance the size of clusters. Our method achieves superior performance on benchmark datasets, confirming its effectiveness for deep clustering.