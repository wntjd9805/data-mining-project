Articulated objects play a significant role in various real-life applications such as robotics, animation, and industrial design. Understanding the structure and motion of these objects is crucial for tasks such as object manipulation and simulation. Previous work in articulated object understanding has relied on expensive and limited supervised learning techniques, or assumes known object categories, making generalization to unseen objects difficult. In this paper, we propose PARIS, a self-supervised approach for joint reconstruction and motion analysis of articulated objects. By observing an object in two states, PARIS reconstructs the shape and appearance of both static and movable parts and predicts the articulation motion parameters. Our approach is category-agnostic and does not require explicit 3D data or motion parameter supervision. We evaluate PARIS on synthetic and real data and demonstrate superior performance compared to previous work in shape and appearance quality, and motion parameter estimation accuracy.