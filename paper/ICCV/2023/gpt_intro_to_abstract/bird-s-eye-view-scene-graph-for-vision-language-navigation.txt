The introduction discusses the challenges of the vision-language navigation (VLN) task, which involves navigating through a 3D environment based on natural language instructions. Existing approaches rely on 2D panoramic observations, which limit their ability to preserve scene layouts and 3D structure. Additionally, indoor environments are characterized by occlusion, making it difficult for the agent to accurately identify objects and landmarks referenced in the instructions. To address these challenges, the use of Bird's-Eye-View (BEV) perception is proposed, which captures spatial context and scene layouts effectively. A BEV Scene Graph (BSG) is introduced to construct an informative navigation graph, using BEV representations to make informed navigation decisions. The agent acquires multi-view observations and performs view transformation to predict oriented bounding boxes, encoding object-level geometric and semantic information. The BSG approach is evaluated on three benchmarks and outperforms state-of-the-art methods in referring expression comprehension and navigation tasks.