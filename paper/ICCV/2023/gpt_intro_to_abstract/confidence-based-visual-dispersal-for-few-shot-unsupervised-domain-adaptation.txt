Deep learning has achieved high performance in computer vision tasks, but suffers from performance drops in real-world scenarios due to the domain gap issue. Unsupervised domain adaptation (UDA) aims to transfer learned knowledge from a labeled source domain to an unlabeled target domain. However, in some real-world scenarios, there is a lack of fully-annotated source data. This has led to increased attention on unsupervised domain adaptation with sparsely labeled source data. Few-shot unsupervised domain adaptation (FUDA) is particularly challenging due to the limited source knowledge. Existing methods in FUDA use self-supervised learning strategies to compensate for the lack of labeled source data. The proposed C-VisDiT method introduces confidence-based strategies to address the label-scarse scenario of FUDA. It includes cross-domain visual dispersal and intra-domain visual dispersal to boost model adaptation. Experimental results on benchmark datasets demonstrate that C-VisDiT outperforms existing methods, establishing new state-of-the-art results.