Abstract:Vision-Language Pre-training (VLP) has become the dominant approach for addressing vision-language tasks. However, current VLP paradigms rely on offline-trained models and lack the ability to continually integrate new knowledge in a dynamic environment. In this paper, we propose Vision-Language Continual Pretraining (VLCP) as a new paradigm for VLP and introduce the P9D dataset, which contains over 1 million product image-text pairs and supports continual pretraining. We identify challenges specific to VLCP, including fixed-dimensional embedding, missing contrast samples, and multi-modal/task optimization. To address these challenges, we develop a method called Compatible momentum contrast with Topology Preservation (CTP), which effectively adjusts uni-modal and multi-modal encoders while preserving sample relationships. We evaluate different continual learning methods in a unified setting and demonstrate that CTP achieves superior performance and efficient training. Our contributions include establishing a baseline library for VLCP, introducing the P9D dataset, and proposing the CTP method.