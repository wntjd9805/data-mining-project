This paper focuses on the problem of classifying sound sources in a continual learning setting using both video frames and audio. Continual learning aims to train models on non-stationary data given sequential tasks for class-incremental learning. Previous methods have used regularization or rehearsal-based approaches to address catastrophic forgetting between old and new classes in image classification. However, these baselines are based on a single input modality and perform poorly for continual audio-visual learning. To address this, the authors propose a multi-modal continual problem by extracting disentangled and compact representations with learnable audio-visual class-incremental tokens. They also discuss the use of prompts, which alter the input text for pre-trained large language models, to resolve continual learning challenges. However, current prompting methods can only deal with one modality and struggle with audio-visual settings. The main challenge in audio-visual learning is that sounds are mixed in the audio space, making it easy for global audio representation to be forgotten. To address this, the authors propose a novel class-incremental grouping network called CIGN, which learns category-wise semantic features to achieve continual audio-visual learning. They leverage audio-visual class tokens and audio-visual grouping to continually aggregate class-aware features and alleviate forgetting parameters learned from previous tasks. Experimental results on benchmarks demonstrate the state-of-the-art performance of CIGN compared to previous baselines. The authors also provide qualitative visualizations and conduct ablation studies to validate the effectiveness of CIGN in learning compact representations for class-incremental learning.