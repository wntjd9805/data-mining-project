Our approach, MAXI, aims to address the shortcomings of Vision Language (VL) models in zero-shot action recognition on video data without finetuning. Previous work has relied on full annotation of action datasets, which is time-consuming and costly. In contrast, MAXI leverages unlabeled video data and a set of language sources to construct text bags for each video and employs Multiple Instance Learning (MIL) for finetuning the VL model. We evaluate MAXI on seven downstream zero-shot and few-shot transfer action recognition benchmarks and demonstrate significant improvements over the source VL model, as well as outperforming baseline models trained in a fully supervised manner. The contributions of this work include the proposal of MAXI, the matching of unlabeled videos with text bags of knowledge, and the use of MIL for finetuning the VL model.