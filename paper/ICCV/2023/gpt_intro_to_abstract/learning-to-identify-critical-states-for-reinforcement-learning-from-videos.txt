This paper introduces a novel framework called the Deep State Identifier, which aims to identify critical states in video-based environments in the context of reinforcement learning (RL). The cumulative reward, or return, of an RL agent is obtained through a series of dynamic interactions between the agent and its environment. However, rewards in RL can be sparse and delayed, making it difficult to determine which decision points were crucial for achieving a desired return. Existing methods for localizing critical states typically rely on explicit action information or policy parameters, limiting their applicability in settings where actions are difficult to measure or annotate. To overcome this limitation, the proposed Deep State Identifier learns to infer critical states from historical visual trajectories of agents without access to explicit action information. The framework consists of a return predictor, which predicts the return based on a visual trajectory, and a critical state detector, which learns a soft mask over the trajectory to identify the frames that are sufficient for accurately predicting the return. The training technique minimizes redundant information through a novel loss function, prioritizing the identification of compact sets of critical states. The importance of states in a trajectory is ranked using the soft mask, allowing for the selection of critical states with high scores. During inference, critical states can be directly detected without relying on a return predictor. The contributions of the paper include the proposal of a framework for identifying critical states in RL from videos without explicit action information, the introduction of new loss functions that enforce compact sets of identified critical states, and the demonstration of the utility of learned critical states for policy improvement and policy comparison.