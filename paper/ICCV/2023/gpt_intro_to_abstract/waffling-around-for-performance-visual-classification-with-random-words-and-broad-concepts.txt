Task-specific tuning of natural language prompts has been shown to improve the performance of large vision-language models (VLMs). However, in scenarios where additional training data is not available, an alternative approach is to query large language models (LLMs) for additional semantic context. Previous work has shown that extending class names with fine-grained descriptors generated by GPT-3 and minimal human intervention can boost results. However, GPT-3 generated descriptors exhibit diversity, limited visual relevance, and ambiguity. To understand the performance gains of using these descriptors, we replace them with randomly selected, class-independent descriptors and observe similar benefits. Building on this observation, we propose WaffleCLIP, a zero-shot method that replaces the LLM-generated descriptors with random words or character lists based on class name length and word counts. WaffleCLIP serves as a sanity check for future methods by not requiring access to LLMs for additional context. We further investigate the benefits of LLM-generated descriptors and find that while they offer a structurally different and complementary impact on classification behavior, the improvements are not solely driven by semantics but rather structured noise ensembling. Instead, we propose introducing actual semantic context through high-level concepts. For scenarios with access to LLMs, we suggest a query mechanism for GPT-3 to generate these concepts and resolve issues of class label ambiguity. In summary, our contributions include the proposal of WaffleCLIP, extensive experiments showcasing its comparable performance to methods relying on LLM-generated descriptors, and the exploration of high-level LLM-generated concepts for improved semantic use and classname disambiguation.