Deep Neural Networks (DNNs) have achieved remarkable performance in vision tasks, but their deployment on edge devices is challenging due to high memory consumption and computation cost. Quantization techniques have emerged as a promising solution to address this challenge by performing computation and storing tensors at lower bit-widths, thereby reducing memory footprint and speeding up inference. Mixed-precision quantization (MQ) is a technique that assigns different bit-widths to neural network layers to optimize accuracy and complexity trade-off. Training-based and training-free approaches have been proposed for MQ, but both have limitations in terms of computational intensity and lack of correlation analysis between proxies and quantization accuracy. To address these limitations, this paper presents a benchmark for training-free proxies in MQ and proposes a framework, EMQ, to automate the discovery of MQ proxies using an evolving algorithm. The framework utilizes a search space comprising existing MQ proxies and evaluates them based on their predictive ability. The paper also introduces a fitness function and selection strategy to improve the search efficiency of the evolution process. Experimental results demonstrate the superiority of the searched MQ proxy, highlighting the effectiveness and flexibility of the proposed approach. The main contributions of this paper include the introduction of MQ-Bench-101 benchmark, the EMQ framework, and the validation of the searched MQ proxy's performance.