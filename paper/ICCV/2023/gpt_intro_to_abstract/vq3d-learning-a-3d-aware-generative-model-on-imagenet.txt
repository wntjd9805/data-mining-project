This paper introduces VQ3D, a novel 3D-aware generative model that can be trained on large and diverse 2D image collections, such as ImageNet. Unlike previous methods that rely on GANs, VQ3D utilizes a two-stage vector quantization formulation, which has shown to be more stable and reliable. The first stage of the model allows for pseudo-depth supervision, enabling better reconstruction and depth prediction. Experimental results demonstrate that VQ3D achieves state-of-the-art generation performance on ImageNet, outperforming existing baselines. Additionally, the model enables 3D-aware image editing and manipulation without the need for expensive inversion optimization techniques. The findings of this study contribute to the advancement of 3D content generation using machine learning techniques.