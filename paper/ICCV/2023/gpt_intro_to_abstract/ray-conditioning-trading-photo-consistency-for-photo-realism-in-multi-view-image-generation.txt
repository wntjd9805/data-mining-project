Modeling the distributions of natural images has been extensively studied in computer science. Generative adversarial networks (GANs) and diffusion models are two types of generative models that have shown impressive capabilities in learning image distributions and generating samples that are almost indistinguishable from real photos. However, these models often prioritize photo-realism over multi-view image generation, where photo-consistency is crucial. Recent research has focused on optimizing for both photo-realism and photo-consistency in multi-view image synthesizers. These synthesizers generally follow a "synthesize-3D-then-render" paradigm, where 3D representations are synthesized and images are rendered at specified camera poses. However, achieving both photo-realism and photo-consistency can be challenging, as they are often competing goals. Fine-scale detail and view-dependent appearance are examples of conflicts that arise in multi-view 3D reconstruction and generation. To address these challenges, this paper introduces a novel ray conditioning mechanism that enables explicit viewpoint control in multi-view image editing. This approach conditions each pixel in a generated image on the corresponding ray, allowing for precise control over generated viewpoints without sacrificing photo-realism. This method outperforms existing approaches in both generation and inversion quality and is particularly well-suited for viewpoint editing in static images. The proposed ray conditioning technique is the first geometry-free multi-view image synthesizer that can generate highly realistic images in high resolution, given only single-view posed image collections. Overall, this paper presents a simple yet effective geometry-free generative model for multi-view image generation, offering greater photo-realism and explicit control of viewpoints compared to geometry-based baselines.