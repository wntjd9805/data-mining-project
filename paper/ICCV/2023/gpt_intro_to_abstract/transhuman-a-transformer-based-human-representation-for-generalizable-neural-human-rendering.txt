Rendering high-fidelity free-viewpoint videos of dynamic human performers is a crucial task for various applications in computer science such as mixed reality, gaming, and telepresence. Existing methods for human-centric reconstruction have focused on optimizing 2D-to-3D estimations/reconstructions or incorporating prior knowledge through multi-knowledge representations. Recent works have integrated Neural Radiance Fields (NeRF) with parametric human models to achieve fair novel view synthesis results, but the per-subject optimization and dense training views required hinder their application. To address these issues, a task of generalizable neural human rendering has been proposed, which trains conditional NeRF using multi-view human videos and can generalize to new subjects with sparse reference views. However, previous methods mainly employ SparseConvNet (SPC)-based human representations, which suffer from volatile observation learning and limited local receptive fields. To overcome these limitations, this paper proposes TransHuman, a framework composed of Transformer-based Human Encoding (TransHE), Deformable Partial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI). TransHE processes the painted SMPL (a parametric human model) using transformers under the canonical space to capture global relationships between human parts and remove pose misalignment. DPaRF deforms the output of TransHE to the observation space, allowing for robust query point encoding. FDI integrates detailed information from the observation space into the coarse human representation. Experimental results on ZJU-MoCap and H36M datasets demonstrate the superior generalization ability and efficiency of TransHuman, outperforming previous methods by significant margins. The contributions of this work include the proposal of the TransHuman framework, the use of transformers for capturing global relationships in human representations, and the exploration of deformation techniques for query point encoding.