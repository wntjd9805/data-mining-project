This paper introduces a new data augmentation framework for fully supervised action segmentation of untrimmed videos. While there have been advancements in action segmenters and learning strategies, the size of benchmark datasets for action segmentation remains relatively small compared to other vision problems such as action recognition or image classification. This poses challenges in training action segmenters, as they tend to overfit on small datasets. To address this issue, we propose augmenting existing datasets with newly generated video sequences to improve the performance of action segmenters. Unlike previous approaches that focus on spatiotemporal, visual domain augmentation or generate unrealistic videos, our approach augments the original training videos directly in the deep feature space. We modify the deep features of video frames to align them closer to class decision boundaries, making the augmented features more challenging for learning and enabling more robust training of the action segmenter. Additionally, we edit action sequences of the original training videos to provide a greater variety of legal action sequences. We formulate the augmentation process as a deep residual meta-model and utilize reinforcement learning (RL) techniques to optimize the feature modifications. We evaluate our approach on benchmark datasets and demonstrate significant performance improvements for various action segmenters. The proposed framework, called Markov Game Video Augmentation (MVGA), outperforms complex models and achieves comparable performance to state-of-the-art approaches. Overall, our work addresses the limited size of action segmentation datasets and enhances the training of action segmenters through deep feature space augmentation.