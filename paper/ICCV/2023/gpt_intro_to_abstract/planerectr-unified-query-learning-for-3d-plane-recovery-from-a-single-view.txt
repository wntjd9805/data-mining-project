Efficient reasoning about surrounding scenes in a virtual 3D world requires frequent updates to the properties of the scenes. However, there is a trade-off between reconstruction quality and efficiency in the creation of authentic 3D geometry. Sparse primitives like point clouds are lightweight but lack topological structures, while dense geometry like volumetric grids and meshes are computational intensive. Planar representation has been proven to be a reliable alternative, as it is compact, efficient, expressive, and generalizable. Single-view plane recovery/reconstruction is a challenging and ill-posed computer vision problem that has been extensively researched. Early attempts used image processing techniques to extract low-level primitives like line segments and vanishing points to extract planar structures. Convolutional Neural Networks (CNNs) have excelled in vision tasks in recent years and have also been applied to plane estimation. Transformers, another type of fundamental vision model, have also made significant progress in various vision tasks. The success of vision Transformers comes from their global/long-range interaction capabilities and query-based set predictions. Previous single-view plane recovery methods separate the prediction of principal components required for plane reconstruction. To address this, we propose a unified model, named PlaneRecTR, that jointly learns all plane-related tasks through query-based learning. Our model achieves state-of-the-art performance in single view plane reconstruction on benchmark datasets and can benefit from stronger backbone vision models. Our contributions include proposing the first single unified framework for single view plane reconstruction, demonstrating the benefits of our unified query learning, and achieving state-of-the-art performance on public benchmark datasets.