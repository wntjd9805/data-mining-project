Abstract:Deep Neural Networks (DNNs) have made significant progress in various security-sensitive applications. However, recent studies have shown that DNNs are vulnerable to adversarial examples, which can mislead these models with imperceptible perturbations. This poses a significant threat to real-world applications, making it crucial to identify the weaknesses of DNNs and develop effective defenses. Existing adversarial attacks fall into two categories: white-box attacks, which have full access to the target model, and black-box attacks, which have limited access. Transferability, the ability of adversarial examples to mislead multiple models, is a key property of these attacks. However, existing attacks often lack transferability. To address this issue, different techniques such as momentum-based methods and input transformations have been proposed. Among these, input transformations have shown promising results. However, current transformation-based attacks transform images globally without considering the local relationships among objects in the image. In this work, we propose a structure invariant attack (SIA) that applies different transformations locally on different parts of the image to enhance the diversity of transformed images while preserving the global structure. Our empirical results demonstrate the benefits of high diversity in transformed images for improving transferability. We introduce a novel image transformation method and design SIA to generate more transferable adversarial examples. Extensive experiments on the ImageNet dataset show that SIA outperforms baseline attacks, demonstrating its effectiveness and generality.