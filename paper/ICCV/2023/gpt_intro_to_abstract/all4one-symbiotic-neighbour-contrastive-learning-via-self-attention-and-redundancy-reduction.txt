Deep learning (DL) models require large and high-quality training datasets, which can be expensive to construct. Self-supervised learning (SSL) offers a solution by allowing DL models to be trained without the need for large annotated data. Instance discrimination learning is a popular SSL approach that trains the model to be invariant to distortions applied to a single image. However, these approaches lack diversity in positive samples and can suffer from model collapse. Neighbour contrastive learning addresses the diversity issue by using nearest neighbours, but relies solely on the first neighbour, limiting its potential. Our work proposes a more efficient way to contrast information from multiple neighbours by using a self-attention mechanism, such as a transformer encoder, to combine the neighbour representations into a single representation. We introduce a new SSL approach called All4One that combines neighbour contrast, feature contrast, and redundancy reduction objectives. Our approach achieves better representation learning and outperforms single-neighbour contrastive approaches on various datasets and backbones.