Few-shot action recognition is a subfield of general action recognition that aims to learn new categories with limited labeled samples. This approach is advantageous as it avoids the time-consuming and labor-intensive process of data annotation typically associated with supervised tasks, making it more suitable for industrial applications. However, few-shot action recognition faces challenges in learning well-generalized models due to limited learning material. Previous attempts to address these challenges have mainly adopted the metric-based framework and episode training. However, these approaches have failed to effectively address the misclassification problem of videos from similar categories. This paper proposes a novel method called GgHM (Graph-guided Hybrid Matching) for few-shot action recognition. The method utilizes a graph neural network (GNN) for constructing task-oriented features and explicitly learns the correlation between video features using the ground truth of the constructed graph edges. In addition, the paper introduces a hybrid prototype matching strategy that combines frame-level and tuple-level matching to cope with video tasks of different types. A learnable dense temporal modeling module is also designed to enhance the representation foundation. Experimental results on four widely-used datasets demonstrate the effectiveness of the proposed method. The contributions of this paper include applying a GNN for task-oriented feature learning, proposing a hybrid prototype matching strategy, and designing a learnable dense temporal modeling module.