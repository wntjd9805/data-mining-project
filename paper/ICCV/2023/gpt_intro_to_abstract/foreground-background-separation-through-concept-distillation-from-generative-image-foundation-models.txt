Supervised pretraining with large-scale datasets, such as ImageNet, has shown improvements in training efficiency and model performance. The development of self-supervision techniques has further led to the creation of foundation models, which are pretrained models that can be adapted to specialized tasks without manual labels. In the field of natural language processing, large-scale foundation models based on the Transformer architecture have already been established. In this paper, we propose a method using latent diffusion models, which are state-of-the-art generative models for text-to-image generation, for zero-shot foreground-background generation tasks. We leverage the learned latent representations from the foundation model to extract weak segmentation masks based on textual prompts. These masks are then used to fine-tune the latent diffusion model for image synthesis and inpainting. We show that a segmentation model trained using these masks achieves performance close to direct supervision, indicating the potential for labor-intensive image annotation workflows to be replaced by concept distillation from generative foundation models. Our contributions include proposing a self-supervised approach for foreground-background segmentation, providing a framework for extracting importance scores from pretrained diffusion models, demonstrating the feasibility of our method on various segmentation tasks, and experimenting with domain-adapted diffusion models for a medical segmentation task.