Continual Learning (CL) aims to develop models that can learn continuously from streaming data and retain previously acquired knowledge while adapting to new information. However, CL faces the challenge of catastrophic forgetting, where the model forgets previously learned knowledge after being trained on new tasks. Traditional offline CL stores all training batches of the current task and trains the model on these samples multiple times. In contrast, online CL focuses on training samples from each task only once, without access to past batches. Online CL exacerbates the distribution shift problem, as the training data distribution changes over time and differs from both the joint distribution of all tasks and the distribution of the task they belong to.To mitigate catastrophic forgetting in online CL, rehearsal-based algorithms use a small memory buffer to approximate the joint distribution of all training data by storing examples of previous tasks. The model is then trained on this memory buffer together with the current task. However, existing methods often view the task-recency bias as a label distribution shift and tackle it as a class imbalance problem, which leads to sub-optimal performance.In this paper, we propose a Consolidated Backward Alignment (CBA) module that can be used with rehearsal-based methods during training to address catastrophic forgetting. The CBA module can be easily removed during the testing stage, incurring no additional computation cost or memory overhead in inference. We evaluate the performance of the proposed method using four rehearsal-based baselines on various benchmarks. Our experiments demonstrate that the proposed CBA module effectively consolidates previously learned knowledge, as evidenced by improved performance in both online CL and offline CL settings. Furthermore, we investigate the method theoretically from gradient alignment and provide insights into why it can effectively alleviate catastrophic distribution shifts.