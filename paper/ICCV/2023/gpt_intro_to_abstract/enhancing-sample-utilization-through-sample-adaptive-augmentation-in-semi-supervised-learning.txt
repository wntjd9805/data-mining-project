Semi-supervised learning (SSL) is a research topic in computer vision that aims to reduce the cost of manual labeling by utilizing unlabeled data. Recent advancements in data augmentation and consistency regularization have shown effectiveness in SSL. However, not all unlabeled samples are effectively utilized, leading to slow or stagnant model performance improvements. Existing SSL models overlook the problem of effectively utilizing all samples and apply the same fixed strong augmentation strategy to all samples. To address this issue, we propose a sample adaptive augmentation (SAA) approach that identifies and augments "naive samples" in a more diverse manner. SAA consists of two modules: sample selection and sample augmentation. The former selects naive samples based on their historical loss, while the latter applies more diverse augmentation to these samples. Experimental results on SSL benchmarks demonstrate that SAA significantly improves performance. We contribute by identifying and emphasizing the importance of effectively utilizing samples, proposing SAA to address this issue, and verifying its validity on SSL benchmarks. Our approach achieves state-of-the-art performance on CIFAR-10 with 40 labels, improving accuracy from 92.50% to 94.76% (in FixMatch) and from 95.01% to 95.31% (in FlexMatch).