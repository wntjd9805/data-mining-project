Semantic segmentation of LiDAR point clouds is crucial for the safe deployment of self-driving vehicles. It allows vehicles to categorize 3D points into specific classes such as vehicles, pedestrians, traffic signs, etc. This information can be used for various applications including online mapping, building localization, and object tracking. However, processing each LiDAR frame independently results in sparse observations and difficulties in handling occluded objects. Previous approaches that utilize a sliding window approach have limited temporal context, and methods that use a memory representation in a range view (RV) struggle with changes in perspective and occlusion. In contrast, our proposed model, MEMORYSEG, uses a 3D latent memory representation. This representation overcomes the limitations of RV by preserving distances between points and enabling learning of size priors for different classes. The memory is updated recurrently, effectively accumulating evidence from past observations. Our method addresses challenges such as aligning the memory with current observations, fusion of sparse memory and dense observations, and handling moving objects. Extensive experiments on benchmark datasets demonstrate that MEMORYSEG outperforms current state-of-the-art LiDAR segmentation methods.