Deep neural networks (DNNs) have achieved remarkable success in computer vision tasks. However, these networks are susceptible to adversarial examples, which are carefully crafted inputs meant to deceive the model. These adversarial examples have also shown to have good transferability, meaning they can fool different models. Recently, a new type of adversarial perturbation called universal adversarial perturbation (UAP) has been discovered. Unlike instance-specific adversarial examples, UAPs can deceive a majority of samples. Generating UAPs is challenging as they need to be effective on various unknown samples and tasks. Currently, there are two main issues in generating UAPs: gradient instability and quantization error. Gradient instability occurs due to the variation in optimization paths for adversarial perturbations, hindering the optimization of attacks. Quantization error, on the other hand, is caused by limitations in the precision of numerical representations. To address these issues, we propose a method called stochastic gradient aggregation (SGA) that enhances gradient stability and reduces quantization error. Our experimental results demonstrate that UAPs generated using SGA exhibit superior generalization compared to existing state-of-the-art methods.