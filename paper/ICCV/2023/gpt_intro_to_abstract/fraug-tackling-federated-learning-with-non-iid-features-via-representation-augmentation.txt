Federated Learning (FL) is a machine learning paradigm that trains a shared model using decentralized data sources. FL offers improved client-server communication efficiency and enhanced data confidentiality by not requiring direct access to client local datasets. However, real-world FL applications face challenges due to non-independent and identically distributed (non-IID) client datasets, leading to local model drifts and overfitting. While previous methods focus on non-IID label space heterogeneity, the problem of feature distribution shift is prevalent in various applications. In this paper, we propose Federated Representation Augmentation (FRAug), a privacy-preserving FL algorithm, to address feature space heterogeneity. FRAug applies data augmentation in the low-dimensional feature embedding space, which is efficient and has fewer confidentiality threats. It aggregates knowledge from different clients in the embedding space and employs a Representation Transformation Network (RTNet) to align client-agnostic synthetic embeddings with the local feature distribution. FRAug achieves state-of-the-art results on benchmark datasets with feature distribution shift and outperforms concurrent FL methods. It is also demonstrated to be applicable in real-world medical datasets. The contributions of this paper include the proposal of FRAug, comprehensive experiments on benchmark datasets, and verification of the maturity and scalability of FRAug on a real-world medical dataset.