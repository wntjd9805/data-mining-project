Anticipating future steps is important for smart AI agents to assist humans in procedural tasks. This is particularly challenging in the realm of cooking activities, where understanding the current step and the recipe being made is necessary. Previous work has used text-based recipes to train anticipation models, but these models only predict a single future realization and do not account for the variability in recipes. In this paper, we propose a new model called GEPSAN, which learns a generative distribution of possible next steps and allows for multiple plausible outputs. Our model consists of a modality encoder, a generative recipe encoder, and an instruction decoder. We use a pretrained video-language feature extractor as our encoder, allowing our model to generalize to video step anticipation without any fine-tuning. Our contributions include the proposal of GEPSAN and demonstrating its state-of-the-art performance in next step anticipation from video, generating diverse and plausible next steps.