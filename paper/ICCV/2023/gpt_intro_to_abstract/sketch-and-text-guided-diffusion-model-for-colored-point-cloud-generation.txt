This paper introduces the concept of Sketch and Text Guided Probabilistic Diffusion (STPD) model for colored point cloud generation. The authors argue that combining sketches and textual descriptions can reduce shape ambiguity and provide more geometric details. They propose a staged probabilistic diffusion model that conditions the denoising process with both sketch and text inputs. To handle the sparsity of sketches, an attention capsule based feature extraction module is introduced. A sketch-text fusion network is also proposed to extract shape and color information from both modalities. Extensive experiments on the ShapeNet and ModelNet40 datasets demonstrate the effectiveness of the proposed model for colored point cloud generation, 3D object classification, and part segmentation. The STPD model achieves state-of-the-art performance in these tasks.