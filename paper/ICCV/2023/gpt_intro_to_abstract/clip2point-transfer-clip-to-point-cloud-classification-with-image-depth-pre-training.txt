Vision-language (V-L) pre-training has shown remarkable success in computer vision tasks. However, there is a domain gap between depth maps and CLIP pre-training images, limiting the effectiveness of pre-training across 3D vision and language. Existing 3D pre-training methods lack sufficient training data, resulting in the need to train deep networks from scratch on downstream datasets. To address this, we propose CLIP2Point, a pre-training scheme that aligns RGB images and depth maps through cross-modality learning and enhances depth aggregation through intra-modality learning. We reconstruct multi-view images and depth maps from 3D models and show that CLIP2Point significantly improves zero-shot point cloud classification performance. To adapt CLIP2Point to downstream tasks, we introduce GDPA, a dual-path structure with global-view aggregators and gated fusion. Extensive experiments demonstrate that CLIP2Point achieves state-of-the-art results on zero-shot, few-shot, and fully-supervised point cloud classification tasks.