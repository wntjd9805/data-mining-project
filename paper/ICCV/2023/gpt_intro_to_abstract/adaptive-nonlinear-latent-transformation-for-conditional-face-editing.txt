Face editing has become an important research area with various applications in entertainment and forensics. Previous works have focused on network architectures and loss functions for face editing using Generative Adversarial Networks (GANs). However, these methods are limited in their ability to handle high-resolution images and generalize to different tasks. StyleGAN has made significant progress in synthesizing photorealistic faces, and its intermediate latent space allows for semantic manipulation of faces. Existing methods for face editing using StyleGAN rely on supervised or unsupervised techniques to manipulate the latent codes. While these methods can produce meaningful transformations, they lack precise user control and may over-manipulate the latent codes. This paper introduces AdaTrans, a novel framework for conditional face editing that addresses these limitations. AdaTrans incorporates an adaptive nonlinear transformation strategy to dynamically estimate the editing direction and step size based on the target attributes and transformation trajectory. It also maximizes the likelihood of edited latent codes and regularizes the transformed trajectory in the distribution of the latent space. Additionally, AdaTrans introduces a disentangled learning strategy that attenuates the entanglement between attributes and reduces the reliance on labeled data. Experimental results demonstrate the effectiveness of AdaTrans in producing disentangled and multi-attribute face editing, even in scenarios with large age gaps or limited labeled data.