Multi-label image classification is a complex task in computer vision that requires predicting multiple objects within an image. It has practical applications in various fields such as image retrieval, scene understanding, recommendation, and biology analysis. This task is more challenging compared to single-label classification as it requires considering label correlations and interactions between the visual and textual domains. Several approaches have been developed to address these challenges, including learning semantic label representations and adopting cross-modal attention-based frameworks. However, these methods often rely on pre-defined label graphs or implicit label dependency exploration, leading to less effective predictions. In this paper, we propose a novel vision-text alignment framework based on conditional transport theory. We formulate multi-label classification as a conditional transport problem, where an image is represented as two discrete distributions over patches and labels. We use the conditional transport distance as a measure of semantic consistency and align the label set to be as close to the visual set as possible. Our framework extracts patch and label embeddings using a Vision Transformer and a BERT model, respectively. We utilize a domain-specific template to reduce the domain shift between the language model and the multi-label classification task. Patch features are collected as a discrete probability distribution, and textual labels are represented as a discrete distribution normalized by the ground-truth labels. The cost matrix in the conditional transport is defined based on the similarity between patches and labels. By minimizing the bidirectional conditional transport cost, we optimize patch-label alignments. We propose sparse and layer-wise formulations to reduce computational cost and enhance interactions across modalities. Experimental results on benchmark datasets demonstrate the effectiveness of our proposed model, showcasing consistent improvements in classification performance.