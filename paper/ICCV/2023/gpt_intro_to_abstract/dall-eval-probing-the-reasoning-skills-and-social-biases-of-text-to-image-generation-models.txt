Generating images from textual descriptions using machine learning is an active area of research. DALL-E, a transformer model trained to generate images from text, has demonstrated novel generation capabilities. However, there has been a lack of concrete quantitative analysis of these models' abilities. Most existing works have evaluated text-to-image generation models using image-text alignment and image quality metrics. To provide new insights, we propose evaluating these models' visual reasoning skills and social biases. We conduct experiments using four publicly available models, and introduce a diagnostic evaluation dataset called PAINTSKILLS to measure object recognition, object counting, and spatial relation understanding. Our experiments reveal that while recent models perform well in object recognition, they struggle with object counting and spatial relation understanding. We also evaluate social biases in the generated images, finding that certain biases are present. Our contributions include the introduction of PAINTSKILLS and the assessment of gender and skin tone bias. Our findings suggest that current text-to-image generation models have room for improvement in challenging visual reasoning skills and addressing social biases. We hope that our evaluation work will facilitate further progress in this field.