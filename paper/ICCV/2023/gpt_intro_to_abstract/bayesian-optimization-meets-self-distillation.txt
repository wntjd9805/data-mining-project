Convolutional Neural Networks (CNNs) have achieved remarkable success in computer vision applications, but their performance depends on hyperparameter configuration. Manual exploration of the hyperparameter space is time-consuming and often leads to sub-optimal performance. Bayesian optimization (BO) has emerged as an effective approach to automating hyperparameter optimization, but it only transfers partial knowledge from previous trials. Self-distillation (SD) has shown promise in knowledge transfer, but it has not been integrated with the BO process. In this paper, we propose a new framework, BOSS, which combines BO and SD to leverage knowledge obtained from previous trials. BOSS suggests hyperparameter configurations based on observations likely to improve performance and selects pre-trained networks from previous trials for further training using SD. Through extensive evaluations, we demonstrate the efficacy of BOSS in improving model performance across various computer vision and medical image analysis tasks. We also provide insights into effective transfer of prior knowledge for CNNs. The main contributions of this paper are the BOSS framework, evaluation experiments showcasing its effectiveness, and in-depth analysis and ablation studies to understand knowledge transfer for CNNs.