Diffusion models have revolutionized image synthesis by generating high-quality and diverse images. However, existing text-to-image algorithms based on diffusion models suffer from fidelity issues, resulting in discrepancies between generated images and text descriptions. These errors can be attributed to inaccurate cross-attention on the text, which controls both spatial and temporal dimensions of the generation process. To address this problem, this paper proposes a new text-to-image algorithm that incorporates explicit control over the spatial-temporal cross-attention map. The algorithm includes a layout predictor and a spatial-temporal attention optimizer to improve fidelity. The layout predictor generates spatial layouts for mentioned objects, while the attention optimizer controls the attention according to the layout. By emphasizing attention over object descriptions and gradually shifting focus from global to local details, the proposed algorithm produces images that better align with descriptions. Experimental results demonstrate the effectiveness of the method in addressing fidelity issues and provide insights into fine-grained control of diffusion models in text-to-image generation tasks.