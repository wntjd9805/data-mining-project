Large-scale vision-language models like CLIP, ALIGN, and CoCa are revolutionizing how we interact with visual representations. These models align the representations of natural images with their textual descriptions and have shown exceptional performance in various tasks. However, the stability of the class prompts used in these models has been questioned. Prompt tuning has emerged as a solution, where a learnable prompt is trained from a small target dataset. Despite its effectiveness on accurately annotated datasets, the robustness of prompt tuning to noisy labels has been overlooked. In this paper, we demonstrate the robustness of prompt tuning to noisy labels and investigate the underlying mechanisms. We hypothesize that the joint text and image embeddings of vision-language models provide a well-defined structure to the classification space, compensating for the degradation caused by label noise. Our experiments confirm the hypothesis and reveal that prompt tuning outperforms traditional fine-tuning and linear probing paradigms in handling noisy labels. We also highlight the importance of keeping the class name in the prompt to regularize the class embeddings. Additionally, we show that CLIP's own predictions can be used to enhance its prompt tuning and improve prediction accuracy. We make the following contributions: demonstrating the robustness of prompt tuning to noisy labels, enhancing robustness through a robust training objective, analyzing the key components contributing to robustness, and proposing a method for unsupervised prompt tuning using noisy pseudo-labels. This method outperforms prior work and achieves improved zero-shot performance even with noisier pseudo-labels.