This paper introduces Box-Constrained Diffusion (BoxDiff), a training-free approach for text-to-image synthesis that allows for control over spatial conditions. Existing models in this field can only be conditioned on class labels or text prompts, limiting their potential in practical applications such as art creation. BoxDiff solves this problem by incorporating simple spatial constraints, such as boxes or scribbles, during the denoising step of Stable Diffusion models. This approach does not require additional model training or massive paired layout-image data. The authors also explore representative sampling to mitigate fidelity issues caused by strong constraints. Experimental results demonstrate that BoxDiff can synthesize photorealistic images following given spatial conditions, surpassing the limitations of existing methods. The contributions of this paper include the proposal of BoxDiff as a training-free solution for text-to-image synthesis, the seamless integration of spatial constraints into the denoising step, and the ability to generate diverse visual concepts beyond the closed world.