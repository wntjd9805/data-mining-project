Automatic radiology report generation has gained significant research interest as a potential intelligent assistant for relieving radiologists' workload. Current methods utilize an encoder-decoder framework, where a medical image is encoded into latent representations using convolutional neural networks (CNNs), and then decoded into natural language sentences using recurrent neural networks (RNNs) or Transformer networks. However, there are challenges in establishing cross-modal alignments (CMA) between visual and textual semantics. These challenges include the different characteristics of continuous visual signals and discrete textual data, the lack of cross-modal interactions in encoding, and the data deviation problem where important details in the report may be sparse. In this paper, we propose a Unify, Align, and Refine (UAR) framework to address these challenges and improve CMA in automatic radiology report generation. The framework includes a Latent Space Unifier (LSU) that tokenizes images into discrete visual tokens, a Cross-modal Representation Aligner (CRA) that learns global CMA, and a Text-to-Image Refiner (TIR) that improves local CMA. Experimental results on benchmark datasets demonstrate that our UAR framework outperforms state-of-the-art methods in terms of accuracy and generates faithful radiology reports.