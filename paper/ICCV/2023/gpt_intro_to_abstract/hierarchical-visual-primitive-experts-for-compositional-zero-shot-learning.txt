This paper introduces the concept of compositional zero-shot learning (CZSL), which involves recognizing novel compositions of known components. The current approaches for CZSL involve independently predicting attribute and object scores, which makes it difficult to consider the contextual interactions between different primitives. Previous methods have addressed this problem by modeling the compositional label embedding for each class and aligning them with visual features. However, the extent to which contextuality is taken into account in the visual domain is still limited. This paper proposes a Composition Transformer (CoT) that aims to enhance visual discrimination and contextuality in CZSL. The CoT utilizes object and attribute experts to generate representative object and attribute embeddings from different layers of the visual network. Contextuality is explicitly modeled through an object-guided attention module that explores intermediate layers of the backbone network. Additionally, a simple minority attribute augmentation (MAA) methodology is introduced to address biased predictions caused by imbalanced data distribution. Experimental results demonstrate that the CoT and MAA significantly improve performance on several CZSL benchmarks, achieving state-of-the-art results.