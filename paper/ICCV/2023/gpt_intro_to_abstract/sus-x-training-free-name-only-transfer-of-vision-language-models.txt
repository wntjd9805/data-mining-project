Vision-language pre-training has become a popular topic in the field of machine learning, with various vision-language models (VLMs) emerging that show exceptional performance on tasks such as classification, cross-modal retrieval, and segmentation. CLIP, a prominent VLM, is trained on a large dataset of image-text pairs and has pioneered the concept of zero-shot transfer in vision-language settings. However, CLIP's zero-shot performance is limited when the downstream dataset differs significantly from its pre-training distribution. Previous approaches to address this issue involve fine-tuning CLIP on subsets of the target task data, which can be unstable and lead to overfitting. Additionally, access to the true distribution of the target task is often limited. In this paper, we propose a novel method called SuS-X for enhancing the zero-shot transfer abilities of VLMs like CLIP without the need for training. SuS-X leverages a curated support set (SuS) that is based on the names of the target categories rather than actual samples from the target distribution. We introduce the TIP-X framework, which adapts VLMs using SuS, and evaluate its performance on multiple benchmark datasets. Our experiments demonstrate that SuS-X outperforms existing zero-shot methods and achieves state-of-the-art results in the training-free name-only transfer setting. Our contributions include the proposal of SuS-X, the effective curation of support sets using parametric or non-parametric methods, and the introduction of TIP-X as a training-free adaptation method in both name-only transfer and few-shot regimes.