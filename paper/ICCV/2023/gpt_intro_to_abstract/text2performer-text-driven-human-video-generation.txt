Text-guided image synthesis has seen significant advancements in recent years, with impressive performance in generating high-quality images consistent with textual descriptions. However, applying these methods to specific tasks, such as generating videos of garments for e-commerce websites, has proven challenging. Existing large-scale text-to-video models struggle to generate plausible results and maintain temporal consistency. Additionally, these models require massive amounts of training data, limiting their applicability to tasks without extensive paired data. To address these challenges, this paper focuses on text-driven human video generation and proposes a novel framework called Text2Performer. This framework aims to generate human videos with consistent representations and complex out-of-plane motions. Text2Performer leverages VQVAE-based frameworks and decomposes the VQ-space into appearance and pose representations, ensuring the maintenance of human identity and facilitating motion modeling. To model intricate human motions, a continuous VQ-diffuser is introduced, which utilizes a transformer architecture to sample meaningful pose representations. By directly predicting continuous pose embeddings and employing diffusion models, Text2Performer achieves more coherent human motions and complete structures.To support text-guided human video generation research, the Fashion-Text2Video Dataset is introduced. This dataset consists of 600 human videos performing fashion shows, manually segmented into clips and paired with text descriptions.The contributions of this paper include the introduction of the text-guided human video generation task, the development of the Text2Performer framework for generating human videos with minimal training data, the decomposition of VQ-space into appearance and pose representations for improved coherence and motion modeling, and the creation of the Fashion-Text2Video Dataset with human motion labels and text descriptions.