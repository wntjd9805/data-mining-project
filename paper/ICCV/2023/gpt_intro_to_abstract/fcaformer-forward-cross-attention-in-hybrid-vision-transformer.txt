This paper introduces the FcaFormer, a new model block and family of models that enhance the performance of vision transformers by densifying attention patterns. Unlike previous approaches that aim to sparsify attention, the FcaFormer connects the input of the standard multi-head attention module with extra tokens from previous blocks, allowing for better token interactions and reuse of information. The densified connections come with a limited increase in computational cost and can replace transformer blocks in existing architectures. Experimental results demonstrate that the FcaFormer achieves improved performance compared to baselines, with higher accuracy and fewer parameters. The contributions of this paper include proposing a new approach to densify attention patterns, introducing the FcaFormer block with learnable scale factors and a token merge and enhancement module, and constructing models with better performance.