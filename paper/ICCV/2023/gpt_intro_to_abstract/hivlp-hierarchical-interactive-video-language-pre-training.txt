Recent research in the field of Natural Language Processing (NLP) has shown the effectiveness of pre-training models on large-scale uncurated data and then fine-tuning them on specific downstream tasks. This approach has been successfully applied in NLP tasks such as BERT, GPT, and T5. It has also been extended to the computer vision domain with models like CLIP, ALIGN, Florence, and BLIP. However, most existing Video-Language Pre-training (VLP) works use either small-scale datasets or large-scale datasets with less diversity. To address this limitation, this paper proposes a Hierarchical interactive Video-Language Pre-training (HiVLP) approach that leverages a Hierarchical Visual Feature Group (HVFG) to enable multi-modal cross-attention. HVFG includes visual features at different scales, allowing for both global representation and fine-grained interaction. The proposed HiVLP model achieves better accuracy by utilizing the multi-scale HVFG.In addition, this paper introduces a Multi-level Vision Contrastive (MVC) loss for self-supervised learning in HiVLP. This loss function promotes the alignment between visual and textual features by applying global-to-local contrastive learning to each scale in HVFG. The authors demonstrate that using more diverse image-text pairs instead of relying solely on the total amount of training pairs is beneficial for VLP.The contributions of this work include: the introduction of HiVLP as the first hierarchical interaction approach for video-language pre-training, the design of the MVC loss for multi-level vision contrastive learning, the emphasis on the importance of diversity in training pairs, and the achievement of state-of-the-art results in text-video retrieval, video-text retrieval, and video captioning tasks.