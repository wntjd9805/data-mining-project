Sketches are highly expressive and have been extensively studied for image retrieval, particularly in the fine-grained setting. While category-level retrieval of 3D shapes using sketches has been explored, there has been limited research on fine-grained instance-level retrieval. Collecting 2D sketches for 3D shapes is challenging as pre-defined viewpoints are required, making it an ill-posed problem. To address this, we propose a method that eliminates the need for large-scale 2D sketch and 3D shape datasets by leveraging pivoting across two separate domains: paired 2D freehand sketches and 3D shapes. We argue that unlike images, sketches for 3D models can be freely rendered using non-photorealistic rendering algorithms, but synthetic sketches lack generalization to real free-hand sketches. Our approach democratises 2D sketch to 3D shape retrieval by enabling amateur users without art training to achieve decent accuracy with their abstract sketches. The key innovation is the use of pivoting, which bridges the gap between the 2D sketch and 3D shape domains. We train the framework using existing datasets from fine-grained sketch-based image retrieval and ShapeNet. To inject 3D-aware knowledge into the 2D shared encoder, we use a Blind Perspective-n-Points algorithm to solve pose and orientation between 2D projections and 3D shapes. Our method achieves impressive performance in sketch-based shape retrieval, comparable to supervised state-of-the-art methods that use paired 2D sketch and 3D shape data. Overall, our contributions include the democratization of fine-grained 2D sketch to 3D shape retrieval, training without paired data, injecting 3D-aware knowledge, and achieving high performance in shape retrieval tasks.