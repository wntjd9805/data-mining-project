In this paper, we address the computational burden of video Transformers by proposing a pruning strategy called Semantic-aware Temporal Accumulation (STA). STA is designed to reduce computation while maintaining accuracy by pruning spatio-temporal tokens based on their temporal redundancy and semantic importance. We evaluate our method on two popular video Transformers, ViT and VideoSwin, and demonstrate significant computation reduction with minimal accuracy drop on action recognition benchmarks. Our method outperforms previous approaches and achieves notable accuracy gains at reduced FLOPs.