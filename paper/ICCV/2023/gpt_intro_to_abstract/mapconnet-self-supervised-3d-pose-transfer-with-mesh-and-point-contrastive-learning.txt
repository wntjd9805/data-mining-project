3D pose transfer is a challenging task that involves transferring the pose of a source geometry to a target geometry while preserving the target's identity. This task has various applications in areas such as animation, human modeling, and virtual reality. However, current methods for 3D pose transfer have limitations, as they often require ground truth correspondence labels or neglect the issue altogether. Requiring ground truth correspondence is expensive, while neglecting correspondence can negatively impact model performance.In this paper, we propose a self-supervised framework called MAPConNet for 3D pose transfer. This framework does not require ground truth correspondence labels or target outputs for supervision. It can be applied in supervised, unsupervised, and semi-supervised settings, allowing for more flexibility in data collection. Our approach is based on contrastive learning, where we introduce mesh-level and point-level contrastive losses to disentangle pose and identity representations and enforce similarity between corresponding points.We present MAPConNet as a novel self-supervised network for 3D pose transfer, achieving state-of-the-art results in human and animal data. Our framework demonstrates competitive and generalizable performance in supervised, unsupervised, and semi-supervised settings. By addressing the limitations of current methods and introducing contrastive learning, our approach improves the quality of pose transfer in complex shapes, such as humans and animals. Overall, our work contributes to the advancement of self-supervised learning on 3D data and has potential applications in various domains.