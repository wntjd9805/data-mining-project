Vision transformers (ViTs) have achieved significant advancements in various vision tasks, but they suffer from data-hungry and overfitting issues. To address these challenges, data mixing augmentation techniques such as CutMix have been used in ViTs training, but they still suffer from image-label inconsistency. In this paper, we propose a novel method called Self-Motivated image Mixing (SMMix) to enhance image mixing with ViTs. SMMix simultaneously motivates image and label enhancement with light training overhead. By leveraging the model's bootstrapping capabilities, SMMix uses the image attention score to select the most attentive region from a source image and paste it into the least attentive region of a target image. This max-min attention region mixing helps alleviate the image-label inconsistency issue. Additionally, SMMix assigns fine-grained labels to different regions in the mixed image, including mixed image label, target image label, and source image label, which further enhances the label assignment. To ensure feature consistency between mixed and unmixed images, SMMix introduces a feature consistency constraint that aligns the feature distributions. We evaluate SMMix on various ViT-based models and demonstrate its superior performance with light training overhead. Our proposed method achieves state-of-the-art accuracy without the need for pre-trained models.