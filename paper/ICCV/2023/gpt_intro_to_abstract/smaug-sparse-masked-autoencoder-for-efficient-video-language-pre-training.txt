This paper introduces SMAUG, an efficient pre-training framework for video-language models. The current practice of video-language pre-training shows strong transfer performances but comes with enormous training costs. To address this issue, the authors propose leveraging the Masked Autoencoder (MAE) paradigm to reduce computational burden while achieving satisfactory performance. Additionally, the authors address the issue of redundant information in image patches and frames by introducing a space-time token sparsification module to remove spatial and temporal redundancies. Experimental results on text-to-video retrieval and video question answering tasks demonstrate that SMAUG achieves state-of-the-art or comparable performances across six datasets. Moreover, SMAUG provides a significant speedup in video-language pre-training, reducing the training time to âˆ¼50 NVIDIA A6000 GPU hours.