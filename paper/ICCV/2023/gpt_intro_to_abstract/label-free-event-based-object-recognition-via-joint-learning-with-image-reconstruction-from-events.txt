This paper introduces the concept of event cameras, which are neuromorphic vision sensors that perceive per-pixel brightness changes with high temporal resolution. These cameras have advantages over conventional frame-based cameras, such as high dynamic range and low latency. However, algorithms for processing events are still in their early stages. Object recognition is a crucial task for event feature learning, but existing methods rely on supervision with category labels, which can be limiting. Additionally, the sparsity of events makes accurate labeling difficult when paired images are not available. This paper addresses the challenge of label-free event-based object recognition without category labels and paired images. The authors propose a joint learning framework that combines event-based object recognition and event-to-image reconstruction. They utilize the Contrastive Language-Image Pre-training (CLIP) model to encode textual features that align with visual features. To address issues with predicted categories, the authors introduce a category-agnostic repulsion loss and a reliable data sampling method. They also incorporate the use of unpaired images to improve recognition accuracy and image quality. Extensive experiments demonstrate the effectiveness of the proposed method compared to unsupervised and supervised approaches. The paper also explores the extension of the framework to event-based zero-shot object recognition. The contributions of this work include the proposed joint learning framework, the reliable data sampling strategy, the local-global reconstruction consistency, and the incorporation of unpaired images. The paper concludes with an overview of the extensive experiments and the superior performance of the proposed method.