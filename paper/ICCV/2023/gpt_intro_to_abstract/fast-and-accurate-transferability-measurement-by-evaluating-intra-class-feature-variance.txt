Transfer learning is a crucial concept in machine learning, especially when labeled data in the target domain are limited or expensive to acquire. Within the field of transfer learning, several variants have emerged, including source-free domain adaptation, multi-source transfer learning, heterogeneous transfer learning, and open-set domain adaptation. Fine-tuning a pre-trained model is a common practice in transfer learning to enhance performance in downstream tasks. The selection of the most suitable pre-trained model is a critical step in transfer learning. Transferability measurement plays a vital role in quantifying the transferability of a pre-trained model from a source task to a target task. Existing methods for transferability measurement are either computationally expensive or limited in their applicability. In this paper, we propose a simple yet effective method called TMI (TRANSFERABILITY MEASUREMENT WITH INTRA-CLASS FEATURE VARIANCE) for measuring transferability. TMI quantifies transferability by measuring the intra-class variance, which evaluates the adaptability of the model to a new task. Unlike previous approaches, TMI does not require an optimal feature extractor and classifier, resulting in more accurate measurements. Our experiments demonstrate that TMI outperforms competitors in selecting pre-trained models for various tasks. Additionally, TMI shows a superior trade-off between running time and transferability. We also evaluate the transferability of self-supervised pre-trained models and selecting the best transferring layer, where TMI achieves the highest correlation coefficients. The rest of the paper discusses related works, presents the proposed method, experimental results, and concludes the findings.