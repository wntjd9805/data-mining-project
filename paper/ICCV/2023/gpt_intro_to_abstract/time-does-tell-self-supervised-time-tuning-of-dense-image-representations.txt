Dense self-supervised learning has gained attention in recent years for learning deep features in an unsupervised manner, particularly for unsupervised semantic segmentation. However, videos, with their additional time dimension, have not been fully explored as a source for unsupervised training. This paper addresses this gap by proposing a novel method, called TIMET, that incorporates temporal consistency in dense feature representations using a temporal self-supervised loss. The method starts with a 2D encoder pre-trained on images and fine-tunes it using unlabeled videos, resulting in improved performance for both videos and images. The proposed method overcomes challenges related to correspondence and dense learning by introducing the Feature-Forwarder module and a spatio-temporally dense clustering module. The proposed approach allows for scaling of self-supervised learning by leveraging video datasets and transferring knowledge to the image domain, achieving state-of-the-art performance for unsupervised semantic segmentation in both images and videos. Overall, this paper contributes to the enhancement of dense self-supervised learning and its application to video and image understanding.