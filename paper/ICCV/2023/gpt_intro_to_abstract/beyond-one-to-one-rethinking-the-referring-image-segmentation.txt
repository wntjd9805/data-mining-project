This paper focuses on the task of referring image segmentation, which aims to segment a target object based on a natural language expression. Unlike traditional semantic segmentation, referring image segmentation is not limited by predefined classes and can selectively segment specific individuals based on the description provided. However, existing methods struggle with one-to-many and one-to-zero samples, where the sentence refers to multiple or no targets in the image, respectively. This paper proposes a Dual Multi-Modal Interaction Network (DMMI) to address these challenges and establishes a new comprehensive dataset called Ref-ZOM (Zero/One/Many). The DMMI network incorporates text information into visual features and enables information flow between the visual and linguistic streams. It utilizes two decoder branches to segment the target based on linguistic information and reconstruct missing information based on visual features. Multi-modal contrastive learning and a Multi-scale Bi-direction Attention module are also employed to improve feature interaction. The Ref-ZOM dataset contains more complex and flowery text expressions and surpasses mainstream datasets in terms of size. Experimental results show that the DMMI network outperforms existing methods on multiple benchmarks and exhibits strong generalization capabilities. The contributions of this paper include identifying the limitations of referring image segmentation, proposing a novel DMMI network, introducing the Ref-ZOM dataset, and achieving state-of-the-art performance on various benchmarks.