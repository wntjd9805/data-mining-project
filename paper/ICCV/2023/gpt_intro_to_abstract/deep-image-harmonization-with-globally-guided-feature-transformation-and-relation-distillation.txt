Image composition is a key operation in computer vision that involves combining regions from different images to create composite images. However, when foregrounds are pasted onto background images, inconsistencies in illumination statistics can arise. Image harmonization techniques aim to adjust the foreground illumination to achieve visual consistency in composite images. Deep learning methods have made significant progress in this area. Existing methods have focused on transforming foreground features to enhance performance, but they often lack the guidance of global information. In this work, we propose a method called GiftNet (globally guided feature transformation) that uses the bottleneck feature from an encoder as global guidance to transform foreground features. We also introduce a novel dataset called ccHarmony, which simulates natural illumination variation for training deep harmonization models. Our approach outperforms existing methods in extensive experiments on benchmark datasets. Overall, our contributions include the development of a globally guided feature transformation method, the addition of intermediate supervision for encoder features, the creation of the ccHarmony dataset, and the demonstration of superior performance compared to existing methods.