Recent research has shown that vision-language pre-training (VLP) models are vulnerable to adversarial examples, despite their impressive performance on various multimodal tasks. Previous work has primarily focused on white-box attacks, where information about the victim model is accessible. However, the transferability of adversarial examples across VLP models in a practical setting, where the victim model is unknown, has not been investigated. This lack of understanding poses a significant security risk for deploying VLP models in real-world applications. This paper addresses this gap by investigating the transferability of adversarial examples across VLP models. We propose a novel approach called SGA (Supplemental Gradient-Augmented) that enhances adversarial transferability through alignment-preserving augmentation and well-designed cross-modal guidance. SGA generates adversarial examples on multimodal augmented input data, disrupting the interactions between modalities for better perturbations. Additionally, SGA iteratively pushes supplemental information away between two modalities under the supervision of another modality, allowing for the perception of gradients from multiple guidance.Experiments conducted on two widely used multimodal datasets demonstrate the effectiveness of SGA in generating adversarial examples that can be successfully transferred across VLP models. SGA outperforms state-of-the-art attack methods in multimodal learning, achieving notable improvements in image-text retrieval under black-box settings and exhibiting superior performance in white-box attack settings. Furthermore, SGA surpasses existing methods in image captioning and yields higher fooling rates on visual grounding.This paper makes the following contributions: 1) It explores the transferability of adversarial examples on popular VLP models through a systematic evaluation2) It introduces SGA, a novel transferable multimodal attack that enhances adversarial transferability through alignment-preserving augmentations and cross-modal guidance3) Extensive experiments demonstrate that SGA consistently improves adversarial transferability across different VLP models compared to state-of-the-art methods.