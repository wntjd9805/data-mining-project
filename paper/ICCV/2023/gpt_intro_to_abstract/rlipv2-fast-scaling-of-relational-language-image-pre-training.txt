The pretraining-finetuning paradigm has made significant advancements in the vision and language domains, particularly through aligned Vision-Language Pre-training (VLP). RLIPv1 was the first attempt to align vision representations and relational texts using VLP, achieving success in Human-Object Interaction (HOI) Detection. However, RLIPv1 faced challenges in terms of slow convergence and scarcity of annotated data. In this paper, we introduce RLIPv2, a model that addresses these challenges by employing a faster converging approach and leveraging larger-scale pseudo-labelled scene graph data. From the model perspective, RLIPv2 incorporates Asymmetric Language-Image Fusion (ALIF), enabling fusion in the encoding stage with sparsified language layers. From the data perspective, we extend object detection datasets with relational annotations using pseudo-labelling techniques. We employ external captioners to generate relation descriptions and utilize RLIPv2 as a Relation Tagger (R-Tagger) to assign relation texts to region pairs. This approach allows us to investigate the scaling behavior of RLIPv2 and achieve improved performance in zero-shot, few-shot, and fine-tuning scenarios. Additionally, we introduce Scene Graph Generation (SGG) as an evaluation task for RLIPv2, showcasing its state-of-the-art performance on Open Images v6 dataset. Overall, RLIPv2 demonstrates its efficacy in tackling relational reasoning tasks and offers promising results in the vision and language domains.