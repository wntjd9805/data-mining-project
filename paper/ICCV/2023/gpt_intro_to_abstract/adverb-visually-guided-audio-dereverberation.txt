Reverberation, caused by audio signals reflecting off surfaces in the environment, degrades the sound quality of far-field speech recordings. The amount of reverberation is influenced by the geometry and materials present in the surroundings. Recent studies have shown that reverberation effects can be estimated from a single image of the environment. Removing reverberation from recorded speech signals is desirable for improving performance in various downstream tasks. While audio-only dereverberation has been extensively studied, using visual cues to solve this problem is relatively underexplored. This is mainly due to the lack of datasets containing both audio and visual information. However, in real-world settings, reverberant audio is naturally accompanied by a visual stream, such as in video conferencing or augmented reality applications. Recent advancements in audio-visual speech enhancement methods have demonstrated significant improvements over audio-only approaches. These methods utilize visual stimuli, such as lip movements, to separate noise from the voice components in degraded speech. However, these approaches assume that the speaker is near and facing the camera, which may not always hold true in mid/far-field scenarios. This paper proposes AdVerb, a novel audio-visual dereverberation framework that leverages visual cues of the environment to estimate clean audio from reverberant audio. AdVerb utilizes a modified conformer block with positional encoding to learn audio-visual dereverberation. It incorporates a geometry-aware module with cross-modal attention between audio and visual modalities to generate a complex ideal ratio mask, which is applied to the reverberant spectrogram to obtain the estimated clean spectrogram. AdVerb solves two objectives, Spectrogram Prediction Loss and Acoustic Token Matching Loss, to retain phonetic and prosodic properties in the output audio. Experimental results show that AdVerb outperforms traditional audio-only and audio-visual baselines in speech enhancement, speech recognition, and speaker verification tasks. User study analysis also demonstrates superior perceptual audio quality assessment compared to prior approaches.