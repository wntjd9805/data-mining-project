This paper introduces a novel lip reading method for low-resource languages that combines general speech knowledge and language-specific knowledge. The method involves training a visual encoder to predict speech units from lip movements in a high-resource language, and a Language-specific Memory-augmented Decoder (LMDecoder) that translates learned speech representations into text using audio-text paired data in the target language. The proposed method is evaluated on English, Spanish, French, Italian, and Portuguese datasets, achieving state-of-the-art performance on English data. The contributions of this paper include analyzing the effectiveness of different pre-training methods and proposing a new approach for developing lip reading models for low-resource languages.