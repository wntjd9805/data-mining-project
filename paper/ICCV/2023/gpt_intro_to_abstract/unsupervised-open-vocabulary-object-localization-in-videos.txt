Deep learning has achieved significant advancements in object-detection and -recognition in both images and videos. However, these models rely on supervised learning, which necessitates expensive manual annotation of training data and may lead to overfitting and limited generalization. Self-supervised learning methods offer a potential solution by utilizing unlabeled visual data to unlock the full potential of deep learning. Recent research has made progress in training segmentation models without supervision, such as using vision transformers and self-supervised learning to create a feature space that clusters patterns of the same object class. Another approach, called object-centric learning (OCL), localizes individual objects in static images. However, few studies have applied the slot-based OCL approach to real-world video data without weak annotation. In this paper, we propose an OCL pipeline for real-world video data with the goals of spatially, temporally, and semantically grouping videos without labeled training data and labeling those groups using an off-the-shelf vision-language model. We utilize CLIP, a vision-language model trained to align text with global image features, and fine-tune its last layer using a self-supervised objective on image data only. Our framework consists of three parts: bottom-up object-centric video representations, adapting CLIP to assign text labels to video slots, and a merging procedure to improve localization and labeling using overlapping information from text and image. Our contributions include the first approach to localize objects in real-world videos without labeled training data and assigning text labels to video slots using a pre-trained CLIP model without additional supervised fine-tuning.