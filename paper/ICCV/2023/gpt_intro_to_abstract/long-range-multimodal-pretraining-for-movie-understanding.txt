Movies are not only a form of entertainment but also serve as a source of inspiration and influence societal behaviors. They have become a prominent topic of study in the computer vision community, serving as a testbed for measuring progress in visual recognition, reasoning, and creative editing. Additionally, movie data has been leveraged to train various AI systems, including computer vision, machine listening, and NLP. While understanding movies has gained attention, it remains an open question how to develop models that utilize abundant sources of movie data to tackle movie-related tasks.Learning from movies poses challenges, especially when labels are unavailable. Existing video self-supervised approaches focus on short clips and do not fully leverage the richness of movies, which lies in their long-range dependencies. The end-to-end learning schemes in these approaches are not feasible for movies due to the computational limitations of encoding long-form sequences.Recent works on long-form video understanding have attempted to address these limitations but mainly focus on the video modality and specific tasks, disregarding audio and text signals. In this work, we argue that long-form videos are rich in visual, auditory, and textual information, and integrating these modalities would lead to a better and more generalizable understanding.We propose a pretraining strategy that leverages multimodal cues and long-range reasoning in movies. Our model is designed to be easily transferred to a range of movie analysis and understanding tasks. We use shot-based input sampling to capture longer multi-shot content, and pretrained base encoders to efficiently encode the input sequence in video, audio, and text modalities. We perform multimodal long-term reasoning using Transformer networks, following a hierarchical approach that combines contextual transformers and cross-modal transformers. We introduce carefully designed losses to enforce intra-modal, inter-modal, and cross-modal relationships over long-range observations.We train and evaluate our model using publicly available movie datasets, demonstrating the transferability and performance benefits of our approach across six different benchmarks. Our contributions include introducing a pretraining strategy that leverages long-range multimodal cues and conducting extensive experiments to validate the effectiveness and transferability of our model. The results consistently outperform the state-of-the-art in movie understanding tasks.