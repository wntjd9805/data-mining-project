Humans excel at solving new tasks while retaining previous knowledge, but deep neural networks often suffer from catastrophic forgetting, where they forget old knowledge when trained for a novel task. Continual learning aims to address this challenge by allowing models to learn sequentially without access to previous data. While continual learning methods have made progress in computer vision, they are primarily designed for convolutional neural networks (CNNs), and vision transformers have received less attention in this area. This paper focuses on continual learning for vision transformers and proposes a dynamically expandable architecture called ConTraCon. ConTraCon utilizes convolutional adaptation on previously learned transformer weights to obtain new weights for new tasks, while a learnable skip-gating mechanism balances between retaining old knowledge and adapting to new information. The proposed approach leverages the learning capabilities of vision transformers with low memory overhead. The authors reweight the key, query, and value weights of the transformer by convolving them with small filters, effectively capturing local dependencies and allowing for minimal increase in model size. To handle scenarios where task information is unavailable during inference, the paper proposes an entropy-based criterion for inferring the task of an image, using multiple augmented views and evaluating the agreement of predictions from different task-specific models. Extensive experiments on benchmark datasets demonstrate the superiority of ConTraCon over existing methods, including exemplar-based approaches using transformers as backbone architectures. ConTraCon achieves better performance with a significant reduction in parameters. The paper concludes with a discussion of the proposed contributions, including the ConTraCon architecture, the entropy-based approach for task inference, and the experimental validation and ablation studies.