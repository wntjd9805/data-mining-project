This paper introduces the concept of survival prediction, which is a complex task in cancer prognosis. The task involves integrating qualitative morphological information from pathology data and quantitative molecular profiles from genomic data. Despite recent advancements in multimodal learning for histology and genomics, there are still several challenges that need to be addressed. One major challenge is effectively fusing key information from heterogeneous modalities, particularly when dealing with large whole slide images. Additionally, the fine-grained visual recognition problem associated with tumor microenvironment (TME) within pathological images poses another challenge. Attention-based multiple instance learning (MIL) has been utilized to identify informative instances, but it lacks a global perspective and may not thoroughly capture information about TME. To address these challenges, the paper proposes a Multimodal Optimal Transport-based Co-Attention Transformer (MOTCat) framework that leverages optimal transport-based co-attention to match instances between histology and genomics from a global perspective. This approach allows for the identification of TME-related patches by considering both spatial associations within TME and genes co-expression. The proposed framework offers advantages over conventional co-attention mechanisms, including global awareness, rigorous matching without labels, and reduced cross-modal heterogeneity gap. A robust and efficient implementation of the framework is also proposed to handle the computational complexity associated with gigapixel whole slide images. Experimental results on five benchmark datasets demonstrate the effectiveness of the proposed method compared to state-of-the-art techniques.