Deep learning models in computer vision have delivered impressive results, but their lack of transparency poses challenges in terms of user trust and their application in safety-critical domains. Explainable artificial intelligence (XAI) has emerged as a solution to address this issue by providing human-comprehensible explanations for deep neural models. However, evaluating XAI methods is challenging due to the absence of ground-truth explanations and the limited quantitative evaluation present in current research. To tackle this, many XAI methods rely on proxy tasks that involve image interventions, such as removing input features, to measure their impact on model output. However, existing evaluation protocols based on pixel-level interventions have limitations in terms of their relevance to human comprehension, their extension to other explanation types, and the introduction of domain shifts. To address these challenges, this paper proposes a thorough evaluation/analysis tool for XAI methods. This is achieved through the creation of a synthetic classification dataset of artificial bird species renderings, which allows for controlled analysis and evaluation of various dimensions of explainability, comparison of different explanation types, avoidance of out-of-domain issues, reduction of the gap between human comprehension and XAI evaluation, automatic analysis of explanation coherence, and examination of multiple combinations of XAI methods and neural models.