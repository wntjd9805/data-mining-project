The demand for 3D content in industries such as film, gaming, simulation, and social media has increased significantly. However, manually creating 3D models to meet this demand is time-consuming and inefficient. While researchers have made progress in generating geometry for 3D models, less attention has been given to generating textures. Existing methods for texture generation often rely on fixed geometric inputs and do not fully consider the interdependent relationship between geometry and texture. This paper proposes a novel approach called DG3D, which combines a generative adversarial network with an adaptive diffusion-augmented module to address the issues of mode collapse and texture incompleteness. DG3D generates 2D features and aligns them with the 3D tetrahedral space to extract geometry and generate textures. Multiple 2D discriminators are used for multi-modal renderings of the textured mesh, providing more supervision and improving the quality of the generated shapes. Extensive experiments and ablations are conducted to demonstrate the effectiveness of DG3D, along with discussions on limitations and potential future improvements.