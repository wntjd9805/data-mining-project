Transformers trained for masked token prediction have recently been applied to neural image and video generation. These models use BERT-like random masks during training and predict tokens in groups during inference, conditioning each inference step on tokens generated in previous steps. By minimizing the cross entropy between the token distribution modeled by the transformer and the true token distribution, these models can be used as compression models. In this paper, we propose a transformer-based approach for neural image compression that outperforms previous models in terms of rate-distortion performance. Our approach uses off-the-shelf transformers and combines ideas from MaskGIT-like input-masked transformers and fully autoregressive attention-masked transformers. To train these models, tokens are uniformly selected and masked in each training step, while during inference, the models generate tokens by predicting a distribution and uncovering a subset of tokens at the input. We show that a fixed schedule performs just as well in terms of negative log likelihood, allowing us to bridge between fully autoregressive and MaskGIT-like transformers. Our core contributions include a vanilla MaskGIT-like transformer that achieves state-of-the-art neural image compression results, and a modified model that masks the transformer twice, resulting in faster runtimes and improved performance.