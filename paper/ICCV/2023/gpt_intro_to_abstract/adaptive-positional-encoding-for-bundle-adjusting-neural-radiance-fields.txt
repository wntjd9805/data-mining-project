Simultaneously localizing and reconstructing scenes from 2D images is a crucial task in computer vision. Traditional methods rely on texture patterns to find correspondences and estimate camera parameters and a sparse 3D point cloud. However, these methods have limitations in creating dense geometric representations. This paper introduces Neural Radiance Fields (NeRF), which use an implicit neural network to parameterize scenes and achieve high-fidelity dense representations. Existing NeRF methods require accurate camera parameters obtained through off-the-shelf tools, limiting their applicability. Some methods optimize camera parameters through frequency encoding but struggle to capture fine-grained information. Positional encoding (PE) plays a critical role in NeRF methods, but fixed handcrafted frequency parameters limit their performance. This paper proposes adaptive positional encoding (APE) for joint optimization of neural radiance fields and camera parameters. Inspired by the Fourier series, APE can better fit the distribution of radiance information. Additionally, period-activated multilayer perceptrons (PMLPs) reconstruct scenes and provide more effective and fine-grained gradients for updating parameters. The proposed method is compared to state-of-the-art methods and shown to learn higher-order representations and estimate camera parameters. An ablation study demonstrates the effectiveness of APE and PMLPs. The contributions of this work include the introduction of APE for bundle-adjusting neural radiance fields, which can jointly reconstruct scenes and update camera parameters, and the use of APE and PMLPs to reconstruct radiance fields and provide more accurate gradients.