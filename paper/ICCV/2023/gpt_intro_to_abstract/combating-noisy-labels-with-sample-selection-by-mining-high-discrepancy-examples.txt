Learning with noisy labels is a challenging problem in weakly supervised learning. In daily life, noisy labels are inevitable, arising from sources such as crowd sourcing and web queries. However, the combination of noisy labels and deep networks leads to poor generalization. Traditional regularization techniques do not effectively address this issue. This paper focuses on the sample selection approach to handle noisy labels. Previous works have proposed self-teaching and cooperative sample selection methods, with the latter showing better performance. These methods involve maintaining two different deep networks that can filter different types of errors introduced by noisy labels. To maintain the different learning abilities of the two networks, a strategy called "Update by Disagreement" is used. However, this method is sample-inefficient for network weight updates, resulting in limited utilization of clean examples in training and impairing generalization. To overcome this problem, this paper proposes a robust learning paradigm called CoDis. CoDis leverages the property that deep networks learn patterns first and encourages involvement of training examples with high discrepancies between the two networks. The discrepancy is measured using the distance of prediction probabilities, allowing for a trade-off between clean examples and network divergence. This method is more sample-efficient compared to previous sample selection procedures and improves generalization. Furthermore, examples with high discrepancies in training are likely to be hard examples that are critical for generalization. Experimental results on simulated and real-world noisy datasets demonstrate the effectiveness of the proposed method, with significant improvements in test accuracy, particularly on class-imbalanced datasets.