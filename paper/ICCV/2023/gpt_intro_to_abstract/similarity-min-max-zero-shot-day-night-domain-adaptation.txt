Deep neural networks are sensitive to low illumination, which poses a challenge for safety-critical computer vision applications. Existing low-light enhancement methods aim to restore low-light images, but they are optimized for human visual perception and may not benefit machine vision tasks. Domain adaptation techniques have been proposed to improve machine vision performance at night, but they rely on accessing target domain data. In extreme scenarios such as deep-space exploration or deep-sea analysis, obtaining target domain data may be challenging. To address this, zero-shot domain adaptation has emerged as a promising research direction, where adaptation is performed without accessing the target domain. In the context of day-night domain adaptation, the challenge is learning illumination-robust representations that are generalizable to both day and night modalities. Previous methods have focused on either image-level or model-level adjustments, without fully capturing the complex day-night domain gap. In this paper, we propose a similarity min-max framework that operates at both the image and model level. Our framework generates a synthetic nighttime domain with minimal feature similarity to the daytime domain, and learns illumination-robust representations by maximizing feature similarity across domains. We present a stable training pipeline that includes an exposure-guided module for reliable nighttime image synthesis and multi-task contrastive learning for domain representation alignment. Our approach achieves day-night adaptation without seeing real nighttime images and demonstrates superior performance on various high-level nighttime vision tasks. Our contributions include the proposed framework, the stable training pipeline, and the evaluation of our method on multiple benchmarks.