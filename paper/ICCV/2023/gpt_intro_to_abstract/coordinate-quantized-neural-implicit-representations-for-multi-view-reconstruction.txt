Learning implicit representations from multi-view images presents a challenge in reconstructing 3D geometry in a scene. Current methods utilize coordinate-based neural networks to infer signed distance or occupancy fields through volume rendering. However, these methods struggle to capture high-frequency details. To address this issue, some approaches incorporate positional encodings as vectors formed by sinusoidal functions of coordinates. However, higher frequencies introduce noise and artifacts. To stabilize optimization with high-frequency positional encodings, some methods learn soft masks to gradually expose high-frequency components over training iterations. However, this strategy is tedious to tune. In this paper, we propose the use of quantized coordinates and positional encodings to learn neural implicit representations. Unlike previous methods that use continuous coordinates and positional encodings, we employ discrete coordinates and positional encodings of discrete coordinates. By discretizing the field at extremely high resolutions, we reduce uncertainty and ambiguity in the field during optimization, leading to more effective inference. Our quantized coordinates do not impose additional computational burden or inconsistencies on neighboring coordinates. We evaluate our approach on multiple benchmarks and demonstrate significant improvements over state-of-the-art methods. Our contributions include the introduction of quantized coordinates, an analysis of how discrete coordinates decrease uncertainty and ambiguity, and the seamless integration of our approach with existing methods.