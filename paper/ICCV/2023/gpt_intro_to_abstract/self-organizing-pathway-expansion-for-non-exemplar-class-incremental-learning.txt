Deep neural networks have shown great performance in fully supervised scenarios, but extending their learning capability to open environments remains a challenge. Incremental learning, which involves continuously learning new tasks while maintaining the ability to identify old tasks, is particularly important in this context. Fine-tuning the network with new data can lead to catastrophic forgetting, where the representation and classifier become biased. Previous approaches have focused on maintaining the feature representation of old classes but still suffer from confusion between old and new classes. In this paper, we propose a self-organizing pathway expansion scheme that mitigates feature interference during incremental learning. We also introduce a pathway-guided feature update mechanism that adjusts classification weights based on pathway similarity. Our method achieves superior performance compared to state-of-the-art approaches on multiple benchmark datasets.  Overall, this work contributes to the field of non-exemplar class-incremental learning and provides insights into the optimization process of deep neural networks in open environments.