Temporal action detection (TAD) is the task of predicting the temporal duration and class label of each action instance in an untrimmed video. Existing methods for TAD rely on discriminative learning models, such as regressing anchor proposals or predicting start/end times of proposals. This paper proposes a novel approach called DiffTAD, which integrates denoising diffusion with a one-stage detection pipeline. The authors highlight the challenges of TAD, including big boundary ambiguity and low efficiency in both denoising diffusion and action detection. To address these challenges, DiffTAD uses random temporal proposals and Gaussian noises to create noisy proposals for training. At inference time, DiffTAD reverses the learned diffusion process to generate action temporal boundaries. The paper also introduces a cross-timestep selective conditioning mechanism to improve inference efficiency. Experimental results on ActivityNet and THUMOS benchmarks demonstrate the effectiveness of DiffTAD compared to previous approaches. Overall, this paper presents a new formulation of the TAD problem using denoising diffusion and achieves superior performance in action detection.