Image classification and generation are two important tasks in computer vision, but existing approaches often perform well in one task while struggling in the other. Energy-Based Models (EBMs) have emerged as a promising class of probabilistic models that can explicitly capture complex distributions. By repurposing standard image classification models as image generators, the Joint Energy Model (JEM) leverages gradient information to guide image generation. However, training EBMs is challenging due to the intractability of likelihood computation and unstable sampling. To address these issues, spectral normalization is often used to constrain the energy model's Lipschitz constant. Additionally, diffusion models have shown competitive performance in image generation by perturbing data with noise to populate low-density regions for stable training. Inspired by the flexibility of JEM and the stability of diffusion models, we propose a novel hybrid model called EGC (Energy-Based Classifier and Generator) that achieves superior performance in both image classification and generation tasks. EGC models the joint distribution of noisy images and labels, using the unconditional score to restore images from noise. The classification probability and unconditional score provide guidance for both tasks. We demonstrate the efficacy of EGC on various datasets, showcasing high-fidelity generated samples and superior classification accuracy. EGC outperforms existing methods and achieves remarkable results without the need for Lipschitz constant constraints. We also show that optimizing gradients of explicit energy functions surpasses Langevin sampling and validate the effectiveness of EGC in inpainting, semantic interpolation, high-resolution image generation, and robustness improvement. Our contributions include the proposal of EGC, a diffusion-trained hybrid model, competitive generation and classification results, and the application of EGC in various tasks.