Sign language translation (SLT) is a significant and interdisciplinary research topic that aims to automatically generate spoken language translations based on sign language videos. It holds both social significance, as it can greatly facilitate communication between deaf-mute and hearing individuals, and academic value. However, SLT faces challenges due to the domain gap between the input video and output text, as well as limited dataset scale. Existing SLT systems commonly adopt a one-pass forward pipeline, but the inherent gap between vision and text makes this mapping difficult. To address these challenges, we propose IP-SLT, a novel framework that incorporates an iterative refinement process inspired by human reading. IP-SLT iteratively enhances the semantic representation of the input sign language video through a recurrent structure. Our framework consists of feature extraction, prototype initialization, and iterative prototype refinement components. We also introduce an iterative distillation loss to leverage the sequential dependence between iterations, and our framework can easily work with different visual backbones. Through extensive experiments, we demonstrate that IP-SLT achieves significant performance improvements over baselines on two prevalent benchmarks. Our contributions include the proposal of IP-SLT, the introduction of the iterative distillation loss, and validation through extensive experiments.