Vision Transformer (VT) has gained significant attention in the field of computer vision for its superior performance in various tasks such as image classification, object detection, and semantic segmentation. The first pure transformer model for image classification, ViT, outperformed convolutional neural networks (CNNs) when applied to large training data, leading to subsequent works improving different aspects of VTs. Position embedding (PE) plays a critical role in providing position information for VTs, with two main categories of methods: PE-based methods (absolute and relative PE) and PE-free methods (modules with inductive bias). Most VTs use absolute PE, but joining it directly with patch embedding in the Transformer Encoders has inherent drawbacks that limit performance. In this paper, by analyzing the input and output of each encoder layer in VTs, we propose a new method called Layer-adaptive Position Embedding (LaPE). LaPE uses two independent layer-normalization (LN) for token embeddings and PE in each layer and delivers PE serially across layers. This approach improves the expressive-ness of PE and achieves better performance than the default absolute PE method and even relative PE. LaPE can also be combined with PE-free methods for further performance improvement. Experimental results on image classification, object detection, and semantic segmentation tasks show that LaPE is a general and effective method for VTs, significantly enhancing model performance with minimal overhead. Overall, our contributions include theoretical analysis on the limitations of absolute PE, proposing the LaPE method, and demonstrating its effectiveness on multiple tasks.