Generative adversarial networks (GANs) have shown impressive capabilities in generating high-quality faces by manipulating a latent code. The latent space of a well-trained GAN is semantically organized, allowing for attribute manipulation by shifting the latent code. While this strategy has become standard for editing images, its application to videos has been less explored. Maintaining temporal coherence is crucial for video editing, as both GAN inversion and attribute manipulation can introduce discontinuity across frames. Existing methods have attempted to address this issue but are video- and attribute-specific, leading to high training costs and limited generalization ability. To address these limitations, we propose a Recurrent vIdeo GAN Inversion and eDiting (RIGID) framework that learns temporal correlations between frames for both inversion and editing tasks. RIGID evolves the image-based StyleGAN generator to output temporally coherent frames. In the inversion task, we formulate the process as a combination of an image-based inverted code and a temporal compensated code to maintain accuracy and consistency. We also disentangle high-frequency artifacts from the main video content in the latent space to reduce incoherence. For editing, we introduce a self-supervised "in-between frame composition constraint" to ensure smoothness in generated videos. RIGID is trained using tailored losses and can handle videos of arbitrary lengths and support live stream editing. It is also attribute-agnostic, allowing for arbitrary attribute manipulations without re-training. Our method achieves temporal coherent inversion and editing with significantly reduced inference time compared to attribute-specific methods. Extensive experiments demonstrate the superiority of our approach over existing methods in terms of quantitative and qualitative evaluations. Overall, our contributions include the proposal of a recurrent video GAN inversion framework that unifies video inversion and editing, explicit modeling of temporal coherence from both inversion and editing ends, and the achievement of attribute-agnostic editing with on-the-fly attribute variation.