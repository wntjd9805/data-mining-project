The paper introduces a novel framework called Speech2Lip for generating personalized audio-synchronized talking videos from a short video and speech. The main challenges in this task include complicated motion patterns, appearance fidelity requirements, and the lack of sufficient training data. Existing methods either focus on speaker-independent methods that lack appearance fidelity or speaker-specific methods that require a large amount of training data. The paper proposes a decomposition-synthesis-composition framework that separates speech-sensitive and speech-insensitive motion/appearance, allowing for effective learning from limited training data. The framework includes a speech-driven implicit model, a Geometry-Aware Mutual Explicit Mapping (GAMEM) module for modeling 3D head motion, a blending network for refining results, and a contrastive sync loss for learning from a short video. Experimental results demonstrate the superiority of the proposed method over state-of-the-art speaker-specific methods in terms of both qualitative and quantitative evaluations.