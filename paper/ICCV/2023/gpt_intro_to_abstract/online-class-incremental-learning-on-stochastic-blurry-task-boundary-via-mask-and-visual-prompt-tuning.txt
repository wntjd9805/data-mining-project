Continual learning is essential for modern deep neural networks to avoid catastrophic forgetting when faced with an ever-changing stream of data. Traditional continual learning scenarios have clear task boundaries, but real-world applications often lack such boundaries and have limited access to data at any given time. To address this, the i-Blurry scenario has been proposed, but it does not fully capture the complexity of real-world data. In this paper, we propose a novel Stochastic incremental Blurry (Si-Blurry) scenario that dynamically reflects the changing distribution of real-world data. We identify intra- and inter-task forgettings and class imbalance as key challenges in the Si-Blurry scenario. To overcome these challenges, we introduce a method called Mask and Visual Prompt tuning (MVP) that incorporates instance-wise logit masking, contrastive visual prompt tuning loss, gradient similarity-based focal loss, and adaptive feature scaling. Experimental results show that our proposed method significantly outperforms existing methods in CIFAR-100, Tiny-ImageNet, and ImageNet-R datasets, demonstrating its effectiveness in addressing the problems of the Si-Blurry scenario.