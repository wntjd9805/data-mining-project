With the availability of large-scale datasets and high-performance computing platforms, deep neural networks have achieved remarkable achievements in various visual tasks. This has led to the integration of neural networks into software products deployed on edge devices. However, during the retraining phase, all neural network parameters are typically changed, requiring users to download all parameters from the cloud platform. This paper proposes the challenge of efficient model updating, aiming to reduce the download overhead of neural network-based software during updating. The proposed approach, named Tiny Updater, combines model pruning and knowledge distillation techniques to determine the optimal parameters that should be changed and achieve comparable accuracy with the fully-updated model. Extensive experiments on multiple datasets and tasks demonstrate that Tiny Updater can update the model with minimal performance degradation by changing only a small subset of parameters, reducing the training overhead by 80% to 90%. This work introduces the challenge of efficient model updating and presents a novel approach to address it, achieving significant improvements in download cost reduction and performance preservation.