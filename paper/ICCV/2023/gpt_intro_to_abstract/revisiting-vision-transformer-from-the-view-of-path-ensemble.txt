The paper introduces a novel view of Vision Transformers (ViTs), suggesting that ViTs can be seen as a collection of paths rather than a single deep network. The authors propose a new Transformer structure with three parallel paths in each layer and utilize an explicit multi-path ensemble network. They investigate the contribution of each path and design path combination and self-distillation techniques to improve the performance of ViTs. Specifically, they propose path pruning and EnsembleScale to optimize the combination of paths, and they use knowledge distillation to transfer knowledge between different paths. The paper's contributions include the new view of ViTs as a collection of paths, the analysis of path contributions, and the proposed path combination and self-distillation methods.