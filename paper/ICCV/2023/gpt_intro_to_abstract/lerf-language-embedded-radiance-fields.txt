Neural Radiance Fields (NeRFs) have revolutionized the creation of photorealistic 3D scenes. However, the output of NeRFs lacks meaningful context, making it difficult to interact with the scenes. In this paper, we propose Language Embedded Radiance Fields (LERF) â€“ an approach that combines natural language processing with NeRFs. LERF optimizes embeddings from a vision-language model to ground language within 3D scenes. Unlike previous methods, LERF preserves the integrity of CLIP embeddings at multiple scales, enabling it to handle a wide range of language queries. We construct LERF by optimizing a language field jointly with NeRF, supervised using CLIP embeddings from different image scales. LERF also incorporates self-supervised DINO features to regulate the optimized language field. Additionally, LERF provides more localized and 3D consistent relevancy maps for text queries compared to 2D CLIP embeddings. Despite its added capabilities, LERF does not significantly slow down the base NeRF implementation. We evaluate LERF on various scenes and demonstrate its ability to localize both fine-grained and abstract queries. Our results show that LERF can generate view-consistent relevancy maps in 3D across different queries and scenes. We compare LERF with popular open-vocab detectors and find that LERF's 3D features can accurately localize queries in real-world scenes. The zero-shot capabilities of LERF make it suitable for robotics, vision-language analysis, and interacting with 3D scenes. Code and data are available at the provided URL.