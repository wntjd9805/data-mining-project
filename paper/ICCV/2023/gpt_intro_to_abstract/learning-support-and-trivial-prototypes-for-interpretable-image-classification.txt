Deep convolutional neural networks (CNNs) have achieved impressive results in visual tasks such as image recognition and object detection. However, their complex architectures and high-dimensional feature spaces make them difficult to interpret and understand. This lack of interpretability hinders their successful application in fields that require transparent decisions, such as disease diagnosis, financial risk assessment, and autonomous driving. To address this issue, recent research has focused on developing interpretable deep learning models, such as the prototype-based gray-box models.In this paper, we introduce a new learning strategy for the prototypical part network (ProtoPNet), a prototype-based model. We propose learning support prototypes that resemble support vectors in support vector machines (SVM) and are located close to the classification boundary. We formulate this strategy using a new closeness loss that minimizes the distance between prototypes of different classes. Our experiments show that the support prototypes produced with our method are more similar to SVM's support vectors, compared to the trivial prototypes learned by ProtoPNet.Furthermore, we propose a new method called ST-ProtoPNet that integrates both support and trivial prototypes. This approach leverages the distinct characteristics of the two sets of prototypes to capture both hard-to-learn and easy-to-learn visual features for classification. Our contributions include the analogy between prototype learning and support vector learning, the introduction of support prototypes to improve interpretability and accuracy, and the development of the ST-ProtoPNet method for interpretable image classification.We conducted extensive experiments on three benchmarks, demonstrating that our ST-ProtoPNet outperforms current state-of-the-art methods in terms of classification accuracy and interpretability. The experiments also showed that the trivial and support prototypes have different characteristics, with the former focusing on local parts and background, while the latter primarily focuses on object parts belonging to the visual class of interest. Overall, our work contributes to the development of interpretable deep learning models for visual tasks.