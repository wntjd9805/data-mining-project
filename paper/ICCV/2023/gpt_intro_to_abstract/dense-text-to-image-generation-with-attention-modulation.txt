Recently, text-to-image models have made significant advancements in generating diverse and high-quality images based on short scene descriptions. However, these models face challenges when dealing with dense captions, often leading to the omission or blending of visual features from different objects. Additionally, users struggle to have precise control over the layout of the generated images solely through text prompts.To address these issues, several recent works have proposed training or fine-tuning layout-conditioned text-to-image models, providing users with spatial control. However, these approaches are computationally expensive and require retraining for new user conditions, domains, and text-to-image base models.In this paper, we propose a training-free method that supports dense captions and offers layout control in generated images. We analyze the intermediate features of a pre-trained text-to-image diffusion model and observe that the image layout is related to self-attention and cross-attention maps. Based on this observation, we dynamically modulate intermediate attention maps according to the layout condition. We also consider the original attention scores' value range and adjust the modulation degree based on each segment's area.Through experiments, we demonstrate that our method improves the performance of existing models and outperforms compositional diffusion models in terms of dense captions, text conditions, layout conditions, and image quality. We evaluate the results using both automatic metrics and user studies. Furthermore, our method yields qualitative results comparable to existing layout-conditioned models. We conduct a detailed ablation study to analyze our design choices and discuss failure cases. The code, models, and data are available at https://github.com/naver-ai/DenseDiffusion.