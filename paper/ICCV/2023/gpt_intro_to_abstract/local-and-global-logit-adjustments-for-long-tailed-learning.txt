Deep learning has greatly improved various vision tasks, but it heavily relies on large-scale well-balanced datasets. However, real-world data often follows a long-tailed distribution, where a few classes have a majority of samples, while most classes have very few samples. This makes training deep learning models on long-tailed data challenging, as the models tend to be dominated by the head classes and perform poorly on tail classes. Existing approaches for handling the long-tail problem include class re-balancing strategies, ensemble learning, and two-stage training processes. However, these methods have limitations in terms of data imbalance during training or limited perception of specialists. In this paper, we propose the Local and Global Logit Adjustments (LGLA) framework for long-tailed recognition. LGLA consists of the Class-aware Logit Adjustment (CLA) strategy and the Adaptive Angular Weighted (AAW) loss. CLA includes Local Logit Adjustment (LLA) and Global Logit Adjustment (GLA), which train experts on the whole dataset while specializing in non-overlapping subsets or achieving a global perspective. AAW introduces adaptive hard sample mining, improving classification performance by re-weighting hard samples. The core idea of LGLA is to give each expert access to the entire dataset during training, leading to diverse recognition abilities. Experimental results on popular long-tailed benchmarks demonstrate that LGLA outperforms state-of-the-art methods. Our contributions include proposing the LGLA method, which combines the strengths of generalists and specialists, and introducing the CLA strategy and AAW loss for better handling long-tailed data.