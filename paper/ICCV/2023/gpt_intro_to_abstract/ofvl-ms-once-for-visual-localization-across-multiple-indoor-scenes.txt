Visual localization is a crucial task in computer vision that involves predicting the camera's position and orientation in a given RGB image. It plays a vital role in various applications, including simultaneous localization and mapping (SLAM) and structure-from-motion (SFM). Traditional methods for visual localization construct associations between 2D keypoints and 3D scene coordinates using local descriptors, followed by using a RANSAC-based algorithm to retrieve the camera pose. However, with the advancements in deep learning, scene coordinate regression (SCoRe) based methods have emerged as state-of-the-art, achieving high localization performance in small static scenes. These methods train a convolutional neural network (CNN) to regress the 3D scene coordinate for each pixel in the input image and calculate the camera pose using the PnP algorithm. While SCoRe-based methods show impressive results, they are scene-specific and require training for new scenes, resulting in an increase in model size with the number of scenes. To address this limitation, we propose a unified SCoRe-based framework called OFVL-MS, which optimizes the visual localization of multiple scenes collectively. We introduce a multi-task learning (MTL) approach, treating each scene's localization as an individual task. OFVL-MS overcomes the problem of gradient conflict by using a layer-adaptive sharing policy in the forward pass. This policy automatically determines whether each active layer of the backbone is shared or not, improving efficiency and reducing model complexity. Additionally, we introduce a gradient normalization algorithm in the backward pass to homogenize gradient magnitudes of the task-shared parameters, ensuring all tasks converge at a similar but optimal pace. Experimental results demonstrate that OFVL-MS achieves excellent localization performance on different benchmark datasets, including the 7-Scenes dataset, 12-Scenes datasets, and our new large indoor dataset LIVL. Furthermore, OFVL-MS can generalize to new scenes with significantly fewer parameters while maintaining superior localization performance. Overall, this work contributes a unified framework for visual localization, offering improvements in efficiency and flexibility while achieving state-of-the-art performance.