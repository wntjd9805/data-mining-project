In this paper, we address the problem of reconstructing the complete configuration of garments, which is important for tasks such as object understanding, VR/AR, and robotic manipulation. Previous works have been limited by the difficulty of annotating real-world garment data, leading to simplifications in pose estimation. We propose a novel recording pipeline called GarmentTwin, which measures garment poses based on their dynamic movements. We collect a large-scale real-world dataset, called ClothPose, that includes 600 garments of 10 categories with annotated poses. We benchmark two relevant non-rigid tasks, reconstruction and pose estimation, on our dataset to facilitate further research in the field. Our contributions include accomplishing garment pose recording in complex configurations in the real world, creating a valuable dataset for garment pose research, and providing benchmarks for non-rigid tasks.