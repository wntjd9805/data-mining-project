This paper introduces the concept of Counterfactual Contrastive Explanation (CCE) as a method for providing human-understandable explanations for deep neural networks (DNNs) in computer vision tasks. While saliency-based approaches highlight informative regions in an image to explain the "Why P?" question, they fail to address the "Why P, rather than Q?" question, which compares the differences between two outcomes. The paper proposes a unified framework that incorporates both P-contrast and O-contrast questions in DNN interpretation. The authors propose a positive-negative saliency scheme and a counterfactual perturbing algorithm for generating counterfactual examples that effectively answer contrastive questions. Experimental results demonstrate the effectiveness of CCE in improving interpretability metrics and aiding users in understanding model predictions.