Temporal action localization is a widely studied topic in computer science, with previous methods focusing on fully or weakly-supervised approaches that require labeled videos and can only detect action classes observed in training. In this paper, we propose a novel few-shot common action localization method that aims to temporally localize action instances in a long untrimmed query video based on a few trimmed support videos describing a common action class. We address the problem of aligning the query and support videos by re-calibrating the support video features under the query video's context and enhancing the temporal dynamics and compatibility of the re-calibrated features. We introduce a three-stage cross-attention mechanism that attends to each support video individually and fuses the support video features in different temporal granularities. We also design a relational classifier with an action classifier and an auxiliary relational module for commonality prediction and refinement. We conduct extensive experimental analyses on two benchmark datasets and achieve state-of-the-art performance. Our contributions include enhancing the representation of the query video by support videos, attending each support video individually, and developing a relational classifier.