This paper addresses the problem of generating realistic talking head videos by using one still source image and one dynamic driving video. The goal is to imitate facial expressions and head movements accurately for different facial identities. The task of talking head video generation has gained significant attention due to its potential applications in digital human broadcast, AI-based human conversation, and virtual anchors in films. Existing works have focused on improving motion estimation and representation to enhance generation quality. However, dynamic and complex motions in the driving video can cause ambiguous generation from the source image, leading to artifacts and degraded quality. To address these challenges, this paper proposes an Implicit Identity Representation Conditioned Memory Compensation Network (MCNet) for high-fidelity talking head generation. The MCNet learns a global facial meta-memory bank and utilizes it to compensate for ambiguous facial details during generation. The proposed approach is evaluated on two competitive datasets and demonstrates improved results compared to state-of-the-art methods. The experiments show the effectiveness of the learned meta-memory bank in handling appearance ambiguities and establishing a clear state-of-the-art performance in talking head generation. The proposed framework also demonstrates generalizability and can enhance the performance of different talking head generation frameworks.