Remote sensing data captured from satellites and planes, using a variety of sensors and processing pipelines, provides valuable information for various applications. Ground Sample Distance (GSD), the physical distance between adjacent pixels in an image, varies across images, ranging from 0.3m to 1km. As a result, the data and objects of interest within these images can vary across wide spatial ranges. However, there are few computer vision methods that specifically address multiscale remote sensing imagery. This paper presents Scale-MAE, a masked reconstruction model that learns relationships between data at different scales during the pretraining process. Scale-MAE utilizes a GSD-based positional encoding and a Laplacian-pyramid decoder to encourage the network to learn multiscale representations. Experimental results demonstrate that Scale-MAE outperforms existing methods in terms of classification accuracy and building segmentation. Overall, Scale-MAE provides improved and more robust multiscale representations for remote sensing data.