Deep neural networks have achieved remarkable performance in computer vision and natural language processing tasks, but their success heavily depends on the availability of large training datasets. However, collecting and processing massive amounts of data can be challenging and computationally expensive. To address this issue, various dataset compression methods, such as Dataset Distillation (DD) and coreset selection, have been proposed to reduce the dataset size while maintaining training performance. However, these methods have limitations in terms of generalization capability and scalability to larger datasets. To overcome these limitations, this paper introduces a novel dataset compression method called Dataset Quantization (DQ). DQ optimizes the selection of data samples based on diversity gains, resulting in a compact dataset with improved coverage and performance across different network architectures. Experimental results demonstrate that DQ outperforms existing methods in terms of dataset compression efficiency and downstream task performance. The proposed method provides a unified framework for compressing datasets and training various network architectures while maintaining state-of-the-art performance. The contributions of this work include the development of DQ, a scalable and efficient dataset compression algorithm, and the empirical validation of its effectiveness in compressing large datasets and generalizing to downstream tasks.