This paper addresses the need for appropriate benchmarks and baselines in the field of Federated Learning (FL). While many papers focus on developing innovative algorithms for FL, less attention is given to benchmarking and testing existing methods. The authors argue that evaluating FL algorithms on a variety of narrow data sets is essential, as most applications of FL involve specific domains and fine-grained differentiation. They also highlight the limitations of evaluating FL algorithms based solely on final accuracy or the number of communication rounds, as these metrics do not consider the computation or communication cost. Furthermore, the authors propose using pre-trained feature extractors as standard baselines in FL, as this approach saves computation and communication resources and avoids the difficulty of training the backbone. Additionally, they emphasize the importance of off-the-shelf performance in FL, as parameter tuning is less feasible in resource-constrained environments. The authors contribute by suggesting rules-of-thumb for benchmark and baseline development, devising an extensive FL image classification benchmark, demonstrating the necessity of using pre-trained features and model reduction tools, and highlighting surprising behaviors of current FL algorithms. The source code for their experiments is publicly available.