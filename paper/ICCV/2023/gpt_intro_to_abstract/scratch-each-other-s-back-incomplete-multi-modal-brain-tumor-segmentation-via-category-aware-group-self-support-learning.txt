This paper focuses on the segmentation of brain tumors in magnetic resonance images (MRI), which is crucial for clinical evaluation and diagnosis. The use of multi-modal images, such as FLAIR, T1c, T1, and T2, has been shown to improve segmentation accuracy. However, the problem of lost modalities is common in clinical practice due to data corruption and other factors. To address this issue, the paper proposes a robust multi-modal approach called Category Aware Group Self-Support (GSS) Learning. GSS establishes self-support groups among modalities during model training, allowing them to collectively make decisions about each category of the tumor. Soft labels generated by the self-support groups are used for knowledge distillation without introducing additional complexity to the network. The paper also introduces a random masking strategy to reduce biases in the initial predictions. Experimental results demonstrate that GSS outperforms state-of-the-art segmentation frameworks on benchmark datasets. The contributions of the paper include the GSS framework, the novel self-support group approach, and the random mask strategy.