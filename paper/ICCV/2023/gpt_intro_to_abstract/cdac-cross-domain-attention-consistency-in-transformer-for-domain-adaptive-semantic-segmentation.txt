The Transformer model has shown strong performance in computer vision tasks but requires a large amount of training data. Curating a large annotated dataset is expensive, especially for tasks like semantic segmentation. Unsupervised domain adaptation (UDA) has been proposed to address this issue. A Transformer-based method called DAFormer has outperformed previous CNN-based UDA methods for semantic segmentation under the UDA setting. However, there is still a domain gap in attention maps, with self-attention focusing on spurious regions in target images. To improve this, we propose cross-domain attention to compute attention scores across different domain images. Our approach includes a cross-domain prediction consistency loss and a cross-domain attention consistency loss to encourage attention-level and output-level alignment. We also introduce a method to enforce attention-level consistency. Our method, called CDAC, achieves state-of-the-art performance in UDA semantic segmentation on various benchmarks.