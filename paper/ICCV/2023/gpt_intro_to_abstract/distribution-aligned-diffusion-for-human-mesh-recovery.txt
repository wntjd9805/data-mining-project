Monocular 3D human mesh recovery is a challenging task with applications in virtual reality, sports motion analysis, and healthcare. Despite significant progress, recovering accurate 3D mesh vertices from a single RGB image remains difficult due to the complex nature of human body shapes, depth ambiguity, and self-occlusion. Diffusion models, which have been successful in generative tasks, offer a promising approach for tackling this task. In this paper, we propose a novel diffusion-based framework called Human Mesh Diffusion (HMDiff) for monocular 3D human mesh recovery. We frame the mesh recovery task as a reverse diffusion process, where we progressively denoise noisy and uncertain inputs to obtain a high-quality mesh prediction. Our framework leverages both the forward and reverse diffusion processes, with the forward process generating step-by-step supervisory signals and the reverse process performing the mesh recovery during training and testing. However, adopting a diffusion-based approach for human mesh recovery poses challenges, including the difficulty in producing accurate 3D mesh outputs with a single RGB image and the lack of input-specific information in the standard diffusion process. To address these challenges, we propose a Distribution Alignment Technique (DAT) to inject input-specific distribution information into the diffusion process, improving convergence and performance. Overall, our HMDiff framework shows promise for effectively recovering high-quality 3D human mesh predictions from uncertain and noisy input data.