Video-to-speech synthesis techniques have been extensively studied in the field of lip-reading research. These techniques reconstruct speech from silent talking face videos without requiring additional text information. However, this is still a challenging task, particularly in multi-speaker and noisy environments, as capturing the complex relationship between lip movements and speech is difficult. Previous studies have used speaker embedding representations obtained from the original audio information to address this challenge. However, manipulating reference audio during inference is not always possible due to various factors. To overcome this limitation, we propose a vision-guided speaker embedding extractor using a self-supervised pre-trained model. This extractor can produce rich speaker embedding information solely from visual input, eliminating the need for audio information during inference. Additionally, we propose a conditional diffusion-based video-to-speech synthesis model named DiffV2S, which utilizes the vision-guided speaker embedding representations. This model reconstructs mel-spectrograms using a denoising diffusion model and speaker embedding guidance. Our experiments on large-scale audio-visual datasets demonstrate that DiffV2S generates high-quality, noise-free speech with detailed content and preserved speaker identity characteristics. This work contributes novel techniques for extracting speaker embedding without audio information and introduces the use of diffusion models in video-to-speech synthesis.