In this paper, we address the problem of accurate and robust scene parsing for autonomous driving in complex environments. We propose a multi-interactive feature learning architecture called SegMiF, which combines image fusion and segmentation tasks. Our approach utilizes a fusion network and a segmentation network, with a hierarchical interactive attention mechanism that bridges the gap between the two networks. We also introduce a dynamic weighting factor in the training scheme to automatically learn optimal parameters. We construct a multi-wave binocular imaging system and a benchmark dataset called FMB, which contains well-registered infrared and visible image pairs with pixel-level annotations. Experimental results show that SegMiF outperforms state-of-the-art methods in accurately assigning categories to each pixel. Our contributions include the joint formulation of image fusion and segmentation, the introduction of hierarchical interactive attention, the interactive feature training scheme, and the construction of the FMB benchmark dataset.