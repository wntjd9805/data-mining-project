Generating realistic and diverse 3D human motions based on various inputs such as action labels, natural language descriptions, and musical cues presents a significant challenge in multiple domains. Motion generation based on language descriptions has garnered interest for enhancing realism and practical applications. However, generating high-quality motion is challenging due to the complex mapping between text and motion modalities. Previous works align the latent feature space between text and motion modalities but may encounter performance degradation with complex textual descriptions. To address these challenges, we propose a priority-centric motion discrete diffusion model (M2DM) for generating motion sequences from textual descriptions in a primary-to-secondary manner. Our model incorporates a Transformer-based VQ-VAE for learning concise, discrete motion representations, and a noise schedule that allocates varying corruption probabilities to tokens based on their priorities. We introduce static and dynamic assessment strategies to evaluate the significance of motion tokens within a sequence. Our priority-centric M2DM demonstrates promising generative capabilities, outperforming existing techniques on benchmark datasets. Contributions include the application of Transformer for capturing long-range dependencies, a priority-centric scheme for motion generation, and state-of-the-art performance on HumanML3D and KIT-ML datasets.