The rapid development of the Internet and mobile devices has allowed researchers to work with massive amounts of video data, which provides valuable information for visual tasks such as human activity recognition. While significant progress has been made in extracting semantics from single images, effectively modeling temporal information remains a crucial problem for video recognition. Previous approaches have used low-level streams or recurrent networks with extracted image features to construct temporal information. Recent advancements include 3D CNNs and factorized 2D+1D CNNs, which directly handle 3D volumetric video data. Many researchers have adopted the "2D backbone + temporal interaction" paradigm, where pre-trained image backbones serve as reasonable initializations and enable end-to-end optimization. Within this paradigm, temporal modeling can be categorized into sequence-wise modeling and pair-wise modeling. While sequence-wise modeling has received extensive attention, pair-wise modeling, which utilizes the relationships between an anchor frame and the others, has been relatively unexplored. In this study, we aim to fill this gap by revisiting the pair-wise relationship using basic arithmetic operations (+, -, *, /) without complex designs. We propose an Arithmetic Temporal Module (ATM) that conducts arithmetic operations between frame features to generate auxiliary temporal cues. We conduct comprehensive ablation studies to determine the optimal instantiation of ATMs, and find that the combination of subtraction and multiplication achieves the best performance. We evaluate our method on representative CNN and Vision Transformer backbones, demonstrating its generality and superiority. Our proposed method achieves high accuracy on popular benchmarks, showcasing the effectiveness of using fundamental arithmetic operations for temporal modeling in video recognition tasks.