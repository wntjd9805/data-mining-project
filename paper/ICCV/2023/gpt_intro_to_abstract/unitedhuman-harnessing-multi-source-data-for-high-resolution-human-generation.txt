Recently, there has been extensive research in the field of human generation tasks, particularly in the synthesis of high-resolution, photo-realistic human images. While face generation has seen significant advancements, early attempts at generating full-body humans have had limited success due to the complexity of human structures and the lack of comprehensive training datasets. This paper argues that the lack of detailed texture information from local body parts, such as hands and faces, is a key factor behind the limited generative capabilities. Additionally, the scarcity of holistic human datasets presents a challenge for training high-resolution human generation models, as creating such datasets from scratch is time-consuming and labor-intensive. However, there is a vast amount of partial-body data available that can aid in various human-related tasks. One promising solution is to supplement the generation process with multiple datasets of human body parts, which can enhance the texture details and maintain diversity in terms of image scale, illumination, and body part position. To address the challenges of aligning disparate body parts and synthesizing images at different resolutions, this paper proposes an end-to-end framework called UnitedHuman. The framework utilizes a Multi-Source Spatial Transformer module for spatial alignment and a Continuous GAN module for arbitrary-scale training, leveraging multiple datasets to generate higher-resolution full-body images. Experimental results demonstrate that UnitedHuman can outperform existing state-of-the-art methods while requiring only a fraction of the high-resolution images needed by those methods. Overall, the contributions of this paper include the exploration of multi-source data for high-resolution human generation, the design of a Multi-source Spatial Transformer for aligning body parts, and the design of a Continuous GAN for multi-resolution and scale-invariant training.