This paper introduces dubbed video generation, which aims to synchronize the lip movements of a face with audio while maintaining its visual identity and pose. With the increasing popularity of video content involving speakers delivering dialogues, dubbed video generation has various applications such as avatar animation, automated creation of audio-visual content, and dubbing of movies. However, accurately synchronizing lip motions with audio has been a challenge, especially in high-resolution videos. The traditional reconstruction losses used in this process often suppress high-frequency textures necessary for high-resolution dubbing. To address this, the paper proposes replacing these traditional losses with shift-invariant objectives, enhancing the shift-invariance of Sync-Loss and reconstruction loss. The authors also propose a pyramid network with supervisions at multiple resolutions to handle the instability of training at high resolution. The resulting dubbed video generator, called SIDGAN, shows significant improvements in visual quality compared to existing methods on different datasets. The paper's contributions include analyzing the importance of shift-invariant learning, developing a coarse-to-fine pyramid model, and demonstrating remarkable performance on both high-resolution and low-resolution datasets.