Semantic segmentation is a challenging task that involves assigning class labels to each pixel in an image. While deep neural networks have led to significant progress in this field, segmentation models trained on specific domains often struggle to generalize well to other domains. This is primarily due to the domain gap between the training and testing domains. To address this issue, unsupervised domain adaptation (UDA) techniques have been proposed to improve the adaptability of segmentation models to the target domain. One popular approach is domain alignment, which aims to align the distribution of the source and target domains in various spaces. However, these methods often lack specific knowledge about the target domain, limiting their effectiveness. To overcome this limitation, self-training methods have been proposed to mine target-specific knowledge. These methods utilize pseudo-labels generated by pre-adapted models to further train the model on the target domain. The quality of these pseudo-labels directly impacts the performance of self-training. To improve the reliability of pseudo-labels, reliability measure-based and uncertainty estimation-based self-training methods have been proposed, which reduce the noise interference and improve performance.In this paper, we explore self-training from a different perspective. We propose a novel self-training framework called Relation Teacher (RTea), which focuses on relation learning in pseudo-labels to improve adaptability. Our framework leverages observational priors to guide the building of relations between pixels. These priors include reliable local relations and available low-level relations. Based on these priors, we propose a pseudo-relation learning-based self-training framework that forces the student model to learn pseudo-relations from the teacher model. This framework enhances the model's certainty and consistency of predictions on the target domain, effectively reducing cross-domain inadaptation.Experimental results demonstrate that our method effectively mines the available knowledge in pseudo-labels and can be easily integrated with existing self-training methods to further improve their performance.