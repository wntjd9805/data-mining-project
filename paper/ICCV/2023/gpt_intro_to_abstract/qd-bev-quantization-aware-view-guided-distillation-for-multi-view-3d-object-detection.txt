Abstract:Autonomous driving requires efficient and lightweight models for camera-only 3D object detection based on bird-eye-view (BEV). Existing methods, primarily focused on LiDAR-based approaches, fail to address the computational and memory costs associated with running state-of-the-art BEV models on camera-only setups. In this paper, we propose a quantization-aware view-guided distillation method (QD-BEV) to address these challenges. Through systematic experiments on quantizing BEV models, we identify the issues with standard quantization-aware training (QAT) methods on BEV. Our proposed QD-BEV incorporates a view-guided distillation (VGD) approach that effectively exploits information from both the image and BEV domains for multi-view 3D object detection. We compare the performance of QD-BEV with previous methods and demonstrate that it outperforms baselines while having significantly smaller model sizes and computational requirements. Our W4A6 quantized QD-BEV-Tiny model achieves a normalized detection score (NDS) of 37.2% with a model size of just 15.8 MB, outperforming the 8Ã— larger BevFormer-Tiny model by 1.8%. These findings contribute to the development of lightweight and efficient models for camera-only 3D object detection based on BEV, advancing autonomous driving research.