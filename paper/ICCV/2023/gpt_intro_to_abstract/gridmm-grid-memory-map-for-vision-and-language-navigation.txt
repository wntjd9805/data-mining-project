This paper introduces the concept of Vision-and-Language Navigation (VLN) tasks, which require an agent to understand natural language instructions and navigate accordingly. The paper discusses two scenarios of VLN: navigation in discrete environments and navigation in continuous environments. In discrete environments, the agent moves between interconnected navigable nodes, while in continuous environments, the agent uses low-level controls for navigation. Historical information during navigation is crucial for environment understanding and instruction grounding. Previous works commonly use recurrent states as historical information, but the paper proposes alternative methods such as encoding trajectory history and actions as a sequence of previous observations. The paper also compares different methods of representing the navigation environment, including topological maps and semantic maps, highlighting their limitations. In contrast, the paper proposes the Grid Memory Map (GridMM) as a visual representation structure for modeling global historical observations. GridMM divides the visited environment into grid regions and dynamically constructs a grid memory bank to update the grid map. Visual features are categorized into the grid map regions based on their coordinates, and an instruction relevance aggregation method is used to capture relevant visual clues. The effectiveness of GridMM is demonstrated through extensive experiments. The paper concludes by outlining the contributions of the study, including the proposal of GridMM, the comparison of different map representations, and the verification of the method's effectiveness.