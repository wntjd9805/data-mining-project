Scene flow estimation, which involves estimating the 3D structure and motion of a dynamic scene, has gained attention in fields such as robotics, augmented reality, and autonomous vehicles. Deep learning has shown progress in scene flow estimation, but existing methods require strict sensor calibrations or expensive devices. Monocular scene flow estimation methods have been presented as a cost-effective solution, but the limited availability of ground-truth training data is a challenge. To address this, multi-task methods have been proposed to jointly learn depth, 2D optical flow, and camera ego-motion networks from monocular sequences. However, the accuracy of these methods lags behind supervised monocular methods. In this paper, we propose a novel approach for self-supervised monocular scene flow estimation that outperforms previous methods. We introduce a network architecture that incorporates 3D geometry-oriented property and ego-motion rigidity. We propose an ego-motion aggregation module with a rigidity soft mask to estimate ego-motion and locate static regions. We also introduce new training losses and strategies to enhance accuracy. Extensive experiments demonstrate the effectiveness of our method, achieving a significant accuracy boost compared to the state-of-the-art in monocular scene flow estimation, depth estimation, and visual odometry.