This paper addresses the limitations of existing text-to-image models in terms of inclusiveness and biases. It proposes a new method called ITI-GEN that achieves inclusiveness in text-to-image generation by leveraging a small set of reference images. The framework uses the CLIP model to obtain embeddings of the reference images and learnable prompts. By aligning the directions of image and prompt features in the joint embedding space, the method aims to effectively represent all desired categories. ITI-GEN can be used with different domains of reference images and is compatible with existing text-based image generation models. The paper demonstrates the effectiveness of ITI-GEN using the Stable Diffusion model and highlights its advantages over other methods. Overall, this work presents a practical and novel approach to inclusive text-to-image generation.