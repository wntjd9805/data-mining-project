Clothes-related activities, such as folding, laundry, and dressing, present challenges in robotics due to the complex dynamics and high-dimensional state representation involved. Training robots in real-world scenarios can be expensive and unsafe, making the development of simulated environments with rich assets a viable alternative. However, there is a lack of large-scale 3D clothing models in existing datasets, limiting the ability to train robots in data-driven approaches. To address this, we introduce ClothesNet, a dataset consisting of 4400 3D cloth mesh models with annotations for clothes-centric robot vision and manipulation tasks. We also set up a simulation environment using a differentiable cloth simulation technique called DiffClothAI, which addresses the challenges of cloth simulation. We demonstrate the usefulness of ClothesNet through benchmark algorithms for clothes classification, edge line segmentation, and keypoint detection, as well as real-world experiments involving a dual-arm robot folding clothes. Our contributions include the creation of ClothesNet, the development of perception and manipulation tasks, and comprehensive experiments to validate the efficacy of our dataset.