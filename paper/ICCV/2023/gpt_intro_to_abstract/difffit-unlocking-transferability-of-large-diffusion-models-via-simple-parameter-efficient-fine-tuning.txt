Denoising diffusion probabilistic models (DDPMs) have shown impressive results in various generative tasks but suffer from high computational costs and storage requirements. This paper proposes DiffFit, a parameter-efficient fine-tuning strategy for large diffusion models, specifically building on the Diffusion Transformer (DiT) model. The motivation for DiffFit comes from BitFit, a technique in natural language processing that fine-tunes only the bias term in a pre-trained model. By extending BitFit to image generative tasks, DiffFit achieves better adaptation to new domains by incorporating learnable scaling factors. The paper provides a theoretical analysis of the mechanism behind DiffFit and evaluates its performance against other fine-tuning techniques on eight downstream datasets. Surprisingly, DiffFit can also be applied to fine-tune low-resolution diffusion models for high-resolution image generation with minimal cost. Experimental results demonstrate that DiffFit outperforms existing techniques in terms of Frechet Inception Distance (FID) and trainable parameters. For example, DiffFit achieves better FID than the state-of-the-art DiT model while reducing training time by 30 times. The contributions of this paper include the proposal of DiffFit as a parameter-efficient fine-tuning approach, a theoretical analysis of its effectiveness, and the demonstration of its scalability in high-resolution image generation tasks.