The paper discusses the reconstruction of 3D scenes from posed images, a problem in computer vision with various applications. The traditional approach involves estimating depth maps and fusing them to create a unified 3D model. However, this often leads to missing geometry and artifacts. An alternative method called Atlas has been proposed, which uses 3D convolutional neural networks to predict the scene's truncated signed distance function (TSDF) and produce smooth surfaces. Several improvements have been made to this framework, but the resulting reconstructions remain coarse. The paper introduces a new system called FineRecon, which addresses three key factors limiting accuracy and detail in prior works. Firstly, it avoids corrupting detail in the training data by supervising predictions only at the exact points where the ground-truth TSDF is known. Secondly, it uses an initial multi-view stereo depth estimation step to enhance the feature volume and guide the 3D CNN towards areas of high surface likelihood, improving reconstruction quality. Thirdly, it proposes a new method to query the TSDF prediction at any point in 3D space, allowing the model to resolve sub-voxel detail and enabling reconstruction at arbitrary resolution without re-training. FineRecon achieves state-of-the-art performance on the ScanNet dataset and produces improved visual detail with fewer artifacts compared to previous methods. The main contributions of the paper are the increased accuracy of training data, the use of a novel MVS depth-guidance strategy, and the enablement of sub-voxel detail reconstruction using a new TSDF prediction architecture.