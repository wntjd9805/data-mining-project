Image denoising is an important task in image restoration, and deep neural networks have become popular for this task due to their ability to learn natural image priors from large-scale datasets. Deep Image Prior (DIP) is a method that uses a randomly initialized convolutional neural network (CNN) to regularize image restoration through its architecture and early-stopping optimization. However, the architectural design for DIP remains an open problem, and the performance of DIP is heavily influenced by architectural choices and the tendency to over-fit the noisy image. Previous approaches have focused on either under-parameterization or neural architecture search (NAS), but these methods have limitations such as over-smoothing and high computational costs. In this paper, we propose a new approach to simplify the architectural design choices for DIP. We analyze the influences of different architectural components on denoising performance and discover that the fixed upsampling operations have a strong bias towards low-frequency content, affecting both peak PSNR and the point of early stopping. Based on this insight, we propose simple strategies for architectural adaptation, such as scaling the depth and width of the network to balance smoothing and preservation of details, and potentially discarding skip connections to simplify the design. We conduct experiments to validate our findings and show that with proper setups, a highly under-parameterized subnetwork can achieve denoising performance that matches or even out-performs larger networks with significantly fewer parameters. This under-parameterization also alleviates the need for early stopping. We also create a Texture-DIP Dataset to associate image texture with architectural design, providing a foundation for future research. Overall, our approach offers a quick, effective, and interpretable solution for architectural adaptation in image denoising.