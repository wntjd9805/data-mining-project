The paper introduces the problem of human pose estimation from monocular images in computer vision. The researchers highlight the importance of this research question in various applications such as human-machine interaction, autonomous driving, animation, sports, and medicine. They discuss recent advances in deep learning-based human pose estimation that show promising results in accurately reconstructing 3D poses from single images. However, they point out that there is a loss of information when projecting 3D poses onto a 2D plane, and argue that a meaningful 3D human pose estimator should be able to recover the full distribution of possible 3D poses for a given 2D pose. The researchers mention the increasing interest in multi-hypothesis human pose estimation, where downstream tasks can benefit from unlikely poses. They highlight existing approaches that estimate fixed-size sets of poses or use variational autoencoders and normalizing flows, but note limitations in their representation and complexity. The goal of the researchers is to develop a multi-hypothesis human pose estimator that is easy to train and produces high-quality pose hypotheses covering the full range of possible and plausible output poses. The paper presents three major contributions: the use of a conditional diffusion model to represent the 3D human pose distribution, a novel sampling strategy to use the full 2D input information from heatmaps, and a transformer architecture to handle joint uncertainties. The researchers also explain the advantages of their diffusion model over other approaches such as VAEs and GANs, and discuss the problem with existing two-step approaches that remove valid structural and depth information. They propose conditioning the diffusion model with an embedding vector computed from joint positions sampled directly from the heatmaps, and introduce an embedding transformer to combine samples and confidences into a single embedding vector that encodes the joint distribution.