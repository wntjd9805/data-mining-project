Indoor scene 3D reconstruction and novel view synthesis (NVS) is a classical topic in computer vision that has applications in virtual reality, robot perception, and visualization. Traditional methods for scene reconstruction focus on geometric registration and fusion using point clouds or voxels, which have limitations in memory usage and scene description. Recent advancements in neural networks have introduced implicit continuous representations for 3D vision tasks, such as NeRF, which use volume rendering techniques to reconstruct radiance fields. However, NeRF-based methods face challenges in view extrapolation and global geometric consistency. To address these challenges, primitive-based methods like NeurMiPs use global plane priors but suffer from discontinuity artifacts in certain regions. In this paper, we propose Primitive-Aware Radiance Fusion (PARF), an incremental reconstruction pipeline that combines NeRF and semantic parsing for improved performance in indoor scene NVS. PARF utilizes a divide-and-conquer strategy that simplifies the representation in global primitive regions while retaining complexity in non-plane details. We introduce a hybrid representation that integrates planar semantics into the radiance field using a discrete semantic volume, enabling efficient training and improved quality. PARF maintains a global scene plane representation, allowing for dynamic fusion and adaptive updates. Evaluations demonstrate the effectiveness of PARF in terms of convergence speed, view extrapolation performance, and scene editing capabilities. Our contributions include the proposal of PARF as a novel hybrid scene representation, an incremental reconstruction framework for primitive-aware radiance fusion, and extensive evaluations showcasing the benefits of our method.