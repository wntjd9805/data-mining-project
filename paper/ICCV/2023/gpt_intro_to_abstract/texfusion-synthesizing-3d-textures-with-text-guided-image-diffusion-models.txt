In recent years, there has been extensive research on deep learning-based 3D object generation, driven by the demand for high-quality 3D assets in various applications. While previous works have focused on the geometric components of 3D assets, textures have received less attention, despite their importance in creating realistic 3D assets. Text-conditioned image diffusion models have emerged as a powerful tool for generating images with visual detail and diversity. These models have also been used as image priors for synthesizing 3D objects with textures using textual guidance. This paper introduces a novel approach called TexFusion for text-driven high-quality 3D texture synthesis. TexFusion leverages the information about textures carried by a pre-trained text-to-image diffusion model's image prior. The current state-of-the-art method for text-to-3D synthesis, Score Distillation Sampling (SDS), has two limitations: it requires a high classifier-free guidance weight for convergence, resulting in low generation diversity, and it involves a lengthy optimization process for every sample. To overcome these limitations, TexFusion utilizes latent diffusion models that encode and generate images in a latent space. It leverages latent diffusion trajectories in multiple object views, encoded by a shared latent texture map. Renders of the shared latent texture map are input to the denoiser of the latent diffusion model, and the output is projected back to the shared texture map in a 3D-consistent manner. To transform the generated latent textures into RGB textures, a neural color field is optimized. The publicly available latent diffusion model SD2-depth is used as the diffusion backbone. Compared to SDS, TexFusion produces textures with more natural tone, stronger view consistency, and faster sampling speed. The proposed method is validated qualitatively and quantitatively on various texture generation tasks, showing high-quality, globally coherent, and detailed textures aligned with the text prompts used for conditioning. TexFusion can generate highly diverse textures without being limited to single categories or explicit 3D textures as training data. Overall, the main contribution of this paper is a novel method for 3D texture generation that achieves state-of-the-art performance in text-driven texture synthesis.