In recent years, there have been advances in deep learning models for computer vision and natural language processing. To meet the demand for deploying these models on resource-constrained edge devices, researchers have proposed network quantization methods to convert high-precision parameters into low-precision ones. However, fine-tuning approaches, which optimize quantized models using training data, are not always applicable due to privacy and security concerns that restrict access to the original data. To address this issue, data-free quantization methods have been proposed, which aim to quantize models without relying on real data. These methods reconstruct the original data from pre-trained models using prior statistical distribution information of the underlying data. However, they overlook the powerful tool of causal reasoning, which aids human learning without relying on data collection. Causal reasoning can extract causal relationships from pre-trained models and ignore irrelevant factors. There are two main challenges in introducing causality into data-free quantization: constructing an informative causal graph in a data-free situation, and using causal language to formalize data generation and network alignment. To tackle these challenges, we propose a novel Causality-Guided Data Free Network Quantization method, called Causal-DFQ. This method constructs a causal graph to model the data-free quantization process and introduces a content-style-decoupled generator and a discrepancy reduction loss to align distributions between pre-trained and quantized models. Our experiments demonstrate that Causal-DFQ significantly improves the performance of data-free low-bit models, surpassing models fine-tuned with real data on the ImageNet dataset. This work contributes a causal perspective on data-free quantization and provides a novel method for leveraging causality in data-free network compression.