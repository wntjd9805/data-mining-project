The 6D pose of an object, which represents the transformation between the object coordinate system and camera coordinate system, is crucial in applications such as robotic grasping and augmented reality. Existing methods for 6D pose estimation rely on either category-specific network training or struggle with estimating precise poses. Some methods use CNNs to detect keypoints on 3D object models, while others predict pixel-wise 3D coordinates for dense correspondence mapping. However, these methods assume a close-set scenario, where the object space is identical in both training and testing phases. To address the open-set problem and generalize to unseen objects, category-level and template-based pose estimation paradigms have been proposed, but they have limitations in generalizing to objects with different appearances or shapes and struggle with occlusions and limited viewpoints. In this paper, we present GCPose, an unseen object 6D pose estimation framework based on dense geometry correspondences. GCPose establishes dense 3D-3D correspondences between object-scene and object-model point clouds using a symmetry-aware matching loss. We train GCPose on a large number of synthetic 3D object models and achieve state-of-the-art performance on various datasets under an open-set pose estimation setting.