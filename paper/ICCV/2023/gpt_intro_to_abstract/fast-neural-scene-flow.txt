Neural Scene Flow Prior (NSFP) is a widely-used method in open-world perception and scene densification using lidar. It achieves state-of-the-art scene flow estimates from dense lidar point clouds and works on various sensor setups without the need for offline learning. However, the runtime optimization of NSFP is significantly slower compared to learning-based methods. This paper introduces a new approach that focuses on the speed of runtime scene flow optimization by addressing the computational cost of the Chamfer distance loss function. The proposed method uses a correspondence-free loss function called distance transform as a proxy for the computationally expensive Chamfer distance loss. Although distance transform has been extensively studied in the graphics and vision community, its application as an efficient loss function in deep geometry has been overlooked. The approach presented in this paper achieves real-time runtime optimization that is computationally efficient and scalable to dense point clouds, while maintaining state-of-the-art performance on out-of-distribution scenes. The performance and computation time of the approach are compared with leading learning-based methods on large lidar datasets, demonstrating significant speedups and comparable performance. This provides a fast and robust solution for dense scene flow estimation in real-time vision and robotic applications. The paper also explores point cloud-based scene flow using runtime optimization and discusses the potential benefits of accelerating coordinate networks and the use of distance transform in 3D point cloud analysis.