This paper presents a preliminary investigation on vision-language representation learning in the medical domain, specifically focusing on zero-shot disease diagnosis and grounding. While significant progress has been made in developing foundational models for vision-language tasks in computer vision, applying these models to the medical domain poses unique challenges. These challenges include limited data availability, the need for domain knowledge to understand medical concepts, and the importance of robustness and explainability in medical diagnosis. Existing work in medical vision-language pre-training (VLP) matches raw reports with image scans without considering medical prior knowledge. In contrast, the proposed knowledge-enhanced visual-language model leverages medical entities extracted from reports and translates them into fine-grained descriptions using a medical knowledge base. These descriptions are then used to establish relationships between medical entities and align image patches with entity descriptions. The model is pre-trained on the MIMIC-CXR dataset and evaluated on various disease diagnosis benchmarks, achieving state-of-the-art performance in zero-shot classification and grounding. Overall, the proposed model addresses the challenges specific to the medical domain and demonstrates promising results for disease diagnosis.