The paper discusses the growing concern of AI safety in computer vision research, particularly in relation to adversarial examples. Adversarial examples are visually similar to regular images but contain small perturbations that can mislead deep neural networks (DNNs) and undermine their reliability. To address this vulnerability, previous methods have focused on developing adversarial attack and defense algorithms. However, the authors observe that the current defense methods have variable adversarial robustness across different target classes, network architectures, and defense methods. To mitigate this issue, the authors propose Adversarial Double Machine Learning (ADML), which estimates causal parameters in adversarial examples to reduce their negative effects on robustness. The authors conduct extensive experiments and analyses on various CNN and Transformer architectures, confirming the effectiveness of ADML in improving adversarial robustness.