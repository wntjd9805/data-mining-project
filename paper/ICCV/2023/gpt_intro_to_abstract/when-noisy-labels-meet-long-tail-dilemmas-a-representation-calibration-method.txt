Deep learning has made significant advancements in various fields driven by large-scale and high-quality annotated datasets. However, obtaining such perfect datasets in practice is challenging due to wrong labeling and class imbalance. Previous methods for learning with noisy labels and long-tailed data separately cannot effectively address the problem when both imperfect situations coexist. This paper introduces a method called RCAL to tackle the problem of learning with noisy labels on long-tailed data. RCAL focuses on representation calibration on the level of deep representations extracted by deep networks. The method employs unsupervised contrastive learning for representation learning, which is not influenced by corrupted training labels, making the representations robust. Two representation calibration strategies, distributional and individual, are then performed to recover representation distributions before data corruption and reduce the hypothesis space of deep networks, respectively. Experimental results on simulated and real-world datasets demonstrate the superiority of RCAL over existing state-of-the-art methods. The theoretical results also confirm the effectiveness of the proposed calibration strategies under certain conditions.