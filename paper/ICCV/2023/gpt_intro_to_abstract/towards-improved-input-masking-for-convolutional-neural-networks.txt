Deep learning methods have achieved great success in solving computer vision tasks but lack interpretability and easy debugging. Perturbation methods have been used to analyze the dependence of model predictions on input. However, many perturbation methods have downsides, such as preserving local sensitivity or drastically changing the input distribution. Recent work has shown that vision transformers are robust to large magnitude input perturbations, making full perturbation methods effective for transformers. Inspired by this, we propose a new masking technique called layer masking for CNNs to mitigate the drawbacks of full perturbation. Layer masking runs the CNN only on the unmasked portion of the image, avoiding distribution shift. We can randomly remove up to 50% of the input to a ResNet-50 while maintaining high accuracy on ImageNet. Our method also allows for precise object masking without leaking information through the shape of the mask. Additionally, layer masking operates at the pixel level and is more flexible than patch-level token dropping for vision transformers. Our masking method also produces LIME scores that align with the most salient features of the image.