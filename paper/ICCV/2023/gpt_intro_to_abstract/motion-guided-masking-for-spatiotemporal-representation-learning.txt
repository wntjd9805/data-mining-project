Video transformers have achieved state-of-the-art performance in video understanding tasks, but they heavily rely on large-scale supervised pretraining from image datasets, which is data-inefficient. Self-supervised learning (SSL) is a promising approach to eliminate the reliance on supervised pretraining. The denoising/masked autoencoder (MAE) has shown promising results in representation learning for vision tasks. However, few SSL methods focus on effectively learning video saliency. In this paper, we propose a novel method called motion-guided masking (MGM) to improve upon previous video MAEs by incorporating video saliency. We argue that the random masking strategy inherited from image MAEs is not optimal for video and propose to use motion as a guide for detecting spatiotemporal saliency. We introduce an improved masking algorithm that uses readily available motion vectors from the H.264 codec to continuously guide the mask. Our proposed MGM outperforms previous video MAEs and achieves state-of-the-art or comparable results on large-scale and small datasets, demonstrating the effectiveness and efficiency of our method. Overall, our contributions include the MGM algorithm, the use of motion vectors for MAE pretraining, and improved performance on various video recognition tasks.