Monocular depth estimation is a crucial task in autonomous driving, allowing for the generation of depth maps using a single camera. Unlike stereo-matching methods, monocular depth estimation does not require rectified images, making it more suitable for self-driving cars. Various monocular depth estimation methods have been proposed in recent years, primarily focusing on self-supervised learning. However, current depth estimation backbones fail to fully utilize the direction sensitivity and environmental dependency of the task, leading to limited performance. In this paper, we propose a new Direction-aware Cumulative Convolution Network (DaCCN) to address these limitations. DaCCN improves feature extraction by adapting it to different directions and efficiently aggregates information from connection regions. We integrate DaCCN into a state-of-the-art baseline model and achieve significant improvements, setting a new state-of-the-art performance on three benchmark datasets. Our contributions include the analysis of direction sensitivity and environmental dependency, the design of a learnable module for adjusting feature extraction, the proposal of a cumulative convolution operation for encoding critical environmental information, and the achievement of improved performance on benchmark datasets.