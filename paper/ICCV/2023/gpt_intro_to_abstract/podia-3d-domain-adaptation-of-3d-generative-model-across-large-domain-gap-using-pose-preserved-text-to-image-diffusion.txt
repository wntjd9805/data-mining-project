Recently, 3D generative models have been developed to enable multi-view consistent and explicitly pose-controlled image synthesis. However, training these models is challenging due to the need for a large number of images and knowledge about their camera pose distribution. Text-guided domain adaptation methods have emerged as a promising solution to overcome the data requirement challenge. These methods leverage pretrained models for text-to-image mapping. While non-adversarial fine-tuning methods have shown impressive results, they suffer from the lack of diversity in text prompts and suboptimal text-image correspondence. To address these issues, we propose a novel pipeline called PODIA-3D, which utilizes pose-preserved text-to-image diffusion models and a specialized-to-general sampling strategy. By synthesizing pose-consistent target images and fine-tuning the 3D generator, our method effectively adapts 3D generators across significant domain gaps. We also propose a debiasing method to improve intra-domain diversity. Our approach outperforms existing methods in terms of text-image correspondence, realism, diversity, and sense of depth.