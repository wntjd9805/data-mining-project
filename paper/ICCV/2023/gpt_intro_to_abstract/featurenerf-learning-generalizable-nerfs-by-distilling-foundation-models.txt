Neural fields have become a popular approach for representing visual signals, particularly the Neural Radiance Fields (NeRF), which has shown promising results in novel view synthesis. However, there is a need to adapt these models for general 3D applications beyond view synthesis. This paper introduces FeatureNeRF, a framework that leverages pre-trained 2D vision foundation models to learn generalizable 3D features. By distilling the knowledge from the foundation models into the 3D space via neural rendering, FeatureNeRF enriches the semantic information of the NeRF features. The framework is evaluated on tasks such as 2D/3D semantic keypoint transfer and object part segmentation, and experiments demonstrate its effectiveness in extracting generalizable 3D semantic features without 3D supervision.