Finding point correspondences between images is crucial in various computer vision applications, such as Structure from Motion, visual localization, SLAM, object recognition, and object tracking. While traditional hand-engineered feature extractors like SIFT, SURF, and ORB have been widely used, recent trends in the field have shifted towards learned methods based on deep architecture. However, these modern detectors lack rotation equivariance and show poor performance when images undergo in-plane rotations. In this paper, we propose a novel feature extractor method called S-TREK that utilizes rotation-equivariant convolutional layers to address this issue. We also introduce a training framework inspired by reinforcement learning to optimize the repeatability of keypoints. Unlike the "detect and describe" approach, we follow a "detect, then describe" paradigm using separate networks for keypoint detection and descriptor extraction. Our experiments demonstrate that S-TREK achieves state-of-the-art repeatability and can accurately recover camera poses, particularly in the presence of in-plane rotation.