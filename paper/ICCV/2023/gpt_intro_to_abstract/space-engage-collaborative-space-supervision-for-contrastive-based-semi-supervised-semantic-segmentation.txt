Semantic segmentation is a crucial task in computer vision that aims to classify each pixel in an image. Although significant progress has been made in training segmentation models using high-quality labeled images, the process of annotating such images is expensive and time-consuming. To address this issue, recent works have focused on semi-supervised semantic segmentation, leveraging unlabeled images to train segmentation models. Most existing methods use self-training or consistency regularization strategies to learn from unlabeled images. However, these methods overlook the semantic information in the representation space and primarily focus on the learning process in the logit space. In this paper, we propose a dual-space collaborative supervision approach called Collaborative Space Supervision (CSS) for contrastive-based semi-supervised semantic segmentation. Our approach utilizes the semantic information in representations to provide more reliable guidance during unlabeled training and enhances the knowledge exchange between the logit and representation spaces. Additionally, we measure the similarity between representations and prototypes and use it as an indicator to guide the learning process in the representation space. Through extensive experiments on two semantic segmentation benchmarks, we demonstrate the effectiveness of our CSS method.