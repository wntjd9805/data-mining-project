In recent years, there has been significant progress in developing image representations that can accurately discriminate objects at a fine-grained or instance level. However, most of this work has focused on specialized representations for specific domains, limiting their scalability and practicality for general purpose visual search systems. This paper introduces the concept of universal image embeddings, which are representations capable of encoding fine-grained visual information from multiple domains. To facilitate research in this area, the authors present the first large-scale dataset, called Universal Embedding Dataset (UnED), consisting of over 4 million images from 349,000 classes across 8 domains. Additionally, the paper provides comprehensive benchmarking and reference implementations of models for universal image embeddings, highlighting the performance differences compared to specialized models. Furthermore, the authors describe the Google Universal Image Embedding Challenge, a public competition that attracted a significant number of researchers and submissions in this field. The results and lessons learned from this challenge serve as valuable insights for further advancements in the development of universal image embeddings. Overall, this paper addresses the lack of standardized datasets and explores the potential of universal representations in the field of image embedding learning.