End-to-end learned image compression systems have gained attention for their competitive performance compared to traditional image coding methods. Transformer-based autoencoders have emerged as attractive alternatives to CNN-based solutions, offering high adaptivity and lower computational cost. Most learned image compression systems are designed for human perception, but there is a growing need for coding techniques for machine perception in high-level recognition tasks. These techniques include multi-task and single-task bitstream approaches. While multi-task bitstream methods aim to serve multiple downstream tasks, single-task bitstream methods allow customization for individual tasks. However, customizing a neural codec for every machine application is costly and challenging. This paper proposes a transfer mechanism that transfers a well-trained Transformer-based image codec from human perception to machine perception without fine-tuning the codec. Inspired by Visual Prompt Tuning, a plug-in mechanism is introduced that injects learnable prompts to both the encoder and decoder of the base codec. The proposed method achieves better rate-accuracy performance on complex machine tasks compared to other transferring-based methods. This work is the first attempt to utilize prompting techniques for low-level image compression. The plug-in nature of the method allows easy integration with other Transformer-based image codecs.