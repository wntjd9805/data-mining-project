Zero-shot Video Object Segmentation (ZVOS) is an important computer vision technique that aims to identify visually attractive objects in video sequences. While previous approaches have explored the integration of appearance and motion features, designing effective multi-stage appearance-motion fusion methods for ZVOS remains an open problem. This paper explores the use of Transformers, which have shown remarkable results in various computer vision tasks, for ZVOS. The authors propose two Transformer variants, namely Context-Sharing Transformer (CST) and Semantic Gathering-Scattering Transformer (SGST), to model contextual dependencies effectively and efficiently. A level-isomerous Transformer paradigm is also introduced, which applies CST and SGST for low-level early fusion and high-level late fusion, respectively. Experimental results demonstrate the superiority of the proposed method compared to existing approaches and the vanilla Transformer-based baseline, achieving real-time performance in ZVOS tasks. This work contributes to the advancement of Transformer-based techniques in the ZVOS field, providing a successful attempt at developing a real-time Transformer-based approach for ZVOS.