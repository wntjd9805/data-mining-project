Recent vision-language pre-training models have shown impressive generalizability on various downstream tasks by infilling hand-crafted prompt templates with real class names. However, identifying suitable prompts for different tasks is time-consuming and requires expert attempts. To address this, recent prompt tuning methods have been proposed to learn a set of soft prompts using labeled data. However, these methods have limitations such as sensitivity to initialization and generalizability degradation. In this paper, we propose the Gradient-RegulAted Meta-prompt learning (GRAM) framework to address these limitations. GRAM jointly meta-learns an efficient soft prompt initialization and a gradient regulating function to prevent damage to generalizability. We utilize large-scale image-text pairs to create a hierarchical structure and derive meta-training tasks by subsampling semantic topics. The meta-optimization objective is to ensure good performance on query sets by fine-tuning prompt initialization and updating gradient regulating function. Our gradient regulating function is learned to regulate the gradient into a consistent direction across domains. Our method is model-agnostic and improves performance and generalizability across prompt tuning methods. We achieve harmonious integration of textual and visual prompt tuning, resulting in superior few-shot generalization performance. Our contributions include the GRAM framework, its easy integration into different methods, and its superior generalizability performance.