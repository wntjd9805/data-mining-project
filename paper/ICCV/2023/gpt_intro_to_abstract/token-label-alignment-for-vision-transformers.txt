The recent advancements in vision transformers (ViTs) have had a significant impact on computer vision, leading to state-of-the-art results in image classification, object detection, and semantic segmentation. The success of ViTs has also inspired the development of high-performance deep architectures and improved convolutional neural networks. However, while data mixing strategies have been extensively studied for convolutional neural networks, their compatibility with ViTs has not been explored extensively. This paper addresses the misalignment between tokens and labels that is caused by self-attention in ViTs, which undermines the global consistency of labels. To overcome this issue, the paper proposes a token-label alignment (TL-Align) method for ViTs, which aligns the labels of tokens after data mixing. The proposed TL-Align method significantly improves the accuracy and generalization of ViTs without introducing additional workload during inference. Experimental results demonstrate the effectiveness and robustness of the proposed method across various ViT variants and downstream tasks.