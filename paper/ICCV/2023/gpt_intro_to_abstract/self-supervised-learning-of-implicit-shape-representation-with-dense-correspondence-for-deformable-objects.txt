Shape representation with dense correspondence is a crucial problem in computer vision, with applications in shape reconstruction, texture mapping, and shape editing. Previous techniques have relied on additional semantic priors or annotations to learn such representations, but these approaches have limitations in terms of use cases and scalability. In this paper, we propose a self-supervised method to learn a neural implicit representation for deformable objects. Our method can generate shapes by deforming a learned template and achieve dense correspondence. We address the challenges posed by highly under-constrained optimization by learning a generative model for the training shapes and enforcing valid shape constraints. We also propose a novel formulation of local rigid constraints that address issues of irregular surface deformation and flipping mappings. Additionally, we address the lack of large-scale deformation priors in previous implicit representation methods by designing hierarchical rigid constraints that utilize spatial context. Our extensive experiments demonstrate the superior capabilities of our learned neural representation in shape reconstruction, deformation interpolation, and dense correspondence, with high-quality results in various applications.