Humans rely on multiple sensory inputs to understand events, with vision, audio, and language providing complementary information. Video data, which combines visual, audio, and text data, is the closest approximation of this setup. Researchers have explored learning meaningful representations by leveraging multiple modalities, focusing on representation learning within each modality or joint multi-modal embeddings. However, learning effective joint embeddings is challenging due to differences across modalities and real-world challenges like misalignment. Current pre-training approaches use contrastive objectives but struggle with generalizability. To address this, we propose a semantic-structure-preserving consistency loss (SSPC) that retains information beneficial for cross-modal embedding while preserving modality-specific semantic structure. We formulate the anchor learning problem as a many-to-many assignment problem and propose the Multi-Assignment Sinkhorn-Knopp (Multi-SK) algorithm to optimize anchor assignments. Our approach outperforms the state-of-the-art in multi-modal self-supervised representation learning on both in- and out-of-domain datasets.