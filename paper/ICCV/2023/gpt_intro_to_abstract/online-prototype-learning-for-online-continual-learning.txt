Current artificial intelligence systems have shown excellent performance on tasks but are prone to forgetting previously learned knowledge, known as catastrophic forgetting. Continual learning (CL) aims to learn continuously from a non-stationary data stream while adapting to new data and mitigating catastrophic forgetting. This paper focuses on class-incremental learning (CIL) in the online CL mode, where the model learns incrementally from a single-pass data stream and cannot access task identifiers at inference. Various online CL methods have been proposed, including replay-based methods. However, these methods focus on sample storage, and this paper is interested in understanding why online learning models fail to generalize well from a new perspective of shortcut learning. Shortcut learning occurs when the neural network focuses on simplistic features and takes shortcuts, leading to poor generalization and catastrophic forgetting. Learning representative features that best characterize the class is crucial to resist shortcut learning and catastrophic forgetting. This paper also addresses the confusion between classes and proposes the Online Prototype learning (OnPro) framework for online continual learning. The framework introduces online prototypes as representative embeddings for groups of instances and presents the Online Prototype Equilibrium (OPE) and Adaptive Prototypical Feedback (APF) methods to learn representative and discriminative features and enhance decision boundaries. Experimental results on benchmark datasets demonstrate the superior performance of the proposed method.