In this paper, we propose ICL-D3IE, an in-context learning framework for large language models (LLMs) to perform visually rich document understanding (VRDU) tasks, specifically document information extraction (DIE). We address the challenges of the modality gap and task gap by utilizing demonstration examples and converting them into textual prompts for the LLMs. We construct diverse demonstrations based on three criteria: benefiting all test documents, including layout information, and predicting labels in an easily extractable format. We enhance the demonstrations iteratively through in-context learning. Experimental results on benchmark datasets demonstrate that ICL-D3IE achieves superior or comparable performance to previous pre-trained methods, both in the in-distribution (ID) and out-of-distribution (OOD) settings. These results indicate the potential of leveraging LLMs for VRD-related tasks.