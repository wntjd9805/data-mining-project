This paper introduces the concept of Universal Attention Matching (UniAM) framework for addressing the challenging problem of Universal Domain Adaptation (UniDA) in deep neural networks. UniDA aims to adapt models trained on a source domain to perform well on an unlabeled target domain, even when the label spaces in the two domains are completely unknown. The proposed UniAM framework leverages both feature and attention information to mitigate the interference caused by domain shift and effectively explore and leverage object information embedded in attention. Specifically, a Compressive Attention Matching (CAM) approach is introduced to solve the attention mismatch problem implicitly by sparsely representing target attentions using source attention prototypes. Furthermore, the framework incorporates domain-wise and category-wise common feature alignment (CFA) and target class separation (TCS) using an adversarial loss and a source contrastive loss. The performance of the UniAM framework is extensively evaluated and shown to outperform current state-of-the-art approaches in UniDA.