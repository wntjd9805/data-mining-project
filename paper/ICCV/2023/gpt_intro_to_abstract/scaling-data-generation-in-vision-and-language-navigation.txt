This paper introduces the task of Vision-and-Language Navigation (VLN), which requires an agent to navigate in photo-realistic environments based on human instructions. The authors highlight the importance of diverse visual-language data for learning and propose utilizing additional traversable environments to improve agent performance. They discuss the challenges involved in creating large-scale training datasets and address sub-problems such as building navigation graphs, recovering corrupted rendered images, and generating navigational instructions. The authors propose an effective paradigm for large-scale VLN training and evaluate the influence of each component in the pipeline. They utilize environments from the HM3D and Gibson datasets, build navigation graphs using viewpoint sampling and aggregation, address image corruption with the Co-Modulated GAN, and train agents for downstream navigation tasks. The paper presents comprehensive experimental results that demonstrate the importance of a traversable navigation graph, recovering photo-realistic images, and learning from additional scenes. The authors validate that an agent trained with augmented instructions can achieve good performance on multiple navigation tasks and show that combining augmented data with original data improves generalization ability. The resulting VLN model achieves a high success rate on various navigation tasks, significantly outperforming previous methods and reducing the gap towards human performance. The paper's contributions include a simple and reproducible training paradigm, analysis of the data augmentation pipeline, and new state-of-the-art results on navigation tasks.