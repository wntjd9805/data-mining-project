Event cameras are vision sensors that capture brightness changes independently for each pixel, referred to as events. These cameras offer advantages such as high temporal resolution and low power consumption, making them attractive for various applications. However, the sparse and asynchronous nature of event cameras poses challenges for classical computer vision algorithms. Deep learning models have been proposed to convert sparse events into dense representations for processing using off-the-shelf neural networks. Despite the increasing number of research papers on event-based vision, there is a lack of comprehensive comparisons of different event representations. Traditionally, comparing representations involves training separate deep learning models, a time-intensive process. In this work, the authors propose a fast method that circumvents the need for network training by computing the Gromov-Wasserstein Discrepancy (GWD) between raw events and event representations. The GWD measures the distortion introduced by converting raw events and provides an upper bound on the accessible information by downstream neural networks. Experimental evidence shows that the GWD preserves task performance rankings across various input representations, datasets, and tasks. Using the GWD, the authors optimize over a large family of event representations, revealing a new representation called ERGO-12 that outperforms existing representations in object detection and recognition tasks. The authors believe that the GWD is a powerful tool for exploring optimized event representations and summarize their contributions as introducing an efficient approach for comparing event representations using GWD, demonstrating its preservation of task performance ranking, and identifying novel event representations through a hyperparameter search.