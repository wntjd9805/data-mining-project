This research paper introduces the Lecture Presentations Multimodal Dataset (LPM Dataset), which serves as a benchmark for evaluating vision-and-language models' understanding of educational content. The dataset consists of over 9000 slides sourced from various educational videos, encompassing natural images, diagrams, equations, tables, and written text. The paper outlines three research tasks: (1) automatic retrieval of spoken explanations for figures, (2) retrieval of illustrations to accompany spoken explanations, and (3) generation of slide explanations. By addressing challenges such as weak crossmodal alignment, novel visual mediums, technical language understanding, and capturing long-range sequences, the paper highlights the need for advancements in multimodal models. The introduction also mentions the potential applications that can leverage multimodal content, including intelligent tutoring systems, recommender systems, and presentation evaluation systems.