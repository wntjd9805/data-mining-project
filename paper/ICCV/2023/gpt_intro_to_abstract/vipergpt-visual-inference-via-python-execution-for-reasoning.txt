This paper introduces ViperGPT1, a framework that addresses the limitations of end-to-end models in computer vision by leveraging code generating large language models. ViperGPT1 allows the compositional reasoning of visual tasks by flexibly composing vision models based on textual queries. The framework generates customized programs for each query, taking images or videos as input and returning the desired result. By providing an API that exposes various visual capabilities, ViperGPT1 can create programs using built-in Python logical and mathematical operators. The model's prior training on code enables it to reason about how to use these functions and implement relevant logic. The simple approach showcases various benefits, including interpretability, logical reasoning, flexibility, compositional task decomposition, adaptability, training-free execution, and generalizability. Experimental results demonstrate remarkable zero-shot performance across tasks in visual grounding, image question answering, and video question answering. The contributions of this work include the proposed framework, achieving state-of-the-art zero-shot results, and the development of a Python library for program synthesis in visual tasks.