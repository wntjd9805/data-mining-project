This paper introduces DiffDis, a unified vision-language diffusion model that combines generative and discriminative tasks. Large-scale diffusion models and Vision-Language Pre-training models have shown impressive results in image synthesis and zero-shot image classification, respectively. DiffDis aims to bridge the gap between these models by empowering the diffusion model with cross-modal discrimination capability. Existing methods, such as GAN and autoregressive image generation solutions, have limitations in terms of mode-collapse and performance. DiffDis formulates the image-text discriminative problem as a generative diffusion process and utilizes a dual-stream network architecture for better fusion of latent images. A unified training paradigm is proposed to jointly train generative and discriminative tasks. Experimental results demonstrate that DiffDis outperforms single-task models in terms of zero-shot classification accuracy and image-text retrieval tasks. The contribution of this work lies in the proposal of DiffDis, the reformulation of the discriminative problem, the dual-stream network architecture, and the overall unified training paradigm. This research serves as an exploration for future studies in unifying generative and discriminative tasks under the diffusion process.