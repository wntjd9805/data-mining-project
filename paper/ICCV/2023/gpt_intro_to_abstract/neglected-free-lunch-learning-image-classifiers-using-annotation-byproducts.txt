Supervised learning of image classifiers involves transferring human intelligence to a parametric model. This transfer is done through human computation tasks, where annotators label images to create a labeled dataset. Traditionally, these labels are the only information used to train the models. However, we question this practice and propose utilizing additional signals called annotation byproducts that are unintentionally generated during the annotation process. These annotation byproducts include various forms of unintentional traces left by annotators, such as click locations and mouse movement data. We introduce a new learning paradigm called Learning using Annotation Byproducts (LUAB), which incorporates these byproducts to enhance the model's performance. We demonstrate the effectiveness of LUAB by enriching the ImageNet and COCO classification training sets with annotation byproducts and show that using this information improves model generalizability and reduces spurious correlations with background features. Our contributions include highlighting the neglected source of information available during image labeling, introducing LUAB as a cost-effective learning paradigm, providing empirical evidence of its benefits, and releasing the ImageNet-AB and COCO-AB datasets for future research.