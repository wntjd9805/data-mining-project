Temporal sentence grounding is a key problem in computer vision, with a growing focus in recent years. The objective of temporal sentence grounding is to locate the start and end timestamps of a specific moment in an untrimmed video that corresponds to a given query sentence. Full supervised temporal sentence grounding (FS-TSG) has achieved impressive results, but the labor-intensive and subjective process of obtaining accurate timestamps limits its scalability and practical applications. Weakly supervised temporal sentence grounding (WS-TSG), which only requires video and query pairs, has gained attention but still lags behind FS-TSG in performance. WS-TSG faces challenges in localization due to the discrepancy between video-level and clip-level annotations. Recently, a new annotating paradigm called glance annotation has been proposed, which requires the timestamp of a random single frame within the temporal boundary of the target moment. Glance annotation incurs minor annotation costs compared to WS-TSG. However, the existing ViGA model based on contrastive learning has limitations. It fails to cover a wide range of target moments and suffers from annotation bias. In addition, it struggles to handle complex query sentences with multiple events. To address these limitations, this paper proposes a Dynamic Gaussian prior based Grounding framework with Glance annotation (D3G). D3G generates a wide range of candidate moments and uses a Semantic Alignment Group Contrastive Learning module (SA-GCL) to align positive sentence-moment pairs in the embedding space. It also introduces a Dynamic Gaussian prior Adjustment module (DGA) to alleviate annotation bias and approximate the distributions of complex moments. Experimental results demonstrate that D3G outperforms other methods under the same annotating paradigm and achieves significant improvements over weakly supervised methods.