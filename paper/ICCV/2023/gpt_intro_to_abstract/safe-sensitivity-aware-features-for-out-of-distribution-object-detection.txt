Deep neural networks (DNNs) perform exceptionally well on tasks that closely match the training data distribution. However, when deployed in real-world scenarios, they often encounter out-of-distribution (OOD) samples that do not belong to the training distribution. In such cases, DNNs tend to fail silently and produce overconfident erroneous predictions, which poses a significant safety risk in critical applications like self-driving vehicles and medical robotics. To address this issue, OOD detection, which distinguishes OOD samples from in-distribution (ID) samples, has gained importance.While OOD detection has been extensively studied in the image classification setting, there is limited research on OOD object detection. To fill this gap, this paper introduces SAFE (Sensitivity-Aware Features), a novel approach to visual OOD object detection. SAFE leverages recent theoretical insights in the behavior of DNN feature spaces, with an emphasis on sensitivity and smoothness constraints. Sensitivity ensures that input distances are preserved in the output, which is crucial for learning robust feature representations that differentiate between ID and OOD data. Additionally, the role of adversarial attacks in OOD generalization is considered.The proposed SAFE approach consists of three core components. First, a subset of sensitivity-aware layers in the object detector's backbone is identified, which exhibit abnormally high activations in response to OOD input variations. These layers, known as SAFE layers, are empirically shown to outperform previous methods that only utilize features from the classification head. Second, object-specific SAFE vectors are extracted, and a multi-layer perceptron (MLP) is trained to classify each detected object as ID or OOD. This posthoc manner of OOD detection does not require retraining the base network and can be applied to any pre-trained object detector with SAFE layers. Finally, the MLP is trained on the surrogate task of distinguishing SAFE vectors of adversarially-perturbed samples and clean ID training samples, eliminating the need for access to real outlier training data or complex generative processes.The proposed SAFE approach achieves state-of-the-art results on multiple established benchmarks for OOD object detection. To facilitate reproduction of the experiments, the code repository for SAFE is publicly available.