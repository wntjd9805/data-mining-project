Creating and editing 3D models can be challenging, requiring specialized knowledge and software. While neural field-based representations have shown promise in capturing fine details and enabling editing tasks, they are limited in their ability to add new structures or adjust geometry. In this paper, we propose a method that allows for flexible and localized object edits guided by textual prompts. We leverage pretrained 2D diffusion models to edit images based on target textual descriptions, regularizing the optimization in 3D space. Unlike previous methods that use neural fields, we utilize voxel-based representations, which offer faster reconstruction and rendering times. We introduce a novel volumetric correlation loss and 2D cross-attention maps to refine the edits and achieve a tight volumetric coupling. We also incorporate a binary volumetric segmentation algorithm to preserve regions unaffected by the edits. Our approach, Vox-E, provides an intuitive voxel editing interface, allowing for local and global edits involving appearance and geometry changes. We compare Vox-E to existing techniques and demonstrate its effectiveness in a wide range of editing tasks. Our contributions include a coupled volumetric representation, a 3D cross-attention-based segmentation technique, and results that showcase the capabilities of our framework in achieving edits previously unattainable.