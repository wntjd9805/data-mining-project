Deploying deep neural networks (DNN) on edge devices with limited resources is challenging due to their computational and memory requirements. Model quantization is an effective strategy to reduce memory requirements by transforming floating-point values into lower precision fixed-point values. However, quantization introduces noise that can affect accuracy, especially for low-bit quantization. Quantization-aware training (QAT) has emerged as a method for achieving low-bit quantization while preserving accuracy by simulating quantization during training. Different hardware platforms support different forms of quantization, resulting in repeated optimization during deployment. This paper proposes an elastic quantization space design to address this issue, enabling flexible deployment models. A robust elastic quantization supernet is trained based on the quantization space, using efficient training strategies to reduce negative gradients caused by different quantization forms. The trained supernet achieves both uniform and mixed-precision quantization and can quickly obtain a quantized model with the desired accuracy. A Conditional Quantization-Aware Accuracy Predictor (CQAP) combined with a genetic algorithm is proposed to efficiently search for optimal mixed-precision quantization models.