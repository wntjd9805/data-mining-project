This paper introduces the concept of Continual Learning (CL) in the field of artificial intelligence (AI) and addresses the issue of catastrophic forgetting that neural networks often experience when incorporating new information. The authors propose a unified CL framework called Learning-Accumulation-Ensemble (LAE) that utilizes the Parameter-Efficient Tuning (PET) method. The LAE framework consists of three steps: learning, accumulation, and ensemble. The authors address three key challenges in the framework: learning with calibrated speed, accumulation of multi-task knowledge, and ensemble of two expert models. Experimental results on CIFAR100 and ImageNet-R benchmarks demonstrate that LAE outperforms previous state-of-the-art approaches in terms of incremental performance.