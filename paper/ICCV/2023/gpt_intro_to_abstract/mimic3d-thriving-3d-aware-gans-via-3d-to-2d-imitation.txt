In recent years, 3D-aware GANs have shown great potential for large-scale realistic 3D content creation. These GANs incorporate 3D representation learning and differentiable rendering into image-level adversarial learning, resulting in the faithful reconstruction of underlying 3D structures. Among different 3D representations, neural radiance field (NeRF) has been proven effective. However, NeRF's volumetric representation brings high computation costs, hindering the generation of high-resolution images with fine details. On the other hand, some methods introduce 2D super-resolution modules to deal with high-resolution 3D-aware GAN training but sacrifice strict 3D consistency. This paper proposes a method called 3D-to-2D imitation that combines the advantages of direct NeRF rendering and 2D super-resolution. The proposed method achieves strict 3D consistency and high-quality image generation simultaneously. Additionally, 3D-aware convolutions are introduced to improve tri-plane learning in the generator, enhancing feature communications. Experimental results demonstrate that the proposed method significantly outperforms previous 3D-aware GANs and achieves comparable image quality to traditional 2D GANs. This method can be extended to other 3D-aware GAN architectures, closing the quality gap between 3D-aware GANs and traditional 2D GANs.