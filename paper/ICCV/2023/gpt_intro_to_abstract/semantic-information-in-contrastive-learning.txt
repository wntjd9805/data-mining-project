Contrastive learning is a popular technique in visual representation learning for transfer learning. It involves discovering the differences between positive and negative samples. The definition of positive-negative samples determines the pretext task and loss function in contrastive learning. Inspired by the human ability to recognize objects in complex environments, the proposed method SemCL aims to extract subjects from their surroundings. By utilizing publicly available datasets with semantic labels, SemCL is a supervised contrastive learning approach. The pretext task of SemCL is to distinguish between a subject and its surroundings. This mimics the spatial recognition mechanism and improves the spatial information understanding of pretrained models. SemCL performs contrast pairwise between positive-negative pairs within an image/scene, with a paired InfoNCE loss. Unlike other approaches, SemCL decouples the number of negative samples from the batch size, resulting in consistent yet competitive results with relatively small batch sizes. Pretrained SemCL models outperform ImageNet pretrained models in semantic segmentation, object detection, instance segmentation, and depth estimation tasks, due to their improved spatial information understanding. They also show gains over previous well-known works in system-level comparisons.