The goal of building a robotic assistant that can perform everyday tasks through language directives has been pursued by the research community. Recent advancements in computer vision, natural language processing, and embodied AI have led to the development of benchmarks for various components of these robotic agents. However, interactive instruction following is an area that requires active research in order to build realistic assistants. This involves agents navigating, interacting with objects, and completing long-horizon tasks by following natural language instructions with egocentric vision. A major challenge in this process is planning a sequence of actions that are relevant to the task at hand. To address this, a novel approach is proposed that divides the long-horizon planning process into two phases: task-relevant prediction and detailed action planning. This approach prioritizes the prediction of the context (i.e., the objects instructed to be manipulated) to improve the agent's ability to plan actions and reduce the loss of environmental knowledge. Additionally, an environment-aware memory is proposed to store information about object states and masks, allowing the agent to interact with objects in their proper states over time. This memory helps address the challenge of tracking dynamic object states and ensures more successful task completion. The proposed approach achieves state-of-the-art success rates in a challenging benchmark for interactive instruction following, demonstrating better generalization to novel environments. The contributions of this work include context-aware planning, environment-aware memory, and improved performance in interactive instruction following benchmarks.