Line segments are important geometric structures used in various computer vision tasks such as SLAM, pose estimation, construction monitoring, and 3D reconstruction. While lines offer stronger constraints than feature points, they are often poorly localized in the image and suffer from lower repeatability. Additionally, lines can cover a large spatial extent and are susceptible to occlusions and perspective effects. Traditional matching techniques for lines are less effective compared to feature points. Recent advancements in deep learning have shown success in feature point matching using Graph Neural Networks (GNNs). However, transferring this approach to line matching is challenging due to the unique characteristics of lines. In this paper, we propose GlueStick, a network that jointly matches keypoints and line segments. By leveraging their complementary nature, the network can resolve ambiguous line matches by considering nearby keypoints and vice versa. We introduce a unified wireframe structure to capture the connectivity between points and lines, replacing previous handcrafted heuristics. Our network takes sparse keypoints, lines, and their descriptors as input and outputs locally discriminative descriptors enriched with context from all features. The network consists of self-attention and cross-attention layers, as well as a line message passing module for communication between neighboring nodes. Experimental results demonstrate significant improvements over existing methods in various datasets and tasks. Our contributions include the replacement of heuristic strategies with a data-driven approach, a novel architecture leveraging local connectivity, and extensive experimental validation.