In the field of image inpainting, the objective is to reconstruct visually plausible images by filling in holes or defects in input images. Traditional inpainting algorithms rely on patch-based methods or convolutional neural networks (CNNs). While CNNs have shown promising performances, they often suffer from visual artifacts. To address this, mask-aware inpainting algorithms have been proposed, which perform adaptive filtering based on the state of each pixel. More recently, transformer-based inpainting algorithms have been introduced, leveraging transformers' success in vision tasks. These algorithms provide decent inpainting results by utilizing global attention and a binary mask to classify tokens as valid or invalid. However, the binary masking approach has limitations. To overcome these limitations, this paper introduces a novel transformer for image inpainting called the continuously masked transformer (CMT). CMT uses a continuous mask to represent the amount of errors in tokens, which allows for more precise inpainting. The proposed approach also incorporates overlapping tokens for improved communication during the masked self-attention process. The CMT algorithm outperforms conventional image inpainting algorithms based on extensive experiments conducted on various datasets. The major contributions of this work include being the first continuous-mask-aware transformer for image inpainting, a novel mask update scheme, the introduction of overlapping tokens, and superior inpainting performance compared to traditional algorithms.