Reconstructing 3D scene information from multi-view images is a challenging task in computer vision and computer graphics. The Neural Radiance Field (NeRF) approach has shown promising results in scene geometry and appearance optimization, but it fails to disentangle incident lighting and surface properties from radiance. This limitation hinders its use in object relighting and material property editing. Recent works have attempted to decompose radiance into material and environmental lighting, but simplified lighting models are insufficient for complex lighting configurations. The Neural Incident Light Field (NeILF) models arbitrary static lighting conditions but lacks an explicit constraint on inter-reflection issues. Additionally, it requires an object mesh as input, which affects material estimation. In this paper, we propose a method that models light fields, geometry, and materials as separate fields. By unifying incident light and outgoing radiance, our approach improves material estimation for object relighting and enhances surface geometry reconstruction. We apply this method to different NeRF systems for material decomposition and surface refinement. We introduce a real-world linear HDR dataset for evaluation purposes and demonstrate that our method achieves state-of-the-art results in geometry reconstruction quality, material estimation accuracy, and novel view rendering quality. Our major contributions include a general light field representation, an optimization scheme for joint geometry, material, and lighting estimation, and a real-world HDR dataset for material estimation evaluation.