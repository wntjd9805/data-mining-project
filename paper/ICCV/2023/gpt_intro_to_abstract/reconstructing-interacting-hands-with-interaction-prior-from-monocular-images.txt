The reconstruction of interacting hands is crucial for enhancing the realism of digital avatars. Previous methods have focused on reconstructing single hands, but local occlusion and similarity between hands can lead to errors. In this paper, we propose a framework that constructs a comprehensive interaction prior using multi-modal datasets and samples this prior based on interaction cues extracted from a monocular image. We break away from the paired data training approach and construct a dataset containing 500K two-hand patterns that are used for unsupervised training of a prior container using a Variational Autoencoder. This allows us to map each interaction pattern to an interaction code in the prior space, considering the correlation between the two hands. Instead of relying on accurate joint localization, we sample the pre-built interaction prior based on an Interaction Adjacency Heatmap (IAH) that captures spatial correlation between joints. The IAHs are converted into interaction codes using a Vision Transformer module, enabling us to sample reasonable interactions from the latent space. Our contributions include a powerful interaction reconstruction framework, an effective feature extraction strategy based on interaction adjacency, and a large-scale multimodal dataset for constructing the latent prior space.