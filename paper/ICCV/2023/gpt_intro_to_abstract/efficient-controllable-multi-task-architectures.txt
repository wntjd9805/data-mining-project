Multi-task learning (MTL) has gained significant attention in computer science for its ability to solve multiple related tasks using a single neural network. However, current MTL architectures lack flexibility in adapting to different user constraints without the need for retraining. In this paper, we propose a method called Efficient Controllable Multi-Task architectures (ECMT) that addresses this problem. ECMT consists of a shared encoder and task-specific decoders with slimable channel widths. By adjusting the capacities of the decoders and jointly controlling the encoder capacity, ECMT enables precise control of task importance and compute budget. Unlike existing methods, ECMT achieves higher controllability by using a stronger backbone for a given compute budget and controlling task preference solely through decoder capacities. The training of ECMT is performed once, and the knowledge of the shared encoder is distilled using a Configuration Invariant knowledge distillation strategy. At test-time, ECMT uses joint constraints to search for the most suitable encoder and decoder width configuration using an evolution-based algorithm. Extensive experiments on benchmark datasets demonstrate ECMT's superior MTL controllability compared to state-of-the-art methods. Our contributions include a method for sampling high-performing MTL sub-architectures from a single SuperNet, a training strategy to enhance sub-architecture performance, and a search strategy for task decoder sampling and shared encoder width configuration. ECMT shows higher controllability by approximately 33.5% in the NYU-v2 dataset and 55% in the Pascal-Context dataset compared to existing methods.