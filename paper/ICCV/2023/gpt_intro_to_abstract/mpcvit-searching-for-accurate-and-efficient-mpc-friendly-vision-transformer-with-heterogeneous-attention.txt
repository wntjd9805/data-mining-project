Privacy has become a major concern in the deployment of machine learning models that handle sensitive data and tasks. Secure multi-party computation (MPC) has emerged as a solution for protecting the privacy of both data and deep neural network (DNN) models. However, existing DNN architectures, including the recently proposed Vision Transformers (ViTs), are not optimized for MPC. These architectures face limitations such as communication overhead and approximation errors when deployed in an MPC setting. In this paper, we analyze the impact of different atomic operations on the accuracy and efficiency of Softmax, a key operation in ViTs. Based on this analysis, we propose a set of attention variants that are optimized for MPC. We also introduce MPCViT, the first MPC-friendly ViT architecture, which features a heterogeneous attention design space and an MPC-aware differentiable neural architecture search (NAS) algorithm for effective optimization. Our experiments show that MPCViT outperforms prior-art models in terms of both accuracy and efficiency in an MPC setting. We further extend MPCViT to optimize other Transformer components simultaneously, achieving even better results.