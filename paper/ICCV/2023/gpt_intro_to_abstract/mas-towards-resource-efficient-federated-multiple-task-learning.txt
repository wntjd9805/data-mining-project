Federated learning (FL) has gained significant attention for enabling privacy-preserving distributed model training among decentralized devices. This approach has been applied to various applications, such as medical imaging analysis, Google Keyboard, and autonomous vehicles. However, many edge devices can only support one FL task at a time, posing challenges for coordinating multiple simultaneous FL tasks. Existing methods, such as one-by-one training and all-in-one training, suffer from trade-offs between training time and test loss. In this paper, we propose Merge and Split (MAS), the first FL system to effectively coordinate and train multiple simultaneous FL tasks under resource constraints. MAS merges tasks into an all-in-one FL task with a multi-task architecture and then splits the task based on their synergies and differences. Experimental results show that MAS achieves better test loss with reduced training time and energy consumption compared to existing methods. Our contributions include formalizing the problem of training multiple simultaneous FL tasks, proposing MAS to address this problem, and demonstrating its benefits through extensive empirical studies.