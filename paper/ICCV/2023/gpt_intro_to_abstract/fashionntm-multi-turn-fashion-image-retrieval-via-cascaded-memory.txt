This paper explores the problem of multi-turn fashion image retrieval, which aims to retrieve fashion images based on user feedback in an iterative manner. The authors highlight that existing methods for fashion retrieval mainly focus on single-turn exchange of information, whereas real-world online shopping customers often provide iterative feedback to refine their search results. To address this, the authors propose a novel memory-based framework called FashionNTM, which incorporates sequential feedback from users across multiple turns. They also introduce a Cascaded Memory Neural Turing Machine (CM-NTM) that enables the encoding and retention of multiple relationships from each turn. Evaluation results on the Multi-turn FashionIQ and Multi-turn Shoes datasets show that the proposed approach outperforms existing state-of-the-art models. Additionally, an interactive analysis demonstrates the system's memory retention and agnosticity to turn order. A user study further validates the effectiveness of the proposed model, as the images retrieved by FashionNTM are preferred by users compared to other multi-turn methods. Overall, this paper contributes to the advancement of multi-turn fashion image retrieval and provides a strong foundation for future research in this area.