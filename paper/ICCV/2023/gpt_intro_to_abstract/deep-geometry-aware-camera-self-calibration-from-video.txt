Camera self-calibration is a process that aims to determine the intrinsic parameters of a camera without the need for a calibration target. Existing approaches either rely on known or learned properties of the environment, limiting their ability to generalize, or require training a model specifically for each camera. In this paper, we propose a camera self-calibration method that combines deep learning with explicit modeling of projection functions and multi-view geometry. We introduce a self-calibrating bundle adjustment layer that optimizes camera intrinsics within a deep learning model. We integrate this layer into the DROID-SLAM architecture, resulting in a system that can infer camera intrinsics from monocular video. Our experimental results demonstrate that our approach consistently achieves higher calibration accuracy than baseline methods on three public datasets.