In this paper, we address the challenge of designing quantization-friendly search spaces for efficient deployment of deep neural networks (DNNs) on edge devices. While INT8 quantization has been widely used to reduce model size and memory cost, prior research has shown that the resulting models still have high latency, making them difficult to deploy in latency-critical scenarios. We propose a novel method called SpaceEvo to automatically design specialized quantization-friendly search spaces for each hardware. Our approach leverages a two-stage quantization Neural Architecture Search (NAS) framework, along with an evolutionary-based search algorithm, to find the best quantized models with low INT8 latency under various constraints. We introduce a latency-aware quality-tradeoff (Q-T) score to quantify the effectiveness of a search space, and propose a block-wise search space quantization scheme to significantly reduce the training and evaluation costs. Our experiments on real-world edge devices and ImageNet demonstrate that our automatically designed search spaces outperform manually-designed search spaces, achieving improved accuracy-latency tradeoffs for INT8 quantized models.