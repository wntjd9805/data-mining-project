The paper introduces the challenging task of identifying surrounding objects in real-world scenes from egocentric video clips using free-form language supervisions. This task is crucial for devices like glass-devices or autonomous robots that need to understand language expressions and ground them into the surrounding world. While there have been extensive efforts in 2D image reference expression comprehension, there is comparatively less research on video-based referring expression comprehension. The lack of real-world, egocentric video datasets has been a limitation in this area. However, the recently proposed Ego4D dataset provides a large-scale collection of egocentric videos, which has enabled the creation of the RefEgo dataset for video-based referring expression comprehension. The RefEgo dataset is unique in that it includes frequent motions, surrounded objects, and objects that may appear at the edge or go out-of-frame. The paper presents valuable baseline models for RefEgo, including the state-of-the-art REC models MDETR and OFA, as well as object tracking techniques to improve spatio-temporal localization of the referred object. The paper aims to contribute to the development of general purpose models for understanding and grounding language expressions in daily-life tasks.