The paper introduces the task of audio-visual segmentation (AVS), which aims to segment objects in a video sequence based on the sounds produced in the audio. Unlike conventional multimodal settings, where each modality can be used individually, audio in AVS serves as a "command" to localize and segment sound producers from visual data. The paper proposes a framework called Explicit Conditional Multimodal Variational Auto-Encoder (ECMVAE) to learn both shared and specific representations in the latent space of each modality. The ECMVAE model utilizes a multimodal variational auto-encoder with a Jensen-Shannon divergence to balance sampling efficiency and sample quality. Constraints are imposed on the factorized representations to maximize the contribution of each modality. Experimental results show that the ECMVAE achieves state-of-the-art performance in AVS, with a significant improvement in multiple sound source segmentation. The main contributions of the paper include the proposed semantic correlated feature representation learning framework, the introduction of constraints for shared and specific representations, and the achievement of state-of-the-art segmentation performance.