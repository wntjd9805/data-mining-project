The paper introduces the Relightable Articulated Neural Avatar (RANA) method, which aims to generate and animate neural avatars of humans in various poses, viewpoints, and lighting environments. Existing methods have limitations in terms of lighting control and body pose manipulation. RANA overcomes these limitations by using a short monocular video clip of the person and target body pose and lighting information during inference. The method involves modeling human body articulations and disentangling texture, geometry, and illumination information. The proposed framework generates refined normal and albedo maps and uses spherical harmonics lighting for shading. The paper also presents a photo-realistic synthetic dataset for evaluation and compares RANA with other baselines using the People Snapshot dataset. The contributions of the paper include the novel framework for learning relightable articulated neural avatars, the ability to synthesize images under different body poses, viewpoints, and lighting conditions, and the introduction of a new dataset for further research in this field.