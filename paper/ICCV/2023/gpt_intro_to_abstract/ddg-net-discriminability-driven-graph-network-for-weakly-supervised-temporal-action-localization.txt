Temporal action localization (TAL) is a significant and challenging task in video understanding, as it involves localizing and categorizing action proposals in untrimmed videos. While fully-supervised methods have made progress in recent years, they require frame-level annotations that are time-consuming and labor-intensive for large-scale datasets. As a result, weakly-supervised approaches that only use video-level labels have gained attention. However, existing weakly-supervised methods struggle to accurately localize action proposals without frame-level annotations, and fail to effectively handle ambiguous snippets in untrimmed videos. To address these challenges, we propose a novel graph network called Discriminablity-Driven Graph Network (DDG-Net). This network explicitly separates ambiguous and discriminative snippets and performs graph inference with well-designed connections to spread complementary information and enhance discriminability. We also introduce a feature consistency loss to maintain the characteristics of snippet-level representations for localization. Experimental results on the THUMOS14 and ActivityNet1.2 datasets demonstrate the effectiveness of our approach, establishing new state-of-the-art results.