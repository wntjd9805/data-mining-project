The development of Human Neural Radiance Fields (NeRFs) has allowed for the reconstruction of high-quality 3D humans from 2D observations, eliminating the need for ground truth 3D geometry information. By leveraging Human NeRFs, 3D humans can be reconstructed directly from 2D observations, saving time and effort. Existing Human NeRF methods can be classified into two categories: those that reconstruct 3D humans from monocular or multi-view videos and those that learn generalizable Human NeRF models. However, these methods require multi-view images under well-defined camera angles, limiting their applicability in real-world scenarios. In this work, we propose SHERF, the first generalizable Human NeRF based on single image inputs. SHERF addresses the challenges of missing information from partial observations and reconstructing animatable 3D humans from a single image. It introduces a hierarchical feature bank and a feature fusion transformer to capture both global structure and local details. SHERF models the 3D human representation in canonical space, allowing for pose transformation and rendering. Our evaluation shows that SHERF outperforms previous state-of-the-art methods in both novel view and novel pose synthesis. Our contributions include the development of SHERF as the first generalizable Human NeRF model for animatable 3D human reconstruction and the use of 3D-aware hierarchical features to recover texture details and complement missing information.