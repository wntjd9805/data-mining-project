Single-view view synthesis is a technique that allows a camera to explore a scene from a single photograph, which has various applications in image editing and augmented or virtual reality. Many approaches have been proposed to address this problem, but they often struggle to accurately represent occluded regions. Layered representations, such as Multiplane Image (MPI), have been shown to be more suitable for single-view view synthesis. However, existing methods that use MPI often sample plane locations randomly, leading to suboptimal scene representations. In this paper, we propose a novel method called LoLep that aims to accurately represent scenes by regressing locally-learned planes. We introduce a disparity sampler that conditions local offsets of planes on a single RGB image, along with optimizing strategies and an occlusion-aware reprojection loss to improve convergence. We also incorporate a self-attention mechanism and a Block-Sampling Self-Attention (BS-SA) module to enhance occlusion inference for large feature maps. Our experiments demonstrate that LoLep outperforms existing methods on different datasets and requires fewer planes to generate high-quality results.