In recent years, dense depth maps have become crucial in various computer vision tasks, such as autonomous driving, 3D object detection, augmented reality, and 3D reconstruction. However, commercially available depth sensors often produce sparse depth maps that cannot accurately capture the full 3D information. To address this limitation, guided depth completion, which aims to propagate depth information from known pixels to the remaining ones under the guidance of an RGB image, has emerged as a promising solution. Existing methods in this area either treat sparse depth maps as ordinary images or extract 3D features using point cloud convolutions. However, these methods have limitations in fully exploiting the 3D geometry information and propagating information between distant pixels. In this work, we propose a feature point cloud aggregation framework that overcomes the limitations of conventional methods. Our framework directly propagates the given sparse depth information to the entire image by leveraging both 2D and 3D features. We introduce a novel local transformer module that exploits 3D geometry information and reconstructs the depth information for each target location. Experimental results demonstrate that our approach, called PointDC, achieves better or comparable results compared to state-of-the-art methods, while also exhibiting higher generalizability to different sparsity levels of the input depth maps and cross-dataset evaluation.