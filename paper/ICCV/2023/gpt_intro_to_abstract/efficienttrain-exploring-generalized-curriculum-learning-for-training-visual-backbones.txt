The success of modern visual backbones in computer science is driven by the interest in exploring big models on large-scale benchmark datasets. However, the training process for these models is time-consuming and expensive, making it unaffordable for many practitioners. To address this issue, this paper proposes a new approach called EfficientTrain, which is based on the concept of curriculum learning. Unlike existing methods that rely on progressively introducing easier-to-harder examples during training, EfficientTrain focuses on adaptively extracting the simpler and more learnable patterns within each training sample. This is achieved through a cropping operation in the frequency domain, which retains the lower-frequency components of images. By triggering this operation at earlier training stages, the overall training cost can be significantly reduced without sacrificing the final model performance. In experiments, EfficientTrain demonstrates improved training efficiency for various popular visual backbones and achieves competitive or better performance compared to baseline methods. Importantly, the proposed approach is also effective for self-supervised learning. Overall, EfficientTrain offers a simple and generalizable solution for reducing the training cost of modern deep networks in computer vision tasks.