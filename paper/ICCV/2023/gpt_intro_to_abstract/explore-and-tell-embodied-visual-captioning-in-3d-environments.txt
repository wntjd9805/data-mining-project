This paper introduces the concept of Embodied Captioning, a novel task that integrates navigation abilities into visual captioning in order to generate more comprehensive and accurate descriptions of visual contents. The current models in visual captioning rely on well-captured images that provide a good viewpoint of the scene, but in real-world scenarios, capturing such images may not always be feasible. To address this limitation, the Embodied Captioning task requires an agent to navigate the environment and reduce visual ambiguity in order to generate a comprehensive paragraph that describes all objects in the scene. The paper presents the proposed Cascade Embodied Captioning model (CaBOT), which consists of a History-aware Navigator and a Trajectory-aware Captioner, and demonstrates its improved performance compared to baseline models. Additionally, the paper introduces the high-quality 3D dataset ET-Cap, which includes manually annotated paragraph descriptions and poses significant challenges for Embodied Captioning. Overall, the contributions of this paper include the proposal of the Embodied Captioning task, the construction of the ET-Cap dataset, and the development of the CaBOT model for future research and improvement in this area.