Deep generative models have made great strides in generating high-quality images, texts, and videos. However, incorporating hierarchical structures into generative models remains a challenge. Hierarchical generative models are crucial for explainable artificial intelligence and are an active area of research. Various methods have been explored to learn hierarchical representations, including conditional hierarchy and architectural hierarchy. Conditional hierarchies use stacked generative models with conditional Gaussian distributions, while architectural hierarchies leverage network architecture to organize high-level and low-level representations. However, these methods often assume limited expressivity in their prior distributions. This study proposes combining latent space energy-based models (EBMs) with multi-layer generator models to improve hierarchical representation learning. The EBM is built on the latent variables across different layers of the generator model, capturing different levels of information. The latent variables are jointly modeled, and their inter-relations are captured through EBM in the latent space. Efficient inference is ensured through an inference model and a joint training scheme. The contributions of this study include the proposed joint EBM prior model, a variational training scheme, and strong empirical results and ablation studies demonstrating the effectiveness of the model in hierarchical learning and image modeling.