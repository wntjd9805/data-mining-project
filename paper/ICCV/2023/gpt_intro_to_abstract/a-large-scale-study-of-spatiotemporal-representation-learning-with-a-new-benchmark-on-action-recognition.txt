In this study, we address the limitations of existing video action recognition datasets by proposing a new benchmark called BEAR (BEnchmark on video Action Recognition). BEAR consists of 18 video action recognition datasets grouped into 5 categories, including anomaly, gesture, daily, sports, and instructional actions. These datasets cover a diverse range of real-world applications and have different characteristics such as appearance-focused, motion-focused, fine-grained, and egocentric. Unlike previous datasets, BEAR strictly separates the train and test sets, ensuring that the test set is held out during training to prevent overfitting.With BEAR, we aim to evaluate spatiotemporal representation learning methods from a more diverse perspective. We investigate the performance of 6 representative video models pre-trained using both supervised and self-supervised learning in various settings such as full-shot, few-shot, and domain adaptation. Through our comprehensive study, we reveal several insights. Firstly, simple 2D video models equipped with strong backbones can outperform transformer-based models. Secondly, the high performance observed on commonly-used large-scale datasets does not necessarily transfer to other application domains. Thirdly, viewpoint shift significantly impacts downstream task performance, and current domain adaptation methods are not able to address this issue effectively. Lastly, self-supervised spatiotemporal representation learning still lags behind supervised learning.The main goal of our research is to provide a unified and challenging evaluation benchmark that can guide future development in video understanding. Our BEAR benchmark enables researchers to evaluate spatiotemporal representation learning from various perspectives and answer important questions such as the performance of models on real applications, the effectiveness of transformer-based models, the sensitivity of models to domain and viewpoint changes, and the performance with limited downstream data. Through this benchmark, we hope to advance the field of video understanding and facilitate the development of more comprehensive pre-training datasets.