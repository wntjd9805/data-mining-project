Low-light images often suffer from degraded brightness and reduced contrast, which hampers subsequent computer vision tasks. Therefore, low-light image enhancement has gained significant attention in recent years. Existing approaches can be categorized into model-based methods and deep learning-based methods. Model-based methods rely on hand-crafted priors and are limited in characterizing diverse low-light factors. Deep learning-based methods, on the other hand, can learn automatically but may be limited by the feature space of the input domain. This paper proposes a novel approach that normalizes the degradation before enhancement and employs a cooperative training strategy to bridge the gap between metric-favorable and visual-friendly results. The proposed method utilizes neural representation and positional encoding to normalize the input and reduce enhancement difficulty. It also employs multi-modal learning to supervise the output from both textual and image perspectives, improving perceptual-oriented enhancement. Additionally, an unsupervised training strategy is developed to reduce reliance on paired data, using a cooperative adversarial constraint procedure. Extensive experiments demonstrate the superiority of the proposed method over state-of-the-art algorithms, even outperforming some supervised methods. The contributions of this work include the utilization of neural representation for low-light image enhancement, the introduction of multi-modal learning, the development of an unsupervised cooperative adversarial learning strategy, and the demonstration of superior performance through experiments on representative benchmarks.