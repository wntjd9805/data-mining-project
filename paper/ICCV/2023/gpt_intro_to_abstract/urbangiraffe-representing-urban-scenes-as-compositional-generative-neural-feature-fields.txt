Generating photorealistic urban scenes is expensive and time-consuming, requiring professional artists. Generative models offer a promising approach to reduce these costs by learning to generate images from data. However, existing methods lack controllability in terms of camera pose, object manipulation, and scene layout. This paper proposes UrbanGIRAFFE, a 3D-aware image synthesis model for urban scenes. The key idea is to leverage a coarse 3D panoptic prior, obtained from existing datasets or pre-trained models, to simplify the learning of complex geometry and incorporate semantic information for scene editing. The model represents the scene as compositional neural feature fields, including stuff, objects, and sky. It employs a semantic voxel-conditioned stuff generator, an object layout prior, and a sky generator to render a composited feature map, which is then upsampled to the target image using a neural renderer. To improve image fidelity, adversarial and reconstruction losses are used. The contributions of this paper are the exploration of the challenging task of 3D-aware urban generative models with diverse controllability, the use of a coarse 3D panoptic prior to address this task, and the demonstration of state-of-the-art performance on both synthetic and real-world datasets.