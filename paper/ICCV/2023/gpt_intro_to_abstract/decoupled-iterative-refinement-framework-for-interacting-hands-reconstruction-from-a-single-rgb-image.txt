Abstract:3D hand reconstruction is essential for various applications such as virtual reality, augmented reality, and robotics. Previous research has made significant progress in single-hand pose estimation and reconstruction using large-scale datasets and deep learning. However, reconstructing interacting hands from a single RGB image remains a challenging task due to mutual occlusion, self-similarity, and complex spatial relationships. This paper presents a decoupled iterative refinement framework for reconstructing interacting hands. The proposed method utilizes a graph convolution network and a transformer to model the intra- and inter-relationships of two hands in a specialized space. It also incorporates a visual feature map to enhance local visual features by fusing joint features. Experimental results demonstrate that our method outperforms state-of-the-art techniques and exhibits strong generalization abilities. The code for our method is publicly available.