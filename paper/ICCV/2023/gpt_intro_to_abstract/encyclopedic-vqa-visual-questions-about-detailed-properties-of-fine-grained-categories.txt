Abstract:Large Vision+Language models (VLMs) have shown impressive performance on Visual Question Answering (VQA) benchmarks, but they often fail to answer questions that require knowledge of detailed properties of fine-grained categories or instances. This is due to the scarcity of such information in the training data and the lack of interpretability and explanations provided by the models. To address these issues, retrieval-augmented models that retrieve knowledge from a database have gained popularity. However, existing VQA datasets do not fully meet the need for encyclopedic knowledge and attribution. In this paper, we introduce the Encyclopedic-VQA dataset, which focuses on fine-grained categories and instances and includes a controlled knowledge base of free-form text and images. We provide ground-truth attribution for each answer and construct complex two-hop questions. Experimental results show that retrieval-augmented models outperform standard VLMs, indicating the potential of these models to handle encyclopedic knowledge. Our dataset poses a strong challenge for VQA and leaves room for further research in improving retrieval-augmented models.