This paper introduces the concept of source-free domain generalization in the field of computer science. Deep neural networks are commonly trained with the assumption that training and test data are independent and identically distributed, which makes them vulnerable to distribution shifts between training and test data. Domain Adaptation (DA) has been studied to enhance the robustness of neural networks to such distribution shifts, but it is limited by the lack of access to a target domain in common training scenarios. Domain Generalization (DG) has emerged as a solution to improve model generalization to unseen domains without relying on specific source domains. However, determining the ideal source domains for DG is challenging, and collecting multi-source domain data for training is costly and sometimes infeasible. The authors propose a novel approach called PromptStyler that leverages large-scale vision-language models to simulate various distribution shifts in the latent space without using any source domain data. They argue that these pre-trained models have already observed a wide variety of domains and can serve as a proxy for multiple source domains. By synthesizing diverse styles in a joint vision-language space via learnable style word vectors, PromptStyler aims to improve the model's generalization capability. The method maximizes style diversity and content consistency in the joint space, allowing for the simulation of distribution shifts.The proposed approach achieves state-of-the-art results on domain generalization benchmarks, including PACS, VLCS, OfficeHome, and Domain-Net, without the need for actual source or target domain data. The training process is efficient, with training time taking approximately 30 minutes using a single RTX 3090 GPU. The model is also smaller and faster at inference compared to the CLIP model.The contributions of this work are threefold: (1) the introduction of source-free domain generalization using prompts, (2) the development of a novel method for simulating images with diverse styles in a joint vision-language space, and (3) the achievement of state-of-the-art results on domain generalization benchmarks without using any actual images.