The paper introduces the concept of Visual Planning for Assistance (VPA), which involves utilizing a multi-modal assistive agent to provide guidance and support in achieving a user's desired goal. The paper proposes a methodology for VPA, employing video action segmentation and transformer-based neural sequence modeling. The authors also present an instantiation of this methodology, called Visual Language Model Planner (VLaMP), which utilizes pre-trained language models to generate action sequences based on visual history. The effectiveness of VLaMP is demonstrated through experiments and comparisons with standardized baselines. The paper concludes by highlighting the contributions of the study, including the introduction of the VPA task, the general-purpose methodology, and the instantiation of VLaMP.