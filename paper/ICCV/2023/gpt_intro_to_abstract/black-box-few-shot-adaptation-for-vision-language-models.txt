Large-scale Vision-Language (V-L) models trained with contrastive learning have achieved remarkable success in few-shot visual adaptation. However, when applied to a new domain, the domain shift exacerbates the V-L modality gap, necessitating adaptation for high accuracy. This paper explores the concept of black-box adaptation, which aims to effectively adapt a V-L model to a new domain using pre-computed features only. Soft prompt learning, inspired by NLP techniques, has emerged as a popular approach for adapting V-L models. It replaces manually designed prompts with learnable vectors to reduce the modality gap. While previous soft prompt learning approaches have shown promising results, they have two limitations: they require access to the model's weights and have a high training cost. To address these limitations, this work proposes Linear Feature Alignment (LFA), a black-box method that bridges the modality gap directly in the feature space without prompting or model weight access. LFA utilizes a simple linear mapping obtained through a simple optimization problem to align image features with their text class prototypes. The contributions of this paper include the introduction of the first black-box method for few-shot adaptation of V-L models and the formulation of LFA for supervised and unsupervised scenarios, as well as for base-to-new generalization. Experimental results demonstrate that LFA achieves better alignment and improved accuracy compared to prompt learning methods, while being more efficient and practical. Furthermore, LFA can align features computed from uni-modal models. Overall, LFA presents a promising approach for effective and efficient V-L model adaptation.