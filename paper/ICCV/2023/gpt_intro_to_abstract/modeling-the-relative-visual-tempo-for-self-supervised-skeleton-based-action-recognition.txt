This paper introduces the topic of human action recognition, which is a fundamental aspect of video understanding. The use of skeleton data in action recognition has gained attention due to its abstract and well-structured information, as well as its resistance to camera viewpoint changes and background distractions. However, most existing methods rely on full supervision, which requires labor-intensive and time-consuming annotations. To address this challenge, the paper explores self-supervised learning for action representation directly from the data. The paper draws inspiration from video self-supervised learning and applies video pretext tasks to skeleton sequences. It specifically focuses on the task of visual tempo prediction, which describes the speed at which actions are performed. However, directly applying this strategy to skeleton sequences has limitations, as the tempo of actions varies based on factors such as gender and age. Moreover, the basic strategy primarily focuses on motion information and neglects appearance information, which is equally important for action recognition.To overcome these limitations, the paper proposes the Relative Visual Tempo Contrastive Learning (RVTCLR) framework for skeleton action representation. Instead of predicting specific visual tempos, the framework compares relative visual tempos within videos. It samples clips with different visual tempos to construct contrastive pairs and trains a network to pull together same-sequence pairs and repel different-sequence pairs. Additionally, the framework introduces an Appearance-Consistency task to explicitly focus on appearance information.Furthermore, to encourage the learning of high-order semantics, the paper introduces a Distribution-Consistency (DC) branch, referred to as RVTCLR+. This branch incorporates Skeleton-specific Data Augmentation, a Fine-grained Skeleton Encoding Module, and a Distribution-aware Diversity Loss. These components enhance the representation learning process and facilitate the extraction of discriminative high-order semantics.The contributions of this work include the introduction of the RVTCLR framework, which leverages relative visual tempo learning to improve skeleton motion information representation. The framework also incorporates an Appearance-Consistency task to simultaneously focus on skeleton appearance information. Additionally, the paper proposes RVTCLR+ with a Distribution-Consistency branch to encourage the learning of high-order semantics through various components. The contrastive tasks are jointly trained using a two-branch structure, allowing the models to learn both spatiotemporal and high-order semantics.