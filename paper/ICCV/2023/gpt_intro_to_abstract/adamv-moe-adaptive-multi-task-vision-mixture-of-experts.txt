Multi-task vision recognition is an important goal in computer science as it allows for simultaneous solution of multiple objectives in real-world applications. This is particularly relevant in robotics and autonomous vehicles, where tasks such as object detection, lane detection, and pedestrian detection need to be performed concurrently. Classic multi-task learning (MTL) methods have used shared representations and specific heads to tackle this problem. However, these models suffer from unstable training and inefficient inference due to conflicts in update directions and the activation of the entire network backbone for all tasks. To address these issues, previous studies have proposed using sparsely activated Mixture-of-Experts (SMoE) layers in the MTL Vision Transformers (ViTs) models. While encouraging results have been achieved, challenges still exist in determining the appropriate network capacity for each task and ensuring effective learning in complex scenarios. In this paper, we propose AdaMV-MoE, a customized MTL MoE that automatically determines the number of experts (or model capacity) for different vision tasks. Our approach monitors validation loss to adaptively activate more or fewer experts to prevent under-fitting or over-fitting. Experimental results demonstrate the effectiveness of AdaMV-MoE in MTL, outperforming existing MTL ViT models in image classification, object detection, and instance segmentation tasks. Specifically, our approach achieves significant improvements in accuracy and performance on the ImageNet and COCO datasets.