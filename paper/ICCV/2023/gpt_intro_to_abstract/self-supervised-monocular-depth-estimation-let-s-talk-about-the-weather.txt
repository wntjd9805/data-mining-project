Depth estimation is a crucial aspect of computer vision with numerous applications such as self-driving, robotics, and scene reconstruction. While traditional computer vision methods for depth estimation focus on multi-view geometry, recent advancements in deep learning have enabled the estimation of depth from a single image. Initially, supervised approaches using expensive ground truth data collected from LIDAR and Radar sensors were employed. However, self-supervised monocular methods utilizing photometric loss have gained popularity due to their low cost and ability to generalize to various scenarios. Despite their potential, self-supervised Monocular Depth Estimation (MDE) methods have struggled with adverse weather conditions and nighttime driving. The widely used KITTI dataset, containing only daytime, dry, and sunny images, fails to capture realistic conditions. Previous attempts to train on different domains or augment datasets have resulted in poorer performance. This paper introduces the Robust-Depth method, which is trained using the KITTI dataset and employs augmentations to improve robustness to environmental changes. Unlike other computer vision domains, dataset augmentation has been found to decrease performance in MDE. This is attributed to the reliance of depth networks on vertical cues, which are disrupted by texture shifts introduced by augmentations. The paper proposes a novel self-supervised loss formulation that leverages the alignment between augmented and unaugmented data to mitigate the negative effects of augmentations. This formulation allows for the use of unaugmented depth maps as soft targets for augmented depth estimations, leading to a fully symmetric bi-directional consistency constraint. Additionally, the paper suggests a range of weather-related data augmentations, vertical cropping, and tiling techniques to reduce the reliance on simplistic depth cues and focus on deeper semantic cues. Experimental analysis, including an ablation study and comparison to State-of-the-Art methods, demonstrates the superiority of the proposed method in adverse weather conditions while performing equally well on sunny weather data.