Diffusion models have gained significant attention in recent years for image generation and manipulation tasks. These models transition from a Gaussian distribution to a data distribution to generate high-quality images. In particular, text-to-image diffusion models have shown impressive capacity in controllable image synthesis. One of the main applications of these models is image manipulation. However, there is an inherent tradeoff between editability and fidelity in these manipulation pipelines. Adding more noise gives the diffusion model more freedom to manipulate the image but makes it harder to retain original semantics during denoising, and vice versa. This tradeoff can lead to unsuccessful manipulations. Existing methods attempt to address this issue by incorporating more guidance information, but they often fail in manipulating global structures. In this work, we propose a different approach where we train an efficient image manipulator supervised by a pretrained diffusion model. Our manipulator mimics the generation capacity of the diffusion model and tackles the tradeoff problem by using correct semantic guidance. We introduce a hybrid quality score that helps select appropriate timesteps for manipulation. Our approach avoids the tradeoff problem and offers efficiency, requiring only one forward for manipulation. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. Our contributions include a novel image manipulation approach that overcomes the tradeoff problem, the introduction of a hybrid quality score to detect semantic-related timesteps, and the demonstration of our method's effectiveness and efficiency through experiments.