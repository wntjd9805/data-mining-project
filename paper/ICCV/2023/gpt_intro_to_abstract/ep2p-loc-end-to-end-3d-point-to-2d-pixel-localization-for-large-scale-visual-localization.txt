Visual localization is a crucial task in computer vision, aiming to estimate the precise camera pose of a 2D image within a 3D reference map. It has diverse applications in autonomous driving, robot navigation, and augmented reality. Previous methods utilize the PnP algorithm to compute the camera pose based on 2D-3D correspondences stored in a 3D reference map built using SfM reconstruction. However, the accuracy of visual localization heavily depends on the quality of the reference map and faces challenges due to varying lighting, weather conditions, and insufficient textures. Recent advancements in 3D sensors offer the potential to generate denser and more accurate 3D point cloud maps without relying on SfM. However, matching descriptors from 2D images with 3D point clouds remains a challenge due to the differences in representations. Several studies have attempted to address this issue but suffer from limitations such as low inlier problems, efficiency issues, or limitations in large-scale visual localization. In this paper, we propose EP2P-Loc, a novel approach to large-scale visual localization that overcomes the low inlier problem, reduces memory usage and search space, and finds all 2D-3D correspondences without the need for keypoint detection. Our method divides the 3D reference map into submaps and retrieves relevant submaps for a query image. We perform 2D patch classification to determine which patch the 3D points belong to and find the exact 2D pixel coordinates of the 3D points using positional encoding. Finally, we employ an end-to-end learnable PnP solver for high-quality camera pose estimation. We demonstrate the effectiveness of our approach through benchmarks on various datasets and show superior performance compared to existing methods. Our method is applicable to different types of 3D point cloud maps and does not assume a one-to-one correspondence between 2D pixels and 3D points.