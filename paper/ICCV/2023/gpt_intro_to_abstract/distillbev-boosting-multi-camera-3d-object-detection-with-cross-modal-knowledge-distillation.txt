Perceiving 3D environments is crucial for autonomous driving, and camera-based approaches have gained attention due to their lower cost and ability to detect distant objects. However, there is still a performance gap compared to LiDAR-based methods. In this paper, we propose DistillBEV, a cross-modal knowledge distillation approach to bridge this gap. We align features learned from images and point clouds, guiding the camera-based detector to imitate the features extracted from LiDAR. We address challenges such as the sparsity of LiDAR scans and the need to locate informative regions for knowledge transfer. We introduce region decomposition, adaptive scaling, and spatial attention to achieve effective feature alignment. Additionally, we incorporate temporal information for both teacher and student models. Our approach consistently improves performance across various teacher-student combinations. Our contributions include presenting cross-modal distillation in birdâ€™s-eye-view (BEV), proposing an effective balancing design, and achieving superior performance. Our code and models are publicly available.