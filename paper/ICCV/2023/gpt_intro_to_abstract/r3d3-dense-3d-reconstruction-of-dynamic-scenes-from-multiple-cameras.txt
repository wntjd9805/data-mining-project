Translating sensory inputs into a dense 3D reconstruction of the environment and tracking the position of the observer is crucial in robotics and the development of autonomous vehicles (AVs). While current systems rely on complex and expensive hardware and software stacks that fuse multiple sensor modalities, multi-camera systems offer a simpler and low-cost alternative available in modern consumer vehicles. However, the problem of image-based dense 3D reconstruction and ego-motion estimation of large-scale, dynamic scenes remains an open research problem, as it poses significant algorithmic challenges. This paper presents R3D3, a system for dense 3D reconstruction and ego-motion estimation from multiple cameras in dynamic outdoor environments. The approach combines monocular cues with geometric depth estimates from spatial inter-camera context and inter-/intra-camera temporal context. The system utilizes an iterative dense correspondence method on frames in a co-visibility graph to compute accurate geometric depth and pose estimates. A depth refinement network is then used to improve the reconstruction of moving objects and uniformly textured areas. The proposed system achieves state-of-the-art performance on multi-camera depth estimation benchmarks and demonstrates superior accuracy and robustness compared to monocular SLAM methods.