3D Lane Detection plays a crucial role in various autonomous driving applications. While LiDAR-based methods have shown progress in 3D perception, recent advances in 3D lane detection prefer using a monocular camera due to its lower deployment cost and longer perception range. However, detecting 3D lanes from monocular images is challenging due to the lack of depth information. Previous methods have utilized camera parameters and inverse perspective mapping (IPM) to transform features into a surrogate space for 3D lane detection. However, these methods often suffer from misalignment between the 3D surrogate and the original image, hindering accurate estimation of the road structure. In this paper, we propose LATR, an anchor-free and NMS-free Transformer architecture that performs 3D lane detection directly on the front view. Our model utilizes lane-aware queries and dynamic 3D ground positional embedding to produce well-aligned 3D features and achieve superior 3D lane detection. We compare our method with previous state-of-the-art (SoTA) methods on benchmark datasets and demonstrate significant improvements in F1 scores. Our contributions include the proposal of LATR, the introduction of a lane-aware query generator, and the evaluation of our method against existing approaches.