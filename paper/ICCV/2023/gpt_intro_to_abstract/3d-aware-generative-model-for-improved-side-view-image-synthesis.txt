Generative Adversarial Networks (GANs) have been successful in generating photo-realistic images by learning the distributions of high-resolution image datasets. However, existing GANs for pose-controllable image generation suffer from inconsistent results and limited pose controllability. To address these issues, 3D-aware GANs have been developed to achieve multi-view consistent image generation. While these models can generate high-resolution images with explicit view control, they still produce unstable image quality depending on the camera pose. This instability is caused by the challenge of simultaneously generating pose-consistent and photo-realistic images from a pose-imbalanced dataset. We propose a novel 3D GAN training method called SideGAN to generate photo-realistic images regardless of the viewing angle. Our approach splits the learning problem into two subproblems: real/fake image discrimination and pose-consistency discrimination. We introduce a dual-branched discriminator and a pose-matching loss to effectively learn pose consistency. Additionally, we present a simple pose sampling strategy to compensate for the insufficient number of side-view images in pose-imbalanced datasets. Our extensive evaluations demonstrate that SideGAN achieves state-of-the-art image and shape quality, particularly at steep view angles.