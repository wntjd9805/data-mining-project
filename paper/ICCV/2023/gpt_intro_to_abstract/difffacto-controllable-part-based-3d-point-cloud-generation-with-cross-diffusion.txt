3D shape generation is an important task in computer science, with point clouds being a popular representation due to their simplicity and availability of data. However, the generation of arbitrary shapes may not be useful as users often have a specific design idea in mind. To address this, we propose the task of controllable part-based generation, which allows users to have control over individual parts when generating novel shapes. This task can be defined in three ways: novel configurations of existing parts, existing configurations of novel parts, and novel configurations of novel parts. Existing graphics literature has explored the first and second cases, but we focus on the more challenging third case. Generating novel shapes with novel parts poses challenges due to the exponential number of plausible shapes and limited training data. Additionally, enabling control requires a method that can vary individual parts and configurations while still generating plausible shapes. To address these challenges, we introduce a probabilistic generative model that learns the distribution of shapes while enabling control over parts and configurations. Our method decomposes the shape space into individual canonicalized parts and their transformations. These factors can be sampled or encoded independently, allowing for different modes of control in generation and editing. We use part stylizers to learn independent latent spaces for each canonicalized part and a transformation sampler to learn a distribution of part configurations. To avoid mode collapse, we learn a multi-modal distribution of part configurations through conditional Implicit Maximum Likelihood. To generate plausible shapes, we introduce a cross-diffusion network that conditions on the proposed factors during the reverse diffusion process. This allows each generated point in the point cloud to be informed of both the global shape and the local part, resulting in more plausible and coherent output shapes while still enabling control. We also introduce a generalized forward diffusion kernel that allows for the explicit encoding of each part's transformations, enabling better shape reconstruction and transformation extrapolation. Our method, DiffFacto, achieves better scores compared to baselines in terms of intra-part and inter-part level scores. We demonstrate that our approach generates novel and coherent shapes through experiments and human studies. We also show that our approach allows for controllable and localized shape editing in various applications such as part-level shape interpolation, shape mixing, and transformation editing.