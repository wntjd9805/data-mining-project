This paper introduces a new approach to point cloud processing using prompt tuning. Prompt tuning involves fixing the parameters of a pre-trained model and introducing a small amount of task-specific learnable parameters into the input space, reducing storage requirements. Previous work has utilized prompt tuning in language and image domains but not in point cloud pre-trained models. The authors propose Instance-aware Dynamic Prompt Tuning (IDPT), which uses a prompt generation module to generate adaptive prompts for different point cloud instances, enhancing the robustness of pre-trained models. IDPT is inserted into the last Transformer layer for more accurate point cloud representation. Experimental results demonstrate the effectiveness of IDPT, achieving competitive performance with full fine-tuning while requiring only 7% of trainable parameters. This is the first exploration of prompt tuning on pre-trained point cloud models, addressing the distributional diversity issue in real-world point cloud data.