Low-light images suffer from visibility limitations and noise interference, impacting downstream tasks. Computational photography methods known as low-light image enhancement (LLIE) have gained attention. However, deep learning-based LLIE methods often ignore the guidance of auxiliary priors from normal-light images, resulting in artifacts and distorted colors. This paper proposes a three-stage LLIE method based on VQ-VAE. It constructs a hierarchical and expressive codebook using residual quantization and incorporates a query module to bridge the gap between low-light features and the codebook. To avoid detail loss from downsampling, a fusion branch fuses features from a pre-trained low-light encoder and normal-light decoder. Additionally, a brightness-aware attention module modulates features based on brightness, improving network robustness. Experimental results demonstrate the superiority of the proposed method over existing state-of-the-art LLIE methods.