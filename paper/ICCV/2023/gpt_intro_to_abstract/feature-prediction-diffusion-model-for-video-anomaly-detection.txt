Video anomaly detection (VAD) is a critical task in ubiquitous video surveillance for public safety. This task involves identifying unusual events that deviate from normal behaviors. However, VAD faces challenges in collecting large-scale labeled data and dealing with the unbounded nature of anomaly events. Many VAD methods have been proposed, with generative modeling being widely used. Generative Adversarial Networks (GAN) and Auto-Encoder (AE) are popular frameworks for VAD, but they have limitations in their generative capacity and reliance on auxiliary models for feature extraction. In recent years, diffusion models (DMs) have gained attention due to their powerful generative capacity and ability to handle various tasks. Motivated by this, we propose a novel DM-based approach for anomaly detection in videos. We use denoising DM modules to learn the distribution of normal samples, with one module focusing on motion and the other on appearance. Our approach uses a simple neural network as an encoder for basic 2D feature extraction, rather than relying on high-level semantic models. We make three main contributions: (1) the introduction of a novel DM-based method for VAD, (2) the design of two types of DDIM modules for motion and appearance learning, and (3) achieving highly comparable performance to methods using 3D semantic features while using only 2D images as input and no auxiliary semantic networks. Experimental results on publicly available datasets demonstrate the superiority of our approach over image feature-based VAD methods and its comparative performance to methods using 3D semantic features.