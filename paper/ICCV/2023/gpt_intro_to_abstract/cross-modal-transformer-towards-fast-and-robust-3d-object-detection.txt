Multi-sensor fusion plays a crucial role in autonomous driving systems, as different sensors provide complementary information. However, merging data from different modalities is challenging due to the significant distribution discrepancy. Existing methods often use unified representations or query-based approaches, but a truly end-to-end pipeline for multi-sensor fusion remains an open question. In this paper, we propose Cross-Modal Transformer (CMT), a simple yet effective end-to-end pipeline for robust 3D object detection. Inspired by the success of the object detection with transformer architecture, we introduce position-aware features by encoding 3D points and BEV coordinates into multi-modal tokens. Furthermore, we include position-guided queries to perform relative coordinates encoding in both image and LiDAR spaces. The CMT framework offers several advantages, including simplicity, strong performance, and robustness under sensor missing conditions. Our experimental results demonstrate that CMT achieves state-of-the-art 3D detection performance on the nuScenes dataset, providing a baseline for future research in this area.