The extraction and matching of image keypoints with descriptors is essential in various computer vision tasks. However, recent studies have shown that private information can be leaked through inversion attacks on feature descriptors. This has led to a growing interest in researching feature obfuscation techniques to protect privacy. Existing methods are primarily evaluated based on empirical performance without a rigorous understanding of attacker-independent privacy properties. In this paper, we focus on privacy protection of feature descriptors and reveal privacy leakage in adversarial affine subspace lifting. We propose a novel approach that formulates privacy protection through the lens of differential privacy, which provides theoretical guarantees and is a gold standard notion of privacy. We introduce two attacks on adversarial affine subspace embeddings and propose LDP-FEAT, a novel descriptor privatization method based on local differential privacy. Our approach achieves strong privacy guarantees while maintaining good performance in visual localization tasks. Our contributions include the development of novel attacks, a privacy protection method with favorable guarantees, and empirical results supporting practical applications.