Pre-trained vision-language models, such as CLIP, have shown promising performance on various downstream tasks without the need for task-specific training data. However, one key challenge is the limited generalization ability of hand-crafted prompts and prompt tuning, especially in zero-shot settings. Test-time prompt tuning (TPT) has been proposed as a solution to generate adaptive prompts for each test sample from an unseen new domain. Previous TPT methods have employed simple data augmentation techniques, resulting in limited diversity in the augmented data and potential misclassification of samples. In this paper, we introduce DiffTPT, a new TPT method that enhances data diversity through diffusion models while maintaining prediction fidelity. DiffTPT utilizes Stable Diffusion, a text-to-image generation model, to generate diverse images with rich visual appearance variations while preserving key semantics. We also incorporate cosine similarity-based filtration to remove spurious augmentations. Experimental results demonstrate that DiffTPT outperforms state-of-the-art TPT methods in zero-shot accuracy by an average of 5.13%. Our contributions include the development of DiffTPT as a method that balances the trade-off between data diversity and prediction fidelity, the proposal of diffusion-based data augmentation for richer visual representations, and the introduction of cosine similarity filtration for improved prediction fidelity. Overall, DiffTPT offers a practical approach for dynamic real-world scenarios where collecting labeled data for unseen target distributions is challenging.