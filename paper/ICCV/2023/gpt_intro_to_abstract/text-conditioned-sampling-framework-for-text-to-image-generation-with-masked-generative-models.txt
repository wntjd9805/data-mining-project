This paper introduces the problem of text-to-image generation and discusses the challenges in generating visual outputs that are semantically aligned with input texts. The authors categorize existing works into transformer-based autoregressive approaches and diffusion-based approaches. While transformer-based models have shown impressive performance, they suffer from unidirectional bias and slow sampling process. Diffusion-based methods reduce computational cost but require excessive sampling steps. The authors propose a new family of generative models, called token-based diffusion models, as an alternative approach. These models quantize latent features into tokens and use categorical corruption process for sampling. Compared to existing models, token-based diffusion models allow for faster generation by sampling multiple tokens simultaneously. However, sampling multiple tokens at once can lead to inconsistencies in generated images. To address this issue, the authors propose a novel sampling approach called Text Conditioned Token Selection (TCTS), which refines images based on the text condition. They also introduce Frequency Adaptive Sampling (FAS) to prevent over-simplification in longer sampling steps. The authors experimentally show the importance of revocable sampling strategies and propose a token selection model that improves text alignment and image quality. They also propose a self-attention-based sampling method to solve the over-simplification problem. Overall, this paper contributes to the field of text-to-image generation by proposing novel sampling approaches that improve text alignment and image quality while reducing the number of sampling steps.