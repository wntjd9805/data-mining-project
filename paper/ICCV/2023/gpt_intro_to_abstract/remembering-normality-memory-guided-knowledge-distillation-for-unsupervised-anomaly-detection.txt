Anomaly detection (AD) has gained significant attention due to its various applications in defect detection, medical diagnosis, and video surveillance. AD is a challenging unsupervised learning problem as it relies only on normal data since anomalous samples are often unavailable during training. Memory bank-based techniques have been proposed to compare normal data with the given target. However, these methods require complex formulations and increase computational complexity as the size of the training set grows. Reconstruction-based methods have also been explored, but they may fail to accurately reconstruct subtle details, leading to large reconstruction errors for anomaly-free data. Recent efforts have focused on knowledge distillation (KD) to detect anomalies at the feature level. However, there is a "normality forgetting" issue in KD-based anomaly detectors. To address this issue, we propose a novel Memory-guided Knowledge Distillation framework that utilizes the NR Memory to recall normal information and strengthen feature normality in the student network. We also devise a normality embedding learning strategy to promote the memorization of normal information from anomaly-free data. Our proposed method, MemKD, outperforms state-of-the-art competitors on five widely used benchmarks and extensive experiments validate its effectiveness.