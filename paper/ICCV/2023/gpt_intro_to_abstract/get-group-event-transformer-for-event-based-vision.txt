Event cameras, a type of bio-inspired vision sensors, capture per-pixel illumination changes asynchronously. Compared to traditional frame cameras, event cameras offer advantages such as high temporal resolution, high dynamic range, and low power consumption. Many applications, including object classification and high-speed object detection, can benefit from event cameras, especially in situations with limited power consumption or challenging motion and lighting conditions.However, event data is stored differently from traditional frames, making it challenging to directly apply image-based neural networks. To address this issue, previous works have proposed converting events into image-like representations, which are then fed to deep neural networks. More recent works have utilized the Transformer architecture or CNN-based backbones to extract features from event representations. However, these designs primarily focus on spatial information and do not fully utilize the temporal and polarity information contained in events.In this paper, we propose a new approach to event-based feature extraction using a Transformer-based backbone called Group Event Transformer (GET). We introduce a novel event representation called Group Token, which groups events based on timestamps and polarities. GET incorporates two key modules: the Event Dual Self-Attention block, which extracts correlations between pixels, polarities, and times within Group Tokens, and the Group Token Aggregation module, which achieves reliable spatial and group-wise token aggregation.We evaluate our proposed approach on various event-based classification and object detection datasets and demonstrate its state-of-the-art performance. Our contributions include the introduction of the Group Token representation, the Event Dual Self-Attention block for effective feature communication, the Group Token Aggregation module for information integration and decoupling, and the development of the GET backbone for event-based vision.