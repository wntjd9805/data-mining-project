Image captioning is a challenging task for computer vision systems, and evaluating the quality of generated captions is also difficult. While automated metrics may rank the best captioning systems higher than humans, human raters still prefer captions generated by humans, indicating limitations in both captioning models and evaluation metrics. One limitation is the lack of specificity in generated captions. Conventional captioning models aim to generate captions with high conditional probability given the image, but they often produce non-specific captions compared to the ground truth. Previous work has focused on reference-based captioning metrics, which penalize captions that are more specific than the ground truth. In this paper, we propose two strategies to guide image captioning models towards generating more specific captions: classifier-free guidance (CFG) and language model (LM) guidance. CFG increases the conditional probability of the image given the caption at the expense of the caption given the image, leading to more specific but less grammatical captions. LM guidance utilizes a few-shot prompted language model to guide the captioning model and achieves slightly better trade-offs between reference-free and reference-based captioning metrics compared to CFG. Additionally, LM guidance significantly improves the captions generated by a model trained on minimally curated web data. Overall, this study introduces strategies to improve the specificity of image captions and provides insights into the trade-offs between evaluation metrics for image captioning systems.