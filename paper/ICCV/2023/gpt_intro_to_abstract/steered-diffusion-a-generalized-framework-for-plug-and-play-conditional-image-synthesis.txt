Deep diffusion-based probabilistic generative models have shown promise in synthesizing high-quality content in various domains, but they often require large amounts of labeled data for conditional generation. To overcome this limitation, we propose a plug-and-play framework called Steered Diffusion, which incorporates pre-existing models to guide an unconditional diffusion model. Unlike existing methods, our approach utilizes the implicit prediction of the diffusion model and allows the use of any pre-trained network for steering. We present experiments on multiple conditional generative tasks, including identity replication, semantic image generation, linear inverse problems, and text-conditioned image editing. Our framework demonstrates effectiveness in both label-level synthesis and image-to-image translation tasks. We also introduce an implicit conditioning-based sampling strategy and a strategy using multiple steps of projected gradient descent to improve sample quality. Overall, our work contributes to the advancement of diffusion-based image editing and image-to-image translation.