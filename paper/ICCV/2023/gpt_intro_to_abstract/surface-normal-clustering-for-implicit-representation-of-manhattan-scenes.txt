This paper addresses the problem of improving 3D neural radiance field representations for indoor Manhattan scenes, specifically focusing on areas such as floors, walls, tables, and wardrobes. Unlike previous approaches, this method aims to exploit the Manhattan scene prior without assuming any knowledge of the scene's structure or semantics. Additionally, the unknown Manhattan coordinate frame is also considered in this approach, making the problem particularly challenging. The simplicity of the Manhattan world assumption allows for intuitive reasoning about complex scenes, but recovering the Manhattan frame from images is a difficult task. Previous methods rely on known 3D reconstruction or image primitives, whereas this approach proposes a novel method for jointly learning the Manhattan frame and neural radiance field in an end-to-end manner. The method utilizes the explicitly derived normals in the implicit neural fields and employs a robust orthogonal normals search to handle non-Manhattan scenes. Experimental results demonstrate the robustness and effectiveness of the proposed method in improving 3D representation in neural radiance fields for a variety of indoor scenes.