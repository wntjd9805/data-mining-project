Abstract: In recent years, text-to-image generative models have shown impressive capabilities in producing creative images based on textual inputs. However, these models often suffer from limitations that result in semantically inaccurate image outputs. In this paper, we propose a method to address two key issues observed in existing models: attention overlap and attention decay. We analyze the cross-attention maps of these models to understand the reasons behind these issues. Based on our observations, we introduce two novel loss functions called attention segregation loss and attention retention loss. The attention segregation loss minimizes the overlap between concepts' cross-attention maps, while the attention retention loss ensures information retention across denoising time steps. Our proposed method significantly improves generation results and produces images that accurately reflect the input text prompt. We conduct extensive evaluations to demonstrate the effectiveness of our method compared to baseline models.