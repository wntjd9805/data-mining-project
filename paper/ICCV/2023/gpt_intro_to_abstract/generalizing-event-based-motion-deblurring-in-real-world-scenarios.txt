Motion blur is a common problem in frame-based cameras, particularly in scenes with dynamic targets or camera ego-motion. Traditional approaches to motion deblurring rely on deconvolution and blur kernel estimation techniques, while recent research has made advancements using deep learning methods. However, these approaches often assume specific motion patterns and struggle to handle complex non-uniform motions and extract precise motion from severely blurred frames. The emergence of event cameras offers a new solution, as their low latency and ability to capture high-contrast edges compensate for the issues caused by motion blur. However, current event-based deblurring methods are limited by the distribution of training data and the spatial resolution disparity between frames and events. This paper proposes a Scale-Aware Network (SAN) that can generalize the performance of event-based motion deblurring in both spatial and temporal domains. The SAN utilizes a Multi-Scale Feature Fusion (MSFF) module to handle different input spatial resolutions and an Exposure-Guided Event Representation (EGER) to select target latent images without model modification. A two-stage self-supervised learning framework is also introduced to fit real-world data distribution and handle varying spatial and temporal scales of motion blur. The paper presents a real-world dataset to facilitate deblurring research and conducts extensive experiments to validate the effectiveness of the proposed approach.