Online shopping has become increasingly popular in recent years, particularly among the younger generation, due to its convenience and cost-effectiveness. In line with this trend, virtual try-on technology has gained attention from fashion brands and online retail platforms, aiming to enhance the online shopping experience by allowing consumers to visualize how garments fit without the need to physically try them on. Virtual try-on methods can be categorized into image-based and 3D model-based approaches, with image-based technology receiving more attention due to its lower cost and accessibility. Image-based virtual try-on methods aim to produce realistic fitting results by replacing the cloth region of a person image with a garment image while preserving the person's appearance and pose. However, existing approaches often suffer from issues such as inappropriate warping of the garment image or inaccurate estimation of the target segmentation map, leading to distorted garment shapes. To address these challenges, this paper proposes a pose-garment keypoints guided inpainting method for image-based virtual try-on. A graph-based model is proposed to extract pose-oriented garment keypoints for garment warping and target segmentation map estimation. Additionally, a semantic-conditioned inpainting scheme is proposed to generate the final try-on image. Extensive experiments demonstrate the effectiveness of the proposed method, with quantitative and qualitative improvements over previous approaches.