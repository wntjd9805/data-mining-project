Estimating the pose of multiple people from overlapping cameras is a crucial vision problem in computer science. Current state-of-the-art methods use volumetric multi-view approaches, which lift 2D image features to a feature volume and regress 3D pose. However, these methods have limitations: they are slow and cannot support real-time inference due to the use of 3D convolutions or cross-view transformers, and they are unable to reason over time, limiting their accuracy for tasks like motion prediction.To address these issues, we propose TEMPO, a multi-view temporal pose estimation method. TEMPO leverages temporal context from previous timesteps to improve pose estimation accuracy and smoothness. Our model is capable of tracking people over time, predicting future poses, and achieving near real-time performance. Inspired by previous work in 3D object detection, TEMPO utilizes recurrently aggregated spatiotemporal context to learn powerful representations efficiently.The TEMPO method consists of three stages: detecting person locations in 3D space by unprojecting image features, regressing 3D bounding boxes, and performing tracking by matching box centers with previous detections. For each detected person, a spatiotemporal pose representation is computed by recurrently combining features from current and previous timesteps. This representation is then decoded into an estimate of the current pose as well as future poses. Importantly, our method can handle temporal tasks like tracking and forecasting without sacrificing efficiency.We evaluate TEMPO on various pose estimation benchmarks, including the CMU Panoptic Studio dataset, where it achieves state-of-the-art results with a 10% improvement. It also performs competitively on the Campus, Shelf, and Human3.6M datasets. We further collect our own multi-view dataset with highly dynamic scenes, on which TEMPO outperforms other methods significantly. We demonstrate the effectiveness of our model in pose tracking and evaluate its pose forecasting quality on the CMU Panoptic dataset. Unlike many existing methods, our approach generalizes across different datasets and camera configurations without requiring additional fine-tuning.In summary, our contributions include developing the most accurate multi-view, multi-person 3D human pose estimation model that utilizes temporal context for smoother and more accurate poses. Our model runs efficiently without performance degradation, supports tracking and forecasting human pose, and shows generalization across multiple datasets and camera configurations.