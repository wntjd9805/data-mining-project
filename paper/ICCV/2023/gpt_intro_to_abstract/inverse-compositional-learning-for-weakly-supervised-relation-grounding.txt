The objective of the Video Relation Grounding (VRG) task is to determine the spatial and temporal extents of a given query relation within an untrimmed video. Existing visual grounding approaches have been effective in previous tasks involving lengthy and intricate linguistic descriptions, but prove to be less productive in the VRG due to the simplicity and sparsity of the information present in a 3-tuple relation phrase. Additionally, current methods treat a relation as a whole without accounting for partial aspects, neglecting crucial cues and information embedded within constituent parts. In this paper, we propose a novel inverse compositional learning (ICL) approach for video relation grounding. We formulate VRG as a joint optimization problem that incorporates both holistic-level reasoning and partial-level reasoning. We devise an inverse loss function to learn the compositional relevance between the relation and visual features, and employ a grounding by classification scheme for partial-level reasoning. Our method outperforms state-of-the-art methods on the ImageNet-VidVRD and HICO-Det datasets.