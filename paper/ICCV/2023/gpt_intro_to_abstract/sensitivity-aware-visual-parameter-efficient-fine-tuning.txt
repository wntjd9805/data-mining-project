This paper introduces a novel approach called Sensitivity-aware visual Parameter-efficient fine-Tuning (SPT) to address the challenge of adapting pre-trained representations to downstream tasks effectively. The de-facto choice for adaptation is full fine-tuning, which tunes all parameters but is storage-intensive. An emerging solution is visual Parameter-Efficient Fine-Tuning (PEFT), which only tunes a small number of trainable parameters and mitigates the storage burden. However, existing PEFT methods neglect task-specific domain gaps and characteristics, limiting their performance. To overcome this limitation, the paper proposes SPT, which identifies task-specific important positions and allocates trainable parameters using unstructured and structured tuning granularities. A sensitivity criterion is introduced to quickly identify sensitive parameters, and unstructured tuning is combined with structured tuning to optimize the representational capability under a desired parameter budget. Experimental results on 24 downstream recognition tasks show that SPT is complementary to existing PEFT methods and achieves significant performance improvements, outperforming the state-of-the-art PEFT methods on the FGVC benchmark. Overall, this paper contributes a fast and effective approach for identifying task-specific important positions and allocating trainable parameters to improve fine-tuning efficiency and accuracy in downstream tasks.