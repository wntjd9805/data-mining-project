Dataset distillation (DD) is a technique that aims to create a smaller synthetic dataset that can match the training effect of a larger original dataset on neural networks. This technique has gained attention due to its potential to alleviate the burden on storage, transmission, and training of deep learning models. Recent advances in DD have shown that models trained with only a few synthetic samples can retain most of the performance of models trained with real data. However, existing methods for DD work iteratively, which introduces computational overhead and significant latency. In this paper, we propose few-shot dataset distillation and focus on the performance of DD when only a few or even a single neural network is available for training. We introduce the concept of distillation space, a dedicated neural network space that optimizes the error on real data for networks trained with the synthetic dataset. To reduce complexity, we propose a translative modeling approach where the synthetic dataset is distilled in a random but fixed neural network space and then translated to the desired space via a translator network. Our experiments show that this approach can achieve comparable performance to existing methods with ∼ 15× faster acceleration. Our contributions include the study of few-shot dataset distillation, the introduction of the quad-level definition of distillation space, and the proposal of an efficient translative modeling approach with a pre-training algorithm.