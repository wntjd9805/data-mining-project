Recent advancements in deep learning, particularly in computer vision and natural language understanding, have been attributed to the availability of web-scale training data. However, in real-world scenarios, user data is spread across various domains, making centralized learning schemes undesirable due to privacy concerns and regulations. As a result, federated learning (FL) has gained significant research interest as a privacy-preserving distributed learning framework. FL has been successfully applied in various applications, including medical image diagnosis and face recognition. The mainstream FL approach, known as FedAvg, learns a global model by averaging model parameters trained on clients' private data. However, this approach fails to handle heterogeneity in data distribution across clients, resulting in performance degradation. Personalized federated learning (pFL) methods have been proposed as an alternative, allowing each client to train a personalized model that adapts to their data distribution. While pFL methods handle data heterogeneity, they are typically limited to small models, leading to restricted feature capabilities and training instability. To leverage the representations derived from large foundation models, which have shown effectiveness in centralized learning, a recent approach called ViT-FL incorporates pre-trained Vision Transformer (ViT) models into FL algorithms. However, transporting entire model parameters between clients and the server becomes computationally and communication-intensive. To address these challenges, this paper proposes a personalized FL scheme called client-specific Prompt Generation (pFedPG). Instead of updating and transporting entire model parameters, pFedPG generates personalized prompts for each client, enabling efficient adaptation to local data distribution. A personalized prompt generation module at the server side learns the underlying optimization directions among clients to produce client-specific prompts. By iteratively training the prompt generator and adapting the prompts for each client, pFedPG achieves effective and efficient model personalization. Experimental results on benchmark datasets demonstrate the superiority of pFedPG over existing pFL approaches, both in handling heterogeneity and training efficiency.