In recent years, Vision-Language Models (VLMs) have shown high effectiveness in various classification, retrieval, and robustness tasks. These models are trained on large-scale datasets consisting of image-text pairs, where the text provides concise captions describing salient attributes in the image. However, real-world multimodal datasets, such as those obtained from healthcare settings or product databases, are more complex and include text that describes multiple attributes occurring in fine-grained regions of the image. This complexity presents a challenge for standard VLMs, as they struggle to learn fine-grained region-attribute relationships. In this paper, we introduce the concept of pairwise complexity and conduct a systematic evaluation of training dataset complexity on the fine-grained reasoning ability of standard VLMs. We propose a self-supervised multimodal representation learning approach called ViLLA, which addresses the limitations of standard VLMs by learning accurate relationships between image regions and textual attributes. ViLLA consists of a two-stage pipeline where a lightweight mapping model decomposes image-text samples into region-attribute pairs, followed by training a standard VLM on these mappings. We demonstrate that ViLLA outperforms standard VLMs on fine-grained reasoning tasks across various domains, including zero-shot object detection and text-region retrieval. Our findings emphasize the need for VLMs that can effectively capture region-attribute relationships in complex multimodal datasets.