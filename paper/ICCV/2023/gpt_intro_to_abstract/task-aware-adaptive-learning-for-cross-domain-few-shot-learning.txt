Traditional deep learning models have shown excellent generalization performance when trained on large datasets with labeled samples. However, in many realistic applications, such as rare disease diagnosis and fine-grained recognition, there is a lack of abundant samples and reliable annotations. Few-shot learning aims to address this issue by enabling models to quickly learn new classes with only a few labeled samples for each one. While meta-learning-based few-shot learning methods have achieved success in settings where the train and test tasks are sampled from the same domain, they often fail to generalize well to novel classes that are different from the source domain. This is due to the fact that previous models primarily focus on adapting to new classes with limited labeled samples, without effectively addressing the domain shift problem between the target task and training domain. Cross-domain few-shot learning methods have been proposed to tackle this problem, introducing task-specific parameters to enable adaptation to the feature distributions of new tasks.Existing cross-domain few-shot learning methods use either an auxiliary network or directly attach task-specific parameters to the pretrained model. While these approaches have improved performance, the fixed number of task-specific parameters may not be flexible enough to account for the diversity of tasks, resulting in less generality. In this paper, we investigate the performance of test tasks when using models equipped with varying sizes of task-specific parameters. We introduce Task-specific Adapters (TA-Modules) attached to the backbone network, and analyze the optimal placement of these parameters in different layers for effective adaptation. Based on these findings, we propose the Task-aware Adaptive Network (TA2-Net) which learns the optimal task-specific parameter policy adaptively. Our contributions include the development of TA2-Net, which can adaptively learn optimal task-specific parameter policies for each target task, and evaluation of our model on the Meta-dataset, showing superior performance compared to existing methods. We also analyze the parameter distribution and discover that domains with significant distribution shift often require more task-specific parameters for effective adaptation. Additionally, deeper layers in the network require more task-specific parameters than shallower layers for learning task-specific features.