Human gaze is an important indicator of human cognition and behavior, with applications in various fields. Gaze estimation methods typically rely on models that are specific to individual persons and require complex camera systems. Appearance-based methods, on the other hand, only require a single webcam but suffer from limited field of view and facial self-occlusion. In this paper, we propose DV-Gaze, a dual-view gaze estimation network that takes advantage of the affordability and larger field of view provided by dual cameras. Our method incorporates dual-view information at multiple feature scales by using dual-view interactive convolution blocks, and employs a dual-view transformer for gaze estimation. We also introduce a dual-view gaze consistency loss function to improve performance. Our contributions include exploring dual-view gaze estimation, proposing DIC blocks for feature extraction, and introducing a dual-view transformer and gaze consistency loss. Our work is the first to explore dual-view gaze direction estimation and demonstrates improved accuracy over single-view methods.