Person re-identification (ReID) is a task that involves retrieving the same person across different camera views. Despite the promising results of current fully supervised ReID methods, their performance tends to decline when applied to unseen domains. To address this issue, unsupervised domain adaptation (UDA) and domain generalizable (DG) ReID techniques have been explored, but they have limitations that hinder their applicability in real-world scenarios. In this paper, we propose a method called Identity-seeking Self-supervised Representation learning (ISR) that aims to learn domain-generalizable ReID representations without any annotation. Unlike previous DG ReID methods that require labeled training data, we leverage a large-scale dataset of unlabeled internet videos for training. The key idea behind ISR is to learn identity discrimination through contrastive information between samples. We introduce a reliability-guided contrastive loss to mitigate the adverse impact of noisy positive pairs in the unsupervised construction of positive pairs. Experimental results demonstrate that ISR outperforms existing methods under both domain-generalizable settings and practical settings such as pre-training for supervised fine-tuning or few-shot learning. The contributions of this paper include the proposal of ISR, the introduction of a reliability-guided contrastive loss, and the verification of its effectiveness through extensive experiments. ISR achieves high performance on benchmark datasets without human annotation and fine-tuning, surpassing the state-of-the-art supervised DG method.