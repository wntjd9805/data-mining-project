Face recognition systems have become widely used in various applications, but they are vulnerable to face presentation attacks. Existing face anti-spoofing (FAS) methods have shown impressive accuracy in intra-domain scenarios but fail to generalize well to unseen target domains due to domain gaps and limited training data. This paper explores the use of vision transformers (ViTs) and multimodal pre-trained models to improve cross-domain FAS performance. The proposed approach involves finetuning multimodal pre-trained ViTs, aligning visual representations with text prompts during finetuning, and employing a multimodal contrastive learning strategy to bridge the domain gap. Experimental results demonstrate that the direct finetuning of multimodal pre-trained ViTs achieves better FAS generalizability. The proposed approach significantly improves cross-domain FAS performance and shows promise in addressing the challenges of large domain gaps and limited training data availability.