According to research, machine learning models lack robustness against adversarially chosen perturbations, as demonstrated by Szegedy et al. These engineered perturbations are indiscernible to humans but cause neural networks to misclassify images with high confidence. One specific concern is the adversarial patch attack, where an attacker physically places a printed patch in a scene to cause a vision network to malfunction. This type of attack is particularly worrying for autonomous vehicles, as it could lead to critical misinterpretations of road signs. The evaluation of such attacks has been challenging, as real-world experiments are costly and not practical on a large scale. Existing evaluation methods, either physically attaching attacks to objects or digitally simulating patch attacks, have significant drawbacks and do not provide rigorous evaluations. In response to this, the authors propose the REalistic Adversarial Patch Benchmark (REAP), which addresses the limitations of previous evaluations. REAP includes a large-scale dataset of road sign images, realistic patch rendering tools, and variations in lighting conditions and image distribution. The authors use REAP to evaluate existing attacks on three different object detection architectures and implement a baseline defense. They find that existing patch attacks are not as effective as expected and that performance on synthetic data does not reflect performance on REAP. Lighting and patch placement are identified as important factors in the attack success rate. The authors believe that the conclusions drawn from REAP are just the beginning and that REAP will support future research in adversarial patches by enabling more accurate evaluations of attacks and defenses.