This paper introduces a method to speed up transformer-based 3D object detection in computer vision tasks. The method is inspired by network pruning and aims to reduce unnecessary tokens in the transformer architecture, which can increase computational complexity. Unlike previous works that focus on image classification, this paper focuses on 3D object detection and addresses the challenge of detecting objects from all tokens. A deterministic halting module is proposed, along with a token recycling mechanism and a differentiable forward-pass to handle the non-differentiability of token halting. The paper also employs a non-uniform token sparsity loss to improve the learning of the halting module. The method is designed for safety-critical applications, such as autonomous driving. The contributions of this paper are summarized as the proposed deterministic halting module, the differentiable forward-pass, and the non-uniform token sparsity loss. The effectiveness of the method is evaluated through theoretical analysis and comparisons with existing approaches.