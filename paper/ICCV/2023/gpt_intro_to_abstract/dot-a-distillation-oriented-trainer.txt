This paper introduces the concept of knowledge distillation, which involves transferring knowledge from a large teacher model to a smaller student model in deep learning tasks. While there have been advancements in learning algorithms and understanding the working mechanism of knowledge distillation, the optimization property of this technique has not been widely investigated. The paper focuses on studying the optimization of the distillation loss and its influence on the task loss during knowledge distillation. It is observed that the distillation loss helps the student model converge to flatter minima, leading to better generalization. However, introducing the distillation loss also results in a trade-off, as the task loss is not converged as effectively as the cross-entropy baseline. The trade-off is counter-intuitive since the teacher model always has a lower task loss due to its larger capacity. To address this trade-off, the paper proposes the Distillation-Oriented Trainer (DOT) which adjusts the optimization orientation by weighing different momentums for the task and distillation losses. DOT ensures that the optimization is dominated by the gradients of the distillation loss, thus breaking the trade-off. Experimental results demonstrate the effectiveness of DOT in achieving better performance and discovering new insights into the optimization of knowledge distillation.