This paper explores the existence of latent compositional structures in the embedding space of natural language processing models. It argues that representations of composite concepts can be decomposed as linear combinations of embedding vectors, which can be considered as "ideal words" for composing new concepts within the model's representation space. The paper focuses on the application of this idea in vision-language models and discusses how linear compositionality can improve image classification and retrieval tasks. The authors also explore the tension between compositionality and contextuality in language and propose methods to approximate compositional meanings from originally contextual representations. The paper presents geometric perspectives on compositional linear structures and discusses their relationship with disentangled representations. It further studies the conditional independence of factors in visual-language models and examines how linear structures can emerge in weakly disentangled data distributions. The empirical results demonstrate that embeddings of composite concepts can be well-approximated as linear compositional structures, leading to effective strategies for classification and retrieval problems. The paper also visualizes manipulations of factored embeddings using a CLIP-guided diffusion model. Overall, the contributions of this work include the description and explanation of compositional linear structures, the investigation of their existence in visual-language models, and the empirical demonstration of their effectiveness in solving tasks in a compositional setting.