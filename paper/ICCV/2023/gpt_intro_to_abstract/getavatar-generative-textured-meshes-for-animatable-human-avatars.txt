Generating high-quality 3D human avatars with explicit control over camera poses, body poses, and shapes has been a long-standing challenge in computer vision and graphics. This problem is particularly difficult for modeling dynamic human bodies with large articulated motions, as existing 3D generative models are not designed to handle body deformations and struggle to manipulate or animate generated avatars. Previous methods incorporating human priors into 3D generative models face challenges in generating avatars with fine geometric details and suffer from high computational costs. In this paper, we propose GETAvatar, a generative model that produces explicit textured 3D meshes with rich surface details for animatable human avatars. GETAvatar improves the quality of geometry by modeling fine geometric details with a normal field, which associates a normal vector with each point on the surface, enabling the capture of realistic details from 2D normal maps. Our model also introduces a body-controllable articulated 3D human representation with body deformation modeling and explicit surface modeling, enabling deformation to target pose and shape and the extraction of the underlying human body surface as an explicit mesh. GETAvatar overcomes the computational inefficiency of existing methods by generating textured meshes in a differentiable manner and rendering high-resolution images using a rasterization-based surface renderer. Extensive experiments demonstrate the effectiveness of GETAvatar in terms of both visual and geometry quality. Overall, our work contributes a generative model that enables high-quality 3D human generation, with control over camera poses, body shapes, and poses, and supports various applications such as re-texturing, 3D reconstruction, and re-animation.