Self-supervised pre-training methods in computer vision aim to learn representations from unannotated data and then fine-tune them for downstream tasks. This requires the design of pretext tasks, as well as generic architectures that can be easily transferred. Previous methods have shown success in high-level tasks but have not achieved breakthroughs in geometric tasks like stereo matching and optical flow. In this paper, we propose a solution that combines the cross-view completion pretext task with a large-scale dataset collection approach. We use real-world images and carefully control visual overlap to generate high-quality training pairs. Additionally, we scale up the model by using larger encoders and decoders, and we introduce a Rotary Positional Embedding to encode relative positional information. We then finetune the pre-trained model on stereo matching and optical flow tasks using a Dense Prediction Transformer. Our architecture achieves state-of-the-art performance on various benchmarks without relying on task-specific designs or prior knowledge.