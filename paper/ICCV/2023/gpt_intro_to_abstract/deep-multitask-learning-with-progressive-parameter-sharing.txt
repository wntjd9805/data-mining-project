Performing multiple tasks simultaneously is a fundamental ability for intelligent agents. While deep neural networks (DNNs) have achieved success in various computer vision applications, learning and performing similar but distinct tasks simultaneously remains a challenge. To address this, researchers proposed the Multi-Task Learning (MTL) paradigm, which trains one model to act as multiple distinct models. However, two problems need addressing: the construction of the parameter-sharing scheme and controlling the optimization process for the best joint task performance. This paper proposes Multitask Learning with Progressive Parameter Sharing (MPPS), a novel progressive parameter-sharing strategy that combines the design of a parameter-sharing scheme with the optimization of training dynamics. MPPS incorporates a dynamic resource allocation strategy at the neuron level to allow adaptive knowledge sharing among different tasks. The paper presents extensive experiments and evaluations, demonstrating the superiority of MPPS in improving task performance in image classification and dense prediction tasks. The paper also introduces the concept of "Exclusive Capacity" and designs an Exclusive Capacity scheduler based on curriculum learning intuition. The contributions of this paper include proposing a novel MTL dynamic resource allocation strategy, introducing the Exclusive Capacity concept, and demonstrating competitive results in various tasks.