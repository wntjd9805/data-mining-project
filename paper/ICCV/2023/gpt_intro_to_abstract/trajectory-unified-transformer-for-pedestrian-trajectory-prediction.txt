Pedestrian trajectory prediction is an important task that links the perception and planning systems in autonomous driving. Existing predictors address the multimodal nature of pedestrian motion by modeling future trajectories in specific spaces. However, these predictors often require post-processing steps, which are time-consuming and neglect probability information. This paper proposes a new framework called TUTR (Trajectory Unified Transformer) that effectively eliminates the need for post-processing. TUTR unifies various components of pedestrian trajectory prediction into an encoder-decoder transformer architecture. It utilizes global prediction and mode-level transformer encoding to parse relationships across different motion modes. It also employs a social-level transformer decoder to consider social interactions with neighbors. Additionally, TUTR incorporates dual prediction to forecast diverse trajectories and corresponding probabilities in parallel. Experimental results on popular pedestrian trajectory datasets demonstrate that TUTR achieves comparable accuracy performance and faster inference speed compared to existing methods, especially in metrics considering probability. The contributions of this paper include the proposal of the TUTR framework, the parsing of relationships across motion modes, and the achievement of state-of-the-art performance in certain metrics while maintaining fast inference speed.