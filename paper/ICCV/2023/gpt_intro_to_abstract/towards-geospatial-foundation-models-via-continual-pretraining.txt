Geospatial technologies play a crucial role in various applications worldwide, enhancing our understanding of the earth and our interactions with it. In recent years, researchers have explored the use of pretrained foundation models in the geospatial domain to improve deep learning models for downstream tasks. Two main approaches have been used: leveraging existing foundation models trained on natural images, and training models specifically for the geospatial domain. While the former approach is convenient, it is not optimal for geospatial data due to the domain gap. The latter approach requires a significant amount of data and training time, making it resource-intensive. In this paper, we propose a different paradigm for producing effective geospatial foundation models with fewer resource costs. We focus on pretraining data selection and utilizing continual pretraining from readily-available ImageNet models. Continual pretraining involves enhancing existing foundation models through a secondary pretraining stage, resulting in stronger models for downstream tasks. We find that using ImageNet representations as an auxiliary distillation objective during pretraining leads to stronger geospatial models. We propose a multi-objective continual pretraining paradigm that leverages ImageNet representations and promotes the acquisition of in-domain features through self-supervised learning. Our proposed Geospatial Foundation Model (GFM) outperforms previous state-of-the-art methods across various geospatial tasks. We evaluate GFM on seven datasets covering change detection, classification, multi-label classification, semantic segmentation, and super-resolution, demonstrating its favorable performance. This novel paradigm enables the creation of highly effective geospatial models with minimal resource costs, providing sustainable benefits for the geospatial community.