Structured information extraction (SIE) is the process of extracting entities and their relationships from documents and returning them in a structured format. Unlike traditional information extraction tasks, SIE assumes the availability of an image representation of the document and utilizes optical character recognition (OCR) to obtain unstructured text. Automating the extraction of structured information from documents is important for efficiency and scalability. Previous approaches to document intelligence have focused on either computer vision or natural language processing (NLP) methods. Recently, models that pre-train on large-scale document collections have shown promising results in various document intelligence tasks. However, existing approaches for SIE, such as IOB tagging and graph-based methods, have limitations in accurately extracting structured information. To address these limitations, this paper introduces a new formulation for SIE that represents entities as anchor words and boxes, and proposes a new model called Document Transformer (DocTr) that combines language and visual object detection for joint vision-language document understanding. Additionally, a new pre-training task called masked detection modeling (MDM) is proposed to improve box prediction in the context of language. Experimental results demonstrate the effectiveness of the proposed formulation and pre-training task in improving SIE performance compared to existing methods.