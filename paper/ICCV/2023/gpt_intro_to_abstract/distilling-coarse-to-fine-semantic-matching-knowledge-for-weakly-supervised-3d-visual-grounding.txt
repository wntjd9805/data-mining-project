3D Visual Grounding (3DVG) involves localizing objects in a scene based on natural language sentences. This task has gained attention due to its various applications. However, existing approaches require bounding box annotations for each sentence query, which is time-consuming and expensive. To address this, we focus on weakly supervised 3DVG, which only requires scene-sentence pairs for training. Weakly supervised 3DVG poses two main challenges: learning to link sentences to corresponding objects in a point cloud with multiple objects, and distinguishing the target object from interfering objects in the scene. To tackle these challenges, we propose a coarse-to-fine semantic matching model that measures the similarity between object proposals and sentences. The model generates object-sentence matching scores guided by coarse-to-fine semantic similarity analysis. It selects proposals with the highest similarity to the sentence, filters out irrelevant proposals, conducts part-of-speech tagging, and reconstructs masked keywords. Object-sentence matching scores are measured using reconstruction loss. We also utilize knowledge distillation to transfer the knowledge of the matching model to a two-stage 3DVG model. Our approach eliminates the need for dense object-sentence annotations and achieves improved performance and reduced inference costs. Experimental results on three datasets validate the effectiveness of our method. This work is the first to address weakly supervised 3DVG at the scene-sentence level.