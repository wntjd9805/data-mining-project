Point cloud videos captured by 3D sensors have become important in fields like robotics and autonomous driving for perceiving the environment. While early approaches focused on static point clouds, more attention has recently been given to point cloud videos. However, annotating point cloud videos is time-consuming, leading to a growing interest in self-supervised learning on dynamic point clouds. This paper proposes a unified self-supervised contrastive learning framework, called PointCPSC, for point cloud videos. The framework enables representations that capture fine-grained dynamics and hierarchical semantics for multiple downstream tasks. It introduces a new pretext task to align predicted prototypes and target prototypes, as well as soft category assignments between predictions and targets. Additionally, it employs a feature similarity-based sample selection strategy for effective representation learning. The framework achieves remarkable performance on various downstream tasks and is supported by extensive ablation studies and visualized analysis.