In this paper, we propose a novel approach for spatiotemporally varying lighting effects in layered-based video editing. We introduce a multiplicative-residual estimator that effectively decomposes the lighting conditions from the video without supervision, enabling better-quality video edits by fusing the edits with expected illuminations. Our approach is efficient in both training and rendering, with improved training time and fewer resources required, allowing for training on higher-resolution or longer videos. The trained model achieves fast video rendering using hashing-based coordinate inference, enabling real-time video editing. Experimental results demonstrate appealing video edits in challenging contexts, such as modifying moving objects, handling occlusion, and manipulating camera motion. Additionally, we explore the use of neural networks for deriving layered representations from videos and incorporate the multiplicative residual representation to model lighting variations for illumination-aware video editing. Our approach contributes to the field of video editing by providing an efficient and effective solution for handling spatiotemporally varying components, resulting in seamless editing across all frames.