In this paper, we address the challenge of story visualization, which involves generating a sequence of images from a paragraph of natural language sentences. This task is challenging due to the need to capture the visual details and context spread across the sentences. Existing benchmark datasets for this task lack linguistic variations, hindering linguistic generalization performance. To overcome these challenges without requiring large-scale data and models, we propose a new memory scheme called Context Memory and Online Text Augmentation (CMOTA). Our model leverages a bi-directional Transformer to encode context and generates pseudo-texts online to handle linguistic variations during inference. We validate the effectiveness of our model on widely used benchmark datasets and show that it outperforms state-of-the-art methods by a large margin. Our contributions include the introduction of a new memory architecture for Transformer to incorporate contextual information and the use of online text augmentation for better linguistic generalization. Our model achieves superior performance with similar or less computational complexity compared to previous methods.