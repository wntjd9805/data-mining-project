Recently, transformer models have achieved significant success in various vision applications, but their deployment on resource-constrained mobile devices remains challenging. Vision transformers (ViTs) rely on global self-attention, which has a quadratic complexity with respect to image resolution, making it impractical for low-powered mobile devices. Convolutional neural networks (CNNs) are the preferred choice for real-time deployment on mobile devices, but they have limitations in capturing long-range dependencies and adapting to variable input resolutions. Therefore, there is a need to develop efficient and flexible models that combine the strengths of both CNNs and transformers. To address this, hybrid approaches using lightweight CNN modules and self-attention have been proposed. However, these approaches still involve inefficient matrix multiplication operations that impact latency on mobile devices. In this work, we propose efficient additive attention, a new approach that eliminates the need for expensive matrix multiplication operations in computing self-attention. We also compute the global context using only query-key interactions, reducing computational complexity. Our proposed attention design can be used at all stages of the network, allowing more effective contextual information capture and achieving a superior speed-accuracy trade-off. We introduce a series of efficient classification models called "SwiftFormer" that utilize our proposed efficient additive attention. Our models achieve state-of-the-art performance with a better trade-off between accuracy and latency compared to existing methods.