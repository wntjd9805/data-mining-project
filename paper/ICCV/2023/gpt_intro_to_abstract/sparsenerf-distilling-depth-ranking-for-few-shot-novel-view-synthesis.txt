Neural radiance fields (NeRFs) have demonstrated impressive capabilities in generating photo-realistic views of scenes. However, collecting dense views of a scene is often expensive and time-consuming, making it necessary to develop few-shot NeRF methods that can learn from sparse views. This paper introduces SparseNeRF, a method that distills depth priors from pre-trained depth models or inaccurate depth maps from consumer-level sensors. By relaxing hard depth constraints and supervising NeRFs with relative depth instead of absolute depth, SparseNeRF improves the performance of few-shot novel view synthesis. Additionally, the proposed spatial continuity constraint maintains the coherent geometry of the scene. Experimental results on various datasets showcase the effectiveness of SparseNeRF and establish it as a new state-of-the-art method in few-shot novel view synthesis.