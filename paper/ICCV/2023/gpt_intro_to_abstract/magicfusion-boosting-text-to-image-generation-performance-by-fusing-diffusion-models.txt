In recent years, there has been significant progress in image generation, thanks to breakthroughs in diffusion models and large-scale training. Pre-trained large models have become invaluable in this field, including text-guided diffusion models trained on various datasets. While these models have impressive capabilities, few explorations have been conducted on ensembling them to combine their strengths. This paper proposes a method called Saliency-aware Noise Blending (SNB) to empower fused text-guided diffusion models for more controllable generation. SNB spatially blends the predicted noises of two diffusion models, preserving the strengths of each individual model. By leveraging the responses of classifier-free guidance, which enhances the difference between given and null texts, SNB automatically aligns the semantics of two noise spaces without additional annotations. The proposed method is training-free and can be completed within a DDIM sampling process. Extensive experiments on different applications demonstrate the efficacy of SNB in empowering pre-trained diffusion models. The remaining sections of the paper discuss related work, describe the proposed method, present evaluations and comparisons, and offer a comprehensive summary and analysis.