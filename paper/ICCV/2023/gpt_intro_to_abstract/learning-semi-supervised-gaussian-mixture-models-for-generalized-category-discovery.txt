The success of deep learning in computer vision tasks is largely due to the availability of large-scale annotated data. However, collecting such data can be expensive and impractical for all object classes. Semi-supervised learning has been proposed as a solution to learn from both labeled and unlabeled data, but existing methods assume a known class number. This paper proposes a unified EM-like framework that combines representation learning and class number estimation. The framework alternates between estimating class prototypes using a variant of the Gaussian Mixture Model and learning better representations using prototypical contrastive learning. The proposed framework is evaluated on various benchmark datasets, achieving state-of-the-art results. The contributions of this paper include demonstrating the reinforcement between representation learning and class number estimation, proposing the EM-like framework, introducing a semi-supervised variant of the Gaussian Mixture Model, and comprehensive evaluation on benchmark datasets.