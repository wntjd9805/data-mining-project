Recent years have seen significant advancements in diffusion-based text-to-image generative models, allowing for the generation of high-quality images based on text prompts. These models have inspired researchers to explore ways to utilize them for image editing. However, there are still limitations in fine-tuning large-scale text-to-image diffusion models, such as overfitting and difficulty in learning multiple personalized concepts. To address these challenges, this paper proposes a compact and efficient parameter space called spectral shift, which focuses on fine-tuning the singular values of weight matrices. This approach helps combat overfitting and language-drifting issues. Additionally, a data-augmentation technique called Cut-Mix-Unmix is introduced to enhance the model's ability to learn multiple personalized concepts. The main contributions of this work include the presentation of the proposed spectral shift parameter space, a text-based single-image editing framework, and the Cut-Mix-Unmix method for data-augmentation. This research opens up opportunities for efficient and effective fine-tuning of large-scale text-to-image diffusion models, paving the way for further exploration in this field.