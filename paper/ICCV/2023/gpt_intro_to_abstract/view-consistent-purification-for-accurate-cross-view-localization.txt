Accurate self-localization is crucial for mobile robotics, specifically in autonomous driving. While GPS is commonly used, its accuracy falls short for autonomous driving. Real-Time Kinematic (RTK) positioning systems can correct GPS errors, but they require costly signal reference stations. Odometry and simultaneous localization and mapping (SLAM) methods can generate accurate short-term trajectories, but they suffer from drift accumulation. Localization techniques that rely on pre-constructed 3D maps face limitations in terms of time and resources. To overcome these limitations, this paper proposes a low-cost localization method using off-the-shelf satellite images as ready-to-use maps. However, the significant disparity between satellite and robot views poses challenges, requiring the purification of view-consistent features. Previous methods approached this as an image retrieval problem, leading to coarse localization accuracy. In contrast, the proposed method utilizes a coarse pose obtained from the autonomous vehicles system to estimate the fine-grained 3-DoF pose. The method incorporates spatially aware deep features, keypoint detection, and matching, as well as confidence maps, to improve localization accuracy and mitigate the impact of moving objects and viewpoint variation. The proposed method is evaluated on two datasets, demonstrating superior localization accuracy compared to previous methods. The contributions include the first sparse visual-only cross-view localization method, a view-consistent keypoint detector, a spatial embedding approach, and a multi-camera fusion approach to improve accuracy.