Learning complex control from high-dimensional observations, such as images, is crucial for various real-world applications. However, this poses challenges in terms of the representation capability of visual encoder models, particularly in complex scenes like self-driving and robot controlling. Training large-scale visual models has shown remarkable progress in computer vision but does not directly benefit reinforcement learning (RL) and can even lead to training deterioration. This paper investigates the phenomenon of oscillating self-overfitting, where larger models suffer from instability during RL training compared to lighter models. To address this issue, the authors propose a novel asymmetric interactive cooperation (AIC) approach for representation learning in RL. This approach separately trains a heavy-optimizable encoder and a light-optimizable encoder using auxiliary tasks and temporal-difference (TD) losses, respectively, and enables effective knowledge exchange between them. The heavy encoder is equipped with latent state abstraction capability without oscillation, while the light encoder transfers expected returns information to the main encoder. The paper also introduces topological distillation between latent state spaces for state abstraction learning without explicit labels. Additionally, a greedy bootstrapping optimization is presented to enhance training stability. Experimental results demonstrate significant performance gains in complex environments like CARLA and Vizdoom. The contributions of this paper include investigating oscillating self-overfitting and proposing the AIC approach, topological distillation, and greedy bootstrapping optimization for improved RL training with visual encoders.