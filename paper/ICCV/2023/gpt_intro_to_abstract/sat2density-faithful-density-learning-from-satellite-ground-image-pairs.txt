The emergence of satellite imagery has greatly enhanced our daily lives by providing a comprehensive view of the planet. However, the specific information provided by satellite imagery and its importance have not been fully explored. In this paper, we propose that the analysis of the geometry, topology, and geography of cross-view observations captured by paired satellite and ground-level images can provide critical insights. We aim to address the challenging problem of synthesizing ground-level images from satellite imagery by leveraging density representations of 3D scenes. The differences in viewpoint and limited visual overlap between satellite and ground-level images create a complex learning problem. Previous research has focused on using conditional generative adversarial networks, but they often result in unsatisfactory synthesis results. Recent studies have shown the importance of accurate 3D scene geometry in generating high-quality ground-view images. We aim to investigate whether more accurate 3D geometry can be achieved using a large collection of satellite-ground image pairs. Our study is motivated by the neural radiance field (NeRF), which has shown promising results in novel view synthesis. We propose a novel approach called Sat2Density, which uses convolutional encode-decoder networks to learn accurate density fields from paired satellite-ground image pairs. We also introduce two supervision signals, non-sky opacity supervision and illumination injection, to improve the learning of density fields. Our approach surpasses previous methods in ground-view panorama synthesis and provides a better understanding of the relationship between satellite and ground-view image data from a 3D geometric perspective.