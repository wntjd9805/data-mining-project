This paper introduces the concept of the Affective Image Filter (AIF), a task that aims to reflect visually-abstract emotions from user-provided text to images. The paper outlines the desired properties of a well-qualified AIF algorithm and proposes the use of a multi-modal transformer architecture to build the AIF model. The model utilizes a valence, arousal, and dominance (VAD) dictionary for understanding emotional words and learns aesthetic style representations from famous paintings. The paper also presents a new dataset with images and corresponding text descriptions for training and evaluating the AIF model. The contributions of the paper include the proposal of the AIF task, the development of the AIF model with transformer architecture, and the collection and processing of the AIF dataset.