Autonomous navigation in complex unknown 3D environments requires a suitable representation of the environment for high-level reasoning. Traditionally, these representations are built explicitly through reconstructions, but end-to-end training can learn them automatically. Spatial representations can emerge in unstructured agents, but spatial inductive biases can support learning actionable representations. This paper explores the use of implicit representations as inductive biases for visual navigation. Two representations, semantic and structural, are learned online during each episode. The paper also introduces a global read mechanism for efficient querying of the implicit representations. The proposed representations are evaluated in the context of Multi-Object Navigation, a challenging visual navigation task. The contributions of the paper include the proposed implicit representations, the global read procedure, performance gains over classical neural agents, and the evaluation and analysis of key design choices and capabilities.