Deep learning approaches for facial manipulation, such as deepfakes, have become increasingly realistic and natural, posing significant security and societal concerns. Current deepfake detection methods have mainly focused on high-quality deepfakes and have struggled to effectively detect low-quality deepfakes, which are often compressed. In this research, we propose QAD, a novel deepfake detection method that can simultaneously detect high and low-quality deepfakes in a single model. We introduce a universal intra-model collaborative learning framework that aligns the distributions of different quality image representations and mitigates the negative effects of compression. We also use the Hilbert-Schmidt Independence Criterion (HSIC) to maximize dependence between high and low-quality image representations, enhancing the model's robustness under heavy input compression. Experimental results demonstrate the effectiveness of QAD, outperforming previous baselines and state-of-the-art methods while requiring fewer computational parameters and no prior knowledge of the inputs. Our contributions include theoretical analysis of the low-quality deepfake classification error, the development of a unified quality-agnostic deepfake detection framework, and the demonstration of QAD's superior performance across multiple benchmark datasets.