Semi-supervised learning (SSL) has been proposed as a solution to utilize unlabeled data in computer vision tasks. However, relying on incorrect pseudo labels can lead to confirmation bias issues. Some recent works have addressed this by discarding unreliable samples using a fixed confidence threshold, but this results in low utilization of the unlabeled set. This paper aims to fully leverage previously uncertain samples in a safe and informative manner. The authors observe that uncertain samples often arise from a small portion of confusion classes. To enhance certainty, they propose shrinking the original class space by detecting and removing confusion classes for the top-1 class. They also introduce adaptive reweighting of the uncertain loss to account for the model state during training. Experimental results on benchmark datasets show that the proposed method, called ShrinkMatch, achieves state-of-the-art performance.