The introduction of the computer science paper discusses the challenges of training deep neural networks (DNNs) due to the difficulty in obtaining large-scale datasets with high-quality annotations. To overcome this challenge, the paper proposes a method called RankMatch that leverages confidence and consistency to improve the performance of DNNs trained with noisy labels. The paper introduces a sample selection strategy called sample selection via confidence voting (SCV) that uses confidence thresholds and cluster centers to identify clean samples. Additionally, the paper proposes a rank contrastive loss (RCL) that encourages consistency between similar hard samples of the same category. The paper demonstrates the effectiveness of RankMatch on various benchmark datasets and highlights its contributions in improving sample selection and feature consistency in learning with noisy labels.