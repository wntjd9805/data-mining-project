Equipping machine learning models with the ability to identify what they do not know is crucial for reliable performance. Existing neural network models perform well on data within the training distribution but struggle with data that has covariate shift or semantic shift. Out-of-distribution (OOD) detection has emerged as a promising approach to address this challenge, aiming to identify test examples that come from a different distribution than the training data. However, existing OOD detection methods primarily focus on detecting examples with semantic shift, limiting their applicability in uncontrolled environments where other types of OOD examples, such as those with covariate shift, may appear. In this paper, we propose a novel framework called Model-Specific Out-of-Distribution (MS-OOD) detection that expands the scope of OOD detection to include covariate shift. We define a ground-truth label for each test example based on whether the deployed classifier would misclassify it, allowing us to study different causes of OOD examples in a unified way. MS-OOD detection aims to detect and reject misclassified ID and covariate shift examples, as well as all semantic shift examples. We conduct an extensive empirical study of MS-OOD detection, considering different sources of OOD examples, deployed classifiers, and OOD detection methods. This study provides new insights into OOD detection and validates existing insights, offering a platform for the research community to unify and select appropriate OOD methods for real-world applications. Our contributions include the proposal of the MS-OOD detection framework and the uncovering of novel insights and unification of existing insights in OOD detection.