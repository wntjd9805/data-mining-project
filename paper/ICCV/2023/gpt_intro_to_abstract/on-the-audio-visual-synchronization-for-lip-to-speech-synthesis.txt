This paper introduces the concept of lip-to-speech (LTS), which involves reconstructing spoken audio from lip movements in a video without sound. Deep learning techniques have been employed to advance this field, but commonly used datasets for LTS training and evaluation contain varying time offsets between the audio and video pairs, known as data asynchrony. This can significantly affect the evaluation of LTS models, particularly when using alignment-sensitive metrics. To address this issue, a time-alignment frontend is proposed to ensure consistent scoring regardless of the offsets. Additionally, a synchronized lip-to-speech (SLTS) model is introduced, incorporating an automatic synchronization mechanism (ASM) to handle both data and model asynchrony. Experimental results demonstrate the effectiveness of the SLTS model on datasets with different levels of asynchrony. Overall, this paper makes contributions to LTS synthesis by addressing asynchrony issues and achieving high-quality and synchronized audio reconstruction.