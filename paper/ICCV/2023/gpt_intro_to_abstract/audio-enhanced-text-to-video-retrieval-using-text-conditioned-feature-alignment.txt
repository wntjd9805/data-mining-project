This paper introduces a text-conditioned feature alignment approach for audio-enhanced text-to-video retrieval. The emergence of online streaming services has led to a vast collection of multimedia assets, making it crucial to retrieve semantically similar content. Text-to-video retrieval is commonly used to find videos that match a textual description by learning a joint latent space for text and video representations. The use of large-scale transformer models has improved the performance of these architectures, with CLIP-based models showing significant advancements. This paper builds upon this approach, incorporating post-processing techniques and proposing a novel audio-enhanced text-conditioned feature alignment method. Extensive experiments on various datasets demonstrate that the proposed method consistently outperforms existing methods in text-to-video retrieval, achieving state-of-the-art performance. The key contributions of this paper are the introduction of the text-conditioned feature alignment approach and the demonstration of its superiority over previous techniques in multiple benchmark datasets.