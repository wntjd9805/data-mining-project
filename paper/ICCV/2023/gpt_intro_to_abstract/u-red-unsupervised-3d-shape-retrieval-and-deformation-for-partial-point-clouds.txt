In the field of 3D semantic scene perception, significant progress has been made in decomposing scenes into constituent objects and reconstructing them to formulate a holistic scene representation. However, existing methods struggle to preserve fine-grained geometric structures. To address this issue, Retrieval and Deformation (R&D) methods have been proposed, which leverage a pre-prepared 3D shape database to generate a clean and compact scene representation. However, these methods face challenges in handling noise and partial observations. To overcome these challenges, we propose U-RED, an unsupervised joint 3D shape retrieval and deformation framework. U-RED is designed to effectively handle noisy, partial, and unseen object observations. We develop U-RED using large-scale synthetic data and simulate real-world occlusions, sensor noise, and scan errors. Our framework allows one-to-many retrieval by encapsulating possible full shapes of a target partial observation on the surface of a high-dimensional unit sphere. We also introduce a novel point-wise residual-guided metric for similarity measurements, which has robustness to noise and can be applied to real-world scenes. The performance of U-RED is evaluated on public synthetic and real datasets, demonstrating state-of-the-art results. We contribute a novel approach for joint 3D shape R&D, an OTM module for one-to-many retrieval, and a residual-guided retrieval technique for robustness to noisy observations.