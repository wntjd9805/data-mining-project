Minimally invasive medical procedures rely on small endoscopes, which typically have a single camera and limited depth perception. However, accurate depth estimation is crucial for functionalities such as tumor size estimation. Existing depth estimation methods face challenges such as lack of annotated data and difficulties in handling fluid presence, illumination changes, and surface deformations. In this paper, we propose a novel approach that addresses these challenges by leveraging a key property of endoscopic imagery. We introduce a deep network that estimates depths and albedos, infers normals, and renders images while considering the attenuation factor due to the distance between the light source and the surface. By minimizing the difference between the original and rendered images during training, we can achieve self-supervision without depth annotations. At inference, our method performs test-time refinement to further improve depth predictions. Our evaluation on a colon dataset demonstrates that our self-supervised approach achieves results comparable to supervised methods and outperforms multi-view self-supervision and synthetic-to-real transfer methods. We also show that our method continues to work effectively on real data, where there is no ground-truth for training. The key contributions of our work include incorporating illumination decline and photometric calibration in the rendering equation and developing a single-view self-supervised method using two-headed network architectures.