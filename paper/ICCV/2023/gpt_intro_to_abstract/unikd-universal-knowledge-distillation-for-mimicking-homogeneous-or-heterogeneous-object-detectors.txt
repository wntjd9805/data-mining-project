Object detection is a crucial task in computer vision with many practical applications. Various frameworks for object detection have been proposed, but different detectors favor different devices, making it important to maximize the accuracy of deployable models. Knowledge Distillation (KD) methods have been successful in boosting the performance of lightweight student models by learning from high-capacity teacher models. However, current detection KD methods are mostly feature-based and not general enough to handle heterogeneous teacher-student pairs. Additionally, the traditional feature-based distillation method is inefficient and costly in terms of computing resources and training time. In this paper, we propose a query-based distillation paradigm called Universal Knowledge Distillation (UniKD) to address these challenges. UniKD allows flexible knowledge transfer in any teacher-student pairs, whether homogeneous or heterogeneous. It avoids the need for case-by-case algorithm design and significantly reduces storage costs compared to feature-based methods. We introduce Adaptive Knowledge Extractor (AKE) modules with deformable cross-attention to extract detection-relevant knowledge from the teacher model. The AKE modules are pre-trained and then attached to the student model to imitate the teacher's output. Experimental results demonstrate the effectiveness and generalization ability of UniKD, achieving better or comparable results compared to existing methods. We also successfully transfer knowledge between traditional detectors and end-to-end Deformable DETR models, improving their performance. Overall, our contributions include the introduction of a new knowledge distillation paradigm, the AKE modules for knowledge extraction, and extensive experimental validation.