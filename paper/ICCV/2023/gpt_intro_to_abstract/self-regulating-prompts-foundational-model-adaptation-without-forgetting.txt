This paper introduces a self-regulating framework for prompt learning in Vision-Language (VL) models. VL models, such as CLIP and ALIGN, have shown strong generalization capabilities for various tasks. Prompt learning has emerged as an efficient alternative to fine-tuning large-scale models. However, prompt overfitting is a challenge, as prompts optimized for task-specific objectives may lead to the loss of generalization capabilities. To address this, the proposed framework guides prompts to optimize for both task-specific and task-agnostic representations. It achieves this through mutual agreement maximization, self-ensemble regularization, and textual diversity regulation. Experimental results demonstrate the effectiveness of the proposed framework in improving base-to-novel generalization, cross-dataset transfer, domain generalization, and few-shot image recognition. The main contributions of this work are addressing prompt overfitting through self-regularization, introducing a weighted self-ensembling strategy for prompts, and proposing text-side diversity to overcome the diversity mismatch between the text and visual domains.