Recovering 3D human mesh from images or videos is crucial but challenging for various applications in computer science, such as human-robot interaction, virtual reality, and motion analysis. Many studies have been conducted to recover 3D human mesh from a single image, using either RGB-based methods or pose-based methods. However, the limited representation ability of parametric models and the trade-off between per-frame accuracy and motion smoothness in video-based methods remain issues. In this paper, we propose a non-parametric approach called the Pose and Mesh Co-Evolution network (PMCE) to recover 3D human mesh from videos. We decouple the task into two parts: video-based 3D pose estimation and mesh vertices regression from 3D pose and image features. We introduce a two-stream encoder and a co-evolution decoder that performs pose and mesh interactions with image-guided Adaptive Layer Normalization (AdaLN). Our method outperforms previous video-based methods in terms of per-frame accuracy and temporal consistency on benchmark datasets. Our contributions include the development of the PMCE model, the design of the co-evolution decoder with AdaLN, and achieving state-of-the-art performance on challenging datasets.