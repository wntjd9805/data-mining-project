Multiple medical imaging modalities are used together to understand brain tumors and their surroundings. Deep learning approaches have been successful in multimodal brain tumor segmentation, but they often require full modality data for training. In practice, only a subset of modalities may be available. This paper focuses on the realistic setting of limited full modality data and proposes a modality-agnostic method using meta-learning. The method learns enriched shared representations that are not biased towards more frequent modalities, even with limited full modality data. It also introduces an adversarial learning strategy to further enhance the representations in the latent space. The proposed approach overcomes the reliance on full modality data and can achieve state-of-the-art performance in missing modality settings.