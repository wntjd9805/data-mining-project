Monocular 3D detection (M3D) is a widely studied topic in computer vision, with applications in autonomous driving and robotic navigation. The task involves establishing correspondences between 2D images and 3D space. Previous approaches have used geometrical priors, keypoint annotations, CAD models, or depth maps to extract 3D information. However, these representations have limitations in terms of uneven density distribution and reliance on accurate depth estimation. Inspired by Neural Radiance Fields (NeRF), we propose a new approach that reformulates intermediate 3D representations in M3D to produce dense and reasonable 3D geometry and occupancy. Our method combines 2D backbone features and normalized frustum 3D coordinates to create 3D Position-aware Frustum features, which are used to generate signed distance fields and radiance fields. Volume rendering is then applied to produce RGB images and depth maps. Unlike previous depth-based methods, our approach generates depth maps based on 3D representations. Our experiments on KITTI and Waymo Open Dataset demonstrate the effectiveness of our approach, which is competitive with state-of-the-art methods. We present MonoNeRD, a novel detection framework that connects Neural Radiance Fields and monocular 3D detection, and introduce volume rendering for 3D detection tasks.