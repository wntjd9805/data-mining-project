The goal of this paper is to develop a computer vision system that can recognize and interpret potential interactions in a 3D scene from a single RGB image. This ability is important for enabling computers to understand and plan interactions with objects, similar to how humans do. The current state of the art in scene understanding falls short in this area, as it primarily focuses on the current state of the scene rather than its potential. Similarly, existing works on articulation and learning interactions primarily focus on specific environments or carefully collected demonstrations, rather than generalizing to new environments. To address this challenge, the authors propose a transformer-based approach that formulates the problem as a prediction-at-a-query-location task. The system aims to answer the question "what can I do here?" by considering various aspects of the object, such as its movability, extent when moved, location in 3D, rigidity, motion constraints, and interaction strategies. By framing the problem in this way, the authors believe the system can be trained on sparse annotations and eventually leverage videos for supervision.To support their approach, the authors introduce a new dataset called the 3D Object Interaction dataset (3DOI), which contains annotated data from diverse sources, including internet and egocentric videos, as well as 3D renderings of scene layouts. The dataset consists of over 50,000 objects across 10,000 images, along with annotations of non-interactable objects. This dataset provides valuable 3D supervision in the form of depth and normals.The authors conduct experiments to evaluate the performance of their approach on both the 3DOI dataset and robotics data. They compare their method with alternative approaches, including generalizing from demonstration data and synthetic data, as well as different network designs. The results demonstrate that their transformer-based model outperforms these alternatives and exhibits strong generalization to the robotics dataset.In summary, the contributions of this paper include the introduction of the novel task of detecting 3D object interactions from a single RGB image, the creation of the 3DOI dataset, and the development of a transformer-based model that achieves high performance on the dataset and robotics data.