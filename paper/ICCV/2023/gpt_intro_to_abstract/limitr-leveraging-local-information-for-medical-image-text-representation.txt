This paper introduces a novel model for learning a joint representation of X-ray images and medical reports. The task of producing a joint representation of medical image-report data is challenging due to the small size and unbalanced nature of the available data. The proposed model utilizes both global and local alignments, as well as lateral images and domain-specific localization information, to capture subtle details and improve performance. The model is evaluated for three different retrieval tasks, including text-image retrieval, phrase-grounding, and class-based retrieval, and achieves state-of-the-art results. The contributions of this work include the proposed alignment scheme, the integration of lateral images and visual structure, and the demonstration of improved performance across various retrieval tasks.