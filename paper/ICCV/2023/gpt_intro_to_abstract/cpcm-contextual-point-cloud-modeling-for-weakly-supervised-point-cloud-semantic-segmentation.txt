The demand for autonomous driving and robotic navigation has led to the need for accurate 3D environment perception through point cloud semantic segmentation. However, fully annotating point cloud data is time-consuming and economically inefficient. Therefore, there has been a focus on learning from weakly-annotated labels, particularly the partial point-wise labeling scheme. This scheme involves labeling only a small portion of points in the point cloud data. However, directly applying supervised cross-entropy loss on the labeled part leads to overfitting. The primary challenge is learning from a significant proportion of unlabeled points to improve model generalization performance. Existing methods have tried to address this challenge by enforcing feature consistency between different augmented or geometrically calibrated point clouds. However, solely relying on feature consistency is insufficient for capturing the complex structures of point clouds. To tackle this issue, we propose Contextual Point Cloud Modeling (CPCM), which consists of region-wise masking and a contextual masked training method. Region-wise masking divides the geometric space into cuboids and masks all points within the selected cuboids. This approach provides a meaningful masked context prediction task and allows for controlling the difficulty of the task. Additionally, we introduce a contextual masked training method that incorporates a masked feature prediction branch into the consistency-based framework. This method enables learning from limited labeled data and helps the model effectively comprehend complex scene context. Our proposed CPCM achieves state-of-the-art performance on two widely-tested benchmarks, ScanNet V2 and S3DIS, demonstrating the superiority of the approach in weakly-supervised point cloud segmentation.