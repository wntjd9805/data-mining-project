Visual relationship detection (VRD) is a significant challenge in computer vision, as it requires understanding the relationships between localized objects connected by predicates. Existing VRD models have primarily focused on training from a single data source, limiting their generalization and scalability. This paper addresses the problem of unifying diverse VRD datasets with heterogeneous label spaces by leveraging recent advances in vision and language models (VLMs). By utilizing large language encoders and contrastive image-text co-training, VLMs can encode similar visual relationships close to each other in the embedding space. Therefore, the proposed UniVRD framework adopts a bottom-up approach, consisting of an object detector and a pair-wise relationship decoder. The VLMs are fine-tuned for object detection, while a lightweight Transformer decoder is used for decoding pair-wise relationships. The label space is defined and unified using natural language instead of categorical integers. The proposed framework demonstrates notable improvements in performance on both human-object interaction detection and scene graph generation tasks, achieving state-of-the-art results on the HICO-DET dataset. The contributions of this work include a novel VRD framework that unifies multiple datasets, effective model training enhancements, and a scalable and generalizable design. This paper provides a flexible starting point for future research in visual relationship understanding tasks.