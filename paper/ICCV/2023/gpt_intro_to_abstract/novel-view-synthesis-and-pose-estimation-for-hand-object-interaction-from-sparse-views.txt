Hand-object interaction is crucial in various applications, such as surgical operation and machinery training. While previous works focus on hand-object interaction detection, reasoning, and pose estimation, the synthesis of novel views of hand-object interactions remains largely unaddressed. Neural rendering has shown promise in generating high-quality images by learning from image collections. However, existing neural rendering approaches do not consider scene context in hand-object interactions. Additionally, the only hand neural rendering model, LISA, is not suitable for hand-object interaction due to inter-occlusions and the requirement for dense camera views.In this paper, we propose a novel-view synthesis and pose estimation system for hand-object interaction scenes with sparse camera views. We leverage scene-specific models built beforehand to improve vision tasks with sparse inputs. Our system consists of an offline stage where we train pose-driven neural rendering models of the hand and object using sparse-view images, and an online stage where we estimate hand and object poses using a differentiable rendering-based model fitting approach. By leveraging geometric constraints, we can accurately understand hand-object interactions and render novel views effectively.However, there are several challenges in achieving our goal. Building neural rendering systems from sparse camera views is difficult due to insufficient visual information and depth ambiguity caused by inter-occlusions. Few-shot neural rendering methods fail under sparse camera views. To address this, we establish a fitting process based on pre-built models to provide strong shape and appearance priors, enabling excellent novel view rendering from sparse views. Additionally, accurately estimating hand and object poses and capturing reasonable interactions is challenging due to occlusions. To tackle this, we propose a differentiable rendering-based model fitting process under geometric constraints that refines poses and enforces spatial context between hand and object, reducing penetration and encouraging tight hand-object regions to contact. We also introduce a stable contact loss to penalize excessive sliding of the hand-object contact area across frames.Our main contributions include presenting the first solution to neural rendering of hand-object interactions from sparse views, using a two-stage approach for accurate pose estimation and photo-realistic novel view synthesis. We leverage geometric constraints for rendering-based model fitting, even under sparse views and inter-occlusions. We introduce a new stable contact loss to enforce consistent hand-object contact regions in video sequences. Additionally, we create a hand-object interaction dataset for neural rendering tasks. Our experiments demonstrate the superior performance of our method in pose estimation and rendering quality compared to previous methods.