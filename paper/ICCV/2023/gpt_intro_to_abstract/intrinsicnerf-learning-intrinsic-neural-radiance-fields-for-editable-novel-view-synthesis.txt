Recently, neural rendering techniques have shown impressive performance in novel view synthesis, but struggle to perform intuitive editing tasks like scene recoloring and relighting. Inverse rendering has been proposed as a solution to enable such editing capabilities, but it introduces assumptions that limit its application to object-specific scenarios. To address this limitation, we introduce intrinsic neural radiance fields, which use intrinsic decomposition to provide interpretable intermediate representations for neural rendering. We propose IntrinsicNeRF, which regresses density, reflectance, shading, and a residual term for multi-view consistency. However, designing such a framework is challenging due to the gaps in optimization between traditional intrinsic decomposition and NeRF-based methods. To overcome this, we propose a distance-aware sampling method to establish local and global relationships between sampled points. Additionally, we present an adaptive reflectance iterative clustering method to handle inconsistencies in similar reflectance regions. To address the problem of clustering adjacent instances with similar reflectance, we introduce a semantic-aware reflectance sparsity constraint. Our experiments on different datasets demonstrate consistent intrinsic decomposition results and high-fidelity novel view synthesis. We also develop video editing software for online scene editing on real-world and synthetic data.