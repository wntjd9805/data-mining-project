Recent research has focused on generating 3D human motion sequences based on natural language descriptions, which has applications in fields such as special effects, games, and virtual reality. However, existing methods struggle to generate fine-grained descriptions of multiple simultaneous actions. In this paper, we propose a method that focuses on the spatial composition of motions, aiming to generate one motion sequence that depicts multiple actions. We introduce a benchmark dataset for evaluating spatial compositions and compare different models. To address the scarcity of compositional data, we propose a synthetic data generation scheme using pretrained language models. Our experiments on the dataset demonstrate the advantages of our synthetic training and the ability of the language model to assign part labels to actions. Our proposed model outperforms baseline methods and provides a new benchmark for spatial compositions in 3D human motions. Our code is publicly available for research purposes.