Simultaneous Localization And Mapping (SLAM) is widely used in various fields such as computer vision, augmented reality, and autonomous driving. SLAM aims to estimate the 3D locations of objects and the camera's poses based on a series of 2D images. Structure From Motion (SFM) is a similar process that involves multiple cameras. The iterative Bundle Adjustment (BA) process is commonly used to solve SLAM, but it can be computationally expensive. In this paper, we propose a method that focuses on reducing the number of iterations required for BA. We introduce a new approach that uses Reinforcement Learning (RL) to learn a dynamic weighting factor, λ, which determines the optimization scheme between two methods (Gradient Descend and Gauss-Newton). By treating the BA process as a game and applying RL, we show that our method can achieve a dynamic and efficient weighting of GD and GN, thereby reducing the number of iterations needed for convergence. Our approach has been validated on benchmark datasets and has shown a significant reduction in the number of iterations required for convergence. Additionally, our method can be trained efficiently on synthetic scenes and can be integrated with other BA acceleration methods. Overall, our work contributes a general approach to learning the optimal value of λ in BA and demonstrates its effectiveness in reducing the time and iterations required for SLAM.