Abstract:Federated Learning (FL) has gained popularity in collaborative machine learning for maintaining privacy in decentralized environments. However, conventional FL algorithms face challenges related to system and statistical heterogeneity. To address these challenges, personalized FL algorithms have emerged, allowing each client to have their own personalized model. Existing personalized FL methods transfer collaborative knowledge implicitly, limiting the direct engagement of clients with other clients' risks. In this paper, we propose Personalized Global Federated Learning (PGFed), a novel framework that enables explicit and adaptive aggregation of empirical risks to personalize the global objective of each client. By estimating the risks through approximation, PGFed avoids extensive communication overhead and privacy concerns. We introduce PGFedMo, a momentum upgrade to efficiently utilize other clients' risks. Experimental results demonstrate that PGFed and PGFedMo outperform existing personalized FL methods, achieving up to a 15.47% boost in accuracy across different FL settings. This work contributes to enhancing the adaptation ability and generalizability of personalized FL algorithms while minimizing communication costs.