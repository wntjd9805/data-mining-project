Optical flow estimation is a fundamental problem in computer vision that involves analyzing the motion of pixels between consecutive images. Traditional methods for optical flow estimation have high computational costs, making them unsuitable for real-time applications. Neural network-based techniques have emerged as a low-cost alternative, but they can be affected by motion blur and temporal aliasing. In this paper, we propose a new approach using event cameras, which log changes in light intensity at each pixel. Event cameras offer high temporal resolution and are less susceptible to motion blur, making them suitable for highly dynamic scenes. We propose two neural network models, LSTM-FlowNet and EfficientSpike-FlowNet, that utilize internal states to retain information over time. We train these models on event data to estimate temporally dense optical flow. Our results show that the EfficientSpike-FlowNet model consumes significantly less energy and predicts more frequent optical flow compared to the baseline model. We also address the challenge of training sequential models on an indefinite-length input and propose modifications to improve their performance. Overall, our contributions include proposing two neural network models for event-based optical flow estimation, developing a training method for continuous inference, and demonstrating the potential of efficient and frequent flow estimation using event cameras.