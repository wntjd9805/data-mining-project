This paper addresses the limitations of current vision-language models (VLMs) in understanding and representing the number of objects in images and text. The authors propose a novel method to enhance the quantitative understanding of VLMs by training them to be sensitive to object counts. They hypothesize that existing VLMs struggle with counting due to the rarity of accurate captions specifying object numbers, the presence of numbers that do not relate to counting, and the lack of explicit counting enforcement in training objectives. To address these issues, the authors curate a clean and diverse counting training set and formulate counting as a discriminative task during fine-tuning. They evaluate their method on two prominent VLMs and demonstrate improved accuracy in zero-shot count classification. Additionally, they introduce a counting benchmark and show the relevance of their model in text-to-image retrieval and text-to-image generation tasks. The main contributions of this work are a novel training framework for VLMs to tackle counting, a new counting benchmark, improved counting performance on widely-used VLMs, and enhanced performance in downstream tasks.