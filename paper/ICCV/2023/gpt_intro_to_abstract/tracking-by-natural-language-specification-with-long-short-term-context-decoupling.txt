This paper introduces the concept of Tracking by Natural Language Specification (TNL), which aims to localize a specific target referred to in a textual query within a given frame. TNL has gained attention in the computer science field due to its ability to localize targets without the need for manually-specified bounding boxes. The integration of natural language understanding into visual object tracking has several benefits, such as breaking limitations imposed by manual bounding boxes and providing additional scene information from textual queries.Most TNL methods follow a similar procedure of encoding visual and linguistic inputs, estimating object locations through multi-modal fusion, and merging frame representations with sentence embeddings. However, it has been observed that visual and linguistic content can be inconsistent, leading to conflicts in direct fusion. Visual models aim to distinguish specific targets from distractors, while linguistic models seek to maximize representation similarity for objects of the same category. This inconsistency hinders the accurate development of TNL frameworks.Existing methods for localizing targets often indirectly differ in their approach, relying on language-guided candidate matching or selection modules while ignoring dynamic surroundings in video flows. This requires additional modules to generate candidate sets, resulting in fragile performance due to reliance on well-designed candidates.To address these limitations, this paper proposes DecoupleTNL, a feature decoupling strategy and context modulation approach in the fusion module design. DecoupleTNL utilizes Transformers for intra- and inter-modality correspondence between vision and language. The proposed framework decouples contextual information into long-term context (textual query) and short-term context (video clip). The Short-term Context-Matching (SCM) branch guides the video network in capturing dynamic scene information, while the Long-term Context-Perceiving (LCP) branch enables the model to perceive future scenes based on the given video clip. Contrastive learning is employed to compare predicted and ground truth representations, and a Long Short-term Modulation (LSM) module is used for feature modulation.The contributions of this paper include the analysis of limitations in TNL trackers and the proposal of a long short-term context decoupling framework for tracking by NL description. It simultaneously models long-term and short-term context information for a more robust learning representation. Furthermore, a long short-term modulation module is introduced to inject context information into frame representations, enabling adaptive perception of dynamic and static surroundings. Extensive experiments on tracking benchmark datasets confirm the effectiveness of the proposed method for tracking by natural language specification.