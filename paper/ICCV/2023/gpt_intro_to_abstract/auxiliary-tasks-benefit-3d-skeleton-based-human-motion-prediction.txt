This paper focuses on 3D skeleton-based human motion prediction, which aims to forecast future human motions based on past observations. This task has numerous applications in fields such as human-machine interaction and autonomous driving. The main challenge in this problem is extracting spatial-temporal dependencies among observed motions in order to enhance the informative nature of the feature representations. Previous methods have proposed dedicated network structures to model these dependencies, but they often neglect either the spatial or temporal aspects. In this paper, the authors propose a new auxiliary learning framework that incorporates auxiliary tasks, including denoising and masking prediction, to capture these dependencies more comprehensively. The proposed framework jointly learns the primary motion prediction task along with the auxiliary tasks, forcing the network to effectively model spatial-temporal dependencies. The authors introduce two types of auxiliary tasks: denoising and masked feature prediction. These tasks aim to recover corrupted coordinates and predict masked joint positions, respectively. The authors propose an auxiliary-adapted transformer network structure that models coordinate-wise spatial-temporal dependencies and is adaptive to incomplete motion data. The proposed method, called AuxFormer, outperforms existing methods in terms of mean per joint position error (MPJPE) on large-scale datasets. The contributions of this work include the introduction of the auxiliary learning framework, the auxiliary-adapted transformer, and the experimental validation of the proposed method's superior performance and robustness.