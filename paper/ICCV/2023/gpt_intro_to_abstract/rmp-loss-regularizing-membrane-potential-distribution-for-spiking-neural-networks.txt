Recently, there has been a focus on making deep neural networks (DNNs) lightweight for energy-limited devices. Various approaches, such as network pruning, network quantization, knowledge transfer/distillation, neural architecture search, and spiking neural networks (SNNs) have been proposed. SNNs, which mimic the working mechanism of brain neurons, offer a unique way to reduce energy consumption. They convert computationally expensive multiplications into computationally convenient additions, making them energy-efficient when implemented on hardware. Specialized neuromorphic hardware based on an event-driven processing paradigm has been developed for efficient implementation of SNNs. Despite the advantages of SNNs, their performance is still not comparable to DNNs. This performance gap is mainly due to the quantization of the membrane potential to 0/1 spikes for SNN firing in implementation, which leads to information loss and decreased accuracy. Previous methods that convert ANN models to SNN models have addressed the quantization error problem by changing the activation function or increasing timesteps, but these approaches are not effective for SNN training. In this paper, we propose a method to directly reduce the quantization error in SNN training without burdening the SNN or decreasing biological plausibility. We suggest redistributing the membrane potential to regions closer to the 0/1 spike, and introduce a membrane potential regularization loss, called RMP-Loss, to encourage membrane potentials to gather around binary spike values during training. Our method not only mitigates information loss but also reduces parameters and computation burdens in the inference phase compared to other methods. Extensive experiments on both static and dynamic datasets demonstrate the superiority of our method over state-of-the-art SNN models.