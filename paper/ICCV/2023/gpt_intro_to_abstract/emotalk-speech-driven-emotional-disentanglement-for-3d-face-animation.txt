Dynamic and realistic speech-driven facial animation has gained increasing interest in various industries such as virtual reality, computer gaming, and film production. However, current methods primarily focus on improving the synchronization between lip movements and speech, overlooking the importance of emotional variation in facial expressions. This gap may lead to the uncanny valley effect, where the absence of emotions in 3D facial animations appears unnatural. To address this issue, we propose a novel speech-driven emotion-enhanced 3D facial animation method that utilizes an emotion disentangling encoder and an emotion-guided feature fusion decoder. The encoder extracts separate latent spaces for content and emotion, while the decoder uses a Transformer module to generate emotion-enhanced blendshape coefficients for facial expressions. We demonstrate through extensive experiments that our method surpasses existing state-of-the-art methods in terms of emotional expression. Additionally, we introduce a large-scale pseudo-3D emotional talking face dataset, enabling the training and evaluation of emotion-enhanced 3D facial animations. This dataset includes semantic meaningful blendshape labels and mesh vertices, allowing for the transfer of facial movements across different virtual characters. In summary, our work contributes an end-to-end neural network, an emotion disentangling encoder, and a comprehensive dataset for speech-driven emotion-enhanced 3D facial animation, providing a significant advancement in this field.