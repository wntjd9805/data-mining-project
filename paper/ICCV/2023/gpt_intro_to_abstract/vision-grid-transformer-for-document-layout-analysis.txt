This paper introduces VGT, a two-stream multi-modal Vision Grid Transformer for document layout analysis. The goal of Document Layout Analysis (DLA) is to transform documents into structured representations, and it plays a crucial role in downstream tasks such as document retrieval and table extraction. However, DLA faces challenges in real-world scenarios, including diverse document types, complex layouts, low-quality images, and semantic understanding. The existing methods for DLA primarily rely on visual features or grid-based representations, but they may have limited semantic understanding capabilities. To address these limitations, the authors propose VGT, which combines visual and textual features for better document layout analysis. VGT includes a Grid Transformer (GiT) that encodes grid-based representations with two new pre-training objectives: Masked Grid Language Modeling (MGLM) for token-level semantics and Segment Language Modeling (SLM) for segment-level semantics. The authors also introduce the D4LA dataset, which is diverse and detailed, to advance DLA research for real-world applications. Experimental results on existing benchmarks (PubLayNet and DocBank) and the D4LA dataset demonstrate that VGT outperforms previous state-of-the-art models. Overall, this work contributes novel approaches in multi-modal DLA and provides a comprehensive dataset for future research.