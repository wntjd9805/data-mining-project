Deep neural networks (DNNs) have achieved remarkable success in fully-supervised learning tasks. However, labeled data is often scarce and expensive to obtain in real-world applications. Semi-supervised learning (SSL) provides a solution by leveraging unlabeled data to improve DNN performance without the need for extensive human annotation. Traditional SSL approaches assume that labeled and unlabeled instances share the same class distribution, but in real scenarios, unlabeled data often contains categories unseen in labeled data, resulting in a class distribution mismatch. SSL under class distribution mismatch has been addressed by various approaches, such as pseudo-labeling and consistency regularization. However, these approaches still struggle to filter out instances with unknown categories while preserving a sufficient number of unlabeled instances with target categories. Additionally, existing SSL approaches heavily rely on the performance of the target classifier, which can be biased by instances with unknown categories and compromise its effectiveness. In this study, we propose a novel SSL approach called weight-aware distillation (WAD), which decouples the SSL error under class distribution mismatch into pseudo-labeling error and invasion error. WAD captures pseudo labels and weights from the representation space, allowing the target classifier to selectively utilize instances from target categories while filtering out instances with unknown categories. We theoretically analyze the population risk of WAD and show that it is tightly bounded. Experimental results demonstrate that WAD outperforms state-of-the-art SSL approaches on multiple datasets.