Face recognition (FR) is commonly used in identity authentication but is vulnerable to face spoofing attacks. To address this issue, Face Anti-Spoofing (FAS) techniques have been utilized to detect and reject spoofing faces. However, existing FAS methods based on deep learning and neural networks suffer from degraded performance when there are domain shifts between training and testing data domains. To overcome this challenge, domain generalization and adaptation techniques have been explored for FAS. Domain generalization aims to develop a generalized FAS model with training data from multiple source domains, but it is still not satisfactory in unseen domains. On the other hand, domain adaptation uses target domain data for model adaptation, but target data collection is expensive and impractical at a static point in time due to constantly changing factors. In this paper, we propose a rehearsal-free Domain Continual Learning (DCL) approach to tackle the FAS problem. Unlike previous work, our approach allows the FAS model to continually evolve with data from constantly varying domains, without the need to store and access previous data. We introduce the Efficient Parameter Transfer Learning (EPTL) paradigm and utilize Adapters for Vision Transformer (ViT) models in the DCL-FAS setting. To extract fine-grained features, we propose a Dynamic Central Difference Convolutional Adapter (DCDCA), which adapts ViT models with adaptive central difference information. To improve generalization, we optimize DCDCA with contrastive regularization and alleviate forgetting using Proxy Prototype Contrastive Regularization (PPCR). Our experiments demonstrate that the proposed approach significantly improves the generalization and anti-forgetting capabilities of FAS models in low-shot and rehearsal-free DCL scenarios.