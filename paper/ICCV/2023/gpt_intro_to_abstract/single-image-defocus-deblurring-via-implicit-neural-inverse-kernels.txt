This paper introduces a deep neural network (NN) approach for single image defocus blur removal, known as Single Image Defocus Deblurring (SIDD). The proposed method explicitly predicts the spatially-varying inverse kernels of defocus Point Spread Functions (PSFs) and performs image deblurring using these predicted inverse kernels. To address the issue of overfitting, the method provides a more accurate model with structural constraints for the inverse kernels. The inverse kernels are modeled using a linear representation under a multi-scale dictionary. The dictionary atoms are constructed to have the same shape but vary mainly in size, motivated by the observation that defocus PSFs within an image tend to have the same shape but vary in size. To overcome the limitation of plain upsampling, the method leverages implicit neural representation (INR) to parameterize multi-scale atoms with both sufficient coverage of high-frequency content and implicit regularization. The method separates the multi-scale representation coefficients into scale-related and shape-related components, which are predicted by two scale-recurrent sub-NNs. A duplex scale-recurrent framework is introduced to perform both fine-to-coarse and coarse-to-fine estimations of the coefficients. Once the coefficients are predicted, the corresponding inverse kernels are applied to the input image for deblurring. The proposed end-to-end NN for SIDD shows improved performance compared to existing methods while maintaining low complexity. The main contributions of this work include the parametric inverse kernel prediction framework, the multi-scale linear representation model, and the duplex scale-recurrent framework for predicting the representation coefficients of inverse kernels.