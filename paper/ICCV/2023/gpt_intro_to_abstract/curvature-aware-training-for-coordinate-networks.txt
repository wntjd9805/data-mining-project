Coordinate networks have shown significant success in signal reconstruction tasks, but they struggle to capture high-frequency details. Positional embedding layers have been used to overcome this limitation, but they can produce noisy gradients. An alternative approach is to use non-traditional activation functions like sine or Gaussian activations, which have well-behaved gradients. However, coordinate networks are typically trained using first-order optimizers, leading to slow training times. In this paper, we propose the use of second-order optimizers like L-BFGS and demonstrate that coordinate networks activated by sine or Gaussian functions can be trained efficiently using this optimizer due to their favorable gradient and curvature conditioning. We compare the convergence rate of sine- and ReLU-coordinate networks trained with L-BFGS and Adam optimizers, showing that the sine-network trained with L-BFGS converges significantly faster. We also address the computational complexity of second-order optimizers by introducing a patch-based decomposition strategy, which accelerates training time. Our contributions include theoretical analysis, empirical validation, and experiments on various reconstruction tasks. The results demonstrate the superiority of sine-/Gaussian-activated coordinate networks trained with L-BFGS over Adam in terms of speed and efficiency.