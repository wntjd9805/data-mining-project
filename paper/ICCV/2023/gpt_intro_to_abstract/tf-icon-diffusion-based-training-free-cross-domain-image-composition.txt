Image composition task involves incorporating unique objects from different photos to create a harmonious image within a specific visual context. This task is challenging as it requires maintaining illumination consistency and preserving identifying features. While large-scale text-to-image models have achieved success in text-driven image generation, conveying precise and nuanced visual details through natural language prompts presents challenges. Diffusion models have shown potential in image-guided composition by incorporating guiding images, but retraining these models on tailored datasets damages their prior and limits their compositional abilities. In this paper, we propose the Training-Free Image COmpositioN (TF-ICON) framework, which allows attention-based text-to-image diffusion models to perform image-guided composition without additional training or fine-tuning. Our framework leverages rich semantic knowledge and enables composition across diverse domains. We introduce an exceptional prompt to accurately invert real images into latent codes, which serve as the starting noise for the diffusion process. By gradually injecting composite self-attention maps, we incorporate contextual information from the background into the incorporated objects, resulting in harmonious compositions. Our contributions include demonstrating the superiority of high-order diffusion ODE solvers for real image inversion, presenting an exceptional prompt for accurate invertibility, proposing a training-free framework for cross-domain image-guided composition, and showcasing the superior performance of our framework compared to prior baselines.