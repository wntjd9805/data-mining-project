This paper introduces the task of Zero-Shot Composed Image Retrieval (ZS-CIR), which aims to combine a reference image and a relative caption without the need for supervised learning. The proposed approach, SEARLE, utilizes a textual inversion network to map images into pseudo-words and leverages the CLIP vision-language model for text-to-image retrieval. To support research on ZS-CIR, the paper also presents a new benchmarking dataset called Composed Image Retrieval on Common Objects in context (CIRCO), which includes multiple annotated ground truths. Experimental results demonstrate that SEARLE outperforms baselines and competing methods on three different datasets. The contributions of this paper include the introduction of ZS-CIR as a new task, the proposal of the SEARLE approach, the creation of the CIRCO dataset, and the achievement of state-of-the-art performance in image retrieval.