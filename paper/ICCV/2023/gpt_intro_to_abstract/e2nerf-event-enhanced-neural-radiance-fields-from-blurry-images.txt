The paper introduces a framework called E2NeRF, which aims to reconstruct a sharp Neural Radiance Field (NeRF) using both blurry images and corresponding event data. The authors address the limitations of NeRF with regards to blurry input images and propose two novel losses to enhance volumetric representation learning. They introduce a blur rendering loss to compare predicted blurry images with input images, and an event rendering loss to refine neural 3D representation learning by simulating event data. The authors also design a camera pose estimation framework to guide the estimation of pose sequences for severe blurry images. Experimental results demonstrate the effectiveness of E2NeRF on both simulated and real-world datasets, outperforming existing methods. This work contributes to the enhancement and robustness of NeRF reconstruction and provides a benchmark for future research on NeRF reconstruction from blurry images and event stream.