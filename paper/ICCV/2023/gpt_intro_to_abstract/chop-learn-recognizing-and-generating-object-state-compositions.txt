The paper discusses the limitation of current data-driven recognition models in generalizing to unseen properties of objects. It highlights the importance of compositional generalization, the ability to recognize and generate new combinations of objects and their states, which is often absent in modern deep learning models. The paper proposes a new dataset called Chop & Learn (ChopNLearn) to support the study of compositional generalization. The dataset consists of video clips and object state combinations captured from multiple viewpoints for 20 objects and 8 cut styles. The paper introduces two new compositional tasks and benchmarks - Compositional Image Generation and Compositional Action Recognition. The objective of the Compositional Image Generation task is to generate images of unseen combinations of objects and states, while the Compositional Action Recognition task aims to recognize unseen object-state transitions. The paper also discusses the challenges and limitations of existing methods and proposes new methods for both tasks. The contributions of the paper include the ChopNLearn dataset, the task of Compositional Image Generation, and the benchmark for Compositional Action Recognition.