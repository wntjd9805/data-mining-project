Feature matching is a fundamental problem in computer vision, but it becomes challenging when dealing with illumination changes, viewpoint changes, motion blur, and occlusion. The classic image matching pipeline consists of feature detection, description, matching, and outlier filtering. Current dominant approaches employ learning-based descriptors with attention-based models. However, these methods struggle with extreme cases and lack scene-aware information. In this work, we propose a novel feature matching model called SAM that introduces group tokens to construct scene-aware features. We use Transformer and token grouping to model the relationship between image tokens and group tokens. A multi-level score strategy is proposed to guide point-level features based on scene-aware grouping information. SAM achieves state-of-the-art performance while demonstrating robustness and interpretability.