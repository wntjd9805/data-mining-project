In this paper, we focus on the problem of compressing neural networks deployed on edge devices due to their limited memory storage. Quantization is a technique that has been developed to effectively compress networks to lower bits without significant performance loss. However, existing quantization approaches suffer from the forgetting catastrophe problem when dealing with streaming data, where old task data is gradually forgotten. To address this issue, we propose a novel lifelong quantization process called LifeQuant. LifeQuant aims to minimize the forgetting rate by minimizing the shift of the search space during quantization and reweighting the influence of old task data in new tasks. We introduce Proximal Quantization Space Search (ProxQ) to minimize the space shift and propose Balanced Lifelong Learning (BaLL) loss to reweight the influence of replay data. Experimental results show that LifeQuant outperforms state-of-the-art quantization approaches in terms of accuracy enhancement and forgetting rate reduction. Our contributions include the development of LifeQuant, theoretical analysis of the forgetting problem, the introduction of ProxQ to minimize space shift, and the design of BaLL loss to reweight replay data in new tasks.