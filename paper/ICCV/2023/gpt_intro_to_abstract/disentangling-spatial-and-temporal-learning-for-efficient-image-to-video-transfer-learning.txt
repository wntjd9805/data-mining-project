Video understanding is a challenging research topic in computer vision. Previous approaches have achieved impressive progress by designing different architectures, such as two-stream models, 3D networks, and Transformers. Recently, there has been increased attention on transferring large-scale language-image pre-training models to video understanding tasks. However, current transfer learning methods for video understanding suffer from inefficiency in training and scalability issues due to the need for back-propagation through frozen parameters.To address these challenges, we propose DiST, a dual-encoder framework for efficiently transferring pre-trained image-text models to video-text models. DiST disentangles spatial and temporal modeling by avoiding back-propagation through pre-trained parameters and introducing a separate encoder for temporal information extraction. To fuse spatial and temporal features, an integration branch is employed. Evaluations on multiple video recognition benchmarks demonstrate that DiST achieves state-of-the-art performance and enables pre-training on large-scale video datasets with limited resources. This highlights the superior scalability and performance of DiST compared to existing approaches.