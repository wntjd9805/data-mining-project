Abstract:In recent years, Vision-Language Pre-training models (VLMs), such as CLIP and ALIGN, have gained popularity for their ability to perform downstream tasks using prior knowledge from large language models (LLMs). These models align visual features with textual descriptions by using contrastive learning. While previous work has focused on single prompt templates for VLMs, the use of multiple prompts remains unexplored. This paper introduces Energy-based Multi-Prompt Learning (EMPL) as a methodology for achieving a balance between in-domain and open-vocabulary generalization in VLMs. The paper provides empirical and theoretical insights on multi-prompt learning, addressing issues related to cross-modal transferability and model generalization. The results of comprehensive experiments demonstrate the effectiveness of EMPL in improving the performance of VLMs.