In this paper, we introduce a new approach for rendering novel normal-light images using Neural Radiance Field (NeRF) models. NeRF is a powerful method for learning scene representations as implicit functions and generating novel view images. However, NeRF models typically require high-quality input images with high visibility and accurate representation of scene illumination and object colors. In low-light conditions, the quality of the images is compromised, resulting in low visibility, amplified noise, and distorted colors.Previous methods have proposed techniques to train NeRF models from degraded inputs, such as blurry or low dynamic range images. However, these methods do not work effectively with sRGB images taken in low-light scenes. To address this limitation, we propose a two-step approach that involves enhancing the low-light input images before training the NeRF model. Existing low-light enhancement models do not maintain consistency across multi-view images and rely on specific mappings that may not generalize well to different scenes.Our proposed method decouples the colors of 3D points into view-dependent and view-independent components within the NeRF optimization process. By manipulating the view-independent component related to lighting, we can enhance brightness, correct colors, reduce noise, and preserve the texture and structure of the scene. We conduct experiments to demonstrate the effectiveness of our approach, showing that it outperforms state-of-the-art NeRF models and baseline methods that combine NeRF with existing enhancement techniques.In summary, our contributions include the decomposition of NeRF into view-dependent and view-independent color components for enhancement, the formulation of an unsupervised method for enhancing lighting and correcting colors, and the collection of a real-world dataset along with extensive experiments to evaluate our method's effectiveness in real-world scenes.