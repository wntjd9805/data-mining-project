Humans have the ability to perceive the direction of sound in a scene and associate it with visual signals to understand events. Sound source localization in visual scenes has been extensively studied, with the assumption that audio and visual signals are temporally correlated. However, previous methods rely on partial supervision and strong initial representations, and may not accurately capture true sound source localization performance. This paper introduces a cross-modal retrieval task as an evaluation criterion to measure the interaction between audio and visual modalities. The authors propose a method that incorporates multiple positive samples into cross-modal contrastive learning to enhance feature alignment and achieve high localization performance and strong cross-modal semantic understanding. The proposed method is evaluated on sound source localization and cross-modal retrieval benchmarks and outperforms state-of-the-art approaches in both tasks. The contributions of this work include an analysis of the limitations of existing sound source localization benchmarks, the proposal of a semantic alignment approach, and the incorporation of multi-views and conceptually similar samples to improve performance.