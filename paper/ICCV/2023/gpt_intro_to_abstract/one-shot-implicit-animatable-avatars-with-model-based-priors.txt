Creating realistic 3D contents of animatable human avatars is crucial for AR/VR applications. However, current methods require high-quality and dense inputs that are often difficult for ordinary users to obtain. To address this, we propose ELICIT, a novel method that learns human-specific neural radiance fields from a single image. We leverage body shape geometry and visual clothing semantic prior to guide the optimization and achieve free-view rendering. Our contributions include the development of two effective model-based priors and a novel sampling strategy for more realistic body part details. We evaluate ELICIT against existing methods and demonstrate its superior performance in free-view rendering and avatar animation on in-the-wild images. ELICIT is the first work that combines four crucial characteristics: single monocular image input, no need for extra training data, recovery of invisible body areas, and being animatable.