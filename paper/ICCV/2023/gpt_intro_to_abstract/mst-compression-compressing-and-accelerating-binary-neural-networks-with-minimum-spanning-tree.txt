Deep Neural Networks (DNNs) have been successfully applied in many artificial intelligence applications, particularly in vision tasks. However, the computational cost and storage requirements of DNNs pose significant challenges for deployment on embedded systems such as mobile devices. Various approaches have been proposed to reduce the energy and resource consumption of DNNs while maintaining their accuracy, including pruning, quantization, distillation, and efficient hardware implementation. Binarized Neural Networks (BNNs) are a type of quantization method where weight values and activations are converted to 1-bit values, leading to significant improvements in compression ratio and computational cost. However, the trade-off for minimizing the bit-width is accuracy degradation, necessitating the development of methods to reduce this gap. Previous methods for compressing BNNs have achieved promising results, but there is still room for improvement. This paper introduces a new method that leverages Minimum Spanning Trees (MSTs) to reorder binary convolution output calculations, significantly reducing the number of XNOR operations. Additionally, a new learning algorithm is proposed to minimize the MST distance of all convolution layers during the training stage. The paper also presents a hardware accelerator for BNNs that applies the MST compression, demonstrating the feasibility and effectiveness of the method in terms of hardware resources. Experimental results on various BNN architectures show that the proposed approach achieves a higher compression ratio compared to previous methods. In terms of hardware acceleration, the proposed method outperforms existing approaches in terms of LUT savings, area efficiency, speed, and accuracy.