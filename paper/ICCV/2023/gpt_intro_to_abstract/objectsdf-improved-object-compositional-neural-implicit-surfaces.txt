Recently, there has been a revival of multi-view 3D reconstruction using the neural implicit surface representation. This approach leverages neural networks to represent 3D scenes as signed distance functions (SDFs), providing a high modeling capability and resolution-free representation. The wide applications of 3D reconstruction, such as in virtual and augmented reality and digital twins, have attracted the attention of the research community.The success of the neural implicit surface representation can be attributed to the Neural Radiance Field (NeRF) model, which links the neural implicit 3D radiance field with 2D images through volume rendering. However, NeRF is primarily designed for view synthesis, limiting its performance in 3D reconstruction. To address this limitation, the neural implicit surface representation and reconstruction predict SDF values instead of radiance field densities, proving their ability in 3D reconstruction.Existing neural implicit surface reconstruction methods mostly focus on representing entire scenes or single objects, neglecting the individual objects within a scene. Few attempts have been made to address this issue by introducing object-compositional representations with additional guidance from 2D instance masks. However, these approaches have limitations, such as the introduction of mapping noise, limited supervision for occluded object SDFs, and the absence of constraints to prevent collisions and overlaps. Additionally, the convergence speed of these approaches is slow due to their heavy MLP structure.To overcome these limitations, this paper proposes a new framework called ObjectSDF++. It builds upon the ObjectSDF model but introduces an occlusion-aware object opacity rendering formulation. This formulation directly volume-renders object opacity to predict individual object opacity masks, providing a stronger constraint for reconstruction quality. Moreover, a physical-based regularization is designed to penalize object intrusion into other objects, ensuring mutually exclusive object SDFs. Monocular geometry cues from a pre-trained model are also enforced to improve the reconstruction quality and convergence speed.The main contributions of this paper include the proposal of an occlusion-aware object opacity rendering scheme, the introduction of an object distinction regularization term, and the evaluation of the method on various datasets. The effectiveness of the proposed components is demonstrated for both object-level and scene-scale surface reconstruction.