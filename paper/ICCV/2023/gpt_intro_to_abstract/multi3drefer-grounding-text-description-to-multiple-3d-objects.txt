Abstract:This paper introduces Multi3DRefer, a dataset and task that addresses the limitations of existing datasets for visual grounding in 3D scenes. While previous datasets assume a unique target object for visual grounding, Multi3DRefer allows for a flexible number of target objects, including zero, single, or multiple objects. The authors modify and enhance the language data from ScanRefer and propose evaluation metrics to benchmark prior work. They also propose a CLIP-based method for the flexible number visual grounding task. The contributions of this paper include generalizing 3D visual grounding to a flexible number of target objects, creating an enhanced dataset with augmented descriptions, benchmarking three prior visual grounding approaches, and designing an end-to-end approach leveraging CLIP embeddings and online rendering for object proposals with contrastive learning.