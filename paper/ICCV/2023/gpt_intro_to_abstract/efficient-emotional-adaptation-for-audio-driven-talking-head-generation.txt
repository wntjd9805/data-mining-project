Recently, there has been increasing attention on synthesizing realistic talking heads for various applications in industry. While audio-driven talking head generation aims to produce realistic videos synchronized with speech, generating emotional talking heads is important to improve their fidelity. Previous one-shot methods focus on achieving audio-visual synchronization for emotion-agnostic talking heads, while recent works have paid attention to generating emotion-aware talking heads. However, these methods lack in-depth thinking in terms of architecture efficiency and guidance flexibility. Therefore, we propose an efficient Emotional Adaptation framework for audio-driven Talking-head (EAT) generation. Our approach involves two stages: enhancing the 3D latent keypoints representation to capture emotional expressions and introducing the Audio-to-Expression Transformer (A2ET) for mapping audio to enhanced keypoints. We also propose learnable guidance and adaptation modules for steering emotional expression generation. Our approach enables rapid transfer of traditional talking-head models to emotional generation tasks with high-quality results and supports zero-shot expression editing with image-text models. Experimental results demonstrate the effectiveness of EAT in emotional talking-head generation, achieving superior performance without guiding emotional videos and state-of-the-art results with only 25% training data. Our study also introduces flexible guidance for talking-head adaptation, allowing for zero-shot expression editing and surpassing previous methods. Overall, our work contributes a new paradigm for emotional talking-head generation and presents innovative architectural and guidance approaches for improved results.