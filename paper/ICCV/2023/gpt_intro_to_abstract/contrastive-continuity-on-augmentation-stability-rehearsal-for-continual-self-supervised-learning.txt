Recently, self-supervised learning (SSL) has gained attention in the computer science community for its potential to learn beneficial representations without manual annotations. However, in real-world scenarios where data is presented progressively over time, it is often infeasible for SSL to collect the entire data streams for training due to the increasing cost and privacy concerns. This has led to the emergence of Continual Self-Supervised Learning (CSSL) approaches that aim to develop continuity in learning to address real-world challenges. CSSL, like Continual Learning (CL), faces the problem of catastrophic forgetting, where the model tends to forget previously learned information. Several methods have been proposed to alleviate this issue in CSSL, such as rehearsal-based and regularization-based methods. However, these methods have their limitations, such as overfitting on rehearsal samples or hindering the model's ability to learn fresh knowledge. To address these challenges, this paper proposes an Augmentation Stability Rehearsal (ASR) approach that selects the most representative and discriminative samples for the rehearsal buffer based on augmentation stability. In addition, a matching strategy is developed for dynamic update of the rehearsal buffer. Furthermore, Contrastive Continuity on Augmentation Stability Rehearsal (C2ASR) is introduced to balance the prevention of catastrophic forgetting and the ability to continuously learn by encouraging consistent feature distribution on rehearsal samples and inconsistent feature distribution on current task samples. The effectiveness of the proposed method is validated on popular CSSL benchmarks, demonstrating superior performance compared to existing methods.