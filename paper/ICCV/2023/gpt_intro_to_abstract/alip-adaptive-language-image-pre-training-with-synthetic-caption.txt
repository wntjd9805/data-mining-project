This paper addresses the challenge of noisy and poorly-aligned image-text pairs in large-scale multi-modal models. The authors propose the Adaptive Language-Image Pre-training (ALIP) method, which integrates raw text supervision and synthetic caption supervision. ALIP utilizes the Language Consistency Gate (LCG) and the Description Consistency Gate (DCG) to dynamically adjust the weights of samples and image-text/caption pairs during training, reducing the impact of noise data. The adaptive contrastive loss influenced by these weights improves pre-training data efficiency. Experimental results demonstrate that ALIP achieves state-of-the-art performance on various downstream tasks. The contributions of this paper include the proposed bi-path model, the design of adaptive contrastive loss, and the successful application of ALIP in multiple tasks.