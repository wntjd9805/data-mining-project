In autonomous driving, the transformation of features from multiple input sensors into a unified egocentric coordinate system is a critical step. This is particularly important in the vision-only setup, where multi-view RGB images are used as input and object detection and segmentation are performed in a unified Bird's Eye View (BEV) space. Early methods for this transformation suffer from a lack of accurate depth information, leading to generalization problems and noise in the BEV space. Additionally, the lack of visibility estimation can result in hallucinations in downstream planning tasks, which can be dangerous in high-speed driving scenarios. To address these limitations, we propose a novel feature transformation pipeline that adopts explicit parametric depth representation and geometric derivations as guidance. Our approach includes a geometry-aware feature lifting module and an occupancy-aware feature aggregation module. We also introduce a visibility map in the BEV space, which helps decouple visible and occluded areas in the estimations and mitigate hallucination effects. Furthermore, we propose a visibility-aware evaluation metric for BEV segmentation that considers visibility information. Our method achieves state-of-the-art results in object detection and semantic segmentation tasks with minimal computational overhead.