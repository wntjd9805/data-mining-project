Understanding videos has become crucial as the number of videos being produced continues to rise. Traditional methods for action recognition using trimmed video clips have achieved significant advancements, but their high cost and impracticality for real-world videos have led researchers towards temporal action detection (TAD). TAD not only classifies actions but also predicts the time boundaries of untrimmed videos. Previous approaches have relied on fixed-length windows called action proposals or point-wise learning to address the challenges of TAD. However, these methods often struggle with low precision and require post-processing techniques such as non-maximum suppression (NMS) for ranking. With the success of DETR in object detection, recent studies have introduced DETR-based methods for TAD. However, it has been observed that the dense attention mechanism in DETR does not perform well in TAD. In this paper, we propose a new framework called Self-DETR to address the issue of dense attention and mitigate the problem of temporal collapse in self-attention. Our framework utilizes cross-attention maps between the encoder and decoder to provide feedback and prevent the collapse. Extensive experiments on public benchmarks demonstrate the effectiveness of Self-DETR in preventing temporal collapse and achieving state-of-the-art performance in THUMOS14 and ActivityNet-v1.3. Our contributions include the introduction of Self-DETR and its ability to retain high attention diversity, as well as its superior performance compared to other DETR-based methods in TAD.