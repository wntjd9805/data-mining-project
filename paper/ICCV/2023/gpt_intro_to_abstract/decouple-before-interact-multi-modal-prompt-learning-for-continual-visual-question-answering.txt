Visual Question Answering (VQA) aims to train machine learning models to accurately answer questions based on visual images. In real-world dynamic environments, the ideal VQA model should be able to generate answers for new questions, new images, and new question-image pairs without forgetting previously learned information. Existing approaches for Continual Learning Visual Question Answering (CL-VQA) formulate the problem as a vision-only or language-only continual learning task, which overlooks the multi-modal nature of CL-VQA and the rich interactions between modalities. In this paper, we propose a comprehensive formulation for CL-VQA that considers both multi-modal and uni-modal perspectives. We design three scenarios based on different input distributions, namely the Continual Vision Scenario, Continual Language Scenario, and Continual Vision-Language Scenario. To address these challenges, we introduce the MulTi-Modal PRompt LearnIng with DecouPLing bEfore InTeraction (TRIPLET) model, which combines pre-trained vision-language models with decoupled prompts and prompt interaction strategies. We evaluate our approach on two CL-VQA benchmarks and demonstrate superior performance compared to state-of-the-art baselines. Our contributions include the formulation of CL-VQA with a multi-modal continual learning setting, the development of the TRIPLET model, and the creation of two CL-VQA benchmarks for empirical evaluations. Our results highlight the effectiveness of TRIPLET and its ability to improve performance in CL-VQA tasks.