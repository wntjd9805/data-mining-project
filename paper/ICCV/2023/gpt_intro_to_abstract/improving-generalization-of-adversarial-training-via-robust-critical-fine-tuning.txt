The pursuit of accurate and trustworthy artificial intelligence (AI) systems is a fundamental objective in the deep learning community. Adversarial examples, which perturb input by a small, human imperceptible noise, pose a significant threat to the security of AI systems. While adversarial training (AT) has been shown to enhance adversarial robustness, it sacrifices generalization on in-distribution data and is vulnerable to certain out-of-distribution (OOD) examples, resulting in unsatisfactory performance. In this paper, we introduce a new concept called Module Robust Criticality (MRC) to investigate the redundant capacity of adversarially trained models for robustness. We empirically find that certain modules exhibit redundant characteristics, resulting in negligible drops in adversarial robustness. Based on these findings, we propose a novel fine-tuning technique called Robust Critical Fine-Tuning (RiFT), which leverages the redundant capacity of non-robust-critical modules to improve generalization while maintaining adversarial robustness. Experimental results demonstrate that RiFT significantly improves both generalization performance and OOD robustness by around 2% while maintaining or even improving the adversarial robustness. Moreover, incorporating RiFT into other AT regimes leads to further enhancements. Our experiments reveal insights into the interplay between generalization, adversarial robustness, and OOD robustness, suggesting the potential for more effective adversarial training methods that leverage the redundant capacity in AT models. The contribution of this work includes the proposal of MRC, the development of RiFT, and the insights gained from the experiments.