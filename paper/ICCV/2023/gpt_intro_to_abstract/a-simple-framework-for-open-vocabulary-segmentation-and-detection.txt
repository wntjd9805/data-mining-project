Developing transferable vision systems that can be applied to novel domains is a key research area in computer science. Previous work, such as CLIP, has demonstrated strong zero-shot transferability, leading researchers to explore open-vocabulary models for fine-grained vision tasks like detection and segmentation. However, these tasks have distinct vocabulary sizes and spatial granularities of supervision. Most previous works have focused on improving performance for either detection or segmentation, but there is a need for a model that can bridge the gap between these tasks and achieve good performance for both. In this paper, we propose OpenSeeD, the first open-vocabulary model that jointly learns from detection and segmentation data. We address the challenges of transferring semantic knowledge and bridging the gap between box and mask supervision through techniques such as a shared semantic space, decoupled decoding, and conditioned mask assistance. Our experiments demonstrate outstanding zero-shot and transfer performance across various tasks and datasets, making OpenSeeD a strong baseline for developing a single open-vocabulary model for both detection and segmentation.