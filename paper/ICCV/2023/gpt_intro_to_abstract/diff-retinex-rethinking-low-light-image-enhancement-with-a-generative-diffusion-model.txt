Low-light images are often affected by various degradations, particularly the loss of scene structures, which not only impacts visual quality but also reduces information content. To address these issues, many low-light image enhancement (LLIE) methods have been proposed, including traditional approaches based on image priors and physical models, as well as learning-based methods utilizing deep learning. However, these methods often suffer from poor generalization and limited application due to manual design and optimization-driven efficiency. In this paper, we propose Diff-Retinex, a physically explainable and generative model for LLIE. Diff-Retinex integrates the advantages of physical models and generative networks by formulating the enhancement problem as Retinex decomposition and conditional image generation. A novel Transformer decomposition network is proposed to improve decomposition applicability, and generative diffusion-based networks are designed to address various degradations in the decomposed components. Our contributions include rethinking LLIE as a conditional image generation task, proposing a Transformer decomposition network, and applying the diffusion model to guide multi-path adjustments for better performance. Experimental results show the effectiveness of Diff-Retinex in enhancing low-light images.