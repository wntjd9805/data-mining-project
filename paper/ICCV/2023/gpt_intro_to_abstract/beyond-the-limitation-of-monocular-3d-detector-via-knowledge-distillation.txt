The development of autonomous driving has led to increased interest in 3D object detection. While multi-view-based and LiDAR-based approaches have shown impressive performance, they require expensive equipment. Monocular vision methods have gained attention due to their lower cost and practicality. However, lightweight 3D monocular detectors often suffer from weak feature extraction and inaccurate depth estimation, resulting in suboptimal performance. To address this issue, knowledge distillation (KD) techniques have been proposed, enabling a compact student model to learn from a larger teacher model. KD has been successful in classification and detection tasks, and some methods have used LiDAR data as input for the teacher model. However, acquiring LiDAR data incurs additional costs. This paper proposes a vision-based strategy for optimizing the student model in 3D monocular distillation, without relying on LiDAR data. The proposed approach outperforms previous state-of-the-art methods across different teacher-student pairs and backbones. The contributions of this work include the design of a perspective matrix for better feature imitation of distant objects, a depth-guided prediction distillation method, and a unified 3D monocular distillation framework that incorporates implicit depth information. Experimental results on KITTI and nuScenes datasets demonstrate the superiority of the proposed method.