With the rapid development of deep learning models, computer vision tasks are increasingly being handled effectively by deep networks. This has led to a growing interest among researchers in developing unified multi-task networks that can perform joint learning and inference on multiple distinct tasks. Multi-Task Learning (MTL) has emerged as a promising approach to avoid repetitive training of single-task models and generate predictions for different tasks with only one-time inference. Learning discriminative task-specific features is a fundamental problem for MTL models. However, existing models have limitations in terms of sharing input task-generic features and static decoding of task-specific features. To address these limitations, this paper proposes a novel multi-task mixture-of-experts framework called "TaskExpert", which dynamically performs task-feature decoding for different inputs and tasks. TaskExpert introduces a set of task-generic expert networks that decompose the backbone feature into representative task-generic features. The dynamically assembled feature activations from different experts produce task-specific representations based on sample-dependent and task-specific gating scores. Additionally, a "multi-task feature memory" is introduced to aggregate the dynamic task-specific features produced at different network layers. This paper also introduces the "Memorial Mixture-of-Experts (MMoE)" module to utilize task features from the multi-task feature memory and improve task-specific feature decoding. The effectiveness of TaskExpert is validated on challenging multi-task visual scene understanding benchmarks, showcasing superior performance compared to previous methods.