Recent years have seen significant advancements in deep learning-based computer vision tasks. However, natural video editing still faces two major challenges: generating high-fidelity edited content while adhering to the original geometry of target objects, and maintaining temporal consistency throughout the entire video. Previous diffusion-based methods have achieved good results in image editing, but applying these models to video editing is challenging. Existing solutions often focus on generating smooth motions rather than maintaining geometric consistency. Additionally, these video diffusion models suffer from high computing complexity, limiting practical applications. In this paper, we propose a novel approach called StableVideo for text-driven diffusion video editing with high temporal consistency. Instead of editing the atlases directly, we update the atlases through editing key video frames. We also introduce temporal dependency constraints for diffusion models to ensure consistent appearance across time. Our approach includes an inter-frame propagation mechanism to generate objects with coherent geometry and an aggregation network to generate edited atlases from key frames. We evaluate our approach through extensive qualitative and quantitative experiments and demonstrate its superiority over existing methods in terms of editing performance and complexity. Our contributions include solving the consistency problem in diffusion video editing using layered atlas approaches, presenting a new video editing framework for high geometry and appearance consistency, and conducting experiments to validate our approach.