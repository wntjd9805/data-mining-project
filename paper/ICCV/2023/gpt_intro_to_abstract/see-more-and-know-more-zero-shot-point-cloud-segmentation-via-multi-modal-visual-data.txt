Point cloud segmentation is a crucial task in computer vision and has applications in autonomous driving, robotics, and virtual reality. Fully supervised methods have achieved impressive performance in point cloud segmentation, but they struggle to generalize to novel objects without labeled training data and require time-consuming manual annotations. Zero-shot learning, which utilizes side information such as word embeddings, has shown promise in recognizing unseen objects. However, existing zero-shot learning methods only consider the alignment between uni-modal visual features and semantic features, limiting their performance. In this paper, we propose a novel multi-modal zero-shot approach for point cloud semantic segmentation. Our method leverages the complementary information from both LiDAR point clouds and images to generate more comprehensive visual features. We introduce a Semantic-Guided Visual Feature Fusion (SGVF) approach to align visual features with semantic features, and we propose Semantic-Visual Feature Enhancement (SVFE) to reduce the semantic-visual domain gap. Extensive comparisons and ablation studies demonstrate the effectiveness of our method, which achieves state-of-the-art performance on SemanticKITTI and nuScenes datasets. Our contributions include the development of a novel approach for multi-modal zero-shot point cloud semantic segmentation and the design of effective feature fusion and enhancement methods that improve recognition of unseen classes.