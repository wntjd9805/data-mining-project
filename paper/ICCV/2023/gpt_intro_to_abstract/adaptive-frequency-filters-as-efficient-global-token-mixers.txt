Remarkable progress has been made in vision network designs, specifically in token mixing in the global scope. Existing dominant network families, such as Transformers, CNNs, and MLPs, achieve global token mixing but are computationally expensive. Recent research has focused on improving the efficiency of token mixing in transformers by squeezing the scope or reducing complexity, but these methods sacrifice the expressiveness and performance of efficient network designs. In this paper, we propose using adaptive frequency filters as efficient global token mixers by leveraging the convolution theorem in signal processing. We frame global token mixing as a large-kernel convolution in the latent space and implement it using a Hadamard product operation in the frequency domain. We also introduce instance-adaptive masks for channel-specific token mixing. Our approach reduces the complexity of token mixing and achieves a better trade-off between accuracy and efficiency. We propose the Adaptive Frequency Filtering (AFF) token mixer and demonstrate its effectiveness and efficiency through extensive experiments on vision tasks. Our contributions include revealing the efficiency of adaptive frequency filtering, comparing it with other frequency-domain neural operators, and building a lightweight vision backbone, AFFNet, which achieves state-of-the-art accuracy and efficiency trade-offs.