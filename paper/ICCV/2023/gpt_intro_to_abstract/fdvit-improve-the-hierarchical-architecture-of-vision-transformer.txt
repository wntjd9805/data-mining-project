Convolutional neural networks (CNNs) and transformers have been widely used in computer vision (CV) and neural language processing (NLP) tasks, respectively. In recent years, vision transformers (ViTs) have shown promising performance in CV tasks, but they suffer from high computational costs and are not suitable for resource-constrained devices. To address this limitation, researchers have proposed hierarchical architectures that reduce the number of patches. However, these approaches result in a significant loss of information and performance. In this paper, we introduce a novel architecture called Flexible Downsampling Vision Transformer (FDViT) that improves the hierarchical architecture of ViTs. FDViT incorporates a flexible downsampling layer that can produce output feature maps with any preset dimension, allowing for a smooth reduction in the spatial dimension to avoid information loss. We also propose a masked auto-encoder architecture to facilitate the training of the FD layer. Experimental results demonstrate that FDViT significantly reduces computational costs while increasing classification performance compared to ViT models. Additionally, FDViT outperforms existing architectures in object detection and semantic segmentation tasks.