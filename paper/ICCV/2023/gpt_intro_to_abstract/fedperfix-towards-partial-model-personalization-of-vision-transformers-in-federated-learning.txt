Federated learning (FL) has gained prominence as a method for training machine learning models on decentralized data without direct data sharing. However, data heterogeneity among participating clients poses a challenge as the data can be non-independent and non-identically distributed (non-IID). Personalized Federated Learning (PFL) has been explored to address this challenge by focusing on local models on the clients rather than the global model on the server. Previous research has investigated full model personalization and partial model personalization, with the latter offering advantages in terms of computation, communication, and privacy. However, the application of partial model personalization to Vision Transformers (ViTs) in the federated learning community has received limited attention. This paper aims to investigate where and how to partially personalize a ViT model for improved performance on client data. The study explores the sensitivity of different layers in the ViT model and identifies self-attention layers and the classification head as suitable candidates for personalization. To bridge global and client-specific knowledge, the paper proposes a novel approach called FedPerfix, which leverages a family of plugins to capture client-specific knowledge for personalization. The approach is evaluated on different datasets, demonstrating state-of-the-art performance with lower resource requirements compared to competitive methods.