Semantic segmentation, which predicts pixel-level category labels for scenes, has been predominantly achieved using deep neural networks. However, these networks rely on the assumption that the training and testing data come from the same distribution, which is often unrealistic in real-world scenarios. Unsupervised domain adaptation techniques have been proposed to address this distribution shift problem, but they typically require full access to the source dataset, which may not always be possible due to restrictions on data sharing.To overcome these limitations, recent research has focused on source-free domain adaptation, where knowledge is transferred from a well-trained source model instead of the source data itself. However, existing techniques for typical unsupervised domain adaptation may not work effectively in a source-free setting due to the lack of supervision from the real source domain.In this paper, we propose a novel two-stream segmentation network for source-free unsupervised domain adaptation. Unlike previous methods that use depth estimation as an auxiliary task, we introduce a multimodal auxiliary network that takes depth information and intermediate representations from the main image encoder as input. Both the main and auxiliary networks are trained on the segmentation task using self-training, and a cross-modal consistency loss is formulated to enforce regularization.Our proposed framework has several advantages. First, the inference-stage model only consists of the main stream, making it a unimodal model that infers from RGB images in the same way as existing models. Second, the asymmetric design of our network introduces modality variance, which helps rectify pseudo labels with multimodal knowledge expansion. Additionally, the cross-modal consistency effectively transfers knowledge from the multimodal auxiliary network to the unimodal main network. Lastly, our framework is more feasible compared to existing depth-aware unsupervised domain adaptation methods as it only requires depth information from the target domain, which can be easily learned from self-supervised depth estimation models.In experimental evaluations on the Cityscapes dataset, our proposed method outperforms prior art by a significant margin, achieving an mIoU of 57.7% and 57.5% when adapting from the GTA5 and SYNTHIA benchmarks, respectively. This demonstrates the effectiveness and potential of our source-free UDA framework utilizing a multimodal auxiliary network and cross-modal consistency.