Arbitrary style transfer (AST) is a computer vision task that aims to render a content image with the artistic style of an arbitrary painting, enabling the generated image to imitate any artistic style. Existing approaches for AST have made notable improvements in feature transformation modules, novel architectures, and practical objectives. The core of AST lies in the matching of content features and style features, with two categories of approaches being holistic feature distribution matching and locality-aware feature matching. Attention-based methods, which fall under the locality-aware feature matching category, have gained research focus for their ability to capture long-range dependencies. However, these methods suffer from two main predicaments: distorted style patterns and unstable matching effects, as well as high computational complexity. The all-to-all attention mechanism used in these methods lacks error tolerance and is sensitive to position variation, leading to distorted style patterns and unstable matching effects. This attention mechanism also has quadratic computational complexity, which is not scalable for large images. To address these limitations, this paper proposes a novel all-to-key attention (A2K) mechanism for AST. A2K matches each query with stable distributed keys that depict the style distribution of all local regions, improving matching error tolerance. A2K also gradually concentrates attention from coarse-grained regions to fine-grained keys, mitigating the "woods for the trees" problem and ensuring semantic correctness. The proposed A2K mechanism reduces sensitivity to position variation and renders consistent style patterns. Experimental results demonstrate the superiority of the proposed approach over state-of-the-art methods in preserving semantic structures and rendering consistent style patterns.