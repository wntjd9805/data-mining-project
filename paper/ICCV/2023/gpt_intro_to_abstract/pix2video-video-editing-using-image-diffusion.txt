Diffusion-based algorithms have gained popularity for image creation due to their stability, high-quality results, and support for conditional sampling. However, the same workflow has not been readily available for video editing. Existing methods for video editing based on image-based workflows or video stylization propagation result in inconsistent results across video frames. In this paper, we propose a method for editing videos using a pre-trained image diffusion model and textual instructions without requiring additional training. Our approach involves inverting the input video clip and allowing the user to edit one of the frames using textual prompts, with the aim of consistently propagating the edit across the rest of the video. To achieve temporal coherence, we inject features from previously edited frames into the self-attention layer of the current frame, allowing for cross-frame attention and generation of images with coherent appearance characteristics. We also adopt a guided diffusion strategy to update intermediate latent codes and enforce similarity to the previous frame. Additionally, we utilize a depth-conditioned image generation model to reason about motion dynamics. Our method, named Pix2Video, is evaluated on various real video clips for both local and global edits, and compared with state-of-the-art approaches. The results show that Pix2Video outperforms or performs on par with the baselines, without requiring compute-intensive preprocessing or video-specific fine-tuning. We demonstrate that our training-free approach utilizing pre-trained image generation models can bring advancements in controlled image editing to videos with no additional cost. The source code is available on the project page.