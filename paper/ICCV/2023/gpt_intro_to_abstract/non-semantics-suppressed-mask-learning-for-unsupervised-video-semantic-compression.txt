Video compression methods have traditionally focused on improving reconstructed video quality for human perception, rather than preserving AI task-specific semantic information. This can negatively impact downstream AI tasks. In response to this problem, research efforts have been directed towards Video Coding for Machine (VCM), which involves lossy video compression for supporting downstream AI tasks. Early approaches transport manually-designed image descriptors, but have limited performance. On the other hand, some methods improve traditional codecs with hand-crafted designs tailored to specific tasks. Others compress feature maps of AI models instead of images, requiring task modules to adapt to these features through supervised learning. While scalable coding methods optimized by supervised learning or task-relevant feature matching loss functions have been proposed, there are few task-agnostic methods that also exploit data-driven semantics. Task-agnostic methods decouple the coding system from downstream tasks and are advantageous in data scarcity scenarios. In this paper, we propose a novel Unsupervised Video Semantic Compression task that meets the requirements of being both task-agnostic and data-driven. We introduce the Semantic-Mining-then-Compensation (SMC) framework as a baseline method to improve semantic coding capability in plain video codecs. The framework utilizes neural networks to extract semantic features from the original and lossy videos, and only transports the residual part. Through self-supervised optimization, which involves masking out a portion of the decoded video patches and using the unmasked parts to reconstruct the masked regions, the framework learns to generate videos with rich semantics. However, the generative learning paradigm of the Masked Image Modeling (MIM) scheme used in this process may result in the extraction of redundant, non-semantic information. To address this, we decrease the non-semantic information entropy of the video by introducing a parameterized Gaussian Mixture Model conditioned on the mined video semantics. Our approach demonstrates superior performance compared to traditional, learnable, and perceptual codecs on various video analysis tasks and datasets. The contributions of this work include the introduction of the SMC framework, the application of the MIM scheme to semantic coding, the Non-Semantics Suppressed (NSS) learning strategy, and the notable performance achieved in video analysis tasks.