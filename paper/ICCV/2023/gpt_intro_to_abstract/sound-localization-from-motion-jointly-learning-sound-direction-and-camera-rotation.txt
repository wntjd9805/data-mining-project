This paper introduces a self-supervised approach for jointly estimating camera rotation from images and sound direction from binaural audio. The authors propose a problem called sound localization from motion (SLfM) and train models through self-supervision, without the need for labeled training data. The models provide each other with self-supervision by predicting the rotation angle and azimuth of sound sources, forcing their predictions to agree with one another. The paper also presents a method for learning representations suited to this task through cross-view binauralization. All components of the model are trained solely on unlabeled audio-visual data. Experimental results demonstrate that paired audio-visual data provides a useful signal for geometry learning. The paper also showcases competitive performance in self-supervised sound localization and rotation estimation tasks, as well as superior features for downstream tasks. Overall, the proposed approach leverages the consistent changes in audio and visual signals to enhance machine perception, particularly in the fields of camera motion estimation and sound source localization.