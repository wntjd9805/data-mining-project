This paper introduces the problem of continual learning in computer science, which focuses on learning from a stream of data and sequentially learning new tasks while avoiding forgetting of previous tasks. The main challenge in continual learning is avoiding catastrophic forgetting, where performance on previous tasks degrades significantly over time when new tasks are learned. Dynamic Network methods have been successful in solving this problem by dynamically modifying the network to solve new tasks. However, these methods can quickly become large in terms of parameters required. To address this issue, the paper proposes a novel approach based on three motivations: reusing instead of re-learning, channel-wise transformations, and lightweight parameters. The proposed method involves reprogramming the CNN layers trained on old tasks to solve new tasks by adding lightweight task-specific reprogramming parameters. This approach achieves a better stability-plasticity balance compared to other methods and requires limited extra parameters during continual learning. The paper also presents the experimental results, showing that the proposed method achieves state-of-the-art performance on task incremental continual learning. Overall, the paper provides a new solution for continual learning in CNNs that allows for unlimited input-to-output mappings and improved performance on new tasks.