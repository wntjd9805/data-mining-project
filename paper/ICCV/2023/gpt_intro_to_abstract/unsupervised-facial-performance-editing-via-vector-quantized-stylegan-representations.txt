High-resolution editing and rendering of photorealistic facial portrait videos are in high demand for virtual human applications such as actor performance editing and video conferencing. State-of-the-art methods use pre-trained generative neural networks, like StyleGAN, to learn a face prior from large datasets for video editing. However, these methods have limitations in terms of explicit and intuitive editing due to the training dataset distribution and attribute constraints. Different approaches leverage 3D face tracking or deep learning techniques for face synthesis and editing, but they lack local control and can't explain topological changes easily. In this paper, we propose an approach that uses vector quantization to learn spatial segmentations automatically from a pre-trained StyleGAN for video editing. Our approach has two stages: Stage-I establishes a map between an input video sequence and the pre-trained StyleGAN, and Stage-II uses the learned semantic segmentation mask as input to a generator network for rendering. We provide a configurable interface for semantic edits, allowing artists to label and combine segmented regions for intuitive editing. Our method is the first to adopt vector quantization for unsupervised segmentation of a pre-trained StyleGAN given a video, and it enables localized semantic video editing with user-guided segmentation masks.