Advances in learning-based implicit neural representations have shown promise for high-fidelity novel-view synthesis and 3D scene representation in applications such as autonomous driving and robotics. However, current techniques have limitations in representing complex urban scenes and require computationally complex per-scene optimization. This paper proposes NeO 360, a generalizable NeRF architecture that infers the representation of 360° unbounded scenes from just a single or a few posed RGB images. NeO 360 utilizes triplanes and local image-level features to optimize multiple unbounded 3D scenes. It also introduces a large-scale 360° unbounded dataset, called NeRDS 360, for training and evaluation. Experimental results demonstrate the effectiveness of NeO 360 for few-shot novel view synthesis and prior-based sampling tasks, showing significant improvements over baselines on the NeRDS 360 dataset. This work contributes to the advancement of novel-view synthesis and scene understanding in complex outdoor environments.