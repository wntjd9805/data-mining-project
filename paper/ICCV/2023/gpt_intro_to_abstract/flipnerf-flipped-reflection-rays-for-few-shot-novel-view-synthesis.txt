The Neural Radiance Field (NeRF) has shown success in generating photo-realistic images from new viewpoints. However, it requires a dense set of training images, making it impractical for sparse views. This paper explores two approaches for few-shot novel view synthesis: pre-training and regularization methods. Pre-training methods require large-scale datasets and prior knowledge, while regularization methods optimize per scene. Both approaches have limitations, such as the high cost of collecting large-scale datasets and reliance on additional resources. To address these challenges, the paper proposes FlipNeRF, a regularization method that utilizes flipped reflection rays as additional training resources. FlipNeRF reconstructs surface normals accurately and reduces floating artifacts with the Uncertainty-aware Emptiness Loss (UE Loss). It also improves feature consistency between original input rays and flipped reflection rays with the Bottleneck Feature Consistency Loss (BFC Loss). Experimental results demonstrate that FlipNeRF outperforms other baselines, especially in challenging scenarios. The contributions of this paper include the proposed FlipNeRF framework, UE Loss, and BFC Loss, as well as achieving state-of-the-art performance on multiple benchmarks.