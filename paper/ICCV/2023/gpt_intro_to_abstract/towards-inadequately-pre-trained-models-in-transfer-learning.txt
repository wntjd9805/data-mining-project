Deep learning has become highly successful in computer vision with the use of well-labeled datasets. However, data annotation is a time-consuming process, especially in specialized domains. Transfer learning allows for training deep models with smaller labeled datasets by leveraging pre-trained models. Fine-tuning the whole network typically achieves higher performance when there are sufficient labeled examples. Alternatively, using the pre-trained model as a feature extractor is important when training resources are limited or end-to-end training is not feasible. Previous research has investigated the transferability of pre-trained models, but the influence of the pre-training process on transfer learning is still not well understood. This paper aims to explore how different stages in the pre-training process impact the transferability of models. The authors conduct extensive transfer learning experiments on multiple benchmark datasets and find that inadequately pre-trained models tend to transfer better than fully pre-trained models when used as feature extractors. Additionally, the authors observe that fine-tuning prefers later pre-training checkpoints compared to feature extraction. The paper also highlights the limitations of using transferability assessment approaches to select pre-trained models, as their scores often do not correlate well with fine-tuning performance. Overall, this work provides insights into the transferability of neural networks and the impact of the pre-training process on transfer learning.