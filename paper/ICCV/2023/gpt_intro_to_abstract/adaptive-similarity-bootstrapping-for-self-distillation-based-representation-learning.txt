Self-supervised learning (SSL) methods have achieved significant advancements in recent years. These methods train features that are invariant to data augmentation by maximizing the similarity between two augmented versions of a single input image. However, this optimization procedure is prone to trivial solutions and can result in a collapsed scenario. Contrastive learning methods have been successful in avoiding collapse by using negative pairs. However, these methods require large batches and are memory inefficient. More recently, self-distillation methods have gained attention for their ability to avoid collapse without the need for large batch sizes. In this paper, we explore the best way to obtain positive pairs for similarity maximization SSL methods. We propose using positive pairs from different images, which makes the features more aligned with common downstream tasks like classification. To do this, we leverage the structure of the latent space and use semantically related images as a proxy for the oracle that indicates valid positive pairs. We introduce a method called AdaSim, which adaptively uses positive pairs sampled from a ranked set of neighbors based on the estimated quality of the latent space. Our experiments show that straightforward bootstrapping without adaptation can lead to performance degradation or collapse. AdaSim outperforms existing methods on standard downstream tasks and provides a flexible approach to similarity bootstrapping in self-distillation methods.