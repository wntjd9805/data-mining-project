We introduce a learning-based depth map fusion frame-work that accepts a set of depth and confidence maps gen-erated by a Multi-View Stereo (MVS) algorithm as input and improves them. This is accomplished by integrat-ing volumetric visibility constraints that encode long-range surface relationships across different views into an end-to-end trainable architecture. We also introduce a depth search window estimation sub-network trained jointly with the larger fusion sub-network to reduce the depth hypoth-esis search space along each ray. Our method learns to model depth consensus and violations of visibility con-straints directly from the data; effectively removing the ne-cessity of fine-tuning fusion parameters. Extensive exper-iments on MVS datasets show substantial improvements in the accuracy of the output fused depth and confidence maps. Our code is available at https://github.com/ nburgdorfer/V-FUSE 