Recently, large-scale diffusion models, e.g., Stable diffu-sion and DallE2, have shown remarkable results on image synthesis. On the other hand, large-scale cross-modal pre-trained models (e.g., CLIP, ALIGN, and FILIP) are com-petent for various downstream tasks by learning to align vision and language embeddings.In this paper, we ex-plore the possibility of jointly modeling generation and dis-crimination. Specifically, we propose DiffDis to unify the cross-modal generative and discriminative pretraining into one single framework under the diffusion process. DiffDis first formulates the image-text discriminative problem as a generative diffusion process of the text embedding from the text encoder conditioned on the image. Then, we propose a novel dual-stream network architecture, which fuses the noisy text embedding with the knowledge of latent images from different scales for image-text discriminative learning.Moreover, the generative and discriminative tasks can ef-ficiently share the image-branch network structure in the multi-modality model. Benefiting from diffusion-based uni-fied training, DiffDis achieves both better generation abil-ity and cross-modal semantic alignment in one architecture.Experimental results show that DiffDis outperforms single-task models on both the image generation and the image-text discriminative tasks, e.g., 1.65% improvement on av-erage accuracy of zero-shot classification over 12 datasets and 2.42 improvement on FID of zero-shot image synthesis.Figure 1. Comparison of our framework and single-task models. (b) The (a) The diffusion-based image generation-only model. image-text discrimination-only model. (c) Our DiffDis joints the discriminative and generative tasks under the diffusion processing into one framework. Better viewed in colors. 