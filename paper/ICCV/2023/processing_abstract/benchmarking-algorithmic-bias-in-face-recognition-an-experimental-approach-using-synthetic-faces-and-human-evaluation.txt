We propose an experimental method for measuring bias in face recognition systems. Existing methods to measure bias depend on benchmark datasets that are collected in the wild and annotated for protected (e.g., race, gender) and unprotected (e.g., pose, lighting) attributes. Such observa-tional datasets only permit correlational conclusions, e.g.,“Algorithm A’s accuracy is different on female and male faces in dataset X.”. By contrast, experimental methods manipulate attributes individually and thus permit causal conclusions, e.g., “Algorithm A’s accuracy is affected by gender and skin color.”Our method is based on generating synthetic faces us-ing a neural face generator, where each attribute of interest is modified independently while leaving all other attributes constant. Human observers crucially provide the ground truth on perceptual identity similarity between synthetic im-age pairs. We validate our method quantitatively by evalu-ating race and gender biases of three research-grade face recognition models. Our synthetic pipeline reveals that for these algorithms, accuracy is lower for Black and EastAsian population subgroups. Our method can also quan-tify how perceptual changes in attributes affect face iden-tity distances reported by these models. Our large synthetic dataset, consisting of 48,000 synthetic face image pairs (10,200 unique synthetic faces) and 555,000 human anno-tations (individual attributes and pairwise identity compar-isons) is available to researchers in this important area. 