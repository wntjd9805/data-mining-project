CutMix is a vital augmentation strategy that determines the performance and generalization ability of vision trans-formers (ViTs). However, the inconsistency between the mixed images and the corresponding labels harms its ef-ﬁcacy. Existing CutMix variants tackle this problem by generating more consistent mixed images or more precise mixed labels, but inevitably introduce heavy training over-head or require extra information, undermining ease of use. To this end, we propose an novel and effective Self-Motivated image Mixing method (SMMix), which motivates both image and label enhancement by the model under training itself. Speciﬁcally, we propose a max-min atten-tion region mixing approach that enriches the attention-focused objects in the mixed images. Then, we introduce a ﬁne-grained label assignment technique that co-trains the output tokens of mixed images with ﬁne-grained su-pervision. Moreover, we devise a novel feature consis-tency constraint to align features from mixed and unmixed images. Due to the subtle designs of the self-motivated paradigm, our SMMix is signiﬁcant in its smaller train-ing overhead and better performance than other CutMix variants.In particular, SMMix improves the accuracy ofDeiT-T/S/B, CaiT-XXS-24/36, and PVT-T/S/M/L by more than +1% on ImageNet-1k. The generalization capability of our method is also demonstrated on downstream tasks and out-of-distribution datasets. Our project is available at https://github.com/ChenMnZ/SMMix. 