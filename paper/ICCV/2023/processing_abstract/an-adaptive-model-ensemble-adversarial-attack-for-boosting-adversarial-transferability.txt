While the transferability property of adversarial exam-ples allows the adversary to perform black-box attacks (i.e., the attacker has no knowledge about the target model), the transfer-based adversarial attacks have gained great atten-tion. Previous works mostly study gradient variation or image transformations to amplify the distortion on critical parts of inputs. These methods can work on transferring across models with limited differences, i.e., from CNNs toCNNs, but always fail in transferring across models with wide differences, such as from CNNs to ViTs. Alternatively, model ensemble adversarial attacks are proposed to fuse outputs from surrogate models with diverse architectures to get an ensemble loss, making the generated adversar-ial example more likely to transfer to other models as it can fool multiple models concurrently. However, existing ensemble attacks simply fuse the outputs of the surrogate models evenly, thus are not efﬁcacious to capture and am-plify the intrinsic transfer information of adversarial exam-ples. In this paper, we propose an adaptive ensemble attack, dubbed AdaEA, to adaptively control the fusion of the out-puts from each model, via monitoring the discrepancy ra-tio of their contributions towards the adversarial objective.Furthermore, an extra disparity-reduced ﬁlter is introduced to further synchronize the update direction. As a result, we achieve considerable improvement over the existing ensem-ble attacks on various datasets, and the proposed AdaEA can also boost existing transfer-based attacks, which fur-ther demonstrates its efﬁcacy and versatility. The source code: https://github.com/CHENBIN99/AdaEA 