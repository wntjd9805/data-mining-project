requiresHowever,Accomplishing household tasks to plan step-by-step actions considering the consequences of previous actions. the state-of-the-art em-bodied agents often make mistakes in navigating the environment and interacting with proper objects due to learning by imitating experts or algorithmic imperfect planners without such knowledge. To improve both vi-sual navigation and object interaction, we propose to consider the consequence of taken actions by CAPEAM (Context-Aware Planning and Environment-Aware Memory) that incorporates semantic context (e.g., appropriate ob-jects to interact with) in a sequence of actions, and the changed spatial arrangement and states of interacted objects (e.g., location that the object has been moved to) in inferring the subsequent actions. We empirically show that the agent with the proposed CAPEAM achieves state-of-the-art performance in various metrics using a challenging interactive instruction following benchmark in both seen and unseen environments by large margins (up to+10.70% in unseen env.). 