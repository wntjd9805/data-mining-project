Self-supervised learning usually uses a large amount of unlabeled data to pre-train an encoder which can be used as a general-purpose feature extractor, such that downstream users only need to perform fine-tuning operations to en-joy the benefit of “large model”. Despite this promising prospect, the security of pre-trained encoder has not been thoroughly investigated yet, especially when the pre-trained encoder is publicly available for commercial use.In this paper, we propose AdvEncoder, the first frame-work for generating downstream-agnostic universal adver-sarial examples based on the pre-trained encoder. AdvEn-coder aims to construct a universal adversarial perturba-tion or patch for a set of natural images that can fool all the downstream tasks inheriting the victim pre-trained en-coder. Unlike traditional adversarial example works, the pre-trained encoder only outputs feature vectors rather than classification labels. Therefore, we first exploit the high fre-quency component information of the image to guide the generation of adversarial examples. Then we design a gen-erative attack framework to construct adversarial pertur-bations/patches by learning the distribution of the attack surrogate dataset to improve their attack success rates and transferability. Our results show that an attacker can suc-cessfully attack downstream tasks without knowing either the pre-training dataset or the downstream dataset. We also tailor four defenses for pre-trained encoders, the re-1National Engineering Research Center for Big Data Technology andSystem 2Services Computing Technology and System Lab 3Hubei KeyLaboratory of Distributed System Security 4Hubei Engineering ResearchCenter on Big Data Security 5Cluster and Grid Computing LabFigure 1: An overview of adversarial examples against dif-ferent downstream tasks based on a pre-trained encoder sults of which further prove the attack ability of AdvEn-coder. Our codes are available at: https://github. com/CGCL-codes/AdvEncoder. 