Vision-Language Pretraining (VLP) has significantly im-proved the performance of various vision-language tasks with the matching of images and texts.In this paper, we propose VL-Match, a Vision-Language framework with En-hanced Token-level and Instance-level Matching. At the to-ken level, a Vision-Language Replaced Token Detection task is designed to boost the substantial interaction between text tokens and images, where the text encoder of VLP works as a generator to generate a corrupted text, and the mul-timodal encoder of VLP works as a discriminator to pre-dict whether each text token in the corrupted text matches the image. At the instance level, in the Image-Text Match-ing task that judges whether an image-text pair is matched, we propose a novel bootstrapping method to generate hard negative text samples that are different from the positive ones only at the token level. In this way, we can force the network to detect fine-grained differences between images and texts. Notably, with a smaller amount of parameters,VL-Match significantly outperforms previous SOTA on all image-text retrieval tasks.Figure 1. (a) Masked Language Modeling (MLM) predicts orig-inal tokens of the masked positions with image and text repre-sentations; (b) Vision-Language Replaced Token Detection (VL-RTD) enhances token-level matching by discriminating whether each token in the generated text aligns with the image and the text context; (c) Image-Text Matching (ITM) predicts whether the given texts match the image; (d) Fine-Grained Image-Text Match-ing (FG-ITM) adds a fine-grained negative sample to enhance the matching ability at instance level. 