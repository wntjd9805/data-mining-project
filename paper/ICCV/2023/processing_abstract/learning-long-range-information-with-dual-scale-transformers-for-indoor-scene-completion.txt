Due to the limited resolution of 3D sensors and the in-evitable mutual occlusion between objects, 3D scans of real scenes are commonly incomplete. Previous scene comple-tion methods struggle to capture long-range spatial context, resulting in unsatisfactory completion results. To alleviate the problem, we propose a novel Dual-Scale TransformerNetwork (DST-Net) that efficiently utilizes both long-range and short-range spatial context information to improve the quality of 3D scene completion. To reduce the heavy com-putation cost of extracting long-range features via trans-formers, DST-Net adopts a self-supervised two-stage com-pletion strategy. In the first stage, we split the input scene into blocks and perform completion on individual blocks. In the second stage, the blocks are merged together as a whole and then further refined to improve completeness. More importantly, we propose a contrastive attention training strategy to encourage the transformers to learn distinguish-able features for better scene completion. Experiments on datasets of Matterport3D, ScanNet, and ICL-NUIM demon-strate that our method can generate better completion re-sults, and our method outperforms the state-of-the-art meth-ods quantitatively and qualitatively. 