Few-shot continual learning is the ability to continually train a neural network from a sequential stream of few-shot data.In this paper, we propose a Few-shot Contin-ual Infomax Learning (FCIL) framework that makes a deep model to continually/incrementally learn new concepts from few labeled samples, relieving the catastrophic forgetting of past knowledge. Speciﬁcally, inspired by the theoretical deﬁnition of transfer entropy, we introduce a feature em-bedding infomax to effectively perform the few-shot learn-ing, which can transfer the strong encoding capability of the base network to learn the feature embedding of these novel classes by maximizing the mutual information of different-level feature distributions. Further, considering that the learned knowledge in the human brain is a generalization of actual information and exists in a certain relational struc-ture, we perform continual structure infomax learning to relieve the catastrophic forgetting problem in the continual learning process. The information structure of this learned knowledge can be preserved through maximizing the mu-tual information across these continual-changing relations of inter-classes. Comprehensive evaluations on CIFAR100, miniImageNet, and CUB200 datasets demonstrate the supe-riority of our FCIL when compared against state-of-the-art methods on the few-shot continual learning task. 