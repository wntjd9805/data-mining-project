Conditional diffusion models have demonstrated impres-sive performance in image manipulation tasks. The gen-eral pipeline involves adding noise to the image and then denoising it. However, this method faces a trade-off prob-lem: adding too much noise affects the fidelity of the image while adding too little affects its editability. This largely limits their practical applicability. In this paper, we pro-pose a novel framework, Selective Diffusion Distillation (SDD), that ensures both the fidelity and editability of im-ages. Instead of directly editing images with a diffusion model, we train a feedforward image manipulation network under the guidance of the diffusion model. Besides, we pro-pose an effective indicator to select the semantic-related timestep to obtain the correct semantic guidance from the dif-fusion model. This approach successfully avoids the dilemma caused by the diffusion process. Our extensive experiments demonstrate the advantages of our framework. Code is re-leased at https://github.com/AndysonYs/Selective-Diffusion-Distillation. 