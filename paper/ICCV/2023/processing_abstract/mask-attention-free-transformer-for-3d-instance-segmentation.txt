Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is com-monly involved.Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar man-ner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall ini-tial instance masks. Therefore, we abandon the mask at-tention design and resort to an auxiliary center regres-sion task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we de-velop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial po-sition queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position en-coding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4 faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmen-tation benchmark, and also demonstrates superior perfor-mance across various datasets. Code and models are avail-able at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.Ã— 