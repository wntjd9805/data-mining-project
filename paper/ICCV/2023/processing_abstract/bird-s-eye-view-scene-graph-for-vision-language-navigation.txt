Instruction: Go to the dining room by front door and push in the chair furthest from the front door.Instruction: Go to the dining room by front door and push in the chair furthest from the front door.Vision-language navigation (VLN), which entails an agent to navigate 3D environments following human in-structions, has shown great advances. However, current agents are built upon panoramic observations, which hin-ders their ability to perceive 3D scene geometry and easily leads to ambiguous selection of panoramic view. To address these limitations, we present a BEV Scene Graph (BSG), which leverages multi-step BEV representations to encode scene layouts and geometric cues of indoor environment un-der the supervision of 3D detection. During navigation,BSG builds a local BEV representation at each step and maintains a BEV-based global scene map, which stores and organizes all the online collected local BEV representations according to their topological relations. Based on BSG, the agent predicts a local BEV grid-level decision score and a global graph-level decision score, combined with a sub-view selection score on panoramic views, for more accu-rate action prediction. Our approach signiÔ¨Åcantly outper-forms state-of-the-art methods on REVERIE, R2R, and R4R, showing the potential of BEV perception in VLN. 