The visual classification performance of vision-language models such as CLIP has been shown to benefit from ad-ditional semantic knowledge from large language models (LLMs) such as GPT-3. In particular, averaging over LLM-generated class descriptors, e.g. “waffle, which has a round shape”, can notably improve generalization performance.In this work, we critically study this behavior and proposeWaffleCLIP, a framework for zero-shot visual classifica-tion which simply replaces LLM-generated descriptors with random character and word descriptors. Without query-ing external models, we achieve comparable performance gains on a large number of visual classification tasks. This allows WaffleCLIP to both serve as a low-cost alterna-tive, as well as a sanity check for any future LLM-based vision-language model extensions. We conduct an extensive experimental study on the impact and shortcomings of ad-ditional semantics introduced with LLM-generated descrip-tors, and showcase how - if available - semantic context is better leveraged by querying LLMs for high-level con-cepts, which we show can be done to jointly resolve po-tential class name ambiguities. Code is available here: https://github.com/ExplainableML/WaffleCLIP. 