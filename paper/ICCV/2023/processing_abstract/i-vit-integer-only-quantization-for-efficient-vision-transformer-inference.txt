Vision Transformers (ViTs) have achieved state-of-the-art performance on various computer vision applications.However, these models have considerable storage and com-putational overheads, making their deployment and efficient inference on edge devices challenging. Quantization is a promising approach to reducing model complexity, and the dyadic arithmetic pipeline can allow the quantized models to perform efficient integer-only inference. Unfortunately, dyadic arithmetic is based on the homogeneity condition in convolutional neural networks, which is not applicable to the non-linear components in ViTs, making integer-only in-ference of ViTs an open issue. In this paper, we propose I-ViT, an integer-only quantization scheme for ViTs, to enableViTs to perform the entire computational graph of inference with integer arithmetic and bit-shifting, and without any floating-point arithmetic. In I-ViT, linear operations (e.g.,MatMul and Dense) follow the integer-only pipeline with dyadic arithmetic, and non-linear operations (e.g., Softmax,GELU, and LayerNorm) are approximated by the proposed light-weight integer-only arithmetic methods. More specif-ically, I-ViT applies the proposed Shiftmax and ShiftGELU, which are designed to use integer bit-shifting to approxi-mate the corresponding floating-point operations. We eval-uate I-ViT on various benchmark models and the results show that integer-only INT8 quantization achieves compa-rable (or even slightly higher) accuracy to the full-precision (FP) baseline. Furthermore, we utilize TVM for practi-cal hardware deployment on the GPU’s integer arithmetic units, achieving 3.72∼4.11× inference speedup compared to the FP model. Code of both Pytorch and TVM is released at https://github.com/zkkli/I-ViT. 