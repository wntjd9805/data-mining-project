Multi-modality image fusion aims to combine different modalities to produce fused images that retain the com-plementary features of each modality, such as functional highlights and texture details. To leverage strong genera-tive priors and address challenges such as unstable train-ing and lack of interpretability for GAN-based generative methods, we propose a novel fusion algorithm based on the denoising diffusion probabilistic model (DDPM). The fusion task is formulated as a conditional generation prob-lem under the DDPM sampling framework, which is fur-ther divided into an unconditional generation subproblem and a maximum likelihood subproblem. The latter is mod-eled in a hierarchical Bayesian manner with latent vari-ables and inferred by the expectation-maximization (EM) algorithm. By integrating the inference solution into the diffusion sampling iteration, our method can generate high-quality fused images with natural image generative priors and cross-modality information from source images. Note that all we required is an unconditional pre-trained gener-ative model, and no Ô¨Åne-tuning is needed. Our extensive experiments indicate that our approach yields promising fusion results in infrared-visible image fusion and medical image fusion. The code is available at https://github. com/Zhaozixiang1228/MMIF-DDFM . 