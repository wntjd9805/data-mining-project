Spiking Neural Networks (SNNs) are promising energy-efficient models for neuromorphic computing. For training the non-differentiable SNN methods, the backpropagation through time (BPTT) with surrogate gradients (SG) method has achieved high performance. However, this method suf-fers from considerable memory cost and training time dur-ing training. In this paper, we propose the Spatial LearningThrough Time (SLTT) method that can achieve high per-formance while greatly improving training efficiency com-pared with BPTT. First, we show that the backpropagation of SNNs through the temporal domain contributes just a lit-tle to the final calculated gradients. Thus, we propose to ignore the unimportant routes in the computational graph during backpropagation. The proposed method reduces the number of scalar multiplications and achieves a small mem-ory occupation that is independent of the total time steps.Furthermore, we propose a variant of SLTT, called SLTT-K, that allows backpropagation only at K time steps, then the required number of scalar multiplications is further re-duced and is independent of the total time steps. Exper-iments on both static and neuromorphic datasets demon-strate superior training efficiency and performance of ourSLTT. In particular, our method achieves state-of-the-art accuracy on ImageNet, while the memory cost and train-ing time are reduced by more than 70% and 50%, re-spectively, compared with BPTT. Our code is available at https://github.com/qymeng94/SLTT. 