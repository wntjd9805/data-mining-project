We propose a novel hairstyle transferred image synthe-sis method considering the underlying head geometry of two input images. In traditional GAN-based methods, trans-ferring hairstyle from one image to the other often makes the synthesized result awkward due to differences in pose, shape, and size of heads. To resolve this, we utilize neural rendering by registering two input heads in the volumetric space to make a transferred hairstyle fit on the head of a target image. Because of the geometric nature of neural ren-dering, our method can render view varying images of syn-thesized results from a single transfer process without caus-ing distortion from which extant hairstyle transfer methods built upon traditional GAN-based generators suffer. We ver-ify that our method surpasses other baselines in view of pre-*Corresponding author: seunggyu.chang@navercorp.comâ€ This work was done during internship at NAVER Cloud. serving the identity and hairstyle of two input images when synthesizing a hairstyle transferred image rendered at any point of view. 