We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt large models on a diverse collection of data while minimizing the expected cost to remove the inﬂuence of training samples from the trained model. This process, also known as selective forgetting or unlearning, is often conducted by partitioning a dataset into shards, training fully independent models on each, then ensembling the re-sulting models.Increasing the number of shards reduces the expected cost to forget but at the same time it increases inference cost and reduces the ﬁnal accuracy of the model since synergistic information between samples is lost dur-ing the independent model training. Rather than treating each shard as independent, SAFE introduces the notion of a shard graph, which allows incorporating limited informa-tion from other shards during training, trading off a mod-est increase in expected forgetting cost with a signiﬁcant increase in accuracy, all while still attaining complete re-moval of residual inﬂuence after forgetting. SAFE uses a lightweight system of adapters which can be trained while reusing most of the computations. This allows SAFE to be trained on shards an order-of-magnitude smaller than cur-rent state-of-the-art methods (thus reducing the forgetting costs) while also maintaining high accuracy, as we demon-strate empirically on ﬁne-grained computer vision datasets. 