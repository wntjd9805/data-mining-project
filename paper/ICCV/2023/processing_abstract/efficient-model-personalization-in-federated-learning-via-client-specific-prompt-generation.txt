Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple dis-tributed clients without sharing their data to preserve pri-vacy. Recently, large-scale pre-trained models (e.g., VisionTransformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust repre-sentations from large-scale models while enabling efficient model personalization for heterogeneous clients, we pro-pose a novel personalized FL framework of client-specificPrompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed frame-work jointly optimizes the stages of personalized prompt adaptation locally and personalized prompt generation globally. The former aims to train visual prompts that adapt foundation models to each client, while the latter ob-serves local optimization directions to generate personal-ized prompts for all clients. Through extensive experiments on benchmark datasets, we show that our pFedPG is favor-able against state-of-the-art personalized FL methods un-der various types of data heterogeneity, allowing computa-tion and communication efficient model personalization. 