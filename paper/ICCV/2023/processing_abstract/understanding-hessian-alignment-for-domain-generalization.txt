Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenar-ios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regu-larizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier’s head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spec-tral norm between the classifier’s head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Fur-thermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients.Our analysis explains the success of many regularizers likeCORAL, IRM, V-REx, Fish, IGA, and Fishr as they regularize part of the classifier’s head Hessian and/or gradient. Finally, we propose two simple yet effective methods to match the classifier’s head Hessians and gradients in an efficient way, based on the Hessian Gradient Product (HGP) and Hutchin-son’s method (Hutchinson), and without directly calculatingHessians. We validate the OOD generalization ability of proposed methods in different scenarios, including transfer-ability, severe correlation shift, label shift and diversity shift.Our results show that Hessian alignment methods achieve promising performance on various OOD benchmarks. The code is available here. 