Recently, the community has made tremendous progress in developing effective methods for point cloud video under-standing that learn from massive amounts of labeled data.However, annotating point cloud videos is usually notori-ously expensive. Moreover, training via one or only a few traditional tasks (e.g., classification) may be insufficient to learn subtle details of the spatio-temporal structure existing in point cloud videos. In this paper, we propose a MaskedSpatio-Temporal Structure Prediction (MaST-Pre) method to capture the structure of point cloud videos without human annotations. MaST-Pre is based on spatio-temporal point-tube masking and consists of two self-supervised learning tasks. First, by reconstructing masked point tubes, our method is able to capture the appearance information of point cloud videos. Second, to learn motion, we propose a temporal cardinality difference prediction task that esti-mates the change in the number of points within a point tube. In this way, MaST-Pre is forced to model the spatial and temporal structure in point cloud videos. Extensive ex-periments on MSRAction-3D, NTU-RGBD, NvGesture, andSHRECâ€™17 demonstrate the effectiveness of the proposed method. The code is available at https://github. com/JohnsonSign/MaST-Pre. 