Video-Language Pre-training (VLP) has become one of the most popular research topics in deep learning. How-ever, compared to image-language pre-training, VLP has lagged far behind due to the lack of large amounts of video-text pairs. In this work, we train a VLP model with a hy-brid of image-text and video-text pairs, which significantly outperforms pre-training with only the video-text pairs. Be-sides, existing methods usually model the cross-modal in-teraction using cross-attention between single-scale visual tokens and textual tokens. These visual features are either of low resolutions lacking fine-grained information, or of high resolutions without high-level semantics. To address the is-sue, we propose Hierarchical interactive Video-LanguagePre-training (HiVLP) that efficiently uses a hierarchical vi-sual feature group for multi-modal cross-attention during pre-training. In the hierarchical framework, low-resolution features are learned with focus on more global high-level semantic information, while high-resolution features carry fine-grained details. As a result, HiVLP has the ability to effectively learn both the global and fine-grained represen-tations to achieve better alignment between video and text inputs. Furthermore, we design a hierarchical multi-scale vision contrastive loss for self-supervised learning to boost the interaction between them. Experimental results show that HiVLP establishes new state-of-the-art results in three downstream tasks, text-video retrieval, video-text retrieval, and video captioning. 