Data heterogeneity is an inherent challenge that hinders the performance of federated learning (FL). Recent stud-ies have identified the biased classifiers of local models as the key bottleneck. Previous attempts have used classifier calibration after FL training, but this approach falls short in improving the poor feature representations caused by training-time classifier biases. Resolving the classifier bias dilemma in FL requires a full understanding of the mech-anisms behind the classifier. Recent advances in neural collapse have shown that the classifiers and feature pro-totypes under perfect training scenarios collapse into an optimal structure called simplex equiangular tight frame (ETF). Building on this neural collapse insight, we pro-pose a solution to the FLâ€™s classifier bias problem by uti-lizing a synthetic and fixed ETF classifier during training.The optimal classifier structure enables all clients to learn unified and optimal feature representations even under ex-tremely heterogeneous data. We devise several effective modules to better adapt the ETF structure in FL, achiev-ing both high generalization and personalization. Extensive experiments demonstrate that our method achieves state-of-the-art performances on CIFAR-10, CIFAR-100, and Tiny-ImageNet. The code is available at https://github. com/ZexiLee/ICCV-2023-FedETF. 