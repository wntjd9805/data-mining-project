Class Incremental Learning (CIL) aims to sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge. We propose to use Masked Au-toencoders (MAEs) as efﬁcient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classiﬁca-tion. Moreover, MAEs can reliably reconstruct original in-put images from randomly selected patches, which we use to store exemplars from past tasks more efﬁciently for CIL.We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable rep-resentations. Our experiments conﬁrm that our approach performs better than the state-of-the-art on CIFAR-100,ImageNet-Subset, and ImageNet-Full. The code is avail-able at https://github.com/scok30/MAE-CIL. 