Vision models often fail systematically on groups of data that share common semantic characteristics (e.g., rare objects or unusual scenes), but identifying these failure modes is a challenge. We introduce AdaVi-sion, an interactive process for testing vision mod-els which helps users identify and ﬁx coherent fail-ure modes. Given a natural language description of a coherent group, AdaVision retrieves relevant im-ages from LAION-5B with CLIP. The user then labels a small amount of data for model correctness, which is used in successive retrieval rounds to hill-climb towards high-error regions, reﬁning the group deﬁnition. Once a group is saturated, AdaVision uses GPT-3 to sug-gest new group descriptions for the user to explore. We demonstrate the usefulness and generality of AdaVi-sion in user studies, where users ﬁnd major bugs in state-of-the-art classiﬁcation, object detection, and im-age captioning models. These user-discovered groups have failure rates 2-3x higher than those surfaced by automatic error clustering methods. Finally, ﬁnetun-ing on examples found with AdaVision ﬁxes the dis-covered bugs when evaluated on unseen examples, with-out degrading in-distribution accuracy, and while also improving performance on out-of-distribution datasets. 