Large scale text-guided diffusion models have garnered significant attention due to their ability to synthesize diverse images that convey complex visual concepts. This gener-ative power has more recently been leveraged to perform text-to-3D synthesis. In this work, we present a technique that harnesses the power of latent diffusion models for edit-ing existing 3D objects. Our method takes oriented 2D im-ages of a 3D object as input and learns a grid-based volu-metric representation of it. To guide the volumetric repre-sentation to conform to a target text prompt, we follow un-conditional text-to-3D methods and optimize a Score Dis-tillation Sampling (SDS) loss. However, we observe that combining this diffusion-guided loss with an image-based regularization loss that encourages the representation not to deviate too strongly from the input object is challeng-ing, as it requires achieving two conflicting goals while viewing only structure-and-appearance coupled 2D projec-tions. Thus, we introduce a novel volumetric regularization loss that operates directly in 3D space, utilizing the explicit nature of our 3D representation to enforce correlation be-tween the global structure of the original and edited object.Furthermore, we present a technique that optimizes cross-attention volumetric grids to refine the spatial extent of the edits. Extensive experiments and comparisons demonstrate the effectiveness of our approach in creating a myriad of edits which cannot be achieved by prior works1. 