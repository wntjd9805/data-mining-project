Video Question Answering (VideoQA) is a challenging task that entails complex multi-modal reasoning. In con-trast to multiple-choice VideoQA which aims to predict the answer given several options, the goal of open-endedVideoQA is to answer questions without restricting candi-date answers. However, the majority of previous VideoQA models formulate open-ended VideoQA as a classification task to classify the video-question pairs into a fixed answer set, i.e., closed-vocabulary, which contains only frequent answers (e.g., top-1000 answers). This leads the model to be biased toward only frequent answers and fail to general-ize on out-of-vocabulary answers. We hence propose a new benchmark, Open-vocabulary Video Question Answering (OVQA), to measure the generalizability of VideoQA models by considering rare and unseen answers. In addition, in or-der to improve the modelâ€™s generalization power, we intro-duce a novel GNN-based soft verbalizer that enhances the prediction on rare and unseen answers by aggregating the information from their similar words. For evaluation, we introduce new baselines by modifying the existing (closed-vocabulary) open-ended VideoQA models and improve their performances by further taking into account rare and un-seen answers. Our ablation studies and qualitative anal-yses demonstrate that our GNN-based soft verbalizer fur-ther improves the model performance, especially on rare and unseen answers. We hope that our benchmark OVQA can serve as a guide for evaluating the generalizability ofVideoQA models and inspire future research. Code is avail-able at https://github.com/mlvlab/OVQA. 