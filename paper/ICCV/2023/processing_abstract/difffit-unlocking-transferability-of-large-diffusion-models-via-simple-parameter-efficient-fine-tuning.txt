Diffusion models have proven to be highly effective in generating high-quality images. However, adapting large pre-trained diffusion models to new domains remains an open challenge, which is critical for real-world applica-tions. This paper proposes DiffFit, a parameter-efﬁcient strategy to ﬁne-tune large pre-trained diffusion models that enable fast adaptation to new domains. DiffFit is em-barrassingly simple that only ﬁne-tunes the bias term and newly-added scaling factors in speciﬁc layers, yet result-ing in signiﬁcant training speed-up and reduced model stor-age costs. Compared with full ﬁne-tuning, DiffFit achievesCorrespondence to {xie.enze, li.zhenguo}@huawei.com 2× training speed-up and only needs to store approximately 0.12% of the total model parameters. Intuitive theoretical analysis has been provided to justify the efﬁcacy of scal-ing factors on fast adaptation. On 8 downstream datasets,DiffFit achieves superior or competitive performances com-pared to the full ﬁne-tuning while being more efﬁcient. Re-markably, we show that DiffFit can adapt a pre-trained low-resolution generative model to a high-resolution one by adding minimal cost. Among diffusion-based methods,DiffFit sets a new state-of-the-art FID of 3.02 on ImageNet 512×512 benchmark by ﬁne-tuning only 25 epochs from a public pre-trained ImageNet 256×256 checkpoint while be-ing 30× more training efﬁcient than the closest competitor. 1