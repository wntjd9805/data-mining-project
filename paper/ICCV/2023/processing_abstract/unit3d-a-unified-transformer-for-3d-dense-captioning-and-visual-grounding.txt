Performing 3D dense captioning and visual grounding requires a common and shared understanding of the under-lying multimodal relationships. However, despite some pre-vious attempts on connecting these two related tasks with highly task-specific neural modules, it remains understud-ied how to explicitly depict their shared nature to learn them simultaneously. In this work, we propose UniT3D, a sim-ple yet effective fully unified transformer-based architecture for jointly solving 3D visual grounding and dense caption-ing. UniT3D enables learning a strong multimodal repre-sentation across the two tasks through a supervised joint pre-training scheme with bidirectional and seq-to-seq ob-jectives. With a generic architecture design, UniT3D allows expanding the pre-training scope to more various training sources such as the synthesized data from 2D prior knowl-edge to benefit 3D vision-language tasks. Extensive experi-ments and analysis demonstrate that UniT3D obtains signif-icant gains for 3D dense captioning and visual grounding. 