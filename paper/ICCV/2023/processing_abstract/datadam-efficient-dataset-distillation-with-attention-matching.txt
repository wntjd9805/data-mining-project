Researchers have long tried to minimize training costs in deep learning while maintaining strong generalization across diverse datasets. Emerging research on dataset dis-tillation aims to reduce training costs by creating a small synthetic set that contains the information of a larger real dataset and ultimately achieves test accuracy equivalent to a model trained on the whole dataset. Unfortunately, the synthetic data generated by previous methods are not guar-anteed to distribute and discriminate as well as the original training data, and they incur significant computational costs.Despite promising results, there still exists a significant per-formance gap between models trained on condensed syn-thetic sets and those trained on the whole dataset. In this paper, we address these challenges using efficient DatasetDistillation with Attention Matching (DataDAM), achieving state-of-the-art performance while reducing training costs.Specifically, we learn synthetic images by matching the spa-tial attention maps of real and synthetic data generated by different layers within a family of randomly initialized neu-ral networks. Our method outperforms the prior methods on several datasets, including CIFAR10/100, TinyImageNet,ImageNet-1K, and subsets of ImageNet-1K across most of the settings, and achieves improvements of up to 6.5% and 4.1% on CIFAR100 and ImageNet-1K, respectively. We also show that our high-quality distilled images have practical benefits for downstream applications, such as continual learn-ing and neural architecture search. 