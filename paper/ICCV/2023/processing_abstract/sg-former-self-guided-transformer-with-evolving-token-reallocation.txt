Vision Transformer has demonstrated impressive success across various vision tasks. However, its heavy computa-tion cost, which grows quadratically with respect to the to-ken sequence length, largely limits its power in handling large feature maps. To alleviate the computation cost, pre-vious works rely on either fine-grained self-attentions re-stricted to local small regions, or global self-attentions but to shorten the sequence length resulting in coarse granu-In this paper, we propose a novel model, termed larity. as Self-guided Transformer (SG-Former), towards effec-tive global self-attention with adaptive fine granularity. At the heart of our approach is to utilize a significance map, which is estimated through hybrid-scale self-attention and evolves itself during training, to reallocate tokens based on the significance of each region.Intuitively, we assign more tokens to the salient regions for achieving fine-grained attention, while allocating fewer tokens to the minor re-gions in exchange for efficiency and global receptive fields.The proposed SG-Former achieves performance superior to state of the art: our base size model achieves 84.7%Top-1 accuracy on ImageNet-1K, 51.2mAP bbAP on CoCo, 52.7mIoU on ADE20K surpassing the Swin Transformer by +1.3% / +2.7 mAP/ +3 mIoU, with lower computa-tion costs and fewer parameters. The code is available at https://github.com/OliverRensu/SG-Former 