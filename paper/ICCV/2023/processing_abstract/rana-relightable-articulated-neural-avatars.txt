We propose RANA, a relightable and articulated neural avatar for the synthesis of humans under arbitrary view-points, body poses, and lighting. We only require a short video clip of the person to create the avatar and assume no knowledge about the lighting environment. We present a novel framework to model humans while disentangling their geometry, texture, and lighting environment from monocu-lar RGB videos. To simplify this otherwise ill-posed task we first estimate the coarse geometry and texture of the per-son via SMPL+D model fitting and then learn an articu-lated neural representation for higher quality image syn-thesis. RANA first generates the normal and albedo maps of the person in any given target body pose and then uses spherical harmonics lighting to generate the shaded im-age in the target lighting environment. We also propose to pre-train RANA using synthetic images and demonstrate that it leads to better disentanglement between geometry and texture while also improving robustness to novel body poses. Finally, we also present a new photo-realistic syn-thetic dataset, Relighting Human, to quantitatively evaluate the performance of the proposed approach. 