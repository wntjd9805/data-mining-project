Current methods for open-vocabulary object detec-tion (OVOD) rely on a pre-trained vision-language model (VLM) to acquire the recognition ability.In this paper, we propose a simple yet effective framework to Distill theKnowledge from the VLM to a DETR-like detector, termedDK-DETR. Specifically, we present two ingenious distilla-tion schemes named semantic knowledge distillation (SKD) and relational knowledge distillation (RKD). To utilize the rich knowledge from the VLM systematically, SKD trans-fers the semantic knowledge explicitly, while RKD exploits implicit relationship information between objects. Further-more, a distillation branch including a group of auxiliary queries is added to the detector to mitigate the negative ef-fect on base categories. Equipped with SKD and RKD on the distillation branch, DK-DETR improves the detection performance of novel categories significantly and avoids disturbing the detection of base categories. Extensive ex-periments on LVIS and COCO datasets show that DK-DETR surpasses existing OVOD methods under the setting that the base-category supervision is solely available. The code and models are available at https://github. com/hikvision-research/opera. 