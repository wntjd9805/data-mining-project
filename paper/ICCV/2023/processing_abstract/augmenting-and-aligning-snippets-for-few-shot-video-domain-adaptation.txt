For video models to be transferred and applied seam-lessly across video tasks in varied environments, Video Un-supervised Domain Adaptation (VUDA) has been intro-duced to improve the robustness and transferability of video models. However, current VUDA methods rely on a vast amount of high-quality unlabeled target data, which may not be available in real-world cases. We thus consider a more realistic Few-Shot Video-based Domain Adapta-tion (FSVDA) scenario where we adapt video models with only a few target video samples. While a few methods have touched upon Few-Shot Domain Adaptation (FSDA) in images and in FSVDA, they rely primarily on spatial augmentation for target domain expansion with alignment performed statistically at the instance level. However, videos contain more knowledge in terms of rich tempo-ral and semantic information, which should be fully con-sidered while augmenting target domains and perform-ing alignment in FSVDA. We propose a novel SSA2lign to address FSVDA at the snippet level, where the target domain is expanded through a simple snippet-level aug-mentation followed by the attentive alignment of snippets both semantically and statistically, where semantic align-ment of snippets is conducted through multiple perspec-tives. Empirical results demonstrate state-of-the-art per-formance of SSA2lign across multiple cross-domain ac-tion recognition benchmarks. Code will be provided at: https://github.com/xuyu0010/SSA2lign. 