Non-exemplar class-incremental learning (NECIL) re-quires deep models to maintain existing knowledge while continuously learning new classes without saving old class samples. In NECIL methods, prototypical representations are usually stored, which inject information from former classes to resist catastrophic forgetting in subsequent in-cremental learning. However, since the model continuously learns new knowledge, the stored prototypical representa-tions cannot correctly model the properties of old classes in the existence of knowledge updates. To address this prob-lem, we propose a novel prototype reminiscence mechanism that incorporates the previous class prototypes with arriv-ing new class features to dynamically reshape old class fea-ture distributions thus preserving the decision boundaries of previous tasks. In addition, to improve the model gen-eralization on both newly arriving classes and old classes, we contribute an augmented asymmetric knowledge aggre-gation approach, which aggregates the overall knowledge of the current task and extracts the valuable knowledge of the past tasks, on top of self-supervised label augmentation.Experimental results on three benchmarks suggest the supe-rior performance of our approach over the SOTA methods. 