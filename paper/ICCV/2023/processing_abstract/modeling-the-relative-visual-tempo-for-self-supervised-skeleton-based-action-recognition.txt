Visual tempo characterizes the dynamics and the tem-poral evolution, which helps describe actions. Recent ap-proaches directly perform visual tempo prediction on skele-ton sequences, which may suffer from insufﬁcient featureIn this paper, we observe that rela-representation issue. tive visual tempo is more in line with human intuition, and thus providing more effective supervision signals. Based on this, we propose a novel Relative Visual Tempo Con-trastive Learning framework for skeleton action Represen-tation (RVTCLR). Speciﬁcally, we design a Relative VisualTempo Learning (RVTL) task to explore the motion informa-tion in intra-video clips, and an Appearance-Consistency (AC) task to learn appearance information simultaneous-ly, resulting in more representative spatiotemporal fea-tures. Furthermore, skeleton sequence data is much s-parser than RGB data, making the network learn short-cuts, and overﬁt to low-level information such as skeleton scales. To learn high-order semantics, we further design a new Distribution-Consistency (DC) branch, containing three components: Skeleton-speciﬁc Data Augmentation (S-DA), Fine-grained Skeleton Encoding Module (FSEM), andDistribution-aware Diversity (DD) Loss. We term our entire method (RVTCLR with DC) as RVTCLR+. Extensive exper-iments on NTU RGB+D 60 and NTU RGB+D 120 datasets demonstrate that our RVTCLR+ can achieve competitive re-sults over the state-of-the-art methods. Code is available at https://github.com/Zhuysheng/RVTCLR.∗Corresponding author. This work was supported in part by the NewGeneration AI Major Project of Ministry of Science and Technology ofChina under Grant 2018AAA0102501 and in part by the National NaturalScience Foundation of China (NSFC) under Grant U21B2027.Figure 1. The basic visual tempo prediction is considered a classi-ﬁcation task, where the learned model is used to assign tempo la-bels to individual video clips. We introduce relative visual tempo learning and appearance-consistency based on contrastive learn-ing. It’s more human-intuitive for modeling actions. The top-1 accuracy on NTU-60 Xsub benchmark supports our claim. 