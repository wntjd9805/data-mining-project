Event cameras have the ability to record continuous and detailed trajectories of objects with high temporal resolu-tion, thereby providing intuitive motion cues for optical flow estimation. Nevertheless, most existing learning-based ap-proaches for event optical flow estimation directly remould the paradigm of conventional images by representing the consecutive event stream as static frames, ignoring the in-herent temporal continuity of event data. In this paper, we argue that temporal continuity is a vital element of event-based optical flow and propose a novel Temporal MotionAggregation (TMA) approach to unlock its potential. Tech-nically, TMA comprises three components: an event split-ting strategy to incorporate intermediate motion informa-tion underlying the temporal context, a linear lookup strat-egy to align temporally fine-grained motion features and a novel motion pattern aggregation module to emphasize consistent patterns for motion feature enhancement. By incorporating temporally fine-grained motion information,TMA can derive better flow estimates than existing meth-ods at early stages, which not only enables TMA to obtain more accurate final predictions, but also greatly reduces the demand for a number of refinements. Extensive exper-iments on DSEC-Flow and MVSEC datasets verify the ef-fectiveness and superiority of our TMA. Remarkably, com-pared to E-RAFT, TMA achieves a 6% improvement in ac-curacy and a 40% reduction in inference time on DSEC-Flow. Code will be available at https://github. com/ispc-lab/TMA. 