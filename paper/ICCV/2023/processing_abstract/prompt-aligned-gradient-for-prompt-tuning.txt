Thanks to the large pre-trained vision-language mod-els (VLMs) like CLIP [37], we can craft a zero-shot clas-sifier by discrete prompt design, e.g., the confidence score of an image being “[CLASS]” can be obtained by using the VLM provided similarity between the image and the prompt sentence “a photo of a [CLASS]”. Further-more, prompting shows great potential for fast adaptation ofVLMs to downstream tasks if we fine-tune the soft prompts with few samples. However, we find a common failure that improper fine-tuning or learning with extremely few-shot samples may even under-perform the zero-shot prediction.Existing methods still address this problem by using tradi-tional anti-overfitting techniques such as early stopping and data augmentation, which lack a principled solution spe-cific to prompting. In this paper, we present Prompt-alignedGradient, dubbed ProGrad to prevent prompt tuning from forgetting the general knowledge learned from VLMs. In particular, ProGrad only updates the prompt whose gradi-ent is aligned (or non-conflicting) to the general knowledge, which is represented as the optimization direction offered by the pre-defined prompt predictions. Extensive experiments under the few-shot learning, domain generalization, base-to-new generalization and cross-dataset transfer settings demonstrate the stronger few-shot generalization ability ofProGrad over state-of-the-art prompt tuning methods. 