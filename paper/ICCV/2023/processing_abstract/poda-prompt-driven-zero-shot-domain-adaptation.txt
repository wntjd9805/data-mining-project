Domain adaptation has been vastly investigated in com-puter vision but still requires access to target images at train time, which might be intractable in some uncommon condi-tions. In this paper, we propose the task of ‘Prompt-drivenZero-shot Domain Adaptation’, where we adapt a model trained on a source domain using only a general descrip-tion in natural language of the target domain, i.e., a prompt.First, we leverage a pretrained contrastive vision-language model (CLIP) to optimize afﬁne transformations of source features, steering them towards the target text embedding while preserving their content and semantics. To achieve this, we propose Prompt-driven Instance Normalization (PIN). Second, we show that these prompt-driven augmen-tations can be used to perform zero-shot domain adapta-tion for semantic segmentation. Experiments demonstrate that our method signiﬁcantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand, even surpassing one-shot unsupervised do-main adaptation. A similar boost is observed on object de-tection and image classiﬁcation. The code is available at https://github.com/astra-vision/PODA . 