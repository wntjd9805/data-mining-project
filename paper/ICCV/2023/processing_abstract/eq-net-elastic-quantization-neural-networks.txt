Current model quantization methods have shown their promising capability in reducing storage space and computa-tion complexity. However, due to the diversity of quantization forms supported by different hardware, one limitation of ex-isting solutions is that usually require repeated optimization for different scenarios. How to construct a model with flexi-ble quantization forms has been less studied. In this paper, we explore a one-shot network quantization regime, namedElastic Quantization Neural Networks (EQ-Net), which aims to train a robust weight-sharing quantization supernet. First of all, we propose an elastic quantization space (including elastic bit-width, granularity, and symmetry) to adapt to vari-ous mainstream quantitative forms. Secondly, we propose theWeight Distribution Regularization Loss (WDR-Loss) andGroup Progressive Guidance Loss (GPG-Loss) to bridge the inconsistency of the distribution for weights and output logits in the elastic quantization space gap. Lastly, we in-corporate genetic algorithms and the proposed ConditionalQuantization-Aware Accuracy Predictor (CQAP) as an es-timator to quickly search mixed-precision quantized neural networks in supernet. Extensive experiments demonstrate that our EQ-Net is close to or even better than its static coun-terparts as well as state-of-the-art robust bit-width methods.Code can be available at https://github.com/xuke225/EQ-Net. 