Exploiting spatial-angular correlation is crucial to light field (LF) image super-resolution (SR), but is highly chal-lenging due to its non-local property caused by the dis-parities among LF images. Although many deep neural networks (DNNs) have been developed for LF image SR and achieved continuously improved performance, exist-ing methods cannot well leverage the long-range spatial-angular correlation and thus suffer a significant perfor-mance drop when handling scenes with large disparity vari-ations.In this paper, we propose a simple yet effective method to learn the non-local spatial-angular correlation for LF image SR. In our method, we adopt the epipolar plane image (EPI) representation to project the 4D spatial-angular correlation onto multiple 2D EPI planes, and then develop a Transformer network with repetitive self-attention operations to learn the spatial-angular correlation by mod-eling the dependencies between each pair of EPI pixels.Our method can fully incorporate the information from all angular views while achieving a global receptive field along the epipolar line. We conduct extensive experiments with insightful visualizations to validate the effectiveness of our method. Comparative results on five public datasets show that our method not only achieves state-of-the-art SR per-formance but also performs robust to disparity variations.Code is publicly available at https://github.com/ZhengyuLiang24/EPIT. 