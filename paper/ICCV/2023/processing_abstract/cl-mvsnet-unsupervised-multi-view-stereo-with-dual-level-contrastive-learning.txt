Unsupervised Multi-View Stereo (MVS) methods have achieved promising progress recently. However, previous methods primarily depend on the photometric consistency assumption, which may suffer from two limitations: indis-tinguishable regions and view-dependent effects, e.g., low-textured areas and reflections. To address these issues, in this paper, we propose a new dual-level contrastive learn-ing approach, named CL-MVSNet. Specifically, our model integrates two contrastive branches into an unsupervisedMVS framework to construct additional supervisory sig-nals. On the one hand, we present an image-level con-trastive branch to guide the model to acquire more con-text awareness, thus leading to more complete depth esti-mation in indistinguishable regions. On the other hand, we exploit a scene-level contrastive branch to boost the rep-resentation ability, improving robustness to view-dependent effects. Moreover, to recover more accurate 3D geometry, we introduce an L0.5 photometric consistency loss, which encourages the model to focus more on accurate points while mitigating the gradient penalty of undesirable ones.Extensive experiments on DTU and Tanks&Temples bench-marks demonstrate that our approach achieves state-of-the-art performance among all end-to-end unsupervised MVS frameworks and outperforms its supervised counterpart by a considerable margin without fine-tuning. 