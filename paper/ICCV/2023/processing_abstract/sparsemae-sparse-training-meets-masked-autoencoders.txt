Masked Autoencoders (MAE) and its variants have proven to be effective for pretraining large-scale VisionTransformers (ViTs). However, small-scale models do not benefit from the pretraining mechanisms due to limited ca-pacity. Sparse training is a method of transferring represen-tations from large models to small ones by pruning unim-portant parameters. However, naively combining MAE fine-tuning with sparse training make the network task-specific, resulting in the loss of task-agnostic knowledge, which is crucial for model generalization.In this paper, we aim to reduce model complexity from large vision transformers pretrained by MAE with assistant of sparse training. We summarize various sparse training methods to prune large vision transformers during MAE pretraining and finetun-ing stages, and discuss their shortcomings. To improve learning both task-agnostic and task-specific knowledge, we propose SparseMAE, a novel two-stage sparse train-ing method that includes sparse pretraining and sparse finetuning. In sparse pretraining, we dynamically prune a small-scale sub-network from a ViT-Base. During finetun-ing, the sparse sub-network adaptively changes its topology connections under the task-agnostic knowledge of the full model. Extensive experimental results demonstrate the ef-fectiveness of our method and its superiority on small-scale vision transformers. Code will be available at https://github.com/aojunzz/SparseMAE. 