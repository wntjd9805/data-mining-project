While multi-task learning (MTL) has become an attrac-tive topic, its training usually poses more difﬁculties than the single-task case. How to successfully apply knowl-edge distillation into MTL to improve training efﬁciency and model performance is still a challenging problem. In this paper, we introduce a new knowledge distillation pro-cedure with an alternative match for MTL of dense predic-tion based on two simple design principles. First, for mem-ory and training efﬁciency, we use a single strong multi-task model as a teacher during training instead of multiple teachers, as widely adopted in existing studies. Second, we employ a less sensitive Cauchy-Schwarz (CS) divergence instead of the Kullback–Leibler (KL) divergence and pro-pose a CS distillation loss accordingly. With the less sen-sitive divergence, our knowledge distillation with an alter-native match is applied for capturing inter-task and intra-task information between the teacher model and the student model of each task, thereby learning more ”dark knowl-edge” for effective distillation. We conducted extensive ex-periments on dense prediction datasets, including NYUD-v2 and PASCAL-Context, for multiple vision tasks, such as se-mantic segmentation, human parts segmentation, depth es-timation, surface normal estimation, and boundary detec-tion. The results show that our proposed method decidedly improves model performance and the practical inference ef-ﬁciency. 