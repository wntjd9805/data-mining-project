Current arbitrary style transfer models are limited to ei-ther image or video domains. In order to achieve satisfying image and video style transfers, two different models are in-evitably required with separate training processes on image and video domains, respectively. In this paper, we show that this can be precluded by introducing UniST, a Unified StyleTransfer framework for both images and videos. At the core of UniST is a domain interaction transformer (DIT ), which first explores context information within the specific domain and then interacts contextualized domain information for joint learning. In particular, DIT enables exploration of temporal information from videos for the image style trans-fer task and meanwhile allows rich appearance texture from images for video style transfer, thus leading to mutual ben-efits. Considering heavy computation of traditional multi-head self-attention, we present a simple yet effective axial multi-head self-attention (AMSA) for DIT , which improves computational efficiency while maintains style transfer per-formance. To verify the effectiveness of UniST, we conduct extensive experiments on both image and video style trans-fer tasks and show that UniST performs favorably against state-of-the-art approaches on both tasks. Code is available at https://github.com/NevSNev/UniST. 