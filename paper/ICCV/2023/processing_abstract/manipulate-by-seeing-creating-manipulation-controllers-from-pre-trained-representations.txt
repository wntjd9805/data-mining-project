Choose Action w/ Min Distance!The ﬁeld of visual representation learning has seen explosive growth in the past years, but its beneﬁts in robotics have been surprisingly limited so far. Prior work uses generic visual representations as a basis to learn (task-speciﬁc) robot action policies (e.g., via be-havior cloning). While the visual representations do accelerate learning, they are primarily used to encode visual observations. Thus, action information has to be derived purely from robot data, which is expensive to collect! In this work, we present a scalable alternative where the visual representations can help directly infer robot actions. We observe that vision encoders express relationships between image observations as distances (e.g., via embedding dot product) that could be used to eﬃciently plan robot behavior. We operationalize this insight and develop a simple algorithm for acquiring a distance function and dynamics predictor, by ﬁne-tuning a pre-trained representation on human collected video sequences. The ﬁnal method is able to substan-tially outperform traditional robot learning baselines (e.g., 70% success v.s. 50% for behavior cloning on pick-place) on a suite of diverse real-world manipula-tion tasks. It can also generalize to novel objects, with-out using any robot demonstrations during train time.For visualizations of the learned policies please check: https://agi-labs.github.io/manipulate-by-seeing/. 