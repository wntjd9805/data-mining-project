Modern object detectors have taken the advantages of backbone networks pre-trained on large scale datasets. Ex-cept for the backbone networks, however, other components such as the detector head and the feature pyramid network (FPN) remain trained from scratch, which hinders the gen-eralization capacity of detectors.In this study, we pro-pose to integrally migrate pre-trained transformer encoder-decoders (imTED) to a detector, constructing a feature ex-traction path which is “fully pre-trained” so that detec-tors’ generalization capacity is maximized. The essential differences between imTED with the baseline detector are twofold: (1) migrating the pre-trained transformer decoder to the detector head while removing the randomly initial-ized FPN from the feature extraction path; and (2) defining a multi-scale feature modulator (MFM) to enhance scale adaptability. Such designs not only reduce randomly initial-ized parameters significantly but also unify detector train-ing with representation learning intendedly. Experiments on the MS COCO object detection dataset show that imTED consistently outperforms its counterparts by ∼2.4 AP. With-out bells and whistles, imTED improves the state-of-the-art of few-shot object detection by up to 7.6 AP. Code is re-leased at https://github.com/LiewFeng/imTED. 