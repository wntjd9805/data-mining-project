Source-Free Video Unsupervised Domain Adaptation (SFVUDA) task consists in adapting an action recognition model, trained on a labelled source dataset, to an unla-belled target dataset, without accessing the actual source data.The previous approaches have attempted to ad-dress SFVUDA by leveraging self-supervision (e.g., enforc-ing temporal consistency) derived from the target data it-self.In this work, we take an orthogonal approach by exploiting “web-supervision” from Large Language-VisionModels (LLVMs), driven by the rationale that LLVMs con-tain a rich world prior surprisingly robust to domain-shift. We showcase the unreasonable effectiveness of in-tegrating LLVMs for SFVUDA by devising an intuitive and parameter-efficient method, which we name DomainAdaptation with Large Language-Vision models (DALL-V), that distills the world prior and complementary source model information into a student network tailored for the target. Despite the simplicity, DALL-V 1 achieves signifi-cant improvement over state-of-the-art SFVUDA methods. 