Increasingly, autoregressive approaches are being used to serialize observed variables based on speciﬁc criteria.The Neural Processes (NPs) model variable distribution as a continuous function and provide quick solutions for differ-ent tasks using a meta-learning framework. This paper pro-poses an autoregressive-based framework for NPs, based on their autoregressive properties. This framework lever-ages the autoregressive stacking effects of various variables to enhance the representation of the latent distribution, con-currently reﬁning local and global relationships within the positional representation through the use of a sliding win-dow mechanism. Autoregression improves function approx-imations in a stacked fashion, thereby raising the upper bound of the optimization. We have designated this frame-work as Autoregressive Neural Processes (AENPs) or Con-ditional Autoregressive Neural Processes (CAENPs). Tradi-tional NP models and their variants aim to capture relation-ships between the context sample points, without addressing either local or global considerations. Speciﬁcally, we cap-ture contextual relationships in the deterministic path and introduce sliding window attention and global attention to reconcile local and global relationships in the context sam-ple points. Autoregressive constraints exist between multi-ple latent variables in the latent paths, thus building a com-plex global structure that allows our model to learn com-plex distributions. Finally, we demonstrate the effectiveness of the NPs or CFANPs models for 1D data, Bayesian opti-mization, and 2D data. 