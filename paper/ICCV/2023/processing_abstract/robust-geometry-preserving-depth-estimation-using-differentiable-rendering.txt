In this study, we address the challenge of 3D scene struc-ture recovery from monocular depth estimation. While tra-ditional depth estimation methods leverage labeled datasets to directly predict absolute depth, recent advancements ad-vocate for mix-dataset training, enhancing generalization across diverse scenes. However, such mixed dataset train-ing yields depth predictions only up to an unknown scale and shift, hindering accurate 3D reconstructions. Exist-ing solutions necessitate extra 3D datasets or geometry-complete depth annotations, constraints that limit their ver-satility. In this paper, we propose a learning framework that*Equal contributions.†Corresponding author. trains models to predict geometry-preserving depth without requiring extra data or annotations. To produce realistic 3D structures, we render novel views of the reconstructed scenes and design loss functions to promote depth estima-tion consistency across different views. Comprehensive ex-periments underscore our framework’s superior generaliza-tion capabilities, surpassing existing state-of-the-art meth-ods on several benchmark datasets without leveraging extra training information. Moreover, our innovative loss func-tions empower the model to autonomously recover domain-specific scale-and-shift coefficients using solely unlabeled images.