We present Contrastive Feature Masking Vision Trans-former (CFM-ViT) - an image-text pretraining methodology that achieves simultaneous learning of image- and region-level representation for open-vocabulary object detection (OVD). Our approach combines the masked autoencoder (MAE) objective into the contrastive learning objective to improve the representation for localization tasks. Un-like standard MAE, we perform reconstruction in the joint image-text embedding space, rather than the pixel space as is customary with the classical MAE method, which causes the model to better learn region-level semantics. More-over, we introduce Positional Embedding Dropout (PED) to address scale variation between image-text pretraining and detection ﬁnetuning by randomly dropping out the po-sitional embeddings during pretraining. PED improves de-tection performance and enables the use of a frozen ViT backbone as a region classiﬁer, preventing the forgetting of open-vocabulary knowledge during detection ﬁnetuning.On LVIS open-vocabulary detection benchmark, CFM-ViT achieves a state-of-the-art 33.9 APr, surpassing the best ap-proach by 7.6 points and achieves better zero-shot detection transfer. Finally, CFM-ViT acquires strong image-level rep-resentation, outperforming the state of the art on 8 out of 12 metrics on zero-shot image-text retrieval benchmarks. 