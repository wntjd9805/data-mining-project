We propose an agglomerative Transformer (AGER) that enables Transformer-based human-object interaction (HOI) detectors to flexibly exploit extra instance-level cues in a single-stage and end-to-end manner for the first time.AGER acquires instance tokens by dynamically clustering patch tokens and aligning cluster centers to instances with textual guidance, thus enjoying two benefits: 1) Integral-ity: each instance token is encouraged to contain all dis-criminative feature regions of an instance, which demon-strates a significant improvement in the extraction of dif-ferent instance-level cues and subsequently leads to a new state-of-the-art performance of HOI detection with 36.75 mAP on HICO-Det. 2) Efficiency: the dynamical clus-tering mechanism allows AGER to generate instance to-kens jointly with the feature learning of the Transformer encoder, eliminating the need of an additional object de-tector or instance decoder in prior methods, thus allow-ing the extraction of desirable extra cues for HOI de-tection in a single-stage and end-to-end pipeline. Con-cretely, AGER reduces GFLOPs by 8.5% and improves FPS by 36%, even compared to a vanilla DETR-like pipeline without extra cue extraction. The code will be available at https://github.com/six6607/AGER.git. 