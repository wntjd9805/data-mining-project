Humans describe the physical world using natural lan-guage to refer to specific 3D locations based on a vast range of properties: visual appearance, semantics, abstract asso-ciations, or actionable affordances. In this work we proposeLanguage Embedded Radiance Fields (LERFs), a method for grounding language embeddings from off-the-shelf mod-els like CLIP into NeRF, which enable these types of open-*Equal contribution, corresponding authors. ended language queries in 3D. LERF learns a dense, multi-scale language field inside NeRF by volume rendering CLIP embeddings along training rays, supervising these embed-dings across training views to provide multi-view consis-tency and smooth the underlying language field. After opti-mization, LERF can extract 3D relevancy maps for a broad range of language prompts interactively in real-time, which has potential use cases in robotics, understanding vision-language models, and interacting with 3D scenes. LERF enables pixel-aligned, zero-shot queries on the distilled 3D 1CLIP embeddings without relying on region proposals or masks, supporting long-tail open-vocabulary queries hier-archically across the volume. See the project website at: https://lerf.io. 