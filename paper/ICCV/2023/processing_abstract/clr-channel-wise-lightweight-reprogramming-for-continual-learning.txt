Continual learning aims to emulate the human abil-ity to continually accumulate knowledge over sequential tasks. The main challenge is to maintain performance on previously learned tasks after learning new tasks, i.e., to avoid catastrophic forgetting. We propose a Channel-wiseLightweight Reprogramming (CLR) approach that helps convolutional neural networks (CNNs) overcome catas-trophic forgetting during continual learning. We show that a CNN model trained on an old task (or self-supervised proxy task) could be “reprogrammed” to solve a new task by using our proposed lightweight (very cheap) reprogram-ming parameter. With the help of CLR, we have a bet-ter stability-plasticity trade-off to solve continual learning problems: To maintain stability and retain previous task ability, we use a common task-agnostic immutable part as the shared “anchor” parameter set. We then add task-specific lightweight reprogramming parameters to reinter-pret the outputs of the immutable parts, to enable plas-ticity and integrate new knowledge. To learn sequential tasks, we only train the lightweight reprogramming param-eters to learn each new task. Reprogramming parameters are task-specific and exclusive to each task, which makes our method immune to catastrophic forgetting. To mini-mize the parameter requirement of reprogramming to learn new tasks, we make reprogramming lightweight by only ad-justing essential kernels and learning channel-wise linear mappings from anchor parameters to task-specific domain knowledge. We show that, for general CNNs, the CLR pa-rameter increase is less than 0.6% for any new task. Our method outperforms 13 state-of-the-art continual learning baselines on a new challenging sequence of 53 image clas-sification datasets. Code and data are in the top link. 