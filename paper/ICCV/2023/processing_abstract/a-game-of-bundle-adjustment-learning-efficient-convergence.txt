Bundle adjustment is the common way to solve localiza-tion and mapping. It is an iterative process in which a sys-tem of non-linear equations is solved using two optimization methods, weighted by a damping factor. In the classic ap-proach, the latter is chosen heuristically by the Levenberg-Marquardt algorithm on each iteration. This might take many iterations, making the process computationally ex-pensive, which might be harmful to real-time applications.We propose to replace this heuristic by viewing the prob-lem in a holistic manner, as a game, and formulating it as a reinforcement-learning task. We set an environment which solves the non-linear equations and train an agent to choose the damping factor in a learned manner. We demonstrate that our approach considerably reduces the number of it-erations required to reach the bundle adjustmentâ€™s conver-gence, on both synthetic and real-life scenarios. We show that this reduction benefits the classic approach and can be integrated with other bundle adjustment acceleration meth-ods. 