We propose ImGeoNet, a multi-view image-based 3D ob-ject detection framework that models a 3D space by an image-induced geometry-aware voxel representation. Un-like previous methods which aggregate 2D features into 3D voxels without considering geometry, ImGeoNet learns to induce geometry from multi-view images to alleviate the confusion arising from voxels of free space, and during the inference phase, only images from multiple views are required. Besides, a powerful pre-trained 2D feature ex-tractor can be leveraged by our representation, leading to a more robust performance. To evaluate the effectiveness of ImGeoNet, we conduct quantitative and qualitative ex-periments on three indoor datasets, namely ARKitScenes,ScanNetV2, and ScanNet200. The results demonstrate thatImGeoNet outperforms the current state-of-the-art multi-view image-based method, ImVoxelNet, on all three datasets in terms of detection accuracy.In addition, ImGeoNet shows great data efficiency by achieving results compara-ble to ImVoxelNet with 100 views while utilizing only 40 views. Furthermore, our studies indicate that our proposed image-induced geometry-aware representation can enable image-based methods to attain superior detection accuracy than the seminal point cloud-based method, VoteNet, in two practical scenarios: (1) scenarios where point clouds are sparse and noisy, such as in ARKitScenes, and (2) scenar-ios involve diverse object classes, particularly classes of small objects, as in the case in ScanNet200. Project page: https://ttaoretw.github.io/imgeonet.*Work done in Amazon.Figure 1. Geometry-aware voxel representation. (Top part) In contrast to prior works [56] (top left) that disregard the underly-ing geometry, our proposed ImGeoNet (top center) successfully preserves the geometric structure with respect to the ground truth (top right) while effectively reducing the number of voxels in free space.In the visualization of ImGeoNet, voxels with a surface probability exceeding a predefined threshold are retained, other-wise removed. The color of each voxel is determined by averaging the colors of ground truth point clouds within the voxel. Missed free-space voxels are marked as cyan. (Bottom part) We present the detection results using bounding cubes that are color-coded based on the predicted categories. 