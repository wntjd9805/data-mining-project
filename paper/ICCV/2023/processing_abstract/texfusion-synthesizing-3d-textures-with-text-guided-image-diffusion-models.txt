We present TexFusion (Texture Diffusion), a new method to synthesize textures for given 3D geometries, using large-scale text-guided image diffusion models. In contrast to re-cent works that leverage 2D text-to-image diffusion models to distill 3D objects using a slow and fragile optimization process, TexFusion introduces a new 3D-consistent gener-ation technique speciﬁcally designed for texture synthesis that employs regular diffusion model sampling on different 2D rendered views. Speciﬁcally, we leverage latent diffu-sion models, apply the diffusion model’s denoiser on a set of 2D renders of the 3D object, and aggregate the differ-ent denoising predictions on a shared latent texture map.Final output RGB textures are produced by optimizing an intermediate neural color ﬁeld on the decodings of 2D ren-ders of the latent texture. We thoroughly validate TexFu-sion and show that we can efﬁciently generate diverse, high quality and globally coherent textures. We achieve state-of-the-art text-guided texture synthesis performance using only image diffusion models, while avoiding the pitfalls of previ-ous distillation-based methods. The text-conditioning offers detailed control and we also do not rely on any ground truth 3D textures for training. This makes our method versatile∗ Equal contribution. and applicable to a broad range of geometry and texture types. We hope that TexFusion will advance AI-based tex-turing of 3D assets for applications in virtual reality, game design, simulation, and more. 