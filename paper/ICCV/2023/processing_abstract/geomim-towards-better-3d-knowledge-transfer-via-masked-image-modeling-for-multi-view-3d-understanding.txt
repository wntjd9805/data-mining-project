Multi-view camera-based 3D detection is a challenging problem in computer vision. Recent works leverage a pre-trained LiDAR detection model to transfer knowledge to a camera-based student network. However, we argue that there is a major domain gap between the LiDAR BEV features and the camera-based BEV features, as they have different char-acteristics and are derived from different sources. In this paper, we propose Geometry Enhanced Masked Image Mod-eling (GeoMIM) to transfer the knowledge of the LiDAR model in a pretrain-finetune paradigm for improving the multi-view camera-based 3D detection. GeoMIM is a multi-camera vision transformer with Cross-View Attention (CVA) blocks that uses LiDAR BEV features encoded by the pre-trained BEV model as learning targets. During pretraining,GeoMIM’s decoder has a semantic branch completing dense perspective-view features and the other geometry branch re-constructing dense perspective-view depth maps. The depth branch is designed to be camera-aware by inputting the camera’s parameters for better transfer capability. Exten-sive results demonstrate that GeoMIM outperforms existing methods on nuScenes benchmark, achieving state-of-the-art performance for camera-based 3D object detection and 3D segmentation. 