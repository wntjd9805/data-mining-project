Deep learning has achieved tremendous success in re-cent years, but most of these successes are built on an independent and identically distributed (IID) assumption.This somewhat hinders the application of deep learning to the more challenging out-of-distribution (OOD) scenarios.Although many OOD methods have been proposed to ad-dress this problem and have obtained good performance on testing data that is of major shifts with training dis-tributions, interestingly, we experimentally find that these methods achieve excellent OOD performance by making a great sacrifice of the IID performance. We call this find-ing the IID-OOD dilemma. Clearly, in real-world applica-tions, distribution shifts between training and testing data are often uncertain, where shifts could be minor, and even close to the IID scenario, and thus it is truly important to design a deep model with the balanced generalization ability between IID and OOD. To this end, in this paper, we investigate an intriguing problem of balancing IID andOOD generalizations and propose a novel Model Agnostic adaPters (MAP) method, which is more reliable and effec-tive for distribution-shift-agnostic real-world data. Our key technical contribution is to use auxiliary adapter layers to incorporate the inductive bias of IID into OOD methods. To achieve this goal, we apply a bilevel optimization to explic-itly model and optimize the coupling relationship between the OOD model and auxiliary adapter layers. We also the-oretically give a first-order approximation to save compu-tational time. Experimental results on six datasets success-fully demonstrate that MAP can greatly improve the perfor-mance of IID while achieving good OOD performance. 