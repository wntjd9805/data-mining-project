Existing text-to-image generation approaches have set high standards for photorealism and text-image corre-spondence, largely beneﬁting from web-scale text-image datasets, which can include up to 5 billion pairs. How-ever, text-to-image generation models trained on domain-speciﬁc datasets, such as urban scenes, medical images, and faces, still suffer from low text-image correspon-dence due to the lack of text-image pairs. Additionally, collecting billions of text-image pairs for a speciﬁc do-main can be time-consuming and costly. Thus, ensur-ing high text-image correspondence without relying on web-scale text-image datasets remains a challenging task.In this paper, we present a novel approach for enhanc-ing text-image correspondence by leveraging available se-mantic layouts.Speciﬁcally, we propose a Gaussian-categorical diffusion process that simultaneously gener-ates both images and corresponding layout pairs. Our experiments reveal that we can guide text-to-image gen-eration models to be aware of the semantics of different image regions, by training the model to generate seman-tic labels for each pixel. We demonstrate that our ap-proach achieves higher text-image correspondence com-pared to existing text-to-image generation approaches in the Multi-Modal CelebA-HQ and the Cityscapes dataset, where text-image pairs are scarce. Codes are available at https://pmh9960.github.io/research/GCDP. 