In this paper, we introduce a novel 3D-aware image gen-eration method that leverages 2D diffusion models. We formulate the 3D-aware image generation task as multi-view 2D image set generation, and further to a sequential unconditional–conditional multiview image generation pro-cess. This allows us to utilize 2D diffusion models to boost the generative modeling power of the method. Additionally, we incorporate depth information from monocular depth es-timators to construct the training data for the conditional diffusion model using only still images.We train our method on a large-scale unstructured 2D image dataset, i.e., ImageNet, which is not addressed by previous methods. It produces high-quality images that sig-nificantly outperform prior methods. Furthermore, our ap-proach showcases its capability to generate instances with large view angles, even though the training images are diverse and unaligned, gathered from “in-the-wild” real-world environments. 1 