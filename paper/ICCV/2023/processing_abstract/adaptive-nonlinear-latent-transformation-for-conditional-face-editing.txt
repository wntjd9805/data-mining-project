Recent works for face editing usually manipulate the la-tent space of StyleGAN via the linear semantic directions.However, they usually suffer from the entanglement of facial attributes, need to tune the optimal editing strength, and are limited to binary attributes with strong supervision sig-nals. This paper proposes a novel adaptive nonlinear latent transformation for disentangled and conditional face edit-ing, termed AdaTrans. Speciﬁcally, our AdaTrans divides the manipulation process into several ﬁner steps; i.e., the direction and size at each step are conditioned on both the facial attributes and the latent codes. In this way, AdaTrans describes an adaptive nonlinear transformation trajectory to manipulate the faces into target attributes while keeping other attributes unchanged. Then, AdaTrans leverages a predeﬁned density model to constrain the learned trajec-tory in the distribution of latent codes by maximizing the∗Corresponding author likelihood of transformed latent code. Moreover, we also propose a disentangled learning strategy under a mutual in-formation framework to eliminate the entanglement among attributes, which can further relax the need for labeled data.Consequently, AdaTrans enables a controllable face edit-ing with the advantages of disentanglement, ﬂexibility with non-binary attributes, and high ﬁdelity. Extensive experi-mental results on various facial attributes demonstrate the qualitative and quantitative effectiveness of the proposedAdaTrans over existing state-of-the-art methods, especially in the most challenging scenarios with a large age gap and few labeled examples. The source code is available at https://github.com/Hzzone/AdaTrans. 