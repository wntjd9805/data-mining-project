Controllable human image generation (HIG) has numer-ous real-life applications. State-of-the-art solutions, such as ControlNet and T2I-Adapter, introduce an additional learnable branch on top of the frozen pre-trained stable dif-fusion (SD) model, which can enforce various conditions, including skeleton guidance of HIG. While such a plug-and-play approach is appealing, the inevitable and uncer-tain conflicts between the original images produced from the frozen SD branch and the given condition incur signifi-cant challenges for the learnable branch, which essentially conducts image feature editing for condition enforcement.*Equal contribution. ‡ Work done during an internship at IDEA.†Corresponding author.In this work, we propose a native skeleton-guided dif-fusion model for controllable HIG called HumanSD. In-stead of performing image editing with dual-branch dif-fusion, we fine-tune the original SD model using a novel heatmap-guided denoising loss. This strategy effectively and efficiently strengthens the given skeleton condition dur-ing model training while mitigating the catastrophic for-getting effects. HumanSD is fine-tuned on the assem-bly of three large-scale human-centric datasets with text-image-pose information, two of which are established in this work. Experimental results show that HumanSD out-performs ControlNet in terms of pose control and image quality, particularly when the given skeleton guidance is sophisticated. Code and data are available at: https://idea-research.github.io/HumanSD/.