CLIP models have demonstrated impressively high zero-shot recognition accuracy, however, their fine-tuning per-formance on downstream vision tasks is sub-optimal. Con-trarily, masked image modeling (MIM) performs exception-ally for fine-tuning on downstream tasks, despite the ab-sence of semantic labels during training. We note that the two tasks have different ingredients: image-level tar-gets versus token-level targets, a cross-entropy loss ver-sus a regression loss, and full-image inputs versus partial-image inputs. To mitigate the differences, we introduce a classical feature map distillation framework, which can si-multaneously inherit the semantic capability of CLIP mod-els while constructing a task incorporated key ingredi-ents of MIM. Experiments suggest that the feature map distillation approach significantly boosts the fine-tuning performance of CLIP models on several typical down-stream vision tasks. We also observe that the approach yields new CLIP representations which share some diag-nostic properties with those of MIM. Furthermore, the fea-ture map distillation approach generalizes to other pre-training models, such as DINO, DeiT and SwinV2-G, reaching a new record of 64.2 mAP on COCO object detection with +1.1 improvement. The code and mod-els are publicly available at https://github.com/SwinTransformer/Feature-Distillation. 