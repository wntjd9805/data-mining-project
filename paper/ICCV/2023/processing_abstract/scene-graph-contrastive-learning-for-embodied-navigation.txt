Training effective embodied AI agents often involves ex-pert imitation, specialized components such as maps, or leveraging additional sensors for depth and localization.Another approach is to use neural architectures alongside self-supervised objectives which encourage better represen-tation learning. However, in practice, there are few guar-antees that these self-supervised objectives encode task-relevant information. We propose the Scene Graph Con-trastive (SGC) loss, which uses scene graphs as training-only supervisory signals. The SGC loss does away with explicit graph decoding and instead uses contrastive learn-ing to align an agent’s representation with a rich graphi-cal encoding of its environment. The SGC loss is simple to implement and encourages representations that encode ob-jects’ semantics, relationships, and history. By using theSGC loss, we attain gains on three embodied tasks: Ob-ject Navigation, Multi-Object Navigation, and Arm PointNavigation. Finally, we present studies and analyses which demonstrate the ability of our trained representation to en-code semantic cues about the environment. 