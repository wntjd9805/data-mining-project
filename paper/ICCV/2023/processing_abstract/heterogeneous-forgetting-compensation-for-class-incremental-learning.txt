Class-incremental learning (CIL) has achieved remark-able successes in learning new classes consecutively while overcoming catastrophic forgetting on old categories. How-ever, most existing CIL methods unreasonably assume that all old categories have the same forgetting pace, and neglect negative influence of forgetting heterogeneity among differ-ent old classes on forgetting compensation. To surmount the above challenges, we develop a novel HeterogeneousForgetting Compensation (HFC) model, which can resolve heterogeneous forgetting of easy-to-forget and hard-to-forget old categories from both representation and gradient as-pects. Specifically, we design a task-semantic aggrega-tion block to alleviate heterogeneous forgetting from rep-resentation aspect. It aggregates local category informa-tion within each task to learn task-shared global represen-tations. Moreover, we develop two novel plug-and-play losses: a gradient-balanced forgetting compensation loss and a gradient-balanced relation distillation loss to alleviate forgetting from gradient aspect. They consider gradient-balanced compensation to rectify forgetting heterogeneity of old categories and heterogeneous relation consistency.Experiments on several representative datasets illustrate ef-fectiveness of our HFC model. The code is available at https://github.com/JiahuaDong/HFC. 