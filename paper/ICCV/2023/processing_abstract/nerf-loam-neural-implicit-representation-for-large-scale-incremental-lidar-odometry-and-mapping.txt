Simultaneously odometry and mapping using LiDAR data is an important task for mobile systems to achieve full autonomy in large-scale environments. However, most existing LiDAR-based methods prioritize tracking quality over reconstruction quality. Although the recently devel-oped neural radiance Ô¨Åelds (NeRF) have shown promis-ing advances in implicit reconstruction for indoor environ-ments, the problem of simultaneous odometry and mapping for large-scale scenarios using incremental LiDAR data re-mains unexplored. To bridge this gap, in this paper, we pro-pose a novel NeRF-based LiDAR odometry and mapping approach, NeRF-LOAM, consisting of three modules neural odometry, neural mapping, and mesh reconstruction. All these modules utilize our proposed neural signed distance function, which separates LiDAR points into ground and non-ground points to reduce Z-axis drift, optimizes odome-try and voxel embeddings concurrently, and in the end gen-erates dense smooth mesh maps of the environment. More-over, this joint optimization allows our NeRF-LOAM to be pre-trained free and exhibit strong generalization abilities when applied to different environments. Extensive eval-uations on three publicly available datasets demonstrate that our approach achieves state-of-the-art odometry and mapping performance, as well as a strong generalization in large-scale environments utilizing LiDAR data. Fur-thermore, we perform multiple ablation studies to validate the effectiveness of our network design. The implementa-tion of our approach will be made available at https://github.com/JunyuanDeng/NeRF-LOAM . 