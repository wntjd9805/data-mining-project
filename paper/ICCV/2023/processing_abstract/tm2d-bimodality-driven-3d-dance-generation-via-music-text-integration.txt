We propose a novel task for generating 3D dance move-ments that simultaneously incorporate both text and mu-sic modalities. Unlike existing works that generate dance movements using a single modality such as music, our goal is to produce richer dance movements guided by the instruc-tive information provided by the text. However, the lack of paired motion data with both music and text modalities limits the ability to generate dance movements that inte-grate both. To alleviate this challenge, we propose to uti-lize a 3D human motion VQ-VAE to project the motions of the two datasets into a latent space consisting of quantized vectors, which effectively mix the motion tokens from the two datasets with different distributions for training. Ad-ditionally, we propose a cross-modal transformer to inte-grate text instructions into motion generation architecture for generating 3D dance movements without degrading the performance of music-conditioned dance generation. To better evaluate the quality of the generated motion, we in-troduce two novel metrics, namely Motion Prediction Dis-tance (MPD) and Freezing Score (FS), to measure the co-herence and freezing percentage of the generated motion.Extensive experiments show that our approach can gener-*Equal contribution: gongkehong@u.nus.edu, dongze@nus.edu.sgâ€ Corresponding author: xinchao@nus.edu.sg ate realistic and coherent dance movements conditioned on both text and music while maintaining comparable perfor-mance with the two single modalities. Code is available at https://garfield-kh.github.io/TM2D/. 