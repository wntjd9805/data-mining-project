Dataset distillation aims to synthesize small datasets with little information loss from original large-scale ones for reducing storage and training costs. Recent state-of-the-art methods mainly constrain the sample synthesis pro-cess by matching synthetic images and the original ones regarding gradients, embedding distributions, or training trajectories. Although there are various matching objec-tives, currently the strategy for selecting original images is limited to naive random sampling. We argue that ran-dom sampling overlooks the evenness of the selected sam-ple distribution, which may result in noisy or biased match-ing targets. Besides, the sample diversity is also not con-strained by random sampling. These factors together lead to optimization instability in the distilling process and de-grade the training efﬁciency. Accordingly, we propose a novel matching strategy named as Dataset distillation byREpresentAtive Matching (DREAM), where only represen-tative original images are selected for matching. DREAM is able to be easily plugged into popular dataset distilla-tion frameworks and reduce the distilling iterations by more than 8 times without performance drop. Given sufﬁcient training time, DREAM further provides signiﬁcant improve-ments and achieves state-of-the-art performances. 