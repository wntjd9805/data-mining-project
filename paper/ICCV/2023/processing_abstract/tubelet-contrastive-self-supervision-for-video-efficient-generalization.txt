We propose a self-supervised method for learning motion-focused video representations. Existing approaches minimize distances between temporally augmented videos, which maintain high spatial similarity. We instead propose to learn similarities between videos with identical local mo-tion dynamics but an otherwise different appearance. We do so by adding synthetic motion trajectories to videos which we refer to as tubelets. By simulating different tubelet mo-tions and applying transformations, such as scaling and ro-tation, we introduce motion patterns beyond what is present in the pretraining data. This allows us to learn a video rep-resentation that is remarkably data efficient: our approach maintains performance when using only 25% of the pre-training videos. Experiments on 10 diverse downstream set-tings demonstrate our competitive performance and gener-alizability to new domains and fine-grained actions. Code is available at https://github.com/fmthoker/tubelet-contrast. 