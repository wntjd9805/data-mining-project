Explaining deep models in a human-understandable way has been explored by many works that mostly explain why an input causes a corresponding prediction (i.e., Why P?).However, seldom they could handle those more complex causal questions like “Why P rather than Q?” and “Why one is P, while another is Q?”, which would better help humans understand the behavior of deep models. Consid-ering the insufﬁcient study on such complex causal ques-tions, we make the ﬁrst attempt to explain different causal questions by contrastive explanations in a uniﬁed frame-work, i.e., Counterfactual Contrastive Explanation (CCE), which visually and intuitively explains the aforementioned questions via a novel positive-negative saliency-based ex-planation scheme. More speciﬁcally, we propose a content-aware counterfactual perturbing algorithm to stimulate contrastive examples, from which a pair of positive and negative saliency maps could be derived to contrastively explain why P (positive class) rather than Q (negative class). Beyond existing works, our counterfactual perturba-tion meets the principles of validity, sparsity, and data dis-tribution closeness at the same time. In addition, by slightly adjusting the objective of perturbation, our framework can adapt to different causal questions. Extensive experimen-tal evaluation demonstrates the effectiveness and superior performance of the proposed CCE on different benchmark metrics for interpretability, including Sanity Check, ClassDeviation Score and Insertion-Deletion tests. A user study is conducted and the results show that user conﬁdence is in-creasing signiﬁcantly when presented with CCE compared to standard saliency map baselines. 