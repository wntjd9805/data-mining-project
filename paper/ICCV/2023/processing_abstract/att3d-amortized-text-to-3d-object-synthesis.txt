Text-to-3D modelling has seen exciting progress by com-bining generative text-to-image models with image-to-3D methods like Neural Radiance Fields. DreamFusion re-cently achieved high-quality results but requires a lengthy, per-prompt optimization to create 3D objects. To address this, we amortize optimization over text prompts by training on many prompts simultaneously with a uniﬁed model, in-stead of separately. With this, we share computation across a prompt set, training in less time than per-prompt optimiza-tion. Our framework – Amortized Text-to-3D (ATT3D) – enables knowledge sharing between prompts to generalize to unseen setups and smooth interpolations between text for novel assets and simple animations. 