Deep neural networks are susceptible to adversarial ex-amples, posing a signiﬁcant security risk in critical applica-tions. Adversarial Training (AT) is a well-established tech-nique to enhance adversarial robustness, but it often comes at the cost of decreased generalization ability. This paper proposes Robustness Critical Fine-Tuning (RiFT), a novel approach to enhance generalization without compromising adversarial robustness. The core idea of RiFT is to exploit the redundant capacity for robustness by ﬁne-tuning the ad-versarially trained model on its non-robust-critical module.To do so, we introduce module robust criticality (MRC), a measure that evaluates the signiﬁcance of a given mod-ule to model robustness under worst-case weight perturba-tions. Using this measure, we identify the module with the lowest MRC value as the non-robust-critical module andﬁne-tune its weights to obtain ﬁne-tuned weights. Subse-quently, we linearly interpolate between the adversarially trained weights and ﬁne-tuned weights to derive the optimalﬁne-tuned model weights. We demonstrate the efﬁcacy ofRiFT on ResNet18, ResNet34, and WideResNet34-10 mod-els trained on CIFAR10, CIFAR100, and Tiny-ImageNet datasets. Our experiments show that RiFT can signiﬁcantly improve both generalization and out-of-distribution robust-ness by around 1.5% while maintaining or even slightly enhancing adversarial robustness. Code is available at https://github.com/Immortalise/RiFT. 