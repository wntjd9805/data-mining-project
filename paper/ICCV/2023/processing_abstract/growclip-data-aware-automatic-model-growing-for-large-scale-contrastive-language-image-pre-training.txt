Cross-modal pre-training has shown impressive perfor-mance on a wide range of downstream tasks, benefiting from massive image-text pairs collected from the Internet.In practice, online data are growing constantly, highlighting the importance of the ability of pre-trained model to learn from data that is continuously growing. Existing works on cross-modal pre-training mainly focus on training a net-work with fixed architecture. However, it is impractical to limit the model capacity when considering the continu-ously growing nature of pre-training data in real-world ap-plications. On the other hand, it is important to utilize the knowledge in the current model to obtain efficient training and better performance. To address the above issues, in this paper, we propose GrowCLIP, a data-driven automatic model growing algorithm for contrastive language-image pre-training with continuous image-text pairs as input. Spe-cially, we adopt a dynamic growth space and seek out the optimal architecture at each growth step to adapt to online learning scenarios. And the shared encoder is proposed in our growth space to enhance the degree of cross-modal fu-sion. Besides, we explore the effect of growth in different dimensions, which could provide future references for the design of cross-modal model architecture. Finally, we em-ploy parameter inheriting with momentum (PIM) to main-tain the previous knowledge and address the issue of the local minimum dilemma. Compared with the existing meth-ods, GrowCLIP improves 2.3% average top-1 accuracy on zero-shot image classification of 9 downstream tasks. As for zero-shot image retrieval, GrowCLIP can improve 1.2% for top-1 image-to-text recall on Flickr30K dataset.*Corresponding author.Figure 1. Top-1 accuracy (%) of zero-shot image classification onImageNet of GrowCLIP and baselines during training at step 4, where the horizontal dotted lines mean the process of supernet training. Our GrowCLIP has the best performance and is more efficient compared with other baselines. 