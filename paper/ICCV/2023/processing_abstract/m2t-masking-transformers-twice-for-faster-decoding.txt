We show how bidirectional transformers trained for masked token prediction can be applied to neural image compression to achieve state-of-the-art results. Such mod-els were previously used for image generation by pro-gressivly sampling groups of masked tokens according to uncertainty-adaptive schedules. Unlike these works, we demonstrate that predeﬁned, deterministic schedules per-form as well or better for image compression. This insight allows us to use masked attention during training in ad-dition to masked inputs, and activation caching during in-ference, to signiﬁcantly speed up our models ( higher⇡ inference speed) at a small increase in bitrate. 1⇥ 4 