Self-supervised monocular depth estimation (SSMDE) aims at predicting the dense depth maps of monocular im-ages, by learning to minimize a photometric loss using spatially neighboring image pairs during training. WhileSSMDE offers a significant scalability advantage over su-pervised approaches, it performs poorly on reflective sur-faces as the photometric constancy assumption of the pho-tometric loss is violated. We note that the appearance of reflective surfaces is view-dependent and often there are views of such surfaces in the training data that are not contaminated by strong specular reflections. Thus, reflec-tive surfaces can be accurately reconstructed by aggregat-ing the predicted depth of these views. Motivated by this observation, we propose 3D distillation: a novel training framework that utilizes the projected depth of reconstructed reflective surfaces to generate reasonably accurate depth pseudo-labels. To identify those surfaces automatically, we employ an uncertainty-guided depth fusion method, com-bining the smoother and more accurate projected depth on reflective surfaces and the detailed predicted depth else-where. In our experiments using the ScanNet and 7-Scenes datasets, we show that 3D distillation not only significantly improves the prediction accuracy, especially on the prob-lematic surfaces, but also that it generalizes well over var-ious underlying network architectures and to new datasets. 