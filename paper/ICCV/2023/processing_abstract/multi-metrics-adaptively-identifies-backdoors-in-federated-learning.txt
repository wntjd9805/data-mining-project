The decentralized and privacy-preserving nature of fed-erated learning (FL) makes it vulnerable to backdoor at-tacks aiming to manipulate the behavior of the resulting model on specific adversary-chosen inputs. However, most existing defenses based on statistical differences take effect only against specific attacks, especially when the malicious gradients are similar to benign ones or the data are highly non-independent and identically distributed (non-IID). In this paper, we revisit the distance-based defense methods and discover that i) Euclidean distance becomes meaning-less in high dimensions and ii) malicious gradients with di-verse characteristics cannot be identified by a single met-ric. To this end, we present a simple yet effective defense strategy with multi-metrics and dynamic weighting to iden-tify backdoors adaptively. Furthermore, our novel defense has no reliance on predefined assumptions over attack set-tings or data distributions and little impact on benign per-formance. To evaluate the effectiveness of our approach, we conduct comprehensive experiments on different datasets under various attack settings, where our method achieves the best defensive performance. For instance, we achieve the lowest backdoor accuracy of 3.06% under the most dif-ficult Edge-case PGD, showing significant superiority over previous defenses. The experiments also demonstrate that our method can be well-adapted to a wide range of non-IID degrees without sacrificing the benign performance. 