Visual-Language Models (VLMs) have significantly ad-vanced video action recognition. Supervised by the seman-tics of action labels, recent works adapt the visual branch of VLMs to learn video representations. Despite the effec-tiveness proved by these works, we believe that the potential of VLMs has yet to be fully harnessed. In light of this, we exploit the semantic units (SU) hiding behind the action la-bels and leverage their correlations with fine-grained items in frames for more accurate action recognition. SUs are entities extracted from the language descriptions of the en-tire action set, including body parts, objects, scenes, and motions. To further enhance the alignments between vi-sual contents and the SUs, we introduce a multi-region at-tention module (MRA) to the visual branch of the VLM.The MRA allows the perception of region-aware visual fea-tures beyond the original global feature. Our method adap-tively attends to and selects relevant SUs with visual fea-tures of frames. With a cross-modal decoder, the selectedSUs serve to decode spatiotemporal video representations.In summary, the SUs as the medium can boost discrim-inative ability and transferability. Specifically, in fully-supervised learning, our method achieved 87.8% top-1 ac-curacy on Kinetics-400. In K=2 few-shot experiments, our method surpassed the previous state-of-the-art by +7.1% and +15.0% on HMDB-51 and UCF-101, respectively. 