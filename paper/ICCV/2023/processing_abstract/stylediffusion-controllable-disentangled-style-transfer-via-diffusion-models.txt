Content and style (C-S) disentanglement is a fundamen-tal problem and critical challenge of style transfer. Existing approaches based on explicit deﬁnitions (e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable nor easy to control, resulting in entangled representations and less satisfying results. In this paper, we propose a newC-S disentangled framework for style transfer without using previous assumptions. The key insight is to explicitly extract the content information and implicitly learn the comple-mentary style information, yielding interpretable and con-trollable C-S disentanglement and style transfer. A simple yet effective CLIP-based style disentanglement loss coor-dinated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space. By further lever-aging the powerful style removal and generative ability of diffusion models, our framework achieves superior results than state of the art and ﬂexible C-S disentanglement and trade-off control. Our work provides new insights into theC-S disentanglement in style transfer and demonstrates the potential of diffusion models for learning well-disentangledC-S characteristics. 