Continual semantic segmentation (CSS) aims to extend an existing model to tackle unseen tasks while retaining its old knowledge. Naively fine-tuning the old model on new data leads to catastrophic forgetting. A common solution is knowledge distillation (KD), where the output distribution of the new model is regularized to be similar to that of the old model. However, in CSS, this is challenging because of the background shift issue. Existing KD-based CSS meth-ods continue to suffer from confusion between the back-ground and novel classes since they fail to establish a re-liable class correspondence for distillation. To address this issue, we propose a new label-guided knowledge distilla-tion (LGKD) loss, where the old model output is expanded and transplanted (with the guidance of the ground truth la-bel) to form a semantically appropriate class correspon-dence with the new model output. Consequently, the useful knowledge from the old model can be effectively distilled into the new model without causing confusion. We con-duct extensive experiments on two prevailing CSS bench-marks, Pascal-VOC and ADE20K, where our LGKD sig-nificantly boosts the performance of three competing meth-ods, especially on novel mIoU by up to +76%, setting new state-of-the-art. Finally, to further demonstrate its gen-eralization ability, we introduce the first CSS benchmark for 3D point cloud based on ScanNet, along with several re-implemented baselines for comparison. Experiments show that LGKD is versatile in both 2D and 3D modali-ties without requiring ad hoc design. Codes are available at https://github.com/Ze-Yang/LGKD. 