We introduce a self-supervised training strategy for burst super-resolution that only uses noisy low-resolution bursts during training. Our approach eliminates the need to care-fully tune synthetic data simulation pipelines, which of-ten do not match real-world image statistics. Compared to weakly-paired training strategies, which require noisy smartphone burst photos of static scenes, paired with a clean reference obtained from a tripod-mounted DSLR cam-era, our approach is more scalable, and avoids the color mismatch between the smartphone and DSLR. To achieve this, we propose a new self-supervised objective that uses a forward imaging model to recover a high-resolution im-age from aliased high frequencies in the burst. Our ap-proach does not require any manual tuning of the forward modelâ€™s parameters; we learn them from data. Further-more, we show our training strategy is robust to dynamic scene motion in the burst, which enables training burst super-resolution models using in-the-wild data. Extensive experiments on real and synthetic data show that, despite only using noisy bursts during training, models trained with our self-supervised strategy match, and sometimes surpass, the quality of fully-supervised baselines trained with syn-thetic data or weakly-paired ground-truth. Finally, we show our training strategy is general using four different burst super-resolution architectures. 