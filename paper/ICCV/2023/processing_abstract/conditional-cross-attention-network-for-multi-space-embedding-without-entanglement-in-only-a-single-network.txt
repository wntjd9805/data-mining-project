Many studies in vision tasks have aimed to create ef-fective embedding spaces for single-label object prediction within an image. However, in reality, most objects possess multiple specific attributes, such as shape, color, and length, with each attribute composed of various classes. To apply models in real-world scenarios, it is essential to be able to distinguish between the granular components of an ob-ject. Conventional approaches to embedding multiple spe-cific attributes into a single network often result in entangle-ment, where fine-grained features of each attribute cannot be identified separately. To address this problem, we pro-pose a Conditional Cross-Attention Network that induces disentangled multi-space embeddings for various specific attributes with only a single backbone. Firstly, we employ a cross-attention mechanism to fuse and switch the infor-mation of conditions (specific attributes), and we demon-strate its effectiveness through a diverse visualization ex-ample. Secondly, we leverage the vision transformer for the first time to a fine-grained image retrieval task and present a simple yet effective framework compared to ex-isting methods. Unlike previous studies where performance varied depending on the benchmark dataset, our proposed method achieved consistent state-of-the-art performance on the FashionAI, DARN, DeepFashion, and Zappos50K benchmark datasets. 