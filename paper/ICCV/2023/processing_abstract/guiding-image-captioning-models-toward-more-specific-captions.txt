Image captioning is conventionally formulated as the task of generating captions for images that match the dis-tribution of reference image-caption pairs. However, refer-ence captions in standard captioning datasets are short and may not uniquely identify the images they describe. These problems are further exacerbated when models are trained directly on image-alt text pairs collected from the inter-net. In this work, we show that it is possible to generate more speciﬁc captions with minimal changes to the train-ing process. We implement classiﬁer-free guidance [14] for an autoregressive captioning model by ﬁne-tuning it to estimate both conditional and unconditional distributions over captions. The guidance scale applied at decoding controls a trade-off between maximizing p(caption|image) and p(image|caption). Compared to standard greedy de-coding, decoding with a guidance scale of 2 substantially improves reference-free metrics such as CLIPScore (0.808 vs. 0.775) and caption→image retrieval performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We further explore the use of language models to guide the decoding process, obtaining small improvements over the Pareto frontier of reference-free vs. reference-based captioning metrics that arises from classiﬁer-free guidance, and substantially improving the quality of captions generated from a model trained only on minimally curated web data. 