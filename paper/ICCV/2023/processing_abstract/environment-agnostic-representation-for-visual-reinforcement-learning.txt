Generalization capability of vision-based deep reinforce-ment learning (RL) is indispensable to deal with dynamic environment changes that exist in visual observations. The high-dimensional space of the visual input, however, imposes challenges in adapting an agent to unseen environments.In this work, we propose Environment Agnostic Reinforce-ment learning (EAR), which is a compact framework for domain generalization of the visual deep RL. Environment-agnostic features (EAFs) are extracted by leveraging three novel objectives based on feature factorization, reconstruc-tion, and episode-aware state shifting, so that policy learning is accomplished only with vital features. EAR is a simple single-stage method with a low model complexity and a fast inference time, ensuring a high reproducibility, while attain-ing state-of-the-art performance in the DeepMind ControlSuite and DrawerWorld benchmarks. Code is available at: https://github.com/doihye/EAR. 