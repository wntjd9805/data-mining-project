Task driven object detection aims to detect object in-stances suitable for affording a task in an image. Its chal-lenge lies in object categories available for the task being too diverse to be limited to a closed set of object vocabu-lary for traditional object detection. Simply mapping cat-egories and visual features of common objects to the taskIn this paper, we propose cannot address the challenge. to explore fundamental affordances rather than object cat-egories, i.e., common attributes that enable different ob-jects to accomplish the same task. Moreover, we propose a novel multi-level chain-of-thought prompting (MLCoT) to extract the affordance knowledge from large language mod-els, which contains multi-level reasoning steps from task to object examples to essential visual attributes with ratio-nales. Furthermore, to fully exploit knowledge to benefit ob-ject recognition and localization, we propose a knowledge-conditional detection framework, namely CoTDet. It con-ditions the detector from the knowledge to generate object queries and regress boxes. Experimental results demon-strate that our CoTDet outperforms state-of-the-art meth-ods consistently and significantly (+15.6 box AP and +14.8 mask AP) and can generate rationales for why objects are detected to afford the task. 