Deep clustering can optimize representations of in-stances (i.e., representation learning) and explore the in-herent data distribution (i.e., clustering) simultaneously, which demonstrates a superior performance over conven-tional clustering methods with given features. However, the coupled objective implies a trivial solution that all instances collapse to the uniform features. To tackle the challenge, a two-stage training strategy is developed for decoupling, where it introduces an additional pre-training stage for rep-resentation learning and then fine-tunes the obtained model for clustering. Meanwhile, one-stage methods are devel-oped mainly for representation learning rather than cluster-ing, where various constraints for cluster assignments are designed to avoid collapsing explicitly. Despite the success of these methods, an appropriate learning objective tailored for deep clustering has not been investigated sufficiently. In this work, we first show that the prevalent discrimination task in supervised learning is unstable for one-stage clus-tering due to the lack of ground-truth labels and positive instances for certain clusters in each mini-batch. To miti-gate the issue, a novel stable cluster discrimination (SeCu) task is proposed and a new hardness-aware clustering cri-terion can be obtained accordingly. Moreover, a global en-tropy constraint for cluster assignments is studied with effi-cient optimization. Extensive experiments are conducted on benchmark data sets and ImageNet. SeCu achieves state-of-the-art performance on all of them, which demonstrates the effectiveness of one-stage deep clustering. 