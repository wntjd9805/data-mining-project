Pre-trained vision-language models (VLMs) have shown impressive performance on various downstream tasks byIn general, utilizing knowledge learned from large data. the performance of VLMs on target tasks can be further im-proved by prompt tuning, which adds context to the input image or text. By leveraging data from target tasks, vari-ous prompt-tuning methods have been studied in the liter-ature. A key to prompt tuning is the feature space align-ment between two modalities via learnable vectors with model parameters fixed. We observed that the alignment becomes more effective when embeddings of each modal-ity are ‘well-arranged’ in the latent space. Inspired by this observation, we proposed distribution-aware prompt tuning (DAPT) for vision-language models, which is simple yet ef-fective. Specifically, the prompts are learned by maximizing inter-dispersion, the distance between classes, as well as minimizing the intra-dispersion measured by the distance between embeddings from the same class. Our extensive ex-periments on 11 benchmark datasets demonstrate that our method significantly improves generalizability. The code is available at https://github.com/mlvlab/DAPT. 