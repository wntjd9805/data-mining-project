Human pose estimation in videos has wide-ranging practical applications across various fields, many of which require fast inference on resource-scarce devices, neces-sitating the development of efficient and accurate algo-rithms. Previous works have demonstrated the feasibil-ity of exploiting motion continuity to conduct pose esti-mation using sparsely sampled frames with transformer-based models. However, these methods only consider the temporal relation while neglecting spatial attention, and the complexity of dot product self-attention calculations in transformers are quadratically proportional to the em-bedding size. To address these limitations, we proposeMixSynthFormer, a transformer encoder-like model withMLP-based mixed synthetic attention. By mixing synthe-sized spatial and temporal attentions, our model incorpo-rates inter-joint and inter-frame importance and can accu-rately estimate human poses in an entire video sequence from sparsely sampled frames. Additionally, the flexi-ble design of our model makes it versatile for other mo-tion synthesis tasks. Our extensive experiments on 2D/3D pose estimation, body mesh recovery, and motion predic-tion validate the effectiveness and efficiency of MixSynth-Former. The code is available at https://github. com/ireneesun/MixSynthFormer.git 