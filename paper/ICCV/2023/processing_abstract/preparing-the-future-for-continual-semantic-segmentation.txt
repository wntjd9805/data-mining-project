In this study, we focus on Continual Semantic Segmenta-tion (CSS) and present a novel approach to tackle the issue of existing methods struggling to learn new classes. The primary challenge of CSS is to learn new knowledge while retaining old knowledge, which is commonly known as the rigidity-plasticity dilemma. Existing approaches strive to address this by carefully balancing the learning of new and old classes during training on new data. Differently, this work aims to avoid this dilemma fundamentally rather than handling the difficulties involved in it. Specifically, we re-veal that this dilemma mainly arises from the greater fluc-tuation of knowledge for new classes because they have never been learned before the current step. Additionally, the data available in incremental steps are usually inadequate, which can impede the modelâ€™s ability to learn discrimina-tive features for both new and old classes. To address these challenges, we introduce a novel concept of pre-learning for future knowledge. Our approach entails optimizing the fea-ture space and output space for unlabeled data, which thus enables the model to acquire knowledge for future classes.With this approach, updating the model for new classes be-comes as smooth as for old classes, effectively avoiding the rigidity-plasticity dilemma. We conducted extensive exper-iments and the results demonstrate a significant improve-ment in the learning of new classes compared to previous state-of-the-art methods. 