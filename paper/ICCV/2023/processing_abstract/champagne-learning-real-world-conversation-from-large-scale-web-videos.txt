Visual information is central to conversation: body ges-tures and physical behaviour, for example, contribute to meaning that transcends words alone. To date, however, most neural conversational models are limited to just text.We introduceCHAMPAGNE, a generative model of con-versations that can account for visual contexts. To trainCHAMPAGNE, we collect and releaseYTD-18M, a large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that con-verts error-prone automatic transcripts to a cleaner dia-logue format while maintaining meaning.Human evaluation reveals that YTD-18M is more sen-sible and speciﬁc than prior resources (MMDialog [17], 1M dialogues), while maintaining visual-groundedness. Exper-iments demonstrate that 1) CHAMPAGNE learns to con-duct conversation from YTD-18M; and 2) when ﬁne-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations. We release data, models, and code at https://seungjuhan.me/champagne. 