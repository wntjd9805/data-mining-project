This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of manyIn this work, we tasks and can learn new tasks quickly. propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but ef-fective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot per-formance: Despite using less than 0.5% of paired train-ing data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task suc-cessfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient em-bodied agents that can quickly learn many tasks. 1 