Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previ-ous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on back-doored examples, CLIP learns spurious correlations be-tween the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space.Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can signifi-cantly manipulate the modelâ€™s behavior, making it difficult to detect or unlearn such correlations. To address this is-sue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the repre-sentations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervised objectives for in-dividual modalities can significantly reduce the impact of the backdoor attack. Additionally, we show that supervised finetuning on task-specific labeled image data removes the backdoor trigger from the CLIP vision encoder. We show empirically that CleanCLIP maintains model performance on benign examples while erasing a range of backdoor at-tacks on multimodal contrastive learning. Code and pre-trained checkpoints are available at https://github. com/nishadsinghi/CleanCLIP. 