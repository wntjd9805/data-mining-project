Place recognition is a key module for long-term SLAM systems. Current LiDAR-based place recognition meth-ods usually use representations of point clouds such as un-ordered points or range images. These methods achieve high recall rates of retrieval, but their performance may de-grade in the case of view variation or scene changes. In this work, we explore the potential of a different representation in place recognition, i.e. birdâ€™s eye view (BEV) images. We validate that, in scenes of slight viewpoint changes, a simpleNetVLAD network trained on BEV images achieves com-parable performance to the state-of-the-art place recogni-tion methods. For robustness to view variations, we pro-pose a rotation-invariant network called BEVPlace. We use group convolution to extract rotation-equivariant local features from the images and NetVLAD for global feature aggregation. In addition, we observe that the distance be-tween BEV features is correlated with the geometry distance of point clouds. Based on the observation, we develop a method to estimate the position of the query cloud, extend-ing the usage of place recognition. The experiments con-ducted on large-scale public datasets show that our method 1) achieves state-of-the-art performance in terms of recall rates, 2) is robust to view changes, 3) shows strong gen-eralization ability, and 4) can estimate the positions of query point clouds. Source codes are publicly available at https://github.com/zjuluolun/BEVPlace . 