Knowledge distillation (KD) has been widely explored in unsupervised anomaly detection (AD). The student is as-sumed to constantly produce representations of typical pat-terns within trained data, named “normality”, and the rep-resentation discrepancy between the teacher and student model is identified as anomalies. However, it suffers from the “normality forgetting” issue. Trained on anomaly-free data, the student still well reconstructs anomalous repre-sentations for anomalies and is sensitive to fine patterns in normal data, which also appear in training. To mitigate this issue, we introduce a novel Memory-guided Knowledge-Distillation (MemKD) framework that adaptively modu-lates the normality of student features in detecting anoma-lies. Specifically, we first propose a normality recall mem-ory (NR Memory) to strengthen the normality of student-generated features by recalling the stored normal informa-tion. In this sense, representations will not present anoma-lies and fine patterns will be well described. Subsequently, we employ a normality embedding learning strategy to pro-mote information learning for the NR Memory. It constructs a normal exemplar set so that the NR Memory can memorize prior knowledge in anomaly-free data and later recall them from the query feature. Consequently, comprehensive ex-periments demonstrate that the proposed MemKD achieves promising results on five benchmarks. 