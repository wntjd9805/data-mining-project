Video depth estimation aims to infer temporally consis-tent depth. Some methods achieve temporal consistency by finetuning a single-image depth model during test time us-ing geometry and re-projection constraints, which is inef-ficient and not robust. An alternative approach is to learn how to enforce temporal consistency from data, but this re-quires well-designed models and sufficient video depth data.To address these challenges, we propose a plug-and-play framework called Neural Video Depth Stabilizer (NVDS) that stabilizes inconsistent depth estimations and can be ap-plied to different single-image depth models without extra effort. We also introduce a large-scale dataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with over two million frames, making it the largest natural-scene video depth dataset to our knowledge. We evaluate our method on the VDW dataset as well as two public bench-marks and demonstrate significant improvements in consis-tency, accuracy, and efficiency compared to previous ap-proaches. Our work serves as a solid baseline and provides a data foundation for learning-based video depth models.We will release our dataset and code for future research. 