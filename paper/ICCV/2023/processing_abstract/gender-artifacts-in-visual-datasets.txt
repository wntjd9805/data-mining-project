Average poseAverage colorFMFMGender biases are known to exist within large-scale vi-sual datasets and can be reﬂected or even ampliﬁed in downstream models. Many prior works have proposed methods for mitigating gender biases, often by attempt-ing to remove gender expression information from images.To understand the feasibility and practicality of these ap-proaches, we investigate what “gender artifacts” exist in large-scale visual datasets. We deﬁne a “gender artifact” as a visual cue correlated with gender, focusing speciﬁ-cally on cues that are learnable by a modern image clas-siﬁer and have an interpretable human corollary. Through our analyses, we ﬁnd that gender artifacts are ubiquitous in the COCO and OpenImages datasets, occurring every-where from low-level information (e.g., the mean value of the color channels) to higher-level image composition (e.g., pose and location of people). Further, bias mitigation meth-ods that attempt to remove gender actually remove more in-formation from the scene than the person. Given the preva-lence of gender artifacts, we claim that attempts to remove these artifacts from such datasets are largely infeasible as certain removed artifacts may be necessary for the down-stream task of object recognition. Instead, the responsibil-ity lies with researchers and practitioners to be aware that the distribution of images within datasets is highly gendered and hence develop fairness-aware methods which are ro-bust to these distributional shifts across groups. 