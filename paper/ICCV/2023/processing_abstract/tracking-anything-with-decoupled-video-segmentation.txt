Training data for video segmentation are expensive to annotate. This impedes extensions of end-to-end al-gorithms to new video segmentation tasks, especially inTo ‘track anything’ without large-vocabulary settings. training on video data for every individual task, we de-velop a decoupled video segmentation approach (DEVA), composed of task-specific image-level segmentation and class/task-agnostic bi-directional temporal propagation.Due to this design, we only need an image-level model for the target task (which is cheaper to train) and a universal temporal propagation model which is trained once and generalizes across tasks. To effectively com-bine these two modules, we use bi-directional propaga-tion for (semi-)online fusion of segmentation hypotheses from different frames to generate a coherent segmenta-tion. We show that this decoupled formulation compares favorably to end-to-end approaches in several data-scarce tasks including large-vocabulary video panoptic segmen-tation, open-world video segmentation, referring video segmentation, and unsupervised video object segmenta-tion. Code is available at: hkchengrex.github.io/Tracking-Anything-with-DEVA. 