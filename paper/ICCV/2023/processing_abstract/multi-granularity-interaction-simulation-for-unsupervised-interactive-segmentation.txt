Interactive segmentation enables users to segment as needed by providing cues of objects, which introduces human-computer interaction for many fields, such as im-age editing and medical image analysis. Typically, mas-sive and expansive pixel-level annotations are spent to train deep models by object-oriented interactions with manu-ally labeled object masks.In this work, we reveal that informative interactions can be made by simulation with semantic-consistent yet diverse region exploration in an un-supervised paradigm. Concretely, we introduce a Multi-granularity Interaction Simulation (MIS) approach to open up a promising direction for unsupervised interactive seg-mentation. Drawing on the high-quality dense features pro-duced by recent self-supervised models, we propose to grad-ually merge patches or regions with similar features to form more extensive regions and thus, every merged region serves as a semantic-meaningful multi-granularity proposal. By randomly sampling these proposals and simulating possi-ble interactions based on them, we provide meaningful in-teraction at multiple granularities to teach the model to un-derstand interactions. Our MIS significantly outperforms non-deep learning unsupervised methods and is even com-parable with some previous deep-supervised methods with-out any annotation. 