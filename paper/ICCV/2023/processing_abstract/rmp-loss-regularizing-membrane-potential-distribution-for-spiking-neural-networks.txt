Spiking Neural Networks (SNNs) as one of the biology-inspired models have received much attention recently. It can significantly reduce energy consumption since they quantize the real-valued membrane potentials to 0/1 spikes to transmit information thus the multiplications of activa-tions and weights can be replaced by additions when im-plemented on hardware. However, this quantization mecha-nism will inevitably introduce quantization error, thus caus-ing catastrophic information loss. To address the quanti-zation error problem, we propose a regularizing membrane potential loss (RMP-Loss) to adjust the distribution which is directly related to quantization error to a range close to the spikes. Our method is extremely simple to implement and straightforward to train an SNN. Furthermore, it is shown to consistently outperform previous state-of-the-art meth-ods over different network architectures and datasets. 