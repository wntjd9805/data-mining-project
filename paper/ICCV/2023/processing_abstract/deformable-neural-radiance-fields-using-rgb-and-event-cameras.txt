Modeling Neural Radiance Fields for fast-moving de-formable objects from visual data alone is a challenging problem. A major issue arises due to the high deforma-tion and low acquisition rates. To address this problem, we propose to use event cameras that offer very fast ac-quisition of visual change in an asynchronous manner. In this work, we develop a novel method to model the de-formable neural radiance fields using RGB and event cam-eras. The proposed method uses the asynchronous stream of events and calibrated sparse RGB frames. In our setup, the camera pose at the individual events –required to in-tegrate them into the radiance fields– remains unknown.Our method jointly optimizes these poses and the radiance field. This happens efficiently by leveraging the collection of events at once and actively sampling the events during learning. Experiments conducted on both realistically ren-dered graphics and real-world datasets demonstrate a sig-nificant benefit of the proposed method over the state-of-the-art and the compared baseline. This shows a promising direction for modeling deformable neural radiance fields in real-world dynamic scenes. We release our code at: https://qimaqi.github.io/DE-NeRF.github.io/ 