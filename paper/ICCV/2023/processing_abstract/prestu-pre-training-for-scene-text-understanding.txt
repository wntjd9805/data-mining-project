The ability to recognize and reason about text embed-ded in visual inputs is often lacking in vision-and-language (V&L) models, perhaps because V&L pre-training methods have often failed to include such an ability in their train-ing objective. In this paper, we propose PRESTU, a novel pre-training recipe dedicated to scene-text understanding (STU). PRESTU introduces OCR-aware pre-training ob-jectives that encourage the model to recognize text from an image and connect it to the rest of the image content.We implement PRESTU using a simple transformer-based encoder-decoder architecture, combined with large-scale image-text datasets with scene text obtained from an off-the-shelf OCR system. We empirically demonstrate the effec-tiveness of this pre-training approach on eight visual ques-tion answering and four image captioning benchmarks. 