Understanding and mapping a new environment are core abilities of any autonomously navigating agent. While classi-cal robotics usually estimates maps in a stand-alone manner with SLAM variants, which maintain a topological or met-ric representation, end-to-end learning of navigation keeps some form of memory in a neural network. Networks are typically imbued with inductive biases, which can range from vectorial representations to birds-eye metric tensors or topological structures. In this work, we propose to structure neural networks with two neural implicit representations, which are learned dynamically during each episode and map the content of the scene: (i) the Semantic Finder pre-dicts the position of a previously seen queried object; (ii) theOccupancy and Exploration Implicit Representation encap-sulates information about explored area and obstacles, and is queried with a novel global read mechanism which directly maps from function space to a usable embedding space. Both representations are leveraged by an agent trained with Re-inforcement Learning (RL) and learned online during each episode. We evaluate the agent on Multi-Object Navigation and show the high impact of using neural implicit represen-tations as a memory source. 