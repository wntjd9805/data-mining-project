We present an approach to estimating camera rotation in crowded, real-world scenes from handheld monocular video. While camera rotation estimation is a well-studied problem, no previous methods exhibit both high accuracy and acceptable speed in this setting. Because the setting is not addressed well by other datasets, we provide a new dataset and benchmark, with high-accuracy, rigorously ver-ified ground truth, on 17 video sequences. Methods de-veloped for wide baseline stereo (e.g., 5-point methods) perform poorly on monocular video. On the other hand, methods used in autonomous driving (e.g., SLAM) lever-age specific sensor setups, specific motion models, or lo-cal optimization strategies (lagging batch processing) and do not generalize well to handheld video. Finally, for dy-namic scenes, commonly used robustification techniques like RANSAC require large numbers of iterations, and be-come prohibitively slow. We introduce a novel generaliza-tion of the Hough transform on SO(3) to efficiently and ro-bustly find the camera rotation most compatible with op-tical flow. Among comparably fast methods, ours reduces error by almost 50% over the next best, and is more ac-curate than any method, irrespective of speed. This repre-sents a strong new performance point for crowded scenes, an important setting for computer vision. The code and the dataset are available at https://fabiendelattre.com/robust-rotation-estimation.Figure 1. Left. A frame from our BUSS dataset of crowded scenes. The red vectors show optical flows compatible with the winning rotation estimate R∗, indicating the rotation of the cam-era. Gray vectors show optical flows not explained purely by R∗.Right. The three axes show the space of rotations in 3D. Each line shows the one-dimensional set of rotations that are compati-ble with a single optical flow vector. The red lines (correspond-ing to the red flow vectors in the top figure) intersect in a single small bin, indicating that their optical flows are compatible with the same rotation. The gray lines, which are affected by other motion effects, are scattered in an unstructured manner, and corre-spond to the gray optical flows above. Our algorithm finds the set of lines with greatest coherence in SO(3), revealing the rotationR∗ of the camera. 