We question the current evaluation practice on diffusion-based puriﬁcation methods. Diffusion-based puriﬁcation methods aim to remove adversarial effects from an input data point at test time. The approach gains increasing at-tention as an alternative to adversarial training due to the disentangling between training and testing. Well-known white-box attacks are often employed to measure the robust-ness of the puriﬁcation. However, it is unknown whether these attacks are the most effective for the diffusion-based puriﬁcation since the attacks are often tailored for adver-sarial training. We analyze the current practices and pro-vide a new guideline for measuring the robustness of puriﬁ-cation methods against adversarial attacks. Based on our analysis, we further propose a new puriﬁcation strategy im-proving robustness compared to the current diffusion-based puriﬁcation methods. 