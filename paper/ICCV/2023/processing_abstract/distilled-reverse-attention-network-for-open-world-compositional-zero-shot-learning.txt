Open-World Compositional Zero-Shot Learning (OW-CZSL) aims to recognize new compositions of seen at-tributes and objects.In OW-CZSL, methods built on the conventional closed-world setting degrade severely due to the unconstrained OW test space. While previous works alleviate the issue by pruning compositions according to external knowledge or correlations in seen pairs, they in-troduce biases that harm the generalization. Some methods thus predict state and object with independently constructed and trained classifiers, ignoring that attributes are highly context-dependent and visually entangled with objects. In this paper, we propose a novel Distilled Reverse AttentionNetwork to address the challenges. We also model attributes and objects separately but with different motivations, cap-turing contextuality and locality, respectively. We further design a reverse-and-distill strategy that learns disentan-gled representations of elementary components in training data supervised by reverse attention and knowledge distil-lation. We conduct experiments on three datasets and con-sistently achieve state-of-the-art (SOTA) performance.Figure 1: Motivation behind our disentangling strategy for OW-CZSL. When extracted features of objects and at-tributes are disentangled (images 3 and 5), their residual features (images 4 and 6) carry sufficient information about each other to classify correctly, and produce large overlap between the object residuals and the attribute features (im-ages 4 and 5). For entangled attribute-object features (im-ages 1 and 3), the phenomena are otherwise reversed (image 2: few object information; images 1 and 4: small overlap). 