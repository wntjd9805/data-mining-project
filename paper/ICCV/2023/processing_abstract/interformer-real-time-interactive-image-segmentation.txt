Interactive image segmentation enables annotators to efficiently perform pixel-level annotation for segmenta-tion tasks. However, the existing interactive segmentation pipeline suffers from inefficient computations of interactive models because of the following two issues. First, anno-tators’ later click is based on models’ feedback of anno-tators’ former click. This serial interaction is unable to utilize model’s parallelism capabilities. Second, in each interaction step, the model handles the invariant image along with the sparse variable clicks, resulting in a pro-cess that’s highly repetitive and redundant. For efficient computations, we propose a method named InterFormer that follows a new pipeline to address these issues.In-terFormer extracts and preprocesses the computationally time-consuming part i.e. image processing from the existing process. Specifically, InterFormer employs a large vision transformer (ViT) on high-performance devices to prepro-cess images in parallel, and then uses a lightweight mod-ule called interactive multi-head self attention (I-MSA) for interactive segmentation. Furthermore, the I-MSA mod-ule’s deployment on low-power devices extends the prac-tical application of interactive segmentation. The I-MSA module utilizes the preprocessed features to efficiently re-sponse to the annotator inputs in real-time. The experiments on several datasets demonstrate the effectiveness of Inter-Former, which outperforms previous interactive segmenta-tion models in terms of computational efficiency and seg-mentation quality, achieve real-time high-quality interac-tive segmentation on CPU-only devices. The code is avail-able at https://github.com/YouHuang67/InterFormer. 