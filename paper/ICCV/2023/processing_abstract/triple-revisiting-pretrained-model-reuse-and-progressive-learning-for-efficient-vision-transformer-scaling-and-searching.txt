One promising way to accelerate transformer training is to reuse small pretrained models to initialize the trans-former, as their existing representation power facilitates faster model convergence. Previous works designed expan-sion operators to scale up pretrained models to the target model before training. Yet, model functionality is difficult to preserve when scaling a transformer in all dimensions at once. Moreover, maintaining the pretrained optimizer states for weights is critical for model scaling, whereas the new weights added during expansion lack these states in pre-trained models. To address these issues, we propose TripLe, which partially scales a model before training, while grow-ing the rest of the new parameters during training by copy-ing both the warmed-up weights with the optimizer states from existing weights. As such, the new parameters intro-duced during training will obtain their training states. Fur-thermore, through serializing the scaling of model width and depth, the functionality of each expansion can be pre-served. We evaluate TripLe in both single-trial model scal-ing and multi-trial neural architecture search (NAS). Due to the fast training convergence of TripLe, the proxy accuracy from TripLe better reveals the model quality compared to from-scratch training in multi-trial NAS. Experiments show that TripLe outperforms from-scratch training and knowl-edge distillation (KD) in both training time and task perfor-mance. TripLe can also be combined with KD to achieve an even higher task accuracy. For NAS, the model obtained from TripLe outperforms DeiT-B in task accuracy with 69% reduction in parameter size and FLOPs. 