Models pre-trained on large datasets such as ImageNet provide the de-facto standard for transfer learning, with both supervised and self-supervised approaches proving ef-fective. However, emerging evidence suggests that any single pre-trained feature will not perform well on di-verse downstream tasks. Each pre-training strategy en-codes a certain inductive bias, which may suit some down-stream tasks but not others. Notably, the augmentations used in both supervised and self-supervised training lead to features with high invariance to spatial and appear-ance transformations. This renders them sub-optimal for tasks that demand sensitivity to these factors.In this paper we develop a feature that better supports diverse downstream tasks by providing a diverse set of sensi-tivities and invariances.In particular, we are inspired by Quality-Diversity in evolution, to deﬁne a pre-training objective that requires high quality yet diverse features— where diversity is deﬁned in terms of transformation (in)variances. Our framework plugs in to both super-vised and self-supervised pre-training, and produces a small ensemble of features. We further show how down-stream tasks can easily and efﬁciently select their preferred (in)variances. Both empirical and theoretical analysis show the efﬁcacy of our representation and transfer learn-ing approach for diverse downstream tasks. Code avail-able at https://github.com/ruchikachavhan/ quality-diversity-pretraining.git 