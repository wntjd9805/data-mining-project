Text-to-image generative models have enabled high-resolution image synthesis across different domains, but re-quire users to specify the content they wish to generate. In this paper, we consider the inverse problem – given a col-lection of different images, can we discover the generative concepts that represent each image? We present an unsu-pervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to gener-ate new artistic and hybrid images, and be further used as a representation for downstream classiﬁcation tasks. 