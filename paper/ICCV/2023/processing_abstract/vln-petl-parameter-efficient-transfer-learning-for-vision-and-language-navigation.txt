The performance of the Vision-and-Language Naviga-tion (VLN) tasks has witnessed rapid progress recently thanks to the use of large pre-trained vision-and-language models. However, full ﬁne-tuning the pre-trained model for every downstream VLN task is becoming costly due to the considerable model size. Recent research hotspot ofParameter-Efﬁcient Transfer Learning (PETL) shows great potential in efﬁciently tuning large pre-trained models for the common CV and NLP tasks, which exploits the most of the representation knowledge implied in the pre-trained model while only tunes a minimal set of parameters. How-ever, simply utilizing existing PETL methods for the more challenging VLN tasks may bring non-trivial degeneration to the performance. Therefore, we present the ﬁrst study to explore PETL methods for VLN tasks and propose a VLN-speciﬁc PETL method named VLN-PETL. Speciﬁcally, we design two PETL modules: Historical Interaction Booster (HIB) and Cross-modal Interaction Booster (CIB). Then we combine these two modules with several existing PETL methods as the integrated VLN-PETL. Extensive experimen-tal results on four mainstream VLN tasks (R2R, REVERIE,NDH, RxR) demonstrate the effectiveness of our proposedVLN-PETL, where VLN-PETL achieves comparable or even better performance to full ﬁne-tuning and outperforms other PETL methods with promising margins. The source code is available at https://github.com/YanyuanQiao/VLN-PETL 