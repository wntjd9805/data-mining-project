Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Con-siderable effort has been invested in designing variousOOD detection methods based on either convolutional neu-ral networks or transformers. However, zero-shot OOD de-tection methods driven by CLIP, which only require classThis pa-names for ID, have received less attention. per presents a novel method, namely CLIP saying “no” (CLIPN), which empowers the logic of saying “no” withinCLIP. Our key motivation is to equip CLIP with the capa-bility of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifi-cally, we design a novel learnable “no” prompt and a “no” text encoder to capture negation semantics within images.Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to associate images with“no” prompts, thereby enabling it to identify unknown sam-ples. Furthermore, we propose two threshold-free inference algorithms to perform OOD detection by utilizing negation semantics from “no” prompts and the text encoder. Experi-mental results on 9 benchmark datasets (3 ID datasets and 6OOD datasets) for the OOD detection task demonstrate thatCLIPN, based on ViT-B-16, outperforms 7 well-used algo-rithms by at least 2.34% and 11.64% in terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K.Our CLIPN can serve as a solid foundation for effectively leveraging CLIP in downstream OOD tasks. The code is available on https://github.com/xmed-lab/CLIPN. 