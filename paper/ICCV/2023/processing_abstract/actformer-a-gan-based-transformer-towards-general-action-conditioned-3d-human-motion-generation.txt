We present a GAN-based Transformer for general action-conditioned 3D human motion generation, includ-ing not only single-person actions but also multi-person interactive actions. Our approach consists of a powerfulAction-conditioned motion TransFormer (ActFormer) under a GAN training scheme, equipped with a Gaussian Pro-cess latent prior. Such a design combines the strong spatio-temporal representation capacity of Transformer, superior-ity in generative modeling of GAN, and inherent temporal correlations from the latent prior. Furthermore, ActFormer can be naturally extended to multi-person motions by alter-nately modeling temporal correlations and human interac-tions with Transformer encoders. To further facilitate re-search on multi-person motion generation, we introduce a new synthetic dataset of complex multi-person combat be-haviors. Extensive experiments on NTU-13, NTU RGB+D 120, BABEL and the proposed combat dataset show that our method can adapt to various human motion represen-tations and achieve superior performance over the state-of-the-art methods on both single-person and multi-person motion generation tasks, demonstrating a promising step to-wards a general human motion generator. The project web-site can be found at https://liangxuy.github.io/actformer/. 