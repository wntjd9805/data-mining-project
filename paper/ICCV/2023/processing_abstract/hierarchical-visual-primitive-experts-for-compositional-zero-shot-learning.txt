Compositional zero-shot learning (CZSL) aims to recog-nize unseen compositions with prior knowledge of known primitives (attribute and object). Previous works for CZSL often suffer from grasping the contextuality between at-tribute and object, as well as the discriminability of vi-sual features, and the long-tailed distribution of real-world compositional data. We propose a simple and scalable framework called Composition Transformer (CoT) to ad-dress these issues. CoT employs object and attribute experts in distinctive manners to generate representative embed-dings, using the visual network hierarchically. The object expert extracts representative object embeddings from theÔ¨Ånal layer in a bottom-up manner, while the attribute expert makes attribute embeddings in a top-down manner with a proposed object-guided attention module that models con-textuality explicitly. To remedy biased prediction caused by imbalanced data distribution, we develop a simple minor-ity attribute augmentation (MAA) that synthesizes virtual samples by mixing two images and oversampling minority attribute classes. Our method achieves SoTA performance on several benchmarks, including MIT-States, C-GQA, andVAW-CZSL. We also demonstrate the effectiveness of CoT in improving visual discrimination and addressing the model bias from the imbalanced data distribution. The code is available at https://github.com/HanjaeKim98/CoT. 