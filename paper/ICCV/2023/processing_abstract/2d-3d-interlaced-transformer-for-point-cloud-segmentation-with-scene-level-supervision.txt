door:(cid:1905)(cid:1905) chair:(cid:1905)(cid:1905)… bathtub: 3D 2D data token class tokenWe present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud seg-mentation. However, existing methods require extra 2D an-notations to achieve 2D-3D information fusion. Consider-ing the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly su-pervised point cloud segmentation using only scene-level class tags. Speciﬁcally, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queriesIt turns out and key-value pairs in the decoder layers. that the 2D and 3D features are iteratively enriched by each other. Experiments show that it performs favorably against existing weakly supervised point cloud segmenta-tion methods by a large margin on the S3DIS and Scan-Net benchmarks. The project page will be available at https://jimmy15923.github.io/mit_web/. 