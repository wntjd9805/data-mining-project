Modern deep learning techniques on automatic multi-modal medical diagnosis rely on massive expert annota-tions, which is time-consuming and prohibitive. Recent masked image modeling (MIM)-based pre-training methods have witnessed impressive advances for learning meaning-ful representations from unlabeled data and transferring to downstream tasks. However, these methods focus on nat-ural images and ignore the specific properties of medical data, yielding unsatisfying generalization performance on downstream medical diagnosis.In this paper, we aim to leverage genetics to boost image pre-training and presentInstead a masked relation modeling (MRM) framework. of explicitly masking input data in previous MIM methods leading to loss of disease-related semantics, we design rela-tion masking to mask out token-wise feature relation in both self- and cross-modality levels, which preserves intact se-mantics within the input and allows the model to learn rich disease-related information. Moreover, to enhance seman-tic relation modeling, we propose relation matching to align the sample-wise relation between the intact and masked fea-tures. The relation matching exploits inter-sample relation by encouraging global constraints in the feature space to render sufficient semantic relation for feature representa-tion. Extensive experiments demonstrate that the proposed framework is simple yet powerful, achieving state-of-the-art transfer performance on various downstream diagnosis tasks. Codes are available at https://github.com/CityU-AIM-Group/MRM . 