View Transformation Module (VTM), where transfor-mations happen between multi-view image features andBird-Eye-View (BEV) representation, is a crucial step in camera-based BEV perception systems. Currently, the two most prominent VTM paradigms are forward projection and backward projection. Forward projection, represented byLift-Splat-Shoot, leads to sparsely projected BEV features without post-processing. Backward projection, with BEV-Former being an example, tends to generate false-positiveBEV features from incorrect projections due to the lack of utilization on depth. To address the above limitations, we propose a novel forward-backward view transformation module. Our approach compensates for the deficiencies in both existing methods, allowing them to enhance each other to obtain higher quality BEV representations mutu-ally. We instantiate the proposed module with FB-BEV, which achieves a new state-of-the-art result of 62.4% NDS on the nuScenes test set. Code and models are available at https://github.com/NVlabs/FB-BEV .BEV-based 3D detection models have gained popularity due to their unified and comprehensive representation abili-* Work done during an internship at NVIDIA. ties for multi-camera inputs, enhancing the performance of both vision-only and multi-modality perception models for autonomous driving [1â€“9]. A typical BEV-based detection model comprises an image backbone, a View Transforma-tion Module (VTM), and a detection head. The VTMs pri-marily function to project multi-view camera features onto the BEV plane. There are two main categories of existing mainstream VTMs based on the projection methods used: forward projection and backward projection. 