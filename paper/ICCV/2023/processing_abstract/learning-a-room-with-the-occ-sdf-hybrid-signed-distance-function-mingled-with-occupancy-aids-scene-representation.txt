Implicit neural rendering, using signed distance function (SDF) representation with geometric priors like depth or surface normal, has made impressive strides in the surface reconstruction of large-scale scenes. However, applying this method to reconstruct a room-level scene from images may miss structures in low-intensity areas and/or small, thin objects. We have conducted experiments on three datasets to identify limitations of the original color rendering loss and priors-embedded SDF scene representation.Our ﬁndings show that the color rendering loss creates an optimization bias against low-intensity areas, resulting in gradient vanishing and leaving these areas unoptimized.To address this issue, we propose a feature-based color ren-dering loss that utilizes non-zero feature values to bring back optimization signals. Additionally, the SDF represen-tation can be inﬂuenced by objects along a ray path, dis-rupting the monotonic change of SDF values when a single object is present. Accordingly, we explore using the occu-pancy representation, which encodes each point separately and is unaffected by objects along a querying ray. Our ex-perimental results demonstrate that the joint forces of the feature-based rendering loss and Occ-SDF hybrid repre-sentation scheme can provide high-quality reconstruction results, especially in challenging room-level scenarios. The code is available at https://github.com/shawLyu/Occ-SDF-Hybrid 