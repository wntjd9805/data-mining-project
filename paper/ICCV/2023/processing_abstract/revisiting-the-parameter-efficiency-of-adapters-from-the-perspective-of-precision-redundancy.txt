Current state-of-the-art results in computer vision de-pend in part on fine-tuning large pre-trained vision mod-els. However, with the exponential growth of model sizes, the conventional full fine-tuning, which needs to store a individual network copy for each tasks, leads to increas-ingly huge storage and transmission overhead. Adapter-based Parameter-Efficient Tuning (PET) methods address this challenge by tuning lightweight adapters inserted into the frozen pre-trained models.In this paper, we investi-gate how to make adapters even more efficient, reaching a new minimum size required to store a task-specific fine-tuned network. Inspired by the observation that the param-eters of adapters converge at flat local minima, we find that adapters are resistant to noise in parameter space, which means they are also resistant to low numerical precision. To train low-precision adapters, we propose a computational-efficient quantization method which minimizes the quanti-zation error. Through extensive experiments, we find that low-precision adapters exhibit minimal performance degra-dation, and even 1-bit precision is sufficient for adapters.The experimental results demonstrate that 1-bit adapters outperform all other PET methods on both the VTAB-1K benchmark and few-shot FGVC tasks, while requiring the smallest storage size. Our findings show, for the first time, the significant potential of quantization techniques in PET, providing a general solution to enhance the parameter ef-ficiency of adapter-based PET methods. Code: https://github.com/JieShibo/PETL-ViT 