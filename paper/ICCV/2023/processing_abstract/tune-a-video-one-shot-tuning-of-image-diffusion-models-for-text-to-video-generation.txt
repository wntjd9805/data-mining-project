To replicate the success of text-to-image (T2I) genera-tion, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising re-*Corresponding Author. sults, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting—One-ShotVideo Tuning, where only one text-video pair is presented.Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key obser-vations: 1) T2I models can generate still images that repre-sent verb terms; 2) extending T2I models to generate mul-tiple images concurrently exhibits surprisingly good con-tent consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efﬁcient one-shot tun-ing strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualita-tive and numerical experiments demonstrate the remarkable ability of our method across various applications. 