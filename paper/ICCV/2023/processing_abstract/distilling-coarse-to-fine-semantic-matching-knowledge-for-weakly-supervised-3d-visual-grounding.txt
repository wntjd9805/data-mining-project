3D visual grounding involves ﬁnding a target object in a 3D scene that corresponds to a given sentence query. Al-though many approaches have been proposed and achieved impressive performance, they all require dense object-sentence pair annotations in 3D point clouds, which are both time-consuming and expensive. To address the prob-lem that ﬁne-grained annotated data is difﬁcult to obtain, we propose to leverage weakly supervised annotations to learn the 3D visual grounding model, i.e., only coarse scene-sentence correspondences are used to learn object-sentence links. To accomplish this, we design a novel se-mantic matching model that analyzes the semantic similar-ity between object proposals and sentences in a coarse-to-ﬁne manner. Speciﬁcally, we ﬁrst extract object propos-als and coarsely select the top-K candidates based on fea-ture and class similarity matrices. Next, we reconstruct the masked keywords of the sentence using each candidate one by one, and the reconstructed accuracy ﬁnely reﬂects the se-mantic similarity of each candidate to the query. Addition-ally, we distill the coarse-to-ﬁne semantic matching knowl-edge into a typical two-stage 3D visual grounding model, which reduces inference costs and improves performance by taking full advantage of the well-studied structure of the ex-isting architectures. We conduct extensive experiments onScanRefer, Nr3D, and Sr3D, which demonstrate the effec-tiveness of our proposed method. 