Geospatial technologies are becoming increasingly es-sential in our world for a wide range of applications, including agriculture, urban planning, and disaster re-sponse. To help improve the applicability and performance of deep learning models on these geospatial tasks, vari-ous works have begun investigating foundation models for this domain. Researchers have explored two prominent ap-proaches for introducing such models in geospatial appli-cations, but both have drawbacks in terms of limited per-formance benefit or prohibitive training cost. Therefore, in this work, we propose a novel paradigm for building highly effective geospatial foundation models with minimal resource cost and carbon impact. We first construct a com-pact yet diverse dataset from multiple sources to promote feature diversity, which we term GeoPile. Then, we in-vestigate the potential of continual pretraining from large-scale ImageNet-22k models and propose a multi-objective continual pretraining paradigm, which leverages the strong representations of ImageNet while simultaneously provid-ing the freedom to learn valuable in-domain features. Our approach outperforms previous state-of-the-art geospatial pretraining methods in an extensive evaluation on seven downstream datasets covering various tasks such as change detection, classification, multi-label classification, seman-tic segmentation, and super-resolution. Code is available at https://github.com/mmendiet/GFM . 