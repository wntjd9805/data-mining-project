In this work, we focus on the task of procedure plan-ning from instructional videos with text supervision, where a model aims to predict an action sequence to transform the initial visual state into the goal visual state. A critical challenge of this task is the large semantic gap between ob-served visual states and unobserved intermediate actions, which is ignored by previous works. Specifically, this se-mantic gap refers to that the contents in the observed vi-sual states are semantically different from the elements of some action text labels in a procedure. To bridge this se-mantic gap, we propose a novel event-guided paradigm, which first infers events from the observed states and then plans out actions based on both the states and predicted events. Our inspiration comes from that planning a proce-dure from an instructional video is to complete a specific event and a specific event usually involves specific actions.Based on the proposed paradigm, we contribute an Event-guided Prompting-based Procedure Planning (E3P) model, which encodes event information into the sequential model-ing process to support procedure planning. To further con-sider the strong action associations within each event, ourE3P adopts a mask-and-predict approach for relation min-ing, incorporating a probabilistic masking scheme for regu-larization. Extensive experiments on three datasets demon-strate the effectiveness of our proposed model. 