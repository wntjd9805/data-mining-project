Domain generalization person re-identification (DG-ReID) aims to train a model on source domains and gen-eralize well on unseen domains. Vision Transformer usu-ally yields better generalization ability than common CNN networks under distribution shifts. However, Transformer-based ReID models inevitably over-fit to domain-specific bi-ases due to the supervised learning strategy on the source domain. We observe that while the global images of differ-ent IDs should have different features, their similar local parts (e.g., black backpack) are not bounded by this con-straint. Motivated by this, we propose a pure Transformer model (termed Part-aware Transformer) for DG-ReID by designing a proxy task, named Cross-ID Similarity Learn-ing (CSL), to mine local visual information shared by differ-ent IDs. This proxy task allows the model to learn generic features because it only cares about the visual similarity of the parts regardless of the ID labels, thus alleviating the side effect of domain-specific biases. Based on the local similarity obtained in CSL, a Part-guided Self-Distillation (PSD) is proposed to further improve the generalization of global features. Our method achieves state-of-the-art performance under most DG ReID settings. The code is available at https://github.com/liyuke65535/Part-Aware-Transformer.Figure 1. (a) We applied different Transformers to DG ReID. Mod-els are trained on Market and tested on MSMT. Results show Vi-sion transformers (blue bars) are better than CNNs (orange bars) even with fewer parameters. (b) Visualization of attention maps of“class token” on source domain (MSMT) and target domain (Mar-ket). We use ViT [4] as the backbone and fuse the attention results of the shallow layers. However, the attention to discriminative in-formation is still limited on target domain. 