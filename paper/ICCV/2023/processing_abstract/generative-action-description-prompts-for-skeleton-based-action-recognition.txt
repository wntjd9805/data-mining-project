Skeleton-based action recognition has recently received considerable attention. Current approaches to skeleton-based action recognition are typically formulated as one-hot classification tasks and do not fully exploit the seman-tic relations between actions. For example, “make victory sign” and “thumb up” are two actions of hand gestures, whose major difference lies in the movement of hands. This information is agnostic from the categorical one-hot encod-ing of action classes but could be unveiled from the ac-tion description. Therefore, utilizing action description in training could potentially benefit representation learning.In this work, we propose a Generative Action-descriptionPrompts (GAP) approach for skeleton-based action recog-nition. More specifically, we employ a pre-trained large-scale language model as the knowledge engine to automati-cally generate text descriptions for body parts movements of actions, and propose a multi-modal training scheme by uti-lizing the text encoder to generate feature vectors for differ-ent body parts and supervise the skeleton encoder for action representation learning. Experiments show that our pro-posed GAP method achieves noticeable improvements over various baseline models without extra computation cost at inference. GAP achieves new state-of-the-arts on popu-lar skeleton-based action recognition benchmarks, includ-ing NTU RGB+D, NTU RGB+D 120 and NW-UCLA. The source code is available at https://github.com/MartinXM/GAP. 