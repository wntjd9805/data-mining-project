Video Instance Segmentation (VIS) aims at segment-ing and categorizing objects in videos from a closed set of training categories, lacking the generalization ability to handle novel categories in real-world videos. To ad-dress this limitation, we make the following three con-tributions. First, we introduce the novel task of Open-Vocabulary Video Instance Segmentation, which aims to si-multaneously segment, track, and classify objects in videos from open-set categories, including novel categories unseen during training. Second, to benchmark Open-VocabularyVIS, we collect a Large-Vocabulary Video Instance Segmen-tation dataset (LV-VIS), that contains well-annotated ob-jects from 1,196 diverse categories, significantly surpass-ing the category size of existing datasets by more than one order of magnitude. Third, we propose an efficientMemory-Induced Transformer architecture, OV2Seg, to first achieve Open-Vocabulary VIS in an end-to-end manner with near real-time inference speed. Extensive experi-ments on LV-VIS and four existing VIS datasets demonstrate the strong zero-shot generalization ability of OV2Seg on novel categories. The dataset and code are released here https://github.com/haochenheheda/LVVIS. 