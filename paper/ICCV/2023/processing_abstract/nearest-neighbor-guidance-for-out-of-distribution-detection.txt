Detecting out-of-distribution (OOD) samples are cru-cial for machine learning models deployed in open-world environments. Classifier-based scores are a standard ap-proach for OOD detection due to their fine-grained detec-tion capability. However, these scores often suffer from overconfidence issues, misclassifying OOD samples distant from the in-distribution region. To address this challenge, we propose a method called Nearest Neighbor Guidance (NNGuide) that guides the classifier-based score to respect the boundary geometry of the data manifold. NNGuide re-duces the overconfidence of OOD samples while preserv-ing the fine-grained capability of the classifier-based score.We conduct extensive experiments on ImageNet OOD de-tection benchmarks under diverse settings, including a sce-nario where the ID data undergoes natural distribution shift. Our results demonstrate that NNGuide provides a significant performance improvement on the base detection scores, achieving state-of-the-art results on both AUROC,FPR95, and AUPR metrics. 