(cid:21)(cid:6)(cid:22)(cid:24)(cid:2)(cid:13)(cid:11)(cid:11)(cid:10)(cid:18)(cid:10)(cid:16)(cid:20)(cid:24)(cid:1)(cid:12)(cid:6)(cid:18)(cid:6)(cid:8)(cid:20)(cid:10)(cid:18)(cid:13)(cid:19)(cid:20)(cid:13)(cid:8)(cid:19) (cid:21)(cid:7)(cid:22)(cid:24)(cid:4)(cid:6)(cid:8)(cid:14)(cid:24)(cid:17)(cid:11)(cid:24)(cid:1)(cid:18)(cid:17)(cid:19)(cid:19)(cid:23)(cid:5)(cid:17)(cid:9)(cid:6)(cid:15)(cid:24)(cid:3)(cid:16)(cid:20)(cid:10)(cid:18)(cid:6)(cid:8)(cid:20)(cid:13)(cid:17)(cid:16)(cid:19)Automatic radiology report generation has attracted enor-mous research interest due to its practical value in reducing the workload of radiologists. However, simultaneously es-tablishing global correspondences between the image (e.g.,Chest X-ray) and its related report and local alignments between image patches and keywords remains challenging.To this end, we propose an Unify, Align and then Reﬁne (UAR) approach to learn multi-level cross-modal alignments and introduce three novel modules: Latent Space Uniﬁer (LSU), Cross-modal Representation Aligner (CRA) and Text-to-Image Reﬁner (TIR). Speciﬁcally, LSU uniﬁes multimodal data into discrete tokens, making it ﬂexible to learn common knowledge among modalities with a shared network. The modality-agnostic CRA learns discriminative features via a set of orthonormal basis and a dual-gate mechanism ﬁrst and then globally aligns visual and textual representations under a triplet contrastive loss. TIR boosts token-level local align-ment via calibrating text-to-image attention with a learnable mask. Additionally, we design a two-stage training proce-dure to make UAR gradually grasp cross-modal alignments at different levels, which imitates radiologists’ workﬂow: writing sentence by sentence ﬁrst and then checking word by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXR benchmark datasets demonstrate the supe-riority of our UAR against varied state-of-the-art methods. 