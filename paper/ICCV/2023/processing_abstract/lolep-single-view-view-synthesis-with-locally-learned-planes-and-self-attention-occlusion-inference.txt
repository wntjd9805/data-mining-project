We propose a novel method, LoLep, which regressesLocally-Learned planes from a single RGB image to represent scenes accurately, thus generating better novel views. Without the depth information, regressing appro-priate plane locations is a challenging problem. To solve this issue, we pre-partition the disparity space into bins and design a disparity sampler to regress local offsets for mul-tiple planes in each bin. However, only using such a sam-pler makes the network not convergent; we further propose two optimizing strategies that combine with different dis-parity distributions of datasets and propose an occlusion-aware reprojection loss as a simple yet effective geometric supervision technique. We also introduce a self-attention mechanism to improve occlusion inference and present aBlock-Sampling Self-Attention (BS-SA) module to address the problem of applying self-attention to large feature maps.We demonstrate the effectiveness of our approach and gen-erate state-of-the-art results on different datasets. Com-pared to MINE, our approach has an LPIPS reduction of 4.8%∼9.0% and an RV reduction of 74.9%∼83.5%. We also evaluate the performance on real-world images and demonstrate the benefits. 