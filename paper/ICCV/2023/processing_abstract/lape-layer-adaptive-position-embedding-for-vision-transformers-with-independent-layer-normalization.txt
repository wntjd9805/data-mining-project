Position information is critical for Vision Transformers (VTs) due to the permutation-invariance of self-attention operations. A typical way to introduce position information is adding the absolute Position Embedding (PE) to patch embedding before entering VTs. However, this approach operates the same Layer Normalization (LN) to token em-bedding and PE, and delivers the same PE to each layer.This results in restricted and monotonic PE across layers, as the shared LN affine parameters are not dedicated toPE, and the PE cannot be adjusted on a per-layer basis.To overcome these limitations, we propose using two inde-pendent LNs for token embeddings and PE in each layer, and progressively delivering PE across layers. By imple-menting this approach, VTs will receive layer-adaptive and hierarchical PE. We name our method as Layer-adaptivePosition Embedding, abbreviated as LaPE, which is sim-ple, effective, and robust. Extensive experiments on image classification, object detection, and semantic segmentation demonstrate that LaPE significantly outperforms the defaultPE method. For example, LaPE improves +1.06% for CCT on CIFAR100, +1.57% for DeiT-Ti on ImageNet-1K, +0.7 box AP and +0.5 mask AP for ViT-Adapter-Ti on COCO, and +1.37 mIoU for tiny Segmenter on ADE20K. This is remarkable considering LaPE only increases negligible pa-rameters, memory, and computational cost.Figure 1. A brief illustration of the default PE joining method and our proposed LaPE. We take T2T-ViT-7 with 1-D sinusoidalPE as an example, and we visualize the position correlation of first 5 layers to explain the emphasis and advantages of our method. (a) By default, token embedding and PE are coupled together and treated with the same Layer Normalization (LN) in each layer.This yields monotonic and limited position correlations. (b) We argue that each layerâ€™s token embedding and PE need independentLNs (LNT, LNP).In this way, the expressiveness of PE is en-hanced and the position correlations are adjusted into hierarchical and layer-adaptive. 