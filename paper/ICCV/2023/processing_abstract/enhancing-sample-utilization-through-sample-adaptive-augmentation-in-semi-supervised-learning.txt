In semi-supervised learning, unlabeled samples can be utilized through augmentation and consistency regulariza-tion. However, we observed certain samples, even undergo-ing strong augmentation, are still correctly classified with high confidence, resulting in a loss close to zero. It indi-cates that these samples have been already learned well and do not provide any additional optimization benefits to the model. We refer to these samples as “naive sam-ples”. Unfortunately, existing SSL models overlook the characteristics of naive samples, and they just apply the same learning strategy to all samples. To further optimize the SSL model, we emphasize the importance of giving at-tention to naive samples and augmenting them in a more diverse manner. Sample adaptive augmentation (SAA) is proposed for this stated purpose and consists of two mod-ules: 1) sample selection module; 2) sample augmentation module. Specifically, the sample selection module picks out naive samples based on historical training informa-tion at each epoch, then the naive samples will be aug-mented in a more diverse manner in the sample augmen-tation module. Thanks to the extreme ease of implementa-tion of the above modules, SAA is advantageous for being simple and lightweight. We add SAA on top of FixMatch and FlexMatch respectively, and experiments demonstrateSAA can significantly improve the models. For example,SAA helped improve the accuracy of FixMatch from 92.50% to 94.76% and that of FlexMatch from 95.01% to 95.31% on CIFAR-10 with 40 labels. The code is available at https://github.com/GuanGui-nju/SAA.*Corresponding author: Yinghuan Shi. Guan Gui, Yinghuan Shi are with the National Key Laboratory for Novel Software Technology and the National Institute of Healthcare Data Science, Nanjing Univer-sity. Lei Qi is with the school of Computer Science and Engineer-ing, Southeast University. This Work is supported by NSFC Program (62222604, 62206052, 62192783), China Postdoctoral Science Founda-tion Project (2023T160100), Jiangsu Natural Science Foundation Project (BK20210224), and CCF-Lenovo Bule Ocean Research Fund. (a) An example of naive sample. (b) Model performance during training.Figure 1: (a) shows an example of naive sample. Its aug-mented versions are correctly classified with high confi-dence, resulting in the loss close to 0. (b) shows the model performance during FixMatch training. Performance im-provements are slow or even stagnant for a period of time. 