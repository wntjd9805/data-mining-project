Recent years have witnessed significant progress in the field of neural surface reconstruction. While extensive focus was put on volumetric and implicit approaches, a number of works have shown that explicit graphics primitives, such as point clouds, can significantly reduce computational com-plexity without sacrificing the reconstructed surface quality.However, less emphasis has been put on modeling dynamic surfaces with point primitives. In this work, we present a dy-namic point field model that combines the representational benefits of explicit point-based graphics with implicit de-formation networks to allow efficient modeling of non-rigid 3D surfaces. Using explicit surface primitives also allows us to easily incorporate well-established constraints such as isometric-as-possible regularization. While learning this deformation model is prone to local optima when trained in a fully unsupervised manner, we propose to also leverage semantic information, such as keypoint correspondence, to guide the deformation learning. We demonstrate how this approach can be used for creating an expressive animat-able human avatar from a collection of 3D scans. Here, previous methods mostly rely on variants of the linear blend skinning paradigm, which fundamentally limits the expres-sivity of such models when dealing with complex cloth ap-pearances, such as long skirts. We show the advantages of our dynamic point field framework in terms of its rep-resentational power, learning efficiency, and robustness to out-of-distribution novel poses. The code for the project is publicly available 1. 