Sparsely-gated Mixture of Expert (MoE), an emerging deep model architecture, has demonstrated a great promise to enable high-accuracy and ultra-efﬁcient model inference.Despite the growing popularity of MoE, little work inves-tigated its potential to advance convolutional neural net-works (CNNs), especially in the plane of adversarial ro-bustness. Since the lack of robustness has become one of the main hurdles for CNNs, in this paper we ask: How to adversarially robustify a CNN-based MoE model? Can we robustly train it like an ordinary CNN model? Our pi-lot study shows that the conventional adversarial training (AT) mechanism (developed for vanilla CNNs) no longer remains effective to robustify an MoE-CNN. To better un-derstand this phenomenon, we dissect the robustness of an MoE-CNN into two dimensions: Robustness of routers (i.e., gating functions to select data-speciﬁc experts) and robustness of experts (i.e., the router-guided pathways de-ﬁned by the subnetworks of the backbone CNN). Our anal-yses show that routers and experts are hard to adapt to each other in the vanilla AT. Thus, we propose a new router-expert alternating Adversarial training framework for MoE, termed ADVMOE. The effectiveness of our pro-posal is justiﬁed across 4 commonly-used CNN model ar-chitectures over 4 benchmark datasets. We ﬁnd that ADV-MOE achieves 1% 4% adversarial robustness improve-ment over the original dense CNN, and enjoys the efﬁciency merit of sparsity-gated MoE, leading to more than 50% in-ference cost reduction. Codes are available at https:// github.com/OPTML-Group/Robust-MoE-CNN .⇠ 