This paper revisits the standard pretrain-then-ﬁnetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervisedMAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we ﬁnd that it scales with the size of the training dataset as well. Thus, ourMAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models.Pre-pretraining consistently improves both the model con-vergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We mea-sure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classiﬁcation, video recog-nition, object detection, low-shot classiﬁcation and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on Food-101 (96.2%). Our study reveals that model initialization plays a signiﬁcant role, even for web-scale pretraining with billions of images. 