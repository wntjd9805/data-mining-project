Vision transformers have achieved leading performance on various visual tasks yet still suffer from high computa-tional complexity. The situation deteriorates in dense pre-diction tasks like semantic segmentation, as high-resolution inputs and outputs usually imply more tokens involved in computations. Directly removing the less attentive tokens has been discussed for the image classiﬁcation task but can not be extended to semantic segmentation since a dense pre-diction is required for every patch. To this end, this work introduces a Dynamic Token Pruning (DToP) method based on the early exit of tokens for semantic segmentation. Mo-tivated by the coarse-to-ﬁne segmentation process by hu-mans, we naturally split the widely adopted auxiliary-loss-based network architecture into several stages, where each auxiliary block grades every token’s difﬁculty level. We canﬁnalize the prediction of easy tokens in advance without completing the entire forward pass. Moreover, we keep k highest conﬁdence tokens for each semantic category to up-hold the representative context information. Thus, compu-tational complexity will change with the difﬁculty of the in-put, akin to the way humans do segmentation. Experiments suggest that the proposed DToP architecture reduces on av-erage 20% ∼ 35% of computational cost for current seman-tic segmentation methods based on plain vision transform-ers without accuracy degradation. The code is available through the following link: https://github.com/ zbwxp/Dynamic-Token-Pruning. 