Viewpoint is a fundamental modality that carries the in-teraction between observers and their environment. This paper proposes the ﬁrst deep-learning framework for the viewpoint modality. The challenge in formulating learning frameworks for viewpoints resides in a suitable multimodal representation that links across the camera viewing space and 3D environment. Traditional approaches reduce the problem to image analysis instances, making them compu-tationally expensive and not adequately modelling the in-trinsic geometry and environmental context of 6DoF view-points. We improve these issues in two ways. 1) We pro-pose a generalized viewpoint representation forgoing the analysis of photometric pixels in favor of encoded viewing ray embeddings attained from point cloud learning frame-works. 2) We propose a novel SE(3)-bijective 6D viewing ray, hyper-ray, that addresses the DoF deﬁciency problem of using 5DoF viewing rays representing 6DoF viewpoints.We demonstrate our approach has both efﬁciency and accu-racy superiority over existing methods in novel real-world environments. 