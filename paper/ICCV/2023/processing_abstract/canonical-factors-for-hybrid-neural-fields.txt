Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesir-able biases that these architectures have for axis-aligned signals—they can lead to radiance field reconstruction dif-ferences of as high as 2 PSNR—and (2) explore how learn-ing a set of canonicalizing transformations can improve rep-resentations by removing these biases. We prove in a simple two-dimensional model problem that a hybrid architecture that simultaneously learns these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using 2D image, signed distance field, and radiance field reconstruction tasks, where we observe im-provements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable ca-pabilities comparable to baselines that are 2x larger, while highlighting weaknesses of standard procedures for evalu-ating neural field representations. 