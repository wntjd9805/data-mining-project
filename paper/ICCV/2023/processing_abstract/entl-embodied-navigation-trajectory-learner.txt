We propose Embodied Navigation Trajectory Learner (ENTL), a method for extracting long sequence representa-tions for embodied navigation. Our approach uniﬁes world modeling, localization and imitation learning into a single sequence prediction task. We train our model using vector-quantized predictions of future states conditioned on cur-rent states and actions. ENTL’s generic architecture en-ables the sharing of the the spatio-temporal sequence en-coder for multiple challenging embodied tasks. We achieve competitive performance on navigation tasks using signif-icantly less data than strong baselines while performing auxiliary tasks such as localization and future frame pre-diction (a proxy for world modeling). A key property of our approach is that the model is pre-trained without any ex-plicit reward signal, which makes the resulting model gen-eralizable to multiple tasks and environments. We release the code at https://github.com/klemenkotar/ENTLFigure 1. We introduce a method for extracting long sequence representations for embodied navigation. The proposed architec-ture enables sharing a spatio-temporal transformer-based back-bone across multiple tasks: navigation, localization, and future frame prediction. 