Semantic segmentation is a crucial task in computer vision that involves segmenting images into semantically meaningful regions at the pixel level. However, existing approaches often rely on expensive human annotations as supervision for model training, limiting their scalability to large, unlabeled datasets. To address this challenge, we present ZeroSeg, a novel method that leverages the exist-ing pretrained vision-language (VL) model (e.g. CLIP vi-sion encoder [39]) to train open-vocabulary zero-shot se-mantic segmentation models. Although acquired exten-sive knowledge of visual concepts, it is non-trivial to ex-ploit knowledge from these VL models to the task of se-mantic segmentation, as they are usually trained at an im-age level. ZeroSeg overcomes this by distilling the visual concepts learned by VL models into a set of segment to-kens, each summarizing a localized region of the target im-age. We evaluate ZeroSeg on multiple popular segmenta-tion benchmarks, including PASCAL VOC 2012, PASCALContext, and COCO, in a zero-shot manner Our approach achieves state-of-the-art performance when compared to other zero-shot segmentation methods under the same train-ing data, while also performing competitively compared to strongly supervised methods. Finally, we also demon-strated the effectiveness of ZeroSeg on open-vocabulary segmentation, through both human studies and qualitative visualizations. The code is publicly available at https://github.com/facebookresearch/ZeroSeg 