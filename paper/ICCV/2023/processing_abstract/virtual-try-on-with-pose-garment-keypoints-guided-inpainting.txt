Virtual try-on is an important technology supporting on-line apparel shopping, which provides consumers with a vir-tual experience to fit garments without physically wearing them. Recently, the image-based virtual try-on has received growing research attention. However, the synthetic results of existing virtual try-on methods usually present distor-tions in garment shape and lose pattern details. In this pa-per, we propose a pose-garment keypoints guided inpaint-ing method for the image-based virtual try-on task, which produces high-fidelity try-on images and well preserves the shapes and patterns of the garments. In our method, hu-man pose and garment keypoints are extracted from source images and constructed as graphs to predict the garment keypoints at the target pose. After which, the predicted key-points are used as guide information to predict the target segmentation map and warp the garment image. The try-on image is finally generated with a semantic-conditioned inpainting scheme using the segmentation map and recom-posed person image as conditions. To verify the effective-ness of our proposed method, we conduct extensive exper-iments on the VITON-HD dataset under both paired and unpaired experimental settings. The qualitative and quan-titative results show that our method significantly outper-forms prior methods at different image resolutions. The codes repository link is https://github.com/lizhi-ntu/KGI. 