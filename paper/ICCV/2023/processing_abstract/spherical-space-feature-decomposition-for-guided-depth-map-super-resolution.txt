Guided depth map super-resolution (GDSR), as a hot topic in multi-modal image processing, aims to upsample low-resolution (LR) depth maps with additional informa-tion involved in high-resolution (HR) RGB images from the same scene. The critical step of this task is to effec-tively extract domain-shared and domain-private RGB/depth features. In addition, three detailed issues, namely blurry edges, noisy surfaces, and over-transferred RGB texture, need to be addressed. In this paper, we propose the Spheri-cal Space feature Decomposition Network (SSDNet) to solve the above issues. To better model cross-modality features,Restormer block-based RGB/depth encoders are employed for extracting local-global features. Then, the extracted features are mapped to the spherical space to complete the separation of private features and the alignment of shared features. Shared features of RGB are fused with the depth features to complete the GDSR task. Subsequently, a spherical contrast reﬁnement (SCR) module is proposed to further address the detail issues. Patches that are clas-siﬁed according to imperfect categories are input into theSCR module, where the patch features are pulled closer to the ground truth and pushed away from the correspond-ing imperfect samples in the spherical feature space via contrastive learning. Extensive experiments demonstrate that our method can achieve state-of-the-art results on four test datasets, as well as successfully generalize to real-world scenes. The code is available at https://github. com/Zhaozixiang1228/GDSR-SSDNet. 