Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural net-works, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift net-works are not as accurate as their full-precision counter-parts, typically suffering from limited weight range encod-ing schemes and quantization loss. In this paper, we pro-pose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive per-formance to full-precision networks for vision and speechIn addition, we introduce a method to de-applications. ploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6× speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift networks do not con-tribute to model capacity and negatively impact inference computation. To address this issue, we propose a zero-free shifting mechanism that simplifies inference and in-creases model capacity. We further propose a sign-scale decomposition design to enhance training efficiency and a low-variance random initialization strategy to improve the model’s transfer learning performance. Our extensive experiments on various computer vision and speech tasks demonstrate that DenseShift outperforms existing low-bit multiplication-free networks and achieves competitive per-formance compared to full-precision networks. Further-more, our proposed approach exhibits strong transfer learn-ing performance without a drop in accuracy. Our code was released on GitHub.Figure 1: Benchmark low-bit DenseShift networks overSOTA low-bit Shift networks on ImageNet using theResNet-18 model architecture. 