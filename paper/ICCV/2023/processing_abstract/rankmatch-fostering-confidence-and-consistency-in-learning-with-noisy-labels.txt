Learning with noisy labels (LNL) is one of the most important and challenging problems in weakly-supervised learning. Recent advances adopt the sample selection strat-egy to mitigate the interference of noisy labels and use small-loss criteria to select clean samples. However, the one-dimensional loss is an over-simplified metric that fails to accommodate the complex feature landscape of various samples, and, hence, is prone to introduce classification er-rors during sample selection.In this paper, we proposeRankMatch, a novel LNL framework that investigates ad-ditional dimensions of confidence and consistency in or-der to combat noisy labels. Confidence-wise, we propose a novel sample selection strategy based on confidence repre-sentation voting instead of the widely-used small-loss cri-terion. This new strategy is capable of increasing sam-ple selection quantity without sacrificing labeling accuracy.Consistency-wise, instead of the widely adopted feature dis-tance metric for measuring the consistency of inner-class samples, we advocate that the rank of principal features is a much more robust indicator. Based on this metric, we propose rank contrastive loss, which strengthens the con-sistency of similar samples regardless of their labels and facilitates feature representation learning. Experimental results on noisy versions of CIFAR-10, CIFAR-100, Cloth-ing1M and WebVision have validated the superiority of our approach over existing state-of-the-art methods. 