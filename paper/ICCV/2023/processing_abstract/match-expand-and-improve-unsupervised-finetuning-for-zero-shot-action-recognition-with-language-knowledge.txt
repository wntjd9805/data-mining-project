Large scale Vision Language (VL) models have shown tremendous success in aligning representations between vi-sual and text modalities. This enables remarkable progress in zero-shot recognition, image generation & editing, and many other exciting tasks. However, VL models tend to over-represent objects while paying much less attention to verbs, and require additional tuning on video data for best zero-shot action recognition performance. While previous work relied on large-scale, fully-annotated data, in this work we propose an unsupervised approach. We adapt aVL model for zero-shot and few-shot action recognition us-ing a collection of unlabeled videos and an unpaired ac-tion dictionary. Based on that, we leverage Large Lan-guage Models and VL models to build a text bag for each unlabeled video via matching, text expansion and caption-ing. We use those bags in a Multiple Instance Learning setup to adapt an image-text backbone to video data. Al-though finetuned on unlabeled video data, our resulting models demonstrate high transferability to numerous un-seen zero-shot downstream tasks, improving the base VL model performance by up to 14%, and even comparing fa-vorably to fully-supervised baselines in both zero-shot and few-shot video recognition transfer. The code is released at https://github.com/wlin-at/MAXI. 