Deep neural networks (DNNs) are vulnerable to back-door attack, which does not affect the network’s perfor-mance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing de-fense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags be-hind a clean model by a large margin.Inspired by the stealthiness and effectiveness of backdoor attack, we pro-pose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned sam-ples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial back-door, once triggered, suppresses the attacker’s backdoor on poisoned data, but has limited influence on clean data.The defense can be carried out during data preprocessing, without any modification to the standard end-to-end train-ing pipeline. We conduct extensive experiments on multi-ple benchmarks with different architectures and representa-tive attacks. Results demonstrate that our method achieves state-of-the-art defense effectiveness with by far the lowest performance drop on clean data. Considering the surpris-ing defense ability displayed by our framework, we call for more attention to utilizing backdoor for backdoor defense.Code is available at https://github.com/damianliumin/non-adversarial backdoor. 