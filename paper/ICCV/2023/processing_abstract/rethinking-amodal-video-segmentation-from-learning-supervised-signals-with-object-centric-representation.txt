Video amodal segmentation is a particularly challeng-ing task in computer vision, which requires to deduce the full shape of an object from the visible parts of it. Recently, some studies have achieved promising performance by us-ing motion flow to integrate information across frames un-der a self-supervised setting. However, motion flow has a clear limitation by the two factors of moving cameras and object deformation. This paper presents a rethink-ing to previous works. We particularly leverage the su-pervised signals with object-centric representation in real-world scenarios. The underlying idea is the supervision signal of the specific object and the features from different views can mutually benefit the deduction of the full mask in any specific frame. We thus propose an Efficient object-centric Representation amodal Segmentation (EoRaS). Spe-cially, beyond solely relying on supervision signals, we de-sign a translation module to project image features into the Birdâ€™s-Eye View (BEV), which introduces 3D informa-tion to improve current feature quality. Furthermore, we propose a multi-view fusion layer based temporal module which is equipped with a set of object slots and interacts with features from different views by attention mechanism to fulfill sufficient object representation completion. As a result, the full mask of the object can be decoded from image features updated by object slots. Extensive experi-ments on both real-world and synthetic benchmarks demon-strate the superiority of our proposed method, achieving state-of-the-art performance. Our code will be released at https://github.com/kfan21/EoRaS. 