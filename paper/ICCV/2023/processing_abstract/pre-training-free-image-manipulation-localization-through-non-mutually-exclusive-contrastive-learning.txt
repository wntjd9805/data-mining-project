Deep Image Manipulation Localization (IML) models suffer from training data insufficiency and thus heavily rely on pre-training. We argue that contrastive learning is more suitable to tackle the data insufficiency problem for IML.Crafting mutually exclusive positives and negatives is the prerequisite for contrastive learning. However, when adopt-ing contrastive learning in IML, we encounter three cate-gories of image patches: tampered, authentic, and contour patches. Tampered and authentic patches are naturally mu-tually exclusive, but contour patches containing both tam-pered and authentic pixels are non-mutually exclusive to them. Simply abnegating these contour patches results in a drastic performance loss since contour patches are deci-sive to the learning outcomes. Hence, we propose the Non-mutually exclusive Contrastive Learning (NCL) framework to rescue conventional contrastive learning from the above dilemma. In NCL, to cope with the non-mutually exclusiv-ity, we first establish a pivot structure with dual branches to constantly switch the role of contour patches between positives and negatives while training. Then, we devise a pivot-consistent loss to avoid spatial corruption caused by the role-switching process. In this manner, NCL both in-herits the self-supervised merits to address the data insuf-ficiency and retains a high manipulation localization accu-racy. Extensive experiments verify that our NCL achieves state-of-the-art performance on all five benchmarks with-out any pre-training and is more robust on unseen real-life samples. https://github.com/Knightzjz/NCL-IML. 