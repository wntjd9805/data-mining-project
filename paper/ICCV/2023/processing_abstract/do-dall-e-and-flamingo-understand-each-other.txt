The ﬁeld of multimodal research focusing on the com-prehension and creation of both images and text has wit-nessed signiﬁcant strides. This progress is exempliﬁed by the emergence of sophisticated models dedicated to image captioning at scale, such as the notable Flamingo model and text-to-image generative models, with DALL-E serving as a prominent example. An interesting question worth ex-ploring in this domain is whether Flamingo and DALL-E understand each other. To study this question, we propose a reconstruction task where Flamingo generates a descrip-tion for a given image and DALL-E uses this description as input to synthesize a new image. We argue that these models understand each other if the generated image is similar to the given image. Speciﬁcally, we study the re-lationship between the quality of the image reconstruction and that of the text generation. We ﬁnd that an optimal description of an image is one that gives rise to a gener-ated image similar to the original one. The ﬁnding mo-tivates us to propose a uniﬁed framework to ﬁnetune the text-to-image and image-to-text models. Concretely, the re-construction part forms a regularization loss to guide the tuning of the models. Extensive experiments on multiple datasets with different image captioning and image gener-ation models validate our ﬁndings and demonstrate the ef-fectiveness of our proposed uniﬁed framework. As DALL-E and Flamingo are not publicly available, we use Stable Dif-fusion and BLIP in the remaining work. Project website: https://dalleflamingo.github.io. 