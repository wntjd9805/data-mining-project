Reconstructing interacting hands from monocular im-ages is indispensable in AR/VR applications. Most ex-isting solutions rely on the accurate localization of each skeleton joint. However, these methods tend to be unre-liable due to the severe occlusion and confusing similar-ity among adjacent hand parts. This also defies human perception because humans can quickly imitate an inter-action pattern without localizing all joints. Our key idea is to first construct a two-hand interaction prior and re-cast the interaction reconstruction task as the conditional sampling from the prior. To expand more interaction states, a large-scale multimodal dataset with physical plausibility is proposed. Then a VAE is trained to further condense these interaction patterns as latent codes in a prior distri-bution. When looking for image cues that contribute to in-teraction prior sampling, we propose the interaction adja-cency heatmap (IAH). Compared with a joint-wise heatmap for localization, IAH assigns denser visible features to those invisible joints. Compared with an all-in-one visi-ble heatmap, it provides more fine-grained local interac-tion information in each interaction region. Finally, the correlations between the extracted features and correspond-ing interaction codes are linked by the ViT module. Com-prehensive evaluations on benchmark datasets have ver-ified the effectiveness of this framework. The code and dataset are publicly available at https://github. com/binghui-z/InterPrior_pytorch. 