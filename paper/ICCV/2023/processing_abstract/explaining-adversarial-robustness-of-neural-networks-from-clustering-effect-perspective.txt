Adversarial training (AT) is the most commonly used mechanism to improve the robustness of deep neural net-works. Recently, a novel adversarial attack against inter-mediate layers exploits the extra fragility of adversarially trained networks to output incorrect predictions. The result implies the insufficiency in the searching space of the adver-sarial perturbation in adversarial training. To straighten out the reason for the effectiveness of the intermediate-layer attack, we interpret the forward propagation as the Cluster-ing Effect, characterizing that the intermediate-layer rep-resentations of neural networks for samples i.i.d. to the training set with the same label are similar, and we theo-retically prove the existence of Clustering Effect by corre-sponding Information Bottleneck Theory. We afterward ob-serve that the intermediate-layer attack disobeys the clus-tering effect of the AT-trained model. Inspired by these sig-nificant observations, we propose a regularization method to extend the perturbation searching space during train-ing, named sufficient adversarial training (SAT). We give a proven robustness bound of neural networks through rig-orous mathematical proof. The experimental evaluations manifest the superiority of SAT over other state-of-the-artAT mechanisms in defending against adversarial attacks against both output and intermediate layers. Our code and Appendix can be found at https://github.com/ clustering-effect/SAT. 