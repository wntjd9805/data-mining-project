Existing neural rendering methods for creating human avatars typically either require dense input signals such as video or multi-view images, or leverage a learned prior from large-scale specific 3D human datasets such that re-construction can be performed with sparse-view inputs.Most of these methods fail to achieve realistic reconstruc-tion when only a single image is available. To enable the data-efficient creation of realistic animatable 3D humans, we propose ELICIT, a novel method for learning human-specific neural radiance fields from a single image.In-spired by the fact that humans can effortlessly estimate the body geometry and imagine full-body clothing from a sin-gle image, we leverage two priors in ELICIT: 3D geome-try prior and visual semantic prior. Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned vertex-based template model (i.e., SMPL) and implements the visual clothing semantic prior with the CLIP-based pre-trained models. Both priors are used to jointly guide the optimization for creating plausible content in the invisible*Equal contribution.â€ Corresponding author. areas. Taking advantage of the CLIP models, ELICIT can use text descriptions to generate text-conditioned unseen re-gions. In order to further improve visual details, we pro-pose a segmentation-based sampling strategy that locally refines different parts of the avatar. Comprehensive eval-uations on multiple popular benchmarks, including ZJU-MoCAP, Human3.6M, and DeepFashion, show that ELICIT outperforms strong baseline methods of avatar creation when only a single image is available. The code is pub-lic for research purposes at https://huangyangyi. github.io/ELICIT 