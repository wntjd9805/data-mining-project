Obtaining personalized 3D animatable avatars from a monocular camera has several real world applications in gaming, virtual try-on, animation, and VR/XR, etc. How-ever, it is very challenging to model dynamic and fine-grained clothing deformations from such sparse data. Ex-isting methods for modeling 3D humans from depth data have limitations in terms of computational efficiency, mesh coherency, and flexibility in resolution and topology. For in-stance, reconstructing shapes using implicit functions and extracting explicit meshes per frame is computationally ex-pensive and cannot ensure coherent meshes across frames.Moreover, predicting per-vertex deformations on a pre-designed human template with a discrete surface lacks flex-ibility in resolution and topology. To overcome these limi-tations, we propose a novel method ‘NSF : Neural SurfaceFields’ for modeling 3D clothed humans from monocular depth. NSF defines a neural field solely on the base sur-face which models a continuous and flexible displacement field. NSF can be adapted to the base surface with dif-ferent resolution and topology without retraining at infer-ence time. Compared to existing approaches, our method eliminates the expensive per-frame surface extraction while maintaining mesh coherency, and is capable of reconstruct-ing meshes with arbitrary resolution without retraining. To foster research in this direction, we release our code in project page at: https://yuxuan-xue.com/nsf.* denotes equal contribution