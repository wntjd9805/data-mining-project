Query: A man opens the door and enters the room.Relevant moments ntRelevan t momentsAlmost all previous text-to-video retrieval works assume that videos are pre-trimmed with short durations. How-ever, in practice, videos are generally untrimmed contain-ing much background content. In this work, we investigate the more practical but challenging Partially Relevant VideoRetrieval (PRVR) task, which aims to retrieve partially rel-evant untrimmed videos with the query input. Particularly, we propose to address PRVR from a new perspective, i.e., distilling the generalization knowledge from the large-scale vision-language pre-trained model and transferring it to a task-speciﬁc PRVR network. To be speciﬁc, we introduce aDual Learning framework with Dynamic Knowledge Distil-lation (DL-DKD), which exploits the knowledge of a large vision-language model as the teacher to guide a student model. During the knowledge distillation, an inheritance student branch is devised to absorb the knowledge from the teacher model. Considering that the large model may be of mediocre performance due to the domain gaps, we further develop an exploration student branch to take the beneﬁts of task-speciﬁc information. In addition, a dynami-cal knowledge distillation strategy is further devised to ad-just the effect of each student branch learning during the training. Experiment results demonstrate that our proposed model achieves state-of-the-art performance on ActivityNet and TVR datasets for PRVR. 