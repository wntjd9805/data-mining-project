Task-Free Continual Learning (TFCL) aims to learn new concepts from a stream of data without any task informa-tion. The Dynamic Expansion Model (DEM) has shown promising results in TFCL by dynamically expanding the modelâ€™s capacity to deal with shifts in the data distribution.However, existing approaches only consider the recognition of the input shift as the expansion signal and ignore the cor-relation between the newly incoming data and previously learned knowledge, resulting in adding and training unnec-essary parameters. In this paper, we propose a novel and effective framework for TFCL, which dynamically expands the architecture of a DEM model through a self-assessment mechanism evaluating the diversity of knowledge among ex-isting experts as expansion signals. This mechanism en-sures learning additional underlying data distributions with a compact model structure. A novelty-aware sample se-lection approach is proposed to manage the memory buffer that forces the newly added expert to learn novel informa-tion from a data stream, which further promotes the diver-sity among experts. Moreover, we also propose to reuse previously learned representation information for learning new incoming data by using knowledge transfer in TFCL, which has not been explored before. The DEM expan-sion and training are regularized through a gradient up-dating mechanism to gradually explore the positive forward transfer, further improving the performance. Empirical re-sults on TFCL benchmarks show that the proposed frame-work outperforms the state-of-the-art while using a rea-sonable number of parameters. The code is available at https://github.com/dtuzi123/SEDEM/. 