Multi-task visual learning is a critical aspect of com-puter vision. Current research, however, predominantly concentrates on the multi-task dense prediction setting, which overlooks the intrinsic 3D world and its multi-view consistent structures, and lacks the capability for versatile imagination. In response to these limitations, we present a novel problem setting â€“ multi-task view synthesis (MTVS), which reinterprets multi-task prediction as a set of novel-view synthesis tasks for multiple scene properties, includ-ing RGB. To tackle the MTVS problem, we propose Mu-vieNeRF, a framework that incorporates both multi-task and cross-view knowledge to simultaneously synthesize multi-ple scene properties. MuvieNeRF integrates two key mod-ules, the Cross-Task Attention (CTA) and Cross-View Atten-tion (CVA) modules, enabling the efficient use of informa-tion across multiple views and tasks. Extensive evaluation on both synthetic and realistic benchmarks demonstrates that MuvieNeRF is capable of simultaneously synthesiz-ing different scene properties with promising visual qual-ity, even outperforming conventional discriminative mod-els in various settings. Notably, we show that MuvieNeRF exhibits universal applicability across a range of NeRF backbones. Our code is available at https://github. com/zsh2000/MuvieNeRF. 