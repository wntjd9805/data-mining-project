Personalized federated learning (PFL) reduces the im-pact of non-independent and identically distributed (non-IID) data among clients by allowing each client to train a personalized model when collaborating with others. A key question in PFL is to decide which parameters of a client should be localized or shared with others. In current main-stream approaches, all layers that are sensitive to non-IID data (such as classifier layers) are generally personalized.The reasoning behind this approach is understandable, as localizing parameters that are easily influenced by non-IID data can prevent the potential negative effect of collabora-tion. However, we believe that this approach is too conser-vative for collaboration. For example, for a certain client, even if its parameters are easily influenced by non-IID data, it can still benefit by sharing these parameters with clients having similar data distribution. This observation empha-sizes the importance of considering not only the sensitivity to non-IID data but also the similarity of data distribution when determining which parameters should be localized inPFL. This paper introduces a novel guideline for client col-laboration in PFL. Unlike existing approaches that prohibit all collaboration of sensitive parameters, our guideline al-lows clients to share more parameters with others, leading to improved model performance. Additionally, we propose a new PFL method named FedCAC, which employs a quan-titative metric to evaluate each parameterâ€™s sensitivity to non-IID data and carefully selects collaborators based on*Corresponding author this evaluation. Experimental results demonstrate that Fed-CAC enables clients to share more parameters with others, resulting in superior performance compared to state-of-the-art methods, particularly in scenarios where clients have diverse distributions. The code is integrated into our FL training framework: https://github.com/kxzxvbk/Fling. 