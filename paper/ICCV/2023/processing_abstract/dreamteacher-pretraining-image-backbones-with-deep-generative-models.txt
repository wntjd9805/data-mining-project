In this work, we introduce a self-supervised feature rep-resentation learning framework DreamTeacher that utilizes generative networks for pre-training downstream image backbones. We propose to distill knowledge from a trained generative model into standard image backbones that have been well engineered for speciﬁc perception tasks. We investi-gate two types of knowledge distillation: 1) distilling learned generative features onto target image backbones as an al-ternative to pretraining these backbones on large labeled datasets such as ImageNet, and 2) distilling labels obtained from generative networks with task heads onto logits of target backbones. We perform extensive analyses on multiple gener-ative models, dense prediction benchmarks, and several pre-training regimes. We empirically ﬁnd that our DreamTeacher signiﬁcantly outperforms existing self-supervised represen-tation learning approaches across the board. UnsupervisedImageNet pre-training with DreamTeacher leads to signiﬁ-cant improvements over ImageNet classiﬁcation pre-training on downstream datasets, showcasing generative models, and diffusion generative models speciﬁcally, as a promising ap-proach to representation learning on large, diverse datasets without requiring manual annotation. 