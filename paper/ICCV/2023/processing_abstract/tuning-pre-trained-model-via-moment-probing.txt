Recently, efficient fine-tuning of large-scale pre-trained models has attracted increasing research interests, where linear probing (LP) as a fundamental module is involved in exploiting the final representations for task-dependent classification. However, most of the existing methods fo-cus on how to effectively introduce a few of learnable pa-rameters, and little work pays attention to the commonly used LP module.In this paper, we propose a novel Mo-ment Probing (MP) method to further explore the potential of LP. Distinguished from LP which builds a linear classifi-cation head based on the mean of final features (e.g., word tokens for ViT) or classification tokens, our MP performs a linear classifier on feature distribution, which provides the stronger representation ability by exploiting richer statisti-cal information inherent in features. Specifically, we repre-sent feature distribution by its characteristic function, which is efficiently approximated by using first- and second-order moments of features. Furthermore, we propose a multi-head convolutional cross-covariance (MHC3) to compute second-order moments in an efficient and effective manner.By considering that MP could affect feature learning, we introduce a partially shared module to learn two recalibrat-ing parameters (PSRP) for backbones based on MP, namelyMP+. Extensive experiments on ten benchmarks using var-ious models show that our MP significantly outperforms LP and is competitive with counterparts at lower training cost, while our MP+ achieves state-of-the-art performance. 