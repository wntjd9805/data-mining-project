Multi-modality fusion and multi-task learning are be-coming trendy in 3D autonomous driving scenario, consid-ering robust prediction and computation budget. However, naively extending the existing framework to the domain of multi-modality multi-task learning remains ineffective and even poisonous due to the notorious modality bias and task conflict. Previous works manually coordinate the learning framework with empirical knowledge, which may lead to sub-optima. To mitigate the issue, we propose a novel yet simple multi-level gradient calibration learning framework across tasks and modalities during optimization. Specifi-cally, the gradients, produced by the task heads and used to update the shared backbone, will be calibrated at the back-boneâ€™s last layer to alleviate the task conflict. Before the calibrated gradients are further propagated to the modal-ity branches of the backbone, their magnitudes will be cal-ibrated again to the same level, ensuring the downstream tasks pay balanced attention to different modalities. Ex-periments on large-scale benchmark nuScenes demonstrate the effectiveness of the proposed method, e.g., an absolute 14.4% mIoU improvement on map segmentation and 1.4% mAP improvement on 3D detection, advancing the appli-cation of 3D autonomous driving in the domain of multi-modality fusion and multi-task learning. We also discuss the links between modalities and tasks. 