While NeRF-based human representations have shown impressive novel view synthesis results, most methods still rely on a large number of images / views for training. In this work, we propose a novel animatable NeRF calledActorsNeRF. It is first pre-trained on diverse human sub-jects, and then adapted with few-shot monocular video frames for a new actor with unseen poses. Building on previous generalizable NeRFs with parameter sharing us-ing a ConvNet encoder, ActorsNeRF further adopts two hu-man priors to capture the large human appearance, shape, and pose variations. Specifically, in the encoded feature space, we will first align different human subjects in a category-level canonical space, and then align the same human from different frames in an instance-level canon-ical space for rendering. We quantitatively and qualita-tively demonstrate that ActorsNeRF significantly outper-forms the existing state-of-the-art on few-shot generaliza-tion to new people and poses on multiple datasets. Project page: https://jitengmu.github.io/ActorsNeRF/. 