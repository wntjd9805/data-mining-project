Text-driven human motion generation in computer vi-sion is both significant and challenging. However, cur-rent methods are limited to producing either deterministic or imprecise motion sequences, failing to effectively con-trol the temporal and spatial relationships required to con-form to a given text description. In this work, we propose a fine-grained method for generating high-quality, condi-tional human motion sequences supporting precise text de-scription. Our approach consists of two key components: 1) a linguistics-structure assisted module that constructs ac-curate and complete language feature to fully utilize text information; and 2) a context-aware progressive reasoning module that learns neighborhood and overall semantic lin-guistics features from shallow and deep graph neural net-works to achieve a multi-step inference. Experiments show that our approach outperforms text-driven motion genera-tion methods on HumanML3D and KIT test sets and gener-ates better visually confirmed motion to the text conditions. 