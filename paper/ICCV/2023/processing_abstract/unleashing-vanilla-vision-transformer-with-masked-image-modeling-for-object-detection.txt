We present an approach to efficiently and effectively adapt a masked image modeling (MIM) pre-trained vanilla VisionTransformer (ViT) for object detection, which is based on our two novel observations: (i) A MIM pre-trained vanillaViT encoder can work surprisingly well in the challenging object-level recognition scenario even with randomly sam-pled partial observations, e.g., only 25% ∼ 50% of the input embeddings. (ii) In order to construct multi-scale rep-resentations for object detection from single-scale ViT, a randomly initialized compact convolutional stem supplants the pre-trained patchify stem, and its intermediate features can naturally serve as the higher resolution inputs of a fea-ture pyramid network without further upsampling or other manipulations. While the pre-trained ViT is only regarded as the 3rd-stage of our detector’s backbone instead of the whole feature extractor. This naturally results in a ConvNet-ViT hybrid architecture. The proposed detector, namedMIMDET, enables a MIM pre-trained vanilla ViT to out-perform leading hierarchical architectures such as SwinTransformer, MViTv2 and ConvNeXt on COCO object de-tection & instance segmentation, and achieves better results compared with the previous best adapted vanilla ViT detec-tor using a more modest fine-tuning recipe while converging 2.8× faster. Code and pre-trained models are available at https://github.com/hustvl/MIMDet. 