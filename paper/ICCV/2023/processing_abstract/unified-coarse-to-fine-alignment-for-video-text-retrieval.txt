The canonical approach to video-text retrieval lever-ages a coarse-grained or fine-grained alignment between visual and textual information. However, retrieving the cor-rect video according to the text query is often challenging as it requires the ability to reason about both high-level (scene) and low-level (object) visual clues and how they re-late to the text query. To this end, we propose a UnifiedCoarse-to-fine Alignment model, dubbed UCOFIA. Specif-ically, our model captures the cross-modal similarity infor-mation at different granularity levels. To alleviate the ef-fect of irrelevant visual clues, we also apply an Interac-tive Similarity Aggregation module (ISA) to consider the importance of different visual features while aggregating the cross-modal similarity to obtain a similarity score for each granularity. Finally, we apply the Sinkhorn-Knopp al-gorithm to normalize the similarities of each level before summing them, alleviating over- and under-representation issues at different levels. By jointly considering the cross-modal similarity of different granularity, UCOFIA allows the effective unification of multi-grained alignments. Em-pirically, UCOFIA outperforms previous state-of-the-artCLIP-based methods on multiple video-text retrieval bench-marks, achieving 2.4%, 1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT, Activity-Net, andDiDeMo, respectively. Our code is publicly available at https://github.com/Ziyang412/UCoFiA. 