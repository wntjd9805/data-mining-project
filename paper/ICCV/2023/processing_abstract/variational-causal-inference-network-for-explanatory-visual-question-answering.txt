Explanatory Visual Question Answering (EVQA) is a re-cently proposed multimodal reasoning task that requires an-swering visual questions and generating multimodal expla-nations for the reasoning processes. Unlike traditional Vi-sual Question Answering (VQA) which focuses solely on an-swering, EVQA aims to provide user-friendly explanations to enhance the explainability and credibility of reasoning models. However, existing EVQA methods typically predict the answer and explanation separately, which ignores the causal correlation between them. Moreover, they neglect the complex relationships among question words, visual re-gions, and explanation tokens. To address these issues, we propose a Variational Causal Inference Network (VCIN) that establishes the causal correlation between predicted answers and explanations, and captures cross-modal rela-tionships to generate rational explanations. First, we uti-lize a vision-and-language pretrained model to extract vi-sual features and question features. Secondly, we propose a multimodal explanation gating transformer that constructs cross-modal relationships and generates rational explana-tions. Finally, we propose a variational causal inference to establish the target causal structure and predict the an-swers. Comprehensive experiments demonstrate the superi-ority of VCIN over state-of-the-art EVQA methods. 