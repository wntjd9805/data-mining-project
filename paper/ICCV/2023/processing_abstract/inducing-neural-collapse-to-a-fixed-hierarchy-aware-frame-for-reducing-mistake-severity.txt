There is a recently discovered and intriguing phe-nomenon called Neural Collapse: at the terminal phase of training a deep neural network for classification, the within-class penultimate feature means and the associated classifier vectors of all flat classes collapse to the vertices of a simplex Equiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon by fixing the related classifier weights to a pre-computed ETF to induce neural collapse and maximize the separation of the learned fea-In this work, tures when training with imbalanced data. we propose to fix the linear classifier of a deep neural net-work to a Hierarchy-Aware Frame (HAFrame), instead of an ETF, and use a cosine similarity-based auxiliary loss to learn hierarchy-aware penultimate features that collapse to the HAFrame. We demonstrate that our approach re-duces the mistake severity of the modelâ€™s predictions while maintaining its top-1 accuracy on several datasets of vary-ing scales with hierarchies of heights ranging from 3 to 12.Code: https://github.com/ltong1130ztr/HAFrame. 