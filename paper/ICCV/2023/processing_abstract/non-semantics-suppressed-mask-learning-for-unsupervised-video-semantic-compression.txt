Most video compression methods aim to improve the de-coded video visual quality, instead of particularly guar-anteeing the semantic-completeness, which deteriorates downstream video analysis tasks, e.g., action recogni-tion.In this paper, we focus on a novel unsupervised video semantic compression problem, where video seman-tics is compressed in a downstream task-agnostic man-ner. To tackle this problem, we first propose a Semantic-Mining-then-Compensation (SMC) framework to enhance the plain video codec with powerful semantic coding ca-pability. Then, we optimize the framework with only un-labeled video data, by masking out a proportion of the compressed video and reconstructing the masked regions of the original video, which is inspired by recent masked im-age modeling (MIM) methods. Although the MIM scheme learns generalizable semantic features, its inner generative learning paradigm may also facilitate the coding frame-work memorizing non-semantic information with extra bit costs. To suppress this deficiency, we explicitly decrease the non-semantic information entropy of the decoded video fea-tures, by formulating it as a parametrized Gaussian MixtureModel conditioned on the mined video semantics. Com-prehensive experimental results demonstrate the proposed approach shows remarkable superiority over previous tra-ditional, learnable, and perceptual quality-oriented video codecs, on three video analysis tasks and seven datasets. 