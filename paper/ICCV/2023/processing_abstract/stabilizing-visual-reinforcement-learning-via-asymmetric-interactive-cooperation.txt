Vision-based reinforcement learning (RL) depends on discriminative representation encoders to abstract the ob-servation states. Despite the great success of increas-ing CNN parameters for many supervised computer vi-sion tasks, reinforcement learning with temporal-difference (TD) losses cannot benefit from it in most complex envi-ronments.In this paper, we analyze that the training in-stability arises from the oscillating self-overfitting of the heavy-optimizable encoder. We argue that serious oscilla-tion will occur to the parameters when enforced to fit the sensitive TD targets, causing uncertain drifting of the latent state space and thus transmitting these perturbations to the policy learning. To alleviate this phenomenon, we propose a novel asymmetric interactive cooperation approach with the interaction between a heavy-optimizable encoder and a supportive light-optimizable encoder, in which both their advantages are integrated including the highly discrimina-tive capability as well as the training stability. We also present a greedy bootstrapping optimization to isolate the visual perturbations from policy learning, where represen-tation and policy are trained sufficiently by turns. Finally, we demonstrate the effectiveness of our method in utilizing larger visual models by first-person highway driving taskCARLA and Vizdoom environments. 