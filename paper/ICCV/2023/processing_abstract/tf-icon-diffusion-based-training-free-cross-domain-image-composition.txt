Text-driven diffusion models have exhibited impres-sive generative capabilities, enabling various image edit-ing tasks. In this paper, we propose TF-ICON, a novelTraining-Free Image COmpositioN framework that har-nesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seam-lessly integrate user-provided objects into a speciﬁc vi-sual context. Current diffusion-based methods often involve costly instance-based optimization or ﬁnetuning of pre-trained models on customized datasets, which can poten-tially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring addi-tional training, ﬁnetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no in-formation, to facilitate text-driven diffusion models in ac-curately inverting real images into latent representations, forming the basis for compositing. Our experiments show that equipping Stable Diffusion with the exceptional prompt outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ, COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile visual domains.Code is available at https://github.com/Shilin-LU/TF-ICON 