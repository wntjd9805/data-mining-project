Token compression aims to speed up large-scale vision transformers (e.g. ViTs) by pruning (dropping) or merg-ing tokens.It is an important but challenging task. Al-though recent advanced approaches achieved great suc-cess, they need to carefully handcraft a compression rate (i.e. number of tokens to remove), which is tedious and leads to sub-optimal performance. To tackle this problem, we propose Differentiable Compression Rate (DiffRate), a novel token compression method that has several appeal-ing properties prior arts do not have. First, DiffRate en-ables propagating the loss function’s gradient onto the com-pression ratio, which is considered as a non-differentiable hyperparameter in previous work.In this case, different layers can automatically learn different compression rates layer-wisely without extra overhead. Second, token pruning and merging can be naturally performed simultaneously inDiffRate, while they were isolated in previous works. Third, extensive experiments demonstrate that DiffRate achieves state-of-the-art performance. For example, by applying the learned layer-wise compression rates to an off-the-shelfViT-H (MAE) model, we achieve a 40% FLOPs reduction and a 1.5× throughput improvement, with a minor accu-racy drop of 0.16% on ImageNet without ﬁne-tuning, even outperforming previous methods with ﬁne-tuning. Codes and models are available at https://github.com/OpenGVLab/DiffRate. 