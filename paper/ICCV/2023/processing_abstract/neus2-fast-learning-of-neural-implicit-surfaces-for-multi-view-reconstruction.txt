Recent methods for neural surface representation and rendering, for example NeuS [59], have demonstrated the remarkably high-quality reconstruction of static scenes.However, the training of NeuS takes an extremely long time (8 hours), which makes it almost impossible to apply them to dynamic scenes with thousands of frames. We propose a fast neural surface reconstruction approach, called NeuS2, which achieves two orders of magnitude improvement in terms of acceleration without compromising reconstruction quality. To accelerate the training process, we parameter-ize a neural surface representation by multi-resolution hash encodings and present a novel lightweight calculation of second-order derivatives tailored to our networks to lever-age CUDA parallelism, achieving a factor two speed up. To further stabilize and expedite training, a progressive learn-ing strategy is proposed to optimize multi-resolution hash encodings from coarse to fine. We extend our method for fast training of dynamic scenes, with a proposed incremen-tal training strategy and a novel global transformation pre-diction component, which allow our method to handle chal-lenging long sequences with large movements and defor-mations. Our experiments on various datasets demonstrate that NeuS2 significantly outperforms the state-of-the-arts in both surface reconstruction accuracy and training speed for both static and dynamic scenes. The code is available at our website: https://vcai.mpi-inf.mpg.de/ projects/NeuS2/.âˆ— Equal contributionFigure 1. We present NeuS2, a fast neural scene reconstruction method. Given a set of multi-view images, NeuS2 can accurately reconstruct the scene geometry and appearance in the order of minutes. This is in stark contrast to previous work [59], which only recovers medium-scale details at significantly increased time (about 8 hours). Moreover, we demonstrate that NeuS2 can also be applied to dynamic scene reconstruction from multi-view videos, where we recover per-frame reconstruction in about 20 seconds. 