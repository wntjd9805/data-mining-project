Self-supervised learning on large-scale multi-modal datasets allows learning semantically meaningful embed-dings in a joint multi-modal representation space without relying on human annotations. These joint embeddings en-able zero-shot cross-modal tasks like retrieval and classifi-cation. However, these methods often struggle to general-ize well on out-of-domain data as they ignore the semantic structure present in modality-specific embeddings. In this context, we propose a novel Semantic-Structure-PreservingConsistency approach to improve generalizability by pre-serving the modality-specific relationships in the joint em-bedding space. To capture modality-specific semantic re-lationships between samples, we propose to learn multi-ple anchors and represent the multifaceted relationship be-tween samples with respect to their relationship with these anchors. To assign multiple anchors to each sample, we propose a novel Multi-Assignment Sinkhorn-Knopp algo-rithm. Our experimentation demonstrates that our pro-posed approach learns semantically meaningful anchors in a self-supervised manner. Furthermore, our evaluation onMSR-VTT and YouCook2 datasets demonstrates that our proposed multi-anchor assignment based solution achieves state-of-the-art performance and generalizes to both in-and out-of-domain datasets. Code: https://github. com/Swetha5/Multi_Sinkhorn_Knopp 