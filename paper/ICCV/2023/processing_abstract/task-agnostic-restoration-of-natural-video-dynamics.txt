In many video restoration/translation tasks, image pro-cessing operations are na¨ıvely extended to the video do-main by processing each frame independently, disregard-ing the temporal connection of the video frames. This dis-regard for the temporal connection often leads to severe temporal inconsistencies. State-Of-The-Art (SOTA) tech-niques that address these inconsistencies rely on the avail-ability of unprocessed videos to implicitly siphon and utilize consistent video dynamics to restore the temporal consis-tency of frame-wise processed videos which often jeopar-dizes the translation effect. We propose a general frame-work for this task that learns to infer and utilize consis-tent motion dynamics from inconsistent videos to mitigate the temporal flicker while preserving the perceptual qual-ity for both the temporally neighboring and relatively dis-tant frames without requiring the raw videos at test time.The proposed framework produces SOTA results on two benchmark datasets, DAVIS and videvo.net, processed by numerous image processing applications. The code and the trained models are available at https://github. com/MKashifAli/TARONVD. 