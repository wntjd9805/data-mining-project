Federated Learning (FL) is a decentralized machine learning paradigm, in which multiple clients collabora-tively train neural networks without centralizing their lo-cal data, and hence preserve data privacy. However, real-world FL applications usually encounter challenges aris-ing from distribution shifts across the local datasets of in-dividual clients. These shifts may drift the global model aggregation or result in convergence to deflected local op-timum. While existing efforts have addressed distribution shifts in the label space, an equally important challenge re-mains relatively unexplored. This challenge involves situa-tions where the local data of different clients indicate iden-tical label distributions but exhibit divergent feature dis-tributions. This issue can significantly impact the global model performance in the FL framework. In this work, we propose Federated Representation Augmentation (FRAug) to resolve this practical and challenging problem. FRAug optimizes a shared embedding generator to capture client consensus. Its output synthetic embeddings are transformed into client-specific by a locally optimized RTNet to augment the training space of each client. Our empirical evalua-tion on three public benchmarks and a real-world medi-cal dataset demonstrates the effectiveness of the proposed method, which substantially outperforms the current state-of-the-art FL methods for feature distribution shifts, includ-ing PartialFed and FedBN. 