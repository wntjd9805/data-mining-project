Personalized federated learning has received an upsurge of attention due to the mediocre performance of conven-tional federated learning (FL) over heterogeneous data.Unlike conventional FL which trains a single global con-sensus model, personalized FL allows different models for different clients. However, existing personalized FL algo-rithms only implicitly transfer the collaborative knowledge across the federation by embedding the knowledge into the aggregated model or regularization. We observed that this implicit knowledge transfer fails to maximize the potential of each client’s empirical risk toward other clients. Based on our observation, in this work, we propose PersonalizedGlobal Federated Learning (PGFed), a novel personalizedFL framework that enables each client to personalize its own global objective by explicitly and adaptively aggregat-ing the empirical risks of itself and other clients. To avoid massive (O(N 2)) communication overhead and potential privacy leakage while achieving this, each client’s risk is estimated through a first-order approximation for other clients’ adaptive risk aggregation. On top of PGFed, we develop a momentum upgrade, dubbed PGFedMo, to more efficiently utilize clients’ empirical risks. Our extensive ex-periments on four datasets under different federated set-tings show consistent improvements of PGFed over previ-ous state-of-the-art methods. The code is publicly available at https://github.com/ljaiverson/pgfed. 