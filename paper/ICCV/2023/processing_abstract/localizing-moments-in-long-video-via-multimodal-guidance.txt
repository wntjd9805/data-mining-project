The recent introduction of the large-scale, long-formMAD and Ego4D datasets has enabled researchers to in-vestigate the performance of current state-of-the-art meth-ods for video grounding in the long-form setup, with inter-esting findings: current grounding methods alone fail at tackling this challenging task and setup due to their in-ability to process long video sequences. In this paper, we propose a method for improving the performance of natu-ral language grounding in long videos by identifying and pruning out non-describable windows. We design a guided grounding framework consisting of a Guidance Model and a base grounding model. The Guidance Model emphasizes describable windows, while the base grounding model ana-lyzes short temporal windows to determine which segments accurately match a given language query. We offer two de-signs for the Guidance Model: Query-Agnostic and Query-Dependent, which balance efficiency and accuracy. Ex-periments demonstrate that our proposed method outper-forms state-of-the-art models by 4.1% in MAD and 4.52% in Ego4D (NLQ), respectively. Code, data and MADâ€™s audio features necessary to reproduce our experiments are available at: https://github.com/waybarrios/guidance-based-video-grounding. 