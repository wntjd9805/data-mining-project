In this paper, we rethink the low-light image enhance-ment task and propose a physically explainable and gen-erative diffusion model for low-light image enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the physical model and the generative network. Further-more, we hope to supplement and even deduce the infor-mation missing in the low-light image through the genera-tive network. Therefore, Diff-Retinex formulates the low-light image enhancement problem into Retinex decompo-sition and conditional image generation.In the Retinex decomposition, we integrate the superiority of attention in Transformer and meticulously design a Retinex Trans-former decomposition network (TDN) to decompose the image into illumination and reï¬‚ectance maps. Then, we design multi-path generative diffusion networks to recon-struct the normal-light Retinex probability distribution and solve the various degradations in these components respec-tively, including dark illumination, noise, color deviation, loss of scene contents, etc. Owing to generative diffusion model, Diff-Retinex puts the restoration of low-light sub-tle detail into practice. Extensive experiments conducted on real-world low-light datasets qualitatively and quantita-tively demonstrate the effectiveness, superiority, and gener-alization of the proposed method. 