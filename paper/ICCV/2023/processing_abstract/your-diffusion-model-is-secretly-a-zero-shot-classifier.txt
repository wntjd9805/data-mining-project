The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit im-pressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; how-ever, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image gener-In this paper, we show that the density estimates ation. from large-scale text-to-image diffusion models like StableDiffusion can be leveraged to perform zero-shot classiﬁ-cation without any additional training. Our generative approach to classiﬁcation, which we call Diffusion Clas-siﬁer, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowl-edge from diffusion models. Although a gap remains be-tween generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has stronger multimodal compositional reasoning abilities than competing discriminative approaches. Finally, we use Dif-fusion Classiﬁer to extract standard classiﬁers from class-conditional diffusion models trained on ImageNet. These models approach the performance of SOTA discrimina-tive classiﬁers and exhibit strong “effective robustness” to distribution shift. Overall, our results are a step to-ward using generative over discriminative models for down-stream tasks. Results and visualizations on our website: diffusion-classifier.github.io/ 