Online continual learning (CL) aims to learn new knowl-edge and consolidate previously learned knowledge from non-stationary data streams. Due to the time-varying train-ing setting, the model learned from a changing distribu-tion easily forgets the previously learned knowledge and bi-ases toward the newly received task. To address this prob-lem, we propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training, such that the classi-fier network is able to learn a stable consolidation of pre-viously learned tasks.In the testing stage, CBA can be removed which introduces no additional computation cost and memory overhead. We theoretically reveal the reason why the proposed method can effectively alleviate catas-trophic distribution shifts, and empirically demonstrate its effectiveness through extensive experiments based on four rehearsal-based baselines and three public continual learn-ing benchmarks1. 