Surrogate gradient (SG) is one of the most effective approaches for training spiking neural networks (SNNs).While assisting SNNs to achieve classiﬁcation performance comparable to artiﬁcial neural networks, SG suffers from the problem of time-consuming training, preventing it fromIn this paper, we formally analyze the efﬁcient learning. backward process of classic SG and ﬁnd that the membrane accumulation through time leads to exponential growth of training time. With this discovery, we propose StabilizedSpiking Flow (SSF), a simple yet effective approach to ac-celerate training of SG-based SNNs. For each spiking neu-ron, SSF averages its input and output activations over time to yield stabilized input and output, respectively. Then, in-stead of back propagating all errors that are related to cur-rent neuron and inherently entangled in time domain, the auxiliary gradient is directly propagated from the stabilized output to input through a devised relationship mapping. Ad-ditionally, SSF method is suitable to different neuron mod-els. Extensive experiments on both static and neuromorphic datasets demonstrate that SNNs trained with SSF approach can achieve performance comparable to the original coun-terparts, while reducing the training time signiﬁcantly. In particular, SSF speeds up the training process of state-of-the-art SNN models up to 10× when time steps equal to 80. 