Recent leading zero-shot video object segmentation (ZVOS) works devote to integrating appearance and mo-tion information by elaborately designing feature fusion modules and identically applying them in multiple fea-ture stages. Our preliminary experiments show that with the strong long-range dependency modeling capacity ofTransformer, simply concatenating the two modality fea-tures and feeding them to vanilla Transformers for fea-ture fusion can distinctly benefit the performance but at a cost of heavy computation. Through further empirical analysis, we find that attention dependencies learned inTransformer in different stages exhibit completely dif-ferent properties: global query-independent dependency in the low-level stages and semantic-specific dependency in the high-level stages. Motivated by the observations, we propose two Transformer variants: i) Context-SharingTransformer (CST) that learns the global-shared contextual information within image frames with a lightweight com-ii) Semantic Gathering-Scattering Transformer putation. (SGST) that models the semantic correlation separately for the foreground and background and reduces the computa-tion cost with a soft token merging mechanism. We ap-ply CST and SGST for low-level and high-level feature fu-sions, respectively, formulating a level-isomerous Trans-* Corresponding author: wyfan@dlut.edu.cn. former framework for ZVOS task. Compared with the base-line that uses vanilla Transformers for multi-stage fusion, ours significantly increase the speed by 13Ã— and achieves new state-of-the-art ZVOS performance. Code is available at https://github.com/DLUT-yyc/Isomer. 