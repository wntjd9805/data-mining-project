Cross-modal alignment is one key challenge for Vision-and-Language Navigation (VLN). Most existing studies concentrate on mapping the global instruction or single sub-instruction to the corresponding trajectory. However, another critical problem of achieving fine-grained align-ment at the entity level is seldom considered. To address this problem, we propose a novel Grounded Entity-LandmarkAdaptive (GELA) pre-training paradigm for VLN tasks. To achieve the adaptive pre-training paradigm, we first intro-duce grounded entity-landmark human annotations into theRoom-to-Room (R2R) dataset, named GEL-R2R. Addition-ally, we adopt three grounded entity-landmark adaptive pre-training objectives: 1) entity phrase prediction, 2) land-mark bounding box prediction, and 3) entity-landmark se-mantic alignment, which explicitly supervise the learning of fine-grained cross-modal alignment between entity phrases and environment landmarks. Finally, we validate our model on two downstream benchmarks: VLN with descriptive in-structions (R2R) and dialogue instructions (CVDN). The comprehensive experiments show that our GELA model achieves state-of-the-art results on both tasks, demonstrat-ing its effectiveness and generalizability. 