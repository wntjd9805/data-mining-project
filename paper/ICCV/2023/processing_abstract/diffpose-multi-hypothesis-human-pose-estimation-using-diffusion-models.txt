Traditionally, monocular 3D human pose estimation em-ploys a machine learning model to predict the most likely 3D pose for a given input image. However, a single im-age can be highly ambiguous and induces multiple plau-sible solutions for the 2D-3D lifting step, which results in overly confident 3D pose predictors. To this end, we pro-pose DiffPose, a conditional diffusion model that predicts multiple hypotheses for a given input image. Compared to similar approaches, our diffusion model is straightfor-ward and avoids intensive hyperparameter tuning, complex network structures, mode collapse, and unstable training.Moreover, we tackle the problem of over-simplification of the intermediate representation of the common two-step ap-proaches which first estimate a distribution of 2D joint lo-cations via joint-wise heatmaps and consecutively use their maximum argument for the 3D pose estimation step. Since such a simplification of the heatmaps removes valid infor-mation about possibly correct, though labeled unlikely, joint locations, we propose to represent the heatmaps as a set of 2D joint candidate samples. To extract information about the original distribution from these samples, we introduce our embedding transformer which conditions the diffusion model. Experimentally, we show that DiffPose improves upon the state of the art for multi-hypothesis pose estima-tion by 3-5% for simple poses and outperforms it by a large margin for highly ambiguous poses.1 