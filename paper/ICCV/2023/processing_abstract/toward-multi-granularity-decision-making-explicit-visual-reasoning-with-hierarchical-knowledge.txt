Answering visual questions requires the ability to parse visual observations and correlate them with a variety of knowledge. Existing visual question answering (VQA) mod-els either pay little attention to the role of knowledge or do not take into account the granularity of knowledge (e.g., attaching the color of “grassland” to “ground”). They have yet to develop the capability of modeling knowl-edge of multiple granularity, and are also vulnerable to spurious data biases. To fill the gap, this paper makes progresses from two distinct perspectives: (1) It presents a Hierarchical Concept Graph (HCG) that discriminates and associates multi-granularity concepts with a multi-layered hierarchical structure, aligning visual observations with knowledge across different levels to alleviate data bi-(2) To facilitate a comprehensive understanding of ases. how knowledge contributes throughout the decision-making process, we further propose an interpretable HierarchicalConcept Neural Module Network (HCNMN). It explicitly propagates multi-granularity knowledge across the hier-archical structure and incorporates them with a sequence of reasoning steps, providing a transparent interface to elaborate on the integration of observations and knowl-edge. Through extensive experiments on multiple challeng-ing datasets (i.e., GQA,VQA,FVQA,OK-VQA), we demon-strate the effectiveness of our method in answering ques-tions in different scenarios. Our code is available at https://github.com/SuperJohnZhang/HCNMN. 