Domain Adaptive Object Detection (DAOD) aims to im-prove the detection performance of target domains by mini-mizing the feature distribution between the source and tar-get domain. Recent approaches usually align such distri-butions in terms of categories through adversarial learn-ing and some progress has been made. However, when ob-jects are non-uniformly distributed at different scales, such category-level alignment causes imbalanced object feature learning, refer as the inconsistency of category alignment at different scales. For better category-level feature alignment, we propose a novel DAOD framework of joint category and scale information, dubbed CSDA, such a design enables ef-fective object learning for different scales. Specifically, our framework is implemented by two closely-related modules: 1) SGFF (Scale-Guided Feature Fusion) fuses the cate-gory representations of different domains to learn category-specific features, where the features are aligned by discrim-inators at three scales. 2) SAFE (Scale-Auxiliary FeatureEnhancement) encodes scale coordinates into a group of tokens and enhances the representation of category-specific features at different scales by self-attention. Based on the anchor-based Faster-RCNN and anchor-free FCOS detec-tors, experiments show that our method achieves state-of-the-art results on three DAOD benchmarks. 