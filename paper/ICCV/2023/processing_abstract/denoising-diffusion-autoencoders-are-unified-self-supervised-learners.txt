Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training. This paper shows that the networks in diffusion models, namely denoising dif-fusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image genera-tion, DDAE has already learned strongly linear-separable representations within its intermediate layers without auxil-iary encoders, thus making diffusion pre-training emerge as a general approach for generative-and-discriminative dual learning. To validate this, we conduct linear probe and fine-tuning evaluations. Our diffusion-based approach achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to contrastive learning and masked autoencoders for the first time. Transfer learning from ImageNet also confirms the suitability of DDAE for Vision Transformers, suggesting the potential to scale DDAEs as unified foundation models.Code is available at github.com/FutureXiang/ddae. (a) Denoising networks in pixel-space and latent-space diffusion models. (b) Evaluating DDAEs as self-supervised representation learners.Figure 1. Denoising Diffusion Autoencoders (DDAE). Top: Dif-fusion networks are essentially equivalent to level-conditional de-noising autoencoders (DAE). The networks are named as DDAEs due to this similarity. Bottom: By linear probe evaluations, we confirm that DDAE can produce strong representations at some in-termediate layers. Truncating and fine-tuning DDAE as vision en-coders further leads to superior image classification performance. 