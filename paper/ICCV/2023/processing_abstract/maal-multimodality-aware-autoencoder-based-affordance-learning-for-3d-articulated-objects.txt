Inferring affordance for 3D articulated objects is a chal-lenging and practical problem. It is a primary problem for applying robots to real-world scenarios. The exploration can be summarized as figuring out where to act and how to act. Correspondingly, the task mainly requires produc-ing actionability scores, action proposals, and success like-lihood scores according to the given 3D object informa-tion and robotic information. Current works usually di-rectly process multi-modal inputs with early fusion and ap-ply critic networks to produce scores, which leads to in-sufficient multi-modal learning ability and inefficiently it-erative training in multiple stages. This paper proposes a novel Multimodality-Aware Autoencoder-based affordanceLearning (MAAL) for the 3D object affordance problem. It is an efficient pipeline, trained in one go, and only requires a few positive samples in training data. More importantly,MAAL contains a MultiModal Energized Encoder (MME) for better multi-modal learning. It comprehensively models all multi-modal inputs from 3D objects and robotic actions.Jointly considering information from multiple modalities, the encoder further learns interactions between robots and objects. MME empowers the better multi-modal learning ability for understanding object affordance. Experimental results and visualizations, based on a large-scale datasetPartNet-Mobility, show the effectiveness of MAAL in learn-ing multi-modal data and solving the 3D articulated object affordance problem. 