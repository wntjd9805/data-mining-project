We present CROSSLOC3D, a novel 3D place recognition method that solves a large-scale point matching problem in a cross-source setting. Cross-source point cloud data cor-responds to point sets captured by depth sensors with dif-ferent accuracies or from different distances and perspec-tives. We address the challenges in terms of developing 3D place recognition methods that account for the repre-sentation gap between points captured by different sources.Our method handles cross-source data by utilizing multi-grained features and selecting convolution kernel sizes that correspond to most prominent features. Inspired by the dif-fusion models, our method uses a novel iterative refinement process that gradually shifts the embedding spaces from dif-ferent sources to a single canonical space for better metric learning. In addition, we present CS-CAMPUS3D, the first 3D aerial-ground cross-source dataset consisting of point cloud data from both aerial and ground LiDAR scans. The point clouds in CS-CAMPUS3D have representation gaps and other features like different views, point densities, and noise patterns. We show that our CROSSLOC3D algorithm can achieve an improvement of 4.74% - 15.37% in terms of the top 1 average recall on our CS-CAMPUS3D bench-mark and achieves performance comparable to state-of-the-art 3D place recognition method on the Oxford RobotCar.The code and CS-CAMPUS3D benchmark will be available at github.com/rayguan97/crossloc3d . 