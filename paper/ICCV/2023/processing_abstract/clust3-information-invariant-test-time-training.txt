Deep Learning models have shown remarkable perfor-mance in a broad range of vision tasks. However, they are often vulnerable to domain shifts at test-time. Test-time training (TTT) methods have been developed in an at-tempt to mitigate these vulnerabilities, where a secondary task is solved at training time, simultaneously with the main task, to be later used as an self-supervised proxy task at test-time.In this work, we propose a novel unsuper-vised TTT technique based on the maximization of MutualInformation between multi-scale feature maps and a dis-crete latent representation, which can be integrated to the standard training as an auxiliary clustering task. Exper-imental results demonstrate competitive classification per-formance on different popular test-time adaptation bench-marks. The code can be found at: https://github. com/dosowiechi/ClusT3.git 