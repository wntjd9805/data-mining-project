Neural ﬁelds, which represent signals as a function pa-rameterized by a neural network, are a promising alterna-tive to traditional discrete vector or grid-based represen-tations. Compared to discrete representations, neural rep-resentations both scale well with increasing resolution, are continuous, and can be many-times differentiable. However, given a dataset of signals that we would like to represent, having to optimize a separate neural ﬁeld for each signal is inefﬁcient, and cannot capitalize on shared information or structures among signals. Existing generalization methods view this as a meta-learning problem and employ gradient-based meta-learning to learn an initialization which is thenﬁne-tuned with test-time optimization, or learn hypernet-works to produce the weights of a neural ﬁeld. We instead propose a new paradigm that views the large-scale training of neural representations as a part of a partially-observed neural process framework, and leverage neural process algorithms to solve this task. We demonstrate that this approach outperforms both state-of-the-art gradient-based meta-learning approaches and hypernetwork approaches. 