Recent text-to-image diffusion models have demon-strated an astonishing capacity to generate high-quality im-ages. However, researchers mainly studied the way of syn-thesizing images with only text prompts. While some works have explored using other modalities as conditions, consid-erable paired data, e.g., box/mask-image pairs, and fine-tuning time are required for nurturing models. As such paired data is time-consuming and labor-intensive to ac-quire and restricted to a closed set, this potentially becomes the bottleneck for applications in an open world. This paper* Corresponding Author focuses on the simplest form of user-provided conditions, e.g., box or scribble. To mitigate the aforementioned prob-lem, we propose a training-free method to control objects and contexts in the synthesized images adhering to the given spatial conditions. Specifically, three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints, are de-signed and seamlessly integrated into the denoising step of diffusion models, requiring no additional training and mas-sive annotated layout data. Extensive experimental results demonstrate that the proposed constraints can control what and where to present in the images while retaining the abil-ity of Diffusion models to synthesize with high fidelity and diverse concept coverage.