Contrastive Language-Image Pretraining has emerged as a prominent approach for training vision and text en-coders with uncurated image-text pairs from the web. To enhance data-efficiency, recent efforts have introduced ad-ditional supervision terms that involve random-augmented views of the image. However, since the image augmentation process is unaware of its text counterpart, this procedure could cause various degrees of image-text misalignments during training. Prior methods either disregarded this dis-crepancy or introduced external models to mitigate the im-pact of misalignments during training. In contrast, we pro-pose a novel metric learning approach that capitalizes on these misalignments as an additional training source, which we term “Misalign, Contrast then Distill (MCD)”. Unlike previous methods that treat augmented images and their text counterparts as simple positive pairs, MCD predicts the continuous scales of misalignment caused by the augmen-tation. Our extensive experimental results show that our proposed MCD achieves state-of-the-art transferability in multiple classification and retrieval downstream datasets. 