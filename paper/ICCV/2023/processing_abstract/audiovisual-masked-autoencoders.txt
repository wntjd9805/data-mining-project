Can we leverage the audiovisual information already present in video to improve self-supervised representation learning? To answer this question, we study various pre-training architectures and objectives within the masked au-toencoding framework, motivated by the success of sim-ilar methods in natural language and image understand-ing. We show that we can achieve significant improvements on audiovisual downstream classification tasks, surpassing the state-of-the-art on VGGSound and AudioSet. Further-more, we can leverage our audiovisual pretraining scheme for multiple unimodal downstream tasks using a single au-diovisual pretrained model. We additionally demonstrate the transferability of our representations, achieving state-of-the-art audiovisual results on Epic Kitchens without pre-training specifically for this dataset. 