Human-Object Interaction (HOI) detection is essential for understanding and modeling real-world events. Ex-isting works on HOI detection mainly focus on static im-ages and a closed setting, where all HOI classes are pro-vided in the training set.In comparison, detecting HOIs in videos in open set scenarios is more challenging. First, under open set circumstances, HOI detectors are expected to hold strong generalizability to recognize unseen HOIs not included in the training data. Second, accurately cap-turing temporal contextual information from videos is diffi-cult, but it is crucial for detecting temporal-related actions such as open, close, pull, push. To this end, we propose ACoLP, a model of Action-centric Chain-of-LookPrompting for open set video HOI detection. ACoLP re-gards actions as the carrier of semantics in videos, which captures the essential semantic information across frames.To make the model generalizable on unseen classes, in-spired by the chain-of-thought prompting in natural lan-guage processing, we introduce the chain-of-look prompt-ing scheme that decomposes prompt generation from large-scale vision-language model into a series of intermediate visual reasoning steps. Consequently, our model captures complex visual reasoning processes underlying the HOI events in videos, providing essential guidance for detecting unseen classes. Extensive experiments on two video HOI datasets, VidHOI and CAD120, demonstrate that ACoLP achieves competitive performance compared with the state-of-the-art methods in the conventional closed setting, and outperforms existing methods by a large margin in the open set setting. Our code is avaliable at https://github. com/southnx/ACoLP. 