Personalized Federated Learning (PFL) represents a promising solution for decentralized learning in heteroge-neous data environments. Partial model personalization has been proposed to improve the efficiency of PFL by se-lectively updating local model parameters instead of ag-gregating all of them. However, previous work on par-tial model personalization has mainly focused on Convo-lutional Neural Networks (CNNs), leaving a gap in un-derstanding how it can be applied to other popular mod-els such as Vision Transformers (ViTs).In this work, we investigate where and how to partially personalize a ViT model.Specifically, we empirically evaluate the sensi-tivity to data distribution of each type of layer. Based on the insights that the self-attention layer and the clas-sification head are the most sensitive parts of a ViT, we propose a novel approach called FedPerfix, which lever-ages plugins to transfer information from the aggregated model to the local client as a personalization. Finally, we evaluate the proposed approach on CIFAR-100, OrganAM-NIST, and Office-Home datasets and demonstrate its effec-tiveness in improving the modelâ€™s performance compared to several advanced PFL methods. Code is available at https://github.com/imguangyu/FedPerfix 