The self-attention mechanism (SAM) is widely used in various fields of artificial intelligence and has successfully boosted the performance of different models. However, cur-rent explanations of this mechanism are mainly based on in-tuitions and experiences, while there still lacks direct mod-eling for how the SAM helps performance. To mitigate this issue, in this paper, based on the dynamical system perspec-tive of the residual neural network, we first show that the in-trinsic stiffness phenomenon (SP) in the high-precision so-lution of ordinary differential equations (ODEs) also widely exists in high-performance neural networks (NN). Thus the ability of NN to measure SP at the feature level is necessary to obtain high performance and is an important factor in the difficulty of training NN. Similar to the adaptive step-size method which is effective in solving stiff ODEs, we show that the SAM is also a stiffness-aware step size adaptor that can enhance the modelâ€™s representational ability to measure intrinsic SP by refining the estimation of stiffness informa-tion and generating adaptive attention values, which pro-vides a new understanding about why and how the SAM can benefit the model performance. This novel perspective can also explain the lottery ticket hypothesis in SAM, design new quantitative metrics of representational ability, and inspire a new theoretic-inspired approach, StepNet. Extensive ex-periments on several popular benchmarks demonstrate thatStepNet can extract fine-grained stiffness information and measure SP accurately, leading to significant improvements in various visual tasks. 