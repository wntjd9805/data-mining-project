This study explores the application of self-supervised learning (SSL) to the task of motion forecasting, an area that has not yet been extensively investigated despite the widespread success of SSL in computer vision and natu-ral language processing. To address this gap, we intro-duce Forecast-MAE, an extension of the mask autoencoders framework that is specifically designed for self-supervised learning of the motion forecasting task. Our approach in-cludes a novel masking strategy that leverages the strong interconnections between agents’ trajectories and road net-works, involving complementary masking of agents’ future or history trajectories and random masking of lane seg-ments. Our experiments on the challenging Argoverse 2 mo-tion forecasting benchmark show that Forecast-MAE, which utilizes standard Transformer blocks with minimal induc-tive bias, achieves competitive performance compared to state-of-the-art methods that rely on supervised learning and sophisticated designs. Moreover, it outperforms the previous self-supervised learning method by a significant margin. Code is available at https://github.com/ jchengai/forecast-mae. 