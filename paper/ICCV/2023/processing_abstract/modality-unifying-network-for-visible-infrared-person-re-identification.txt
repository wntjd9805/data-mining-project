Visible-infrared person re-identification (VI-ReID) is a challenging task due to large cross-modality discrepancies and intra-class variations. Existing methods mainly focus on learning modality-shared representations by embedding different modalities into the same feature space. As a result, the learned feature emphasizes the common patterns across modalities while suppressing modality-specific and identity-aware information that is valuable for Re-ID. To address these issues, we propose a novel Modality Unifying Net-work (MUN) to explore a robust auxiliary modality for VI-ReID. First, the auxiliary modality is generated by combin-ing the proposed cross-modality learner and intra-modality learner, which can dynamically model the modality-specific and modality-shared representations to alleviate both cross-modality and intra-modality variations. Second, by align-ing identity centres across the three modalities, an identity alignment loss function is proposed to discover the discrim-inative feature representations. Third, a modality align-ment loss is introduced to consistently reduce the distribu-tion distance of visible and infrared images by modality pro-totype modeling. Extensive experiments on multiple public datasets demonstrate that the proposed method surpasses the current state-of-the-art methods by a significant margin. 