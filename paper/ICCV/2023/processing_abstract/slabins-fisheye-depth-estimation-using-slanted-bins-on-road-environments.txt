(cid:8)(cid:5)(cid:2) (cid:4)(cid:12)(cid:18)(cid:21)(cid:13) (cid:9)(cid:15)(cid:10)(cid:16)(cid:21)(cid:12)(cid:11)(cid:1)(cid:7)(cid:3)(cid:6)(cid:1)(cid:19)(cid:12)(cid:18)(cid:19)(cid:12)(cid:20)(cid:12)(cid:16)(cid:21)(cid:10)(cid:21)(cid:14)(cid:17)(cid:16)Although 3D perception for autonomous vehicles has fo-cused on frontal-view information, more than half of fatal accidents occur due to side impacts in practice (e.g., T-bone crash). Motivated by this fact, we investigate the prob-lem of side-view depth estimation, especially for monocu-lar ﬁsheye cameras, which provide wide FoV information.However, since ﬁsheye cameras head road areas, it ob-serves road areas mostly and results in severe distortion on object areas, such as vehicles or pedestrians. To al-leviate these issues, we propose a new ﬁsheye depth es-timation network, SlaBins, that infers an accurate and dense depth map based on a geometric property of road en-vironments; most objects are standing (i.e., orthogonal) on the road environments. Concretely, we introduce a slanted multi-cylindrical image (MCI) representation, which al-lows us to describe a distance as a radius to a cylindri-cal layer orthogonal to the ground regardless of the cam-era viewing direction. Based on the slanted MCI, we es-timate a set of adaptive bins and a per-pixel probability map for depth estimation. Then by combining it with the estimated slanted angle of viewing direction, we directly in-fer a dense and accurate depth map for ﬁsheye cameras.Experiments demonstrate that SlaBins outperforms the state-of-the-art methods in both qualitative and quantita-tive evaluation on the SynWoodScape and KITTI-360 depth datasets. For more information, you can visit our project page https://syniez.github.io/SlaBins/. 