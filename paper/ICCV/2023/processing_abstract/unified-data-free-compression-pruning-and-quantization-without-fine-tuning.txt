Structured pruning and quantization are promising ap-proaches for reducing the inference time and memory foot-print of neural networks. However, most existing methods require the original training dataset to fine-tune the model.This not only brings heavy resource consumption but also is not possible for applications with sensitive or proprietary data due to privacy and security concerns. Therefore, a few data-free methods are proposed to address this problem, but they perform data-free pruning and quantization separately, which does not explore the complementarity of pruning and quantization. In this paper, we propose a novel framework named Unified Data-Free Compression(UDFC), which per-forms pruning and quantization simultaneously without any data and fine-tuning process. Specifically, UDFC starts with the assumption that the partial information of a dam-aged(e.g., pruned or quantized) channel can be preserved by a linear combination of other channels, and then de-rives the reconstruction form from the assumption to restore the information loss due to compression. Finally, we for-mulate the reconstruction error between the original net-work and its compressed network, and theoretically deduce the closed-form solution. We evaluate the UDFC on the large-scale image classification task and obtain significant improvements over various network architectures and com-pression methods. For example, we achieve a 20.54% accu-racy improvement on ImageNet dataset compared to SOTA method with 30% pruning ratio and 6-bit quantization onResNet-34. Code will be available at here. 