This paper proposes joint attention estimation in a single image. Different from related work in which only the gaze-related attributes of people are independently employed, (i) their locations and actions are also employed as contex-tual cues for weighting their attributes, and (ii) interac-tions among all of these attributes are explicitly modeled in our method. For the interaction modeling, we propose a novel Transformer-based attention network to encode joint attention as low-dimensional features. We introduce a spe-cialized MLP head with positional embedding to the Trans-former so that it predicts pixelwise confidence of joint atten-tion for generating the confidence heatmap. This pixelwise prediction improves the heatmap accuracy by avoiding the ill-posed problem in which the high-dimensional heatmap is predicted from the low-dimensional features. The estimated joint attention is further improved by being integrated with general image-based attention estimation. Our method out-performs SOTA methods quantitatively in comparative ex-periments. Code: https://github.com/chihina/PJAE. 