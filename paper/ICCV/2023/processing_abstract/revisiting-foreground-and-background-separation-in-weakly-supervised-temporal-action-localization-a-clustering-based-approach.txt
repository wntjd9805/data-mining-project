Weakly-supervised temporal action localization aims to localize action instances in videos with only video-level action labels. Existing methods mainly embrace a localization-by-classification pipeline that optimizes the snippet-level prediction with a video classification loss.However, this formulation suffers from the discrepancy be-tween classification and detection, resulting in inaccurate separation of foreground and background (F&B) snippets.To alleviate this problem, we propose to explore the un-derlying structure among the snippets by resorting to un-supervised snippet clustering, rather than heavily relying on the video classification loss. Specifically, we propose a novel clustering-based F&B separation algorithm. It com-prises two core components: a snippet clustering compo-nent that groups the snippets into multiple latent clusters and a cluster classification component that further classi-fies the cluster as foreground or background. As there are no ground-truth labels to train these two components, we introduce a unified self-labeling mechanism based on op-timal transport to produce high-quality pseudo-labels that match several plausible prior distributions. This ensures that the cluster assignments of the snippets can be accu-rately associated with their F&B labels, thereby boosting the F&B separation. We evaluate our method on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves promising performance on all three bench-marks while being significantly more lightweight than pre-vious methods. Code is available at https://github. com/Qinying-Liu/CASE 