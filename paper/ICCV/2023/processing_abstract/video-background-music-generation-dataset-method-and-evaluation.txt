Music is essential when editing videos, but selecting mu-sic manually is difficult and time-consuming. Thus, we seek to automatically generate background music tracks given video input. This is a challenging task since it requires music-video datasets, efficient architectures for video-to-music generation, and reasonable metrics, none of which currently exist. To close this gap, we introduce a complete recipe including dataset, benchmark model, and evaluation metric for video background music generation. We presentSymMV, a video and symbolic music dataset with various musical annotations. To the best of our knowledge, it is the first video-music dataset with rich musical annotations. We also propose a benchmark video background music gener-ation framework named V-MusProd, which utilizes music priors of chords, melody, and accompaniment along with video-music relations of semantic, color, and motion fea-tures. To address the lack of objective metrics for video-music correspondence, we design a retrieval-based metricVMCP built upon a powerful video-music representation learning model. Experiments show that with our dataset, V-MusProd outperforms the state-of-the-art method in both music quality and correspondence with videos. We be-lieve our dataset, benchmark model, and evaluation met-ric will boost the development of video background mu-sic generation. Our dataset and code are available at https://github.com/zhuole1025/SymMV . 