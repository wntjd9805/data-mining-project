Understanding the emotions in text and presenting them visually is a very challenging problem that requires a deep understanding of natural language and high-quality image synthesis simultaneously. In this work, we propose AffectiveImage Filter (AIF), a novel model that is able to understand the visually-abstract emotions from the text and reﬂect them to visually-concrete images with appropriate colors and tex-tures. We build our model based on the multi-modal trans-former architecture, which uniﬁes both images and texts into tokens and encodes the emotional prior knowledge.Various loss functions are proposed to understand complex emotions and produce appropriate visualization. In addi-tion, we collect and contribute a new dataset with abundant aesthetic images and emotional texts for training and eval-uating the AIF model. We carefully design four quantitative metrics and conduct a user study to comprehensively eval-# Equal contributions. * Corresponding author. uate the performance, which demonstrates our AIF model outperforms state-of-the-art methods and could evoke spe-ciﬁc emotional responses from human observers. 