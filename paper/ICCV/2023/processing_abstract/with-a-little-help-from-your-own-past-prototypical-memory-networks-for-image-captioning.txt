Image captioning, like many tasks involving vision and language, currently relies on Transformer-based architec-tures for extracting the semantics in an image and trans-lating it into linguistically coherent descriptions. Although successful, the attention operator only considers a weighted summation of projections of the current input sample, there-fore ignoring the relevant semantic information which can come from the joint observation of other samples. In this paper, we devise a network which can perform attention over activations obtained while processing other training samples, through a prototypical memory model. Our mem-ory models the distribution of past keys and values through the definition of prototype vectors which are both discrimi-native and compact. Experimentally, we assess the perfor-mance of the proposed model on the COCO dataset, in com-parison with carefully designed baselines and state-of-the-art approaches, and by investigating the role of each of the proposed components. We demonstrate that our proposal can increase the performance of an encoder-decoder Trans-former by 3.7 CIDEr points both when training in cross-entropy only and when fine-tuning with self-critical se-quence training. Source code and trained models are avail-able at: https://github.com/aimagelab/PMA-Net. 