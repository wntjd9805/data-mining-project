Talking head video generation aims to animate a human face in a still image with dynamic poses and expressions us-ing motion information derived from a target-driving video, while maintaining the personâ€™s identity in the source im-age. However, dramatic and complex motions in the driv-ing video cause ambiguous generation, because the still source image cannot provide sufficient appearance infor-mation for occluded regions or delicate expression vari-ations, which produces severe artifacts and significantly degrades the generation quality. To tackle this problem, we propose to learn a global facial representation space, and design a novel implicit identity representation condi-tioned memory compensation network, coined as MCNet, for high-fidelity talking head generation. Specifically, we devise a network module to learn a unified spatial facial meta-memory bank from all training samples, which can provide rich facial structure and appearance priors to com-pensate warped source facial features for the generation.Furthermore, we propose an effective query mechanism based on implicit identity representations learned from the discrete keypoints of the source image. It can greatly facil-itate the retrieval of more correlated information from the memory bank for the compensation. Extensive experiments demonstrate that MCNet can learn representative and com-plementary facial memory, and can clearly outperform pre-vious state-of-the-art talking head generation methods onVoxCeleb1 and CelebV datasets. Please check our Project. 