Recent years have witnessed a remarkable success of large deep learning models. However, training these mod-els is challenging due to high computational costs, painfully slow convergence, and overfitting issues.In this paper, we present Deep Incubation, a novel approach that en-ables the efficient and effective training of large models by dividing them into smaller sub-modules which can be trained separately and assembled seamlessly. A key chal-lenge for implementing this idea is to ensure the compat-ibility of the independently trained sub-modules. To ad-dress this issue, we first introduce a global, shared meta model, which is leveraged to implicitly link all the mod-ules together, and can be designed as an extremely small network with negligible computational overhead. Then we propose a module incubation algorithm, which trains each sub-module to replace the corresponding component of the meta model and accomplish a given learning task.Despite the simplicity, our approach effectively encour-ages each sub-module to be aware of its role in the tar-get large model, such that the finally-learned sub-modules can collaborate with each other smoothly after being as-sembled. Empirically, our method can outperform end-to-end (E2E) training in well-established training setting and shows transferable performance gain for downstream tasks (e.g., object detection and image segmentation onCOCO and ADE20K). Our code is available at https://github.com/LeapLabTHU/Deep-Incubation. 