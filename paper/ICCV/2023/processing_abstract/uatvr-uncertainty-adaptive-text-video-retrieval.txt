With the explosive growth of web videos and emerg-ing large-scale vision-language pre-training models, e.g.,CLIP, retrieving videos of interest with text instructions has attracted increasing attention. A common practice is to transfer text-video pairs to the same embedding space and craft cross-modal interactions with certain entities in spe-cific granularities for semantic correspondence. Unfortu-nately, the intrinsic uncertainties of optimal entity combina-tions in appropriate granularities for cross-modal queries are understudied, which is especially critical for modalities with hierarchical semantics, e.g., video, text, etc.In this paper, we propose an Uncertainty-Adaptive Text-Video Re-trieval approach, termed UATVR, which models each look-up as a distribution matching procedure. Concretely, we add additional learnable tokens in the encoders to adap-tively aggregate multi-grained semantics for flexible high-level reasoning. In the refined embedding space, we rep-resent text-video pairs as probabilistic distributions where prototypes are sampled for matching evaluation. Com-prehensive experiments on four benchmarks justify the su-periority of our UATVR, which achieves new state-of-the-art results on MSR-VTT (50.8%), VATEX (64.5%), MSVD (49.7%), and DiDeMo (45.8%). The code is available at https://github.com/bofang98/UATVR. 