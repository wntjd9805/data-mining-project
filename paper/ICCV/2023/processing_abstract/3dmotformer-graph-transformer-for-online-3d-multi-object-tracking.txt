Tracking 3D objects accurately and consistently is cru-cial for autonomous vehicles, enabling more reliable down-stream tasks such as trajectory prediction and motion plan-ning. Based on the substantial progress in object detec-tion in recent years, the tracking-by-detection paradigm has become a popular choice due to its simplicity and ef-ficiency. State-of-the-art 3D multi-object tracking (MOT) approaches typically rely on non-learned model-based al-gorithms such as Kalman Filter but require many manu-ally tuned parameters. On the other hand, learning-based approaches face the problem of adapting the training to the online setting, leading to inevitable distribution mis-match between training and inference as well as suboptimal performance. In this work, we propose 3DMOTFormer, a learned geometry-based 3D MOT framework building upon the transformer architecture. We use an Edge-AugmentedGraph Transformer to reason on the track-detection bi-partite graph frame-by-frame and conduct data associa-tion via edge classification. To reduce the distribution mis-match between training and inference, we propose a novel online training strategy with an autoregressive and recur-rent forward pass as well as sequential batch optimiza-tion. Using CenterPoint detections, our approach achieves 71.2% and 68.2% AMOTA on the nuScenes validation and test split, respectively.In addition, a trained 3DMOT-Former model generalizes well across different object de-tectors. Code is available at: https://github.com/ dsx0511/3DMOTFormer. 