Distilling knowledge from convolutional neural net-works (CNNs) is a double-edged sword for vision trans-formers (ViTs). It boosts the performance since the image-friendly local-inductive bias of CNN helps ViT learn faster and better, but leading to two problems: (1) Network de-signs of CNN and ViT are completely different, which leads to different semantic levels of intermediate features, mak-ing spatial-wise knowledge transfer methods (e.g., feature mimicking) inefﬁcient. (2) Distilling knowledge from CNN limits the network convergence in the later training period since ViT’s capability of integrating global information is suppressed by CNN’s local-inductive-bias supervision.To this end, we present Cumulative Spatial KnowledgeDistillation (CSKD). CSKD distills spatial-wise knowledge to all patch tokens of ViT from the corresponding spatial re-sponses of CNN, without introducing intermediate features.Furthermore, CSKD exploits a Cumulative Knowledge Fu-sion (CKF) module, which introduces the global response of CNN and increasingly emphasizes its importance dur-ing the training. Applying CKF leverages CNN’s local in-ductive bias in the early training period and gives full play to ViT’s global capability in the later one. Extensive ex-periments and analysis on ImageNet-1k and downstream datasets demonstrate the superiority of our CSKD. Code: https://github.com/Zzzzz1/CSKD 