Previous work on adversarial examples typically involves a ﬁxed norm perturbation budget, which fails to capture the way humans perceive perturbations. Recent work has shifted towards natural unrestricted adversarial examples (UAEs) that breaks `p perturbation bounds but nonetheless remain semantically plausible. Current methods use GAN or VAE to generate UAEs by perturbing latent codes. How-ever, this leads to loss of high-level information, resulting in low-quality and unnatural UAEs. In light of this, we proposeAdvDiffuser, a new method for synthesizing natural UAEs using diffusion models. It can generate UAEs from scratch or conditionally based on reference images. To generate nat-ural UAEs, we perturb predicted images to steer their latent code towards the adversarial sample space of a particular classiﬁer. We also propose adversarial inpainting based on class activation mapping to retain the salient regions of the image while perturbing less important areas. On CIFAR-10,CelebA and ImageNet, we demonstrate that it can defeat the most robust models on the RobustBench leaderboard with near 100% success rates. Furthermore, The synthesizedUAEs are not only more natural but also stronger compared to the current state-of-the-art attacks. Speciﬁcally, com-pared with GA-attack, the UAEs generated with AdvDiffuser exhibit 6 smallerFID scores and 0.28 higher in SSIM metrics, making them perceptually stealthier. Finally, adversarial training withAdvDiffuser further improves the model robustness against attacks with unseen threat models.1 smaller LPIPS perturbations, 2 3⇥⇠⇥ 