(cid:42)(cid:85)(cid:82)(cid:88)(cid:83)(cid:3)(cid:50)(cid:85)(cid:71)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:38)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:87)(cid:86)(cid:3)(cid:11)(cid:42)(cid:85)(cid:82)(cid:38)(cid:82)(cid:12)(cid:3)(cid:47)(cid:82)(cid:86)(cid:86) (cid:68)(cid:79)(cid:79)(cid:3)(cid:83)(cid:82)(cid:86)(cid:17)(cid:3)(cid:71)(cid:76)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:86)(cid:3)(cid:31)(cid:3)(cid:68)(cid:79)(cid:79)(cid:3)(cid:81)(cid:72)(cid:74)(cid:17)(cid:3)(cid:71)(cid:76)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:86)Contrastive learning has become an important tool in learning representations from unlabeled data mainly rely-ing on the idea of minimizing distance between positive data pairs, e.g., views from the same images, and maximizing distance between negative data pairs, e.g., views from dif-ferent images. This paper proposes a new variation of the contrastive learning objective, Group Ordering Constraints (GroCo), that leverages the idea of sorting the distances of positive and negative pairs and computing the respec-tive loss based on how many positive pairs have a larger distance than the negative pairs, and thus are not ordered correctly. To this end, the GroCo loss is based on differen-tiable sorting networks, which enable training with sorting supervision by matching a differentiable permutation ma-trix, which is produced by sorting a given set of scores, to a respective ground truth permutation matrix. Applying this idea to groupwise pre-ordered inputs of multiple positive and negative pairs allows introducing the GroCo loss with implicit emphasis on strong positives and negatives, leading to better optimization of the local neighborhood. We eval-uate the proposed formulation on various self-supervised learning benchmarks and show that it not only leads to im-proved results compared to vanilla contrastive learning but also shows competitive performance to comparable meth-ods in linear probing and outperforms current methods in k-NN performance. 1 