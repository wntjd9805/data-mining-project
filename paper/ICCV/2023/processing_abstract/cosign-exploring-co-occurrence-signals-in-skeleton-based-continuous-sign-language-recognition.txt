The co-occurrence signals (e.g., hand shape, facial ex-pression, and lip pattern) play a critical role in Continu-ous Sign Language Recognition (CSLR). Compared to RGB data, skeleton data provide a more efficient and concise option, and lay a good foundation for the co-occurrence exploration in CSLR. However, skeleton data are often used as a tool to assist visual grounding and have not at-tracted sufficient attention.In this paper, we propose a simple yet effective GCN-based approach, named CoSign, to incorporate Co-occurrence Signals and explore the po-tential of skeleton data in CSLR. Specifically, we pro-pose a group-specific GCN to better exploit the knowl-edge of each signal and a complementary regularization to prevent complex co-adaptation across signals. Fur-thermore, we propose a two-stream framework that grad-ually fuses both static and dynamic information in skeleton data. Experimental results on three public CSLR datasets (PHOENIX14, PHOENIX14-T and CSL-Daily) show that the proposed CoSign achieves competitive performance with recent video-based approaches while reducing the computation cost during training. 