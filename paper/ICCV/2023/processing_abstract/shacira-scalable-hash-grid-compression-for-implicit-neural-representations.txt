Implicit Neural Representations (INR) or neural fields have emerged as a popular framework to encode multime-dia signals such as images and radiance fields while retain-ing high-quality. Recently, learnable feature grids proposed by MÂ¨uller et al. [1] have allowed significant speed-up in the training as well as the sampling of INRs by replacing a large neural network with a multi-resolution look-up ta-ble of feature vectors and a much smaller neural network.However, these feature grids come at the expense of large memory consumption which can be a bottleneck for stor-age and streaming applications. In this work, we proposeSHACIRA, a simple yet effective task-agnostic framework for compressing such feature grids with no additional post-hoc pruning/quantization stages. We reparameterize feature grids with quantized latent weights and apply entropy regu-larization in the latent space to achieve high levels of com-pression across various domains. Quantitative and qualita-tive results on diverse datasets consisting of images, videos, and radiance fields, show that our approach outperforms existing INR approaches without the need for any large datasets or domain-specific heuristics. Our project page is available at https://shacira.github.io. 