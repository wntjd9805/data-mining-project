Recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images.While impressive, the images often fall short of depicting subtle details and are susceptible to errors due to ambigu-ity in the input text. One way of alleviating these issues is to train diffusion models on class-labeled datasets. This ap-proach has two disadvantages: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, affect-ing the quality and diversity of the generated images, or (ii) the input is a hard-coded label, as opposed to free-form text, limiting the control over the generated images.In this work, we propose a non-invasive fine-tuning tech-nique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discrim-inative signals from a pretrained classifier. This is done by iteratively modifying the embedding of an added input token of a text-to-image diffusion model, by steering gen-erated images toward a given target class according to a classifier. Our method is fast compared to prior fine-tuning methods and does not require a collection of in-class im-ages or retraining of a noise-tolerant classifier. We evalu-ate our method extensively, showing that the generated im-ages are: (i) more accurate and of higher quality than stan-dard diffusion models, (ii) can be used to augment train-ing data in a low-resource setting, and (iii) reveal informa-tion about the data used to train the guiding classifier. The code is available at https://github.com/idansc/ discriminative_class_tokens. 