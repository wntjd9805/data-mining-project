Multi-label image classiﬁcation is a prediction task that aims to identify more than one label from a given image. This paper considers the semantic consistency of the latent space between the visual patch and linguistic label domains and introduces the conditional transport (CT) theory to bridge the acknowledged gap. While recent cross-modal attention-based studies have attempted to align such two representa-tions and achieved impressive performance, they required carefully-designed alignment modules and extra complex operations in the attention computation. We ﬁnd that by formulating the multi-label classiﬁcation as a CT problem, we can exploit the interactions between the image and label efﬁciently by minimizing the bidirectional CT cost. Specif-ically, after feeding the images and textual labels into the modality-speciﬁc encoders, we view each image as a mix-ture of patch embeddings and a mixture of label embeddings, which capture the local region features and the class proto-types, respectively. CT is then employed to learn and align those two semantic sets by deﬁning the forward and back-ward navigators. Importantly, the deﬁned navigators in CT distance model the similarities between patches and labels, which provides an interpretable tool to visualize the learned prototypes. Extensive experiments on three public image benchmarks show that the proposed model consistently out-performs the previous methods. 