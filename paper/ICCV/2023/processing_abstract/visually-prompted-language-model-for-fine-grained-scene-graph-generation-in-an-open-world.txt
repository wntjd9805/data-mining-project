Scene Graph Generation (SGG) aims to extract <sub-ject, predicate, object> relationships in images for vision understanding. Although recent works have made steady progress on SGG, they still suffer long-tail distribution is-sues that tail-predicates are more costly to train and hard to distinguish due to a small amount of annotated data com-pared to frequent predicates. Existing re-balancing strate-gies try to handle it via prior rules but are still confined to pre-defined conditions, which are not scalable for variousIn this paper, we propose a Cross-models and datasets. modal prediCate boosting (CaCao) framework, where a visually-prompted language model is learned to generate diverse fine-grained predicates in a low-resource way. The proposed CaCao can be applied in a plug-and-play fash-ion and automatically strengthen existing SGG to tackle the long-tailed problem. Based on that, we further in-troduce a novel Entangled cross-modal prompt approach for open-world predicate scene graph generation (Epic), where models can generalize to unseen predicates in a zero-shot manner. Comprehensive experiments on three bench-mark datasets show that CaCao consistently boosts the per-formance of multiple scene graph generation models in a model-agnostic way. Moreover, our Epic achieves compet-itive performance on open-world predicate prediction. The data and code for this paper are publicly available.1 