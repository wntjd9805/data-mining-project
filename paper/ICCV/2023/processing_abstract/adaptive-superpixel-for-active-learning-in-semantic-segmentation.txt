Learning semantic segmentation requires pixel-wise an-notations, which can be time-consuming and expensive. To reduce the annotation cost, we propose a superpixel-based active learning (AL) framework, which collects a dominant label per superpixel instead. To be specific, it consists of adaptive superpixel and sieving mechanisms, fully ded-icated to AL. At each round of AL, we adaptively merge neighboring pixels of similar learned features into super-pixels. We then query a selected subset of these super-pixels using an acquisition function assuming no uniform superpixel size. This approach is more efficient than ex-isting methods, which rely only on innate features such asRGB color and assume uniform superpixel sizes. Obtain-ing a dominant label per superpixel drastically reduces an-notatorsâ€™ burden as it requires fewer clicks. However, it inevitably introduces noisy annotations due to mismatches between superpixel and ground truth segmentation. To ad-dress this issue, we further devise a sieving mechanism that identifies and excludes potentially noisy annotations from learning. Our experiments on both Cityscapes and PAS-CAL VOC datasets demonstrate the efficacy of adaptive su-perpixel and sieving mechanisms. 