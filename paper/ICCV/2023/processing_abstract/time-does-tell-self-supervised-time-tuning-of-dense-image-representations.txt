Spatially dense self-supervised learning is a rapidly growing problem domain with promising applications for unsupervised segmentation and pretraining for dense down-stream tasks. Despite the abundance of temporal data in the form of videos, this information-rich source has been largely overlooked. Our paper aims to address this gap by proposing a novel approach that incorporates temporal consistency in dense self-supervised learning. While meth-ods designed solely for images face difﬁculties in achieving even the same performance on videos, our method improves not only the representation quality for videos – but also im-ages. Our approach, which we call time-tuning, starts from image-pretrained models and ﬁne-tunes them with a novel self-supervised temporal-alignment clustering loss on unla-beled videos. This effectively facilitates the transfer of high-level information from videos to image representations.Time-tuning improves the state-of-the-art by 8-10% for un-supervised semantic segmentation on videos and matches it for images. We believe this method paves the way for further self-supervised scaling by leveraging the abundant avail-ability of videos. The implementation can be found here : https://github.com/SMSD75/Timetuning 