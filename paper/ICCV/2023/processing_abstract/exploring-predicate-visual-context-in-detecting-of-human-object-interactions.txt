Recently, the DETR framework has emerged as the dom-inant approach for humanâ€“object interaction (HOI) re-In particular, two-stage transformer-based HOI search. detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained con-textual information, eschewing pose and orientation infor-mation in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of com-plex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experi-ments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaining low training cost. 