In this paper, we consider enhancing medical visual-language pre-training (VLP) with domain-specific knowl-edge, by exploiting the paired image-text reports from the radiological daily practice. In particular, we make the fol-lowing contributions: First, unlike existing works that di-rectly process the raw reports, we adopt a novel triplet ex-traction module to extract the medical-related information, avoiding unnecessary complexity from language grammar and enhancing the supervision signals; Second, we pro-pose a novel triplet encoding module with entity translation by querying a knowledge base, to exploit the rich domain knowledge in medical field, and implicitly build relation-ships between medical entities in the language embedding space; Third, we propose to use a Transformer-based fu-sion model for spatially aligning the entity description with visual signals at the image patch level, enabling the abil-ity for medical diagnosis; Fourth, we conduct thorough experiments to validate the effectiveness of our architec-ture, and benchmark on numerous public benchmarks e.g.,ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumotho-rax, COVIDx CXR-2, COVID Rural, and EdemaSeverity.In both zero-shot and fine-tuning settings, our model has demonstrated strong performance compared with the for-mer methods on disease classification and grounding. 