Synthesizing expression is essential to create realistic talking faces. Previous works consider expressions and mouth shapes as a whole and predict them solely from audio inputs. However, the limited information contained in au-dio, such as phonemes and coarse emotion embedding, may not be suitable as the source of elaborate expressions. Be-sides, since expressions are tightly coupled to lip motions, generating expression from other sources is tricky and al-ways neglects expression performed on mouth region, lead-*Corresponding author. ing to inconsistency between them. To tackle the issues, this paper proposes Emotional Motion Memory Net (EMMN) that synthesizes expression overall on the talking face via emotion embedding and lip motion instead of the sole au-dio. Specifically, we extract emotion embedding from au-dio and design Motion Reconstruction module to decom-pose ground truth videos into mouth features and expres-sion features before training, where the latter encode all facial factors about expression. During training, the emo-tion embedding and mouth features are used as keys, and the corresponding expression features are used as values to create key-value pairs stored in the proposed Motion Mem-ory Net. Hence, once the audio-relevant mouth featuresand emotion embedding are individually predicted from au-dio at inference time, we treat them as a query to retrieve the best-matching expression features, performing expres-sion overall on the face and thus avoiding inconsistent re-sults. Extensive experiments demonstrate that our method can generate high-quality talking face videos with accurate lip movements and vivid expressions on unseen subjects. 