Vision-language models such as CLIP [27] learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classiﬁcation task through few-shot prompt tuning. We ﬁnd that such a prompt tuning process is highly robust to la-bel noises. This intrigues us to study the key reasons con-tributing to the robustness of the prompt tuning paradigm.We conducted extensive experiments to explore this prop-erty and ﬁnd the key factors are: 1) the ﬁxed classname to-kens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy sam-ples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classiﬁcation. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, signiﬁcantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CEWu/PTNL. 