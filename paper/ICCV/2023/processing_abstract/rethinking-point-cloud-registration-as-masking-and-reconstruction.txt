Point cloud registration is essential in computer vision and robotics. In this paper, a critical observation is made that the invisible parts of each point cloud can be directly utilized as inherent masks, and the aligned point cloud pair can be regarded as the reconstruction target. Motivated by this observation, we rethink the point cloud registra-tion problem as a masking and reconstruction task. To this end, a generic and concise auxiliary training network, theMasked Reconstruction Auxiliary Network (MRA), is pro-posed. The MRA reconstructs the complete point cloud by separately using the encoded features of each point cloud obtained from the backbone, guiding the contextual features in the backbone to capture fine-grained geometric details and the overall structures of point cloud pairs. Unlike re-cently developed high-performing methods that incorporate specific encoding methods into transformer models, which sacrifice versatility and introduce significant computational complexity during the inference process, our MRA can be easily inserted into other methods to further improve regis-tration accuracy. Additionally, the MRA is detached after training, thereby avoiding extra computational complexity during the inference process. Building upon the MRA, we present a novel transformer-based method, the Masked Re-construction Transformer (MRT), which achieves both pre-cise and efficient alignment using standard transformers.Extensive experiments conducted on the 3DMatch, Model-Net40, and KITTI datasets demonstrate the superior perfor-mance of our MRT over state-of-the-art methods. Codes are available at https://github.com/CGuangyan-BIT/MRA. 