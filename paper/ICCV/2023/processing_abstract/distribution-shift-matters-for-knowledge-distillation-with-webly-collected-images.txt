Knowledge distillation aims to learn a lightweight stu-dent network from a pre-trained teacher network. In prac-tice, existing knowledge distillation methods are usually infeasible when the original training data is unavailable due to some privacy issues and data management consid-erations. Therefore, data-free knowledge distillation ap-proaches proposed to collect training instances from the In-ternet. However, most of them have ignored the common distribution shift between the instances from original train-ing data and webly collected data, affecting the reliability of the trained student network. To solve this problem, we propose a novel method dubbed “Knowledge Distillation between Different Distributions” (KD3), which consists of three components. Speciﬁcally, we ﬁrst dynamically select useful training instances from the webly collected data ac-cording to the combined predictions of teacher network and student network. Subsequently, we align both the weighted features and classiﬁer parameters of the two networks for knowledge memorization. Meanwhile, we also build a new contrastive learning block called MixDistribution to gen-erate perturbed data with a new distribution for instance alignment, so that the student network can further learn a distribution-invariant representation.Intensive experi-ments on various benchmark datasets demonstrate that our proposed KD3 can outperform the state-of-the-art data-free knowledge distillation approaches. 