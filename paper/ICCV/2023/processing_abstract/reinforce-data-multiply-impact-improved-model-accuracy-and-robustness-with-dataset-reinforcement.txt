We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model archi-tecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a DatasetReinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distilla-tion with state-of-the-art models with various data augmen-tations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Mod-els trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., seg-mentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet valida-tion set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R.Expected Calibration Error (ECE) on the ImageNet vali-dation set is also reduced by 9.9%. Using this backbone with Mask-RCNN for object detection on MS-COCO, the mean average precision improves by 0.8%. We reach similar gains for MobileNets, ViTs, and Swin-Transformers. ForMobileNetV3 and Swin-Tiny, we observe significant improve-ments on ImageNet-R/A/C of up to 20% improved robust-ness. Models pretrained on ImageNet+ and fine-tuned onCIFAR-100+, Flowers-102+, and Food-101+, reach up to 3.4% improved accuracy. The code, datasets, and pretrained models are available at https://github.com/apple/ml-dr. 