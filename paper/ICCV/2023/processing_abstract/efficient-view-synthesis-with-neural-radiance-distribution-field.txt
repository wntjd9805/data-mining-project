Recent work on Neural Radiance Fields (NeRF) has demonstrated significant advances in high-quality view syn-thesis. A major limitation of NeRF is its low rendering effi-ciency due to the need for multiple network forwardings to render a single pixel. Existing methods to improve NeRF ei-ther reduce the number of required samples or optimize the implementation to accelerate the network forwarding. De-spite these efforts, the problem of multiple sampling persists due to the intrinsic representation of radiance fields. In con-trast, Neural Light Fields (NeLF) reduce the computation cost of NeRF by querying only one single network forward-ing per pixel. To achieve a close visual quality to NeRF, existing NeLF methods require significantly larger network capacities which limits their rendering efficiency in prac-tice. In this work, we propose a new representation calledNeural Radiance Distribution Field (NeRDF) that targets efficient view synthesis in real-time. Specifically, we use a small network similar to NeRF while preserving the ren-dering speed with a single network forwarding per pixel as in NeLF. The key is to model the radiance distribution along each ray with frequency basis and predict frequency weights using the network. Pixel values are then computed via vol-ume rendering on radiance distributions. Experiments show that our proposed method offers a better trade-off among speed, quality, and network size than existing methods: we achieve a ∼254× speed-up over NeRF with similar network size, with only a marginal performance decline. Our project page is at yushuang-wu.github.io/NeRDF. 