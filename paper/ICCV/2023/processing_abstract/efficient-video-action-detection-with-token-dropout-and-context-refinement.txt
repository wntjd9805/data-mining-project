Streaming video clips with large-scale video tokens im-pede vision transformers (ViTs) for efficient recognition, especially in video action detection where sufficient spa-tiotemporal representations are required for precise ac-tor identification.In this work, we propose an end-to-end framework for efficient video action detection (EVAD) based on vanilla ViTs. Our EVAD consists of two special-ized designs for video action detection. First, we propose a spatiotemporal token dropout from a keyframe-centric per-spective. In a video clip, we maintain all tokens from its keyframe, preserve tokens relevant to actor motions from other frames, and drop out the remaining tokens in this clip. Second, we refine scene context by leveraging remain-ing tokens for better recognizing actor identities. The re-gion of interest (RoI) in our action detector is expanded into temporal domain. The captured spatiotemporal ac-tor identity representations are refined via scene context in a decoder with the attention mechanism. These two de-signs make our EVAD efficient while maintaining accuracy, which is validated on three benchmark datasets (i.e., AVA,UCF101-24, JHMDB). Compared to the vanilla ViT back-bone, our EVAD reduces the overall GFLOPs by 43% and improves real-time inference speed by 40% with no perfor-mance degradation. Moreover, even at similar computa-tional costs, our EVAD can improve the performance by 1.1 mAP with higher resolution inputs. Code is available at https://github.com/MCG-NJU/EVAD. 