Remarkable advances have been achieved recently in learning neural representations that characterize object ge-ometry, while generating textured objects suitable for down-stream applications and 3D rendering remains at an early stage. In particular, reconstructing textured geometry from images of real objects is a significant challenge â€“ recon-structed geometry is often inexact, making realistic texturing a significant challenge. We present Mesh2Tex, which learns a realistic object texture manifold from uncorrelated collec-tions of 3D object geometry and photorealistic RGB images, by leveraging a hybrid mesh-neural-field texture representa-tion. Our texture representation enables compact encoding of high-resolution textures as a neural field in the barycen-tric coordinate system of the mesh faces. The learned texture manifold enables effective navigation to generate an object texture for a given 3D object geometry that matches to an input RGB image, which maintains robustness even under challenging real-world scenarios where the mesh geometry approximates an inexact match to the underlying geome-try in the RGB image. Mesh2Tex can effectively generate realistic object textures for an object mesh to match real im-ages observations towards digitization of real environments, significantly improving over previous state of the art.Project page: alexeybokhovkin.github.io/ mesh2tex/