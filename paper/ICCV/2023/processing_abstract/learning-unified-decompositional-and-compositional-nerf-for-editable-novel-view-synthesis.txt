Implicit neural representations have shown powerful ca-pacity in modeling real-world 3D scenes, offering superior performance in novel view synthesis. In this paper, we tar-get a more challenging scenario, i.e., joint scene novel view synthesis and editing based on implicit neural scene repre-sentations. State-of-the-art methods in this direction typ-ically consider building separate networks for these two tasks (i.e., view synthesis and editing). Thus, the model-ing of interactions and correlations between these two tasks is very limited, which, however, is critical for learning high-quality scene representations. To tackle this problem, in this paper, we propose a unified Neural Radiance Field (NeRF) framework to effectively perform joint scene decomposition and composition for modeling real-world scenes. The de-composition aims at learning disentangled 3D representa-tions of different objects and the background, allowing for scene editing, while scene composition models an entire scene representation for novel view synthesis. Specifically, with a two-stage NeRF framework, we learn a coarse stage for predicting a global radiance field as guidance for point sampling, and in the second fine-grained stage, we perform scene decomposition by a novel one-hot object radiance field regularization module and a pseudo supervision via in-painting to handle ambiguous background regions occluded by objects. The decomposed object-level radiance fields are further composed by using activations from the decomposi-tion module. Extensive quantitative and qualitative results show the effectiveness of our method for scene decomposi-tion and composition, outperforming state-of-the-art meth-ods for both novel-view synthesis and editing tasks1. 