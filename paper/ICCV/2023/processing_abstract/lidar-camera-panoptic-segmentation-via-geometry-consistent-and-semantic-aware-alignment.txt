3D panoptic segmentation is a challenging perception task that requires both semantic segmentation and instance segmentation.In this task, we notice that images could provide rich texture, color, and discriminative information, which can complement LiDAR data for evident performance improvement, but their fusion remains a challenging prob-lem. To this end, we propose LCPS, the first LiDAR-CameraPanoptic Segmentation network. In our approach, we con-duct LiDAR-Camera fusion in three stages: 1) an Asyn-chronous Compensation Pixel Alignment (ACPA) module that calibrates the coordinate misalignment caused by asyn-chronous problems between sensors; 2) a Semantic-AwareRegion Alignment (SARA) module that extends the one-to-one point-pixel mapping to one-to-many semantic rela-tions; 3) a Point-to-Voxel feature Propagation (PVP) mod-ule that integrates both geometric and semantic fusion in-formation for the entire point cloud. Our fusion strategy improves about 6.9% PQ performance over the LiDAR-only baseline on NuScenes dataset. Extensive quantitative and qualitative experiments further demonstrate the effective-ness of our novel framework. The code will be released at https://github.com/zhangzw12319/lcps.git. 