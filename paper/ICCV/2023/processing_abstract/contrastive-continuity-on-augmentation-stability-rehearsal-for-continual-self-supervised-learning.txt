Self-supervised learning has attracted a lot of attention recently, which is able to learn powerful representations without any manual annotations. However, self-supervised learning needs to develop the ability to continuously learn to cope with a variety of real-world challenges, i.e., Con-tinual Self-Supervised Learning (CSSL). Catastrophic for-getting is a notorious problem in CSSL, where the model tends to forget the learned knowledge.In practice, sim-ple rehearsal or regularization will bring extra negative ef-fects while alleviating catastrophic forgetting in CSSL, e.g., overﬁtting on the rehearsal samples or hindering the model from encoding fresh information. In order to address catas-trophic forgetting without overﬁtting on the rehearsal sam-ples, we propose Augmentation Stability Rehearsal (ASR) in this paper, which selects the most representative and dis-criminative samples by estimating the augmentation stabil-ity for rehearsal. Meanwhile, we design a matching strategy for ASR to dynamically update the rehearsal buffer. In addi-tion, we further propose Contrastive Continuity on Augmen-tation Stability Rehearsal (C2ASR) based on ASR. We show that C2ASR is an upper bound of the Information Bottleneck (IB) principle, which suggests that C2ASR essentially pre-serves as much information shared among seen task streams as possible to prevent catastrophic forgetting and dismisses the redundant information between previous task streams and current task stream to free up the ability to encode fresh information. Our method obtains a great achievement com-pared with state-of-the-art CSSL methods on a variety ofCSSL benchmarks. 