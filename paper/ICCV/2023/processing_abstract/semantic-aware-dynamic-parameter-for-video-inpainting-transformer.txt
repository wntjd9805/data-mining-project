Recent learning-based video inpainting approaches have achieved considerable progress. However, they still cannot fully utilize semantic information within the video frames and predict improper scene layout, failing to restore clear object boundaries for mixed scenes. To mitigate this prob-lem, we introduce a new transformer-based video inpaint-ing technique that can exploit semantic information within the input and considerably improve reconstruction qual-ity.In this study, we use the mixture-of-experts scheme and train multiple experts to handle mixed scenes, includ-ing various semantics. We leverage these multiple experts and produce locally (token-wise) different network param-eters to achieve semantic-aware inpainting results. Exten-sive experiments on YouTube-VOS and DAVIS benchmark datasets demonstrate that, compared with existing conven-tional video inpainting approaches, the proposed method has superior performance in synthesizing visually pleasing videos with much clearer semantic structures and textures. 