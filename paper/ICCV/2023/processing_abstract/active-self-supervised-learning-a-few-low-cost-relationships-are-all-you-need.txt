Self-Supervised Learning (SSL) has emerged as the so-lution of choice to learn transferable representations from unlabeled data. However, SSL requires to build sam-ples that are known to be semantically akin, i.e. posi-tive views. Requiring such knowledge is the main lim-itation of SSL and is often tackled by ad-hoc strategies e.g. applying known data-augmentations to the same in-put. In this work, we formalize and generalize this princi-ple through Positive Active Learning (PAL) where an ora-cle queries semantic relationships between samples. PAL achieves three main objectives. First, it unveils a theoreti-cally grounded learning framework beyond SSL, based on similarity graphs, that can be extended to tackle supervised and semi-supervised learning depending on the employed oracle. Second, it provides a consistent algorithm to em-bed a priori knowledge, e.g. some observed labels, into any SSL losses without any change in the training pipeline.Third, it provides a proper active learning framework yield-ing low-cost solutions to annotate datasets, arguably bring-ing the gap between theory and practice of active learning that is based on simple-to-answer-by-non-experts queries of semantic relationships between inputs. 