Event-based motion deblurring has shown promising re-sults by exploiting low-latency events. However, current approaches are limited in their practical usage, as they as-sume the same spatial resolution of inputs and specific blur-riness distributions. This work addresses these limitations and aims to generalize the performance of event-based de-blurring in real-world scenarios. We propose a scale-aware network that allows flexible input spatial scales and en-ables learning from different temporal scales of motion blur.A two-stage self-supervised learning scheme is then devel-oped to fit real-world data distribution. By utilizing the rel-ativity of blurriness, our approach efficiently ensures the re-stored brightness and structure of latent images and further generalizes deblurring performance to handle varying spa-tial and temporal scales of motion blur in a self-distillation manner. Our method is extensively evaluated, demonstrat-ing remarkable performance, and we also introduce a real-world dataset consisting of multi-scale blurry frames and events to facilitate research in event-based deblurring.Multimedia MaterialThe Multi-Scale Real-world Blurry Dataset (MS-RBD) and our Pytorch implementation are available at: https://github.com/XiangZ-0/GEM. 