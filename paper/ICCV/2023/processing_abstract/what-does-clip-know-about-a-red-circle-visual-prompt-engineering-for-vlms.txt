Large-scale Vision-Language Models, such as CLIP, learn powerful image-text representations that have found numerous applications, from zero-shot classification to text-to-image generation. Despite that, their capabilities for solving novel discriminative tasks via prompting fall behind those of large language models, such as GPT-3. Here we ex-plore the idea of visual prompt engineering for solving com-puter vision tasks beyond classification by editing in image space instead of text. In particular, we discover an emer-gent ability of CLIP, where, by simply drawing a red cir-cle around an object, we can direct the modelâ€™s attention to that region, while also maintaining global information. We show the power of this simple approach by achieving state-of-the-art in zero-shot referring expressions comprehension and strong performance in keypoint localization tasks. Fi-nally, we draw attention to some potential ethical concerns of large language-vision models. 