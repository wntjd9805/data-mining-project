Knowledge distillation (KD) has become a standard method to boost the performance of lightweight object de-tectors. Most previous works are feature-based, where stu-dents mimic the features of homogeneous teacher detectors.However, distilling the knowledge from the heterogeneous teacher fails in this manner due to the serious semantic gap, which greatly limits the flexibility of KD in practical appli-cations. Bridging this semantic gap now requires case-by-case algorithm design which is time-consuming and heavily relies on experienced adjustment. To alleviate this problem, we propose Universal Knowledge Distillation (UniKD), in-troducing additional decoder heads with deformable cross-attention called Adaptive Knowledge Extractor (AKE). InUniKD, AKEs are first pretrained on the teacher’s output to infuse the teacher’s content and positional knowledge into a fixed-number set of knowledge embeddings. The fixedAKEs are then attached to the student’s backbone to en-courage the student to absorb the teacher’s knowledge in these knowledge embeddings. In this query-based distilla-tion paradigm, detection-relevant information can be dy-namically aggregated into a knowledge embedding set and transferred between different detectors. When the teacher model is too large for online inference, its output can be stored on disk in advance to save the computation overhead, which is more storage efficient than feature-based meth-ods. Extensive experiments demonstrate that our UniKD can plug and play in any homogeneous or heterogeneous teacher-student pairs and significantly outperforms conven-tional feature-based KD. 