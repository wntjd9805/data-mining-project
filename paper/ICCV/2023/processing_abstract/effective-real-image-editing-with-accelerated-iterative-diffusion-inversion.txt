Despite all recent progress, it is still challenging to edit and manipulate natural images with modern generative models.When using Generative Adversarial Network (GAN), one major hurdle is in the inversion process mapping a real im-age to its corresponding noise vector in the latent space, since it is necessary to be able to reconstruct an image to edit its contents. Likewise for Denoising Diffusion ImplicitModels (DDIM), the linearization assumption in each inver-sion step makes the whole deterministic inversion process unreliable. Existing approaches that have tackled the prob-lem of inversion stability often incur in significant trade-offs in computational efficiency. In this work we propose anAccelerated Iterative Diffusion Inversion method, dubbedAIDI, that significantly improves reconstruction accuracy with minimal additional overhead in space and time com-plexity. By using a novel blended guidance technique, we show that effective results can be obtained on a large range of image editing tasks without large classifier-free guidance in inversion. Furthermore, when compared with other diffu-sion inversion based works, our proposed process is shown to be more robust for fast image editing in the 10 and 20 diffusion stepsâ€™ regimes. 