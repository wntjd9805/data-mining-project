Being able to perceive the semantics and the spatial structure of the environment is essential for visual naviga-tion of a household robot. However, most existing works only employ visual backbones pre-trained either with in-dependent images for classification or with self-supervised learning methods to adapt to the indoor navigation domain, neglecting the spatial relationships that are essential to the learning of navigation. Inspired by the behavior that hu-mans naturally build semantically and spatially meaningful cognitive maps in their brains during navigation, in this pa-per, we propose a novel navigational-specific visual repre-sentation learning method by contrasting the agent’s ego-centric views and semantic maps (Ego2-Map). We apply the visual transformer as the backbone encoder and train the model with data collected from the large-scale Habitat-Matterport3D environments. Ego2-Map learning transfers the compact and rich information from a map, such as ob-jects, structure and transition, to the agent’s egocentric rep-resentations for navigation. Experiments show that agents using our learned representations on object-goal naviga-tion outperform recent visual pre-training methods. More-over, our representations significantly improve vision-and-language navigation in continuous environments for both high-level and low-level action spaces, achieving new state-of-the-art results of 47% SR and 41% SPL on the test server. 