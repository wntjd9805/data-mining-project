We study the problem of future step anticipation in proce-dural videos. Given a video of an ongoing procedural activ-ity, we predict a plausible next procedure step described in rich natural language. While most previous work focuses on the problem of data scarcity in procedural video datasets, another core challenge of future anticipation is how to ac-count for multiple plausible future realizations in natural settings. This problem has been largely overlooked in pre-vious work. To address this challenge, we frame future step prediction as modelling the distribution of all possible can-didates for the next step. Speciﬁcally, we design a gener-ative model that takes a series of video clips as input, and generates multiple plausible and diverse candidates (in nat-ural language) for the next step. Following previous work, we side-step the video annotation scarcity by pretraining our model on a large text-based corpus of procedural ac-tivities, and then transfer the model to the video domain.Our experiments, both in textual and video domains, show that our model captures diversity in the next step prediction and generates multiple plausible future predictions. More-over, our model establishes new state-of-the-art results onYouCookII, where it outperforms existing baselines on the next step anticipation. Finally, we also show that our model can successfully transfer from text to the video domain zero-shot, i.e., without ﬁne-tuning or adaptation, and produces good-quality future step predictions from video. 