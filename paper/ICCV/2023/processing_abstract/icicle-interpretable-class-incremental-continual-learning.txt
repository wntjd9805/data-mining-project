(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:76)(cid:80)(cid:68)(cid:74)(cid:72) (cid:55)(cid:68)(cid:86)(cid:78)(cid:3)(cid:20)(cid:3)Continual learning enables incremental learning of new tasks without forgetting those previously learned, result-ing in positive knowledge transfer that can enhance per-formance on both new and old tasks. However, contin-ual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCrementalLEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach.It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the Ô¨Åne-grained setting; and task-recency bias compensation devoted to prototypical parts.Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the ex-isting exemplar-free methods of common class-incremental learning when applied to concept-based models. 