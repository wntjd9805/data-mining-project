In computer vision, 2D convolution is arguably the most important operation performed by a ConvNet. Unsurpris-ingly, it has been the focus of intense software and hard-ware optimization and enjoys highly efficient implementa-In this work, we ask an intriguing question: can tions. we make a ConvNet work without 2D convolutions? Sur-prisingly, we find that the answer is yesâ€”we show that aConvNet consisting entirely of 1D convolutions can do just as well as 2D on ImageNet classification. Specifically, we find that one key ingredient to a high-performing 1D Con-vNet is oriented 1D kernels: 1D kernels that are oriented not just horizontally or vertically, but also at other angles.Our experiments show that oriented 1D convolutions can not only replace 2D convolutions but also augment exist-ing architectures with large kernels, leading to improved accuracy with minimal FLOPs increase. A key contri-bution of this work is a highly-optimized custom CUDA implementation of oriented 1D kernels, specialized to the depthwise convolution setting. Our benchmarks demon-strate that our custom CUDA implementation almost per-fectly realizes the theoretical advantage of 1D convolution: it is faster than a native horizontal convolution for any ar-bitrary angle. Code is available at https://github. com/princeton-vl/Oriented1D. 