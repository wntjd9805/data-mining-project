Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet.A plethora of work characterized by using a two-streamVision-Language model architecture that learns a joint rep-resentation of video-text pairs has become a prominent ap-proach for the VTR task. However, these models operate un-der the assumption of bijective video-text correspondences and neglect a more practical scenario where video con-tent usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap be-tween the previous training objective and real-world appli-cations, leading to the potential performance degradationIn this study, we in-of earlier models during inference. troduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multi-ple different events, as a niche scenario of the conventionalVideo-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representa-tion and a new MeVTR loss for the MeVTR task. Compre-hensive experiments show that this straightforward frame-work outperforms other models in the Video-to-Text andText-to-Video tasks, effectively establishing a robust base-line for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at https://github.com/gengyuanmax/MeVTR. 