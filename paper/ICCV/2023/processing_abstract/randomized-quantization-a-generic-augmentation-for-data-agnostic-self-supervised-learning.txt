Self-supervised representation learning follows a the data and paradigm of withholding some part of tasking the network to predict it from the remaining part.Among many techniques, data augmentation lies at the core for creating the information gap. Towards this end, tool masking has emerged as a generic and powerful where content is withheld along the sequential dimension, e.g., spatial in images, temporal in audio, and syntactic in language.In this paper, we explore the orthogonal channel dimension for generic data augmentation by exploiting precision redundancy. The data for each chan-nel is quantized through a non-uniform quantizer, with the quantized value sampled randomly within randomly sampled quantization bins.From another perspective, quantization is analogous to channel-wise masking, as it removes the information within each bin, but preserves the information across bins. Our approach significantly surpasses existing generic data augmentation methods, while showing on par performance against modality-specific augmentations. We comprehensively evaluate our approach on vision, audio, 3D point clouds, as well as the DABS benchmark which is comprised of variousThe code is available at https: data modalities.//github.com/microsoft/random_quantize. 