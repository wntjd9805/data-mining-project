This paper presents IP-SLT, a simple yet effective frame-work for sign language translation (SLT). Our IP-SLT adopts a recurrent structure and enhances the semantic rep-resentation (prototype) of the input sign language video via an iterative refinement manner. Our idea mimics the be-havior of human reading, where a sentence can be digested repeatedly, till reaching accurate understanding. Techni-cally, IP-SLT consists of feature extraction, prototype ini-tialization, and iterative prototype refinement. The initial-ization module generates the initial prototype based on the visual feature extracted by the feature extraction module.Then, the iterative refinement module leverages the cross-attention mechanism to polish the previous prototype by ag-gregating it with the original video feature. Through re-peated refinement, the prototype finally converges to a more stable and accurate state, leading to a fluent and appro-priate translation. In addition, to leverage the sequential dependence of prototypes, we further propose an iterative distillation loss to compress the knowledge of the final itera-tion into previous ones. As the autoregressive decoding pro-cess is executed only once in inference, our IP-SLT is ready to improve various SLT systems with acceptable overhead.Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the IP-SLT. 