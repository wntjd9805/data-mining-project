Recent inversion methods have shown that real images can be inverted into StyleGAN’s latent space and numer-ous edits can be achieved on those images thanks to the se-mantically rich feature representations of well-trained GAN models. However, extensive research has also shown that image inversion is challenging due to the trade-off betweenIn this paper, high-fidelity reconstruction and editability. we tackle an even more difficult task, inverting erased im-ages into GAN’s latent space for realistic inpaintings and editings. Furthermore, by augmenting inverted latent codes with different latent samples, we achieve diverse inpaint-ings. Specifically, we propose to learn an encoder and mix-ing network to combine encoded features from erased im-ages with StyleGAN’s mapped features from random sam-ples. To encourage the mixing network to utilize both in-puts, we train the networks with generated data via a novel set-up. We also utilize higher-rate features to prevent color inconsistencies between the inpainted and unerased parts.We run extensive experiments and compare our method with state-of-the-art inversion and inpainting methods. Qualita-tive metrics and visual comparisons show significant im-provements. 