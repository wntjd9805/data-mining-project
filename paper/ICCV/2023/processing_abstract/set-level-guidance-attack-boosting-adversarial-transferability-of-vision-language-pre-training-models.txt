Vision-language pre-training (VLP) models have shown vulnerability to adversarial examples in multimodal tasks.Furthermore, malicious adversaries can be deliberately transferred to attack other black-box models. However, ex-isting work has mainly focused on investigating white-box attacks. In this paper, we present the first study to investigate the adversarial transferability of recent VLP models. We ob-serve that existing methods exhibit much lower transferabil-ity, compared to the strong attack performance in white-box settings. The transferability degradation is partly caused by the under-utilization of cross-modal interactions. Particu-larly, unlike unimodal learning, VLP models rely heavily on cross-modal interactions and the multimodal alignments are many-to-many, e.g., an image can be described in various natural languages. To this end, we propose a highly trans-ferable Set-level Guidance Attack (SGA) that thoroughly leverages modality interactions and incorporates alignment-preserving augmentation with cross-modal guidance. Ex-perimental results demonstrate that SGA could generate adversarial examples that can strongly transfer across dif-ferent VLP models on multiple downstream vision-language tasks. On image-text retrieval, SGA significantly enhances the attack success rate for transfer attacks from ALBEF toTCL by a large margin (at least 9.78% and up to 30.21%), compared to the state-of-the-art. Our code is available at https://github.com/Zoky-2020/SGA.Figure 1: Comparison of attack success rates (ASR) using five different attacks on image-text retrieval. Adversar-ial examples are crafted on the source model (ALBEF) to attack the target white-box model or black-box models. The first three columns refer to the image-only PGD attack [24], text-only BERT-Attack [19] (BA), and the combined sepa-rate unimodal attack (SA), which all belong to the methods without cross-modal interactions. The fourth column is the state-of-the-art multimodal Co-Attack [41] (CA) that em-ploys single-pair cross-modal interactions. The last column is the proposed Set-level Guidance Attack (SGA), which leverages multiple set-level cross-modal interactions, suc-cessfully attacking the white-box model and transferring to attack all black-box models with the highest ASR. More discussions are in Section 3. 