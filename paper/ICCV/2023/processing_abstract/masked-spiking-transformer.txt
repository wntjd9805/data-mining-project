The combination of Spiking Neural Networks (SNNs) and Transformers has attracted significant attention due to their potential for high energy efficiency and high-performance nature. However, existing works on this topic typically rely on direct training, which can lead to subop-timal performance. To address this issue, we propose to leverage the benefits of the ANN-to-SNN conversion method to combine SNNs and Transformers, resulting in signifi-cantly improved performance over existing state-of-the-artSNN models. Furthermore, inspired by the quantal synap-tic failures observed in the nervous system, which reduce the number of spikes transmitted across synapses, we in-troduce a novel Masked Spiking Transformer (MST) frame-work. This incorporates a Random Spike Masking (RSM) method to prune redundant spikes and reduce energy con-sumption without sacrificing performance. Our experi-mental results demonstrate that the proposed MST model achieves a significant reduction of 26.8% in power con-sumption when the masking ratio is 75% while maintaining the same level of performance as the unmasked model. The code is available at: https://github.com/bic-L/Masked-Spiking-Transformer. 