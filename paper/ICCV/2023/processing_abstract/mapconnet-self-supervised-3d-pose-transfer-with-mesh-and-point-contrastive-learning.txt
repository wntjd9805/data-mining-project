3D pose transfer is a challenging generation task that aims to transfer the pose of a source geometry onto a tar-get geometry with the target identity preserved. Many prior methods require keypoint annotations to find correspon-dence between the source and target. Current pose trans-fer methods allow end-to-end correspondence learning but require the desired final output as ground truth for supervi-sion. Unsupervised methods have been proposed for graph convolutional models but they require ground truth corre-spondence between the source and target inputs. We present a novel self-supervised framework for 3D pose transfer which can be trained in unsupervised, semi-supervised, or fully supervised settings without any correspondence labels.We introduce two contrastive learning constraints in the la-tent space: a mesh-level loss for disentangling global pat-terns including pose and identity, and a point-level loss for discriminating local semantics. We demonstrate quantita-tively and qualitatively that our method achieves state-of-the-art results in supervised 3D pose transfer, with compa-rable results in unsupervised and semi-supervised settings.Our method is also generalisable to unseen human and an-imal data with complex topologiesâ€ . 