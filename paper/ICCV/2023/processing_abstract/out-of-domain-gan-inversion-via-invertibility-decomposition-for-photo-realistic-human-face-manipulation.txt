The fidelity of Generative Adversarial Networks (GAN) inversion is impeded by Out-Of-Domain (OOD) areas (e.g., background, accessories) in the image. Detecting the OOD areas beyond the generation ability of the pre-trained model and blending these regions with the input image can en-hance fidelity. The “invertibility mask” figures out theseOOD areas, and existing methods predict the mask with the reconstruction error. However, the estimated mask is usu-ally inaccurate due to the influence of the reconstruction er-ror in the In-Domain (ID) area. In this paper, we propose a novel framework that enhances the fidelity of human face in-*Corresponding author. version by designing a new module to decompose the input images to ID and OOD partitions with invertibility masks.Unlike previous works, our invertibility detector is simul-taneously learned with a spatial alignment module. We it-eratively align the generated features to the input geome-try and reduce the reconstruction error in the ID regions.Thus, the OOD areas are more distinguishable and can be precisely predicted. Then, we improve the fidelity of our re-sults by blending the OOD areas from the input image with the ID GAN inversion results. Our method produces photo-realistic results for real-world human face image inversion and manipulation. Extensive experiments demonstrate our method’s superiority over existing methods in the quality of GAN inversion and attribute manipulation. Our code is available at: AbnerVictor/OOD-GAN-inversion