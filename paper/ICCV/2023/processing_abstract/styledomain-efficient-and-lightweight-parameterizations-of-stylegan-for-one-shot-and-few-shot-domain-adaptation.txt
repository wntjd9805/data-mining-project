Domain adaptation of GANs is a problem of ﬁne-tuningGAN models pretrained on a large dataset (e.g. StyleGAN) to a speciﬁc domain with few samples (e.g. painting faces, sketches, etc.). While there are many methods that tackle this problem in different ways, there are still many impor-tant questions that remain unanswered. In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focusing on the StyleGAN model. We perform a detailed exploration of the most im-portant parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. As a result of this study, we propose new efﬁcient and lightweight parameter-izations of StyleGAN for domain adaptation. Particularly, we show that there exist directions in StyleSpace (StyleDo-main directions) that are sufﬁcient for adapting to similar domains. For dissimilar domains, we propose Afﬁne+ andAfﬁneLight+ parameterizations that allows us to outper-form existing baselines in few-shot adaptation while having signiﬁcantly less training parameters. Finally, we exam-ine StyleDomain directions and discover their many sur-*Equal contribution prising properties that we apply for domain mixing and cross-domain image morphing. Source code can be found at https://github.com/AIRI-Institute/StyleDomain. 