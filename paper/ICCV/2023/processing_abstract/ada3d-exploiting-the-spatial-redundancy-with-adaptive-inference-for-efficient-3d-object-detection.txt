Voxel-based methods have achieved state-of-the-art per-formance for 3D object detection in autonomous driv-ing. However, their significant computational and memory costs pose a challenge for their application to resource-constrained vehicles. One reason for this high resource consumption is the presence of a large number of redundant background points in Lidar point clouds, resulting in spa-tial redundancy in both 3D voxel and BEV map represen-tations. To address this issue, we propose an adaptive in-ference framework called Ada3D, which focuses on reduc-ing the spatial redundancy to compress the model’s com-putational and memory cost. Ada3D adaptively filters the redundant input, guided by a lightweight importance pre-dictor and the unique properties of the Lidar point cloud.Additionally, we maintain the BEV features’ intrinsic spar-sity by introducing the Sparsity Preserving Batch Normal-ization. With Ada3D, we achieve 40% reduction for 3D voxels and decrease the density of 2D BEV feature maps from 100% to 20% without sacrificing accuracy. Ada3D reduces the model computational and memory cost by 5×, and achieves 1.52× / 1.45× end-to-end GPU latency and 1.5× / 4.5× GPU peak memory optimization for the 3D and 2D backbone respectively. 