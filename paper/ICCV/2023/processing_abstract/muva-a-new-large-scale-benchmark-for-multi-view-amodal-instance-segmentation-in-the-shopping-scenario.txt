Amodal Instance Segmentation (AIS) endeavors to ac-curately deduce complete object shapes that are partially or fully occluded. However, the inherent ill-posed nature of single-view datasets poses challenges in determining oc-cluded shapes. A multi-view framework may help allevi-ate this problem, as humans often adjust their perspective when encountering occluded objects. At present, this ap-proach has not yet been explored by existing methods and datasets. To bridge this gap, we propose a new task calledMulti-view Amodal Instance Segmentation (MAIS) and in-troduce the MUVA dataset, the first MUlti-View AIS dataset that takes the shopping scenario as instantiation. MUVA provides comprehensive annotations, including multi-view amodal/visible segmentation masks, 3D models, and depth maps, making it the largest image-level AIS dataset in terms of both the number of images and instances. Additionally, we propose a new method for aggregating representative features across different instances and views, which demon-strates promising results in accurately predicting occluded objects from one viewpoint by leveraging information from other viewpoints. Besides, we also demonstrate that MUVA can benefit the AIS task in real-world scenarios. 1 