Simulation has emerged as an indispensable tool for scaling and accelerating the development of self-driving systems. A critical aspect of this is simulating realistic and diverse human behavior and intent. In this work, we pro-pose a holistic framework for learning physically plausible human dynamics from real driving scenarios, narrowing the gap between real and simulated human behavior in safety-critical applications. We show that state-of-the-art meth-ods underperform in driving scenarios where video data is recorded from moving vehicles, and humans are frequently partially or fully occluded. Furthermore, existing methods often disregard the global scene where humans are situated, resulting in various motion artifacts like foot sliding, float-ing, or ground penetration. To address this challenge, we propose an approach that incorporates physics with a rein-forcement learning-based motion controller to learn human dynamics for driving scenarios. Our framework can sim-ulate physically plausible human dynamics that accurately match observed human motions and infill motions for oc-cluded body parts, while improving the physical plausibil-ity of the entire motion sequence. Experiments on the chal-lenging Waymo Open Dataset show that our method out-performs state-of-the-art motion capture approaches signif-icantly in recovering high-quality, physically plausible, and scene-aware human dynamics. 