Multi-modality image fusion and segmentation play a vital role in autonomous driving and robotic operation.Early efforts focus on boosting the performance for only one task, e.g., fusion or segmentation, making it hard to reach ‘Best of Both Worlds’. To overcome this issue, in this paper, we propose a Multi-interactive Feature learn-ing architecture for image fusion and Segmentation, namelySegMiF, and exploit dual-task correlation to promote the performance of both tasks. The SegMiF is of a cascade structure, containing a fusion sub-network and a commonly used segmentation sub-network. By slickly bridging inter-mediate features between two components, the knowledge learned from the segmentation task can effectively assist the fusion task. Also, the benefited fusion network sup-ports the segmentation one to perform more pretentiously.Besides, a hierarchical interactive attention block is estab-lished to ensure fine-grained mapping of all the vital in-formation between two tasks, so that the modality/semantic features can be fully mutual-interactive. In addition, a dy-namic weight factor is introduced to automatically adjust the corresponding weights of each task, which can balance the interactive feature correspondence and break through the limitation of laborious tuning. Furthermore, we con-struct a smart multi-wave binocular imaging system and collect a full-time multi-modality benchmark with 15 anno-tated pixel-level categories for image fusion and segmenta-tion. Extensive experiments on several public datasets and our benchmark demonstrate that the proposed method out-puts visually appealing fused images and perform averagely 7.66% higher segmentation mIoU in the real-world scene than the state-of-the-art approaches. The source code and benchmark are available at https://github.com/JinyuanLiu-CV/SegMiF. 