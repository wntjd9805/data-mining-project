Novel view synthesis and 3D modeling using implicit neural ﬁeld representation are shown to be very effective for calibrated multi-view cameras. Such representations are known to beneﬁt from additional geometric and seman-tic supervision. Most existing methods that exploit addi-tional supervision require dense pixel-wise labels or local-ized scene priors. These methods cannot beneﬁt from high-level vague scene priors provided in terms of scenes’ de-scriptions. In this work, we aim to leverage the geometric prior of Manhattan scenes to improve the implicit neural radiance ﬁeld representations. More precisely, we assume that only the knowledge of the indoor scene (under investi-gation) being Manhattan is known – with no additional in-formation whatsoever – with an unknown Manhattan coor-dinate frame. Such high-level prior is used to self-supervise the surface normals derived explicitly in the implicit neu-ral ﬁelds. Our modeling allows us to cluster the derived normals and exploit their orthogonality constraints for self-supervision. Our exhaustive experiments on datasets of di-verse indoor scenes demonstrate the signiﬁcant beneﬁt of the proposed method over the established baselines. The source code will be available at https://github. com/nikola3794/normal-clustering-nerf. 