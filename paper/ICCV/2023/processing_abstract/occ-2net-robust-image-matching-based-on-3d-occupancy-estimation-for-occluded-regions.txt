Image matching is a fundamental and critical task in var-ious visual applications, such as Simultaneous Localization and Mapping (SLAM) and image retrieval, which require accurate pose estimation. However, most existing methods ignore the occlusion relations between objects caused by camera motion and scene structure. In this paper, we pro-pose Occ2Net, a novel image matching method that models occlusion relations using 3D occupancy and infers match-ing points in occluded regions. Thanks to the inductive bias encoded in the Occupancy Estimation (OE) module, it greatly simpliÔ¨Åes bootstrapping of a multi-view consis-tent 3D representation that can then integrate information from multiple views. Together with an Occlusion-Aware (OA) module, it incorporates attention layers and rotation alignment to enable matching between occluded and visible points. We evaluate our method on both real-world and sim-ulated datasets and demonstrate its superior performance over state-of-the-art methods on several metrics, especially in occlusion scenarios.Figure 1. Schematic diagram of the Occ2Net. (a) and (b) are im-ages taken from different viewpoints, while (c) shows the match-ing process for occluded regions. In (c), two monitors are shown with green and red masks indicating areas that are visible in (b) but occluded in (a). By using Occ2Net to extract consistent occupancy features and match them between (a) and (b), the monitors that are occluded in (a) can still be matched in (b), thus enabling Occ2Net to have the ability to perform matching under occlusion. 