We introduce the task of localizing a flexible number of objects in real-world 3D scenes using natural language de-scriptions. Existing 3D visual grounding tasks focus on localizing a unique object given a text description. How-ever, such a strict setting is unnatural as localizing poten-tially multiple objects is a common need in real-world sce-narios and robotic tasks (e.g., visual navigation and ob-ject rearrangement). To address this setting we proposeMulti3DRefer, generalizing the ScanRefer dataset and task.Our dataset contains 61926 descriptions of 11609 objects, where zero, single or multiple target objects are referenced by each description. We also introduce a new evaluation metric and benchmark methods from prior work to enable further investigation of multi-modal 3D scene understand-ing. Furthermore, we develop a better baseline leveraging 2D features from CLIP by rendering object proposals on-line with contrastive learning, which outperforms the state of the art on the ScanRefer benchmark. 