Plentiful adversarial attack researches have revealed the fragility of deep neural networks (DNNs), where the imper-ceptible perturbations can cause drastic changes in the out-put. Among the diverse types of attack methods, gradient-based attacks are powerful and easy to implement, arousing wide concern for the security problem of DNNs. Howev-er, under the black-box setting, the existing gradient-based attacks have much trouble in breaking through DNN mod-els with defense technologies, especially those adversari-ally trained models. To make adversarial examples more transferable, in this paper, we explore the ﬂuctuation phe-nomenon on the plus-minus sign of the adversarial pertur-bations’ pixels during the generation of adversarial exam-ples, and propose an ingenious Gradient Relevance Attack (GRA). Speciﬁcally, two gradient relevance frameworks are presented to better utilize the information in the neighbor-hood of the input, which can correct the update direction adaptively. Then we adjust the update step at each iteration with a decay indicator to counter the ﬂuctuation. Exper-iment results on a subset of the ILSVRC 2012 validation set forcefully verify the effectiveness of GRA. Furthermore, the attack success rates of 68.7% and 64.8% on Tencen-t Cloud and Baidu AI Cloud further indicate that GRA can craft adversarial examples with the ability to transfer across both datasets and model architectures. Code is released at https://github.com/RYC-98/GRA. 