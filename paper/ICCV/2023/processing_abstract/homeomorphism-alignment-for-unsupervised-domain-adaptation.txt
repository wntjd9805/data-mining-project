Existing unsupervised domain adaptation (UDA) meth-ods rely on aligning the features from the source and tar-get domains explicitly or implicitly in a common space (i.e., the domain invariant space). Explicit distribution matching ignores the discriminability of learned features, while the implicit counterpart such as self-supervised learning suf-fers from pseudo-label noises. With distribution alignment, it is challenging to acquire a common space which main-tains fully the discriminative structure of both domains.In this work, we propose a novel HomeomorphisM Align-ment (HMA) approach characterized by aligning the source and target data in two separate spaces. SpeciÔ¨Åcally, an invertible neural network based homeomorphism is con-structed. Distribution matching is then used as a sewing up tool for connecting this homeomorphism mapping be-tween the source and target feature spaces. Theoretically, we show that this mapping can preserve the data topologi-cal structure (e.g., the cluster/group structure). This prop-erty allows for more discriminative model adaptation by leveraging both the original and transformed features of source data in a supervised manner, and those of target do-main in an unsupervised manner (e.g., prediction consis-tency). Extensive experiments demonstrate that our method can achieve the state-of-the-art results. Code is released at https://github.com/buerzlh/HMA. 