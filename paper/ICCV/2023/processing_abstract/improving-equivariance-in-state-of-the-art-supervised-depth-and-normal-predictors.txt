Dense depth and surface normal predictors should pos-sess the equivariant property to cropping-and-resizing â€“ cropping the input image should result in cropping the same output image. However, we find that state-of-the-art depth and normal predictors, despite having strong performances, surprisingly do not respect equivariance. The problem ex-ists even when crop-and-resize data augmentation is em-ployed during training. To remedy this, we propose an equivariant regularization technique, consisting of an av-eraging procedure and a self-consistency loss, to explicitly promote cropping-and-resizing equivariance in depth and normal networks. Our approach can be applied to bothCNN and Transformer architectures, does not incur extra cost during testing, and notably improves the supervised and semi-supervised learning performance of dense pre-dictors on Taskonomy tasks. Finally, finetuning with our loss on unlabeled images improves not only equivariance but also accuracy of state-of-the-art depth and normal pre-dictors when evaluated on NYU-v2. 