Grounding textual expressions on scene objects from first-person views is a truly demanding capability in de-veloping agents that are aware of their surroundings and behave following intuitive text instructions. Such capabil-ity is of necessity for glass-devices or autonomous robots to localize referred objects in the real-world. In the conven-tional referring expression comprehension tasks of images, however, datasets are mostly constructed based on the web-crawled data and donâ€™t reflect diverse real-world structures on the task of grounding textual expressions in diverse ob-jects in the real world. Recently, a massive-scale egocen-tric video dataset of Ego4D was proposed. Ego4D cov-ers around the world diverse real-world scenes including numerous indoor and outdoor situations such as shopping, cooking, walking, talking, manufacturing, etc. Based on egocentric videos of Ego4D, we constructed a broad cov-erage of the video-based referring expression comprehen-sion dataset: RefEgo. Our dataset includes more than 12k video clips and 41 hours for video-based referring expres-sion comprehension annotation. In experiments, we com-bine the state-of-the-art 2D referring expression compre-hension models with the object tracking algorithm, achiev-ing the video-wise referred object tracking even in difficult conditions: the referred object becomes out-of-frame in the middle of the video or multiple similar objects are presented in the video. 