One critical challenge in 6D object pose estimation from a single RGBD image is efficient integration of two different modalities, i.e., color and depth. In this work, we tackle this problem by a novel Deep Fusion Transformer (DFTr) block that can aggregate cross-modality features for improving pose estimation. Unlike existing fusion methods, the pro-posed DFTr can better model cross-modality semantic cor-relation by leveraging their semantic similarity, such that globally enhanced features from different modalities can be better integrated for improved information extraction.Moreover, to further improve robustness and efficiency, we introduce a novel weighted vector-wise voting algorithm that employs a non-iterative global optimization strategy for precise 3D keypoint localization while achieving near real-time inference. Extensive experiments show the effec-tiveness and strong generalization capability of our pro-posed 3D keypoint voting algorithm. Results on four widely used benchmarks also demonstrate that our method outper-forms the state-of-the-art methods by large margins. Code is available at https://github.com/junzastar/DFTr Voting. 