Monocular depth estimation is known as an ill-posed task in which objects in a 2D image usually do not con-tain sufficient information to predict their depth. Thus, it acts differently from other tasks (e.g., classification and segmentation) in many ways.In this paper, we find that self-supervised monocular depth estimation shows a di-rection sensitivity and environmental dependency in the feature representation. But the current backbones bor-rowed from other tasks pay less attention to handling different types of environmental information, limiting the overall depth accuracy. To bridge this gap, we propose a new Direction-aware Cumulative Convolution Network (DaCCN), which improves the depth feature representa-tion in two aspects. First, we propose a direction-aware the feature extrac-module, which can learn to adjust tion in each direction, facilitating the encoding of differ-ent types of information. Secondly, we design a new cu-mulative convolution to improve the efficiency for aggre-gating important environmental information. Experiments show that our method achieves significant improvements on three widely used benchmarks, KITTI, Cityscapes, andMake3D, setting a new state-of-the-art performance on the popular benchmarks with all three types of self-supervision. https://github.com/wencheng256/DaCCN. 