This paper proposes a fine-grained self-localization method for outdoor robotics that utilizes a flexible num-ber of onboard cameras and readily accessible satellite im-ages. The proposed method addresses limitations in exist-ing cross-view localization methods that struggle to handle noise sources such as moving objects and seasonal vari-ations.It is the first sparse visual-only method that en-hances perception in dynamic environments by detecting view-consistent key points and their corresponding deep features from ground and satellite views, while removing off-the-ground objects and establishing homography trans-formation between the two views. Moreover, the proposed method incorporates a spatial embedding approach that leverages camera intrinsic and extrinsic information to re-duce the ambiguity of purely visual matching, leading to improved feature matching and overall pose estimation ac-curacy. The method exhibits strong generalization and is robust to environmental changes, requiring only geo-poses as ground truth. Extensive experiments on the KITTI andFord Multi-AV Seasonal datasets demonstrate that our pro-posed method outperforms existing state-of-the-art meth-ods, achieving median spatial accuracy errors below 0.5 meters along the lateral and longitudinal directions, and a median orientation accuracy error below 2â—¦ 1. 