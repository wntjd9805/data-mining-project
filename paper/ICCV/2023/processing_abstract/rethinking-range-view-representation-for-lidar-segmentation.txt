LiDAR segmentation is crucial for autonomous driving perception. Recent trends favor point- or voxel-based meth-ods as they often yield better performance than the tradi-tional range view representation. In this work, we unveil several key factors in building powerful range view mod-els. We observe that the “many-to-one” mapping, semantic incoherence, and shape deformation are possible impedi-ments against effective learning from range view projec-tions. We present RangeFormer – a full-cycle framework comprising novel designs across network architecture, data augmentation, and post-processing – that better handles the learning and processing of LiDAR point clouds from the range view. We further introduce a Scalable Training fromRange view (STR) strategy that trains on arbitrary low-resolution 2D range images, while still maintaining satis-factory 3D segmentation accuracy. We show that, for the first time, a range view method is able to surpass the point, voxel, and multi-view fusion counterparts in the competingLiDAR semantic and panoptic segmentation benchmarks, i.e., SemanticKITTI, nuScenes, and ScribbleKITTI. 