As the model size of pre-trained language models full fine-tuning becomes pro-(PLMs) grows rapidly, hibitively expensive for model training and storage.In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifica-tions (e.g., Adapter and LoRA) into encoder-decoder PLMs.By tuning a small set of trainable parameters, these tech-niques perform on par with full fine-tuning. However, ex-cessive modular modifications and neglecting the function-ality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues.In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a va-riety of model-agnostic VL-PET modules can be instanti-ated from our framework for better efficiency and effective-ness trade-offs. We further propose lightweight PET mod-ule designs to enhance VL alignment and modeling for the encoders and maintain text generation for the decoders.Extensive experiments conducted on four image-text tasks and four video-text tasks demonstrate the efficiency, ef-fectiveness and transferability of our VL-PET framework.In particular, our VL-PETlarge with lightweight PET mod-ule designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37% (7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate the enhanced effect of employing our VL-PET designs on ex-isting PET techniques, enabling them to achieve signifi-cant performance improvements. Our code is available at https://github.com/HenryHZY/VL-PET.*Corresponding author.Figure 1. Relative average performance gain of difference PET techniques w.r.t to full fine-tuning. Experiments are conducted with three seeds on four image-text tasks based on BART-base. 