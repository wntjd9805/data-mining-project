Dataset distillation aims at a small synthetic dataset to mimic the training performance on neural networks of a given large dataset. Existing approaches heavily rely on an iterative optimization to update synthetic data and multiple forward-backward passes over thousands of neural network spaces, which introduce significant overhead for computa-tion and are inconvenient in scenarios requiring high effi-ciency. In this paper, we focus on few-shot dataset distilla-tion, where a distilled dataset is synthesized with only a few or even a single network. To this end, we introduce the no-tion of distillation space, such that synthetic data optimized only in this specific space can achieve the effect of those optimized through numerous neural networks, with dramat-ically accelerated training and reduced computational cost.To learn such a distillation space, we first formulate the problem as a quad-level optimization framework and pro-pose a bi-level algorithm. Nevertheless, the algorithm in its original form has a large memory footprint in practice due to the back-propagation through an unrolled computational graph. We then convert the problem of learning the distilla-tion space to a first-order one based on image translation.Specifically, the synthetic images are optimized in an arbi-trary but fixed neural space and then translated to those in the targeted distillation space. We pre-train the translator on some large datasets like ImageNet so that it requires only a limited number of adaptation steps on the target dataset.Extensive experiments demonstrate that the translator af-ter pre-training and a limited number of adaptation steps achieves comparable distillation performance with state of the arts, with ∼ 15× acceleration. It also exerts satisfac-tory generalization performance across different datasets, storage budgets, and numbers of classes. 