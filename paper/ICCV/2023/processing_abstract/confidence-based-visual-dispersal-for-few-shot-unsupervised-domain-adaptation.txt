Unsupervised domain adaptation aims to transfer knowledge from a fully-labeled source domain to an un-labeled target domain. However, in real-world scenarios, providing abundant labeled data even in the source domain can be infeasible due to the difficulty and high expense of annotation. To address this issue, recent works consider theFew-shot Unsupervised Domain Adaptation (FUDA) where only a few source samples are labeled, and conduct knowl-edge transfer via self-supervised learning methods. Yet ex-isting methods generally overlook that the sparse label set-ting hinders learning reliable source knowledge for trans-fer. Additionally, the learning difficulty difference in tar-get samples is different but ignored, leaving hard target samples poorly classified. To tackle both deficiencies, in this paper, we propose a novel Confidence-based VisualDispersal Transfer learning method (C-VisDiT) for FUDA.Specifically, C-VisDiT consists of a cross-domain visual dis-persal strategy that transfers only high-confidence source knowledge for model adaptation and an intra-domain vi-sual dispersal strategy that guides the learning of hard tar-get samples with easy ones. We conduct extensive exper-iments on Office-31, Office-Home, VisDA-C, and Domain-Net benchmark datasets and the results demonstrate that the proposed C-VisDiT significantly outperforms state-of-the-art FUDA methods. Our code is available at https://github.com/Bostoncake/C-VisDiT. 