Early exiting has become a promising approach to im-proving the inference efficiency of deep networks. By struc-turing models with multiple classifiers (exits), predictions for “easy” samples can be generated at earlier exits, negat-ing the need for executing deeper layers. Current multi-exit networks typically implement linear classifiers at interme-diate layers, compelling low-level features to encapsulate high-level semantics. This sub-optimal design invariably undermines the performance of later exits. In this paper, we propose Dynamic Perceiver (Dyn-Perceiver) to decou-ple the feature extraction procedure and the early classifi-cation task with a novel dual-branch architecture. A fea-ture branch serves to extract image features, while a classi-fication branch processes a latent code assigned for clas-sification tasks. Bi-directional cross-attention layers are established to progressively fuse the information of both branches. Early exits are placed exclusively within the classification branch, thus eliminating the need for linear separability in low-level features. Dyn-Perceiver consti-tutes a versatile and adaptable framework that can be built upon various architectures. Experiments on image classi-fication, action recognition, and object detection demon-strate that our method significantly improves the inference efficiency of different backbones, outperforming numerous competitive approaches across a broad range of computa-tional budgets. Evaluation on both CPU and GPU plat-forms substantiate the superior practical efficiency of Dyn-Perceiver. Code is available at https://www.github. com/LeapLabTHU/Dynamic_Perceiver. 