Deep neural networks are vulnerable to adversarial ex-amples, dictating the imperativeness to test the model’s ro-bustness before deployment. Transfer-based attackers craft adversarial examples against surrogate models and transfer them to victim models deployed in the black-box situation.To enhance the adversarial transferability, structure-based attackers adjust the backpropagation path to avoid the at-tack from overﬁtting the surrogate model. However, exist-ing structure-based attackers fail to explore the convolution module in CNNs and modify the backpropagation graph heuristically, leading to limited effectiveness.In this pa-per, we propose backPropagation pAth Search (PAS), solv-ing the aforementioned two problems. We ﬁrst propose Skip-Conv to adjust the backpropagation path of convolution by structural reparameterization. To overcome the drawback of heuristically designed backpropagation paths, we further construct a Directed Acyclic Graph (DAG) search space, utilize one-step approximation for path evaluation and em-ploy Bayesian Optimization to search for the optimal path.We conduct comprehensive experiments in a wide range of transfer settings, showing that PAS improves the attack suc-cess rate by a huge margin for both normally trained and defense models. 