An emerging line of work has sought to generate plau-sible imagery from touch. Existing approaches, however, tackle only narrow aspects of the visuo-tactile synthesis problem, and lag signiﬁcantly behind the quality of cross-modal synthesis methods in other domains. We draw on re-cent advances in latent diffusion to create a model for syn-thesizing images from tactile signals (and vice versa) and apply it to a number of visuo-tactile synthesis tasks. Using this model, we signiﬁcantly outperform prior work on the tactile-driven stylization problem, i.e., manipulating an im-age to match a touch signal, and we are the ﬁrst to success-fully generate images from touch without additional sources of information about the scene. We also successfully use our model to address two novel synthesis problems: gen-erating images that do not contain the touch sensor or the hand holding it, and estimating an image’s shading from its reﬂectance and touch. Project Page: https:// fredfyyang.github.io/vision-from-touch/ 