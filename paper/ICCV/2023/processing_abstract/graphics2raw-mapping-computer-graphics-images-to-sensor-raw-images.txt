Computer graphics (CG) rendering platforms produce imagery with ever-increasing photo realism. The narrow-ing domain gap between real and synthetic imagery makes it possible to use CG images as training data for deep learn-ing models targeting high-level computer vision tasks, such as autonomous driving and semantic segmentation. CG im-ages, however, are currently not suitable for low-level vision tasks targeting RAW sensor images. This is because RAW images are encoded in sensor-speciﬁc color spaces and in-cur pre-white-balance color casts caused by the sensor’s response to scene illumination. CG images are rendered di-rectly to a device-independent perceptual color space with-out needing white balancing. As a result, it is necessary to apply a mapping procedure to close the domain gap be-tween graphics and RAW images. To this end, we intro-duce a framework to process graphics images to mimic RAW sensor images accurately. Our approach allows a one-to-many mapping, where a single graphics image can be trans-formed to match multiple sensors and multiple scene illumi-nations. In addition, our approach requires only a handful of example RAW-DNG ﬁles from the target sensor as param-eters for the mapping process. We compare our method to alternative strategies and show that our approach produces more realistic RAW images and provides better results on three low-level vision tasks: RAW denoising, illumination estimation, and neural rendering for night photography. Fi-nally, as part of this work, we provide a dataset of 292 real-istic CG images for training low-light imaging models. 