Although convolutional neural networks (CNNs) have been proposed to remove adverse weather conditions in sin-gle images using a single set of pre-trained weights, they fail to restore weather videos due to the absence of tempo-ral information. Furthermore, existing methods for remov-ing adverse weather conditions (e.g., rain, fog, and snow) from videos can only handle one type of adverse weather.In this work, we propose the first framework for restoring videos from all adverse weather conditions by developing a video adverse-weather-component suppression network (ViWS-Net). To achieve this, we first devise a weather-agnostic video transformer encoder with multiple trans-former stages. Moreover, we design a long short-term tem-poral modeling mechanism for weather messenger to early fuse input adjacent video frames and learn weather-specific information. We further introduce a weather discrimi-nator with gradient reversion, to maintain the weather-invariant common information and suppress the weather-specific information in pixel features, by adversarially pre-dicting weather types. Finally, we develop a messenger-driven video transformer decoder to retrieve the residual weather-specific feature, which is spatiotemporally aggre-gated with hierarchical pixel features and refined to pre-dict the clean target frame of input videos. Experimen-tal results, on benchmark datasets and real-world weather videos, demonstrate that our ViWS-Net outperforms cur-rent state-of-the-art methods in terms of restoring videos degraded by any weather condition. 