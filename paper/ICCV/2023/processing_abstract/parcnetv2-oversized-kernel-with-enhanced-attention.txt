Transformers have shown great potential in various computer vision tasks. By borrowing design concepts from transformers, many studies revolutionized CNNs and showed remarkable results. This paper falls in this line of studies. Specifically, we propose a new convolutional neu-ral network, ParCNetV2, that extends the research line ofParCNetV1 by bridging the gap between CNN and ViT. It introduces two key designs: 1) Oversized Convolution (OC) with twice the size of the input, and 2) Bifurcate Gate Unit (BGU) to ensure that the model is input adaptive. FusingOC and BGU in a unified CNN, ParCNetV2 is capable of flexibly extracting global features like ViT, while maintain-ing lower latency and better accuracy. Extensive experi-ments demonstrate the superiority of our method over other convolutional neural networks and hybrid models that com-bine CNNs and transformers. The code are publicly avail-able at https://github.com/XuRuihan/ParCNetV2. 