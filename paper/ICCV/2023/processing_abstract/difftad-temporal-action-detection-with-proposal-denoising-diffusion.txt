We propose a new formulation of temporal action de-tection (TAD) with denoising diffusion, DiffTAD in short.Taking as input random temporal proposals, it can yield ac-tion proposals accurately given an untrimmed long video.This presents a generative modeling perspective, against previous discriminative learning manners. This capabil-ity is achieved by first diffusing the ground-truth propos-als to random ones (i.e., the forward/noising process) and then learning to reverse the noising process (i.e., the back-ward/denoising process). Concretely, we establish the de-noising process in the Transformer decoder (e.g., DETR) by introducing a temporal location query design with faster convergence in training. We further propose a cross-step selective conditioning algorithm for inference accelera-tion. Extensive evaluations on ActivityNet and THUMOS show that our DiffTAD achieves top performance com-pared to previous art alternatives. The code is available at https://github.com/sauradip/DiffusionTAD. 