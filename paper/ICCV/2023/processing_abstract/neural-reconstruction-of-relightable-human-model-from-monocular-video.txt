Creating relightable and animatable human characters from monocular video at a low cost is a critical task for dig-ital human modeling and virtual reality applications. This task is complex due to intricate articulation motion, a wide range of ambient lighting conditions, and pose-dependent clothing deformations. In this paper, we introduce a novel self-supervised framework that takes a monocular video of a moving human as input and generates a 3D neural rep-resentation capable of being rendered with novel poses un-der arbitrary lighting conditions. Our framework decom-poses dynamic humans under varying illumination into neu-ral fields in canonical space, taking into account geometry and spatially varying BRDF material properties. Addition-ally, we introduce pose-driven deformation fields, enabling bidirectional mapping between canonical space and obser-vation. Leveraging the proposed appearance decomposi-tion and deformation fields, our framework learns in a self-supervised manner. Ultimately, based on pose-driven defor-mation, recovered appearance, and physically-based ren-dering, the reconstructed human figure becomes relightable and can be explicitly driven by novel poses. We demonstrate significant performance improvements over previous works and provide compelling examples of relighting from monoc-ular videos of moving humans in challenging, uncontrolled capture scenarios. 