Deep learning algorithms require large amounts of la-beled data for effective performance, but the presence of noisy labels often significantly degrade their performance.Although recent studies on designing a robust objective function to label noise, known as the robust loss method, have shown promising results for learning with noisy la-bels, they suffer from the issue of underfitting not only noisy samples but also clean ones, leading to suboptimal model performance. To address this issue, we propose a novel learning framework that selectively suppresses noisy sam-ples while avoiding underfitting clean data. Our frame-work incorporates label confidence as a measure of label noise, enabling the network model to prioritize the training of samples deemed to be noise-free. The label confidence is based on the robust loss methods, and we provide theoreti-cal evidence that our method can reach the optimal point of the robust loss, subject to certain conditions. Furthermore, the proposed method is generalizable and can be combined with existing robust loss methods, making it suitable for a wide range of applications of learning with noisy labels.We evaluate our approach on both synthetic and real-world datasets, and the experimental results demonstrate its ef-fectiveness in achieving outstanding classification perfor-mance compared to state-of-the-art methods. 