Overfitting to the source domain is a common issue in gradient-based training of deep neural networks. To com-pensate for the over-parameterized models, numerous reg-ularization techniques have been introduced such as those based on dropout. While these methods achieve significant improvements on classical benchmarks such as ImageNet, their performance diminishes with the introduction of do-main shift in the test set i.e. when the unseen data comes from a significantly different distribution. In this paper, we move away from the classical approach of Bernoulli sam-pled dropout mask construction and propose to base the selection on gradient-signal-to-noise ratio (GSNR) of net-workâ€™s parameters. Specifically, at each training step, pa-rameters with high GSNR will be discarded. Furthermore, we alleviate the burden of manually searching for the opti-mal dropout ratio by leveraging a meta-learning approach.We evaluate our method on standard domain generalization benchmarks and achieve competitive results on classifica-tion and face anti-spoofing problems. 