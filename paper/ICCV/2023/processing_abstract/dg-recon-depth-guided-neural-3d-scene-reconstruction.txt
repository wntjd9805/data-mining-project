A key challenge in neural 3D scene reconstruction from monocular images is to fuse features back projected from various views without any depth or occlusion informa-tion. We address this by leveraging monocular depth pri-ors, which effectively guide the fusion to improve sur-face prediction and skip over irrelevant, ambiguous, or occluded features. Furthermore, we revisit the average-based fusion used by most neural 3D reconstruction meth-ods and propose two alternatives, a variance-based and a cross-attention-based fusion module, that are more efficient and effective than the average-based and self-attention-based counterparts. Compared to the NeuralRecon base-line, the proposed DG-Recon models significantly improve the reconstruction quality and completeness while remain-ing in real-time. Our method achieves state-of-the-art on-line reconstruction results on the ScanNet dataset and is on par with the current best offline method, which repeat-edly accesses keyframes from the entire video sequence.Our ScanNet-trained model also generalizes robustly to the challenging 7-Scenes dataset and a subset of SUN3D con-taining scenes as big as an entire floor.Figure 1. Depth-guided back projection and fusion. Each grid on the left figure represents a digitized 2D world. The orange rounded rectangle represents a table. The blue circle sketches a chair and the grey rectangle indicates the wall. Non-white cells picture back-projected features from K different camera viewsO(1), . . . , O(K). A cell with a red cross in it indicates an erro-neous back projection. The depth priors, denoted as the white sur-face curve in the middle row, introduce geometry awareness to the back projection and cross-view fusion. Objects, e.g. chairs and the monitor, become sharper, more complete, and better separated. 