Most nighttime semantic segmentation studies are based on domain adaptation approaches and image input. How-ever, limited by the low dynamic range of conventional cam-eras, images fail to capture structural details and boundary information in low-light conditions. Event cameras, as a new form of vision sensors, are complementary to conven-tional cameras with their high dynamic range. To this end, we propose a novel unsupervised Cross-Modality DomainAdaptation (CMDA) framework to leverage multi-modality (Images and Events) information for nighttime semantic seg-mentation, with only labels on daytime images. In CMDA, we design the Image Motion-Extractor to extract motion infor-mation and the Image Content-Extractor to extract content information from images, in order to bridge the gap between different modalities (Images ⇌ Events) and domains (Day⇌ Night). Besides, we introduce the first image-event night-time semantic segmentation dataset. Extensive experiments on both the public image dataset and the proposed image-event dataset demonstrate the effectiveness of our proposed approach. We open-source our code, models, and dataset at https://github.com/XiaRho/CMDA.Figure 1. Images captured at different moments in the same lo-cation show that the low dynamic range of frame-based cameras leads to reduced color contrast and detailed edges of objects at night. To overcome this challenge, we introduce event cameras that have a high dynamic range and are capable of capturing more nighttime details. In comparison to the semantic segmentation re-sults obtained from daytime images [36], nighttime images result in misclassification cases [14]. However, our proposed CMDA improves this by introducing event modality for the first time. 