Understanding 4D scene context in real world has be-come urgently critical for deploying sophisticated AI sys-tems.In this paper, we propose a brand new scene un-derstanding paradigm called “Context Graph Generation (CGG)”, aiming at abstracting holistic semantic informa-tion in the complicated 4D world. The CGG task capitalizes on the calibrated multiview videos of a dynamic scene, and targets at recovering semantic information (coordination, trajectories and relationships) of the presented objects in the form of spatio-temporal context graph in 4D space. We also present a benchmark 4D video dataset “RealGraph”, the first dataset tailored for the proposed CGG task. The raw data of RealGraph is composed of calibrated and syn-chronized multiview videos. We exclusively provide manual annotations including object 2D&3D bounding boxes, cat-egory labels and semantic relationships. We also make sure the annotated ID for every single object is temporally and spatially consistent. We propose the first CGG baseline al-gorithm, Multiview-based Context Graph Generation Net-work (MCGNet), to empirically investigate the legitimacy of CGG task on RealGraph dataset. We nevertheless re-veal the great challenges behind this task and encourage the community to explore beyond our solution. Our project page is at https://github.com/THU-luvision/RealGraph . 