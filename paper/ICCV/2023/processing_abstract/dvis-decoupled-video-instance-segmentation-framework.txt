Video instance segmentation (VIS) is a critical task with diverse applications, including autonomous driving and video editing. Existing methods often underperform on complex and long videos in real world, primarily due to two factors. Firstly, ofﬂine methods are limited by the tightly-coupled modeling paradigm, which treats all frames equally and disregards the interdependencies between ad-jacent frames. Consequently, this leads to the introduc-tion of excessive noise during long-term temporal align-ment. Secondly, online methods suffer from inadequate utilization of temporal information. To tackle these chal-lenges, we propose a decoupling strategy for VIS by di-viding it into three independent sub-tasks: segmentation, tracking, and reﬁnement. The efﬁcacy of the decoupling strategy relies on two crucial elements: 1) attaining pre-cise long-term alignment outcomes via frame-by-frame as-sociation during tracking, and 2) the effective utilization of temporal information predicated on the aforementioned ac-curate alignment outcomes during reﬁnement. We introduce a novel referring tracker and temporal reﬁner to construct the Decoupled VIS framework (DVIS). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by 7.3 AP and 9.6 VPQ on theOVIS and VIPSeg datasets, which are the most challeng-ing and realistic benchmarks. Moreover, thanks to the de-coupling strategy, the referring tracker and temporal re-ﬁner are super light-weight (only 1.69% of the segmenterFLOPs), allowing for efﬁcient training and inference on a single GPU with 11G memory. The code is available at https://github.com/zhang-tao-whu/DVIS. 