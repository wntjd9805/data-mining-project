Scene flow estimation, which predicts the 3D displace-ments of point clouds, is a fundamental task in autonomous driving. Most methods have adopted a coarse-to-fine struc-ture to balance computational efficiency with accuracy, par-ticularly when handling large displacements. However, in-accuracies in the initial coarse layer’s scene flow estimates may accumulate, leading to incorrect final estimates. To al-leviate this, we introduce a novel Iterative Hierarchical Net-work——IHNet. This approach circulates high-resolution estimated information (scene flow and feature) from the pre-ceding iteration back to the low-resolution layer of the cur-rent iteration. Serving as a guide, the high-resolution es-timated scene flow, instead of initializing the scene flow from zero, provides a more precise center for low-resolution layer to identify matches. Meanwhile, the decoder’s fea-ture at the high-resolution layer can contribute essential movement information. Furthermore, based on the recur-rent structure, we design a resampling scheme to enhance the correspondence between points across two consecutive frames. By employing the previous estimated scene flow to fine-tune the target frame’s coordinates, we can sig-nificantly reduce the correspondence discrepancy between two frame points, a problem often caused by point spar-sity. Following this adjustment, we continue to estimate the scene flow using the newly updated coordinates, along with the reencoded feature. Our approach outperforms the recent state-of-the-art method WSAFlowNet by 20.1% onFlyingThings3D and 56.0% on KITTI scene flow datasets according to EPE3D metric. The code is available at https://github.com/wangyunlhr/IHNet. 