regular video benchmarks.In this paper, we show that recent advances in video rep-resentation learning and pre-trained vision-language mod-els allow for substantial improvements in self-supervised video object localization. We propose a method that first localizes objects in videos via a slot attention approach and then assigns text to the obtained slots. The latter is achieved by an unsupervised way to read localized semantic informa-tion from the pre-trained CLIP model. The resulting video object localization is entirely unsupervised apart from the implicit annotation contained in CLIP, and it is effectively the first unsupervised approach that yields good results on∗ Equal contribution. Ke Fan is the first intern author, Zechen Bai is the first FTE author, they contributed equally. Work down during Ke Fan’s internship in AWS Shanghai AI Lab† Corresponding authors. Yanwei Fu and Ke Fan are with the School of Data Science, Fudan University. 