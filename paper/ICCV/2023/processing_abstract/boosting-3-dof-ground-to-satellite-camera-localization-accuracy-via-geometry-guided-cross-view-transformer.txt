Image retrieval-based cross-view localization methods often lead to very coarse camera pose estimation, due to the limited sampling density of the database satellite im-ages. In this paper, we propose a method to increase the accuracy of a ground camera’s location and orientation by estimating the relative rotation and translation between the ground-level image and its matched/retrieved satellite im-age. Our approach designs a geometry-guided cross-view transformer that combines the benefits of conventional ge-ometry and learnable cross-view transformers to map the ground-view observations to an overhead view. Given the synthesized overhead view and observed satellite feature maps, we construct a neural pose optimizer with strong global information embedding ability to estimate the rela-tive rotation between them. After aligning their rotations, we develop an uncertainty-guided spatial correlation to generate a probability map of the vehicle locations, from which the relative translation can be determined. Exper-imental results demonstrate that our method significantly outperforms the state-of-the-art. Notably, the likelihood of restricting the vehicle lateral pose to be within 1m of itsGround Truth (GT) value on the cross-view KITTI dataset has been improved from 35.54% to 76.44%, and the likeli-hood of restricting the vehicle orientation to be within 1◦ of its GT value has been improved from 19.64% to 99.10%. 