Transfer learning is beneﬁcial by allowing the expres-sive features of models pretrained on large-scale datasets to be ﬁnetuned for the target task of smaller, more domain-speciﬁc datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the ﬁnetuned model. In this work, we investigate bias when conceptualized as both spurious cor-relations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we ﬁnd that (1) mod-els ﬁnetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the ﬁnetuning dataset, and often with a negligible impact to performance. Our ﬁnd-ings imply that careful curation of the ﬁnetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model. 