Place recognition based on point clouds (LiDAR) is an important component for autonomous robots or self-driving vehicles. Current SOTA performance is achieved on accu-mulated LiDAR submaps using either point-based or voxel-based structures. While voxel-based approaches nicely in-tegrate spatial context across multiple scales, they do not exhibit the local precision of point-based methods. As a result, existing methods struggle with fine-grained match-ing of subtle geometric features in sparse single-shot Li-DAR scans. To overcome these limitations, we proposeCASSPR as a method to fuse point-based and voxel-based approaches using cross attention transformers. CASSPR leverages a sparse voxel branch for extracting and ag-gregating information at lower resolution and a point-wise branch for obtaining fine-grained local information.CASSPR uses queries from one branch to try to match structures in the other branch, ensuring that both extract self-contained descriptors of the point cloud (rather than one branch dominating), but using both to inform the out-put global descriptor of the point cloud. Extensive exper-iments show that CASSPR surpasses the state-of-the-art by a large margin on several datasets (Oxford RobotCar,TUM, USyd). For instance, it achieves AR@1 of 85.6% on the TUM dataset, surpassing the strongest prior model byâˆ¼15%. Our code is publicly available.1 