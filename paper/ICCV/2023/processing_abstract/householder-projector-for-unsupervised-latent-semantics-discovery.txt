Generative Adversarial Networks (GANs), especially the recent style-based generators (StyleGANs), have versatile semantics in the structured latent space. Latent semantics discovery methods emerge to move around the latent code such that only one factor varies during the traversal. Re-cently, an unsupervised method proposed a promising direc-tion to directly use the eigenvectors of the projection matrix that maps latent codes to features as the interpretable direc-tions. However, one overlooked fact is that the projection matrix is non-orthogonal and the number of eigenvectors is too large. The non-orthogonality would entangle semantic attributes in the top few eigenvectors, and the large dimen-sionality might result in meaningless variations among the directions even if the matrix is orthogonal. To avoid these issues, we propose Householder Projector, a flexible and general low-rank orthogonal matrix representation based on Householder transformations, to parameterize the pro-jection matrix. The orthogonality guarantees that the eigen-vectors correspond to disentangled interpretable semantics, while the low-rank property encourages that each identi-fied direction has meaningful variations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and eval-uate the models on several benchmarks. Within only 1% of the original training steps for fine-tuning, our projector helps StyleGANs to discover more disentangled and pre-cise semantic attributes without sacrificing image fidelity.Code is publicly available via https://github.com/KingJamesSong/HouseholderGAN .Figure 1: Motivation of our proposed Householder Projec-tor. Here “Projector” denotes the projection matrix that maps latent codes to features, i.e., the modulation weight of StyleGANs. (Top Left) The singular value imbalance of the non-orthogonal projector would entangle multiple se-mantics in the top interpretable directions. (Top Right) Due to the large dimensionality of the projector, directly en-forcing vanilla orthogonality would spread the data varia-tions among all the eigenvectors, leading to imperceptible and meaningless traversal. (Bottom) Our Householder Pro-jector equips the projection matrix with low-rank orthog-onal properties, which simultaneously disentangles seman-tics into multiple equally-important eigenvectors and guar-antees that each direction could correspond to semantically-meaningful variations. 