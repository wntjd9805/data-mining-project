Referring video object segmentation aims to segment a referent throughout a video sequence according to a nat-ural language expression. It requires aligning the natural language expression with the objectsâ€™ motions and their dy-namic associations at the global video level but segmenting objects at the frame level. To achieve this goal, we propose to simultaneously maintain a global referent token and a sequence of object queries, where the former is responsi-ble for capturing video-level referent according to the lan-guage expression, while the latter serves to better locate and segment objects with each frame. Furthermore, to ex-plicitly capture object motions and spatial-temporal cross-modal reasoning over objects, we propose a novel temporal collection-distribution mechanism for interacting between the global referent token and object queries. Specifically, the temporal collection mechanism collects global informa-tion for the referent token from object queries to the tempo-ral motions to the language expression. In turn, the tempo-ral distribution first distributes the referent token to the ref-erent sequence across all frames and then performs efficient cross-frame reasoning between the referent sequence and object queries in every frame. Experimental results show that our method outperforms state-of-the-art methods on all benchmarks consistently and significantly. 