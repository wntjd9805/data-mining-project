Nearest neighbour-based methods have proved to be one of the most successful self-supervised learning (SSL) approaches due to their high generalization capabilities.However, their computational efficiency decreases when more than one neighbour is used. In this paper, we propose a novel contrastive SSL approach, which we call All4One, that reduces the distance between neighbour representa-tions using ”centroids” created through a self-attention mechanism. We use a Centroid Contrasting objective along with single Neighbour Contrasting and Feature Contrast-ing objectives. Centroids help in learning contextual in-formation from multiple neighbours whereas the neighbour contrast enables learning representations directly from the neighbours and the feature contrast allows learning repre-sentations unique to the features. This combination enablesAll4One to outperform popular instance discrimination ap-proaches by more than 1% on linear classification evalua-tion for popular benchmark datasets and obtains state-of-the-art (SoTA) results. Finally, we show that All4One is robust towards embedding dimensionalities and augmenta-tions, surpassing NNCLR and Barlow Twins by more than 5% on low dimensionality and weak augmentation settings.Source code is available in https://github.com/ImaGonEs/all4one.Figure 1: Simplified architecture of All4One. All4One uses three different objective functions that contrast differ-ent representations: Centroid objective contrasts the contex-tual information extracted from multiple neighbours while the Neighbour objective assures diversity [14]. Addition-ally, the Feature contrast objective measures the correlation of the generated features and increases their independence. 