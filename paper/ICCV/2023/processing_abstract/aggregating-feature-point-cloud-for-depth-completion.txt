Guided depth completion aims to recover dense depth maps by propagating depth information from the given pix-els to the remaining ones under the guidance of RGB im-ages. However, most of the existing methods achieve this using a large number of iterative refinements or stacking repetitive blocks. Due to the limited receptive field of con-ventional convolution, the generalizability with respect to different sparsity levels of input depth maps is impeded.To tackle these problems, we propose a feature point cloud aggregation framework to directly propagate 3D depth in-formation between the given points and the missing ones.We extract 2D feature map from images and transform the sparse depth map to point cloud to extract sparse 3D fea-tures. By regarding the extracted features as two sets of fea-ture point clouds, the depth information for a target location can be reconstructed by aggregating adjacent sparse 3D features from the known points using cross attention. Based on this, we design a neural network, called as PointDC, to complete the entire depth information reconstruction pro-cess. Experimental results show that, our PointDC achieves superior or competitive results on the KITTI benchmark andNYUv2 dataset. In addition, the proposed PointDC demon-strates its higher generalizability to different sparsity levels of the input depth maps and cross-dataset evaluation. 