State-of-the-art visual grounding models can achieve high detection accuracy, but they are not designed to dis-tinguish between all objects versus only certain objects of interest. In natural language, in order to specify a partic-ular object or set of objects of interest, humans use deter-miners such as “my”, “either” and “those”. Determiners, as an important word class, are a type of schema in nat-ural language about the reference or quantity of the noun.Existing grounded referencing datasets place much less em-phasis on determiners, compared to other word classes such as nouns, verbs and adjectives. This makes it difficult to de-velop models that understand the full variety and complex-ity of object referencing. Thus, we have developed and re-leased the DetermiNet dataset 1, which comprises 250,000 synthetically generated images and captions based on 25 determiners. The task is to predict bounding boxes to iden-tify objects of interest, constrained by the semantics of the given determiner. We find that current state-of-the-art vi-sual grounding models do not perform well on the dataset, highlighting the limitations of existing models on reference and quantification tasks. 