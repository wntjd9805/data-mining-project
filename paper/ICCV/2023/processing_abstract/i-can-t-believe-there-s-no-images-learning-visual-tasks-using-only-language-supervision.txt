Many high-level skills that are required for computer vi-sion tasks, such as parsing questions, comparing and con-trasting semantics, and writing descriptions, are also re-quired in other domains such as natural language process-ing.In this paper, we ask whether it is possible to learn those skills from text data and then transfer them to vision tasks without ever training on visual training data. Key to our approach is exploiting the joint embedding space of contrastively trained vision and language encoders. In practice, there can be systematic differences between em-bedding spaces for different modalities in contrastive mod-els, and we analyze how these differences affect our ap-proach and study strategies to mitigate this concern. We produce models using only text training data on four repre-sentative tasks: image captioning, visual entailment, visual question answering and visual news captioning, and eval-uate them on standard benchmarks using images. We Ô¨Ånd these models perform close to models trained on images, while surpassing prior work for captioning and visual en-tailment in this text-only setting by over 9 points, and out-performing all prior work on visual news by over 30 points.We also showcase a variety of stylistic image captioning models that are trained using no image data and no human-curated language data, but instead using readily-available text data from books, the web, or language models. 