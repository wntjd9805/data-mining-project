Contrastive language-image pretraining (CLIP) has demonstrated remarkable success in various image tasks.However, how to extend CLIP with effective temporal mod-eling is still an open and crucial problem. Existing factor-ized or joint spatial-temporal modeling trades off between the efficiency and performance. While modeling temporal information within straight through tube is widely adopted in literature, we find that simple frame alignment already provides enough essence without temporal attention. To this end, in this paper, we proposed a novel Implicit Learn-able Alignment (ILA) method, which minimizes the tempo-ral modeling effort while achieving incredibly high perfor-mance. Specifically, for a frame pair, an interactive point is predicted in each frame, serving as a mutual informa-tion rich region. By enhancing the features around the interactive point, two frames are implicitly aligned. The aligned features are then pooled into a single token, which is leveraged in the subsequent spatial self-attention. Our method allows eliminating the costly or insufficient tempo-ral self-attention in video. Extensive experiments on bench-marks demonstrate the superiority and generality of our module. Particularly, the proposed ILA achieves a top-1 ac-curacy of 88.7% on Kinetics-400 with much fewer FLOPs compared with Swin-L and ViViT-H. Code is released at https://github.com/Francis-Rings/ILA. 