Object understanding in egocentric visual data is ar-guably a fundamental research topic in egocentric vision.However, existing object datasets are either non-egocentric or have limitations in object categories, visual content, and annotation granularities.In this work, we intro-duce EgoObjects, a large-scale egocentric dataset for fine-grained object understanding.Its Pilot version contains over 9K videos collected by 250 participants from 50+ countries using 4 wearable devices, and over 650K ob-ject annotations from 368 object categories. Unlike prior datasets containing only object category labels, EgoObjects also annotates each object with an instance-level identifier, and includes over 14K unique object instances. EgoOb-jects was designed to capture the same object under diverse background complexities, surrounding objects, distance, lighting and camera motion. In parallel to the data collec-tion, we conducted data annotation by developing a multi-stage federated annotation process to accommodate the growing nature of the dataset. To bootstrap the research onEgoObjects, we present a suite of 4 benchmark tasks around the egocentric object understanding, including a novel in-stance level- and the classical category level object detec-tion. Moreover, we also introduce 2 novel continual learn-ing object detection tasks. The dataset and API are avail-able at https://github.com/facebookresearch/EgoObjects. 