Frame quality deterioration is one of the main chal-lenges in the field of video understanding. To compen-sate for the information loss caused by deteriorated frames, recent approaches exploit transformer-based integration modules to obtain spatio-temporal information. However, these integration modules are heavy and complex. Fur-thermore, each integration module is specifically tailored for its target task, making it difficult to generalise to mul-tiple tasks.In this paper, we present a neat and uni-fied framework, called Spatio-Temporal Prompting Net-work (STPN). It can efficiently extract robust and accu-rate video features by dynamically adjusting the input fea-tures in the backbone network. Specifically, STPN predicts several video prompts containing spatio-temporal informa-tion of neighbour frames. Then, these video prompts are prepended to the patch embeddings of the current frame as the updated input for video feature extraction. More-over, STPN is easy to generalise to various video tasks be-cause it does not contain task-specific modules. Without bells and whistles, STPN achieves state-of-the-art perfor-mance on three widely-used datasets for different video un-derstanding tasks, i.e., ImageNetVID for video object de-tection, YouTubeVIS for video instance segmentation, andGOT-10k for visual object tracking. Codes are available at https://github.com/guanxiongsun/STPN 