Recently, stereo vision based on lightweight RGBD cam-eras has been widely used in various fields. However, lim-ited by the imaging principles, the commonly used RGB-D cameras based on TOF, structured light, or binocular vision acquire some invalid data inevitably, such as weak reflec-tion, boundary shadows, and artifacts, which may bring ad-verse impacts to the follow-up work. In this paper, we pro-pose a new model for depth image completion based on theAttention Guided Gated-convolutional Network (AGG-Net), through which more accurate and reliable depth images can be obtained from the raw depth maps and the correspondingRGB images. Our model employs a UNet-like architecture which consists of two parallel branches of depth and color features. In the encoding stage, an Attention Guided Gated-Convolution (AG-GConv) module is proposed to realize the fusion of depth and color features at different scales, which can effectively reduce the negative impacts of invalid depth data on the reconstruction. In the decoding stage, an Atten-tion Guided Skip Connection (AG-SC) module is presented to avoid introducing too many depth-irrelevant features to the reconstruction. The experimental results demonstrate that our method outperforms the state-of-the-art methods on the popular benchmarks NYU-Depth V2, DIML, andSUN RGB-D. https://github.com/htx0601/AGG-Net 