The superior performance of modern deep networks usu-ally comes with a costly training procedure. This paper presents a new curriculum learning approach for the effi-cient training of visual backbones (e.g., vision Transform-ers). Our work is inspired by the inherent learning dynam-ics of deep networks: we experimentally show that at an earlier training stage, the model mainly learns to recog-nize some ‘easier-to-learn’ discriminative patterns within each example, e.g., the lower-frequency components of im-ages and the original information before data augmen-tation. Driven by this phenomenon, we propose a cur-riculum where the model always leverages all the train-ing data at each epoch, while the curriculum starts with only exposing the ‘easier-to-learn’ patterns of each exam-ple, and introduces gradually more difficult patterns. To implement this idea, we 1) introduce a cropping opera-tion in the Fourier spectrum of the inputs, which enables the model to learn from only the lower-frequency compo-nents efficiently, 2) demonstrate that exposing the features of original images amounts to adopting weaker data aug-mentation, and 3) integrate 1) and 2) and design a cur-riculum learning schedule with a greedy-search algorithm.The resulting approach, EfficientTrain, is simple, general, yet surprisingly effective. As an off-the-shelf method, it re-duces the wall-time training cost of a wide variety of popu-lar models (e.g., ResNet, ConvNeXt, DeiT, PVT, Swin, andCSWin) by > 1.5× on ImageNet-1K/22K without sacrific-ing accuracy. It is also effective for self-supervised learn-ing (e.g., MAE). Code is available at https://github. com/LeapLabTHU/EfficientTrain. 