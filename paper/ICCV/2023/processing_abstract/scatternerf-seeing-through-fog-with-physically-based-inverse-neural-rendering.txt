Vision in adverse weather conditions, whether it be snow, rain, or fog is challenging.In these scenarios, scatter-ing and attenuation severly degrades image quality. Han-dling such inclement weather conditions, however, is essen-tial to operate autonomous vehicles, drones and robotic ap-plications where human performance is impeded the most.A large body of work explores removing weather-induced image degradations with dehazing methods. Most meth-ods rely on single images as input and struggle to gener-alize from synthetic fully-supervised training approaches or to generate high fidelity results from unpaired real-world datasets. With data as bottleneck and most of today’s training data relying on good weather conditions with in-clement weather as outlier, we rely on an inverse render-ing approach to reconstruct the scene content. We intro-duce ScatterNeRF, a neural rendering method which ade-quately renders foggy scenes and decomposes the fog-free background from the participating media – exploiting the multiple views from a short automotive sequence without the need for a large training data corpus. Instead, the ren-dering approach is optimized on the multi-view scene itself, which can be typically captured by an autonomous vehicle, robot or drone during operation. Specifically, we propose a disentangled representation for the scattering volume and the scene objects, and learn the scene reconstruction with physics-inspired losses. We validate our method by captur-ing multi-view In-the-Wild data and controlled captures in a large-scale fog chamber. Our code and datasets are avail-able at https://light.princeton.edu/scatternerf. 