This work focuses on training a single visual relation-ship detector predicting over the union of label spaces from multiple datasets. Merging labels spanning differ-ent datasets could be challenging due to inconsistent tax-onomies. The issue is exacerbated in visual relationship de-tection when second-order visual semantics are introduced between pairs of objects. To address this challenge, we propose UniVRD, a novel bottom-up method for UniﬁedVisual Relationship Detection by leveraging vision and lan-guage models (VLMs). VLMs provide well-aligned image and text embeddings, where similar relationships are op-timized to be close to each other for semantic uniﬁcation.Our bottom-up design enables the model to enjoy the ben-eﬁt of training with both object detection and visual rela-tionship datasets. Empirical results on both human-object interaction detection and scene-graph generation demon-strate the competitive performance of our model. UniVRD achieves 38.07 mAP on HICO-DET, outperforming the cur-rent best bottom-up HOI detector by 14.26 mAP. More im-portantly, we show that our uniﬁed detector performs as well as dataset-speciﬁc models in mAP, and achieves fur-ther improvements when we scale up the model. Our code will be made publicly available on GitHub1. 