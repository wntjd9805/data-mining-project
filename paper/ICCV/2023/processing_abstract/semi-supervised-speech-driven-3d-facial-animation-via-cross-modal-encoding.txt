Existing Speech-driven 3D facial animation methods typically follow the supervised paradigm, involving regres-sion from speech to 3D facial animation. This paradigm faces two major challenges: the high cost of supervision acquisition, and the ambiguity in mapping between speech and lip movements. To address these challenges, this study proposes a novel cross-modal semi-supervised framework, comprising a Speech-to-Image Transcoder and a Face-to-Geometry Regressor. The former jointly learns a com-mon representation space from speech and image domains, enabling the transformation of speech into semantically-consistent facial images. The latter is responsible for re-constructing 3D facial meshes from the transformed im-ages. Both modules require minimal effort to acquire the necessary training data, thereby obviating the dependence on costly supervised data. Furthermore, the joint learn-ing scheme enables the fusion of intricate visual features into speech encoding, thereby facilitating the transforma-tion of subtle speech variations into nuanced lip movements, ultimately enhancing the ﬁdelity of 3D face reconstruc-tions. Consequently, the ambiguity of the direct mapping of speech-to-animation is signiﬁcantly reduced, leading to coherent and high-ﬁdelity generation of lip motion. Exten-sive experiments demonstrate that our approach produces competitive results compared to supervised methods. 