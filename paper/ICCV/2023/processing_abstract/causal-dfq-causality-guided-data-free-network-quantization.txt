Model quantization, which aims to compress deep neu-ral networks and accelerate inference speed, has greatly facilitated the development of cumbersome models on mo-bile and edge devices. There is a common assumption in quantization methods from prior works that training data is available.In practice, however, this assumption can-not always be fulfilled due to reasons of privacy and se-curity, rendering these methods inapplicable in real-life sit-uations. Thus, data-free network quantization has recently received significant attention in neural network compres-sion. Causal reasoning provides an intuitive way to model causal relationships to eliminate data-driven correlations, making causality an essential component of analyzing data-free problems. However, causal formulations of data-free quantization are inadequate in the literature. To bridge this gap, we construct a causal graph to model the data genera-tion and discrepancy reduction between the pre-trained and quantized models.Inspired by the causal understanding, we propose the Causality-guided Data-free Network Quan-tization method, Causal-DFQ, to eliminate the reliance on data via approaching an equilibrium of causality-driven in-tervened distributions. Specifically, we design a content-style-decoupled generator, synthesizing images conditioned on the relevant and irrelevant factors; then we propose a discrepancy reduction loss to align the intervened distribu-tions of the pre-trained and quantized models. It is worth noting that our work is the first attempt towards introduc-ing causality to data-free quantization problem. Extensive experiments demonstrate the efficacy of Causal-DFQ. The code is available at Causal-DFQ. 