Recent works have shown that 3D-aware GANs trained on unstructured single image collections can generate mul-tiview images of novel instances. The key underpinnings to achieve this are a 3D radiance field generator and a volume rendering process. However, existing methods ei-ther cannot generate high-resolution images (e.g., up to 256×256) due to the high computation cost of neural vol-ume rendering, or rely on 2D CNNs for image-space upsam-pling which jeopardizes the 3D consistency across different views. This paper proposes a novel 3D-aware GAN that can generate high resolution images (up to 1024×1024) while keeping strict 3D consistency as in volume render-ing. Our motivation is to achieve super-resolution directly in the 3D space to preserve 3D consistency. We avoid the otherwise prohibitively-expensive computation cost by ap-plying 2D convolutions on a set of 2D radiance manifolds*Work done when JX and YD were interns at MSRA. 1Project page: https://jeffreyxiang.github.io/GRAM-HD/ defined in the recent generative radiance manifold (GRAM) approach, and apply dedicated loss functions for effectiveGAN training at high resolution. Experiments on FFHQ and AFHQv2 datasets show that our method can produce high-quality 3D-consistent results that significantly outper-form existing methods. It makes a significant step towards closing the gap between traditional 2D image generation and 3D-consistent free-view generation. 1 