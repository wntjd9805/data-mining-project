Data-free knowledge distillation (DFKD) is a promis-ing approach for addressing issues related to model com-pression, security privacy, and transmission restrictions.Although the existing methods exploiting DFKD have achieved inspiring achievements in coarse-grained clas-siﬁcation, in practical applications involving ﬁne-grained classiﬁcation tasks that require more detailed distinctions between similar categories, sub-optimal results are ob-tained. To address this issue, we propose an approach called DFKD-FGVC that extends DFKD to ﬁne-grained vi-sual categorization (FGVC) tasks. Our approach utilizes an adversarial distillation framework with attention gen-erator, mixed high-order attention distillation, and seman-tic feature contrast learning. Speciﬁcally, we introduce a spatial-wise attention mechanism to the generator to syn-thesize ﬁne-grained images with more details of discrimi-native parts. We also utilize the mixed high-order attention mechanism to capture complex interactions among parts and the subtle differences among discriminative features of the ﬁne-grained categories, paying attention to both lo-cal features and semantic context relationships. Moreover, we leverage the teacher and student models of the distilla-tion framework to contrast high-level semantic feature maps in the hyperspace, comparing variances of different cat-egories. We evaluate our approach on three widely-usedFGVC benchmarks (Aircraft, Cars196, and CUB200) and demonstrate its superior performance. Code is available at https://github.com/RoryShao/DFKD-FGVC.git 