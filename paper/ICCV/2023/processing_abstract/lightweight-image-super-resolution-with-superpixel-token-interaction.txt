Transformer-based methods have demonstrated impres-sive results on single-image super-resolution (SISR) task.However, self-attention mechanism is computationally ex-pensive when applied to the entire image. As a result, current approaches divide low-resolution input images into small patches, which are processed separately and then fused to generate high-resolution images. Nevertheless, this conventional regular patch division is too coarse and lacks interpretability, resulting in artifacts and non-similar structure interference during attention operations. To ad-dress these challenges, we propose a novel super token in-teraction network (SPIN). Our method employs superpix-els to cluster local similar pixels to form the explicable lo-cal regions and utilizes intra-superpixel attention to enable local information interaction.It is interpretable because only similar regions complement each other and dissimi-lar regions are excluded. Moreover, we design a super-pixel cross-attention module to facilitate information prop-agation via the surrogation of superpixels. Extensive ex-periments demonstrate that the proposed SPIN model per-forms favorably against the state-of-the-art SR methods in terms of accuracy and lightweight. Code is available at https://github.com/ArcticHare105/SPIN . 