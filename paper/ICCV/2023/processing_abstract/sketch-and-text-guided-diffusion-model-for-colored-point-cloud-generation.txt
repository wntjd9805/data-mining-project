Diffusion probabilistic models have achieved remark-able success in text guided image generation. However, generating 3D shapes is still challenging due to the lack of sufficient data containing 3D models along with their de-scriptions. Moreover, text based descriptions of 3D shapes are inherently ambiguous and lack details. In this paper, we propose a sketch and text guided probabilistic diffusion model for colored point cloud generation that conditions the denoising process jointly with a hand drawn sketch of the object and its textual description. We incrementally dif-fuse the point coordinates and color values in a joint dif-fusion process to reach a Gaussian distribution. Colored point cloud generation thus amounts to learning the reverse diffusion process, conditioned by the sketch and text, to it-eratively recover the desired shape and color. Specifically, to learn effective sketch-text embedding, our model adap-tively aggregates the joint embedding of text prompt and the sketch based on a capsule attention network. Our model uses staged diffusion to generate the shape and then as-sign colors to different parts conditioned on the appear-ance prompt while preserving precise shapes from the first stage. This gives our model the flexibility to extend to multi-ple tasks, such as appearance re-editing and part segmenta-tion. Experimental results demonstrate that our model out-performs recent state-of-the-art in point cloud generation. 