Video action segmentation involves categorizing each frame or short snippet of an untrimmed video into prede-ﬁned action categories. Despite notable advancements in recent years, a considerable number of current approaches still rely on frame-wise segmentation that tends to render fragmentary results. To address it, we present an inno-vative approach for video action segmentation, centeredInitially, around contextually reﬁned temporal keypoints. our method identiﬁes a set of sparse, over-complete tem-poral keypoints through non-local visual cues, with each keypoint representing a potential action segment candi-date. Subsequent enhancements to these initial keypoints are achieved through iterative reﬁning and re-assembling operations. Driven by the notion that optimal temporal key-points should collectively resemble the true ground-truth structurally, we introduce a module that conducts graph matching between the keypoint-derived graph and the ref-erence graph constructed from accurate annotations. This module effectively learns structural features used to fur-ther reﬁne the initial keypoints. Moreover, a set of pre-deﬁned rules is applied to re-assemble all temporal key-points. The unﬁltered temporal keypoints, resulting from these operations, are harnessed to generate the ﬁnal ac-tion segments. We extensively evaluate our method across three video benchmarks: 50salads, GTEA, and Breakfast.Our proposed approach consistently demonstrates substan-tial improvements over existing methods, establishing its su-periority in video action segmentation. It achieves F 1@50 scores (one of the key performance metrics for this task) of 79.5%, 83.4%, and 60.5%, respectively, v.s. previous state-of-the-art 78.5%, 79.8% and 57.4%. 