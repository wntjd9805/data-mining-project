We introduce AiT, a unified output representation for var-ious vision tasks, which is a crucial step towards general-purpose vision task solvers. Despite the challenges posed by the high-dimensional and task-specific outputs, we show-case the potential of using discrete representation (VQ-VAE) to model the dense outputs of many computer vi-sion tasks as a sequence of discrete tokens. This is in-spired by the established ability of VQ-VAE to conserve the structures spanning multiple pixels using few discrete codes. To that end, we present a modified shallower ar-chitecture for VQ-VAE that improves efficiency while keep-ing prediction accuracy. Our approach also incorporates uncertainty into the decoding process by using a soft fu-sion of the codebook entries, providing a more stable training process, which notably improved prediction accu-racy. Our evaluation of AiT on depth estimation and in-stance segmentation tasks, with both continuous and dis-crete labels, demonstrates its superiority compared to other unified models. The code and models are available at https://github.com/SwinTransformer/AiT. 