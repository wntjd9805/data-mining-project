Learning a policy with great generalization to unseen environments remains challenging but critical in visual re-inforcement learning. Despite the success of augmenta-tion combination in the supervised learning generalization, naively applying it to visual RL algorithms may damage the training efficiency, suffering from serve performance degradation.In this paper, we first conduct qualitative analysis and illuminate the main causes: (i) high-variance gradient magnitudes and (ii) gradient conflicts existed in various augmentation methods. To alleviate these issues, we propose a general policy gradient optimization frame-work, named Conflict-aware Gradient Agreement Augmen-tation (CG2A), and better integrate augmentation combina-tion into visual RL algorithms to address the generalization bias. In particular, CG2A develops a Gradient AgreementSolver to adaptively balance the varying gradient magni-tudes, and introduces a Soft Gradient Surgery strategy to al-leviate the gradient conflicts. Extensive experiments demon-strate that CG2A significantly improves the generalization performance and sample efficiency of visual RL algorithms. 