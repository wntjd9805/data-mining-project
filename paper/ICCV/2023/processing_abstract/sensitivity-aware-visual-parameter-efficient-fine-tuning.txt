Visual Parameter-Efficient Fine-Tuning (PEFT) has be-come a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimiza-tion difficulty. However, existing PEFT methods introduce trainable parameters to the same positions across differ-ent tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to intro-duce and how to allocate trainable parameters by proposing a novel Sensitivity-aware visual Parameter-efficient fine-Tuning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a de-sired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tun-ing for a given task in a data-dependent way. Next, ourSPT further boosts the representational capability for the weight matrices whose number of sensitive parameters ex-ceeds a pre-defined threshold by utilizing existing struc-tured tuning methods, e.g., LoRA [21] or Adapter [20], to replace directly tuning the selected sensitive parameters (unstructured tuning) under the budget. Extensive exper-iments on a wide range of downstream recognition tasks show that our SPT is complementary to the existing PEFT methods and largely boosts their performance, e.g., SPT im-proves Adapter with supervised pre-trained ViT-B/16 back-bone by 4.2% and 1.4% mean Top-1 accuracy, reachingSOTA performance on FGVC and VTAB-1k benchmarks, re-spectively. Source code is at https://github.com/ ziplab/SPT. 