Pre-training across 3D vision and language remains un-der development because of limited training data. Re-cent works attempt to transfer vision-language (V-L) pre-training methods to 3D vision. However, the domain gap between 3D and images is unsolved, so that V-L pre-trained models are restricted in 3D downstream tasks. To ad-dress this issue, we propose CLIP2Point, an image-depth pre-training method by contrastive learning to transferCLIP to the 3D domain, and adapt it to point cloud clas-sification. We introduce a new depth rendering setting that forms a better visual effect, and then render 52,460 pairs of images and depth maps from ShapeNet for pre-training. The pre-training scheme of CLIP2Point com-bines cross-modality learning to enforce the depth fea-tures for capturing expressive visual and textual features and intra-modality learning to enhance the invariance of depth aggregation. Additionally, we propose a novel GatedDual-Path Adapter (GDPA), i.e., a dual-path structure with global-view aggregators and gated fusion for down-It allows the ensemble stream representative learning. of CLIP and CLIP2Point, tuning pre-training knowledge to downstream tasks in an efficient adaptation. Experi-mental results show that CLIP2Point is effective in trans-ferring CLIP knowledge to 3D vision. CLIP2Point out-performs other 3D transfer learning and pre-training net-works, achieving state-of-the-art results on zero-shot, few-shot, and fully-supervised classification. Codes are avail-able at: https://github.com/tyhuang0428/CLIP2Point. 