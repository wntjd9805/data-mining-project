Many educational videos use slide presentations, a se-quence of visual pages that contain text and ﬁgures ac-companied by spoken language, which are constructed and presented carefully in order to optimally transfer knowl-edge to students. Previous studies in multimedia and psy-chology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing vision-language models to aid in student learning as intel-ligent teacher assistants, we introduce the Lecture Presen-tations Multimodal (LPM) Dataset as a large-scale bench-mark testing the capabilities of vision-and-language models in multimodal understanding of educational videos. Our dataset contains aligned slides and spoken language, for 180+ hours of video and 9000+ slides, with 10 lecturers from various subjects (e.g., computer science, dentistry, bi-ology). We introduce three research tasks, (1) ﬁgure-to-text retrieval, (2) text-to-ﬁgure retrieval, and (3) genera-tion of slide explanations, which are grounded in multi-media learning and psychology principles to test a vision-language model’s understanding of multimodal content. We provide manual annotations to help implement these tasks and establish baselines on them. Comparing baselines and human student performances, we ﬁnd that state-of-the-art vision-language models (zero-shot and ﬁne-tuned) strug-gle in (1) weak crossmodal alignment between slides and spoken text, (2) learning novel visual mediums, (3) tech-nical language, and (4) long-range sequences. We intro-duce PolyViLT, a novel multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches for retrieval. We conclude by shedding light on the challenges and opportunities in multimodal un-derstanding of educational presentation videos. 