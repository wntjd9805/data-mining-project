Input ImageGroundTruthMKDPatch-wiseDiscrepancy+Intra-Correlation+Inter-Correlation (Ours)Humans recognize anomalies through two aspects: larger patch-wise representation discrepancies and weaker patch-to-normal-patch correlations. However, the previ-ous AD methods didn’t sufﬁciently combine the two com-plementary aspects to design AD models. To this end, we ﬁnd that Transformer can ideally satisfy the two as-pects as its great power in the uniﬁed modeling of patch-In wise representations and patch-to-patch correlations. this paper, we propose a novel AD framework: FOcus-the-Discrepancy (FOD), which can simultaneously spot the patch-wise, intra- and inter-discrepancies of anomalies.The major characteristic of our method is that we reno-vate the self-attention maps in transformers to Intra-Inter-Correlation (I2Correlation). The I2Correlation contains a two-branch structure to ﬁrst explicitly establish intra-and inter-image correlations, and then fuses the features of two-branch to spotlight the abnormal patterns. To learn the intra- and inter-correlations adaptively, we propose theRBF-kernel-based target-correlations as learning targets for self-supervised learning. Besides, we introduce an en-tropy constraint strategy to solve the mode collapse issue in optimization and further amplify the normal-abnormal distinguishability. Extensive experiments on three unsuper-vised real-world AD benchmarks show the superior perfor-mance of our approach. Code will be available at https://github.com/xcyao00/FOD. 