Reducing communication overhead in federated learn-ing (FL) is challenging but crucial for large-scale dis-tributed privacy-preserving machine learning. While meth-ods utilizing sparsification or other techniques can largely reduce the communication overhead, the convergence rate is also greatly compromised. In this paper, we propose a novel method named Single-Step Synthetic Features Com-pressor (3SFC) to achieve communication-efficient FL by directly constructing a tiny synthetic dataset containing synthetic features based on raw gradients. Therefore, 3SFC can achieve an extremely low compression rate when the constructed synthetic dataset contains only one data sam-ple. Additionally, the compressing phase of 3SFC utilizes a similarity-based objective function so that it can be op-timized with just one step, considerably improving its per-formance and robustness. To minimize the compressing er-ror, error feedback (EF) is also incorporated into 3SFC.Experiments on multiple datasets and models suggest that 3SFC has significantly better convergence rates compared to competing methods with lower compression rates (i.e., up to 0.02%). Furthermore, ablation studies and visual-izations show that 3SFC can carry more information than competing methods for every communication round, further validating its effectiveness. 