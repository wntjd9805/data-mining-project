Vision-and-language navigation (VLN) requires an em-bodied agent to navigate in realistic 3D environments using natural language instructions. Existing VLN methods suffer from training on small-scale environments or unreasonable path-instruction datasets, limiting the generalization to un-seen environments. There are massive house tour videos onYouTube, providing abundant real navigation experiences and layout information. However, these videos have not been explored for VLN before. In this paper, we propose to learn an agent from these videos by creating a large-scale dataset which comprises reasonable path-instruction pairs from house tour videos and pre-training the agent on it. To achieve this, we have to tackle the challenges of auto-matically constructing path-instruction pairs and exploiting real layout knowledge from raw and unlabeled videos. To address these, we Ô¨Årst leverage an entropy-based method to construct the nodes of a path trajectory. Then, we propose an action-aware generator for generating instruc-tions from unlabeled trajectories. Last, we devise a trajec-tory judgment pretext task to encourage the agent to mine the layout knowledge. Experimental results show that our method achieves state-of-the-art performance on two popu-lar benchmarks (R2R and REVERIE). Code is available at https://github.com/JeremyLinky/YouTube-VLN 