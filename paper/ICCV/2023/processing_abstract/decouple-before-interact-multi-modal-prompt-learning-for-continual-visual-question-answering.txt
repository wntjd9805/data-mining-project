In the real world, a desirable Visual Question An-swering model is expected to provide correct answers to new questions and images in a continual setting (recog-nized as CL-VQA). However, existing works formulate CL-VQA from a vision-only or language-only perspective, and straightforwardly apply the uni-modal continual learning (CL) strategies to this multi-modal task, which is improper and suboptimal. On the one hand, such a partial for-mulation may result in limited evaluations. On the other hand, neglecting the interactions between modalities will lead to poor performance. To tackle these challenging is-sues, we propose a comprehensive formulation for CL-VQA from the perspective of multi-modal vision-language fusion.Based on our formulation, we further propose MulTi-ModalPRompt LearnIng with DecouPLing bEfore InTeraction (TRIPLET), a novel approach that builds on a pre-trained vision-language model and consists of decoupled prompts and prompt interaction strategies to capture the complex interactions between modalities. In particular, decoupled prompts contain learnable parameters that are decoupled w.r.t different aspects, and the prompt interaction strategies are in charge of modeling interactions between inputs and prompts. Additionally, we build two CL-VQA benchmarks for a more comprehensive evaluation. Extensive experi-ments demonstrate that our TRIPLET outperforms state-of-the-art methods in both uni-modal and multi-modal contin-ual settings for CL-VQA. 