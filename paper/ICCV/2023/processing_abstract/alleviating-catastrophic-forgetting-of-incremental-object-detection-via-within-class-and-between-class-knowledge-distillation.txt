Incremental object detection (IOD) task requires a model to learn continually from newly added data. However, di-rectly fine-tuning a well-trained detection model on a new task will sharply decrease the performance on old tasks, which is known as catastrophic forgetting. Knowledge dis-tillation, including feature distillation and response distil-lation, has been proven to be an effective way to alleviate catastrophic forgetting. However, previous works on feature distillation heavily rely on low-level feature information, while under-exploring the importance of high-level seman-tic information. In this paper, we discuss the cause of catas-trophic forgetting in IOD task as destruction of semantic feature space. We propose a method that dynamically dis-tills both semantic and feature information with consider-ation of both between-class discriminativeness and within-class consistency on Transformer-based detector. Between-class discriminativeness is preserved by distilling class-level semantic distance and feature distance among various categories, while within-class consistency is preserved by distilling instance-level semantic information and feature information within each category. Extensive experiments are conducted on both Pascal VOC and MS COCO bench-marks. Our method outperforms all the previous CNN-based SOTA methods under various experimental scenar-ios, with a remarkable mAP improvement from 36.90% to 39.80% under one-step IOD task. (a) Teacher (b) Fine-Tune ep1 (c) Fine-Tune ep12 (d) LwF baseline (e) DMD (f) DMD + IFDFigure 1. Visualization of semantic feature space of old cate-gories. (a) represents the semantic feature space of old categories before adding new categories. (b)-(f) represent the semantic fea-ture space of old categories after adding new categories. (b) is (c) is at epoch 12 using at epoch 1 using fine-tuning method. fine-tuning method. The top three figures illustrate the cause of catastrophic forgetting as destruction of within-class con-sistency and between-class discrimnativeness. (d) using LwF baseline method. (f) using DMD (e) using our DMD method. method and IFD method at the same time. The bottom three fig-ures prove that our method can alleviate catastrophic forget-ting via maintaining the within-class consistency and between-class discrimnativeness from teacher to student. 