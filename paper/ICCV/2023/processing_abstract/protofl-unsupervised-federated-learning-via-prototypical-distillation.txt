Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for au-thentication systems. However, limited round communica-tions, scarce representation, and scalability pose signifi-cant challenges to its deployment, hindering its full po-tential. In this paper, we propose ‘ProtoFL’, PrototypicalRepresentation Distillation based unsupervised FederatedLearning to enhance the representation power of a global model and reduce round communication costs. Addition-ally, we introduce a local one-class classifier based on nor-malizing flows to improve performance with limited data.Our study represents the first investigation of using FL to improve one-class classification performance. We con-duct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, andKeystroke-Dynamics, to demonstrate the superior perfor-mance of our proposed framework over previous methods in the literature. 