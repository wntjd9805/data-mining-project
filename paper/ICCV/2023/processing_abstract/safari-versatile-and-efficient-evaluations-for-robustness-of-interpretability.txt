Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explain-able AI (XAI) community, explanations lack robustnessâ€” indistinguishable input perturbations may lead to differentXAI results. Thus, it is vital to assess how robust DL inter-pretability is, given an XAI method. In this paper, we iden-tify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not compre-hensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation meth-ods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respec-tively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to esti-mate rare event probabilities, is used for evaluating overall robustness. Experiments show that the accuracy, sensitiv-ity, and efficiency of our methods outperform the state-of-the-arts. Finally, we demonstrate two applications of our methods: ranking robust XAI methods and selecting train-ing schemes to improve both classification and interpreta-tion robustness. 