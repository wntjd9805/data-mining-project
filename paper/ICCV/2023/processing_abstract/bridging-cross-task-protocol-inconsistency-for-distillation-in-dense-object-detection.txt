Knowledge distillation (KD) has shown potential for learning compact models in dense object detection. How-ever, the commonly used softmax-based distillation ignores the absolute classification scores for individual categories.Thus, the optimum of the distillation loss does not neces-sarily lead to the optimal student classification scores for dense object detectors. This cross-task protocol inconsis-tency is critical, especially for dense object detectors, since the foreground categories are extremely imbalanced. To ad-dress the issue of protocol differences between distillation and classification, we propose a novel distillation method with cross-task consistent protocols, tailored for the dense object detection. For classification distillation, we address the cross-task protocol inconsistency problem by formulat-ing the classification logit maps in both teacher and stu-dent models as multiple binary-classification maps and ap-plying a binary-classification distillation loss to each map.For localization distillation, we design an IoU-based Lo-calization Distillation Loss that is free from specific net-work structures and can be compared with existing local-ization distillation losses. Our proposed method is sim-ple but effective, and experimental results demonstrate its superiority over existing methods. Code is available at https://github.com/TinyTigerPan/BCKD. 