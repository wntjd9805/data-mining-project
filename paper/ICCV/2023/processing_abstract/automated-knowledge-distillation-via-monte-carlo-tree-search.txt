In this paper, we present Auto-KD, the ﬁrst automated search framework for optimal knowledge distillation design.Traditional distillation techniques typically require hand-crafted designs by experts and extensive tuning costs for different teacher-student pairs. To address these issues, we empirically study different distillers, ﬁnding that they can be decomposed, combined, and simpliﬁed. Based on these observations, we build our uniform search space with ad-vanced operations in transformations, distance functions, and hyperparameters components. For instance, the trans-formation parts are optional for global, intra-spatial, and inter-spatial operations, such as attention, mask, and multi-scale. Then, we introduce an effective search strategy based on the Monte Carlo tree search, modeling the search space as a Monte Carlo Tree (MCT) to capture the dependency among options. The MCT is updated using test loss and representation gap of student trained by candidate distillers as the reward for better exploration-exploitation balance. To accelerate the search process, we exploit ofﬂine processing without teacher inference, sparse training for student, and proxy settings based on distillation properties. In this way, our Auto-KD only needs small costs to search for optimal distillers before the distillation phase. Moreover, we expandAuto-KD for multi-layer and multi-teacher scenarios with training-free weighted factors. Our method is promising yet practical, and extensive experiments demonstrate that it generalizes well to different CNNs and Vision Transformer models and attains state-of-the-art performance across a range of vision tasks, including image classiﬁcation, object detection, and semantic segmentation. Code is provided at https://github.com/lilujunai/Auto-KD. 