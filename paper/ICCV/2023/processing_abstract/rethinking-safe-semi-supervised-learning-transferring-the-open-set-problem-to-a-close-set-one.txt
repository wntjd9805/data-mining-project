Conventional semi-supervised learning (SSL) lies in the close-set assumption that the labeled and unlabeled sets contain data with the same seen classes, called in-distribution (ID) data.In contrast, safe SSL investigates a more challenging open-set problem where unlabeled set may involve some out-of-distribution (OOD) data with un-seen classes, which could harm the performance of SSL.When we are experimenting with the mainstream safe SSL methods, we have a surprising finding that all OOD data show a clear tendency to gather in the feature space. This inspires us to solve the safe SSL problem from a fresh per-spective. Specifically, for a classification task with K seen classes, we utilize a prototype network not only to generateK prototypes of all seen classes, but also explicitly model an additional prototype for the OOD data, transferring the K-way classification on the open-set to the (K+1)-way on the close-set. In this way, the typical SSL techniques (e.g., con-sistency regularization and pseudo labeling) can be applied to tackle the safe SSL problem without additional consider-ation of OOD data processing like other safe SSL methods do. Particularly, considering the possible low-confidence pseudo labels, we further propose an iterative negative learning (INL) paradigm to enforce the network learning knowledge from complementary labels on wider classes, im-proving the networkâ€™s classification performance. Extensive experiments on four benchmark datasets show that our ap-proach remarkably lifts the performance on safe SSL and outperforms the state-of-the-art methods. 