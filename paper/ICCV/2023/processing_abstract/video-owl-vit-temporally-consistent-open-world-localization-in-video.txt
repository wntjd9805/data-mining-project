We present an architecture and a training recipe that adapts pretrained open-world image models to localiza-tion in videos. Understanding the open visual world (with-out being constrained by ﬁxed label spaces) is crucial for many real-world vision tasks. Contrastive pre-training on large image-text datasets has recently led to signiﬁcant im-provements for image-level tasks. For more structured tasks involving object localization applying pre-trained models is more challenging. This is particularly true for video tasks, where task-speciﬁc data is limited. We show suc-cessful transfer of open-world models by building on theOWL-ViT open-vocabulary detection model and adapting it to video by adding a transformer decoder. The decoder propagates object representations recurrently through time by using the output tokens for one frame as the object queries for the next. Our model is end-to-end trainable on video data and enjoys improved temporal consistency com-pared to tracking-by-detection baselines, while retaining the open-world capabilities of the backbone detector. We evaluate our model on the challenging TAO-OW benchmark and demonstrate that open-world capabilities, learned from large-scale image-text pretraining, can be transferred suc-cessfully to open-world localization across diverse videos. 