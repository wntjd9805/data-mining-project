In recent years, prompt tuning has proven effective in adapting pre-trained vision-language models to down-stream tasks. These methods aim to adapt the pre-trained models by introducing learnable prompts while keeping pre-trained weights frozen. However, learnable prompts can affect the internal representation within the self-attention module, which may negatively impact performance vari-ance and generalization, especially in data-deficient set-tings. To address these issues, we propose a novel ap-proach, Read-only Prompt Optimization (RPO). RPO lever-ages masked attention to prevent the internal representa-tion shift in the pre-trained model. Further, to facilitate the optimization of RPO, the read-only prompts are ini-tialized based on special tokens of the pre-trained model.Our extensive experiments demonstrate that RPO outper-forms CLIP and CoCoOp in base-to-new generalization and domain generalization while displaying better robust-ness. Also, the proposed method achieves better generaliza-tion on extremely data-deficient settings, while improving parameter efficiency and computational overhead. Code is available at https://github.com/mlvlab/RPO. 