Sparsely activated Mixture-of-Experts (MoE) is becom-ing a promising paradigm for multi-task learning (MTL).Instead of compressing multiple tasksâ€™ knowledge into a sin-gle model, MoE separates the parameter space and only utilizes the relevant model pieces given task type and its input, which provides stabilized MTL training and ultra-efficient inference. However, current MoE approaches adopt a fixed network capacity (e.g., two experts in usual) for all tasks.It potentially results in the over-fitting of simple tasks or the under-fitting of challenging scenar-ios, especially when tasks are significantly distinctive inIn this paper, we propose an adaptive their complexity.MoE framework for multi-task vision recognition, dubbedAdaMV-MoE. Based on the training dynamics, it auto-matically determines the number of activated experts for each task, avoiding the laborious manual tuning of opti-mal model size. To validate our proposal, we benchmark it on ImageNet classification and COCO object detection& instance segmentation which are notoriously difficult to learn in concert, due to their discrepancy. Extensive ex-periments across a variety of vision transformers demon-strate a superior performance of AdaMV-MoE, compared to MTL with a shared backbone and the recent state-of-the-art (SoTA) MTL MoE approach. Codes are available on-line: https://github.com/google-research/ google-research/tree/master/moe_mtl. 