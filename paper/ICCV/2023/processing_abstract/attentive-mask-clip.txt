In vision-language modeling, image token removal is an efﬁcient augmentation technique to reduce the cost of en-coding image features. The CLIP-style models, however, have been found to be negatively impacted by this tech-nique. We hypothesize that removing a large portion of image tokens may inadvertently destroy the semantic in-formation associated to a given text description, resulting in misaligned paired data in CLIP training. To address this issue, we propose an attentive token removal approach, which retains a small number of tokens that have a strong semantic correlation to the corresponding text description.The correlation scores are dynamically evaluated through an EMA-updated vision encoder. Our method, termed at-tentive mask CLIP, outperforms original CLIP and CLIP variant with random token removal while saving the train-ing time. In addition, our approach also enables efﬁcient multi-view contrastive learning. Experimentally, by train-ing ViT-B on YFCC-15M dataset, our approach achieves 43.9% top-1 accuracy on ImageNet-1K zero-shot classi-ﬁcation, 62.7/42.1 and 38.0/23.2 I2T/T2I retrieval accu-racy on Flickr30K and MS COCO, outperforming SLIP by+1.1%, +5.5/+0.9, and +4.4/+1.3, respectively, while be-ing 2.30× faster. An efﬁcient version of our approach runs 1.16× faster than the plain CLIP model, while achieving signiﬁcant gains of +5.3%, +11.3/+8.0, and +9.5/+4.9 on these benchmarks, respectively. Code will be release in https://github.com/microsoft/A-CLIP. 