We present an image formation model and optimization procedure that combines the advantages of neural radi-ance ﬁelds and structured light imaging. Existing depth-supervised neural models rely on depth sensors to accurately capture the scene’s geometry. However, the depth maps re-covered by these sensors can be prone to error, or even fail outright. Instead of depending on the ﬁdelity of processed depth maps from a structured light system, a more princi-pled approach is to explicitly model the raw structured light images themselves. Our proposed approach enables the esti-mation of high-ﬁdelity depth maps, including for objects with complex material properties (e.g., partially-transparent sur-faces). Besides computing depth, the raw structured light im-ages also confer other useful radiometric cues, which enable predicting surface normals and decomposing scene appear-ance in terms of a direct, indirect, and ambient component.We evaluate our framework quantitatively and qualitatively on a range of real and synthetic scenes, and decompose scenes into their constituent components for novel views. 