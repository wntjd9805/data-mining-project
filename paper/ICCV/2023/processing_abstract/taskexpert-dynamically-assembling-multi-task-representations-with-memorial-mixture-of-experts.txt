Learning discriminative task-specific features simulta-neously for multiple distinct tasks is a fundamental prob-lem in multi-task learning. Recent state-of-the-art mod-els consider directly decoding task-specific features from one shared task-generic feature (e.g., feature from a back-bone layer), and utilize carefully designed decoders to pro-duce multi-task features. However, as the input feature is fully shared and each task decoder also shares decod-ing parameters for different input samples, it leads to a static feature decoding process, producing less discrimi-native task-specific representations. To tackle this limita-tion, we propose TaskExpert, a novel multi-task mixture-of-experts model that enables learning multiple representa-tive task-generic feature spaces and decoding task-specific features in a dynamic manner. Specifically, TaskExpert in-troduces a set of expert networks to decompose the back-bone feature into several representative task-generic fea-tures. Then, the task-specific features are decoded by us-ing dynamic task-specific gating networks operating on the decomposed task-generic features. Furthermore, to estab-lish long-range modeling of the task-specific representa-tions from different layers of TaskExpert, we design a multi-task feature memory that updates at each layer and acts as an additional feature expert for dynamic task-specific fea-ture decoding. Extensive experiments demonstrate that ourTaskExpert clearly outperforms previous best-performing methods on all 9 metrics of two competitive multi-task learning benchmarks for visual scene understanding (i.e.,PASCAL-Context and NYUD-v2). Codes and models will be made publicly available. 