Semi-supervised learning is attracting blooming atten-tion, due to its success in combining unlabeled data. To miti-gate potentially incorrect pseudo labels, recent frameworks mostly set a fixed confidence threshold to discard uncertain samples. This practice ensures high-quality pseudo labels, but incurs a relatively low utilization of the whole unlabeled set. In this work, our key insight is that these uncertain sam-ples can be turned into certain ones, as long as the confusion classes for the top-1 class are detected and removed. Invoked by this, we propose a novel method dubbed ShrinkMatch to learn uncertain samples. For each uncertain sample, it adap-tively seeks a shrunk class space, which merely contains the original top-1 class, as well as remaining less likely classes. Since the confusion ones are removed in this space, the re-calculated top-1 confidence can satisfy the pre-defined threshold. We then impose a consistency regularization be-tween a pair of strongly and weakly augmented samples in the shrunk space to strive for discriminative representations.Furthermore, considering the varied reliability among un-certain samples and the gradually improved model during training, we correspondingly design two reweighting princi-ples for our uncertain loss. Our method exhibits impressive performance on widely adopted benchmarks. 