Prototypical part network (ProtoPNet) methods have been designed to achieve interpretable classiﬁcation by associating predictions with a set of training prototypes, which we refer to as trivial prototypes because they are trained to lie far from the classiﬁcation boundary in the feature space. Note that it is possible to make an anal-ogy between ProtoPNet and support vector machine (SVM) given that the classiﬁcation from both methods relies on computing similarity with a set of training points (i.e., triv-ial prototypes in ProtoPNet, and support vectors in SVM).However, while trivial prototypes are located far from the classiﬁcation boundary, support vectors are located close to this boundary, and we argue that this discrepancy with the well-established SVM theory can result in ProtoPNet mod-els with inferior classiﬁcation accuracy. In this paper, we aim to improve the classiﬁcation of ProtoPNet with a new method to learn support prototypes that lie near the classi-ﬁcation boundary in the feature space, as suggested by theSVM theory. In addition, we target the improvement of clas-siﬁcation results with a new model, named ST-ProtoPNet, which exploits our support prototypes and the trivial pro-totypes to provide more effective classiﬁcation. Experi-mental results on CUB-200-2011, Stanford Cars, and Stan-ford Dogs datasets demonstrate that ST-ProtoPNet achieves state-of-the-art classiﬁcation accuracy and interpretability results. We also show that the proposed support prototypes tend to be better localised in the object of interest rather than in the background region. 