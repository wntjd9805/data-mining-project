This paper presents ER-NeRF, a novel conditional Neu-ral Radiance Fields (NeRF) based architecture for talking portrait synthesis that can concurrently achieve fast con-vergence, real-time rendering, and state-of-the-art perfor-mance with small model size. Our idea is to explicitly ex-ploit the unequal contribution of spatial regions to guide talking portrait modeling. Specifically, to improve the ac-curacy of dynamic head reconstruction, a compact and ex-pressive NeRF-based Tri-Plane Hash Representation is in-troduced by pruning empty spatial regions with three pla-nar hash encoders. For speech audio, we propose a Re-gion Attention Module to generate region-aware condition feature via an attention mechanism. Different from exist-ing methods that utilize an MLP-based encoder to learn the cross-modal relation implicitly, the attention mecha-nism builds an explicit connection between audio features and spatial regions to capture the priors of local motions.Moreover, a direct and fast Adaptive Pose Encoding is in-troduced to optimize the head-torso separation problem by mapping the complex transformation of the head pose into spatial coordinates. Extensive experiments demonstrate that our method renders better high-fidelity and audio-lips synchronized talking portrait videos, with realistic details and high efficiency compared to previous methods. Code is available at https://github.com/Fictionarry/ER-NeRF. 