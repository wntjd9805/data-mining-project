Secure multi-party computation (MPC) enables compu-tation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transform-ers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communi-cation complexity, but can be selectively replaced or lin-earized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbedMPCViT, to enable accurate yet efficient ViT inference inMPC. Based on a systematic latency and accuracy evalua-tion of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space.We also develop a simple yet effective MPC-aware neu-ral architecture search algorithm for fast Pareto optimiza-tion. To further boost the inference efficiency, we proposeMPCViT+, to jointly optimize the Softmax attention and other network components, including GeLU, matrix multi-plication, etc. With extensive experiments, we demonstrate that MPCViT achieves 1.9%, 1.3% and 3.6% higher ac-curacy with 6.2×, 2.9× and 1.9× latency reduction com-pared with baseline ViT, MPCFormer and THE-X on theTiny-ImageNet dataset, respectively. MPCViT+ further achieves a better Pareto front compared with MPCViT. The code and models for evaluation are available at https://github.com/PKU-SEC-Lab/mpcvit. 