We present OpenSeeD, a simple Open-vocabularySegmentation and Detection framework that jointly learns from different segmentation and detection datasets. To bridge the gap of vocabulary and annotation granularity, we first introduce a pre-trained text encoder to encode all the visual concepts in two tasks and learn a common se-mantic space for them. This gives us reasonably good re-sults compared with the counterparts trained on segmen-tation task only. To further reconcile them, we identify two discrepancies: i) task discrepancy – segmentation re-quires extracting masks for both foreground objects and background stuff, while detection merely cares about the former; ii) data discrepancy – box and mask annotations are with different spatial granularity, and thus not directly interchangeable. To address these issues, we propose a de-coupled decoding to reduce the interference between fore-ground/background and a conditioned mask decoding to assist in generating masks for given boxes. To this end, we develop a simple encoder-decoder model encompass-ing all three techniques and train it jointly on COCO andObjects365. After pre-training, our model exhibits com-petitive or stronger zero-shot transferability for both seg-mentation and detection. Specifically, OpenSeeD beats the state-of-the-art method for open-vocabulary instance and panoptic segmentation across 5 datasets, and outperforms previous work for open-vocabulary detection on LVIS andODinW under similar settings. When transferred to specific tasks, our model achieves new SoTA for panoptic segmenta-tion on COCO and ADE20K, and instance segmentation onADE20K and Cityscapes (The bottom row in Fig. 1 shows a comparison of the performance of OpenSeeD and pre-vious SoTA methods). Finally, we note that OpenSeeD is the first to explore the potential of joint training on seg-mentation and detection, and hope it can be received as a strong baseline for developing a single model for both tasks in the open world. Code will be released at https://github.com/IDEA-Research/OpenSeeD.∗Equal contribution. List in random.†Equal advisory contribution.This work is developed during an internship at IDEA.