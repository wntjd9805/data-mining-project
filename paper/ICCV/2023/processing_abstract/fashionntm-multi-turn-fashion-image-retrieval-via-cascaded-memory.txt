Multi-turn textual feedback-based fashion image re-trieval focuses on a real-world setting, where users can it-eratively provide information to refine retrieval results un-til they find an item that fits all their requirements. In this work, we present a novel memory-based method, calledFashionNTM, for such a multi-turn system. Our frame-work incorporates a new Cascaded Memory Neural TuringMachine (CM-NTM) approach for implicit state manage-ment, thereby learning to integrate information across all past turns to retrieve new images, for a given turn. Un-like vanilla Neural Turing Machine (NTM), our CM-NTM operates on multiple inputs, which interact with their re-spective memories via individual read and write heads, to learn complex relationships. Extensive evaluation results show that our proposed method outperforms the previous state-of-the-art algorithm by 50.5%, on Multi-turn Fash-ionIQ [60] – the only existing multi-turn fashion dataset currently, in addition to having a relative improvement of 12.6% on Multi-turn Shoes – an extension of the single-turn Shoes dataset [5] that we created in this work. Fur-ther analysis of the model in a real-world interactive set-ting demonstrates two important capabilities of our model – memory retention across turns, and agnosticity to turn order for non-contradictory feedback. Finally, user study results show that images retrieved by FashionNTM were fa-vored by 83.1% over other multi-turn models. 