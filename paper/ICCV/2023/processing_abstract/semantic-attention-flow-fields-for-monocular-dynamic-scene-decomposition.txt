From video, we reconstruct a neural volume that cap-tures time-varying color, density, scene flow, semantics, and attention information. The semantics and attention let us identify salient foreground objects separately from the back-ground across spacetime. To mitigate low resolution seman-tic and attention features, we compute pyramids that trade detail with whole-image context. After optimization, we per-form a saliency-aware clustering to decompose the scene. To evaluate real-world scenes, we annotate object masks in theNVIDIA Dynamic Scene and DyCheck datasets. We demon-strate that this method can decompose dynamic scenes in an unsupervised way with competitive performance to a super-vised method, and that it improves foreground/background segmentation over recent static/dynamic split methods.Project webpage: https://visual.cs.brown.edu/saff 