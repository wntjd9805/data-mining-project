Relational Language-Image Pre-training (RLIP) aims to align vision representations with relational texts, thereby advancing the capability of relational reasoning in com-puter vision tasks. However, hindered by the slow conver-gence of RLIPv11 architecture and the limited availability of existing scene graph data, scaling RLIPv1 is challenging.In this paper, we propose RLIPv2, a fast converging model that enables the scaling of relational pre-training to large-scale pseudo-labelled scene graph data. To enable fast scal-ing, RLIPv2 introduces Asymmetric Language-Image Fu-sion (ALIF), a mechanism that facilitates earlier and deeper gated cross-modal fusion with sparsified language encoding layers. ALIF leads to comparable or better performance than RLIPv1 in a fraction of the time for pre-training and fine-tuning. To obtain scene graph data at scale, we extend object detection datasets with free-form relation labels by introducing a captioner (e.g., BLIP) and a designed Rela-tion Tagger. The Relation Tagger assigns BLIP-generated relation texts to region pairs, thus enabling larger-scale re-lational pre-training. Through extensive experiments con-ducted on Human-Object Interaction Detection and SceneGraph Generation, RLIPv2 shows state-of-the-art perfor-mance on three benchmarks under fully-finetuning, few-shot and zero-shot settings. Notably, the largest RLIPv2 achieves 23.29mAP on HICO-DET without any fine-tuning, yields 32.22mAP with just 1% data and yields 45.09mAP with 100% data. Code and models are publicly available at https://github.com/JacobYuan7/RLIPv2. 