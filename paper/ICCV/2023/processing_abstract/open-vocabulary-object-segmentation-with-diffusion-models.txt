The goal of this paper is to extract the visual-language correspondence from a pre-trained text-to-image diffusion model, in the form of segmentation map, i.e., simultaneously generating images and segmentation masks for the corre-sponding visual entities described in the text prompt. We make the following contributions: (i) we pair the existingStable Diffusion model with a novel grounding module, that can be trained to align the visual and textual embedding space of the diffusion model with only a small number of* Both the authors have contributed equally to this project.â€  denotes corresponding author. object categories; (ii) we establish an automatic pipeline for constructing a dataset, that consists of {image, segmen-tation mask, text prompt} triplets, to train the proposed grounding module; (iii) we evaluate the performance of open-vocabulary grounding on images generated from the text-to-image diffusion model and show that the module can well segment the objects of categories beyond seen ones at training time, as shown in Fig. 1; (iv) we adopt the aug-mented diffusion model to build a synthetic semantic seg-mentation dataset, and show that, training a standard seg-mentation model on such dataset demonstrates competitive performance on the zero-shot segmentation (ZS3) bench-mark, which opens up new opportunities for adopting thepowerful diffusion model for discriminative tasks. 