Text-driven content creation has evolved to be a trans-formative technique that revolutionizes creativity. Here we study the task of text-driven human video generation, where a video sequence is synthesized from texts describing the ap-pearance and motions of a target performer. Compared to general text-driven video generation, human-centric video generation requires maintaining the appearance of synthe-In this sized human while performing complex motions. work, we present Text2Performer to generate vivid human videos with articulated motions from texts. Text2Performer has two novel designs: 1) decomposed human representa-tion and 2) diffusion-based motion sampler. First, we de-compose the VQVAE latent space into human appearance and pose representation in an unsupervised manner by uti-lizing the nature of human videos.In this way, the ap-pearance is well maintained along the generated frames.Then, we propose continuous VQ-diffuser to sample a sequence of pose embeddings. Unlike existing VQ-based methods that operate in the discrete space, continuous VQ-diffuser directly outputs the continuous pose embeddings for better motion modeling. Finally, motion-aware masking strategy is designed to mask the pose embeddings spatial-temporally to enhance the temporal coherence. Moreover, to facilitate the task of text-driven human video genera-tion, we contribute a Fashion-Text2Video dataset with man-ually annotated action labels and text descriptions. Ex-tensive experiments demonstrate that Text2Performer gen-erates high-quality human videos (up to 512 Ã— 256 res-olution) with diverse appearances and flexible motions.Our project page is https://yumingj.github.io/ projects/Text2Performer.html 