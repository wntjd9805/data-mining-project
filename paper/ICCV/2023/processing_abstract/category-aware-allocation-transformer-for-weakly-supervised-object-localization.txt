(A) CAM-based MechanismWeakly supervised object localization (WSOL) aims to localize objects based on only image-level labels as su-pervision. Recently, transformers have been introduced into WSOL, yielding impressive results. The self-attention mechanism and multilayer perceptron structure in trans-formers preserve long-range feature dependency, facilitat-ing complete localization of the full object extent. How-ever, current transformer-based methods predict bounding boxes using category-agnostic attention maps, which may lead to confused and noisy object localization. To address this issue, we propose a novel Category-aware AllocationTRansformer (CATR) that learns category-aware repre-sentations for specific objects and produces correspond-ing category-aware attention maps for object localization.First, we introduce a Category-aware Stimulation Mod-ule (CSM) to induce learnable category biases for self-attention maps, providing auxiliary supervision to guide the learning of more effective transformer representations.Second, we design an Object Constraint Module (OCM) to refine the object regions for the category-aware attention maps in a self-supervised manner. Extensive experiments on the CUB-200-2011 and ILSVRC datasets demonstrate that the proposed CATR achieves significant and consistent performance improvements over competing approaches. 