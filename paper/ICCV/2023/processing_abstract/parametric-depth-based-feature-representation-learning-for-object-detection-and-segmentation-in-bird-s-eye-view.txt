Recent vision-only perception models for autonomous driving achieved promising results by encoding multi-view image features into Bird’s-Eye-View (BEV) space. A criti-cal step and the main bottleneck of these methods is trans-forming image features into the BEV coordinate frame. This paper focuses on leveraging geometry information, such as depth, to model such feature transformation. Existing works rely on non-parametric depth distribution modeling leading to significant memory consumption, or ignore the geome-try information to address this problem.In contrast, we propose to use parametric depth distribution modeling for feature transformation. We first lift the 2D image features to the 3D space defined for the ego vehicle via a predicted parametric depth distribution for each pixel in each view.Then, we aggregate the 3D feature volume based on the 3D space occupancy derived from depth to the BEV frame. Fi-nally, we use the transformed features for downstream tasks such as object detection and semantic segmentation. Exist-ing semantic segmentation methods do also suffer from an hallucination problem as they do not take visibility infor-mation into account. This hallucination can be particularly problematic for subsequent modules such as control and planning. To mitigate the issue, our method provides depth uncertainty and reliable visibility-aware estimations. We further leverage our parametric depth modeling to present a novel visibility-aware evaluation metric that, when taken∗The work is done during an internship at NVIDIA into account, can mitigate the hallucination problem. Ex-tensive experiments on object detection and semantic seg-mentation on the nuScenes datasets demonstrate that our method outperforms existing methods on both tasks. 