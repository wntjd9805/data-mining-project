Recently, DALL-E [45], a multimodal transformer lan-guage model, and its variants including diffusion models have shown high-quality text-to-image generation capabil-ities. However, despite the realistic image generation re-sults, there has not been a detailed analysis of how to eval-uate such models. In this work, we investigate the visual reasoning capabilities and social biases of different text-to-image models, covering both multimodal transformer lan-guage models and diffusion models. First, we measure three visual reasoning skills: object recognition, object count-ing, and spatial relation understanding. For this, we pro-pose PAINTSKILLS, a compositional diagnostic evaluation dataset that measures these skills. Despite the high-fidelity image generation capability, a large gap exists between the performance of recent models and the upper bound accu-racy in object counting and spatial relation understanding skills. Second, we assess the gender and skin tone biases by measuring the gender/skin tone distribution of gener-ated images across various professions and attributes. We demonstrate that recent text-to-image generation models learn specific biases about gender and skin tone from web image-text pairs. We hope our work will help guide future progress in improving text-to-image generation models on visual reasoning skills and learning socially unbiased rep-resentations.1 