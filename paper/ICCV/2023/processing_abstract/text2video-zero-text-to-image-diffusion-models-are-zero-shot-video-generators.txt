Recent text-to-video generation approaches rely on com-putationally heavy training and require large-scale videoIn this paper, we introduce a new task, zero-datasets. shot text-to-video generation, and propose a low-cost ap-proach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (e.g.Stable Diffusion), making them suitable for the video do-main. Our key modiﬁcations include (i) enriching the la-*Equal contribution. tent codes of the generated frames with motion dynamics to keep the global scene and the background time consis-tent; and (ii) reprogramming frame-level self-attention us-ing a new cross-frame attention of each frame on the ﬁrst frame, to preserve the context, appearance, and identity of the foreground object. Experiments show that this leads to low overhead, yet high-quality and remarkably consistent video generation. Moreover, our approach is not limited to text-to-video synthesis but is also applicable to other tasks such as conditional and content-specialized video gener-ation, and Video Instruct-Pix2Pix, i.e., instruction-guidedvideo editing. As experiments show, our method performs comparably or sometimes better than recent approaches, despite not being trained on additional video data. Our code is publicly available at: https://github.com/Picsart-AI-Research/Text2Video-Zero. 