Learning novel concepts, remembering previous knowl-edge, and adapting it to future tasks occur simultaneously throughout a human’s lifetime. To model such comprehensive abilities, continual zero-shot learning (CZSL) has recently been introduced. However, most existing methods overused unseen semantic information that may not be continually accessible in realistic settings. In this paper, we address the challenge of continual zero-shot learning where unseen infor-mation is not provided during training, by leveraging genera-tive modeling. The heart of the generative-based methods is to learn quality representations from seen classes to improve the generative understanding of the unseen visual space. Mo-tivated by this, we introduce generalization-bound tools and provide the ﬁrst theoretical explanation for the beneﬁts of generative modeling to CZSL tasks. Guided by the theoret-ical analysis, we then propose our learning algorithm that employs a novel semantically guided Generative RandomWalk (GRW) loss. The GRW loss augments the training by continually encouraging the model to generate realistic and characterized samples to represent the unseen space. Our algorithm achieves state-of-the-art performance on AWA1,AWA2, CUB, and SUN datasets, surpassing existing CZSL methods by 3-7%. The code has been made available here https://github.com/wx-zhang/IGCZSL . 