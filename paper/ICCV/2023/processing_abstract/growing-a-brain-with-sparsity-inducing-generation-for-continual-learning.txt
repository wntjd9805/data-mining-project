Deep neural networks suffer from catastrophic forgetting in continual learning, where they tend to lose information about previously learned tasks when optimizing a new in-coming task. Recent strategies isolate the important param-eters for previous tasks to retain old knowledge while learn-ing the new task. However, using the fixed old knowledge might act as an obstacle to capturing novel representations.To overcome this limitation, we propose a framework that evolves the previously allocated parameters by absorbing the knowledge of the new task. The approach performs un-der two different networks. The base network learns knowl-edge of sequential tasks, and the sparsity-inducing hyper-network generates parameters for each time step for evolv-ing old knowledge. The generated parameters transform old parameters of the base network to reflect the new knowl-edge. We design the hypernetwork to generate sparse pa-rameters conditional to the task-specific information and the structural information of the base network. We evalu-ate the proposed approach on class-incremental and task-incremental learning scenarios for image classification and video action recognition tasks. Experimental results show that the proposed method consistently outperforms a large variety of continual learning approaches for those scenar-ios by evolving old knowledge. 