Recent advancements in pre-trained vision-language models, such as CLIP, have enabled the segmentation of ar-bitrary concepts solely from textual inputs, a process com-monly referred to as open-vocabulary semantic segmenta-tion (OVS). However, existing OVS techniques confront a fundamental challenge: the trained classifier tends to over-fit on the base classes observed during training, resulting in suboptimal generalization performance to unseen classes.To mitigate this issue, recent studies have proposed the use of an additional frozen pre-trained CLIP for classification.Nonetheless, this approach incurs heavy computational overheads as the CLIP vision encoder must be repeatedly forward-passed for each mask, rendering it impractical for real-world applications. To address this challenge, our ob-jective is to develop a fast OVS model that can perform com-parably or better without the extra computational burden of the CLIP image encoder during inference. To this end, we propose a core idea of preserving the generalizable repre-sentation when fine-tuning on known classes. Specifically, we introduce a text diversification strategy that generates a set of synonyms for each training category, which pre-vents the learned representation from collapsing onto spe-cific known category names. Additionally, we employ a text-guided knowledge distillation method to preserve the gener-alizable knowledge of CLIP. Extensive experiments demon-strate that our proposed model achieves robust generaliza-tion performance across various datasets. Furthermore, we perform a preliminary exploration of open-vocabulary video segmentation and present a benchmark that can facil-itate future open-vocabulary research in the video domain.*Equal contribution. Work done during internships at ByteDance. (cid:0)Corresponding author.Figure 1. Performance vs. computational cost. The radius of the circle represents the FLOPs during inference. To avoid overfitting to the seen categories, some methods [14, 44] introduce an extra frozen CLIP during inference. However, such a strategy leads to heavy computation overhead (red •). In comparison, our method generalizes well on both seen and unseen categories with much smaller computational cost (blue •). 