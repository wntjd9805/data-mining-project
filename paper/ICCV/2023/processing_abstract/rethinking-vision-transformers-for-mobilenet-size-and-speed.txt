With the success of Vision Transformers (ViTs) in com-puter vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mo-bile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorpo-rate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweightCNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast asMobileNet and maintain a similar size? We revisit the de-sign choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by optimizing latency and number of parameters simultaneously. The pro-posed models, EfficientFormerV2, achieve 3.5% higher top-1 accuracy than MobileNetV2 on ImageNet-1K with similar latency and parameters. This work demonstrate that prop-erly designed and optimized vision transformers can achieve high performance even with MobileNet-level size and speed1. 