Acquiring contact patterns between hands and nonrigid objects is a common concern in the vision and robotics community. However, existing learning-based methods fo-cus more on contact with rigid ones from monocular im-ages. When adopting them for nonrigid contact, a major problem is that the existing contact representation is re-stricted by the geometry of the object. Consequently, con-tact neighborhoods are stored in an unordered manner and contact features are difﬁcult to align with image cues. At the core of our approach lies a novel hand-object contact representation called RUPs (Region Unwrapping Proﬁles), which unwrap the roughly estimated hand-object surfaces as multiple high-resolution 2D regional proﬁles. The re-gion grouping strategy is consistent with the hand kinematic bone division because they are the primitive initiators for a composite contact pattern. Based on this representation, our Regional Unwrapping Transformer (RUFormer) learns the correlation priors across regions from monocular inputs and predicts corresponding contact and deformed transfor-mations. Our experiments demonstrate that the proposed framework can robustly estimate the deformed degrees and deformed transformations, which makes it suitable for both nonrigid and rigid contact. 