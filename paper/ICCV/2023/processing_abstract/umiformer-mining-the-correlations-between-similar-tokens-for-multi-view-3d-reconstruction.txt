In recent years, many video tasks have achieved break-throughs by utilizing the vision transformer and establish-ing spatial-temporal decoupling for feature extraction. Al-though multi-view 3D reconstruction also faces multiple images as input, it cannot immediately inherit their suc-cess due to completely ambiguous associations between unstructured views. There is not usable prior relation-ship, which is similar to the temporally-coherence prop-erty in a video. To solve this problem, we propose a novel transformer network for Unstructured Multiple Im-ages (UMIFormer). It exploits transformer blocks for de-coupled intra-view encoding and designed blocks for to-ken rectification that mine the correlation between simi-lar tokens from different views to achieve decoupled inter-view encoding. Afterward, all tokens acquired from various branches are compressed into a fixed-size compact repre-sentation while preserving rich information for reconstruc-tion by leveraging the similarities between tokens. We em-pirically demonstrate on ShapeNet and confirm that our de-coupled learning method is adaptable for unstructured mul-tiple images. Meanwhile, the experiments also verify our model outperforms existing SOTA methods by a large mar-gin. Code will be available at https://github.com/GaryZhu1996/UMIFormer. 