Cross-modal garment synthesis and manipulation will significantly benefit the way fashion designers generate gar-ments and modify their designs via flexible linguistic inter-faces. However, despite the significant progress that has been made in generic image synthesis using diffusion mod-els, producing garment images with garment part level se-mantics that are well aligned with input text prompts and then flexibly manipulating the generated results still re-mains a problem. Current approaches follow the general text-to-image paradigm and mine cross-modal relations via simple cross-attention modules, neglecting the structural correspondence between visual and textual representations in the fashion design domain. In this work, we instead intro-duce DiffCloth, a diffusion-based pipeline for cross-modal*Equal contribution. â€ Corresponding author. garment synthesis and manipulation, which empowers dif-fusion models with flexible compositionality in the fashion domain by structurally aligning the cross-modal semantics.Specifically, we formulate the part-level cross-modal align-ment as a bipartite matching problem between the linguisticAttribute-Phrases (AP) and the visual garment parts which are obtained via constituency parsing and semantic seg-mentation, respectively. To mitigate the issue of attribute confusion, we further propose a semantic-bundled cross-attention to preserve the spatial structure similarities be-tween the attention maps of attribute adjectives and part nouns in each AP. Moreover, DiffCloth allows for manipu-lation of the generated results by simply replacing APs in the text prompts. The manipulation-irrelevant regions are recognized by blended masks obtained from the bundled at-tention maps of the APs and kept unchanged. Extensive ex-periments on the CM-Fashion benchmark demonstrate thatDiffCloth both yields state-of-the-art garment synthesis re-sults by leveraging the inherent structural information and supports flexible manipulation with region consistency. 