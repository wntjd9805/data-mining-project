Semi-supervised learning (SSL) methods effectively leverage unlabeled data to improve model generalization.However, SSL models often underperform in open-set sce-narios, where unlabeled data contain outliers from novelIn this categories that do not appear in the labeled set. paper, we study the challenging and realistic open-set SSL setting, where the goal is to both correctly classify in-liers and to detect outliers.Intuitively, the inlier classi-ﬁer should be trained on inlier data only. However, weﬁnd that inlier classiﬁcation performance can be largely improved by incorporating high-conﬁdence pseudo-labeled data, regardless of whether they are inliers or outliers. Also, we propose to utilize non-linear transformations to sepa-rate the features used for inlier classiﬁcation and outlier detection in the multi-task learning framework, preventing adverse effects between them. Additionally, we introduce pseudo-negative mining, which further boosts outlier detec-tion performance. The three ingredients lead to what we call Simple but Strong Baseline (SSB) for open-set SSL. In experiments, SSB greatly improves both inlier classiﬁcation and outlier detection performance, outperforming existing methods by a large margin. Our code will be released at https://github.com/YUE-FAN/SSB. 