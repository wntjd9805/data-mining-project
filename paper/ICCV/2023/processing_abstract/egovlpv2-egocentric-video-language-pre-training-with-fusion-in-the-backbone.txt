Video-language pre-training (VLP) has become increas-ingly important due to its ability to generalize to vari-ous vision and language tasks. However, existing ego-centric VLP frameworks utilize separate video and lan-guage encoders and learn task-specific cross-modal infor-mation only during fine-tuning, limiting the development of a unified system.In this work, we introduce the sec-ond generation of egocentric video-language pre-training (EgoVLPv2), a significant improvement from the previous generation, by incorporating cross-modal fusion directly into the video and language backbones. EgoVLPv2 learns strong video-text representation during pre-training and reuses the cross-modal attention modules to support dif-ferent downstream tasks in a flexible and efficient man-ner, reducing fine-tuning costs. Moreover, our proposed fusion in the backbone strategy is more lightweight and compute-efficient than stacking additional fusion-specific layers. Extensive experiments on a wide range of VL tasks demonstrate the effectiveness of EgoVLPv2 by achieving con-sistent state-of-the-art performance over strong baselines across all downstream. Our project page can be found at https://shramanpramanick.github.io/EgoVLPv2/. 