Large-scale text-to-image diffusion models have signif-icantly improved the state of the art in generative image modeling and allow for an intuitive and powerful user inter-face to drive the image generation process. Expressing spa-tial constraints, e.g. to position specific objects in particular locations, is cumbersome using text; and current text-based image generation models are not able to accurately follow such instructions. In this paper we consider image gener-ation from text associated with segments on the image can-vas, which combines an intuitive natural language interface with precise spatial control over the generated content. We propose ZestGuide, a “zero-shot” segmentation guidance approach that can be plugged into pre-trained text-to-image diffusion models, and does not require any additional train-ing. It leverages implicit segmentation maps that can be ex-tracted from cross-attention layers, and uses them to align the generation with input masks. Our experimental results combine high image quality with accurate alignment of gen-erated content with input segmentations, and improve over prior work both quantitatively and qualitatively, including methods that require training on images with corresponding segmentations. Compared to Paint with Words, the previous state-of-the art in image generation with zero-shot segmen-tation conditioning, we improve by 5 to 10 mIoU points on the COCO dataset with similar FID scores. 