Omnidirectional images (ODIs) have become increas-ingly popular, as their large field-of-view (FoV) can offer viewers the chance to freely choose the view directions in immersive environments such as virtual reality. The M¨obius transformation is typically employed to further provide the opportunity for movement and zoom on ODIs, but apply-ing it to the image level often results in blurry effect and aliasing problem. In this paper, we propose a novel deep learning-based approach, called OmniZoomer, to incorpo-rate the M¨obius transformation into the network for move-ment and zoom on ODIs. By learning various transformed feature maps under different conditions, the network is en-hanced to handle the increasing edge curvatures, which al-leviates the blurry effect. Moreover, to address the alias-∗ Intern at ARC Lab, Tencent PCG.† Corresponding author ing problem, we propose two key components. Firstly, to compensate for the lack of pixels for describing curves, we enhance the feature maps in the high-resolution (HR) space and calculate the transformed index map with a spa-tial index generation module. Secondly, considering thatODIs are inherently represented in the spherical space, we propose a spherical resampling module that combines the index map and HR feature maps to transform the feature maps for better spherical correlation. The transformed fea-ture maps are decoded to output a zoomed ODI. Experi-ments show that our method can produce HR and high-quality ODIs with the flexibility to move and zoom in to the object of interest. Project page is available at http://vlislab22.github.io/OmniZoomer/. 