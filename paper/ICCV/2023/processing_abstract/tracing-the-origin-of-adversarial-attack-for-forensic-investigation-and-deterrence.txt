Deep neural networks are vulnerable to adversarial at-tacks. In this paper, we take the role of investigators who want to trace the attack and identify the source, that is, the particular model which the adversarial examples are gener-ated from. Techniques derived would aid forensic investiga-tion of attack incidents and serve as deterrence to potential attacks. We consider the buyers-seller setting where a ma-chine learning model is to be distributed to various buyers and each buyer receives a slightly different copy with the same functionality. A malicious buyer generates adversar-ial examples from a particular copy Mi and uses them to attack other copies. From these adversarial examples, the investigator wants to identify the source Mi. To address this problem, we propose a two-stage separate-and-trace framework. The model separation stage generates multiple copies of a model for the same classification task. This pro-cess injects unique features into each copy so that adversar-ial examples generated have distinct and traceable features.We give a parallel structure which pairs a unique tracer with the original classification model in each copy and a variational autoencoder (VAE)-based training method to achieve this goal. The tracing stage takes in adversarial ex-amples and a few candidate models, and identifies the likely source. Based on the unique features induced by the tracer, we could effectively trace the potential adversarial copy by considering the output logits from each tracer. Empirical results show that it is possible to trace the origin of the ad-versarial example and the mechanism can be applied to a wide range of architectures and datasets. 