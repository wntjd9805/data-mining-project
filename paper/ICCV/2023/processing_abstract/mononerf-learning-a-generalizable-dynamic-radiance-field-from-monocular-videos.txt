In this paper, we target at the problem of learning a generalizable dynamic radiance ﬁeld from monocular videos. Different from most existing NeRF methods that are based on multiple views, monocular videos only contain one view at each timestamp, thereby suffering from ambi-guity along the view direction in estimating point features and scene ﬂows. Previous studies such as DynNeRF dis-ambiguate point features by positional encoding, which is not transferable and severely limits the generalization abil-†: Corresponding author. ity. As a result, these methods have to train one inde-pendent model for each scene and suffer from heavy com-putational costs when applying to increasing monocular videos in real-world applications. To address this, We propose MonoNeRF to simultaneously learn point features and scene ﬂows with point trajectory and feature corre-spondence constraints across frames. More speciﬁcally, we learn an implicit velocity ﬁeld to estimate point tra-jectory from temporal features with Neural ODE, which is followed by a ﬂow-based feature aggregation module to obtain spatial features along the point trajectory. We jointly optimize temporal and spatial features in an end-to-end manner. Experiments show that our MonoNeRF is 1able to learn from multiple scenes and support new ap-plications such as scene editing, unseen frame synthesis, and fast novel scene adaptation. Codes are available at https://github.com/tianfr/MonoNeRF. 