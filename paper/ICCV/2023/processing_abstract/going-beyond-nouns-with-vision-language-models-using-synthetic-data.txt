Large-scale pre-trained Vision & Language (VL) mod-els have shown remarkable performance in many applica-tions, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost ar-bitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models.For example, their difficulty to understand Visual LanguageConcepts (VLC) that go ‘beyond nouns’ such as the mean-ing of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional rea-soning such as understanding the significance of the or-der of the words in a sentence.In this work, we investi-gate to which extent purely synthetic data could be lever-aged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We con-tribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC under-standing and compositional reasoning of VL models. Addi-tionally, we propose a general VL finetuning strategy for effectively leveraging SyViC towards achieving these im-provements. Our extensive experiments and ablations onVL-Checklist, Winoground, and ARO benchmarks demon-strate that it is possible to adapt strong pre-trained VL mod-els with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their zero-shot accuracy. 