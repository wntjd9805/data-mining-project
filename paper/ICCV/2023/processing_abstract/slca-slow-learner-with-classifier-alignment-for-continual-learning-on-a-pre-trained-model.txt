The goal of continual learning is to improve the perfor-mance of recognition models in learning sequentially ar-rived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training.However, how to adaptively exploit the pre-trained knowl-edge for each incremental task while maintaining its gen-eralizability remains an open question.In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selec-tively reducing the learning rate can almost resolve this is-sue in the representation layer, we propose a simple but ex-tremely effective approach named Slow Learner with Clas-sifier Alignment (SLCA), which further improves the clas-sification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fash-ion. Across a variety of scenarios, our proposal provides substantial improvements for CLPM (e.g., up to 49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, SplitImageNet-R, Split CUB-200 and Split Cars-196, respec-tively), and thus outperforms state-of-the-art approaches by a large margin. Based on such a strong baseline, critical factors and promising directions are analyzed in-depth to facilitate subsequent research. Code has been made avail-able at: https://github.com/GengDavid/SLCA. 