The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in var-In this paper, we re-ious object detection algorithms. veal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector’s performance, general-ization ability, and convergence speed. To this end, we pro-pose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training pro-cess into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level seman-tics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised paradigm. As depicted in Fig-ure 1, extensive experiments demonstrate that AlignDet can achieve significant improvements across diverse protocols,⋆Equal contribution. †Corresponding author. such as detection algorithm, model backbone, data setting, and training schedule. For example, AlignDet improvesFCOS by 5.3 mAP, RetinaNet by 2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs. 