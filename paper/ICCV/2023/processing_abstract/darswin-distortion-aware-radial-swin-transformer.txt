Wide-angle lenses are commonly used in perception tasks requiring a large field of view. Unfortunately, these lenses produce significant distortions making conventional models that ignore the distortion effects unable to adapt to wide-angle images. In this paper, we present a novel transformer-based model that automatically adapts to the distortion pro-duced by wide-angle lenses. We leverage the physical char-acteristics of such lenses, which are analytically defined by the radial distortion profile (assumed to be known), to de-velop a distortion aware radial swin transformer (DarSwin).In contrast to conventional transformer-based architectures,DarSwin comprises a radial patch partitioning, a distortion-based sampling technique for creating token embeddings, and an angular position encoding for radial patch merging.We validate our method on classification tasks using syntheti-cally distorted ImageNet data and show through extensive ex-periments that DarSwin can perform zero-shot adaptation to unseen distortions of different wide-angle lenses. Compared to other baselines, DarSwin achieves the best results (in terms of Top-1 accuracy) with significant gains when trained on bounded levels of distortions (very-low, low, medium, and high) and tested on all including out-of-distribution distortions. The code and models are publicly available at https://lvsn.github.io/darswin/Swin [28]DarSwin (ours)Figure 1: Illustration of (cartesian) Swin [28] (left) and our (radial) DarSwin (right) given a wide-angle image (middle).While Swin [28] computes attention on the predefined win-dows over square image patches (bottom left orange region),DarSwin performs radial transformations using distortion-aware radial patches and computes the attention on windows defined over radial patches (shown in orange region bot-tom right), which enables greater generalization capabilities across different lenses. 