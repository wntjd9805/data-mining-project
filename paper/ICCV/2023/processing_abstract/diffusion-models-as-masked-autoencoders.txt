There has been a longstanding belief that generation can facilitate a true understanding of visual data. In line with this, we revisit generatively pre-training visual repre-sentations in light of recent interest in denoising diffusion models. While directly pre-training with diffusion models does not produce strong representations, we condition diffu-sion models on masked input and formulate diffusion mod-els as masked autoencoders (DiffMAE). Our approach is capable of (i) serving as a strong initialization for down-stream recognition tasks, (ii) conducting high-quality im-age inpainting, and (iii) being effortlessly extended to video where it produces state-of-the-art classification accuracy.We further perform a comprehensive study on the pros and cons of design choices and build connections between dif-fusion models and masked autoencoders. Project page. 