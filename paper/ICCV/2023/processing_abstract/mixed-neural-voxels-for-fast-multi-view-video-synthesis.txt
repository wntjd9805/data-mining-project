Synthesizing high-ﬁdelity videos from real-world multi-view input is challenging due to the complexities of real-world environments and high-dynamic movements. Previ-ous works based on neural radiance ﬁelds have demon-strated high-quality reconstructions of dynamic scenes.However, training such models on real-world scenes is time-consuming, usually taking days or weeks.In this paper, we present a novel method named MixVoxels to efﬁciently represent dynamic scenes, enabling fast training and ren-dering speed. The proposed MixVoxels represents the 4D dynamic scenes as a mixture of static and dynamic voxels and processes them with different networks.In this way, the computation of the required modalities for static vox-els can be processed by a lightweight model, which essen-tially reduces the amount of computation as many daily dynamic scenes are dominated by static backgrounds. To distinguish the two kinds of voxels, we propose a novel variation ﬁeld to estimate the temporal variance of each voxel. For the dynamic representations, we design an in-ner product time query method to efﬁciently query multiple time steps, which is essential to recover the high-dynamic movements. As a result, with 15 minutes of training for dy-namic scenes with inputs of 300-frame videos, MixVoxels achieves better PSNR than previous methods. For render-ing, MixVoxels can render a novel view video with 1K reso-lution at 37 fps. Codes and trained models are available at https://github.com/fengres/mixvoxels. 