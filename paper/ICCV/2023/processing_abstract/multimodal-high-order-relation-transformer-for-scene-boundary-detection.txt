Scene boundary detection breaks down long videos in-to meaningful story-telling units and plays a crucial role in high-level video understanding. Despite signiﬁcant ad-vancements in this area, this task remains a challenging problem as it requires a comprehensive understanding of multimodal cues and high-level semantics. To tackle this issue, we propose a multimodal high-order relation trans-former, which integrates a high-order encoder and an adap-tive decoder in a uniﬁed framework. By modeling the mul-timodal cues and exploring similarities between the shot-s, the encoder is capable of capturing high-order relations between shots and extracting shot features with context se-mantics. By clustering the shots adaptively, the decoder can discover more universal switch pattern between successive scenes, thus helping scene boundary detection. Extensive experimental results on three standard benchmarks demon-strate that the proposed model performs favorably against state-of-the-art video scene detection methods. 