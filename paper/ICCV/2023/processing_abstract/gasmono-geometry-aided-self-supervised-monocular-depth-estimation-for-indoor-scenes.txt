This paper tackles the challenges of self-supervised monocular depth estimation in indoor scenes caused by large rotation between frames and low texture. We ease the learning process by obtaining coarse camera poses from monocular sequences through multi-view geometry to deal with the former. However, we found that limited by the scale ambiguity across different scenes in the train-ing dataset, a na¨ıve introduction of geometric coarse poses cannot play a positive role in performance improvement, which is counter-intuitive. To address this problem, we propose to reﬁne those poses during training through ro-tation and translation/scale optimization. To soften the effect of the low texture, we combine the global reason-ing of vision transformers with an overﬁtting-aware, iter-ative self-distillation mechanism, providing more accurate depth guidance coming from the network itself. Experi-ments on NYUv2, ScanNet, 7scenes, and KITTI datasets support the effectiveness of each component in our frame-work, which sets a new state-of-the-art for indoor self-supervised monocular depth estimation, as well as out-standing generalization ability. Code and models are avail-able at https://github.com/zxcqlf/GasMono 