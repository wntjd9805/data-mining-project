Predicting attention regions of interest is an impor-tant yet challenging task for self-driving systems. Existing methodologies rely on large-scale labeled traffic datasets that are labor-intensive to obtain. Besides, the huge do-main gap between natural scenes and traffic scenes in current datasets also limits the potential for model train-ing. To address these challenges, we are the first to in-troduce an unsupervised way to predict self-driving atten-tion by uncertainty modeling and driving knowledge in-tegration. Our approachâ€™s Uncertainty Mining Branch (UMB) discovers commonalities and differences from mul-tiple generated pseudo-labels achieved from models pre-trained on natural scenes by actively measuring the un-certainty. Meanwhile, our Knowledge Embedding Block (KEB) bridges the domain gap by incorporating driving knowledge to adaptively refine the generated pseudo-labels.Quantitative and qualitative results with equivalent or even more impressive performance compared to fully-supervised state-of-the-art approaches across all three public datasets demonstrate the effectiveness of the proposed method and the potential of this direction. The code is available at https://github.com/zaplm/DriverAttention.Figure 1. Illustration of the proposed unsupervised self-driving at-tention prediction model. Instead of relying on the ground truth labels provided by traffic datasets, our method only uses pseudo-labels generated from models pre-trained on natural scenes, and then refined the results by uncertainty mining and knowledge em-bedding. The red dashed line corresponds to the pre-training stage, the black dashed line refers to the training process, and the black solid line means the testing process. 