Transfer learning has been a popular learning paradigm in the deep learning era, especially in annotation-insufficient scenarios. Better ImageNet pre-trained mod-els have been demonstrated, from the perspective of archi-tecture, by previous research to have better transferability to downstream tasks[26]. However, in this paper, we find that during the same pre-training process, models at middle epochs, which are inadequately pre-trained, can outper-form fully trained models when used as feature extractors (FE), while the fine-tuning (FT) performance still grows with the source performance. This reveals that there is not a solid positive correlation between top-1 accuracy on Im-ageNet and the transferring result on target data. Based on the contradictory phenomenon between FE and FT that a better feature extractor fails to be fine-tuned better ac-cordingly, we conduct comprehensive analyses on features before the softmax layer to provide insightful explanations.Our discoveries suggest that, during pre-training, models tend to first learn spectral components corresponding to large singular values and the residual components con-tribute more when fine-tuning.Figure 1. Toy experiment of transfer learning from a ResNet18[19] model pre-trained on CIFAR10[28] to a subset of MNIST[29]. FE means viewing the pre-trained model as a feature extractor, and FT means fine-tuning the whole model. It can be seen from the figure that the 5th-epoch model brings the best FE performance, which suggests that further pre-training on the source task would harm the feature quality for the target task. When fine-tuning the whole model, more adequate pre-training tends to deliver higher transfer learning performance. 