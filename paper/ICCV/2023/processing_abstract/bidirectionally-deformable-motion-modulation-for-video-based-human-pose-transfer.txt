Video-based human pose transfer is a video-to-video generation task that animates a plain source human im-age based on a series of target human poses. Considering the difficulties in transferring highly structural patterns on the garments and discontinuous poses, existing methods of-ten generate unsatisfactory results such as distorted textures and flickering artifacts. To address these issues, we propose a novel Deformable Motion Modulation (DMM) that uti-lizes geometric kernel offset with adaptive weight modula-tion to simultaneously perform feature alignment and style transfer. Different from normal style modulation used in style transfer, the proposed modulation mechanism adap-tively reconstructs smoothed frames from style codes ac-cording to the object shape through an irregular recep-tive field of view. To enhance the spatio-temporal consis-tency, we leverage bidirectional propagation to extract the hidden motion information from a warped image sequence generated by noisy poses. The proposed feature propa-gation significantly enhances the motion prediction ability by forward and backward propagation. Both quantitative and qualitative experimental results demonstrate superior-ity over the state-of-the-arts in terms of image fidelity and visual continuity. The source code is publicly available at github.com/rocketappslab/bdmm. 