The performance of text-video retrieval has been sig-nificantly improved by vision-language cross-modal learn-ing schemes. The typical solution is to directly align the global video-level and sentence-level features during learn-ing, which would ignore the intrinsic video-text relations, i.e., a text description only corresponds to a spatio-temporal part of videos. Hence, the matching process should con-sider both fine-grained spatial content and various tempo-ral semantic events. To this end, we propose a text-video learning framework with progressive spatio-temporal pro-totype matching. Specifically, the matching process is de-composed into two complementary phases: object-phrase prototype matching and event-sentence prototype matching.In the object-phrase prototype matching phase, the spa-tial prototype generation mechanism predicts key patches or words, which are aggregated into object or phrase proto-types. Importantly, optimizing the local alignment between object-phrase prototypes helps the model perceive spatial details.In the event-sentence prototype matching phase, we design a temporal prototype generation mechanism to associate intra-frame objects and interact inter-frame tem-poral relations. Such progressively generated event proto-types can reveal semantic diversity in videos for dynamic matching. Validated by comprehensive experiments, our method consistently outperforms the state-of-the-art meth-ods on four video retrieval benchmark.1 