Multi-task learning faces two challenging issues: (1) the high cost of annotating labels for all tasks and (2) balanc-ing the training progress of various tasks with different na-tures. To resolve the label annotation issue, we construct a large-scale ”partially annotated” multi-task dataset by combining task-specific datasets. However, the numbers of annotations for individual tasks are imbalanced, which may escalate an imbalance in training progress. To bal-ance the training progress, we propose an achievement-based multi-task loss to modulate training speed based on the ”achievement,” defined as the ratio of current accu-racy to single-task accuracy. Then, we formulate the multi-task loss as a weighted geometric mean of individual task losses instead of a weighted sum to prevent any task from dominating the loss. In experiments, we evaluated the ac-curacy and training speed of the proposed multi-task loss on the large-scale multi-task dataset against recent multi-task losses. The proposed loss achieved the best multi-task accuracy without incurring training time overhead. Com-pared to single-task models, the proposed one achieved 1.28%, 1.65%, and 1.18% accuracy improvement in ob-ject detection, semantic segmentation, and depth estima-tion, respectively, while reducing computations to 33.73%.Source code is available at https://github.com/ samsung/Achievement-based-MTL. 