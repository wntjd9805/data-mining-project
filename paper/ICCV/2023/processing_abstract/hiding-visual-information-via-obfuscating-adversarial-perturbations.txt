Growing leakage and misuse of visual information raise security and privacy concerns, which promotes the de-velopment of information protection. Existing adversar-ial perturbations-based methods mainly focus on the de-identification against deep learning models. However, the inherent visual information of the data has not been well protected.In this work, inspired by the Type-I adversar-ial attack, we propose an Adversarial Visual InformationHiding (AVIH) method to protect the visual privacy of data.Specifically, the method generates obfuscating adversar-ial perturbations to obscure the visual information of the data. Meanwhile, it maintains the hidden objectives to be correctly predicted by models. In addition, our method does not modify the parameters of the applied model, which makes it flexible for different scenarios. Experimental re-sults on the recognition and classification tasks demonstrate that the proposed method can effectively hide visual infor-mation and hardly affect the performances of models. The code is available at https://github.com/suzhigangssz/AVIH. 