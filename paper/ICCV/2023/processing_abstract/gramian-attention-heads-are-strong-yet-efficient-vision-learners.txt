We introduce a novel architecture design that enhances ex-pressiveness by incorporating multiple head classifiers (i.e., classification heads) instead of relying on channel expan-sion or additional building blocks. Our approach employs attention-based aggregation, utilizing pairwise feature simi-larity to enhance multiple lightweight heads with minimal resource overhead. We compute the Gramian matrices to reinforce class tokens in an attention layer for each head.This enables the heads to learn more discriminative repre-sentations, enhancing their aggregation capabilities. Fur-thermore, we propose a learning algorithm that encourages heads to complement each other by reducing correlation for aggregation. Our models eventually surpass state-of-the-art CNNs and ViTs regarding the accuracy-throughput trade-off on ImageNet-1K and deliver remarkable perfor-mance across various downstream tasks, such as COCO object instance segmentation, ADE20k semantic segmenta-tion, and fine-grained visual classification datasets. The effectiveness of our framework is substantiated by practi-cal experimental results and further underpinned by gen-eralization error bound. We release the code publicly at: https://github.com/Lab-LVM/imagenet-models. 