The inability of DNNs to explain their black-box be-havior has led to a recent surge of explainability meth-ods. However, there are growing concerns that these ex-plainability methods are not robust and trustworthy. In this work, we perform the first robustness analysis of Neuron Ex-planation Methods under a unified pipeline and show that these explanations can be significantly corrupted by ran-dom noises and well-designed perturbations added to their probing data. We find that even adding small random noise with a standard deviation of 0.02 can already change the assigned concepts of up to 28% neurons in the deeper lay-ers. Furthermore, we devise a novel corruption algorithm and show that our algorithm can manipulate the explana-tion of more than 80% neurons by poisoning less than 10% of probing data. This raises the concern of trusting NeuronExplanation Methods in real-life safety and fairness critical applications. 