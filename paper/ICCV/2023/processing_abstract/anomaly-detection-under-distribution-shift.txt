Anomaly detection (AD) is a crucial machine learn-ing task that aims to learn patterns from a set of normal training samples to identify abnormal samples in test data.Most existing AD studiesassume that the training and test data are drawn from the same data distribution, but the test data can have large distribution shifts arising in many real-world applications due to different natural variations such as new lighting conditions, object poses, or back-ground appearances, rendering existing AD methods inef-fective in such cases. In this paper, we consider the prob-lem of anomaly detection under distribution shift and es-tablish performance benchmarks on four widely-used AD and out-of-distribution (OOD) generalization datasets. We demonstrate that simple adaptation of state-of-the-art OOD generalization methods to AD settings fails to work effec-tively due to the lack of labeled anomaly data. We further introduce a novel robust AD approach to diverse distribu-tion shifts by minimizing the distribution gap between in-distribution and OOD normal samples in both the training and inference stages in an unsupervised way. Our exten-sive empirical results on the four datasets show that our ap-proach substantially outperforms state-of-the-art AD meth-ods and OOD generalization methods on data with vari-ous distribution shifts, while maintaining the detection ac-curacy on in-distribution data. Code and data are available at https://github.com/mala-lab/ADShift. 