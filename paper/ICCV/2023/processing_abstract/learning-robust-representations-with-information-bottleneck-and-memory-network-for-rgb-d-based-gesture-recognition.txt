Although previous RGB-D-based gesture recognition methods have shown promising performance, researchers often overlook the interference of task-irrelevant cues like illumination and background. These unnecessary factors are learned together with the predictive ones by the net-work and hinder accurate recognition.In this paper, we propose a convenient and analytical framework to learn a robust feature representation that is impervious to gesture-irrelevant factors. Based on the Information Bottleneck theory, two rules of Sufficiency and Compactness are de-rived to develop a new information-theoretic loss function, which cultivates a more sufficient and compact represen-tation from the feature encoding and mitigates the impact of gesture-irrelevant information. To highlight the predic-tive information, we further integrate a memory network.Using our proposed content-based and contextual memory addressing scheme, we weaken the nuisances while pre-serving the task-relevant information, providing guidance for refining the feature representation. Experiments con-ducted on three public datasets demonstrate that our ap-proach leads to a better feature representation and achieves better performance than state-of-the-art methods. The code of our method is available at: https://github.com/Carpumpkin/InBoMem. 