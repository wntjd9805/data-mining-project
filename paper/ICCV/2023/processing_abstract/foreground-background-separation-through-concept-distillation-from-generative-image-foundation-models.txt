Curating datasets for object segmentation is a difficult task. With the advent of large-scale pre-trained genera-tive models, conditional image generation has been given a significant boost in result quality and ease of use. In this paper, we present a novel method that enables the genera-tion of general foreground-background segmentation mod-els from simple textual descriptions, without requiring seg-mentation labels. We leverage and explore pre-trained la-tent diffusion models, to automatically generate weak seg-mentation masks for concepts and objects. The masks are then used to fine-tune the diffusion model on an inpaint-ing task, which enables fine-grained removal of the object, while at the same time providing a synthetic foreground and background dataset. We demonstrate that using this method beats previous methods in both discriminative and generative performance and closes the gap with fully su-pervised training while requiring no pixel-wise object la-bels. We show results on the task of segmenting four dif-ferent objects (humans, dogs, cars, birds) and a use case scenario in medical image analysis. The code is available at https://github.com/MischaD/fobadiffusion. 