The focal point of egocentric video understanding is modelling hand-object interactions. Standard models, e.g.CNNs or Vision Transformers, which receive RGB frames as input perform well, however, their performance improves further by employing additional input modalities (e.g. ob-ject detections, optical ﬂow, audio, etc.) which provide cues complementary to the RGB modality. The added com-plexity of the modality-speciﬁc modules, on the other hand, makes these models impractical for deployment. The goal of this work is to retain the performance of such a multi-modal approach, while using only the RGB frames as input at inference time. We demonstrate that for egocentric ac-tion recognition on the Epic-Kitchens and the Something-Something datasets, students which are taught by multi-modal teachers tend to be more accurate and better cal-ibrated than architecturally equivalent models trained on ground truth labels in a unimodal or multimodal fashion.We further adopt a principled multimodal knowledge dis-tillation framework, allowing us to deal with issues which occur when applying multimodal knowledge distillation in a na¨ıve manner. Lastly, we demonstrate the achieved re-duction in computational complexity, and show that our ap-proach maintains higher performance with the reduction of the number of input views. We release our code at: https://github.com/gorjanradevski/multimodal-distillation 