We present a new method for open-vocabulary univer-sal image segmentation, which is capable of performing in-stance, semantic, and panoptic segmentation under a uni-fied framework. Our approach, called MasQCLIP, seam-lessly integrates with a pre-trained CLIP model by utilizing its dense features, thereby circumventing the need for exten-sive parameter training. MasQCLIP emphasizes two new aspects when building an image segmentation method with a CLIP model: 1) a student-teacher module to deal with masks of the novel (unseen) classes by distilling informa-tion from the base (seen) classes; 2) a fine-tuning process to update model parameters for the queries Q within theCLIP model. Thanks to these two simple and intuitive de-signs, MasQCLIP is able to achieve state-of-the-art perfor-mances with a substantial gain over the competing methods by a large margin across all three tasks, including open-vocabulary instance, semantic, and panoptic segmentation.Project page is at https://masqclip.github.io/. 