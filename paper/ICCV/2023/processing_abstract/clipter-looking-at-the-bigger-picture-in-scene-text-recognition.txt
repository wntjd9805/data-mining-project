Reading text in real-world scenarios often requires un-derstanding the context surrounding it, especially when dealing with poor-quality text. However, current scene text recognizers are unaware of the bigger picture as they op-In this study, we harness erate on cropped text images. the representative capabilities of modern vision-language models, such as CLIP, to provide scene-level information to the crop-based recognizer. We achieve this by fusing a rich representation of the entire image, obtained from the vision-language model, with the recognizer word-level features via a gated cross-attention mechanism. This component grad-ually shifts to the context-enhanced representation, allow-ing for stable fine-tuning of a pretrained recognizer. We demonstrate the effectiveness of our model-agnostic frame-work, CLIPTER (CLIP TExt Recognition), on leading text recognition architectures and achieve state-of-the-art re-sults across multiple benchmarks. Furthermore, our anal-ysis highlights improved robustness to out-of-vocabulary words and enhanced generalization in low-data regimes. 