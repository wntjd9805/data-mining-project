Transformer has recently gained considerable popu-larity in low-level vision tasks, including image super-resolution (SR). These networks utilize self-attention along different dimensions, spatial or channel, and achieve im-pressive performance. This inspires us to combine the two dimensions in Transformer for a more powerful rep-resentation capability. Based on the above idea, we pro-pose a novel Transformer model, Dual Aggregation Trans-former (DAT), for image SR. Our DAT aggregates fea-tures across spatial and channel dimensions, in the inter-block and intra-block dual manner. Specifically, we alter-nately apply spatial and channel self-attention in consec-utive Transformer blocks. The alternate strategy enablesDAT to capture the global context and realize inter-block feature aggregation. Furthermore, we propose the adaptive interaction module (AIM) and the spatial-gate feed-forward network (SGFN) to achieve intra-block feature aggregation.AIM complements two self-attention mechanisms from cor-responding dimensions. Meanwhile, SGFN introduces ad-ditional non-linear spatial information in the feed-forward network. Extensive experiments show that our DAT sur-passes current methods. Code and models are obtainable at https://github.com/zhengchen1999/DAT. 