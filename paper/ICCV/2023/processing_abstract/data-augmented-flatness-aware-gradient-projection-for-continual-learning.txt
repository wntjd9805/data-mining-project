The goal of continual learning (CL) is to continuously learn new tasks without forgetting previously learned old tasks. To alleviate catastrophic forgetting, gradient projec-tion based CL methods require that the gradient updates of new tasks are orthogonal to the subspace spanned by old tasks. This limits the learning process and leads to poor performance on the new task due to the projection constraint being too strong. In this paper, we ﬁrst revisit the gradient projection method from the perspective of ﬂatness of loss surface, and ﬁnd that unﬂatness of the loss surface leads to catastrophic forgetting of the old tasks when the projec-tion constraint is reduced to improve the performance of new tasks. Based on our ﬁndings, we propose a Data Aug-mented Flatness-aware Gradient Projection (DFGP) method to solve the problem, which consists of three modules: data and weight perturbation, ﬂatness-aware optimization, and gradient projection. Speciﬁcally, we ﬁrst perform a ﬂatness-aware perturbation on the task data and current weights toﬁnd the case that makes the task loss worst. Next, ﬂatness-aware optimization optimizes both the loss and the ﬂatness of the loss surface on raw and worst-case perturbed data to obtain a ﬂatness-aware gradient. Finally, gradient projec-tion updates the network with the ﬂatness-aware gradient along directions orthogonal to the subspace of the old tasks.Extensive experiments on four datasets show that our method improves the ﬂatness of loss surface and the performance of new tasks, and achieves state-of-the-art (SOTA) performance in the average accuracy of all tasks. 