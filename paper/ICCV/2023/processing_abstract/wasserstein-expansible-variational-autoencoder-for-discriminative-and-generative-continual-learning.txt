Task-Free Continual Learning (TFCL) represents a chal-lenging learning paradigm where a model is trained on the non-stationary data distributions without any knowledge of the task information, thus representing a more practical approach. Despite promising achievements by the Varia-tional Autoencoder (VAE) mixtures in continual learning, such methods ignore the redundancy among the probabilis-tic representations of their components when performing model expansion, leading to mixture components learning similar tasks. This paper proposes the Wasserstein Ex-pansible Variational Autoencoder (WEVAE), which evalu-ates the statistical similarity between the probabilistic rep-resentation of new data and that represented by each mix-ture component and then uses it for deciding when to ex-pand the model. Such a mechanism can avoid unneces-sary model expansion while ensuring the knowledge diver-sity among the trained components. In addition, we pro-pose an energy-based sample selection approach that as-signs high energies to novel samples and low energies to the samples which are similar to the modelâ€™s knowledge.Extensive empirical studies on both supervised and unsu-pervised benchmark tasks demonstrate that our model out-performs all competing methods. The code is available at https://github.com/dtuzi123/WEVAE/. 