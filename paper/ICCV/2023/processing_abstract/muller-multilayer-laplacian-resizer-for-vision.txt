Image resizing operation is a fundamental preprocess-ing module in modern computer vision. Throughout the deep learning revolution, researchers have overlooked the potential of alternative resizing methods beyond the commonly used resizers that are readily available, such as nearest-neighbors, bilinear, and bicubic. The key question of our interest is whether the front-end resizer affects the performance of deep vision models? In this paper, we present an extremely lightweight multilayer Laplacian resizer with only a handful of trainable parameters, dubbedMULLER resizer. MULLER has a bandpass nature in that it learns to boost details in certain frequency subbands that beneﬁt the downstream recognition models. We show that MULLER can be easily plugged into various training pipelines, and it effectively boosts the performance of the underlying vision task with little to no extra cost.Speciﬁcally, we select a state-of-the-art vision Transformer,MaxViT [50], as the baseline, and show that, if trained with MULLER, MaxViT gains up to 0.6% top-1 accu-racy, and meanwhile enjoys 36% inference cost saving to achieve similar top-1 accuracy on ImageNet-1k, as compared to the standard training scheme.Notably,MULLER’s performance also scales with model size and training data size such as ImageNet-21k and JFT, and it including is widely applicable to multiple vision tasks, image classiﬁcation, object detection and segmentation, as well as image quality assessment. The code is available https://github.com/google-research/ at google-research/tree/master/muller. 