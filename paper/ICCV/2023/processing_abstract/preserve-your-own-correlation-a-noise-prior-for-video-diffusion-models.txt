Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and tempo-rally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still chal-lenging. Also, training a video diffusion model is compu-tationally much more expensive than its image counterpart.In this work, we explore finetuning a pretrained image dif-fusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffu-sion leads to sub-optimal performance. Our carefully de-signed video noise prior leads to substantially better perfor-mance. Extensive experimental validation shows that our*Work done during an internship at NVIDIA. model, Preserve Your Own COrrelation (PYoCo), attainsSOTA zero-shot text-to-video results on the UCF-101 andMSR-VTT benchmarks. It also achieves SOTA video gener-ation quality on the small-scale UCF-101 benchmark with a 10Ã— smaller model using significantly less computation than the prior art. The project page is available at https://research.nvidia.com/labs/dir/pyoco/. 