Current unsupervised video deraining methods are in-efficient in modeling the intricate spatio-temporal proper-ties of rain, which leads to unsatisfactory results. In this paper, we propose a novel approach by integrating a bio-inspired event camera into the unsupervised video derain-ing pipeline, which enables us to capture high temporal res-olution information and model complex rain characteris-tics. Specifically, we first design an end-to-end learning-based network consisting of two modules, the asymmetric separation module and the cross-modal fusion module. The two modules are responsible for segregating the features of the rain-background layer, and for positive enhancement and negative suppression from a cross-modal perspective, respectively. Second, to regularize the network training, we elaborately design a cross-modal contrastive learning method that leverages the complementary information from event cameras, exploring the mutual exclusion and similar-ity of rain-background layers in different domains. This en-courages the deraining network to focus on the distinctive characteristics of each layer and learn a more discrimina-tive representation. Moreover, we construct the first real-world dataset comprising rainy videos and events using a hybrid imaging system. Extensive experiments demonstrate the superior performance of our method on both synthetic and real-world datasets. 