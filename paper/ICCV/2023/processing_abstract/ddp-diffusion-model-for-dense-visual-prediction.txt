We propose a simple, efficient, yet powerful framework for dense visual predictions based on the conditional dif-fusion pipeline. Our approach follows a “noise-to-map” generative paradigm for prediction by progressively remov-ing noise from a random Gaussian distribution, guided by the image. The method, called DDP, efficiently extends the denoising diffusion process into the modern perception pipeline. Without task-specific design and architecture cus-tomization, DDP is easy to generalize to most dense pre-diction tasks, e.g., semantic segmentation and depth esti-mation. In addition, DDP shows attractive properties such as dynamic inference and uncertainty awareness, in con-trast to previous single-step discriminative methods. We show top results on three representative tasks with six di-verse benchmarks, without tricks, DDP achieves state-of-the-art or competitive performance on each task compared to the specialist counterparts. For example, semantic seg-mentation (83.9 mIoU on Cityscapes), BEV map segmenta-tion (70.6 mIoU on nuScenes), and depth estimation (0.05REL on KITTI). We hope that our approach will serve as a solid baseline and facilitate future research. 