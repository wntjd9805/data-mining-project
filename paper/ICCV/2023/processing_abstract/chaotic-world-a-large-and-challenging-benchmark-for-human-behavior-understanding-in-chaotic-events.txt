Understanding and analyzing human behaviors (actions and interactions of people), voices, and sounds in chaotic events is crucial in many applications, e.g., crowd manage-ment, emergency response services. Different from human behaviors in daily life, human behaviors in chaotic events are generally different in how they behave and influence others, and hence are often much more complex. How-ever, currently there is lack of a large video dataset for analyzing human behaviors in chaotic situations. To this end, we create the first large and challenging multi-modal dataset, Chaotic World, that simultaneously provides dif-ferent levels of fine-grained and dense spatio-temporal an-notations of sounds, individual actions and group interac-tion graphs, and even text descriptions for each scene in each video, thereby enabling a thorough analysis of com-plicated behaviors in crowds and chaos. Our dataset con-sists of a total of 299,923 annotated instances for detect-ing human behaviors for Spatiotemporal Action Localiza-tion in chaotic events, 224,275 instances for identifying interactions between people for Behavior Graph Analy-sis in chaotic events, 336,390 instances for localizing rel-evant scenes of interest in long videos for Spatiotempo-ral Event Grounding, and 378,093 instances for triangu-lating the source of sound for Event Sound Source Local-ization. Given the practical complexity and challenges in chaotic events (e.g., large crowds, serious occlusions, com-plicated interaction patterns), our dataset shall be able to facilitate the community to develop, adapt, and eval-uate various types of advanced models for analyzing hu-man behaviors in chaotic events. We also design a sim-ple yet effective IntelliCare model with a Dynamic Knowl-edge Pathfinder module that intelligently learns from multi-ple tasks and can analyze various aspects of a chaotic scene in a unified architecture. This method achieves promising results in experiments. Dataset and code can be found at https://github.com/sutdcv/Chaotic-World. 