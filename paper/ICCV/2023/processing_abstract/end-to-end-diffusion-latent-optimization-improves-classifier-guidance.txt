Classifier guidance—using the gradients of an image classifier to steer the generations of a diffusion model— has the potential to dramatically expand the creative con-trol over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control.We highlight this approximation’s shortcomings and propose a novel guidance method: Direct Optimization of Diffu-sion Latents (DOODL), which enables plug-and-play guid-ance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more pre-cise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidance: using CLIP guidance to improve generations of complex prompts from DrawBench, using fine-grained visual classifiers to expand the vocabu-lary of Stable Diffusion, enabling image-conditioned gen-eration with a CLIP visual encoder, and improving image aesthetics using an aesthetic scoring network. 