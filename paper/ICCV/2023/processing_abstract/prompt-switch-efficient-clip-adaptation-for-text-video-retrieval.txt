In text-video retrieval, recent works have beneﬁted from the powerful learning capabilities of pre-trained text-image foundation models (e.g., CLIP) by adapting them to the video domain. A critical problem for them is how to ef-fectively capture the rich semantics inside the video us-ing the image encoder of CLIP. To tackle this, state-of-the-art methods adopt complex cross-modal modeling tech-niques to fuse the text information into video frame rep-resentations, which, however, incurs severe efﬁciency is-sues in large-scale retrieval systems as the video repre-sentations must be recomputed online for every text query.In this paper, we discard this problematic cross-modal fu-sion process and aim to learn semantically-enhanced rep-resentations purely from the video, so that the video rep-resentations can be computed ofﬂine and reused for differ-ent texts. Concretely, we ﬁrst introduce a spatial-temporal“Prompt Cube” into the CLIP image encoder and itera-tively switch it within the encoder layers to efﬁciently in-corporate the global video semantics into frame represen-tations. We then propose to apply an auxiliary video cap-tioning objective to train the frame representations, which facilitates the learning of detailed video semantics by pro-viding ﬁne-grained guidance in the semantic space. With a naive temporal fusion strategy (i.e., mean-pooling) on the enhanced frame representations, we obtain state-of-the-art performances on three benchmark datasets, i.e., MSR-VTT,MSVD, and LSMDC. 