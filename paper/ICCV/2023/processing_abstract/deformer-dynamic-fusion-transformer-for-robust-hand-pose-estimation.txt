Accurately estimating 3D hand pose is crucial for under-standing how humans interact with the world. Despite re-markable progress, existing methods often struggle to gen-erate plausible hand poses when the hand is heavily oc-cluded or blurred. In videos, the movements of the hand al-low us to observe various parts of the hand that may be oc-cluded or blurred in a single frame. To adaptively leverage the visual clue before and after the occlusion or blurring for robust hand pose estimation, we propose the Deformer: a framework that implicitly reasons about the relationship be-tween hand parts within the same image (spatial dimension) and different timesteps (temporal dimension). We show that a naive application of the transformer self-attention mech-anism is not sufficient because motion blur or occlusions in certain frames can lead to heavily distorted hand fea-tures and generate imprecise keys and queries. To address this challenge, we incorporate a Dynamic Fusion Module into Deformer, which predicts the deformation of the hand and warps the hand mesh predictions from nearby frames to explicitly support the current frame estimation. Further-more, we have observed that errors are unevenly distributed across different hand parts, with vertices around fingertips having disproportionately higher errors than those around the palm. We mitigate this issue by introducing a new loss function called maxMSE that automatically adjusts the weight of every vertex to focus the model on critical hand parts. Extensive experiments show that our method signifi-cantly outperforms state-of-the-art methods by 10%, and is more robust to occlusions (over 14%). 