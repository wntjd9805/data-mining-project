Vision-Language Pretraining (VLP) has shown impres-sive results on diverse downstream tasks by offline train-ing on large-scale datasets. Regarding the growing na-ture of real-world data, such an offline training paradigm on ever-expanding data is unsustainable, because models lack the continual learning ability to accumulate knowl-edge constantly. However, most continual learning studies are limited to uni-modal classification and existing multi-modal datasets cannot simulate continual non-stationary data stream scenarios. To support the study of Vision-Language Continual Pretraining (VLCP), we first con-tribute a comprehensive and unified benchmark datasetP9D which contains over one million product image-text pairs from 9 industries. The data from each industry as an independent task supports continual learning and conforms to the real-world long-tail nature to simulate pretraining on web data. We comprehensively study the characteris-tics and challenges of VLCP, and propose a new algorithm:Compatible momentum contrast with Topology Preserva-tion, dubbed CTP. The compatible momentum model ab-sorbs the knowledge of the current and previous-task mod-els to flexibly update the modal feature. Moreover, TopologyPreservation transfers the knowledge of embedding across tasks while preserving the flexibility of feature adjustment.The experimental results demonstrate our method not only achieves superior performance compared with other base-lines but also does not bring an expensive training burden.Dataset and codes are available at https://github. com/KevinLight831/CTP. 