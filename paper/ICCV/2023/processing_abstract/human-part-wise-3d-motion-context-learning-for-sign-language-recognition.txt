In this paper, we propose P3D, the human part-wise mo-tion context learning framework for sign language recogni-tion. Our main contributions lie in two dimensions: learn-ing the part-wise motion context and employing the pose ensemble to utilize 2D and 3D pose jointly. First, our empirical observation implies that part-wise context en-coding benefits the performance of sign language recogni-tion. While previous methods of sign language recognition learned motion context from the sequence of the entire pose, we argue that such methods cannot exploit part-specific mo-tion context. In order to utilize part-wise motion context, we propose the alternating combination of a part-wise encod-ing Transformer (PET) and a whole-body encoding Trans-former (WET). PET encodes the motion contexts from a part sequence, while WET merges them into a unified context. By learning part-wise motion context, our P3D achieves supe-rior performance on WLASL compared to previous state-of-the-art methods. Second, our framework is the first to ensemble 2D and 3D poses for sign language recognition.Since the 3D pose holds rich motion context and depth in-formation to distinguish the words, our P3D outperformed the previous state-of-the-art methods employing a pose en-semble.Figure 1. Overall pipeline of proposed system. We first extract expressive 3D human pose from RGB video using off-the-shelf pose estimation methods [5, 36]. The expressive 3D human pose consists of 2D pose, 3D pose and facial expression. Then our pro-posed P3D predicts the word from the expressive 3D human pose. 