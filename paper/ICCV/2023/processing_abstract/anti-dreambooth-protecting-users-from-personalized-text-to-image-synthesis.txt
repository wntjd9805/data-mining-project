Text-to-image diffusion models are nothing but a revo-lution, allowing anyone, even without design skills, to cre-ate realistic images from simple text inputs. With power-ful personalization tools like DreamBooth, they can gen-erate images of a specific person just by learning from his/her few reference images. However, when misused, such a powerful and convenient tool can produce fake news or disturbing content targeting any individual vic-tim, posing a severe negative social impact.In this pa-per, we explore a defense system called Anti-DreamBooth against such malicious use of DreamBooth. The system aims to add subtle noise perturbation to each userâ€™s im-age before publishing in order to disrupt the generation quality of any DreamBooth model trained on these per-turbed images. We investigate a wide range of algorithms for perturbation optimization and extensively evaluate them on two facial datasets over various text-to-image model versions. Despite the complicated formulation of Dream-Booth and Diffusion-based text-to-image models, our meth-ods effectively defend users from the malicious use of those models. Their effectiveness withstands even adverse con-ditions, such as model or prompt/term mismatching be-tween training and testing. Our code will be available at https://github.com/VinAIResearch/Anti-DreamBooth.git. 