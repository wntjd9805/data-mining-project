Nowadays, transformer networks have demonstrated su-perior performance in many computer vision tasks.In a multi-view 3D reconstruction algorithm following this paradigm, self-attention processing has to deal with intri-cate image tokens including massive information when fac-ing heavy amounts of view input. The curse of informa-tion content leads to the extreme difficulty of model learn-ing. To alleviate this problem, recent methods compress the token number representing each view or discard the at-tention operations between the tokens from different views.Obviously, they give a negative impact on performance.Therefore, we propose long-range grouping attention (LGA) based on the divide-and-conquer principle. Tokens from all views are grouped for separate attention operations. The tokens in each group are sampled from all views and can provide macro representation for the resided view. The richness of feature learning is guaranteed by the diversity among different groups. An effective and efficient encoder can be established which connects inter-view features us-ing LGA and extract intra-view features using the standard self-attention layer. Moreover, a novel progressive upsam-pling decoder is also designed for voxel generation with rel-atively high resolution. Hinging on the above, we construct a powerful transformer-based network, called LRGT. Ex-perimental results on ShapeNet verify our method achievesCodeSOTA accuracy in multi-view reconstruction. is available at https://github.com/LiyingCV/Long-Range-Grouping-Transformer. 