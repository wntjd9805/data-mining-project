We introduce a novel superpoint-based transformer ar-chitecture for efficient semantic segmentation of large-scale 3D scenes. Our method incorporates a fast algorithm to par-tition point clouds into a hierarchical superpoint structure, which makes our preprocessing 7 times faster than existing superpoint-based approaches. Additionally, we leverage a self-attention mechanism to capture the relationships be-tween superpoints at multiple scales, leading to state-of-the-art performance on three challenging benchmark datasets:S3DIS (76.0% mIoU 6-fold validation), KITTI-360 (63.5% on Val), and DALES (79.6%). With only 212k parameters, our approach is up to 200 times more compact than other state-of-the-art models while maintaining similar perfor-mance. Furthermore, our model can be trained on a singleGPU in 3 hours for a fold of the S3DIS dataset, which is 7× to 70× fewer GPU-hours than the best-performing meth-ods. Our code and models are accessible at github.com/ drprojects/superpoint_transformer. 