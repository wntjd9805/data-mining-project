Diffusion-based models have achieved state-of-the-art performance on text-to-image synthesis tasks. However, one critical limitation of these models is the low ﬁdelity of gen-erated images with respect to the text description, such as missing objects, mismatched attributes, and mislocated ob-jects. One key reason for such inconsistencies is the inac-curate cross-attention to text in both the spatial dimension, which controls at what pixel region an object should appear, and the temporal dimension, which controls how different levels of details are added through the denoising steps. In this paper, we propose a new text-to-image algorithm that adds explicit control over spatial-temporal cross-attention in diffusion models. We ﬁrst utilize a layout predictor to predict the pixel regions for objects mentioned in the text.We then impose spatial attention control by combining the attention over the entire text description and that over the local description of the particular object in the correspond-ing pixel region of that object. The temporal attention con-trol is further added by allowing the combination weights to change at each denoising step, and the combination weights are optimized to ensure high ﬁdelity between the image and the text. Experiments show that our method generates im-ages with higher ﬁdelity compared to diffusion-model-based baselines without ﬁne-tuning the diffusion model. Our code is publicly available.1 