Recently, semantic segmentation models trained with image-level text supervision have shown promising results in challenging open-world scenarios. However, these mod-els still face difficulties in learning fine-grained semantic alignment at the pixel level and predicting accurate ob-ject masks. To address this issue, we propose MixReorg, a novel and straightforward pre-training paradigm for se-mantic segmentation that enhances a modelâ€™s ability to re-organize patches mixed across images, exploring both lo-cal visual relevance and global semantic coherence. Our approach involves generating fine-grained patch-text pairs data by mixing image patches while preserving the corre-spondence between patches and text. The model is then trained to minimize the segmentation loss of the mixed im-ages and the two contrastive losses of the original and re-stored features. With MixReorg as a mask learner, conven-tional text-supervised semantic segmentation models can achieve highly generalizable pixel-semantic alignment abil-ity, which is crucial for open-world segmentation. After training with large-scale image-text data, MixReorg mod-els can be applied directly to segment visual objects of ar-bitrary categories, without the need for further fine-tuning.Our proposed framework demonstrates strong performance on popular zero-shot semantic segmentation benchmarks, outperforming GroupViT by significant margins of 5.0%, 6.2%, 2.5%, and 3.4% mIoU on PASCAL VOC2012, PAS-CAL Context, MS COCO, and ADE20K, respectively. 