Previous research has studied the task of segmenting cine-matic videos into scenes and into narrative acts. However, these studies have overlooked the essential task of multi-modal alignment and fusion for effectively and efficiently processing long-form videos (> 60min). In this paper, we in-troduce Multimodal alignmEnt aGgregation and distillAtion (MEGA) for cinematic long-video segmentation. MEGA tack-les the challenge by leveraging multiple media modalities.The method coarsely aligns inputs of variable lengths and different modalities with alignment positional encoding. To maintain temporal synchronization while reducing compu-tation, we further introduce an enhanced bottleneck fusion layer which uses temporal alignment. Additionally, MEGA employs a novel contrastive loss to synchronize and transfer labels across modalities, enabling act segmentation from labeled synopsis sentences on video shots. Our experimental results show that MEGA outperforms state-of-the-art meth-ods on MovieNet dataset for scene segmentation (with anAverage Precision improvement of +1.19%) and on TRI-POD dataset for act segmentation (with a Total Agreement improvement of +5.51%). 