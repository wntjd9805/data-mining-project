Video analysis tasks rely heavily on identifying the pix-els from different frames that correspond to the same visual target. To tackle this problem, recent studies have advo-cated feature learning methods that aim to learn distinc-tive representations to match the pixels, especially in a self-supervised fashion. Unfortunately, these methods have dif-ﬁculties for tiny or even single-pixel visual targets. Pixel-wise video correspondences were traditionally related to optical ﬂows, which however lead to deterministic corre-spondences and lack robustness on real-world videos. We address the problem of learning features for establishing pixel-wise correspondences. Motivated by optical ﬂows as well as the self-supervised feature learning, we propose to use not only labeled synthetic videos but also unlabeled real-world videos for learning ﬁne-grained representations in a holistic framework. We adopt an adversarial learning scheme to enhance the generalization ability of the learned features. Moreover, we design a coarse-to-ﬁne framework to pursue high computational efﬁciency. Our experimental results on a series of correspondence-based tasks demon-strate that the proposed method outperforms state-of-the-art rivals in both accuracy and efﬁciency. 