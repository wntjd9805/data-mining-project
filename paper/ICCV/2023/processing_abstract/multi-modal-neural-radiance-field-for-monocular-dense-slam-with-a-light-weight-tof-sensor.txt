Light-weight time-of-flight (ToF) depth sensors are com-pact and cost-efficient, and thus widely used on mobile de-vices for tasks such as autofocus and obstacle detection.However, due to the sparse and noisy depth measurements, these sensors have rarely been considered for dense ge-ometry reconstruction.In this work, we present the first dense SLAM system with a monocular camera and a light-weight ToF sensor. Specifically, we propose a multi-modal implicit scene representation that supports rendering both the signals from the RGB camera and light-weight ToF sen-sor which drives the optimization by comparing with the raw sensor inputs. Moreover, in order to guarantee suc-cessful pose tracking and reconstruction, we exploit a pre-dicted depth as an intermediate supervision and develop a coarse-to-fine optimization strategy for efficient learning of the implicit representation. At last, the temporal infor-mation is explicitly exploited to deal with the noisy sig-nals from light-weight ToF sensors to improve the accu-racy and robustness of the system. Experiments demon-strate that our system well exploits the signals of light-weight ToF sensors and achieves competitive results both on camera tracking and dense scene reconstruction. Project page: https://zju3dv.github.io/tof_slam/. 