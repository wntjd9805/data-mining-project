We present Text2Tex, a novel method for generating high-quality textures for 3D meshes from the given text prompts.Our method incorporates inpainting into a pre-trained depth-aware image diffusion model to progressively syn-thesize high resolution partial textures from multiple view-points. To avoid accumulating inconsistent and stretched artifacts across views, we dynamically segment the rendered view into a generation mask, which represents the gener-ation status of each visible texel. This partitioned view representation guides the depth-aware inpainting model to generate and update partial textures for the corresponding regions. Furthermore, we propose an automatic view se-quence generation scheme to determine the next best view for updating the partial texture. Extensive experiments demonstrate that our method significantly outperforms the existing text-driven approaches and GAN-based methods. 