In addition to color and textural information, geometry provides important cues for 3D scene reconstruction. How-ever, current reconstruction methods only include geometry at the feature level thus not fully exploiting the geometric information.In contrast, this paper proposes a novel geometry in-tegration mechanism for 3D scene reconstruction. Our approach incorporates 3D geometry at three levels, i.e. feature learning, feature fusion, and network supervision.First, geometry-guided feature learning encodes geomet-ric priors to contain view-dependent information. Second, a geometry-guided adaptive feature fusion is introduced which utilizes the geometric priors as a guidance to adap-tively generate weights for multiple views. Third, at the su-pervision level, taking the consistency between 2D and 3D normals into account, a consistent 3D normal loss is de-signed to add local constraints.Large-scale experiments are conducted on the Scan-Net dataset, showing that volumetric methods with our ge-ometry integration mechanism outperform state-of-the-art methods quantitatively as well as qualitatively. Volumetric methods with ours also show good generalization on the 7-Scenes and TUM RGB-D datasets. 