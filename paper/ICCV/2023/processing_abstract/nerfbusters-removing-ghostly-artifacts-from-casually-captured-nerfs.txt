Casually captured Neural Radiance Fields (NeRFs) suf-fer from artifacts such as floaters or flawed geometry when rendered outside the input camera trajectory. Existing eval-uation protocols often do not capture these effects, since they usually only assess image quality at every 8th frame of the training capture. To aid in the development and eval-uation of new methods in novel-view synthesis, we propose a new dataset and evaluation procedure, where two camera trajectories are recorded of the scene: one used for train-ing, and the other for evaluation. In this more challenging in-the-wild setting, we find that existing hand-crafted regu-larizers do not remove floaters nor improve scene geometry.Thus, we propose a 3D diffusion-based method that lever-ages local 3D priors and a novel density-based score dis-tillation sampling loss to discourage artifacts during NeRF optimization. We show that this data-driven prior removes floaters and improves scene geometry for casual captures. 