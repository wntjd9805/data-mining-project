This study addresses the issue of recovering a 3D human mesh from a single image when the human is obscured. While current methods focus on improving 2D alignment techniques, they often neglect the important aspect of 3D alignment. Additionally, these methods struggle to separate the target human from occlusion or background in crowded scenes. To overcome these challenges, a desirable approach would involve fusing 2D and 3D features and optimizing the 3D space globally. To achieve this, the paper proposes a framework called 3D Joint Contrastive Learning with Transformers (JOTR). The JOTR framework utilizes an encoder-decoder transformer architecture to fuse 2D and 3D representations and implements a novel 3D joint contrastive learning approach to provide explicit global supervision for the 3D feature space. This approach includes two contrastive losses: joint-to-joint contrast to enhance the similarity of semantically similar voxels (i.e., human joints) and joint-to-non-joint contrast to ensure discrimination from other elements (e.g., occlusions and background). The method is evaluated qualitatively and quantitatively, demonstrating superior performance compared to state-of-the-art methods on both occlusion-specific and standard benchmarks. The code for the proposed method is available at https://github.com/xljh0520/JOTR.