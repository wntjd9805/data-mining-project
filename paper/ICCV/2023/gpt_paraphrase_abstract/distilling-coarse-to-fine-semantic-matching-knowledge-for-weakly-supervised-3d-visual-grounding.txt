3D visual grounding is the process of identifying an object in a 3D scene based on a given sentence query. Existing approaches to this task require dense object-sentence pair annotations in 3D point clouds, which are time-consuming and expensive to obtain. To overcome this limitation, we propose a method that utilizes weakly supervised annotations to learn the 3D visual grounding model. Instead of relying on fine-grained annotations, we use coarse scene-sentence correspondences to establish object-sentence links. To achieve this, we introduce a novel semantic matching model that analyzes the semantic similarity between object proposals and sentences in a coarse-to-fine manner. First, we extract object proposals and select the top-K candidates based on feature and class similarity matrices. Then, we reconstruct the masked keywords of the sentence using each candidate individually, and the accuracy of the reconstruction reflects the semantic similarity of each candidate to the query. Furthermore, we distill the knowledge of coarse-to-fine semantic matching into a typical two-stage 3D visual grounding model. This not only reduces inference costs but also improves performance by leveraging the well-studied structure of existing architectures. We evaluate our proposed method on three benchmark datasets: ScanRefer, Nr3D, and Sr3D. The experimental results demonstrate the effectiveness of our approach in achieving accurate 3D visual grounding.