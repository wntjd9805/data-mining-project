Current scene text recognizers lack the ability to understand the context surrounding the text, especially in cases of poor-quality text. This study aims to address this limitation by utilizing modern vision-language models, like CLIP, to provide scene-level information to the recognizer. By combining the rich representation of the entire image from the vision-language model with the word-level features of the recognizer using a gated cross-attention mechanism, we gradually enhance the context of the recognition process. This allows for stable fine-tuning of a pretrained recognizer. Our framework, CLIPTER (CLIP Text Recognition), is model-agnostic and demonstrates superior performance compared to other text recognition architectures. It achieves state-of-the-art results on various benchmarks and exhibits improved robustness to out-of-vocabulary words and enhanced generalization in low-data scenarios.