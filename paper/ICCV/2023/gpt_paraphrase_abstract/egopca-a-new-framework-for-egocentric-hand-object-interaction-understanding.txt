This paper introduces a new framework called EgoPCA to improve Egocentric Hand-Object Interaction (Ego-HOI) recognition. Despite the availability of large-scale datasets like Ego4D and EPIC-KITCHENS, current research in Ego-HOI primarily relies on resources derived from third-person video action recognition, leading to a domain gap between first- and third-person action videos. EgoPCA addresses this gap by employing Probing, Curation, and Adaption techniques. The framework includes comprehensive pre-train sets, balanced test sets, and a new baseline, along with a training-finetuning strategy. By utilizing EgoPCA, the paper achieves state-of-the-art performance on Ego-HOI benchmarks and introduces new mechanisms and settings to facilitate further research in this field. The provided data and findings are expected to open up new avenues for Ego-HOI research. This research is supported in part by the Research Grant Council of the Hong Kong SAR under grant no. 16201420.