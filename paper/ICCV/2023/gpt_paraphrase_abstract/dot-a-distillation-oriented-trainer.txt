This paper discusses the trade-off between task and distillation losses in knowledge distillation, where knowledge is transferred from a large model to a smaller one. The authors observe that introducing distillation loss limits the convergence of task loss due to insufficient optimization of distillation loss. To address this trade-off, they propose the Distillation-Oriented Trainer (DOT), which separately considers the gradients of task and distillation losses and applies a larger momentum to distillation loss for faster optimization. Empirical evidence shows that DOT breaks the trade-off and optimizes both losses sufficiently. Extensive experiments demonstrate the superiority of DOT, with a notable +2.59% accuracy improvement on ImageNet-1k for the ResNet50-MobileNetV1 pair. Overall, DOT greatly benefits the student model's optimization properties in terms of loss convergence and model generalization. The code for DOT can be found at https://github.com/megvii-research/mdistiller.