The Vision Transformer has outperformed ConvNet in various computer tasks due to its effective global modeling capability through attention mechanisms. However, the Transformer still utilizes ConvNet feature aggregation scheme for generating hierarchical feature maps. This results in the confusion of semantic information in the grid area of images after feature aggregation, which hinders accurate modeling of global relationships by attention. To overcome this issue, we propose the Hierarchy Aware Feature Aggregation framework (HAFA). HAFA enhances the extraction of local features in shallow layers where semantic information is weak, while also aggregating patches with similar semantics in deep layers. This allows for clear semantic information in the aggregated patches, enabling the attention mechanism to accurately model global information at the semantic level. Extensive experiments demonstrate significant improvements in image classification, object detection, and semantic segmentation tasks when using the HAFA framework compared to baseline models.