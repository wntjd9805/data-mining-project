The generation of 3D human motion is crucial for the creative industry. While recent advancements in generative models with domain knowledge have improved the generation of common motions based on text input, there is still room for improvement when it comes to generating more diverse motions. To address this, we propose ReMoDiffuse, a motion generation framework that combines a diffusion model with a retrieval mechanism to enhance the generalizability and diversity of text-driven motion generation. ReMoDiffuse incorporates three key designs:   1) Hybrid Retrieval: This component finds suitable references from a database based on both semantic and kinematic similarities.   2) Semantic-Modulated Transformer: It selectively absorbs knowledge from the retrieved samples, allowing the model to adapt to the differences between the retrieved motions and the target motion sequence.   3) Condition Mixture: This component improves the utilization of the retrieval database during inference, overcoming the scale sensitivity that arises in classifier-free guidance.   Extensive experiments demonstrate that ReMoDiffuse performs better than state-of-the-art methods in terms of both text-motion consistency and motion quality, particularly for generating more diverse motions. The project page for ReMoDiffuse can be found at https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html.