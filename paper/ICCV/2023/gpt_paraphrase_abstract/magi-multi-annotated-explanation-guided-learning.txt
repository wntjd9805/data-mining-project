Explanation supervision is a method where a model is trained with human-generated explanations to enhance its predictability. However, the accuracy of the human annotations poses a challenge. To address this, using multiple annotations is a viable approach. However, utilizing multiple annotations effectively is difficult due to the noise in annotations, the lack of information about the relationship between annotations and annotators, and missing annotations. To overcome these challenges, we propose a framework called MAGI (Multi-annotated explanation-guided learning) that generates comprehensive and high-quality annotations. The framework includes a generative model that generates annotations from all annotators and uses variational inference to infer them based on the characteristics of each annotator. Additionally, an alignment mechanism is incorporated into the generative model to determine the correspondence between annotations and annotators during training. Experimental results on medical imaging datasets demonstrate the effectiveness of our framework in handling noisy annotations and achieving superior prediction performance compared to existing state-of-the-art methods.