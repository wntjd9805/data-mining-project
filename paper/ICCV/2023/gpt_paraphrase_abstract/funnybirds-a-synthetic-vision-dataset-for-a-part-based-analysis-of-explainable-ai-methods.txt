Explainable artificial intelligence (XAI) is a field that aims to understand how complex deep neural models work. However, evaluating XAI is challenging because there are no clear explanations to compare against. To address this problem, we introduce a new synthetic vision dataset called FunnyBirds and evaluation protocols. This dataset allows us to make meaningful changes to images, such as removing parts of objects. This enables us to analyze explanations at a more human-like level, rather than evaluating pixel by pixel. By comparing model outputs for inputs with removed parts, we can estimate the importance of different parts, which should be reflected in the explanations. Additionally, by mapping explanations to a common space of part importances, we can analyze various types of explanations in a unified framework. Using these tools, we assess 24 combinations of neural models and XAI methods, presenting their strengths and weaknesses in a fully automatic and systematic manner.