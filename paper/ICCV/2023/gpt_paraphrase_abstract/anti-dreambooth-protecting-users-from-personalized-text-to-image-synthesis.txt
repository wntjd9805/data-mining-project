Text-to-image diffusion models have revolutionized the creation of realistic images from simple text inputs, making it accessible to anyone, regardless of design skills. However, the misuse of these models can lead to the production of fake news and disturbing content, posing a negative social impact. To address this issue, we present a defense system called Anti-DreamBooth. This system introduces subtle noise perturbation to user images before publication, disrupting the generation quality of any DreamBooth model trained on these perturbed images. We explore various algorithms for perturbation optimization and extensively evaluate them on facial datasets and different versions of text-to-image models. Our methods effectively defend users against the malicious use of these models, even in adverse conditions such as model or prompt/term mismatching between training and testing. The code for our system is available at https://github.com/VinAIResearch/Anti-DreamBooth.git.