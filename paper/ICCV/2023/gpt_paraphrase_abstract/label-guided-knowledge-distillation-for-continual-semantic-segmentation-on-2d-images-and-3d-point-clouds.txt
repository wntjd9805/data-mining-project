Continual semantic segmentation (CSS) is a method used to expand a model's capabilities to handle new tasks while retaining its previous knowledge. Fine-tuning the model on new data alone can lead to catastrophic forgetting. Knowledge distillation (KD) is a common solution, where the output distribution of the new model is regulated to be similar to that of the old model. However, in CSS, there is a challenge known as the background shift issue. Existing KD-based CSS methods struggle with distinguishing between the background and new classes because they fail to establish a reliable class correspondence for distillation. To address this problem, we propose a new approach called label-guided knowledge distillation (LGKD). This method involves expanding and transplanting the old model's output, guided by the ground truth label, to establish a semantically appropriate class correspondence with the new model's output. As a result, valuable knowledge from the old model can be effectively distilled into the new model without causing confusion between classes. We conducted extensive experiments on two widely used CSS benchmarks, Pascal-VOC and ADE20K, and found that our LGKD significantly improves the performance of three competing methods, particularly in terms of novel mIoU, where the improvement can reach up to +76%, setting a new state-of-the-art. Furthermore, we introduced a CSS benchmark for 3D point cloud based on ScanNet and provided several re-implemented baselines for comparison. The experiments demonstrated that LGKD is versatile in both 2D and 3D modalities without requiring additional design adjustments. The source code for LGKD is available at https://github.com/Ze-Yang/LGKD.