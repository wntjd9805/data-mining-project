Training effective embodied AI agents often involves expert imitation, specialized components, or additional sensors. Another approach is to use neural architectures with self-supervised objectives for representation learning. However, the effectiveness of these objectives in encoding task-relevant information is uncertain. To address this, we propose the Scene Graph Contrastive (SGC) loss, which uses scene graphs as training signals. Unlike traditional graph decoding methods, SGC employs contrastive learning to align an agent's representation with a detailed graphical encoding of its environment. This loss is easy to implement and encourages representations that capture object semantics, relationships, and history. By utilizing the SGC loss, we achieve improvements in three embodied tasks: Object Navigation, Multi-Object Navigation, and Arm Point Navigation. Additionally, we provide studies and analyses that demonstrate the ability of our trained representation to encode semantic cues about the environment.