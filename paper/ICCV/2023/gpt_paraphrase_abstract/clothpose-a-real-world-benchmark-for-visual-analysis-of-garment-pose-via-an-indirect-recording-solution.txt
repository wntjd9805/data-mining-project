The task of visually analyzing garments for pose estimation is challenging due to the difficulty of annotating the complete configurations of garments in the real world. To address this issue, we propose a recording system called GarmentTwin that can track garment poses in dynamic settings such as manipulation. GarmentTwin collects garment models and RGB-D manipulation videos from the real world and replays the manipulation process using physics-based animation. This allows us to obtain deformed garments with poses roughly aligned with real-world observations. We then use an optimization-based approach to fit the pose with real-world observations. We validate the fitting results quantitatively and qualitatively. With GarmentTwin, we create a large-scale dataset called ClothPose, which includes 30,000 RGB-D frames from 2,400 video clips featuring 600 garments across 10 categories. We evaluate two tasks on the ClothPose dataset: non-rigid reconstruction and pose estimation. Our experiments demonstrate that existing baseline methods struggle with highly large non-rigid deformation of manipulated garments. Thus, we believe that the recording system and the dataset can support research on pose estimation tasks for non-rigid objects. We will make the datasets, models, and codes publicly available.