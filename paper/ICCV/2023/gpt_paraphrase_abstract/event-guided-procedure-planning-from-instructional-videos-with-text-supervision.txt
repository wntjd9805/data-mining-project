This study focuses on procedure planning using instructional videos with text supervision. The goal is to predict an action sequence that will transform the initial visual state into the desired visual state. The main challenge lies in the significant semantic gap between observed visual states and unobserved intermediate actions, which previous studies have overlooked. The semantic gap refers to the fact that the contents of observed visual states differ semantically from the action labels mentioned in the procedure. To address this gap, the researchers propose an event-guided paradigm. This paradigm involves inferring events from observed states and then planning actions based on both the states and predicted events. The inspiration behind this approach is the understanding that planning a procedure from an instructional video involves completing a specific event, which typically requires specific actions. To implement this paradigm, the researchers introduce the Event-guided Prompting-based Procedure Planning (E3P) model. This model incorporates event information into the sequential modeling process to support procedure planning. To account for the strong action associations within each event, the E3P model adopts a mask-and-predict approach for relation mining. This approach incorporates a probabilistic masking scheme for regularization. The effectiveness of the proposed model is demonstrated through extensive experiments on three datasets.