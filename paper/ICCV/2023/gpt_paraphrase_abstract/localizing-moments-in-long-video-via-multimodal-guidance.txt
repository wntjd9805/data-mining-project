The recent release of the large-scale, long-form MAD and Ego4D datasets has allowed researchers to study the effectiveness of current video grounding methods in the long-form scenario. However, it has been discovered that existing grounding methods struggle with this challenging task due to their inability to handle long video sequences. To address this issue, we propose a method that enhances natural language grounding in long videos by identifying and removing non-describable windows. Our approach involves a guided grounding framework consisting of a Guidance Model and a base grounding model. The Guidance Model focuses on describable windows, while the base grounding model analyzes short temporal windows to identify segments that match a given language query accurately. We present two designs for the Guidance Model: Query-Agnostic and Query-Dependent, which strike a balance between efficiency and accuracy. Experimental results demonstrate that our method outperforms state-of-the-art models by 4.1% in MAD and 4.52% in Ego4D (NLQ). Reproducing our experiments is possible using the provided code, data, and MAD's audio features, which can be found at: https://github.com/waybarrios/guidance-based-video-grounding.