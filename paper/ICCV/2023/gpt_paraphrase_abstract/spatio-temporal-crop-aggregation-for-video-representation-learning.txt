We present a new approach called SCALE (Spatio-temporal Crop Aggregation for video representation LEarning) that provides high scalability during both training and inference. Our method constructs long-range video features by learning from sets of video clip-level features obtained from a pre-trained backbone. To train the model, we introduce a self-supervised objective that involves predicting masked clip features. We incorporate sparsity into the input by randomly selecting a subset of video clips, and into the loss function by only reconstructing the sparse inputs. Additionally, we employ dimensionality reduction by operating in the latent space of a pre-trained backbone applied to individual video clips. These techniques make our method highly efficient to train and effective in transfer learning. Our experiments demonstrate that our video representation achieves state-of-the-art performance in linear, nonlinear, and k-NN probing on popular action classification and video understanding datasets.