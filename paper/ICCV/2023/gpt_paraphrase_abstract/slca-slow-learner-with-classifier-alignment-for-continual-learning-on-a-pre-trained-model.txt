Continual learning aims to enhance the performance of recognition models by learning from sequentially arrived data. While most existing approaches start from scratch, recent efforts have focused on leveraging pre-training. However, effectively utilizing pre-trained knowledge for each new task while maintaining its generalizability remains a challenge. In this study, we extensively analyze continual learning on a pre-trained model (CLPM) and identify the main challenge as progressive overfitting. We observe that selectively reducing the learning rate can mitigate this issue in the representation layer. To address the overfitting problem, we propose an effective approach called Slow Learner with Classifier Alignment (SLCA). SLCA improves the classification layer by modeling class-wise distributions and aligning the classification layers post-hoc. Our proposal achieves substantial improvements for CLPM across various scenarios, outperforming state-of-the-art approaches by a significant margin (e.g., up to 49.76%, 50.05%, 44.69%, and 40.16% on Split CIFAR-100, SplitImageNet-R, Split CUB-200, and Split Cars-196, respectively). We provide the code for our approach at https://github.com/GengDavid/SLCA. Additionally, we analyze critical factors and suggest promising directions for future research based on our strong baseline.