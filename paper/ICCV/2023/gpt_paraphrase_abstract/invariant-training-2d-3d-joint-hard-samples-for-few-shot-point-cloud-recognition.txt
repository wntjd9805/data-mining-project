We address the issue of limited data in recognizing 3D objects in few-shot point clouds by combining predictions from a traditional 3D model and a well-trained 2D model. Surprisingly, this ensemble approach, although seemingly straightforward, has not been proven effective in recent 2D-3D models. We identify the problem lies in the inadequate training for "joint hard samples", which are instances with high confidence predictions but on incorrect labels, indicating a lack of collaboration between the 2D and 3D models. To overcome this, we propose INVJOINT, an invariant training strategy that emphasizes training on the challenging samples and seeks invariance between conflicting 2D and 3D predictions. INVJOINT enables the learning of more collaborative 2D and 3D representations for improved ensemble performance. We extensively evaluate our approach on various datasets, including ModelNet10/40, ScanObjectNN, Toys4K, and ShapeNet-Core, and demonstrate the superiority of INVJOINT. The code for our method will be made publicly available. Figure 1 illustrates a comparison between our framework and existing 2D-3D methods. These methods can be categorized into three groups: (a) directly projecting point clouds into multi-view images and fine-tuning 2D models with a fixed backbone, (b) indirectly utilizing 2D pretrained knowledge as a constraint or supervision and transferring it through knowledge distillation or contrastive learning, and then using the optimized 3D pathway for prediction, and (c) our INVJOINT approach, which leverages the strengths of both 2D and 3D models through joint prediction during inference.