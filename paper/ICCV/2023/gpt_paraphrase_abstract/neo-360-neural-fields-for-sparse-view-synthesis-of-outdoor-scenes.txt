Recent advances in implicit neural representations have yielded impressive results in synthesizing novel views. However, these methods have a limitation in that they require computationally expensive per-scene optimization from multiple views. As a result, their application is restricted to bounded urban settings where only a few views of objects or backgrounds are available. To overcome this challenge, we propose a new approach called NeO 360, which utilizes neural fields for sparse view synthesis of outdoor scenes. NeO 360 is a versatile method that can reconstruct 360-degree scenes using a single or a few posed RGB images. Our approach focuses on capturing the distribution of complex real-world outdoor 3D scenes and employs a hybrid image-conditional triplanar representation that can be accessed from any point in the scene. This representation combines the strengths of voxel-based and bird's-eye-view (BEV) representations, resulting in greater effectiveness and expressiveness. The unique representation offered by NeO 360 enables learning from a vast collection of unbounded 3D scenes, while also allowing generalization to new views and novel scenes even with just a single image during inference. We evaluate our approach on a challenging 360-degree unbounded dataset called NeRDS 360 and demonstrate that NeO 360 surpasses state-of-the-art generalizable methods in novel view synthesis. Additionally, NeO 360 provides editing and composition capabilities. To learn more about our project, please visit our project page: zubair-irshad.github.io/projects/neo360.html.