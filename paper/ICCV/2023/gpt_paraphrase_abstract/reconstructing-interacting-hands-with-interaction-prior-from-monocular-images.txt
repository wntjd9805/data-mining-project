To enable augmented reality/virtual reality applications, it is crucial to reconstruct interacting hands from single images. However, existing methods that rely on accurately localizing each skeleton joint often fail due to occlusion and similarity between adjacent hand parts, which also goes against human perception as humans can imitate interaction patterns without localizing all joints. Our approach involves constructing a prior for two-hand interactions and transforming the reconstruction task into conditional sampling from the prior. To enhance the range of interaction states, we introduce a large-scale multimodal dataset with physical plausibility. Additionally, we train a VAE to condense these interaction patterns as latent codes in a prior distribution. For identifying image cues that contribute to interaction prior sampling, we propose the interaction adjacency heatmap (IAH), which assigns denser visible features to invisible joints and provides more detailed local interaction information within each interaction region compared to a joint-wise heatmap or an all-in-one visible heatmap. Finally, we establish links between the extracted features and corresponding interaction codes using the ViT module. Extensive evaluations on benchmark datasets confirm the effectiveness of our framework. The code and dataset can be accessed at https://github.com/binghui-z/InterPrior_pytorch.