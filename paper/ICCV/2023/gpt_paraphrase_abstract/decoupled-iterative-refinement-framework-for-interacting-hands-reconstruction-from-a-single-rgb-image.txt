Reconstructing interacting hands from a single RGB image is a difficult task due to occlusion and similar appearances between the hands. This often leads to misalignment of estimated hand meshes and the image. Additionally, the complex spatial relationship between interacting hands makes it challenging to accurately determine hand poses and train networks. To address these issues, we propose a decoupled iterative refinement framework that focuses on achieving pixel-aligned hand reconstruction while efficiently modeling hand spatial relationships. Our approach involves defining two feature spaces - a 2D visual feature space and a 3D joint feature space. We extract joint-wise features from the visual feature map and use a graph convolution network and a transformer to perform intra- and inter-hand information interaction in the 3D joint feature space. We then project the joint features with global information back into the 2D visual feature space and enhance them using 2D convolution. By iteratively refining the features in both spaces, our method achieves accurate and robust reconstruction of interacting hands. We evaluate our method on the InterHand2.6M dataset and demonstrate its superior performance compared to existing two-hand reconstruction methods.