Large pre-trained vision-language models (VLMs) such as CLIP have enabled the creation of zero-shot classifiers through discrete prompt design. These classifiers utilize the similarity between an image and a prompt sentence to determine the confidence score of the image belonging to a specific class. However, fine-tuning the soft prompts with limited samples or improper techniques can lead to underperformance compared to zero-shot prediction.  Existing methods address this issue using traditional anti-overfitting techniques like early stopping and data augmentation, which do not offer a principled solution specific to prompting. To overcome this limitation, we propose Prompt-aligned Gradient (ProGrad) in this paper. ProGrad prevents prompt tuning from forgetting the general knowledge acquired from VLMs. It achieves this by updating only the prompt gradients that align with or do not conflict with the general knowledge, represented by the optimization direction provided by pre-defined prompt predictions.  We conducted extensive experiments in various settings, including few-shot learning, domain generalization, base-to-new generalization, and cross-dataset transfer. The results demonstrate that ProGrad outperforms state-of-the-art prompt tuning methods in terms of few-shot generalization ability.