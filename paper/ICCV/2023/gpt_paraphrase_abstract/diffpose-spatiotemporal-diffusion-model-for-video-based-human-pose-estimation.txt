Recently, denoising diffusion probabilistic models have gained attention in computer vision for tasks such as object detection and image segmentation. However, applying these models to multi-frame human pose estimation is challenging due to the temporal dimension in videos. Additionally, accurately localizing human joints requires learning representations that focus on keypoint regions, but it is unclear how to achieve this with diffusion-based methods. To address these challenges, we propose DiffPose, a novel diffusion architecture for video-based human pose estimation. We introduce the SpatioTemporal Representation Learner, which aggregates visual evidence across frames and uses the resulting features as a condition in each denoising step. This allows for better utilization of temporal information. Furthermore, we present the Lookup-based Multi-Scale Feature Interaction mechanism, which determines the correlations between local joints and global contexts at multiple scales. This mechanism generates precise representations that prioritize keypoint regions. By extending diffusion models, DiffPose exhibits two unique characteristics in pose estimation. Firstly, it has the ability to combine multiple sets of pose estimates to improve prediction accuracy, particularly for challenging joints. Secondly, it allows for adjusting the number of iterative steps for feature refinement without the need for retraining the model. DiffPose achieves state-of-the-art results on three benchmark datasets: PoseTrack2017, PoseTrack2018, and PoseTrack21.