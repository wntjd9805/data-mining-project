We present a new method called Iterated Integrated Attributions (IIA) that aims to explain the predictions made by vision models. IIA uses iterative integration of the input image, internal representations generated by the model, and their gradients to generate precise and focused explanation maps. We conducted extensive evaluations on different tasks, datasets, and network architectures to demonstrate the effectiveness of IIA. Our results show that IIA produces accurate explanation maps and outperforms other state-of-the-art explanation techniques.