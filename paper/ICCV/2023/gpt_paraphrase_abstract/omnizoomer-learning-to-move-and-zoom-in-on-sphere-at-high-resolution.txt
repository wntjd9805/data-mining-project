This paper introduces a new deep learning-based approach called OmniZoomer for incorporating the M¨obius transformation into the network to enable movement and zoom on omnidirectional images (ODIs). The M¨obius transformation is commonly used for this purpose, but it often leads to blurriness and aliasing issues when applied at the image level. The proposed approach addresses these problems by learning transformed feature maps under different conditions, allowing the network to handle increasing edge curvatures and alleviate blurriness. To address the aliasing problem, two key components are proposed. Firstly, the feature maps are enhanced in the high-resolution space and a spatial index generation module is used to calculate the transformed index map. Secondly, a spherical resampling module is introduced to transform the feature maps using the index map and high-resolution feature maps, improving spherical correlation. The transformed feature maps are then decoded to generate a zoomed ODI. Experimental results demonstrate that the proposed method can produce high-resolution and high-quality ODIs with the ability to freely move and zoom in on objects of interest. More information about the project can be found on the project page at http://vlislab22.github.io/OmniZoomer/.