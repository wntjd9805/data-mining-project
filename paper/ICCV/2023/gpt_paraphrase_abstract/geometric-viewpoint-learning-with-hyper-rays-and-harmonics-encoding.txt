This paper presents the first deep-learning framework for the viewpoint modality, which is crucial for the interaction between observers and their surroundings. The main challenge in developing learning frameworks for viewpoints is finding a suitable multimodal representation that connects the camera viewing space and the 3D environment. Traditional approaches simplify the problem by focusing on image analysis, but this approach is computationally expensive and does not adequately capture the intrinsic geometry and environmental context of 6DoF (degrees of freedom) viewpoints. To address these issues, the authors propose two improvements. Firstly, they suggest a generalized viewpoint representation that avoids analyzing photometric pixels and instead utilizes encoded viewing ray embeddings obtained from point cloud learning frameworks. Secondly, they introduce a novel SE(3)-bijective 6D viewing ray called a hyper-ray, which overcomes the deficiency of using 5DoF viewing rays to represent 6DoF viewpoints. The authors demonstrate that their approach offers both efficiency and accuracy advantages over existing methods in real-world environments.