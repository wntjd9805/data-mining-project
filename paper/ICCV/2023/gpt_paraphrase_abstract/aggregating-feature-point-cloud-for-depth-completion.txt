Current methods for guided depth completion use iterative refinements or repetitive blocks, which limit their generalizability to different sparsity levels of input depth maps. To address this, we propose a feature point cloud aggregation framework that directly propagates 3D depth information between given and missing points. We extract 2D feature maps from images and convert sparse depth maps to point clouds to extract sparse 3D features. By treating these features as two sets of feature point clouds, we can reconstruct depth information for a target location by aggregating adjacent sparse 3D features from known points using cross attention. Our neural network, PointDC, implements this process and achieves superior or competitive results on the KITTI benchmark and NYUv2 dataset. PointDC also demonstrates higher generalizability to different sparsity levels of input depth maps and cross-dataset evaluation.