Reducing communication overhead in privacy-preserving machine learning is a challenge for federated learning (FL) at scale. While existing methods can reduce communication overhead through sparsification or other techniques, they often compromise the convergence rate. To address this, we propose a novel method called Single-Step Synthetic Features Compressor (3SFC). This method constructs a small synthetic dataset with synthetic features based on raw gradients, achieving a very low compression rate even with just one data sample. The compressing phase of 3SFC utilizes a similarity-based objective function, allowing it to be optimized in just one step and improving its performance and robustness. We also incorporate error feedback (EF) into 3SFC to minimize compressing errors. Experimental results on various datasets and models demonstrate that 3SFC outperforms competing methods in terms of convergence rates while maintaining lower compression rates (up to 0.02%). Ablation studies and visualizations further validate the effectiveness of 3SFC, showing that it carries more information than other methods in each communication round.