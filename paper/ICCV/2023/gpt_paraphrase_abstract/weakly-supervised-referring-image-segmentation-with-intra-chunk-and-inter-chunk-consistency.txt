This study proposes a weakly supervised learning method for referring image segmentation, which aims to locate objects in images based on natural language expressions. Unlike previous approaches that rely on expensive segmentation labels, this method only requires readily available image-text pairs. The authors first train a visual-linguistic model for image-text matching and use Grad-CAM to generate a visual saliency map that identifies regions corresponding to each word. However, they identify two major issues with Grad-CAM. Firstly, it fails to consider critical semantic relationships between words. To address this, the authors introduce intra-chunk and inter-chunk consistency to model word relationships. Secondly, Grad-CAM only identifies small regions of the referred object, resulting in low recall. To overcome this limitation, the authors refine the localization maps using self-attention in Transformer and unsupervised object shape prior. Experimental results on three benchmarks demonstrate that their approach outperforms comparable techniques in terms of performance. Additionally, the authors show that their method is applicable to different levels of supervision and achieves better results than recent methods.