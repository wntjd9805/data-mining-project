We present a novel approach to enhance the performance of spatio-temporal representations in ego-centric videos. Our method focuses on improving object-awareness during training by training the model to predict hand positions, object positions, and semantic labels using paired captions. During inference, the model only requires RGB frames as input and can track and ground objects, even though it was not explicitly trained for this task. We evaluate the performance of the object-aware representations learned by our model through zero-shot testing on various video-text retrieval and classification benchmarks, as well as using them for long-term video understanding tasks. In all cases, our model outperforms the state of the art, including networks trained with larger batch sizes. Additionally, we demonstrate that by using noisy image-level detection as pseudo-labels in training, our model learns to provide better bounding boxes and ground the words in the associated text descriptions. Overall, our model can serve as a drop-in replacement for an ego-centric video model and improve performance through visual-text grounding.