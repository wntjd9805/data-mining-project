This study focuses on anticipating future steps in procedural videos. The main challenge is not only the scarcity of data in procedural video datasets but also accounting for multiple plausible future outcomes in natural settings, which has been overlooked in previous research. To address this challenge, the researchers propose a generative model that takes video clips as input and generates multiple diverse and plausible next step candidates in natural language. The model is pretrained on a text-based corpus of procedural activities and then transferred to the video domain. Experimental results demonstrate that the model accurately predicts diverse future steps and outperforms existing baselines in next step anticipation on the YouCookII dataset. Additionally, the model shows the ability to transfer from text to video without fine-tuning or adaptation, producing high-quality future step predictions.