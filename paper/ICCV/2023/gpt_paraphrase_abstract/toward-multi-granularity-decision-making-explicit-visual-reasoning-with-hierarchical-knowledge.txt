Current visual question answering (VQA) models lack attention to knowledge and do not consider the granularity of knowledge. They also struggle to model knowledge of multiple granularity and are susceptible to biased data. To address these issues, this study introduces a Hierarchical Concept Graph (HCG) that connects multi-granularity concepts with a hierarchical structure. This allows for the alignment of visual observations and knowledge at different levels, reducing data biases. Additionally, the study proposes an interpretable Hierarchical Concept Neural Module Network (HCNMN) to better understand how knowledge contributes to the decision-making process. HCNMN propagates multi-granularity knowledge across the hierarchical structure and integrates it with a sequence of reasoning steps, providing a transparent interface for the integration of observations and knowledge. The effectiveness of the proposed method is demonstrated through experiments on challenging datasets (GQA, VQA, FVQA, OK-VQA). The code for the method is publicly available at https://github.com/SuperJohnZhang/HCNMN.