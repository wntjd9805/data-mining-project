This study addresses the challenge of recovering 3D scene structure from monocular depth estimation. Traditional methods use labeled datasets to predict absolute depth, but recent advancements suggest training models with mixed datasets to improve generalization. However, this approach only provides depth predictions up to an unknown scale and shift, making accurate 3D reconstructions difficult. Existing solutions require additional 3D datasets or depth annotations, which limit their versatility. To overcome these limitations, this paper proposes a learning framework that trains models to predict geometry-preserving depth without extra data or annotations. To generate realistic 3D structures, the framework renders new views of the reconstructed scenes and incorporates loss functions that encourage consistency in depth estimation across different views. Extensive experiments demonstrate the framework's superior generalization capabilities, surpassing existing state-of-the-art methods on various benchmark datasets without relying on additional training information. Furthermore, the framework's innovative loss functions enable the model to autonomously recover domain-specific scale-and-shift coefficients using only unlabeled images.