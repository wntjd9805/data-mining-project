The quantization scale and bit-width are crucial factors to consider when quantizing a neural network. Previous research has focused on optimizing quantization scales globally using gradient methods, but we have observed that small perturbations in the quantization scale can have a significant impact on accuracy. In particular, a 0.5-0.8% accuracy boost can be achieved in 4-bit quantized vision transformers. However, gradient methods struggle in this scenario due to the non-smooth nature of the test loss landscape. To address this, we propose a new approach called Evol-Q, which utilizes evolutionary search to effectively navigate the non-smooth landscape. We also introduce an infoNCE loss that helps combat overfitting on a small calibration dataset and makes traversal of the non-smooth surface easier. Our experiments demonstrate that Evol-Q improves the top-1 accuracy of a fully quantized ViT-Base by 10.30%, 0.78%, and 0.15% for 3-bit, 4-bit, and 8-bit weight quantization levels. Furthermore, extensive testing on various CNN and ViT architectures shows the robustness of Evol-Q in extreme quantization scenarios. The code for Evol-Q can be found at https://github.com/enyac-group/evol-q.