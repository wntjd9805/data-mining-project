Vision Transformers (ViTs) have proven to be highly effective in various visual tasks due to their strong ability to learn from large amounts of data. However, it has been unexpectedly observed that ViTs perform poorly when applied to face recognition (FR) scenarios involving extremely large datasets. This paper aims to investigate the reasons behind this phenomenon and identifies two key factors: the existing data augmentation approach and hard sample mining strategy are not suitable for ViTs-based FR models as they fail to consider the preservation of face structural information and the utilization of local token information. To address these issues, this study introduces a new and improved FR model called TransFace. TransFace incorporates a patch-level data augmentation strategy called DPAP, which introduces random perturbations to the dominant patches, effectively increasing sample diversity and mitigating overfitting in ViTs. Additionally, TransFace utilizes a hard sample mining strategy named EHSM that dynamically adjusts the importance weight of easy and hard samples during training based on the information entropy of the local tokens. This approach leads to more stable predictions. Experimental results on various benchmarks demonstrate the superior performance of TransFace. The code and models for TransFace are publicly available at https://github.com/DanJun6737/TransFace.