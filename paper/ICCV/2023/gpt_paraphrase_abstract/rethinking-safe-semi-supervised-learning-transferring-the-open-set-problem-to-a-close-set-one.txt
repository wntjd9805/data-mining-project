Conventional semi-supervised learning (SSL) assumes that labeled and unlabeled data have the same classes. In contrast, safe SSL deals with the challenge of unlabeled data containing unseen classes. Surprisingly, we found that out-of-distribution (OOD) data tends to cluster in the feature space. This led us to propose a fresh perspective on solving the safe SSL problem. Our approach involves using a prototype network to generate prototypes for seen classes and an additional prototype for OOD data. This transforms the open-set classification problem into a close-set problem. We can then apply SSL techniques without the need for separate processing of OOD data. Additionally, we introduce an iterative negative learning paradigm to improve the network's classification performance by leveraging low-confidence pseudo labels and complementary knowledge from wider classes. Our experiments on benchmark datasets demonstrate that our approach significantly improves safe SSL performance and outperforms existing methods.