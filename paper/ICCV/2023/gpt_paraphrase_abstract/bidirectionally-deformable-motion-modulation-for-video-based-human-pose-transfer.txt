Video-based human pose transfer is a challenging task in which a plain source human image is animated based on a series of target human poses. Existing methods often produce unsatisfactory results due to difficulties in transferring structural patterns on garments and handling discontinuous poses. In this study, we propose a new approach called Deformable Motion Modulation (DMM) that addresses these issues. DMM utilizes geometric kernel offset with adaptive weight modulation to align features and transfer styles simultaneously. Unlike traditional style modulation, our method reconstructs smoothed frames based on style codes and object shape using an irregular receptive field of view. To improve spatio-temporal consistency, we employ bidirectional propagation to extract hidden motion information from a sequence of warped images generated by noisy poses. This feature propagation greatly enhances motion prediction by propagating information both forward and backward. Our experimental results, both quantitative and qualitative, demonstrate that our approach outperforms existing methods in terms of image fidelity and visual continuity. The source code for our method is publicly available on GitHub.