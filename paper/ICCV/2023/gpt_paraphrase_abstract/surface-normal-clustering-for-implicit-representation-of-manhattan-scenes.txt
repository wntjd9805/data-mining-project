We investigate the use of implicit neural field representation for novel view synthesis and 3D modeling with calibrated multi-view cameras. While these representations can benefit from additional geometric and semantic supervision, existing methods often require dense pixel-wise labels or localized scene priors, making it difficult to leverage high-level scene descriptions. In this study, we focus on leveraging the geometric prior of Manhattan scenes to enhance implicit neural radiance field representations. Specifically, we assume knowledge of the indoor scene being Manhattan, without any additional information or a known Manhattan coordinate frame. This high-level prior is used to self-supervise the surface normals derived from the implicit neural fields. Our approach involves clustering the derived normals and leveraging their orthogonality constraints for self-supervision. Extensive experiments on diverse indoor scene datasets demonstrate the significant benefits of our proposed method compared to established baselines. The source code for our method is available at https://github.com/nikola3794/normal-clustering-nerf.