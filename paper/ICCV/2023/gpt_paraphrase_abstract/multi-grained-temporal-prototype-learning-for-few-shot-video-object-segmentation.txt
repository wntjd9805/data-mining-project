This study focuses on Few-Shot Video Object Segmentation (FSVOS), which involves segmenting objects in a query video using only a few annotated support images of the same category. This area of research has received little attention so far. The authors propose a method that builds upon IPMT, a leading few-shot image segmentation technique that combines external support guidance with adaptive query guidance cues. In order to address the temporal correlation present in video data, the authors introduce multi-grained temporal guidance information. This involves breaking down the query video into a clip prototype and a memory prototype, which capture local and long-term internal temporal guidance respectively. Frame prototypes are then utilized for each frame independently to provide fine-grained adaptive guidance and enable bidirectional communication between clip and frame prototypes. To mitigate the impact of noisy memory, the authors leverage the structural similarity relation between different predicted regions and the support, which helps in selecting reliable memory frames. Additionally, a new segmentation loss function is proposed to improve the category discriminability of the learned prototypes. Experimental results demonstrate that the proposed video IPMT model outperforms previous models on two benchmark datasets. The code for the model is available at https://github.com/nankepan/VIPMT.