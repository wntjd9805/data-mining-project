Deep neural networks (DNNs) are susceptible to back-door attacks, where the network's behavior can be manipulated by adding a trigger pattern without affecting its performance on clean data. Existing defense methods have reduced the success rate of these attacks but still suffer from decreased prediction accuracy on clean data compared to a clean model. To address this, we propose a simple yet highly effective defense framework that injects non-adversarial backdoors specifically targeting poisoned samples.  Our defense framework follows the steps of a backdoor attack by first identifying a small set of suspected samples and then applying a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data while having limited impact on clean data. This defense can be integrated into the data preprocessing stage without requiring modifications to the standard end-to-end training pipeline.  We conducted extensive experiments on various benchmarks using different architectures and representative attacks. The results demonstrate that our method achieves state-of-the-art defense effectiveness with the lowest performance drop on clean data. Given the surprising defense ability of our framework, we emphasize the need for more attention to be given to utilizing backdoors for backdoor defense.  For more information and access to the code, please visit our GitHub repository at https://github.com/damianliumin/non-adversarial backdoor.