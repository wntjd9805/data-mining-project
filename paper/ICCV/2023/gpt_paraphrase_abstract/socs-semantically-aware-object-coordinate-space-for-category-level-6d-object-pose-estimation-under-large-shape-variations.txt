Learning-based methods for estimating the 6D pose of objects at the category level often use normalized object coordinate space (NOCS). However, these NOCS-based methods struggle with objects that have significant shape variations within a category. This is because the coordinates generated by globally aligning objects are semantically inconsistent, which makes it difficult for the model to learn and generalize coordinate regression. To address this issue, we propose a new approach called Semantically-aware Object Coordinate Space (SOCS). SOCS is constructed by warping and aligning objects using a sparse set of keypoints that have semantically meaningful correspondence. Unlike NOCS, SOCS ensures semantic coherence, meaning that any point on an object's surface can be mapped to a meaningful location in SOCS. This allows for more accurate estimation of pose and size, even when dealing with large shape variations. To effectively regress coordinates in SOCS, we introduce a novel multi-scale coordinate-based attention network. Our method is easy to train, performs well in handling intra-category shape variations, and is robust to occlusions between objects. The code for our method is available at: https://github.com/wanboyan/SOCS.