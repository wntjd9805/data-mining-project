Recently, self-supervised learning has gained significant attention as a method for learning powerful representations without manual annotations. However, in order to address the challenges of real-world scenarios, such as continual learning, self-supervised learning needs to continuously adapt and avoid catastrophic forgetting, where the model forgets previously learned knowledge. Traditional approaches like rehearsal or regularization can have negative effects, such as overfitting or hindering the model's ability to encode new information. To overcome these issues, we propose Augmentation Stability Rehearsal (ASR), a method that selects the most representative and discriminative samples by estimating the stability of augmented data for rehearsal. Additionally, we introduce a matching strategy to dynamically update the rehearsal buffer in ASR. Furthermore, we propose Contrastive Continuity on Augmentation Stability Rehearsal (C2ASR) based on ASR. We demonstrate that C2ASR is an upper bound of the Information Bottleneck (IB) principle, preserving as much shared information among seen task streams to prevent catastrophic forgetting while dismissing redundant information between previous and current task streams to facilitate the encoding of fresh information. Our method achieves significant improvements compared to state-of-the-art continual self-supervised learning methods on various benchmarks.