Significant progress has been made in enhancing the ability of models to handle natural distribution shifts through recent pre-training strategies and improved fine-tuning methods. However, these methods rely on access to large amounts of labeled data, and it is unclear how effective they are when the amount of training data is limited. To address this gap, we conducted a comprehensive study on the robustness of models to various natural distribution shifts under low-shot conditions. Our study encompassed different datasets, architectures, pre-trained initializations, and state-of-the-art interventions for improving robustness. Importantly, we discovered that no single model consistently outperforms others in terms of robustness, and some existing interventions fail to improve robustness in certain datasets under low-shot conditions, despite being effective in the full-shot regime. We hope that our findings will encourage the research community to prioritize this practically important problem. Our code and low-shot subsets are publicly accessible at the provided URL.