Current shadow detection datasets often have missing or incorrect shadow labels, which can negatively impact the performance of deep learning models trained on such data. To address this problem, we propose SILT, the Shadow-aware It-erative Label Tuning framework. SILT takes into account the noise in shadow labels and trains the deep model in a self-training manner. We incorporate strong data augmentations and shadow counterfeiting to help the network better identify non-shadow areas and reduce overfitting. Additionally, we develop a simple yet effective label tuning strategy that combines global-local fusion and shadow-aware filtering to encourage the network to make significant improvements on the noisy labels. We evaluate SILT by relabeling the test set of the SBU dataset and conducting various experiments. Our results demonstrate that even a basic U-Net trained with SILT outperforms all existing methods by a significant margin. When trained on SBU, UCF, or ISTD, our network successfully reduces the Balanced Error Rate by 25.2%, 36.9%, and 21.3% respectively compared to the best state-of-the-art method.