Current diffusion-based image generators have made significant progress in producing high-quality and diverse samples. However, this success has not yet fully extended to 3D generation. Existing diffusion methods for 3D generation suffer from limitations such as generating low-resolution outputs that are consistent in 3D, or producing detailed 2D views of 3D objects with potential structural flaws and lacking realism or view consistency.To address these challenges, we present a novel method called HOLOFUSION. Our approach combines the strengths of existing methods to generate high-fidelity, plausible, and diverse 3D samples using only a collection of multi-view 2D images as input. The process begins by generating coarse 3D samples using a modified version of the HoloDiffusion generator. Subsequently, a large number of views of the coarse 3D model are independently rendered and upsampled to enhance detail. These views are then distilled into a single, high-fidelity implicit 3D representation that ensures view-consistency of the final renders.The super-resolution network, which adds detail to the rendered views, is trained as an integral part of HOLOFUSION in an end-to-end manner. Additionally, a new sampling scheme is introduced for the final distillation, capturing the space of super-resolved signals. To evaluate the performance of our method, we compare it against several existing baselines including DreamFusion, Get3D, EG3D, and HoloDiffusion. Our results, achieved on the challenging CO3Dv2 dataset, demonstrate that HOLOFUSION produces the most realistic outputs, to the best of our knowledge.