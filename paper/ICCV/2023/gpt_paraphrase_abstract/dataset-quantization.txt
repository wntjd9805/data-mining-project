State-of-the-art deep neural networks require large amounts of data to be trained effectively. However, the computational and memory costs associated with training these networks make it challenging to do so on limited hardware resources, particularly for large language models and computer vision models. To overcome this, recent dataset distillation methods have been developed to reduce the number of training samples by creating smaller synthetic datasets through gradient matching. However, these methods have limitations as the synthesized datasets are biased and perform poorly when used to train unseen network architectures. To address this issue, we propose a new framework called dataset quantization (DQ) that compresses large-scale datasets into smaller subsets suitable for training any neural network architecture. Extensive experiments demonstrate that DQ can generate condensed datasets with state-of-the-art compression ratios for lossless model training. Notably, DQ is the first method capable of successfully distilling large-scale datasets like ImageNet-1k with a high compression ratio. By using 60% of data from ImageNet and 20% from Alpaca's instruction tuning data, models trained with DQ show negligible or no drop in performance for both vision tasks (classification, semantic segmentation, and object detection) and language tasks (instruction tuning tasks like BBH and DROP).