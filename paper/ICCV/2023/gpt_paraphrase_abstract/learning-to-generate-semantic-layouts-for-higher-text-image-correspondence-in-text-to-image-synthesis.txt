Current text-to-image generation methods have made significant advancements in photorealism and text-image correspondence, mainly due to the availability of large-scale text-image datasets containing billions of pairs. However, models trained on domain-specific datasets, such as urban scenes, medical images, and faces, still struggle with low text-image correspondence due to the limited availability of text-image pairs in these domains. Moreover, collecting such large amounts of domain-specific text-image pairs is time-consuming and expensive. Therefore, achieving high text-image correspondence without relying on web-scale datasets remains a challenging task. This paper introduces a novel approach to enhance text-image correspondence by utilizing semantic layouts. Specifically, a Gaussian-categorical diffusion process is proposed to generate images and corresponding layout pairs simultaneously. Experimental results demonstrate that our approach can guide text-to-image generation models to understand the semantics of different image regions by generating semantic labels for each pixel. We show that our approach outperforms existing text-to-image generation methods in terms of text-image correspondence on the Multi-Modal CelebA-HQ and the Cityscapes datasets, where text-image pairs are scarce. The source code for our approach is available at https://pmh9960.github.io/research/GCDP.