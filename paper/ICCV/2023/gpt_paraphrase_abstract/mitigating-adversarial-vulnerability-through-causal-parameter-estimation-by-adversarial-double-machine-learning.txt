Adversarial examples created by manipulating visual inputs can greatly impact the decision-making process of deep neural networks. As a result, several defense methods based on adversarial training have emerged as the standard approach for enhancing robustness. Despite recent progress, we have noticed that the vulnerabilities to adversarial attacks still vary across different targets and certain weaknesses persist. Interestingly, even with more complex network architectures and advanced defense techniques, these vulnerabilities cannot be fully addressed. To tackle this issue, we propose a novel approach called Adversarial Double Machine Learning (ADML). This approach enables us to quantify the extent of vulnerability caused by adversarial perturbations and analyze the impact of different treatments on the desired outcomes. ADML can accurately estimate the causal parameters associated with adversarial attacks and mitigate the potential negative effects that may compromise the network's robustness. By incorporating a causal perspective into the study of adversarial vulnerability, ADML provides a solution to this peculiar phenomenon. Through extensive experiments conducted on various convolutional neural network (CNN) and Transformer architectures, we demonstrate that ADML significantly improves the network's robustness against adversarial attacks, surpassing previous methods by a large margin. Additionally, ADML effectively addresses the observed vulnerabilities, confirming its efficacy in defending against adversarial perturbations.