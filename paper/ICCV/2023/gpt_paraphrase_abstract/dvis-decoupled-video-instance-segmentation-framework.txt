Video instance segmentation (VIS) is an important task in various applications such as autonomous driving and video editing. Current methods struggle to perform well on complex and long videos in real-world scenarios due to two main reasons. Firstly, offline methods treat all frames equally and do not consider the interdependencies between adjacent frames, leading to excessive noise during long-term temporal alignment. Secondly, online methods fail to effectively utilize temporal information. To address these challenges, we propose a decoupling strategy for VIS, dividing it into three independent sub-tasks: segmentation, tracking, and refinement. The effectiveness of this strategy relies on achieving precise long-term alignment through frame-by-frame association during tracking, and utilizing temporal information based on accurate alignment outcomes during refinement. Our approach, called Decoupled VIS (DVIS), introduces a novel referring tracker and temporal refiner. DVIS achieves state-of-the-art performance in both VIS and VPS, surpassing current methods by 7.3 AP and 9.6 VPQ on challenging benchmarks. Additionally, the referring tracker and temporal refiner are lightweight, allowing for efficient training and inference on a single GPU. The code is available at the provided GitHub link.