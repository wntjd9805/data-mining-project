Scene boundary detection, an important aspect of video understanding, remains challenging due to the need for a comprehensive understanding of multimodal cues and high-level semantics. In order to address this issue, we propose a multimodal high-order relation transformer that combines a high-order encoder and an adaptive decoder in a unified framework. The encoder effectively captures high-order relations between shots and extracts shot features with contextual semantics by modeling the multimodal cues and exploring similarities between shots. On the other hand, the decoder clusters shots adaptively and identifies universal switch patterns between successive scenes, thereby aiding in scene boundary detection. Our proposed model outperforms state-of-the-art video scene detection methods according to extensive experimental results on three standard benchmarks.