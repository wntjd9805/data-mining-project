We propose a method for referring video object segmentation, which involves segmenting a specific object throughout a video based on a natural language expression. Our approach involves maintaining a global referent token and a sequence of object queries simultaneously. The referent token captures the overall referent in the video based on the language expression, while the object queries help locate and segment objects in each frame. To capture object motions and spatial-temporal reasoning between objects, we introduce a novel temporal collection-distribution mechanism. This mechanism allows for interaction between the global referent token and object queries. The temporal collection mechanism collects global information from object queries, temporal motions, and the language expression to update the referent token. The temporal distribution process distributes the referent token across all frames in the video and facilitates efficient cross-frame reasoning between the referent sequence and object queries.Our experimental results demonstrate that our method consistently and significantly outperforms state-of-the-art techniques on various benchmarks.