Transformer-based detectors, also known as DETRs, have gained popularity due to their simple framework. However, their large model size and lengthy processing time pose challenges for real-world deployment. To address this, knowledge distillation (KD) can be used to compress these detectors while maintaining comparable detection performance and reducing inference cost. However, existing KD methods for traditional convolution-based detectors may not be directly applicable to DETRs.In this study, we propose a novel knowledge distillation method called DETRDistill, specifically designed for DETR-families. Firstly, we introduce Hungarian-matching logits distillation, which encourages the student model to replicate the exact predictions of the teacher DETRs. Additionally, we propose target-aware feature distillation to enable the student model to learn from the object-centric features of the teacher model. Furthermore, we incorporate query-prior assignment distillation to accelerate the learning process of the student DETR by leveraging well-trained queries and stable assignments from the teacher model.We conducted extensive experiments on the COCO dataset to validate the effectiveness of our approach. Remarkably, DETRDistill consistently improves various DETRs by more than 2.0 mAP, even surpassing the performance of their respective teacher models.