Autonomous driving relies on LiDAR-based semantic perception tasks, which are crucial but challenging. To overcome the difficulties caused by object motion and occlusion, temporal information is necessary to enhance and complete perception. Previous methods either stack historical frames directly or use a 4D spatio-temporal neighborhood with KNN, resulting in duplicated computation and decreased real-time performance. However, stacking all historical points can lead to redundant and misleading information. To address this, we propose the Sparse Voxel-Adjacent Query Network (SVQNet) for 4D LiDAR semantic segmentation. We efficiently utilize the historical frames by dividing them into two groups based on the current points. The Voxel-Adjacent Neighborhood group carries local enhancing knowledge, while the Historical Context group completes the global knowledge. We introduce new modules to select and extract instructive features from these two groups. SVQNet achieves state-of-the-art performance in LiDAR semantic segmentation on the SemanticKITTI benchmark and the nuScenes dataset.