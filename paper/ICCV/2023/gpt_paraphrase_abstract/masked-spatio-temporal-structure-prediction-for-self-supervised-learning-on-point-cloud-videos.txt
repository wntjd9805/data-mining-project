In recent times, significant progress has been made in developing effective techniques for understanding point cloud videos by utilizing large amounts of labeled data. However, the process of annotating point cloud videos is widely known to be expensive. Additionally, training models using only one or a few traditional tasks, such as classification, may not be enough to capture the intricate details of the spatio-temporal structure present in point cloud videos. This paper introduces a method called Masked Spatio-Temporal Structure Prediction (MaST-Pre) that aims to capture the structure of point cloud videos without the need for human annotations. MaST-Pre utilizes spatio-temporal point-tube masking and comprises two self-supervised learning tasks. Firstly, by reconstructing masked point tubes, our method can capture the visual information of point cloud videos. Secondly, to learn motion, we propose a task that predicts the difference in the number of points within a point tube over time. This forces MaST-Pre to model the spatial and temporal structure in point cloud videos. Extensive experiments conducted on various datasets, including MSRAction-3D, NTU-RGBD, NvGesture, and SHRECâ€™17, demonstrate the effectiveness of the proposed method. The code for MaST-Pre can be found at https://github.com/JohnsonSign/MaST-Pre.