We propose a method to improve the accuracy of ground camera localization by estimating the relative rotation and translation between ground-level images and their matched satellite images. Our approach combines conventional geometry and learnable cross-view transformers to map ground-view observations to an overhead view. We use a neural pose optimizer to estimate the relative rotation and develop an uncertainty-guided spatial correlation to generate a probability map of vehicle locations. Experimental results show that our method outperforms the state-of-the-art, significantly improving the likelihood of restricting vehicle lateral pose and orientation within certain ranges.