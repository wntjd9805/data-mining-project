The quadratic computational complexity of self-attention has been a persistent problem in using Transformer models for vision tasks. Linear attention, on the other hand, provides a more efficient alternative with its linear complexity by approximating the Softmax operation using carefully designed mapping functions. However, current linear attention methods either suffer from significant performance degradation or introduce additional computational overhead from the mapping functions. In this study, we propose a new approach called the Focused Linear Attention module, which aims to achieve both high efficiency and expressiveness. We analyze the factors that contribute to the performance degradation of linear attention, specifically the focus ability and feature diversity. To address these limitations, we introduce a simple yet effective mapping function and an efficient rank restoration module to enhance the expressiveness of self-attention while maintaining low computational complexity. Extensive experiments demonstrate that our linear attention module can be applied to various advanced vision Transformers and consistently improves performance on multiple benchmarks. The code for our approach is available at https://github.com/LeapLabTHU/FLatten-Transformer.