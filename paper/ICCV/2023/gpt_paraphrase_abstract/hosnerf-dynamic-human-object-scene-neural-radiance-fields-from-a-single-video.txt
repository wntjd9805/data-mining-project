We present HOSNeRF, an innovative method for generating 360° free-viewpoint renderings of dynamic human-object scenes using a single monocular video. Our approach allows for pausing the video at any point and generating detailed renderings of the entire scene, including dynamic humans, objects, and backgrounds, from any desired viewpoint. The first challenge we address is the complex object motions that occur during human-object interactions. To tackle this, we introduce object bones into the conventional human skeleton hierarchy. This allows us to effectively estimate large object deformations in our dynamic human-object model.The second challenge is the varying interactions between humans and objects at different times. To handle this, we introduce two learnable object state embeddings. These embeddings serve as conditions for learning our human-object representation and scene representation, respectively.Extensive experiments demonstrate that HOSNeRF outperforms state-of-the-art approaches by a significant margin of 40% to 50% in terms of LPIPS (learned perceptual image patch similarity). To facilitate further research and exploration, we provide the code, data, and impressive examples of 360° free-viewpoint renderings from single videos on our website: https://showlab.github.io/HOSNeRF.