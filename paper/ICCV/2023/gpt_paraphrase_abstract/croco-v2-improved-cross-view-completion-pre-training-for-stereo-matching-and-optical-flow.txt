Despite their success in high-level tasks, self-supervised pre-training methods have not fully addressed dense geometric vision tasks like stereo matching and optical flow. Research is actively exploring the application of self-supervised concepts to geometric tasks, such as instance discrimination and masked image modeling. This study builds upon the cross-view completion framework, a variation of masked image modeling that utilizes a second view from the same scene, making it suitable for binocular downstream tasks. However, the application of this concept has been limited due to challenges in collecting real-world image pairs and the inability of vanilla transformers to generalize to dense downstream tasks that prioritize relative position over absolute position. To address these limitations, the study focuses on three areas of improvement. Firstly, a method to collect real-world image pairs at a large scale is introduced. Secondly, the use of relative positional embeddings is explored, demonstrating significant improvements in the performance of vision transformers. Lastly, the study scales up vision transformer-based cross-completion architectures by utilizing a large amount of data. Through these advancements, the study achieves state-of-the-art results in stereo matching and optical flow without relying on classical task-specific techniques, such as correlation volume, iterative estimation, image warping, or multi-scale reasoning. This paves the way for the development of universal vision models.