Prompt engineering is a valuable technique used to enhance the performance of pre-trained models on various tasks. Previous studies have shown that providing specific prompts can significantly improve the accuracy of models. However, while prompt learning has been extensively explored for text-based tasks, there is limited research on what constitutes a good visual prompt for image recognition. Additionally, existing methods for tuning visual prompts have shown poorer generalization compared to text-based prompting. This paper introduces a novel approach called LoGoPrompt, which leverages synthetic text images as visual prompts for vision-language models. The paper addresses the challenge of incorporating these synthetic text images as class-wise prompts or predicting the class first. The proposed method does not require any trainable visual prompt parameters and consistently outperforms state-of-the-art methods in few-shot learning across 16 datasets. The paper focuses on the abstraction of the answer format.