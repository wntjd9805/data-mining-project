Recent advancements in text-video retrieval have utilized pre-trained text-image models such as CLIP to improve performance in the video domain. However, a key challenge for these models is effectively capturing the rich semantics within videos using the image encoder of CLIP. Existing methods have employed complex cross-modal techniques to fuse text information into video frame representations, but this approach is inefficient for large-scale retrieval systems as the representations need to be recomputed online for each text query.  This paper proposes a different approach by focusing on learning semantically-enhanced representations solely from the video. By doing so, the video representations can be computed offline and reused for different texts. The proposed method involves introducing a spatial-temporal "Prompt Cube" into the CLIP image encoder. This cube is iteratively switched within the encoder layers to efficiently incorporate global video semantics into frame representations. Additionally, an auxiliary video captioning objective is applied to train the frame representations, providing fine-grained guidance in the semantic space and facilitating the learning of detailed video semantics.  To evaluate the effectiveness of the proposed method, a naive temporal fusion strategy, specifically mean-pooling, is applied to the enhanced frame representations. Experimental results on three benchmark datasets (MSR-VTT, MSVD, and LSMDC) demonstrate state-of-the-art performance. Overall, this approach eliminates the need for cross-modal fusion in large-scale retrieval systems and achieves impressive results in video retrieval tasks.