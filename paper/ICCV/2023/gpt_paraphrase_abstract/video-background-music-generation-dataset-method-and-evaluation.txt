Automatically generating background music tracks for videos is a difficult and time-consuming task. Currently, there is a lack of music-video datasets, efficient architectures for video-to-music generation, and reasonable metrics. In order to address these challenges, we have developed a comprehensive solution. Firstly, we introduce SymMV, a video and symbolic music dataset that includes various musical annotations. This dataset is the first of its kind to have rich musical annotations. Additionally, we propose V-MusProd, a benchmark video background music generation framework. V-MusProd utilizes music priors such as chords, melody, and accompaniment, as well as video-music relations such as semantic, color, and motion features. To evaluate the correspondence between video and music, we have created a retrieval-based metric called VMCP, which is based on a powerful video-music representation learning model. Experimental results demonstrate that V-MusProd outperforms existing methods in terms of music quality and correspondence with videos. We believe that our dataset, benchmark model, and evaluation metric will contribute to the advancement of video background music generation. The dataset and code are publicly available on GitHub at https://github.com/zhuole1025/SymMV.