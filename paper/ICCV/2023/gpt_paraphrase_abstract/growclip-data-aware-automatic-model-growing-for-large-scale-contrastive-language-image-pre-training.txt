Cross-modal pre-training has achieved impressive results by utilizing large amounts of image-text pairs collected from the Internet. However, as the amount of online data continues to grow, it is essential for pre-trained models to be able to learn from this constantly expanding data. Existing approaches in cross-modal pre-training focus on training a network with a fixed architecture, which is not practical considering the continuous growth of pre-training data in real-world applications. Additionally, it is important to leverage the knowledge in the current model to improve training efficiency and performance.To address these challenges, this paper introduces GrowCLIP, an algorithm for automatic model growth in contrastive language-image pre-training using continuous image-text pairs as input. GrowCLIP utilizes a dynamic growth space to identify the optimal architecture at each growth step, allowing it to adapt to online learning scenarios. The proposed growth space includes a shared encoder to enhance cross-modal fusion. The paper also explores the impact of growth in different dimensions, providing insights for future cross-modal model architecture designs. Furthermore, the paper introduces parameter inheriting with momentum (PIM) to maintain previous knowledge and overcome the local minimum dilemma.Experimental results demonstrate that GrowCLIP outperforms existing methods, achieving a 2.3% improvement in average top-1 accuracy for zero-shot image classification across nine downstream tasks. In terms of zero-shot image retrieval, GrowCLIP improves top-1 image-to-text recall by 1.2% on the Flickr30K dataset.Figure 1 illustrates the top-1 accuracy of zero-shot image classification on ImageNet during training at step 4, showing that GrowCLIP achieves the best performance and is more efficient compared to other baselines.Overall, GrowCLIP presents an effective and efficient approach for cross-modal pre-training that can adapt to the continuously growing nature of online data, resulting in improved performance in zero-shot image classification and retrieval tasks.