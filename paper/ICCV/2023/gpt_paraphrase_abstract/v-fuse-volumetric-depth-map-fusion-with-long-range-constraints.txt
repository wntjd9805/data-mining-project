We present a novel framework for enhancing depth and confidence maps generated by a Multi-View Stereo (MVS) algorithm. Our approach incorporates volumetric visibility constraints to capture surface relationships across different views, utilizing an end-to-end trainable architecture. Additionally, we introduce a sub-network to estimate the depth search window, which reduces the search space for depth hypotheses. Our method learns to model depth consensus and violations of visibility constraints directly from the data, eliminating the need for manual fine-tuning of fusion parameters. Extensive experiments on MVS datasets demonstrate significant improvements in the accuracy of the fused depth and confidence maps. The code for our framework is available at https://github.com/nburgdorfer/V-FUSE.