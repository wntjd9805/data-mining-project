This paper explores the use of synthetic data generated by a generative model as an alternative to manually collecting and annotating images with pixel-wise labels. The authors propose a method called DiffuMask, which automatically generates accurate semantic masks for synthetic images generated by the Off-the-shelf Stable Diffusion model. DiffuMask leverages the cross-attention map between text and image to localize class-specific regions and create high-resolution, class-discriminative pixel-wise masks. By using text-guided cross-attention information and practical techniques, DiffuMask significantly reduces the time and effort required for data collection and annotation. Experimental results show that segmentation methods trained on synthetic data using DiffuMask perform competitively compared to those trained on real data, with some classes achieving results close to the state-of-the-art. In the zero-shot setting, DiffuMask achieves new state-of-the-art results on the Unseen classes of VOC 2012. The project website can be accessed at DiffuMask.