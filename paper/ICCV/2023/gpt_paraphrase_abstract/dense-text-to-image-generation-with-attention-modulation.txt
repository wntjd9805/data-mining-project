Current text-to-image diffusion models face challenges in generating realistic images from dense captions, where each caption provides a detailed description of a specific image region. In order to overcome this limitation, we propose DenseDiffusion, a method that does not require training and can adapt a pre-trained text-to-image model to handle dense captions while also allowing control over the layout of the scene. Initially, we analyze the relationship between the layouts of generated images and the intermediate attention maps of the pre-trained model. Based on this analysis, we develop an attention modulation technique that guides objects to appear in specific regions based on layout guidance. Remarkably, our method achieves improved image generation performance with dense captions, as demonstrated by both automatic and human evaluation scores, without the need for additional fine-tuning or datasets. Moreover, our approach produces visual results of similar quality to models that are specifically trained with layout conditions. The code and data for DenseDiffusion are publicly available at https://github.com/naver-ai/DenseDiffusion.