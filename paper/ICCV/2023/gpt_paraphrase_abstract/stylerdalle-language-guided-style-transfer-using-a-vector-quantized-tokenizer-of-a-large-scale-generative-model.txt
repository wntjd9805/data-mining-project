Previous research in style transfer has focused on transferring simple features like color and texture, while neglecting more abstract concepts such as overall art expression or painter-specific traits. However, models like DALL-E or CLIP have the ability to capture these abstract semantics by leveraging large datasets of images and textual documents. This paper introduces StylerDALLE, a style transfer method that combines these models and utilizes natural language to describe abstract art styles. The proposed approach formulates the language-guided style transfer task as a non-autoregressive token sequence translation, specifically from an input content image to an output stylized image, within the discrete latent space of a pretrained vector-quantized tokenizer (such as the discrete variational auto-encoder of DALL-E). To incorporate style information, the paper proposes a reinforcement learning strategy with CLIP-based language supervision, enabling simultaneous stylization and content preservation. Experimental results demonstrate the effectiveness of StylerDALLE in transferring art styles using language instructions at various levels of detail. The code for StylerDALLE is available at https://github.com/zipengxuc/StylerDALLE.