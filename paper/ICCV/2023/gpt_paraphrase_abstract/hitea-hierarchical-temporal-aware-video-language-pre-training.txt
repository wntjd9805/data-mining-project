This paper introduces a video-language pre-training framework called HiTeA, which takes advantage of the unique temporal characteristics of videos. Unlike previous methods that rely on image-language pre-training paradigms, HiTeA proposes two novel pre-training tasks to generate multi-modal representations that incorporate fine-grained temporal moment information and temporal contextual relations between video-text pairs. The first task, called cross-modal moment exploration, mines paired texts to explore moments in videos and create detailed video moment representations. The second task, called multi-modal temporal relation exploration, aligns video-text pairs at different time resolutions to capture the inherent temporal contextual relations. Additionally, the paper introduces the shuffling test to evaluate the temporal reliance of datasets and video-language pre-training models. The proposed HiTeA framework achieves state-of-the-art results on 15 video-language understanding and generation tasks, particularly on datasets focused on temporal aspects, with significant improvements. HiTeA also demonstrates strong generalization ability when transferred to downstream tasks in a zero-shot manner.