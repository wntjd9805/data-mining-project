This abstract discusses the challenge of Referring Video Object Segmentation (R-VOS) which aims to segment objects in a video based on a linguistic expression. Existing methods for R-VOS assume that the object mentioned in the expression appears in the video, which is not always the case in real-world scenarios. To address this issue, the authors propose a new task called Robust R-VOS (R2-VOS) that can handle semantic mismatches by accepting unpaired video-text inputs. They propose a model that jointly models the R-VOS problem and its dual, text reconstruction, and introduces a structural text-to-text cycle constraint to enforce semantic consensus between video-text pairs. This approach effectively addresses linguistic diversity and outperforms previous methods that relied on point-wise constraints. The authors also construct a new evaluation dataset, R2-Youtube-VOS, to measure the model's robustness. The proposed model achieves state-of-the-art performance on R-VOS benchmarks, including Ref-DAVIS17, Ref-Youtube-VOS, and R2-Youtube-VOS dataset.