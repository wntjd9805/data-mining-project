This study introduces a novel dense SLAM system that combines a monocular camera with a light-weight time-of-flight (ToF) sensor. While ToF sensors are commonly used for tasks like autofocus and obstacle detection on mobile devices, they have not been extensively utilized for dense geometry reconstruction due to their sparse and noisy depth measurements. The proposed system employs a multi-modal implicit scene representation that incorporates both the RGB camera and the ToF sensor signals. The optimization process compares these signals with the raw sensor inputs. To ensure accurate pose tracking and reconstruction, a predicted depth is used as intermediate supervision, and a coarse-to-fine optimization strategy is implemented for efficient learning of the implicit representation. Additionally, the system leverages temporal information to enhance the accuracy and robustness of the ToF sensor signals. Experimental results demonstrate that the system effectively utilizes the signals from light-weight ToF sensors and achieves competitive performance in camera tracking and dense scene reconstruction. Further details and information can be found on the project page: https://zju3dv.github.io/tof_slam/.