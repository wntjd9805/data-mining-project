Domain generalization (DG) aims to develop robust models that can generalize well even when faced with unknown distribution shifts. One important aspect of DG that has not been thoroughly explored is the selection of optimizers. Currently, most DG methods rely on the widely used benchmark, DomainBed, and default to using the Adam optimizer for all datasets. However, our research reveals that Adam may not be the optimal choice for the majority of existing DG methods and datasets. Taking into consideration the perspective of loss landscape flatness, we propose a novel approach called Flatness-Aware Minimization for Domain Generalization (FAD). FAD efficiently optimizes both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of FAD's out-of-distribution (OOD) generalization error and convergence. Through our experiments, we demonstrate the superior performance of FAD on various DG datasets.