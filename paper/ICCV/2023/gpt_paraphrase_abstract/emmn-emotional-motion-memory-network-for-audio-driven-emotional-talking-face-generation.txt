This study addresses the challenge of synthesizing realistic talking faces by proposing the Emotional Motion Memory Net (EMMN) model. Previous methods have focused on predicting expressions and mouth shapes solely from audio inputs, which may not provide sufficient information for detailed expressions. Additionally, generating expressions from alternative sources often neglects the mouth region, leading to inconsistencies. To overcome these limitations, EMMN synthesizes expressions by incorporating emotion embedding and lip motion instead of relying solely on audio. The model extracts emotion embedding from audio and decomposes ground truth videos into mouth features and expression features during training. These features are then stored in the Motion Memory Net as key-value pairs. During inference, the predicted mouth features and emotion embedding are used as a query to retrieve the best-matching expression features, resulting in consistent overall expressions on the face. Extensive experiments demonstrate that the proposed method generates high-quality talking face videos with accurate lip movements and vivid expressions on unseen subjects.