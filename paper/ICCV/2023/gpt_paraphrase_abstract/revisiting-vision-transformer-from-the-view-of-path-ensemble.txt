This study presents a new perspective on Vision Transformers (ViTs) by viewing them as ensemble networks consisting of multiple parallel paths with varying lengths. The traditional cascade of multi-head self-attention and feed-forward network in each transformer layer is transformed into three parallel paths. The identity connection is utilized to form an explicit multi-path ensemble network. These paths serve two functions: providing features for the classifier and offering lower-level feature representation for longer paths. The study explores the impact of each path on the final prediction and identifies underperforming paths. To address this, path pruning and EnsembleScale skills are proposed to remove underperforming paths and optimize the combination of paths. The study also demonstrates that these path combination strategies enable ViTs to go deeper and act as high-pass filters. Additionally, self-distillation is applied to enhance the representation of paths for subsequent paths by transferring knowledge from long paths to short paths. Further research is needed to better understand and design ViTs from this new perspective.