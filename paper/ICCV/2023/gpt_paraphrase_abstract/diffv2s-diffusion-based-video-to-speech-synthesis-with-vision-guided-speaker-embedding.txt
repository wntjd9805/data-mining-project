Recent research has made significant progress in the field of video-to-speech synthesis, which involves generating speech solely from visual input. However, previous studies have faced challenges in accurately synthesizing speech due to a lack of guidance for the model to infer the correct content and sound. To overcome this issue, researchers have incorporated an additional speaker embedding as a reference for the speaking style, using auditory information. However, obtaining audio information from the corresponding video input is not always feasible, particularly during the inference stage. In this paper, we propose a new approach that extracts vision-guided speaker embeddings using a self-supervised pre-trained model and prompt tuning technique. By doing so, we can generate rich speaker embedding information solely from visual input, eliminating the need for extra audio information during inference. Using these extracted speaker embeddings, we develop a diffusion-based video-to-speech synthesis model called DiffV2S, which is conditioned on the speaker embeddings and visual representation extracted from the video input. DiffV2S not only preserves the phoneme details present in the video frames but also produces a highly intelligible mel-spectrogram that maintains the speaker identities of multiple speakers. Our experimental results demonstrate that DiffV2S achieves state-of-the-art performance compared to previous video-to-speech synthesis techniques.