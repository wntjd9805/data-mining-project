Generating realistic videos based on speech input remains a challenge due to issues like inaccurate lip shape and poor image quality. This is because previous methods focus mainly on the limited facial area driven by speech, making it difficult to learn a mapping function for the entire head image, especially with a short training video. To address this, we propose a framework called Speech2Lip that separates speech-sensitive and speech-insensitive motion/appearance to facilitate effective learning from limited data. We use a speech-driven implicit model to generate lip images, focusing on speech-sensitive motion and appearance. To model speech-insensitive motion, we introduce a geometry-aware mutual explicit mapping module that establishes mappings between different head poses. This allows us to synthesize talking videos with natural head movements by pasting the generated lip images onto head images with arbitrary poses. We also introduce a Blend-Net and contrastive sync loss to enhance synthesis performance. Our model achieves state-of-the-art results in visual quality and speech-visual synchronization, even with training videos of just a few minutes. The code for our model is available at the provided GitHub link.