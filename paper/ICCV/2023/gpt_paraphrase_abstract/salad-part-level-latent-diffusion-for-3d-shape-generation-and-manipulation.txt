We introduce a novel cascaded diffusion model that utilizes a part-level implicit 3D representation. Our model achieves superior generation quality and allows for part-level shape editing and manipulation without the need for additional training in a conditional setup. While diffusion models have shown impressive capabilities in data generation and guided reverse processes for zero-shot completion and editing, previous research on 3D diffusion models has focused on improving generation capabilities and lacked structural information for completion and editing tasks. To address this limitation, we propose our innovative diffusion model using a part-level implicit representation. To effectively learn diffusion with high-dimensional embedding vectors of parts, we employ a cascaded framework that first learns diffusion on a low-dimensional subspace encoding extrinsic parameters of parts and then on the high-dimensional subspace encoding intrinsic attributes. Our experiments demonstrate that our method outperforms previous ones in both generation and part-level completion and manipulation tasks. For more information about our project, please visit our project page at https://salad3d.github.io.