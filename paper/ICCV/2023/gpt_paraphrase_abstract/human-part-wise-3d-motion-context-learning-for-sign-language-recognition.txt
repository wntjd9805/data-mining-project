In this study, we introduce P3D, a framework for sign language recognition that focuses on learning the context of human motion in different body parts. Our contributions can be summarized in two aspects: the learning of part-wise motion context and the utilization of both 2D and 3D pose through a pose ensemble. Firstly, we observe that encoding the context of motion in different body parts improves the performance of sign language recognition. Previous approaches in this field have primarily focused on learning motion context from the entire pose sequence, neglecting the potential benefits of part-specific motion context. To address this limitation, we propose a combination of two transformers: a part-wise encoding Transformer (PET) and a whole-body encoding Transformer (WET). PET encodes the motion context from a sequence of body parts, while WET integrates the encoded information into a unified context. By incorporating part-wise motion context learning, our P3D framework achieves superior performance on the WLASL dataset compared to state-of-the-art methods.Secondly, our framework is the first to employ an ensemble of 2D and 3D poses for sign language recognition. The inclusion of 3D pose information enhances the ability to capture rich motion context and depth-related cues, which are crucial for distinguishing between different signs. As a result, our P3D framework outperforms previous state-of-the-art methods that solely rely on a single pose representation.Figure 1 illustrates the overall pipeline of our proposed system. We start by extracting expressive 3D human pose from RGB video using existing pose estimation methods. The expressive 3D human pose includes 2D pose, 3D pose, and facial expression. Subsequently, our P3D framework predicts the sign language word based on the extracted expressive 3D human pose.In summary, our P3D framework introduces the concept of learning part-wise motion context and utilizes an ensemble of 2D and 3D poses for sign language recognition. Through empirical evaluations on the WLASL dataset, we demonstrate the superiority of our approach compared to existing state-of-the-art methods.