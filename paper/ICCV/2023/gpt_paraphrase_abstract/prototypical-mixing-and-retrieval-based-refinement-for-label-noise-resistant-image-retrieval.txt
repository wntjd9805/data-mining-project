This paper explores the issue of label noise in neural network models used for image retrieval. Label noise can result in overfitting or memorization of noisy samples during optimization, and accurately identifying noisy samples is challenging. To address this problem, we propose a new approach called TI-TAN, which corrects label noise and mitigates memorization effects simultaneously. Our method involves creating prototypes with Gaussian distributions in the hidden space, which guide the synthesis of samples. These samples are then used in a similarity learning framework with varying emphasis based on the prototypical structure, reducing overfitting. We also retrieve comparable samples for each prototype, refining noisy samples in an accurate and class-balanced manner. Our experiments on five benchmark datasets demonstrate the superiority of TI-TAN compared to other baselines.