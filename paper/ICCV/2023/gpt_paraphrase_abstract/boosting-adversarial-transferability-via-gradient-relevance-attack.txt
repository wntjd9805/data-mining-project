Deep neural networks (DNNs) are vulnerable to imperceptible perturbations that can significantly affect their output, as demonstrated by numerous adversarial attack studies. Among these attacks, gradient-based methods have gained attention due to their power and ease of implementation, highlighting concerns about DNN security. However, existing gradient-based attacks struggle to bypass DNN models with defense technologies, particularly adversarially trained models, in the black-box setting. To enhance the transferability of adversarial examples, this paper explores the fluctuation phenomenon in the sign of adversarial perturbations' pixels during their generation and introduces a novel technique called Gradient Relevance Attack (GRA). GRA utilizes two gradient relevance frameworks to effectively leverage information from the input's neighborhood, enabling adaptive correction of the update direction. Additionally, a decay indicator is employed to counter the fluctuation by adjusting the update step at each iteration. Experimental results on a subset of the ILSVRC 2012 validation set demonstrate the effectiveness of GRA. Notably, GRA achieves impressive attack success rates of 68.7% and 64.8% on Tencen-t Cloud and Baidu AI Cloud, respectively, indicating its ability to generate adversarial examples that can transfer across different datasets and model architectures. The code for GRA is publicly available at https://github.com/RYC-98/GRA.