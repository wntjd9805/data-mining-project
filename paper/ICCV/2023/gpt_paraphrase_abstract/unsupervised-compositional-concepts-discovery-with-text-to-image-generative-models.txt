This study focuses on the reverse problem of text-to-image generative models. Instead of users specifying the content they want to generate, the goal is to discover the generative concepts that represent each image from a collection of different images. An unsupervised approach is proposed to disentangle various art styles, objects, lighting, and image classes. The discovered generative concepts accurately represent image content, allowing for the generation of new artistic and hybrid images through recombination and composition. Furthermore, these concepts can be utilized as representations for downstream classification tasks.