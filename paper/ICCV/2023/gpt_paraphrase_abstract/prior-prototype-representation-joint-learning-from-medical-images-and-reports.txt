We introduce a novel framework for representation learning in the context of medical images and reports. Unlike traditional methods that focus on global alignment, our framework incorporates both global and local alignment techniques to capture fine-grained representations. Additionally, we propose a cross-modality conditional reconstruction module that enables information exchange between modalities during training. To handle long reports, we construct a sentence-wise prototype memory bank, allowing the network to focus on specific visual and linguistic features. Furthermore, we suggest a non-auto-regressive generation paradigm for reconstructing non-sequential reports. Our experimental results demonstrate superior performance in various downstream tasks, including classification, retrieval, segmentation, and detection. The proposed method outperforms existing state-of-the-art approaches across multiple datasets and dataset sizes. The code for our framework is publicly available at https://github.com/QtacierP/PRIOR.