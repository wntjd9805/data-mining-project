In this study, we examine the causes of social biases in Vision Transformers (ViTs) and explore potential solutions. We analyze the impact of training data, model architecture, and training objectives on the biases encoded in the embedding spaces of ViTs. Our results indicate that counterfactual augmentation training using diffusion-based image editing can partially mitigate biases but does not completely eliminate them. Additionally, we find that larger models exhibit less bias than smaller models, and models trained with discriminative objectives are less biased than those trained with generative objectives. Interestingly, we also discover inconsistencies in the learned social biases, as ViTs trained on the same dataset using different self-supervised objectives can exhibit opposite biases. These findings provide valuable insights into the factors contributing to the emergence of social biases and suggest that significant improvements in fairness can be achieved through thoughtful model design choices.