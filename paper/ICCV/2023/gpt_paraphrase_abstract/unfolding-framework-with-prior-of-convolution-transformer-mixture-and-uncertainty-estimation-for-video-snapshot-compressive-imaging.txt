We address the problem of compressive imaging in video snapshots, where multiple frames are captured using different masks and combined into a single measurement. Reconstructing multi-frame images from a single measurement is a challenging task, but deep unfolding networks (DUNs) have shown success by combining optimization algorithms and neural networks. In this paper, we propose a model under the DUN framework that incorporates a 3D Convolution-Transformer Mixture (CTM) module with an efficient and scalable attention model. This module enables the learning of correlations between temporal and spatial dimensions using Transformer. To our knowledge, this is the first time that Transformer is applied to video snapshot compressive imaging reconstruction. Additionally, we introduce variance estimation to capture high-frequency information that is often overlooked in previous studies. Experimental results demonstrate that our proposed method achieves state-of-the-art performance, with a 1.2dB improvement in peak signal-to-noise ratio (PSNR) compared to previous algorithms. The code for our method can be found at https://github.com/zsm1211/CTM-SCI.