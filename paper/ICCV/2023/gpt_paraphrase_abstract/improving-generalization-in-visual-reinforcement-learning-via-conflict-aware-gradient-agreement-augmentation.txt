Learning a policy that can effectively generalize to unfamiliar environments is a major challenge in visual reinforcement learning. While augmentation combination has proven successful in improving generalization in supervised learning, applying it directly to visual RL algorithms can lead to decreased training efficiency and poor performance. This paper aims to address this issue by conducting a qualitative analysis and identifying two main causes: high-variance gradient magnitudes and gradient conflicts resulting from different augmentation methods. To mitigate these problems, the authors propose a novel policy gradient optimization framework called Conflict-aware Gradient Agreement Augmentation (CG2A). CG2A incorporates augmentation combination into visual RL algorithms to overcome generalization bias. It introduces a Gradient Agreement Solver to dynamically balance varying gradient magnitudes and a Soft Gradient Surgery strategy to alleviate gradient conflicts. Extensive experiments demonstrate that CG2A significantly enhances generalization performance and sample efficiency in visual RL algorithms.