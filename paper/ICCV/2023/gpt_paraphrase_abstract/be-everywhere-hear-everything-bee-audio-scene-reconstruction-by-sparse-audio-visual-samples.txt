This paper presents a method for reconstructing immersive audio experiences in dynamic scenes using a limited number of audio and visual samples. The proposed method, called BEE (everywhere and hear everything), consists of two main modules: Joint Audio-Visual Representation and Integrated Rendering Head. The first module extracts informative audio-visual features from sparse reference samples, while the second module integrates audio samples with learned time-frequency transformations to obtain the desired sound. The experiments demonstrate that BEE outperforms existing methods in terms of sound quality, generalizability to unseen scenes, and real-time processing speed.