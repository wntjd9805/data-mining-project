Recent advancements in learning-based video inpainting methods have made significant progress. However, these approaches still struggle to effectively incorporate semantic information from video frames and often fail to accurately restore object boundaries in mixed scenes. To address this issue, we propose a novel transformer-based video inpainting technique that leverages semantic information to greatly enhance reconstruction quality. Our approach involves training multiple experts using a mixture-of-experts scheme to handle mixed scenes with diverse semantics. By utilizing these multiple experts, we can generate distinct network parameters at a local (token-wise) level, resulting in inpainting results that are highly aware of the semantic context. Through extensive experiments on benchmark datasets like YouTube-VOS and DAVIS, we demonstrate that our method outperforms existing conventional video inpainting approaches. It excels at synthesizing visually appealing videos with significantly clearer semantic structures and textures.