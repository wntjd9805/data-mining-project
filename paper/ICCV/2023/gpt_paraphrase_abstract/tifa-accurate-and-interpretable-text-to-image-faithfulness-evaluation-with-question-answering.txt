Despite the efforts of numerous researchers, engineers, and artists to improve text-to-image generation models, these systems often fail to accurately align images with the corresponding text inputs. To address this issue, we introduce TIFA (Text-to-Image Faithfulness evaluation with Question Answering), an automatic evaluation metric that measures the faithfulness of generated images to their text inputs using visual question answering (VQA). TIFA generates question-answer pairs using a language model based on the given text input, and then checks whether existing VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows for detailed and understandable evaluations of generated images, and it is also more closely correlated with human judgments compared to existing metrics. To facilitate the evaluation of text-to-image models, we present TIFA v1.0, a benchmark dataset consisting of 4,000 diverse text inputs and 25,000 questions across 12 categories such as objects and counting. Using TIFA v1.0, we comprehensively evaluate existing text-to-image models and identify their limitations and challenges. For example, we find that current models perform well in terms of color and material, but struggle with tasks like counting, spatial relations, and composing multiple objects. We hope that our benchmark will enable a careful measurement of research progress in text-to-image synthesis and provide valuable insights for further studies.