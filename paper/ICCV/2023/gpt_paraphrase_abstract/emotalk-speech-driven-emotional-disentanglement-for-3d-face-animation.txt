This paper proposes a new approach for generating realistic facial expressions that correspond to speech content and emotion. Current methods often overlook emotional expressions or fail to separate them from speech content. To address this, the authors introduce an end-to-end neural network that disentangles different emotions in speech to generate expressive 3D facial animations. They utilize an emotion disentangling encoder (EDE) to separate emotion and content in speech by reconstructing speech signals with different emotion labels. Then, they employ an emotion-guided feature fusion decoder to generate a 3D talking face with enhanced emotion. The decoder utilizes disentangled identity, emotional, and content embeddings to generate controllable personal and emotional styles. Due to the scarcity of 3D emotional talking face data, the authors use facial blendshapes to reconstruct plausible 3D faces from 2D emotional data and create a large-scale 3D emotional talking face dataset (3D-ETF) to train their network. Experimental results and user studies demonstrate that their approach outperforms existing methods and produces more diverse facial movements. The authors recommend watching the supplementary video for a better understanding of their approach. You can find the video at this link: https://ziqiaopeng.github.io/emotalk.