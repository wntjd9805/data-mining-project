Using individual models to solve multiple visual tasks can be time-consuming and resource-intensive. Multi-task learning is a more efficient approach as it allows for the sharing of knowledge across different tasks. However, balancing the loss for each task in multi-task learning can be challenging and may result in decreased performance. To address this issue, we propose a novel framework that combines computation- and parameter-sharing to achieve efficient and accurate performance in multiple visual tasks. Our method is inspired by transfer learning, aiming to reduce computational and parameter storage costs while maintaining desired performance levels. We divide the tasks into a base task and sub-tasks, and share a significant portion of activations and parameters/weights between them to minimize redundancies and enhance knowledge sharing. Through evaluation on NYUD-v2 and PASCAL-context datasets, we demonstrate that our method outperforms state-of-the-art transformer-based multi-task learning techniques in terms of accuracy while utilizing fewer computational resources. Additionally, we extend our method to video stream inputs, further reducing computational costs by efficiently sharing information across both the temporal and task domains. Our code is publicly available at https://github.com/sarashoouri/EfficientMTL.