Vision Transformers are highly accurate in visual recognition tasks, but they come with high computational costs, especially in video recognition. To address this issue, we propose a method called Eventful Transformers that exploits temporal redundancy in video processing. This method identifies and reprocesses only the tokens that have changed significantly over time, reducing the computational burden. Eventful Transformers can be easily converted from existing Transformers without the need for re-training, and they provide adaptive control over compute cost during runtime. We evaluate our approach on large-scale datasets for video object detection and action recognition, namely ImageNet VID and EPIC-Kitchens 100. Our method achieves substantial computational savings (around 2-4 times) while maintaining close to the original accuracy.