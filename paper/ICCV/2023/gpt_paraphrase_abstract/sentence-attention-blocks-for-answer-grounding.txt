1.2. Answer Grounding is the task of finding relevant visual evidence for Visual Question Answering. Existing attention methods for this task have three main problems: they cannot use pre-trained networks or benefit from large data pre-training, they lack a solid foundation in previous designs, limiting the learning potential of the network, and they are complex and difficult to implement or improve.In this paper, we propose a new architectural block called the Sentence Attention Block to address these issues. This block recalibrates image feature-maps by considering the inter-dependencies between the feature-maps and sentence embedding. We show visually how this block filters out irrelevant feature-maps channels based on the sentence embedding. Starting with a well-known attention method, we make minor modifications to achieve state-of-the-art accuracy. Our method is flexible, allowing the use of different pre-trained backbone networks, and it is simple to understand and implement.We validate the effectiveness of our method on multiple datasets, including TextVQA-X, VQS, VQA-X, and VizWiz-VQA-Grounding. We conduct several ablation studies to demonstrate the effectiveness of our design choices.In summary, our approach addresses the problems of existing attention methods for answer grounding by introducing the Sentence Attention Block, which improves accuracy and is easy to use and understand.