In realistic scenarios, clients often have limited labeled data due to the expensive and laborious process of labeling. This limited labeling leads to poor generalization abilities of local models, as they may have class-distribution mismatch with unlabeled data. To overcome this, clients can benefit from a global model trained across clients, but data heterogeneity poses a challenge. In our research, we propose FEDLABEL, where clients selectively choose between local and global models to pseudo-label their unlabeled data based on expertise. We also use global-local consistency regularization to minimize divergence between the two models' outputs when they have identical pseudo-labels. Our method does not require additional experts or parameters to be communicated, and it does not assume server-labeled data or fully labeled clients. Experimental results demonstrate that FEDLABEL outperforms other semi-supervised FL baselines by 8-24% in both cross-device and cross-silo settings. It even outperforms standard fully supervised FL baselines with only 5-20% labeled data.