Self-supervised monocular depth estimation (SSMDE) is a method that predicts the depth of monocular images by minimizing a photometric loss using neighboring image pairs during training. However, SSMDE performs poorly on reflective surfaces because the photometric constancy assumption is violated. We observe that reflective surfaces have a view-dependent appearance, and there are training data views without strong specular reflections. Therefore, we propose 3D distillation, a new training framework that uses the depth of reconstructed reflective surfaces to generate accurate depth pseudo-labels. We automatically identify these surfaces using an uncertainty-guided depth fusion method that combines the more accurate projected depth on reflective surfaces with the detailed predicted depth elsewhere. Our experiments on ScanNet and 7-Scenes datasets demonstrate that 3D distillation significantly improves prediction accuracy, especially on problematic surfaces. It also generalizes well across different network architectures and new datasets.