In Video Object Detection (VID), current methods utilize temporal contexts from videos to improve object representations in each frame. However, these methods fail to consider the different identities of objects and treat all temporal contexts equally. This paper aims to address this issue by focusing on the identity-consistent temporal contexts of each object, enabling more comprehensive object representations and better handling of object appearance variations like occlusion and motion blur. However, implementing this approach on existing VID models faces efficiency challenges due to redundant region proposals and non-parallel frame-wise predictions. To overcome this, we propose ClipVID, a VID model with Identity-Consistent Aggregation (ICA) layers designed to extract fine-grained and identity-consistent temporal contexts. Our model reduces redundancies through a set prediction strategy, making the ICA layers efficient and allowing for parallel clip-wise predictions for the entire video clip. Extensive experiments demonstrate the superiority of our method, achieving state-of-the-art performance (84.7% mAP) on the ImageNet VID dataset while running at a speed approximately 7 times faster (39.3 fps) than previous state-of-the-art methods.