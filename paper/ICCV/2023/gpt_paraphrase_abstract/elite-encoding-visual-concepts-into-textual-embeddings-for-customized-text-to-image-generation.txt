This paper introduces a learning-based encoder for customized text-to-image generation that avoids the computational and memory burden associated with optimization-based methods. The encoder consists of a global mapping network and a local mapping network. The global mapping network projects hierarchical features of an image into multiple words in the textual word embedding space, including a primary word for editable concept and auxiliary words to exclude irrelevant disturbances. The local mapping network injects encoded patch features into cross attention layers to provide omitted details without compromising the editability of primary concepts. The proposed method is compared to existing optimization-based approaches on user-defined concepts, demonstrating high-fidelity inversion, robust editability, and significantly faster encoding. The code for the method is publicly available at https://github.com/csyxwei/ELITE.