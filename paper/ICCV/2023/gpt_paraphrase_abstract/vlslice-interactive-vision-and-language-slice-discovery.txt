Recent research in the field of vision-and-language has demonstrated the effectiveness of large-scale pretraining in training models that can be applied to various tasks. While this approach improves overall performance on datasets, it often fails to address biases present in the data. Analyzing model performance on specific subgroups that target these biases is crucial but requires significant time and resources for annotation. Previous attempts to automatically discover these subgroups have limitations, especially when dealing with complex inputs like vision-and-language models. This paper introduces VLSlice, an interactive system that allows users to identify coherent subgroups in unlabeled image sets. These subgroups, referred to as vision-and-language slices, exhibit consistent behavior across both visual and linguistic domains. In a user study involving 22 participants, VLSlice proved to be effective in generating diverse and coherent slices quickly. The tool has been made publicly available.