Simultaneous odometry and mapping using LiDAR data is crucial for achieving full autonomy in large-scale environments. Existing LiDAR-based methods prioritize tracking quality over reconstruction quality, neglecting the problem of simultaneous odometry and mapping for large-scale scenarios. In this paper, we propose a novel approach called NeRF-LOAM, which combines neural radiance fields (NeRF) with LiDAR odometry and mapping. NeRF-LOAM consists of three modules: neural odometry, neural mapping, and mesh reconstruction. Our approach utilizes a neural signed distance function to separate LiDAR points into ground and non-ground points, reducing Z-axis drift and optimizing odometry and voxel embeddings concurrently. This optimization process generates dense smooth mesh maps of the environment. Additionally, our NeRF-LOAM can be pre-trained and exhibits strong generalization abilities across different environments. Extensive evaluations on publicly available datasets demonstrate that our approach achieves state-of-the-art odometry and mapping performance in large-scale environments using LiDAR data. We also conducted ablation studies to validate the effectiveness of our network design. The implementation of our approach is available at https://github.com/JunyuanDeng/NeRF-LOAM.