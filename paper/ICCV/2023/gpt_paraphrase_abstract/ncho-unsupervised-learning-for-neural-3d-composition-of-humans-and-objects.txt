Recent advancements in deep generative models have allowed for the synthesis of 3D digital humans. However, existing approaches treat clothed humans as a single entity, disregarding the individual components of clothing and accessories. This limitation hinders the ability to create new identities and restricts the expressive and controllable nature of generative 3D avatars. While some methods attempt to address this by using synthetic data, the authenticity of the interaction between humans and objects is compromised due to a domain gap, and manual asset creation is challenging to scale for a wide range of objects. This study introduces a novel framework that learns a compositional generative model of humans and objects, such as backpacks, coats, scarves, and more, from real-world 3D scans. The proposed model takes into account the spatial relationship between humans and objects and incorporates the shape changes that occur through physical contact. The main challenge lies in the fact that, as humans and objects are in contact, their 3D scans are merged into a single entity. To overcome this, the approach leverages two sets of 3D scans of a single person, one with objects and one without. By learning to decompose objects and naturally compose them back into a generative human model in an unsupervised manner, the model achieves remarkable generalization capabilities. Even with a simple setup requiring the capture of a single subject with objects, the experiments demonstrate the ability to naturally compose objects into diverse identities in various poses and combine multiple objects, even if unseen during training. The project page, which provides more information, can be found at https://taeksuu.github.io/ncho.