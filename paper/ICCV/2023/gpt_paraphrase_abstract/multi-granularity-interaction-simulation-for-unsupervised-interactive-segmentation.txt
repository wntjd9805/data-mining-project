Interactive segmentation is a valuable tool in various fields, including image editing and medical image analysis, as it allows users to segment objects by providing cues. Traditionally, this process requires extensive pixel-level annotations to train deep models through manual labeling of object masks. However, we propose a new approach that leverages simulation to create informative interactions without the need for supervision.  Our approach, called Multi-granularity Interaction Simulation (MIS), aims to simulate interactions through semantic-consistent and diverse region exploration in an unsupervised manner. To achieve this, we utilize the dense features generated by recent self-supervised models. We gradually merge patches or regions with similar features to create larger regions, where each merged region represents a semantic-meaningful multi-granularity proposal. By randomly sampling these proposals and simulating interactions based on them, we provide meaningful interactions at multiple granularities to teach the model to understand interactions.  Through our MIS approach, we achieve significant improvements over non-deep learning unsupervised methods. Interestingly, our results are even comparable to some previous deep-supervised methods, despite not requiring any annotation. This demonstrates the potential of our approach for unsupervised interactive segmentation.