This research paper focuses on democratizing the process of retrieving 3D shapes from 2D sketches. The aim is to remove the reliance on large-scale, specifically sourced datasets for both 2D sketches and 3D shapes, as well as the restrictions on the quality of the sketch and the viewpoint from which it is drawn. The proposed system is trainable using existing datasets and allows users to sketch regardless of their drawing skills and without limitations on the view angle. This is achieved through the use of pivoting and novel designs that incorporate 3D understanding of 2D sketches into the system. The authors perform pivoting using two datasets from different research domains: pairs of 2D sketches and photos from the sketch-based image retrieval field (SBIR), and 3D shapes from ShapeNet. By pivoting between these datasets, the authors are able to address the challenges of democratisation. The abstraction level of 2D sketches in the SBIR dataset allows for flexibility in drawing quality, and the system works without the need for specifically sourced 2D sketch and 3D model pairs. To further democratize the sketching viewpoint, the authors use Blind Perspective-n-Points (BPnP) to "lift" 2D sketches to 3D space and inject 3D-aware information into the sketch encoder. The results demonstrate competitive performance compared to fully-supervised baselines, while achieving the democratisation goals set.