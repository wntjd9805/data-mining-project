Image token removal is a popular technique in vision-language modeling for reducing the encoding cost of image features. However, this technique has been found to have a negative impact on CLIP-style models. The reason behind this is that removing a large portion of image tokens can unintentionally eliminate the semantic information associated with a given text description, leading to misaligned paired data in CLIP training. To tackle this problem, we propose an attentive token removal approach that preserves a small number of tokens that exhibit a strong semantic correlation with the corresponding text description. We determine the correlation scores dynamically using an EMA-updated vision encoder. Our method, called attentive mask CLIP, not only surpasses the original CLIP model and a CLIP variant with random token removal but also reduces training time. Moreover, our approach enables efficient multi-view contrastive learning. In our experiments, we trained ViT-B on the YFCC-15M dataset and achieved impressive results. For ImageNet-1K zero-shot classification, we obtained a top-1 accuracy of 43.9%. Additionally, we achieved retrieval accuracies of 62.7/42.1 and 38.0/23.2 (I2T/T2I) on the Flickr30K and MS COCO datasets, respectively, outperforming the SLIP model by +1.1%, +5.5/+0.9, and +4.4/+1.3. Notably, our approach is 2.30 times faster than SLIP. We also developed an efficient version of our approach that runs 1.16 times faster than the plain CLIP model and achieves significant gains of +5.3%, +11.3/+8.0, and +9.5/+4.9 on the aforementioned benchmarks. The code for our approach will be available at https://github.com/microsoft/A-CLIP.