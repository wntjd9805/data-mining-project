Pretrained visual models on large-scale benchmarks are effective in creating powerful representations for downstream tasks. However, existing approaches like fine-tuning have drawbacks. Initializing or regularizing the downstream model based on the pretrained one fails to retain knowledge in the fine-tuning phase and can lead to overfitting. On the other hand, imposing strong constraints on the weights or feature maps of the downstream model without considering semantic drift often results in insufficient optimization. To address these issues, we propose a new fine-tuning framework called DR-Tune. DR-Tune incorporates distribution regularization by ensuring that the downstream task head reduces its classification error on the pretrained feature distribution. This prevents overfitting while enabling effective training of downstream encoders. Additionally, we introduce the semantic calibration (SC) module to align the global shape and class centers of the pretrained and downstream feature distributions, reducing interference caused by semantic drift. Extensive experiments on popular image classification datasets demonstrate that DR-Tune consistently enhances performance when combined with various backbones and pretraining strategies. The code for DR-Tune is available at: https://github.com/weeknan/DR-Tune.