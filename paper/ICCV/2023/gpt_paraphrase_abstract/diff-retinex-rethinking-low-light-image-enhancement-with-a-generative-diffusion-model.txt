This paper introduces Diff-Retinex, a physically explainable and generative diffusion model for enhancing low-light images. Diff-Retinex combines the benefits of a physical model and a generative network to address the limitations of low-light images. The proposed method formulates the enhancement problem by decomposing the image into illumination and reflectance maps using a Retinex Transformer decomposition network (TDN). It then utilizes multi-path generative diffusion networks to reconstruct the normal-light Retinex probability distribution and address various degradations such as dark illumination, noise, color deviation, and loss of scene contents. The generative diffusion model enables the restoration of low-light subtle details. Extensive experiments on real-world low-light datasets confirm the effectiveness, superiority, and generalization of Diff-Retinex.