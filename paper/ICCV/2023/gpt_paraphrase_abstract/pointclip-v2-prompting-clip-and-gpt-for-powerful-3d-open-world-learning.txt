Large-scale pre-trained models have been successful in vision and language tasks but have limited capability when it comes to 3D point clouds, only being able to perform classification. In this paper, we propose Point-CLIP V2, a unified 3D open-world learner that combines CLIP and GPT to achieve zero-shot 3D classification, segmentation, and detection. We address the challenge of aligning 3D data with pre-trained language knowledge through two key design elements. Firstly, we use a shape projection module to prompt CLIP into generating more realistic depth maps, reducing the gap between projected point clouds and natural images. Secondly, we prompt the GPT model to generate 3D-specific text for CLIP's textual encoder. Without any specific training in 3D domains, our approach outperforms PointCLIP by a significant margin in zero-shot 3D classification on three datasets. Furthermore, Point-CLIP V2 can be easily extended to few-shot 3D classification, zero-shot 3D part segmentation, and 3D object detection, showcasing its generalization capabilities in unified 3D open-world learning. The code for our approach is available at the provided GitHub link.