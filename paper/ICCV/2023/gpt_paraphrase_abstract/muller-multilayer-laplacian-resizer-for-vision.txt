This paper explores the impact of different image resizing methods on the performance of deep vision models. While commonly used resizers like nearest-neighbors, bilinear, and bicubic are readily available, alternative methods have been overlooked. The authors introduce a lightweight multilayer Laplacian resizer called MULLER, which boosts details in specific frequency subbands to improve downstream recognition models. MULLER can be easily integrated into various training pipelines and enhances the performance of vision tasks with minimal additional cost. The authors demonstrate the effectiveness of MULLER by applying it to the state-of-the-art vision Transformer, MaxViT, resulting in a 0.6% increase in top-1 accuracy and a 36% reduction in inference cost on ImageNet-1k compared to standard training. MULLER's performance is also scalable with model and training data size and applicable to various vision tasks such as image classification, object detection, segmentation, and image quality assessment. The code for MULLER is available on the Google Research GitHub repository.