Knowledge distillation is a popular method for compressing models and improving the performance of lightweight student models by transferring knowledge from well-performing teacher models. This method has been widely used in computer vision tasks, including face recognition. However, current face recognition distillation methods, such as Feature Consistency Distillation (FCD), tend to reduce the intra-class similarities of the student model compared to the teacher model. To address this issue, we propose a new face recognition distillation method called ICD-Face.ICD-Face introduces intra-class compactness distillation into the existing distillation framework. In this method, we first calculate similarity distributions for both the teacher and student models. To ensure high-quality positive pairs, we utilize feature banks. Next, we estimate the probability distributions of both models and introduce the Similarity Distribution Consistency (SDC) loss to enhance the intra-class compactness of the student model. Through extensive experiments on multiple benchmark datasets, we demonstrate the effectiveness of our proposed ICD-Face method for face recognition.