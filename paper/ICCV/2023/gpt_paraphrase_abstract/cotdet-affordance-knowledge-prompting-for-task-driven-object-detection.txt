This study focuses on task-driven object detection, which involves detecting objects in an image that are suitable for performing a specific task. The challenge in this area is that the object categories relevant to the task are too diverse to be limited to a predefined set of objects typically used in traditional object detection. The authors propose an alternative approach that explores fundamental affordances instead of relying on object categories. Affordances refer to the common attributes that enable different objects to accomplish the same task. To extract affordance knowledge, the authors introduce a novel method called multi-level chain-of-thought prompting (MLCoT), which utilizes large language models to reason from the task to object examples and essential visual attributes. Additionally, the authors propose a knowledge-conditional detection framework called CoTDet to fully leverage this knowledge for object recognition and localization. CoTDet conditions the detector using the knowledge to generate object queries and predict bounding boxes. Experimental results demonstrate that CoTDet consistently outperforms existing methods, achieving higher box and mask average precision scores (+15.6 and +14.8, respectively). Furthermore, CoTDet is capable of generating rationales that explain why objects are detected as suitable for the given task.