Visual Parameter-Efficient Fine-Tuning (PEFT) has emerged as a viable alternative to full fine-tuning for adapting pre-trained vision models to specific tasks. PEFT involves tuning a small number of parameters while keeping the majority of parameters frozen, which helps reduce storage requirements and optimization challenges. However, existing PEFT methods rely on human heuristics to introduce trainable parameters at fixed positions across different tasks, overlooking domain gaps. To address this limitation, we propose a novel approach called Sensitivity-aware visual Parameter-efficient fine-Tuning (SPT). SPT dynamically allocates trainable parameters to task-specific important positions, taking into account a desired parameter budget. First, SPT identifies the sensitive parameters that need tuning for a particular task in a data-dependent manner. Then, for weight matrices with a large number of sensitive parameters exceeding a predefined threshold, SPT leverages structured tuning methods like LoRA or Adapter to enhance their representational capability. This is done by replacing the direct tuning of selected sensitive parameters (unstructured tuning) under the given budget.Extensive experiments conducted on various downstream recognition tasks demonstrate that SPT complements existing PEFT methods and significantly improves their performance. For instance, when applied to Adapter with a supervised pre-trained ViT-B/16 backbone, SPT achieves a mean Top-1 accuracy improvement of 4.2% and 1.4%, respectively, on FGVC and VTAB-1k benchmarks, surpassing the state-of-the-art performance. The source code for SPT can be found at https://github.com/ziplab/SPT.