Our objective is to generate 3D human movements by combining simultaneous actions described in text inputs, such as waving hand while walking. This type of composition, which involves moving different body parts at the same time, is referred to as spatial compositing. To achieve this, we leverage the knowledge encoded in powerful language models like GPT-3 by prompting it with questions like "what body parts are involved in a specific action?" and providing a list of body parts and examples. With this action-part mapping, we are able to combine body parts from two motions and develop an automated method for spatially composing two actions. However, the limited availability of training data with compositional actions necessitates the creation of synthetic data using this approach. We use this synthetic data to train a new state-of-the-art text-to-motion generation model called SINC ("Simultaneous action Compositions for 3D human motions"). Our experiments demonstrate that training with GPT-guided synthetic data enhances the generation of spatial compositions compared to baseline models. The code for our model is publicly accessible at sinc.is.tue.mpg.de. Figure 1 showcases the goal of our research, illustrating six examples of input-output pairs where our model generates 3D motions that simultaneously perform a pair of actions, such as putting hands on the waist and moving the torso to the left. The output only includes the abstraction of performing both actions in one motion.