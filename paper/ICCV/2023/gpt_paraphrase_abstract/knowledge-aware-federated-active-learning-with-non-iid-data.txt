Federated learning allows multiple decentralized clients to collaborate on learning without sharing their local data. However, the cost of annotating data on local clients is still a challenge. This paper proposes a federated active learning approach to efficiently learn a global model with limited annotation resources while protecting data privacy. The main challenge in federated active learning is the mismatch between the active sampling goals of the global model and the asynchronous local clients, especially when data is distributed non-IID. To address this challenge, the paper introduces Knowledge-Aware Federated Active Learning (KAFAL), which includes Knowledge-Specialized Active Sampling (KSAS) and Knowledge-Compensatory Federated Update (KCFU). KSAS is a novel active sampling method designed for federated active learning, sampling actively based on discrepancies between local and global models. KSAS enhances specialized knowledge in local clients to ensure informative data for both local clients and the global model. KCFU compensates for client heterogeneity caused by limited data and non-IID data distributions by leveraging the global model's assistance. Extensive experiments demonstrate the superiority of KAFAL over recent active learning methods. The code for KAFAL is available at https://github.com/ycao5602/KAFAL.