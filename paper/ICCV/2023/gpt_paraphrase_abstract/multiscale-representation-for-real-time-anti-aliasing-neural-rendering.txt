The neural radiance field (NeRF) is a rendering scheme that uses ray casting to render pixels. However, it produces blurry results when training images are taken at different scales and creates aliasing artifacts for test images taken from distant views. To address this problem, Mip-NeRF introduced a multiscale representation using a conical frustum to encode scale information. However, this approach is only suitable for offline rendering as it relies on integrated positional encoding (IPE) to query a multilayer perceptron (MLP). To overcome this limitation, we propose Mip-VoG, which is an explicit multiscale representation with a deferred architecture for real-time anti-aliasing rendering. Our approach includes a density Mip-VoG for scene geometry and a feature Mip-VoG with a small MLP for view-dependent color. Mip-VoG represents scene scale using level of detail (LOD) derived from ray differentials and uses quadrilinear interpolation to map a queried 3D location to its features and density from neighboring down-sampled voxel grids. Notably, our approach is the first to offer multiscale training and real-time anti-aliasing rendering simultaneously. We conducted experiments on a multiscale dataset and found that our approach outperforms state-of-the-art real-time rendering baselines.