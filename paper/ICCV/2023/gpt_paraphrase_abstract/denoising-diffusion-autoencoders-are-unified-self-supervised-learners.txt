This study explores the potential of diffusion models, specifically denoising diffusion autoencoders (DDAE), to acquire discriminative representations for classification through generative pre-training. The researchers demonstrate that DDAEs, as part of diffusion models, are capable of self-supervised learning and can learn linear-separable representations within their intermediate layers without the need for auxiliary encoders. This establishes diffusion pre-training as a general approach for dual learning, encompassing both generative and discriminative aspects. Linear probe and fine-tuning evaluations validate the effectiveness of the diffusion-based approach, achieving high accuracy rates of 95.9% on CIFAR-10 and 50.0% on Tiny-ImageNet, comparable to contrastive learning and masked autoencoders. Additionally, transfer learning from ImageNet supports the suitability of DDAEs for Vision Transformers, highlighting their potential for scalability as foundational models. The code for this study can be accessed on GitHub at github.com/FutureXiang/ddae. The study includes denoising networks in both pixel-space and latent-space diffusion models and evaluates DDAEs as self-supervised representation learners. The results confirm that DDAEs can generate strong representations at intermediate layers, and further truncating and fine-tuning DDAEs as vision encoders improves image classification performance.