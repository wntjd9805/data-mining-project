The text-to-image person re-identification (T2I-ReID) task requires a pre-training step. However, there are two inconsistencies between these tasks that can affect performance. Firstly, there is a domain gap between the generic images/texts used in pre-trained models and the specific person data in T2I-ReID, especially for texts. Secondly, the pre-training processes for images and texts are independent, despite the need for cross-modality learning in T2I-ReID. To address these issues, we propose a new unified pre-training pipeline called UniPT. We create a large-scale text-labeled person dataset called "LUPerson-T" by generating pseudo-textual descriptions using the CLIP paradigm. We then use a vision-and-language pre-training framework to align the feature space of images and texts during pre-training, ensuring consistency between the pre-training task and the T2I-ReID task at both data and training levels. Our UniPT achieves competitive Rank-1 accuracy on various datasets without any additional complexity. The LUPerson-T dataset and code are available at https://github.com/ZhiyinShao-H/UniPT.