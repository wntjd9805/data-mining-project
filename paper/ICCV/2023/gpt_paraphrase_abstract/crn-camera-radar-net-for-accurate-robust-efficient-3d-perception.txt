An accurate and fast 3D perception system is crucial for autonomous driving, which includes tasks such as 3D object detection, tracking, and segmentation. While low-cost camera-based approaches have shown promise, they are susceptible to poor lighting and weather conditions and have a high localization error. Therefore, combining cameras with low-cost radar, which provides precise long-range measurements and operates reliably in all environments, is a promising solution that has not been thoroughly explored. This paper introduces Camera Radar Net (CRN), a new framework that fuses camera and radar data to generate a semantically rich and spatially accurate bird's-eye-view (BEV) feature map for various tasks. To address the lack of spatial information in images, we transform perspective view image features into BEV using sparse but accurate radar points. We then combine the image and radar feature maps in BEV using a multi-modal deformable attention mechanism that addresses the spatial misalignment between inputs. In real-time settings, CRN operates at 20 frames per second (FPS) and achieves performance comparable to LiDAR detectors on the nuScenes dataset, even outperforming them at longer distances (100m setting). In offline settings, CRN achieves impressive results, with a 62.4% NDS and 57.5% mAP on the nuScenes test set, ranking first among all camera and camera-radar 3D object detectors.