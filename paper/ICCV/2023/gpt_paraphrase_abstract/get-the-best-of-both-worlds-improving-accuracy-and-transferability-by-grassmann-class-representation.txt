We propose a new approach called the Grassmann Class Representation (GCR) that extends the concept of class vectors in neural networks to linear subspaces on the Grassmann manifold. By representing each class as a subspace and defining the logit as the norm of the projection of a feature onto the class subspace, GCR enables simultaneous improvement in accuracy and feature transferability. We integrate Riemannian SGD into deep learning frameworks to jointly optimize the class subspaces along with the other model parameters. Compared to the traditional vector representation, subspaces offer more powerful representative capabilities.In our experiments on the ImageNet-1K dataset, we observed significant reductions in the top-1 errors of various models, including ResNet50-D, ResNeXt50, Swin-T, and Deit3-S. The errors were reduced by 5.6%, 4.5%, 3.0%, and 3.5% respectively. Additionally, subspaces provide flexibility for features to vary, and we found that increasing the subspace dimension leads to greater intra-class feature variability. Consequently, the quality of GCR features is improved for downstream tasks.To evaluate the effectiveness of GCR features, we conducted linear transfer accuracy experiments on six datasets. For ResNet50-D, the average linear transfer accuracy improved from 77.98% to 79.70% compared to the strong baseline of vanilla softmax. Similarly, for Swin-T, it improved from 81.5% to 83.4%, and for Deit3, it improved from 73.8% to 81.4%. These results demonstrate the potential of the Grassmann class representation in various applications.To facilitate further exploration and adoption of GCR, we have released the code at https://github.com/innerlee/GCR.