Vision Transformers (VTs) require position information because self-attention operations are permutation-invariant. Currently, position information is introduced by adding absolute Position Embedding (PE) to patch embedding before entering VTs. However, this method applies the same Layer Normalization (LN) to token embedding and PE, resulting in restricted and monotonic PE across layers. Additionally, the shared LN affine parameters do not specifically cater to PE, preventing adjustment on a per-layer basis. To address these limitations, we propose a new approach that utilizes two independent LNs for token embeddings and PE in each layer, allowing for progressive delivery of PE across layers. This implementation provides VTs with layer-adaptive and hierarchical PE. We name this method Layer-adaptive Position Embedding (LaPE), which is simple, effective, and robust. Extensive experiments conducted on image classification, object detection, and semantic segmentation demonstrate that LaPE outperforms the default PE method. For instance, LaPE improves performance by +1.06% for CCT on CIFAR100, +1.57% for DeiT-Ti on ImageNet-1K, +0.7 box AP and +0.5 mask AP for ViT-Adapter-Ti on COCO, and +1.37 mIoU for tiny Segmenter on ADE20K. Remarkably, LaPE achieves these improvements while only increasing parameters, memory, and computational cost by a negligible amount.Figure 1 illustrates the default PE joining method and our proposed LaPE. Using T2T-ViT-7 with 1-D sinusoidal PE as an example, we visualize the position correlation of the first 5 layers to explain the emphasis and advantages of our method. By default, token embedding and PE are coupled together and treated with the same LN in each layer, resulting in monotonic and limited position correlations. However, we argue that each layer's token embedding and PE should have independent LNs (LNT, LNP). This enhances the expressiveness of PE and adjusts the position correlations into a hierarchical and layer-adaptive structure.In summary, our LaPE method significantly outperforms the default PE method in various tasks, while only incurring a negligible increase in parameters, memory, and computational cost.