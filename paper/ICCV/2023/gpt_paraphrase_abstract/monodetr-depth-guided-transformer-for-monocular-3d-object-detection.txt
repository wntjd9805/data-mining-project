Autonomous driving poses a challenge in detecting 3D objects concurrently using monocular vision. Most existing methods rely on 2D detectors to locate object centers and then estimate 3D attributes based on local visual features. However, this approach fails to comprehend scene-level 3D spatial structures and neglects long-range inter-object depth relations. To address this, we propose a novel framework called MonoDETR, which combines monocular detection with a depth-guided Transformer (DETR). Our modified transformer incorporates depth awareness and utilizes contextual depth cues to guide the detection process. Additionally, we introduce a foreground depth map prediction and a specialized depth encoder to extract non-local depth embeddings. We formulate 3D object candidates as learnable queries and employ a depth-guided decoder to facilitate object-scene depth interactions. This allows each object query to estimate its 3D attributes adaptively from depth-guided regions in the image, breaking free from the limitations of local visual features. MonoDETR achieves state-of-the-art performance on the KITTI benchmark without requiring dense depth annotations. Furthermore, our depth-guided modules can be easily integrated into multi-view 3D object detectors on the nuScenes dataset, showcasing their excellent generalization capacity. The code for MonoDETR is available at https://github.com/ZrrSkywalker/MonoDETR.