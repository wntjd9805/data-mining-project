This study explores the issue of biased representation in text-to-image generative models, which often leads to unequal portrayal of underrepresented groups. The researchers aim to develop inclusive text-to-image generative models that can generate images in a way that ensures equal distribution across desired attributes. However, directly specifying these attributes in the prompts often leads to subpar results due to linguistic ambiguity or model misrepresentation. Therefore, this paper proposes a different approach that emphasizes the idea that images can convey concepts more effectively than text. For attributes that are difficult to express in text, such as skin tones, example images can be used instead. Based on this insight, the authors introduce a novel approach called ITI-GEN1, which utilizes reference images to generate inclusive images based on prompts. The key idea is to learn prompt embeddings that can effectively represent all desired attribute categories. Importantly, ITI-GEN does not require model fine-tuning, making it computationally efficient for enhancing existing text-to-image models. Extensive experiments demonstrate that ITI-GEN significantly improves upon state-of-the-art models in generating inclusive images from prompts.