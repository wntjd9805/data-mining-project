This paper aims to enhance the speed of video perception tasks such as segmentation and human pose estimation by exploiting redundancies across frames. Unlike existing methods that use optical flow or sparse convolutions to avoid redundant computations, we propose a different approach: low-bit quantization. We observe that residuals, which represent the difference in network activations between adjacent frames, possess quantization-friendly properties. Building upon this observation, we introduce a novel quantization scheme called Residual Quantization (ResQ) for video networks. ResQ improves accuracy compared to standard frame-by-frame quantization by incorporating temporal dependencies. Additionally, we develop a method to dynamically adjust the bit-width based on video changes. Through experiments on semantic segmentation, video object segmentation, and human pose estimation benchmarks, we demonstrate the superiority of our model over standard quantization methods and existing efficient video perception models across various architectures.