Text-to-3D modeling has made significant advancements by combining text-to-image models with image-to-3D techniques like Neural Radiance Fields. A recent method called DreamFusion has achieved high-quality results in creating 3D objects, but it requires time-consuming optimization for each prompt. To overcome this limitation, we propose a new approach called Amortized Text-to-3D (ATT3D), which trains on multiple prompts simultaneously using a unified model instead of separately optimizing each prompt. This allows us to share computation across prompt sets and significantly reduce training time compared to per-prompt optimization. ATT3D facilitates knowledge sharing between prompts, enabling generalization to new scenarios and smooth transitions between text inputs for creating novel assets and simple animations.