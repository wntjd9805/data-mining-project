Neural Architecture Search (NAS) has shown promise in designing vision transformers (ViT) with high performance, but creating lightweight and low-latency ViT models for different mobile devices is still a challenge. To address this, we propose ElasticViT, a two-stage NAS approach. First, we train a high-quality ViT supernet that covers a wide range of mobile devices. Then, we search for an optimal sub-network (subnet) for direct deployment. However, current supernet training methods using uniform sampling face the issue of gradient conflict, as the sampled sub-nets can have vastly different model sizes, leading to different optimization directions and inferior performance. To overcome this, we introduce two new sampling techniques: complexity-aware sampling and performance-aware sampling. Complexity-aware sampling limits the difference in FLOPs among the sampled subnets, while still covering different-sized subnets. Performance-aware sampling selects subnets with good accuracy to reduce gradient conflicts and improve supernet quality. Our discovered models, ElasticViT, achieve top-1 accuracy ranging from 67.2% to 80.0% on ImageNet, with model sizes ranging from 60M to 800MFLOPs, surpassing prior CNNs and ViTs in terms of accuracy and latency. Additionally, our tiny and small models are the first ViT models to outperform state-of-the-art CNNs with significantly lower latency on mobile devices. For example, ElasticViT-S1 runs 2.62 times faster than EfficientNet-B0 with 0.1% higher accuracy.