This study focuses on object recognition from sparse and noisy events in situations where paired images and category labels are not available. The authors propose a joint approach that combines object recognition and image reconstruction. They first reconstruct images from events and then use Contrastive Language-Image Pre-training (CLIP) for object recognition, which improves recognition by incorporating a rich context of images. To bridge the textual features of predicted categories and the visual features of reconstructed images, the authors introduce category-guided attraction loss and category-agnostic repulsion loss. They also employ a reliable data sampling strategy and local-global reconstruction consistency to enhance joint learning. Additionally, the authors propose a prototype-based approach using unpaired images to improve prediction accuracy and reconstruction quality. Extensive experiments demonstrate the effectiveness of their method, particularly for zero-shot object recognition. The project code is available at the provided GitHub link.