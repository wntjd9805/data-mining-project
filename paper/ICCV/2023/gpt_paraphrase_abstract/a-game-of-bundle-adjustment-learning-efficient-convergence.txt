Bundle adjustment is a common method used for solving localization and mapping problems. It involves solving a system of nonlinear equations through an iterative process using optimization methods weighted by a damping factor. In the traditional approach, the damping factor is chosen heuristically using the Levenberg-Marquardt algorithm in each iteration. This heuristic approach can be computationally expensive and detrimental to real-time applications. To address this issue, we propose a novel approach that treats the problem as a game and formulates it as a reinforcement learning task. We create an environment that solves the nonlinear equations and train an agent to learn how to select the damping factor. Our approach significantly reduces the number of iterations needed to achieve convergence in bundle adjustment, both in synthetic and real-life scenarios. Furthermore, we demonstrate that this reduction in iterations can benefit the traditional approach and can be combined with other methods for accelerating bundle adjustment.