Computer vision research has traditionally focused on developing systems that can handle various transformations present in real-world data. This has typically been achieved through techniques such as data augmentation or incorporating invariances directly into the system's architecture. However, determining the appropriate level of invariance is challenging, as too much or too little can have negative effects and the optimal amount may vary for each instance. Ideally, the system should be able to learn the necessary invariance from the data and apply it during testing.To address this, we approach invariance as a prediction problem. Given an image, we predict a distribution of transformations that can be applied and average the predictions to make invariant predictions. By combining this approach with a graphical model, we create a flexible and adaptable form of invariance that can be generalized across different scenarios. Our experiments demonstrate the effectiveness of this method in aligning datasets, identifying prototypes, adapting to poses outside the distribution, and achieving general invariance across classes.When used as a data augmentation technique, our method shows improved accuracy and robustness on CIFAR 10, CIFAR10-LT, and TinyImageNet datasets.