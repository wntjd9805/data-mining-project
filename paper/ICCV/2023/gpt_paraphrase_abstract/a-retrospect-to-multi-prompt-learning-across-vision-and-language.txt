The vision community is currently experiencing significant advancements in Vision-Language Pretraining Models (VLMs), which have the potential to revolutionize the field. Prompt learning is crucial for effectively utilizing VLMs, as it allows for quick adaptation to specific tasks with limited resources. However, the majority of existing research focuses on single-prompt paradigms and neglects the technical potential of multi-prompt learning. This paper aims to address this gap by offering a principled analysis of vision-language multi-prompt learning.We extend the concept of the constant modality gap to learnable prompts and demonstrate the advantages of vision-language transfer through multi-prompt augmentation, both empirically and theoretically. Based on this observation, we propose an Energy-based Multi-prompt Learning (EMPL) approach, which generates multiple prompt embeddings by sampling instances from an energy-based distribution implicitly defined by VLMs. EMPL is not only efficient in terms of parameters but also effectively balances in-domain and out-of-domain open-vocabulary generalization.We have conducted comprehensive experiments to validate our claims and evaluate the performance of EMPL. The results support our arguments and demonstrate the superiority of EMPL in vision-language multi-prompt learning.