Advancements in deep generative models have enabled the creation of realistic images in various tasks. However, these generated images often have visual imperfections in specific areas that need to be corrected. In this study, we present a comprehensive empirical examination of Perceptual Artifacts Localization (PAL) in diverse image synthesis tasks. We introduce a new dataset consisting of 10,168 generated images, each labeled with perceptual artifact information at the pixel level across ten synthesis tasks. We train a segmentation model on this dataset, which effectively identifies artifacts across different tasks. Furthermore, we demonstrate its ability to adapt to new models with minimal training samples. We also propose a novel zoom-in inpainting pipeline that seamlessly rectifies perceptual artifacts in the generated images. Through our experiments, we highlight various downstream applications, including automated artifact correction, non-referential image quality evaluation, and abnormal region detection in images. The dataset and code for this study are available at the provided link.