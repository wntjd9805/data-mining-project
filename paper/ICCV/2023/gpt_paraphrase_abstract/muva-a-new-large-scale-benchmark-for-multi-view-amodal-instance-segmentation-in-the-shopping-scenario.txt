Amodal Instance Segmentation (AIS) aims to accurately determine the complete shapes of objects that are partially or fully occluded. However, single-view datasets present challenges in determining occluded shapes due to their inherent lack of information. To address this issue, we propose a new approach called Multi-view Amodal Instance Segmentation (MAIS) that leverages multiple views to improve shape inference. We introduce the MUVA dataset, the first MUlti-View AIS dataset, which focuses on the shopping scenario. MUVA provides comprehensive annotations, including multi-view amodal/visible segmentation masks, 3D models, and depth maps. It is the largest image-level AIS dataset in terms of both the number of images and instances. Additionally, we propose a novel method for aggregating representative features across instances and views, which shows promising results in accurately predicting occluded objects from a single viewpoint by utilizing information from other viewpoints. Furthermore, we demonstrate that MUVA can be beneficial for the AIS task in real-world scenarios.