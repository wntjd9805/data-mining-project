Vision-Language (V-L) models trained using contrastive learning to align visual and language modalities have proven to be effective few-shot learners. Soft prompt learning is commonly used for adapting V-L models to new domains, but it requires access to model weights and can be computationally impractical for large models. To address these limitations, this study presents a black-box method for V-L few-shot adaptation. This method operates on pre-computed image and text features, eliminating the need for model weight access. Additionally, it is significantly faster during training, supports both supervised and unsupervised training, and can align features from uni-modal models. The proposed approach, called Linear Feature Alignment (LFA), uses a simple linear approach for V-L realignment in the target domain. LFA is initialized from a closed-form solution to a least-squares problem and is iteratively updated using a re-ranking loss. Despite its simplicity, LFA outperforms soft prompt learning methods, as demonstrated through extensive experiments on 11 image and 2 video datasets. The code for LFA is available at: https://github.com/saic-fi/LFA.