Recently, there have been attempts to apply the image masked autoencoder (MAE) with random masking to videos, yielding promising outcomes. However, videos require both spatial and temporal information for comprehensive comprehension, unlike images. Therefore, the random masking strategy inherited from the image MAE proves to be less effective for video MAE. To address this, we propose a novel masking algorithm called motion-guided masking (MGM) that utilizes motion vectors to guide the positioning of each mask over time. These motion-based correspondences can be directly obtained from the compressed format of the video, making our approach efficient and scalable. By incorporating MGM into video MAE, we achieve a performance improvement of up to +1.3% compared to previous state-of-the-art methods on challenging video benchmarks (Kinetics-400 and Something-Something V2). Moreover, our MGM achieves comparable performance to previous video MAE models but requires up to 66% fewer training epochs. Additionally, we demonstrate that MGM exhibits better generalization in downstream transfer learning and domain adaptation tasks on the UCF101, HMDB51, and Diving48 datasets, outperforming baseline methods by up to +4.9%.