Language-guided visual navigation research has shown a need for diverse environments and abundant supervision to train agents that can generalize well. To address the limited data problem in existing vision-and-language navigation datasets, we propose a method to generate large-scale training data. We utilize over 1200 realistic environments from HM3D and Gibson datasets and create 4.9 million pairs of instruction and trajectory by leveraging online resources. We study the impact of each component of this method on agent performance and explore how to effectively use the augmented data for pre-training and fine-tuning. With our extensive dataset, we achieve significant improvement in agent performance, increasing the success rate by 11% compared to the previous state-of-the-art. Additionally, we reduce the gap between navigating seen and unseen environments to less than 1%, a substantial improvement over the previous best method which had an 8% gap. Our method also enables different models to achieve new state-of-the-art results in continuous environments for CVDN, REVERIE, and R2R.