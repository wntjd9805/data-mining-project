Live commerce refers to the selling of products online through live streaming. However, the increasing diversity of customer demands for online products presents challenges in recognizing products during live streams. Previous studies have mainly focused on fashion clothing and used single-modal input, which does not accurately represent the real-world scenario where multimodal data from various categories are present.   To address this issue, we introduce LPR4M, a large-scale multimodal dataset that includes 34 categories and consists of three modalities: image, video, and text. This dataset is 50 times larger than any publicly available dataset. LPR4M contains diverse videos and noise modality pairs, reflecting real-world problems with a long-tailed distribution.   Additionally, we propose a model called cRoss-vIew semantiC alignmEnt (RICE) to learn discriminative instance features from the image and video views of the products. This is achieved through instance-level contrastive learning and cross-view patch-level feature propagation. We also introduce a novel Patch Feature Reconstruction loss to penalize the semantic misalignment between cross-view patches.   Extensive experiments demonstrate the effectiveness of RICE and provide insights into the importance of dataset diversity and expressivity. To facilitate further research, we have made the dataset and code available at https://github.com/adxcreative/RICE.