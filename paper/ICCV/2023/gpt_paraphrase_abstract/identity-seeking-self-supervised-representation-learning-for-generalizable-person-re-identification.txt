This paper aims to develop a person re-identification (ReID) representation that can be applied to various domains without the need for annotation. Previous methods in ReID have been limited by the high cost of annotation, resulting in the use of limited labeled data for training. To address this limitation, we propose a method called Identity-seeking Self-supervised Representation learning (ISR) that utilizes large-scale unsupervised data for training. ISR constructs positive pairs from inter-frame images by modeling the instance association as a maximum-weight bipartite matching problem. We also introduce a reliability-guided contrastive loss to mitigate the impact of noisy positive pairs, ensuring that reliable positive pairs dominate the learning process. The training cost of ISR scales linearly with the data size, making it feasible to utilize large-scale data for training. Experimental results show that the learned representation exhibits superior generalization ability. Without human annotation and fine-tuning, ISR achieves 87.0% Rank-1 on the Market-1501 dataset and 56.4% Rank-1 on the MSMT17 dataset, outperforming the best supervised domain-generalizable method by 5.0% and 19.5% respectively. In the pre-trainingâ†’fine-tuning scenario, ISR achieves state-of-the-art performance, with 88.4% Rank-1 on the MSMT17 dataset. The code for ISR is available at https://github.com/dcp15/ISR_ICCV2023_Oral.