The main challenge in reconstructing 3D scenes from monocular images is how to combine features from different views without any depth or occlusion information. To overcome this challenge, we propose using monocular depth priors, which guide the fusion process to improve surface prediction and exclude irrelevant or occluded features. Additionally, we suggest two alternative fusion modules - a variance-based and a cross-attention-based module - that are more efficient and effective than the commonly used average-based and self-attention-based methods in neural 3D reconstruction. Our proposed DG-Recon models demonstrate significant improvements in reconstruction quality and completeness compared to the NeuralRecon baseline, while still maintaining real-time performance. We achieve state-of-the-art online reconstruction results on the ScanNet dataset and perform on par with the best offline method, which accesses keyframes from the entire video sequence. Our ScanNet-trained model also generalizes well to the challenging 7-Scenes dataset and a subset of SUN3D that includes scenes as large as an entire floor. The figure provided illustrates the process of depth-guided back projection and fusion, where each grid represents a 2D world and different camera views contribute back-projected features. The depth priors introduce geometry awareness, resulting in sharper, more complete, and better separated objects in the reconstructed scene.