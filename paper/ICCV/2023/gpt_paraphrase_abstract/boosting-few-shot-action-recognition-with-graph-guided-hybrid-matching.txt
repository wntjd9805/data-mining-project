Class prototype construction and matching are important components of few-shot action recognition. Previous methods have focused on developing modules for modeling spatiotemporal relations or complex algorithms for temporal alignment. However, these methods have overlooked the significance of class prototype construction and matching, resulting in unsatisfactory performance when recognizing similar categories in different tasks. In this study, we introduce GgHM, a novel framework that utilizes Graph-guided Hybrid Matching. Specifically, we utilize a graph neural network to guide the construction of task-oriented features, thereby optimizing the correlation between intra- and inter-class features. Additionally, we propose a hybrid matching strategy that combines frame-level and tuple-level matching to classify videos with diverse styles. To further enhance the temporal representation of video features, we introduce a learnable dense temporal modeling module, which provides a stronger foundation for the matching process. Experimental results on various few-shot datasets consistently demonstrate that GgHM outperforms other challenging baselines, highlighting the effectiveness of our approach. The code for GgHM will be made publicly available at https://github.com/jiazheng-xing/GgHM.