We propose a new method for simultaneous localization and mapping (SLAM) using monocular RGBD input. Our approach involves creating a neural scene representation in the form of a point cloud, which is generated iteratively based on the input data. Unlike other neural SLAM methods that use a sparse grid, our approach allows for dynamic adaptation of the density of anchor points based on the information density of the input. This reduces runtime and memory usage in less detailed regions while focusing more points on resolving fine details. Our method achieves better or comparable results in tracking, mapping, and rendering accuracy compared to existing dense neural RGBD SLAM methods on various datasets. The source code for our approach is available at the given link.