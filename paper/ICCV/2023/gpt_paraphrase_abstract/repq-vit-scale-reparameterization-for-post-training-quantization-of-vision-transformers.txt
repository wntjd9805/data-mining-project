Post-training quantization (PTQ) is a practical method for compressing models that requires minimal data for calibration. However, existing PTQ schemes for vision transformers (ViTs) often suffer from significant accuracy degradation, especially in low-bit cases. To address this issue, we propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale reparameterization. RepQ-ViT separates the quantization and inference processes, using complex quantizers for quantization and scale-reparameterized simplified quantizers for inference. This approach ensures accurate quantization without sacrificing inference efficiency, distinguishing it from existing methods that compromise quantization performance to match specific hardware.   Our focus is on two components with extreme distributions: post-LayerNorm activations with varying inter-channel characteristics and post-Softmax activations with power-law features. We apply channel-wise quantization and log2 quantization to these components respectively. Then, we reparameterize the scales to enable hardware-friendly layer-wise quantization and log2 quantization for inference, with minimal impact on accuracy or computational costs.   We conduct extensive experiments on multiple vision tasks using different model variants, and the results demonstrate that RepQ-ViT outperforms existing strong baselines. Importantly, it significantly improves the accuracy of 4-bit PTQ of ViTs to a usable level without the need for hyperparameters or expensive reconstruction procedures. The code for RepQ-ViT is available at https://github.com/zkkli/RepQ-ViT.