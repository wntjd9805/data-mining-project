While CLIP and similar models have been utilized for various video-level tasks on edited videos, their application in temporal localization within unedited videos remains largely unexplored. To address this gap, we propose an innovative approach called UnLoc. This approach leverages pretrained image and text towers and employs a video-text fusion model that utilizes tokens. The fusion module generates an output used to construct a feature pyramid. Each level of the pyramid is connected to a head that predicts a relevancy score and start/end time displacements for each frame. Unlike previous methods, our architecture facilitates Moment Retrieval, Temporal Localization, and Action Segmentation using a single-stage model. Furthermore, our approach eliminates the need for action proposals, motion-based pretrained features, or representation masking. In contrast to specialized models, our unified approach achieves state-of-the-art results across all three localization tasks. The code for our approach is available at: https://github.com/google-research/scenic.