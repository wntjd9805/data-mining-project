Neural Radiance Fields (NeRF) is a powerful method for rendering 3D representations from multiple images. However, it struggles with blurry input, which is common in real-world scenarios. To address this, we propose Event-Enhanced NeRF (E2NeRF), which combines data from a bio-inspired event camera and an RGB camera. We introduce a blur rendering loss and an event rendering loss to incorporate the event stream into the learning process. Additionally, we develop a camera pose estimation framework guided by the event stream to make the method applicable to practical situations. Unlike previous approaches, our framework effectively utilizes the relationship between events and images, resulting in both image deblurring and high-quality novel view image generation. Extensive experiments on synthetic and real-world data demonstrate the effectiveness of E2NeRF, particularly in complex and low-light scenes. Our code and datasets are publicly available.