Implicit neural representations have demonstrated their ability to model real-world 3D scenes effectively, particularly in novel view synthesis. This paper aims to address a more challenging scenario by simultaneously tackling joint scene novel view synthesis and editing using implicit neural scene representations. Current approaches in this area usually build separate networks for view synthesis and editing, limiting the modeling of interactions and correlations between these tasks, which is crucial for learning high-quality scene representations. To overcome this limitation, we propose a unified framework called Neural Radiance Field (NeRF) that effectively performs joint scene decomposition and composition for modeling real-world scenes. The decomposition stage focuses on learning disentangled 3D representations of different objects and the background, enabling scene editing. On the other hand, the composition stage models the entire scene representation for novel view synthesis. Our two-stage NeRF framework consists of a coarse stage that predicts a global radiance field as guidance for point sampling, and a fine-grained stage that performs scene decomposition using a novel one-hot object radiance field regularization module. Additionally, we employ pseudo supervision via in-painting to handle ambiguous background regions occluded by objects. The decomposed object-level radiance fields are then composed using activations from the decomposition module. Extensive quantitative and qualitative evaluations demonstrate the effectiveness of our method in both scene decomposition and composition, surpassing state-of-the-art methods in novel-view synthesis and editing tasks.