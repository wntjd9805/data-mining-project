Contrastive Language-Image Pre-training (CLIP) has become a popular method for training vision-language models. It has shown impressive performance on various tasks without any fine-tuning. However, fine-tuning is still necessary to fully utilize its potential, but it can be resource-intensive and unstable. Recent methods that try to avoid fine-tuning still require access to target task images. This paper introduces a different approach called SuS-X, which enables "name-only transfer" without intensive fine-tuning or labeled data. SuS-X achieves state-of-the-art zero-shot classification results on 19 benchmark datasets and also performs well in the training-free few-shot setting. The code for SuS-X is available at the provided GitHub link.