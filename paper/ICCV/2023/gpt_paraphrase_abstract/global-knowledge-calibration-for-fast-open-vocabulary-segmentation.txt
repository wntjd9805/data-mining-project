Advancements in pre-trained vision-language models have allowed for the segmentation of arbitrary concepts based solely on textual inputs, known as open-vocabulary semantic segmentation (OVS). However, existing OVS techniques face a challenge in terms of over-fitting on the observed base classes during training, resulting in poor generalization to unseen classes. To address this issue, recent studies have proposed using an additional frozen pre-trained CLIP for classification. However, this approach is computationally expensive as the CLIP vision encoder needs to be repeatedly processed for each mask, making it impractical for real-world applications. To tackle this challenge, our objective is to develop a fast OVS model that performs comparably or better without the computational burden of the CLIP image encoder during inference. To achieve this, we propose the core idea of preserving the generalizable representation during fine-tuning on known classes. We introduce a text diversification strategy that generates synonyms for each training category, preventing the learned representation from collapsing onto specific category names. Additionally, we employ a text-guided knowledge distillation method to preserve the generalizable knowledge of CLIP. Extensive experiments demonstrate that our proposed model achieves robust generalization performance across various datasets. Furthermore, we explore open-vocabulary video segmentation and present a benchmark for future research in this domain. Figure 1 illustrates the performance of different methods in relation to computational cost. Some methods introduce an extra frozen CLIP during inference to avoid overfitting to seen categories, but this leads to heavy computational overhead. In comparison, our method generalizes well on both seen and unseen categories with a much smaller computational cost.