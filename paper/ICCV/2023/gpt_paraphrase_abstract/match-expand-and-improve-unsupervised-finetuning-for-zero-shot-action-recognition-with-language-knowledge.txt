We propose an unsupervised approach for zero-shot and few-shot action recognition in large scale Vision Language (VL) models. These models have been successful in aligning visual and text representations, but they tend to focus more on objects and less on verbs. Our approach involves adapting a VL model using unlabeled videos and an unpaired action dictionary. We use Large Language Models and VL models to create a text bag for each video, and then use Multiple Instance Learning to adapt an image-text backbone to video data. Despite being finetuned on unlabeled video data, our models show high transferability to unseen zero-shot tasks, improving the performance of the base VL model by up to 14%. Our models even perform favorably compared to fully-supervised baselines in both zero-shot and few-shot video recognition transfer. The code for our approach is available at https://github.com/wlin-at/MAXI.