Large-scale Vision-Language Models, such as CLIP, have been successful in learning effective image-text representations with various applications. However, compared to large language models like GPT-3, their ability to solve new discriminative tasks through prompting is limited. To address this, we propose the concept of visual prompt engineering, which involves editing images instead of using text prompts to solve computer vision tasks. Through experimentation, we discover that CLIP has an inherent capability to focus its attention on specific regions of an image by drawing a red circle around an object, while still considering global information. We demonstrate the effectiveness of this approach by achieving state-of-the-art results in zero-shot referring expressions comprehension and significant performance improvements in keypoint localization tasks. Additionally, we raise awareness about potential ethical concerns associated with large language-vision models.