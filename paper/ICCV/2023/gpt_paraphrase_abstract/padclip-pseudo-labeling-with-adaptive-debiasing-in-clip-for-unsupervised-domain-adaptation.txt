Traditional Unsupervised Domain Adaptation (UDA) is a method used to address learning tasks in an unlabeled target domain by leveraging labeled data from a source domain. However, when there is a large domain gap between the source and target domains, UDA becomes more challenging. To overcome this, a practical approach is to use a large-scale pre-trained model, such as CLIP, which has shown promising zero-shot generalizability. However, when fine-tuning CLIP specifically for a target domain, it suffers from catastrophic forgetting, where new domain knowledge overrides its pre-trained knowledge and reduces accuracy by half. To mitigate this issue, we propose a method called Catastrophic Forgetting Measurement (CFM) that adjusts the learning rate to prevent excessive training. Additionally, we introduce PADCLIP, which combines CLIP's zero-shot prediction with Adaptive Debiasing and causal inference adjusted with momentum and CFM. PADCLIP enables end-to-end training on both the source and target domains without extra overhead. We achieved the best results on four public datasets, with a significant improvement of 18.5% accuracy on DomainNet.