The success of Vision Transformers (ViTs) in image tasks has led to research on adapting them for video tasks. However, there is a significant gap between image and video, making it challenging for ViTs to learn spatiotemporal information. While video-specialized models like UniFormer can transfer to the video domain more seamlessly, their unique architectures require lengthy image pretraining, which limits scalability. To leverage the potential of powerful open-source image ViTs for video understanding, we propose UniFormerV2, which combines the concise style of the UniFormer block with redesigned local and global relation aggregators that integrate advantages from both ViTs and UniFormer. Our UniFormerV2 achieves state-of-the-art performance on 8 popular video benchmarks, including Kinetics-400/600/700, Moments in Time, Something-SomethingV1/V2, ActivityNet, and HACS. Notably, UniFormerV2 is the first model to achieve 90% top-1 accuracy on Kinetics-400, to the best of our knowledge.