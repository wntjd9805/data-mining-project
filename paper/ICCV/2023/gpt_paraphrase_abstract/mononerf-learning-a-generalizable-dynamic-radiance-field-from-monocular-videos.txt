This paper addresses the problem of learning a dynamic radiance field from monocular videos. Unlike existing methods that rely on multiple views, monocular videos only have one view per timestamp, leading to ambiguity in estimating point features and scene flows. Previous approaches, such as DynNeRF, use positional encoding to disambiguate point features, but this limits generalization and requires training separate models for each scene. To overcome these limitations, the authors propose MonoNeRF, which simultaneously learns point features and scene flows by incorporating point trajectory and feature correspondence constraints across frames. They use a Neural ODE to estimate point trajectory from temporal features and a flow-based feature aggregation module to obtain spatial features along the trajectory. Temporal and spatial features are jointly optimized in an end-to-end manner. Experimental results demonstrate the effectiveness of MonoNeRF in learning from multiple scenes and supporting applications like scene editing, unseen frame synthesis, and fast novel scene adaptation. The code for MonoNeRF is available at https://github.com/tianfr/MonoNeRF.