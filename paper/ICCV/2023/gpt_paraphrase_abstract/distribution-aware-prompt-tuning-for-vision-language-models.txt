Pre-trained vision-language models (VLMs) have achieved impressive results on various tasks by leveraging knowledge obtained from large datasets. However, the performance of VLMs can be further enhanced by prompt tuning, which involves adding context to the input image or text. Prompt tuning methods have been extensively studied in the literature, aiming to align the feature space between different modalities using learnable vectors and fixed model parameters. Our observation suggests that the alignment is more effective when the embeddings of each modality are well-organized in the latent space. Based on this insight, we propose a simple yet effective approach called distribution-aware prompt tuning (DAPT) for vision-language models. DAPT learns prompts by maximizing the inter-dispersion, which measures the distance between classes, while minimizing the intra-dispersion, which quantifies the distance between embeddings from the same class. Our extensive experiments on 11 benchmark datasets demonstrate that DAPT significantly improves the generalizability of VLMs. The code for implementing DAPT can be found at https://github.com/mlvlab/DAPT.