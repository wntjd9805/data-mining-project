This paper focuses on cross-domain face reenactment, which involves driving a cartoon image with a real video and vice versa. While previous works have focused on within-domain reenactment, where a portrait is driven by a real video, applying those methods directly to cross-domain animation results in inaccurate expression transfer, blurring effects, and apparent artifacts due to the differences between cartoon and real faces. Only a few works have attempted cross-domain face reenactment, such as AnimeCeleb, which requires constructing a dataset with pose vector and cartoon image pairs through animating 3D characters, making it impractical without paired data. To address this issue, we propose a novel method for cross-domain reenactment without paired data. Our approach involves a transformer-based framework that aligns motions from different domains into a common latent space. Motion transfer is then conducted by adding latent codes. We use two domain-specific motion encoders and two learnable motion base memories to capture domain properties. A source query transformer and a driving transformer are utilized to project domain-specific motion to a canonical space and then back to the source domain. Since paired data is not available, we introduce a novel cross-domain training scheme using data from both domains with a designed analogy constraint. Additionally, we contribute a cartoon dataset in Disney style. Extensive evaluations demonstrate the superiority of our method compared to competing methods.