Text-to-image diffusion models often have implicit assumptions about the world when generating images. These assumptions can be useful but can also be outdated, incorrect, or reflect social biases in the training data. Therefore, it is important to control these assumptions without explicit user input or expensive re-training. This study proposes a method called Text-to-Image Model Editing (TIME) that aims to edit implicit assumptions in a pre-trained diffusion model. TIME takes a pair of inputs: a "source" prompt with under-specified information that the model assumes (e.g., "a pack of roses"), and a "destination" prompt that describes the same scene but with a specified desired attribute (e.g., "a pack of blue roses"). By updating the cross-attention layers of the model, which assign visual meaning to textual tokens, the method modifies the projection matrices so that the source prompt is projected closer to the destination prompt. This editing process only modifies a small percentage of the model's parameters and is highly efficient, taking less than one second. To evaluate the effectiveness of model editing approaches, the study introduces the TIME Dataset (TIMED), which consists of 147 pairs of source and destination prompts from different domains. Experimental results using Stable Diffusion demonstrate that TIME successfully edits the model, generalizes well to unseen prompts, and has minimal impact on unrelated image generations.