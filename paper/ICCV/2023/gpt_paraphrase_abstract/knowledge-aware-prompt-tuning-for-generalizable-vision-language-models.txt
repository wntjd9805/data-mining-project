A novel framework called Knowledge-Aware Prompt Tuning (KAPT) is proposed in this paper for improving the performance of vision-language models. These models, such as CLIP, have shown impressive transfer learning capabilities when working with manually designed prompts. However, previous approaches that use learnable prompts tend to overfit to seen classes and struggle to generalize to unseen classes.Taking inspiration from human intelligence, the KAPT framework incorporates external knowledge into the vision-language models to better recognize novel categories of objects. Two types of knowledge-aware prompts are designed for the text encoder: a discrete prompt that extracts key information from object category descriptions, and a learned continuous prompt that captures overall contexts. This allows the model to leverage the distinctive characteristics of category-related external knowledge.Additionally, an adaptation head is designed for the visual encoder to aggregate salient attentive visual cues. This helps establish discriminative and task-aware visual representations. The effectiveness of KAPT is validated through extensive experiments on 11 widely-used benchmark datasets. The results demonstrate its superiority in few-shot image classification, particularly in generalizing to unseen categories.Compared to the state-of-the-art Co-CoOp method, KAPT achieves better performance with an absolute gain of 3.22% on new classes and 2.57% in terms of harmonic mean. Overall, the KAPT framework improves the transfer learning capabilities of vision-language models by incorporating external knowledge and designing effective prompts.