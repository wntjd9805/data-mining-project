Cutting-edge visual grounding models have shown impressive accuracy in object detection, but they lack the ability to distinguish between all objects and only specific objects of interest. In human language, determiners like "my", "either", and "those" are used to specify particular objects or sets of objects. Determiners are a crucial part of natural language that provide information about the reference or quantity of a noun. However, existing grounded referencing datasets do not give enough importance to determiners compared to other word classes like nouns, verbs, and adjectives. Consequently, developing models that fully comprehend the complexity of object referencing becomes challenging. To address this limitation, we have introduced the DetermiNet dataset, consisting of 250,000 artificially generated images and captions that revolve around 25 determiners. In this dataset, the task is to predict bounding boxes that identify objects of interest based on the semantics of the given determiner. Our experiments reveal that current state-of-the-art visual grounding models struggle to perform well on this dataset, which highlights their limitations in reference and quantification tasks.