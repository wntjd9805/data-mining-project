This study introduces Vision Middleware (ViM), a novel learning approach that enables seamless transfer of a single foundation model to various downstream tasks. ViM comprises a collection of lightweight plug-in modules, each independently trained on a midstream dataset with a shared frozen backbone. By leveraging the knowledge gained from midstream tasks, ViM facilitates effective integration of the module zoo, benefiting downstream tasks. This design offers three key advantages. Firstly, it enhances efficiency by enabling the reuse of the upstream backbone without the need for retraining. Secondly, it ensures scalability by allowing the addition of new modules without impacting existing ones. Lastly, ViM can incorporate numerous midstream tasks, reducing the gap between upstream and downstream performance. With these advantages in mind, ViM has the potential to serve as a robust tool for assisting foundation models. Collaboration within the community can further enhance and maintain ViM's capabilities.