Event cameras have distinct advantages over standard cameras, such as low power consumption, low latency, high temporal resolution, and high dynamic range. However, the success of visual applications using event cameras relies on an efficient scene representation, with Neural Radiance Field (NeRF) being the preferred method. Recent studies have explored the reconstruction of NeRF from moving event cameras, but they have limitations in terms of dependence on dense and low-noise event streams, as well as generalization to different contrast threshold values and camera speeds.   To address these limitations, we propose a novel method called Robust e-NeRF. This method allows for the direct and robust reconstruction of NeRFs from moving event cameras under various real-world conditions, including sparse and noisy events generated during non-uniform motion. It comprises two main components: a realistic event generation model that considers intrinsic parameters (such as time independence, asymmetric threshold, and refractory period) and non-idealities (like pixel-to-pixel threshold variation), as well as a pair of normalized reconstruction losses that can effectively generalize to arbitrary speed profiles and intrinsic parameter values without prior knowledge.   We conducted experiments using real and realistically simulated sequences to validate the effectiveness of our approach. Additionally, we have made our code, synthetic dataset, and improved event simulator publicly available.