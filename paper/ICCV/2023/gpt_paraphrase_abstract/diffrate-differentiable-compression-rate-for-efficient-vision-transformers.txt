Token compression is a technique used to enhance the speed of large-scale vision transformers by reducing or combining tokens. However, determining the optimal compression rate has been a challenging task, requiring manual adjustment and resulting in subpar performance. To address this issue, we propose a novel method called Differentiable Compression Rate (DiffRate). DiffRate enables the gradient of the loss function to be applied to the compression ratio, which was previously considered a non-differentiable hyperparameter. This allows each layer to learn its own compression rate automatically, without any extra computational burden. Additionally, DiffRate allows for simultaneous token pruning and merging, whereas previous methods treated these processes separately. Our extensive experiments demonstrate that DiffRate achieves state-of-the-art performance. For instance, when applied to an off-the-shelf ViT-H (MAE) model, DiffRate reduces FLOPs by 40% and improves throughput by 1.5Ã—, with only a minor accuracy drop of 0.16% on ImageNet without fine-tuning. It even outperforms previous methods that utilized fine-tuning. The codes and models for DiffRate can be found at https://github.com/OpenGVLab/DiffRate.