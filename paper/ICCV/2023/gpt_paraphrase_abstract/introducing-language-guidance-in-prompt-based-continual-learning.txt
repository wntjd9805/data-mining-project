Continual Learning aims to learn a single model on a sequence of tasks without access to data from previous tasks. The main challenge in this field is catastrophic forgetting, which refers to a decline in performance on previously seen tasks. Some existing methods address this issue by using a replay buffer to store data from previous tasks. However, this approach becomes costly when the number of tasks is large or when data privacy is a concern. Alternatively, prompt-based methods have been proposed, which store task information in a learnable prompt pool. This prompt pool guides a frozen image encoder on how to solve each task. In our work, we argue that the classes encountered in each task can be encoded in the same embedding space as a pre-trained language encoder. To address this, we introduce Language Guidance for Prompt-based Continual Learning (LGCL) as an extension for prompt-based methods. LGCL is compatible with any model and incorporates language guidance at both the task and class levels. Through extensive experimentation, we consistently demonstrate that LGCL significantly improves the performance of prompt-based continual learning methods, surpassing the current state-of-the-art. Remarkably, these improvements are achieved without requiring any additional learnable parameters.