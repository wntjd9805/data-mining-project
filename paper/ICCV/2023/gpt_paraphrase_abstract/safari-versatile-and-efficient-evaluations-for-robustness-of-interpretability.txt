The interpretability of deep learning poses a challenge for trustworthy artificial intelligence (AI). While Explainable AI (XAI) techniques have been developed to address this issue, they often lack robustness, as small changes in input can lead to different explanations. Therefore, it is crucial to assess the robustness of DL interpretability methods. This paper identifies three main challenges: existing metrics are not comprehensive, XAI techniques are highly diverse, and misinterpretations are rare occurrences. To overcome these challenges, the authors propose two black-box evaluation methods. The first method focuses on worst-case interpretation discrepancies and uses a genetic algorithm with a customized fitness function for efficient evaluation. The second method employs Subset Simulation to estimate the overall robustness, particularly for rare events. Experimental results demonstrate that the proposed methods outperform existing approaches in terms of accuracy, sensitivity, and efficiency. Furthermore, the authors showcase two applications of their methods: ranking robust XAI techniques and selecting training schemes to improve both classification and interpretation robustness.