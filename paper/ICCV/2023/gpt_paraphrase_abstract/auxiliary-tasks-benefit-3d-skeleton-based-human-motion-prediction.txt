This study addresses the challenge of predicting human motion by exploring spatial-temporal dependencies in observed motions. Previous methods have focused on network structures to model these dependencies, but this paper introduces a new approach using a model learning framework with auxiliary tasks. The auxiliary tasks involve corrupting the coordinates of partial body joints through masking or adding noise, and the objective is to recover these corrupted coordinates based on the remaining coordinates. To handle these auxiliary tasks, the paper proposes a novel auxiliary-adapted transformer that can handle incomplete and corrupted motion data and recover coordinates by capturing spatial-temporal dependencies. By incorporating auxiliary tasks, the auxiliary-adapted transformer is able to capture more comprehensive spatial-temporal dependencies among body joint coordinates, resulting in improved feature learning. Extensive experiments demonstrate that this method outperforms state-of-the-art methods by significant margins in terms of 3D mean per joint position error on multiple datasets. The method is also shown to be more robust in cases of missing data and noisy data. The code for this method is available at the provided GitHub link.