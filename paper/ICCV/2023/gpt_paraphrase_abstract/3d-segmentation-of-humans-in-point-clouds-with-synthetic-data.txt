The increasing relevance of human-centered robotics and AR/VR applications has highlighted the need for segmenting humans in 3D indoor scenes. We propose a comprehensive approach that tackles the joint tasks of 3D human semantic segmentation, instance segmentation, and multi-human body-part segmentation. Previous studies have largely avoided directly segmenting humans in cluttered 3D scenes due to the lack of annotated training data featuring humans interacting with such scenes. To overcome this challenge, we present a framework for generating synthetic training data that involves synthetic humans interacting with real 3D scenes. Additionally, we introduce a novel transformer-based model called Human3D, which is the first end-to-end model capable of segmenting multiple human instances and their body-parts in a unified manner. The synthetic data generation framework offers the advantage of generating diverse and realistic human-scene interactions with highly accurate ground truth. Our experiments demonstrate that pre-training on synthetic data enhances performance across various 3D human segmentation tasks. Furthermore, we showcase that Human3D surpasses the performance of state-of-the-art 3D segmentation methods specific to individual tasks.