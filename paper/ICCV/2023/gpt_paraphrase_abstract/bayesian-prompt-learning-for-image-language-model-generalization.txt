The use of foundational image-language models in downstream tasks has gained significant attention due to their effective adaptation through prompt learning. Prompt learning involves treating a portion of the language model input as trainable while keeping the rest fixed, optimizing an Empirical Risk Minimization objective. However, Empirical Risk Minimization is susceptible to distributional shifts, which can negatively impact the model's ability to generalize to unseen prompts during training. To address this issue, we propose a Bayesian approach to prompt learning, leveraging the regularization capabilities of Bayesian methods and formulating prompt learning as a variational inference problem. Our approach focuses on regularizing the prompt space, reducing overfitting to seen prompts, and enhancing prompt generalization for unseen prompts. We implement our framework by modeling the input prompt space probabilistically, introducing an a priori distribution that is compatible with both unconditional and conditional prompt learning approaches. Through empirical evaluations on 15 benchmarks, we demonstrate that Bayesian prompt learning effectively covers the prompt space, prevents the learning of irrelevant features, and leverages transferable invariant features. This leads to improved generalization for unseen prompts, even across different datasets and domains. The code for our proposed method is available at the following GitHub repository: https://github.com/saic-fi/Bayesian-Prompt-Learning.