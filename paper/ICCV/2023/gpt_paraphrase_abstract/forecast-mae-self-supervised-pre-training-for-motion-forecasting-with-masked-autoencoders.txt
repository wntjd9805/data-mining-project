This study investigates the use of self-supervised learning (SSL) in motion forecasting, a field that has not been extensively studied despite the success of SSL in computer vision and natural language processing. To fill this research gap, we propose a new framework called Forecast-MAE, which is a modified version of the mask autoencoders framework specifically designed for self-supervised learning of motion forecasting. Our approach includes a unique masking strategy that takes advantage of the strong connections between agents' trajectories and road networks. It involves masking agents' future or history trajectories in a complementary manner and randomly masking lane segments. We conduct experiments on the challenging Argoverse 2 motion forecasting benchmark and find that Forecast-MAE, which uses standard Transformer blocks with minimal bias, achieves competitive performance compared to state-of-the-art supervised learning methods with sophisticated designs. Additionally, it outperforms the previous self-supervised learning method by a significant margin. The code for our approach is available at https://github.com/jchengai/forecast-mae.