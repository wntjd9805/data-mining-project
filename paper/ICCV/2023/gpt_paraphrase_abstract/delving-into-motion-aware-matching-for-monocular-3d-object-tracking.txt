This paper introduces MoMA-M3T, a framework for 3D multi-object tracking using low-cost camera sensors. The framework addresses the lack of exploration of the motion cue of objects over time in existing monocular-based approaches. MoMA-M3T consists of three motion-aware components. Firstly, it represents the possible movement of an object in relation to all object tracklets as motion features in the feature space. Secondly, it models the historical object tracklet in a spatial-temporal perspective using a motion transformer. Lastly, it proposes a motion-aware matching module to associate historical object tracklets with current observations for tracking. The framework is evaluated on the nuScenes and KITTI datasets, demonstrating competitive performance compared to state-of-the-art methods. Additionally, MoMA-M3T is flexible and can be easily integrated into existing image-based 3D object detectors without the need for re-training. The code and models for MoMA-M3T are available at the provided GitHub link.