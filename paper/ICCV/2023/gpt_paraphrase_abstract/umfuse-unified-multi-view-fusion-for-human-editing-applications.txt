Numerous methods have been developed to edit human poses in images, but most of them rely on a single image as input, which can be problematic when the target pose is different from the input pose. To address this issue, we propose a new approach that utilizes multiple views of the human model to generate more accurate edited images. We design a multi-view fusion network that combines pose key points and texture information from multiple source images, resulting in an explainable per-pixel appearance retrieval map. We also incorporate encodings from a separate network trained on a single-view human reposing task to enhance the accuracy and coherence of the generated images. We demonstrate the effectiveness of our approach on two new editing tasks - multi-view human reposing and mix-and-match human image generation. We also investigate the limitations of single-view editing and highlight the advantages of using multiple views. More details and results can be found on our project webpage.