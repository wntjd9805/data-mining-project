Current knowledge distillation methods are effective for transferring knowledge from a teacher network to a student network in multi-class single-label learning. However, these methods are not suitable for multi-label learning scenarios where instances are associated with multiple semantic labels. This is because the prediction probabilities do not sum to one and minor classes may be ignored in the feature maps of the entire example. In this study, we introduce a new method for multi-label knowledge distillation. It addresses the issue by dividing the multi-label learning problem into binary classification problems and utilizing the informative semantic knowledge from the logits. Additionally, the method enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on various benchmark datasets demonstrate that the proposed method outperforms other existing methods by avoiding knowledge counteraction among labels. The code for the proposed method is available at the provided GitHub link.