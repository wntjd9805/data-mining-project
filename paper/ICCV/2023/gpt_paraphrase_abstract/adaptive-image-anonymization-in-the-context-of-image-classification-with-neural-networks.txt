Deep learning methods are widely used in computer vision tasks, but they are vulnerable to adversarial attacks, particularly in the case of image anonymization. Anonymizing images is important for preserving privacy and securing personal information. However, current anonymization techniques can cause classifiers to provide different class decisions before and after anonymization, reducing their reliability and usability. To address this problem, we propose a novel anonymization procedure that ensures classifiers remain invariant to class decision changes on anonymized images without modifying the classification models. We conducted experiments on the ImageNet benchmark and a large-scale industrial toll classification dataset, showing that our method achieved 0% rate of class decision change compared to 15.95% on ImageNet and 0.18% on the toll dataset using na√Øve anonymization approaches. Our approach also demonstrates potential for similar problems in different domains.