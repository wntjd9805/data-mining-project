The paper introduces ViewRefer, a multi-view framework for 3D visual grounding that addresses the view discrepancy issue by incorporating view cues from both text and 3D modalities. To leverage linguistic knowledge, the text branch of ViewRefer utilizes large-scale language models like GPT to generate multiple geometry-consistent descriptions for a single grounding text. In the 3D modality, a transformer fusion module with inter-view attention is introduced to enhance the interaction of objects across views. Additionally, the framework incorporates learnable multi-view prototypes to capture scene-agnostic knowledge for different views. These prototypes enhance the framework through a view-guided attention module for more robust text features and a view-guided scoring strategy during final prediction. The proposed paradigm outperforms existing methods, achieving superior performance on three benchmarks (Sr3D, Nr3D, and ScanRefer) with improvements of +2.8%, +1.5%, and +1.35%, respectively.