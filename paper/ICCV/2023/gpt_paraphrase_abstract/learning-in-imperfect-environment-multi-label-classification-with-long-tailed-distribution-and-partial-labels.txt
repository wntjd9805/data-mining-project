Conventional methods for multi-label classification (MLC) assume that all samples are fully labeled and have the same distribution. However, this assumption is not realistic for large-scale MLC data that has a long-tailed distribution and partial labels. To address this problem, we propose a new task called Partial Labeling and Long-Tailed Multi-Label Classification (PLT-MLC) to consider these imperfect learning environments together. We observe that existing LT-MLC and PL-MLC approaches fail to solve PLT-MLC, resulting in a significant decline in performance on our proposed PLT-MLC benchmarks. Therefore, we present an end-to-end learning framework called COMIC (Correction → Modification → Balance) to address these challenges.The COMIC framework employs a bootstrapping philosophy to simultaneously correct missing labels with confident predictions and use these recalled labels during training. We introduce a novel multi-focal modifier loss that addresses head-tail imbalance and positive-negative imbalance, modifying attention for different samples based on the long-tailed class distribution. Additionally, we develop a balanced training strategy by distilling the model's learning effect from head and tail samples, resulting in a balanced classifier that maintains stable performance for all samples.Our experimental study demonstrates that COMIC outperforms general MLC, LT-MLC, and PL-MLC methods in terms of effectiveness and robustness on our newly created PLT-MLC datasets. The codes and benchmarks for COMIC are available at the following link: https://github.com/wannature/COMIC.