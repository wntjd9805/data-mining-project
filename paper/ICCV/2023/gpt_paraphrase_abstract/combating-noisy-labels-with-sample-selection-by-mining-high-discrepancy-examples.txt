The use of sample selection in learning with noisy labels is popular, where two deep networks are trained simultaneously to leverage their different learning abilities. However, maintaining divergence between the networks is crucial to prevent them from converging to a consensus. Previous methods achieved this by identifying data where the prediction labels of the two networks differ. However, this approach is not efficient for generalization as it only utilizes a small number of clean examples for training. To address this issue, we propose a simple yet effective method called CoDis. Our approach selects possibly clean data that exhibit high-discrepancy prediction probabilities between the two networks. By training on these selected data points with high probability discrepancies, we are able to maintain the divergence between the networks. Additionally, our method is more sample-efficient compared to previous approaches as the condition of high discrepancies is less strict than complete disagreement, allowing for more data to be considered for training. Furthermore, our proposed method enables the mining of hard clean examples to enhance generalization. Empirical results demonstrate the superiority of CoDis over multiple baselines in terms of the robustness of trained models.