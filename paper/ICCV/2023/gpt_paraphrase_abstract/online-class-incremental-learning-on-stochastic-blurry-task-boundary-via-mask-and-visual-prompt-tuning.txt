Continual learning aims to acquire knowledge from a continuous flow of data, but it typically assumes a fixed number of data and tasks with well-defined task boundaries. However, in real-world situations, the number of input data and tasks constantly changes in a statistical manner rather than a static one. While incremental learning scenarios with blurry task boundaries have been introduced to address these issues to some extent, they do not fully capture the statistical properties of real-world situations due to their fixed ratio of disjoint and blurry samples.  In this study, we propose a new scenario called Si-Blurry, which introduces stochastic properties to incremental learning with blurry task boundaries, aligning it more closely with real-world situations. We identify two major challenges in the Si-Blurry scenario: intra- and inter-task forgettings and the class imbalance problem. To tackle these challenges, we present a novel approach called Mask and Visual Prompt tuning (MVP).  To address the issues of intra- and inter-task forgetting, MVP incorporates instance-wise logit masking and contrastive visual prompt tuning loss. These techniques enable the model to discern the classes to be learned in the current batch while consolidating previous knowledge. Additionally, to mitigate the class imbalance problem, we introduce a gradient similarity-based focal loss and adaptive feature scaling. These methods help to reduce overfitting to major classes and underfitting to minor classes.  Extensive experiments demonstrate that our proposed MVP approach outperforms existing state-of-the-art methods in the challenging Si-Blurry scenario. The code for implementing our approach is publicly available at https://github.com/moonjunyyy/Si-Blurry.