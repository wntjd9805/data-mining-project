Diffusion-based models have become highly effective in text-to-image synthesis tasks. However, one major drawback of these models is that the generated images often lack accuracy in relation to the text description. This includes issues like missing objects, attributes that don't match, and objects being incorrectly positioned. The main cause of these inconsistencies lies in the inaccurate cross-attention to text, both in terms of spatial dimension (determining where an object should appear in the image) and temporal dimension (controlling the level of detail added during the denoising process). To address this problem, our paper introduces a new text-to-image algorithm that enhances control over spatial-temporal cross-attention in diffusion models. The algorithm begins by utilizing a layout predictor to determine the pixel regions where objects mentioned in the text should be placed. Spatial attention control is then imposed by combining attention over the complete text description with attention over the local description of the specific object in its corresponding pixel region. Additionally, temporal attention control is incorporated by allowing the combination weights to vary at each denoising step, and these weights are optimized to ensure high fidelity between the image and the text. Experimental results demonstrate that our method generates images with significantly greater fidelity compared to diffusion-model-based baselines, even without fine-tuning the diffusion model. Furthermore, we have made our code publicly available for others to use.