Diffusion models have been successful in generating high-quality images, but adapting them to new domains remains challenging. This paper introduces DiffFit, a parameter-efficient strategy for fine-tuning large pre-trained diffusion models to enable quick adaptation to new domains. DiffFit is a simple approach that only fine-tunes the bias term and newly-added scaling factors in specific layers. Despite its simplicity, DiffFit significantly speeds up training and reduces model storage costs. It achieves a 2x training speed-up compared to full fine-tuning and requires storing only approximately 0.12% of the total model parameters. The effectiveness of scaling factors on fast adaptation is theoretically analyzed. On eight downstream datasets, DiffFit performs competitively or even outperforms full fine-tuning while being more efficient. Notably, DiffFit demonstrates the ability to adapt a pre-trained low-resolution generative model to a high-resolution one with minimal cost. Among diffusion-based methods, DiffFit achieves a new state-of-the-art FID (Fr√©chet Inception Distance) of 3.02 on the ImageNet 512x512 benchmark by fine-tuning for only 25 epochs from a publicly available pre-trained ImageNet 256x256 checkpoint. It is also 30x more training efficient than the closest competitor.