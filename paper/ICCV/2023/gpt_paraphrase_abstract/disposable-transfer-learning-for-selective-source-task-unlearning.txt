Transfer learning is commonly used to train deep neural networks (DNN) and develop powerful representations. Even after adapting a pre-trained model for a specific task, the performance of the feature extractor is still somewhat retained. Since the performance of the pre-trained model is considered the owner's private property, it is natural to seek exclusive rights to the generalized performance of the pre-trained weights. To address this, we propose a new approach to transfer learning called disposable transfer learning (DTL), which discards only the source task without compromising the performance of the target task. To achieve this, we introduce a novel loss function called Gradient Collision loss (GC loss). GC loss selectively eliminates the source knowledge by directing the gradient vectors of mini-batches in different directions. The success of unlearning the source task is measured using piggyback learning accuracy (PL accuracy), which estimates the vulnerability of knowledge leakage by retraining the cleaned model on a subset of source data or new downstream data. We demonstrate the effectiveness of GC loss in solving the DTL problem by showing that the model trained with GC loss maintains performance on the target task while significantly reducing PL accuracy.