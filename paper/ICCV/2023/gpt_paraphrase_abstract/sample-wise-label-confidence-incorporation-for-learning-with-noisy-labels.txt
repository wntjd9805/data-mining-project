Deep learning algorithms require large amounts of labeled data for optimal performance. However, the presence of noisy labels can significantly impair their effectiveness. Previous research has introduced the robust loss method as a solution to handle label noise, but this approach suffers from underfitting both noisy and clean samples, resulting in suboptimal model performance. To overcome this challenge, we propose a novel learning framework that selectively suppresses noisy samples while avoiding underfitting clean data. Our framework incorporates label confidence, which serves as a measure of label noise, enabling the network model to prioritize the training of noise-free samples. We utilize robust loss methods to estimate label confidence and present theoretical evidence that our method can achieve the optimal point of the robust loss under certain conditions. Additionally, our approach is versatile and can be combined with existing robust loss methods, making it suitable for various applications of learning with noisy labels. We assess the effectiveness of our approach using synthetic and real-world datasets, and the experimental results demonstrate its superiority in achieving exceptional classification performance compared to state-of-the-art methods.