Current methods for model quantization have been successful in reducing storage space and computational complexity. However, these methods often require repeated optimization for different hardware scenarios, limiting their flexibility. This paper introduces Elastic Quantization Neural Networks (EQ-Net), a one-shot network quantization approach that trains a weight-sharing quantization supernet. EQ-Net utilizes an elastic quantization space that can adapt to various quantization forms, including bit-width, granularity, and symmetry. To address the inconsistency between weight and output logits distributions, two regularization losses, Weight Distribution Regularization Loss (WDR-Loss) and Group Progressive Guidance Loss (GPG-Loss), are proposed. Additionally, genetic algorithms and the Conditional Quantization-Aware Accuracy Predictor (CQAP) are incorporated to efficiently search for mixed-precision quantized neural networks in the supernet. Extensive experiments demonstrate that EQ-Net performs on par with or even surpasses static counterparts and state-of-the-art bit-width methods. The code for EQ-Net is available at https://github.com/xuke225/EQ-Net.