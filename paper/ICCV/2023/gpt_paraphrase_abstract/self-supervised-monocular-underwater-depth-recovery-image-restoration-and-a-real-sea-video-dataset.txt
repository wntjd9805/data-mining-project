Underwater depth estimation and image restoration in underwater environments is a difficult task due to the lack of large-scale underwater-paired datasets and the inherent ill-posed nature of the problem. Previous attempts at underwater depth estimation have relied on either haze information, stereo images, or adjacent frames in videos. In order to improve the accuracy of depth estimation from a single underwater image, we propose a deep learning method that combines both haze and geometry information during training. By incorporating the physical model of underwater image formation and the view-synthesis constraint from neighboring frames in monocular videos, we disentangle the input image to estimate the scene radiance as well. Our proposed method, called USe-ReDI-Net, is completely self-supervised and can simultaneously output the depth map and restored image in real-time at a rate of 55 frames per second.To enable self-supervision in monocular settings, we have created a dataset called DRUVA (Dataset of Real-world Underwater Videos of Artifacts) by collecting video sequences of 20 different submerged artifacts in shallow sea waters. DRUVA is the first underwater video dataset to include almost full azimuthal coverage of each artifact. Through extensive experiments on both our DRUVA dataset and other underwater datasets, we have demonstrated the superiority of USe-ReDI-Net over previous methods for both underwater depth estimation and image restoration. The DRUVA dataset is publicly available at https://github.com/nishavarghese15/DRUVA.