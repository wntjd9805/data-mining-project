The use of multi-camera bird's-eye-view (BEV) representations for 3D perception is gaining popularity in the autonomous driving industry due to the cost-effectiveness of cameras. However, there is a noticeable difference in performance between multi-camera BEV and LiDAR-based 3D object detection. This is mainly because LiDAR provides accurate depth and geometry measurements, while inferring such 3D information from image input alone is challenging. This study proposes enhancing the representation learning of a multi-camera BEV-based detector by training it to mimic the features of a well-trained LiDAR-based detector. An effective balancing strategy is introduced to ensure that the student detector focuses on learning crucial features from the teacher detector, and knowledge transfer is generalized to multi-scale layers through temporal fusion. Extensive evaluations are conducted on various representative models of multi-camera BEV, and the experiments demonstrate that our approach significantly improves the performance of the student models, achieving state-of-the-art results on the popular benchmark nuScenes.