In this paper, we introduce a method called SurroundOcc for predicting 3D occupancy in autonomous driving scenarios using multi-camera images. While existing methods mainly focus on detecting 3D objects, they struggle to describe objects with arbitrary shapes and infinite classes. Our method aims to provide a more comprehensive understanding of the 3D scene.To achieve this, we first extract multi-scale features from each image and utilize spatial 2D-3D attention to transform them into the 3D volume space. We then employ 3D convolutions to progressively upsample the volume features and apply supervision at multiple levels. In order to generate dense occupancy predictions, we develop a pipeline that creates dense occupancy ground truth without requiring extensive occupancy annotations. This involves separately combining multi-frame LiDAR scans of dynamic objects and static scenes. We then use Poisson Reconstruction to fill in any holes and voxelize the mesh to obtain dense occupancy labels.We evaluate our method extensively on the nuScenes and SemanticKITTI datasets, and the results demonstrate the superiority of our approach. The code and dataset used in our experiments are publicly available at https://github.com/weiyithu/SurroundOcc.