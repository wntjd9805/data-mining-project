Despite the good performance of many image classifiers on average, their performance can significantly decline when faced with specific subgroups of data that were not well-represented in the training data. These systematic errors can have negative effects on fairness for minority groups and can also impact the robustness and safety of the classifiers under different conditions. The main challenge lies in identifying these underperforming subgroups when they are not specifically labeled and are rarely encountered. To address this challenge, we utilize recent advancements in text-to-image models and search for textual descriptions of subgroups ("prompts") that result in low performance from the target model on synthesized data conditioned by the prompts. To handle the increasing number of subgroups, we utilize combinatorial testing. We refer to this process as PROMPTATTACK, as it can be seen as an adversarial attack in the prompt space. In a controlled experiment, we analyze the coverage and detectability of subgroups using PROMPTATTACK and find that it accurately identifies systematic errors. We then apply PROMPTATTACK to ImageNet classifiers and discover new systematic errors on rare subgroups.