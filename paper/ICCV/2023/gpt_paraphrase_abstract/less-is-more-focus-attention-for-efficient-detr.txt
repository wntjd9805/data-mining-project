DETR-like models have greatly improved the performance of detectors and surpassed traditional convolutional models. However, treating all tokens equally in the encoder structure leads to unnecessary computational burden. Recent sparsification strategies aim to reduce attention complexity by focusing on informative tokens, but they rely on unreliable model statistics. Additionally, simply reducing the token population significantly impairs detection performance, limiting the usability of sparse models. In this study, we propose Focus-DETR, which prioritizes attention on more informative tokens to achieve a better balance between computational efficiency and model accuracy. Our approach involves reconstructing the encoder with dual attention, incorporating a token scoring mechanism that considers both object localization and category semantic information from multi-scale feature maps. We efficiently discard background queries and enhance the semantic interaction of fine-grained object queries based on the scores. Compared to state-of-the-art sparse DETR-like detectors under the same conditions, Focus-DETR achieves comparable complexity while achieving a 50.4AP (+2.2) on the COCO dataset. The code for our method is available in both torch-version† and mindspore-version‡.