Deep neural networks have made significant progress in various visual tasks and are now widely used in edge device software. However, when updating neural network-based software, users often have to download all the parameters of the network again, which negatively impacts user experience. To address this issue, we propose a new training methodology called Tiny Updater, inspired by advancements in model compression. Tiny Updater utilizes pruning and knowledge distillation techniques to update the software by downloading only a small percentage (10% to 20%) of the network's parameters, instead of all of them. We conducted experiments on eleven datasets across three tasks, including image classification, image-to-image translation, and video recognition, which demonstrated the effectiveness of Tiny Updater. The codes for Tiny Updater can be found at https://github.com/ArchipLab-LinfengZhang/TinyUpdater.