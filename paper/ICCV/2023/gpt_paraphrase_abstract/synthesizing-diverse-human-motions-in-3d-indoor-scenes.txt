We introduce a new approach for populating 3D indoor scenes with virtual humans that can realistically navigate and interact with objects. Existing methods rely on expensive and limited training data, making it difficult to capture the full range of human-scene interactions in complex environments. To overcome this, we propose a reinforcement learning-based approach that allows virtual humans to autonomously navigate and interact using learned motion control policies. These policies are based on realistic motion primitives learned from large-scale motion capture data. For navigation, we use a scene-aware policy with innovative state and reward designs for collision avoidance. By combining this with path-finding algorithms, we can generate diverse human motions in 3D scenes while avoiding obstacles. To facilitate fine-grained human-object interactions, we curate interaction goal guidance using a marker-based body representation and features based on the signed distance field. Our method can synthesize realistic and diverse human-object interactions, even in scenarios with different object shapes, orientations, starting positions, and poses. Experimental results show that our approach outperforms current methods in terms of motion naturalness and diversity. The code, models, and video results are available at the provided link.