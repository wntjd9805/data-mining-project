This paper discusses the need for a pretrained multimodal model that can effectively perform various movie understanding tasks. The authors propose a strategy called Long-range Multimodal Pretraining, which utilizes movie data to train transferable multimodal and cross-modal encoders. The main idea is to learn from all modalities in a movie by observing and extracting relationships over a long-range. The authors conduct ablation studies on the LVU benchmark to validate their modeling choices and the importance of learning from long-range time spans. The proposed model achieves state-of-the-art performance on several LVU tasks and is more data efficient compared to previous works. Additionally, the transferability of the model is evaluated, resulting in a new state-of-the-art performance in five different benchmarks.