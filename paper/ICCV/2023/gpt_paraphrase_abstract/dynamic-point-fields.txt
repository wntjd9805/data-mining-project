In recent years, there has been significant progress in the field of neural surface reconstruction. While volumetric and implicit approaches have received much attention, some studies have shown that explicit graphics primitives, like point clouds, can reduce computational complexity without compromising the quality of the reconstructed surface. However, less focus has been given to modeling dynamic surfaces using point primitives. In this study, we introduce a dynamic point field model that combines the advantages of explicit point-based graphics with implicit deformation networks to efficiently model non-rigid 3D surfaces. The use of explicit surface primitives enables the incorporation of well-established constraints, such as isometric-as-possible regularization, with ease. Although learning this deformation model can be challenging due to local optima in fully unsupervised training, we propose leveraging semantic information, such as keypoint correspondence, to guide the deformation learning process. We demonstrate the applicability of our approach by creating an expressive and animatable human avatar from a collection of 3D scans. Previous methods in this area mostly rely on linear blend skinning, which limits the expressiveness of models when dealing with complex cloth appearances. We highlight the advantages of our dynamic point field framework in terms of its representational power, learning efficiency, and robustness to novel poses outside the training set. The code for this project is available to the public.