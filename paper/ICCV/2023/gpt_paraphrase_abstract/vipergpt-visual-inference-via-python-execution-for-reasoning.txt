Answering visual queries is a complex task that requires both visual processing and reasoning. The prevailing approach, end-to-end models, does not explicitly distinguish between these two components, leading to limitations in interpretability and generalization. Learning modular programs has emerged as a promising alternative, but it has proven difficult to simultaneously learn the programs and modules. In this study, we propose ViperGPT, a framework that combines code-generation models with vision-and-language models to create subroutines for generating responses to queries. ViperGPT utilizes an API to access available modules and generates Python code to compose them, which is then executed. This straightforward approach requires no additional training and achieves state-of-the-art performance across various complex visual tasks.