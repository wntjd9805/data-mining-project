The field of NeRF (Neural Radiance Fields) has recently focused on developing cross-scene generalizable models that can generate new views of unseen scenes. Existing approaches have used end-to-end neural architectures, replacing scene representation and rendering modules with neural networks like transformers, to enable feed-forward inference for novel view synthesis. However, these architectures still have limitations when it comes to diverse scenes. To overcome this, we propose incorporating the Mixture-of-Experts (MoE) concept from large language models (LLMs) into these architectures. MoE has shown superior generalization ability by balancing overall model capacity and per-instance specialization. We enhance the GNT (Generalizable NeRF Architecture) model with MoE, and also introduce a shared permanent expert and a geometry-aware consistency loss to ensure cross-scene consistency and spatial smoothness, which are crucial for generalizable view synthesis. Our model, called GNT with Mixture-of-View-Experts (GNT-MOVE), achieves state-of-the-art results in transferring to unseen scenes, demonstrating significantly improved cross-scene generalization in zero-shot and few-shot settings. The code for our model is available at https://github.com/VITA-Group/GNT-MOVE.