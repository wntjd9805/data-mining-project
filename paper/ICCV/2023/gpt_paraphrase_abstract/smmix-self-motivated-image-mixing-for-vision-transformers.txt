CutMix is an important strategy for improving the performance and generalization ability of vision transformers (ViTs). However, the inconsistency between mixed images and their corresponding labels reduces its effectiveness. Existing variations of CutMix address this issue by generating more consistent mixed images or more accurate mixed labels, but these approaches come with drawbacks such as increased training overhead or the need for additional information, making them less user-friendly. To overcome these limitations, we propose a new and effective method called Self-Motivated image Mixing (SMMix). SMMix encourages both image and label enhancement by the model itself during training. Specifically, we introduce a max-min attention region mixing technique that enriches the attention-focused objects in the mixed images. Additionally, we develop a fine-grained label assignment method that simultaneously trains the output tokens of mixed images with fine-grained supervision. Furthermore, we introduce a novel feature consistency constraint to align the features of mixed and unmixed images.Due to the thoughtful design of our self-motivated paradigm, SMMix offers significant advantages in terms of reduced training overhead and improved performance compared to other variations of CutMix. Notably, SMMix enhances the accuracy of various ViT models, including DeiT-T/S/B, CaiT-XXS-24/36, and PVT-T/S/M/L, by more than 1% on the ImageNet-1k dataset. We also demonstrate the generalization capability of our method on different downstream tasks and out-of-distribution datasets.For those interested, our project is openly available at https://github.com/ChenMnZ/SMMix.