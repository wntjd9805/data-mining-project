This paper introduces a new approach to image inpainting called Continuous-Mask-Aware Transformer (CMT). The CMT utilizes a continuous mask to represent the errors in tokens. The mask is initialized and used during the self-attention process. To enhance the masked self-attention, the concept of overlapping tokens is introduced. The mask is updated by modeling the error propagation in the masked self-attention. The CMT algorithm employs multiple Masked Self-Attention and Mask Update (MSAU) layers to predict initial inpainting results. These results are then refined to reconstruct a more accurate image. Experimental results on various datasets demonstrate that the proposed CMT algorithm outperforms existing algorithms significantly. The source codes for CMT can be accessed at https://github.com/keunsoo-ko/CMT.