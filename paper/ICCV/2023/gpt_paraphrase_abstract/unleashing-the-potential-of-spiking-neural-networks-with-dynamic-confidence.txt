This paper introduces a novel approach called Dynamic Confidence to address the trade-off between accuracy and latency in spiking neural networks (SNNs). The method involves extracting confidence information from SNN outputs over time and using it to create a decision-making agent that can determine when to terminate each inference. Dynamic Confidence offers several significant advantages for SNNs. Firstly, it optimizes latency dynamically during runtime, resulting in an average 40% speedup across different settings. Unlike existing low-latency SNN algorithms, this approach allows for real-time optimization. Secondly, the decision-making agent is easy to construct and robust in parameter space, making it highly implementable. Lastly, Dynamic Confidence enables the visualization of an SNN's potential, setting a target for current SNNs to strive towards. For example, a ResNet-50 SNN can achieve 82.47% accuracy on the ImageNet dataset within an average of only 4.71 time steps if it terminates at the most appropriate time point for each input sample. To unlock the potential of SNNs, a reliable decision-making agent needs to be constructed and provided with accurate estimation of ground truth, which is facilitated by Dynamic Confidence. This paper represents a meaningful step towards realizing the potential of SNNs. The code for implementing Dynamic Confidence is available.