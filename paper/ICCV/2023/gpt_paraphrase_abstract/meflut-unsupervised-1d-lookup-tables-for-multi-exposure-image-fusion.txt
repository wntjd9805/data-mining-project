This paper presents a novel method for achieving high-quality multi-exposure image fusion (MEF). The authors propose encoding fusion weights into a 1D lookup table (LUT), which maps pixel intensity values to fusion weights. They learn a separate 1D LUT for each exposure, allowing pixels from different exposures to independently query the appropriate LUT for fusion. To improve the quality of MEF, the authors incorporate attention mechanisms in various dimensions, including frame, channel, and spatial dimensions. They also create a new MEF dataset, including ground-truth samples manually tuned by professionals, to train their network in an unsupervised manner. Extensive experiments demonstrate the effectiveness of their proposed components, with their approach outperforming the state-of-the-art (SOTA) both qualitatively and quantitatively on their dataset and another representative dataset called SICE. Additionally, their 1D LUT approach is computationally efficient, taking less than 4ms to process a 4K image on a PC GPU. The proposed method has been implemented in millions of Android mobile devices from various brands worldwide, highlighting its high quality, efficiency, and robustness. The code for the method is available at: https://github.com/Hedlen/MEFLUT.