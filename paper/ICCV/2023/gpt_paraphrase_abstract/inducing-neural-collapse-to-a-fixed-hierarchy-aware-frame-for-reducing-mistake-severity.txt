A phenomenon called Neural Collapse has been discovered in deep neural networks used for classification. During the final phase of training, the penultimate feature means and associated classifier vectors of all flat classes collapse to the vertices of a simplex Equiangular Tight Frame (ETF). Previous research has attempted to utilize this phenomenon by fixing classifier weights to a pre-computed ETF, aiming to induce neural collapse and enhance the separation of learned features when training with imbalanced data. However, in this study, we propose an alternative approach by fixing the linear classifier of a deep neural network to a Hierarchy-Aware Frame (HAFrame) instead of an ETF. Additionally, we introduce a cosine similarity-based auxiliary loss to learn hierarchy-aware penultimate features that collapse to the HAFrame. The results of our experiments show that our approach reduces the severity of prediction mistakes while maintaining top-1 accuracy on various datasets with hierarchies of different heights (ranging from 3 to 12). The code for our approach can be found at: https://github.com/ltong1130ztr/HAFrame.