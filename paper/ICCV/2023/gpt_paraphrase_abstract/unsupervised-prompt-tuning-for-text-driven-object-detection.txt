Grounded language-image pre-trained models have shown strong zero-shot generalization to various object detection tasks. However, these models heavily rely on prompt engineering, which is a laborious process. Existing approaches address this issue by tuning text prompts using downstream training data in a few-shot or fully supervised manner. However, there is a lack of research on optimizing text prompts without using any annotations. In this paper, we propose an Unsupervised Prompt Tuning framework for text-driven object detection. This framework consists of two novel mean teaching mechanisms. In conventional mean teaching, the quality of pseudo boxes improves as training progresses, but there is a risk of overfitting noisy pseudo boxes. To overcome this problem, we introduce Nested Mean Teaching, which uses nested annotations to supervise teacher-student mutual learning in a bi-level optimization manner. Additionally, we propose Dual Complementary Teaching, which utilizes an offline pre-trained teacher and an online mean teacher through data-augmentation-based complementary labeling to prevent confirmation bias. By combining these two mechanisms, our unsupervised prompt tuning framework achieves significant performance improvement on various object detection datasets.