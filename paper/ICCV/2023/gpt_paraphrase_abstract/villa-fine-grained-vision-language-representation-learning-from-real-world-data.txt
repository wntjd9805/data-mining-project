Vision-language models (VLMs) like CLIP and ALIGN are typically trained on image-caption pairs from the web. However, real-world multimodal datasets, such as healthcare data, are more complex, with images (e.g., X-rays) paired with detailed text (e.g., physician reports) describing various attributes in fine-grained regions of the image. These high pairwise complexity samples have not been previously evaluated in terms of how well VLMs can capture the relationships between image regions and textual attributes. In this work, we make two key contributions. Firstly, we systematically evaluate the performance of standard VLMs on datasets with increasing pairwise complexity and find that they struggle to learn region-attribute relationships, experiencing performance degradations of up to 37% on retrieval tasks. To address this issue, our second contribution is the introduction of ViLLA. ViLLA is designed to capture fine-grained region-attribute relationships from complex datasets and consists of two components. The first component is a lightweight, self-supervised mapping model that decomposes image-text samples into region-attribute pairs. The second component is a contrastive VLM that learns representations from the generated region-attribute pairs. We conduct experiments across four domains (synthetic, product, medical, and natural images) and demonstrate that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks such as zero-shot object detection (achieving up to 3.6 AP50 points on COCO and 0.6 mAP points on LVIS) and retrieval (achieving up to 14.2 R-Precision points). This highlights the effectiveness of ViLLA in capturing the relationships between image regions and textual attributes in complex datasets.