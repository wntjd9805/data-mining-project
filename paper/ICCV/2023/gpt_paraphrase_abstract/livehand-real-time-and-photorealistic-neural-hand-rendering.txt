The digitization of the human hand is a significant problem as it is the primary means by which we interact with our environment. While previous research has focused on modeling the hand's geometry, little attention has been given to capturing its photo-realistic appearance. This is particularly crucial for applications in extended reality and gaming, where real-time rendering is essential. In this study, we propose a novel neural-implicit approach to achieve photo-realistic rendering of hands in real-time. This is a challenging task due to the textured nature of hands and their complex articulations that are influenced by pose. However, we demonstrate that our carefully designed method can successfully address these challenges. Our approach involves training on low-resolution renderings of a neural radiance field, incorporating a 3D-consistent super-resolution module, and employing mesh-guided sampling and space canonicalization techniques. We introduce a novel application of perceptual loss in the image space, which plays a critical role in accurately learning the hand's details. Additionally, we present a live demo showcasing our real-time photo-realistic rendering of the human hand, taking into account pose and view-dependent appearance effects. We thoroughly evaluate and analyze our design choices, confirming that they optimize both rendering speed and quality. For further information, including video results and access to our code, please visit our project website: https://vcai.mpi-inf.mpg.de/projects/LiveHand/.