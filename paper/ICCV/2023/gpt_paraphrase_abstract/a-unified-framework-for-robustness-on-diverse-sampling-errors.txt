Recent research has shown that machine learning algorithms, such as convolutional neural networks, often struggle to make accurate predictions when there is a significant difference between the data used for training and the data used for testing. In an attempt to address this issue, previous studies have focused on dealing with specific types of distribution shifts, such as generalizing to new domains or removing biases. However, accurately estimating the distribution mismatch is difficult because the target distribution is unknown during training. Therefore, it is more practical to adopt a conservative approach that can handle unexpected and diverse distributions effectively. Our approach aims to enable adaptive inference by considering the target distribution, which is only accessible during testing. Instead of assuming and fixing the target distribution during training, our method allows the model to adjust the feature space it refers to for each prediction, known as instance-wise adaptive inference. Through extensive evaluation, we have demonstrated the effectiveness of our method in achieving generalization on diverse distributions.