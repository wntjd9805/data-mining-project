Model quantization has become a popular technique for compressing deep neural networks and improving their inference speed on mobile and edge devices. However, most quantization methods assume the availability of training data, which may not always be feasible in real-life situations due to privacy and security concerns. To address this limitation, data-free network quantization has emerged as a promising approach. Causal reasoning, which analyzes causal relationships and eliminates data-driven correlations, is crucial in addressing data-free problems. Yet, existing literature lacks adequate causal formulations for data-free quantization. To bridge this gap, we propose a causal graph to model the data generation process and discrepancy reduction between pre-trained and quantized models. Inspired by causal understanding, we introduce Causal-DFQ, a method that leverages causality to eliminate the reliance on data by achieving an equilibrium of causality-driven intervened distributions. Specifically, we design a content-style-decoupled generator that synthesizes images based on relevant and irrelevant factors, and we introduce a discrepancy reduction loss to align the intervened distributions of the pre-trained and quantized models. Notably, our work is the first attempt to incorporate causality into the data-free quantization problem. Extensive experiments demonstrate the effectiveness of Causal-DFQ, and the code is available at Causal-DFQ.