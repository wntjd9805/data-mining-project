We introduce Viewset Diffusion, a diffusion-based generator that produces 3D objects using only 2D data as input. We observe a direct correspondence between collections of 2D views (viewsets) and 3D models. To exploit this relationship, we train a diffusion model to generate viewsets, while simultaneously training a neural network generator to reconstruct the corresponding 3D models. Our approach involves fitting the diffusion model to a large dataset of viewsets for a specific object category. The resulting generator can be conditioned on any number of input views. When conditioned on a single view, it performs 3D reconstruction considering the inherent ambiguity of the task, allowing for the generation of multiple solutions consistent with the input. Our model achieves efficient reconstruction in a feed-forward manner and is trained using rendering losses with as few as three views per viewset. Additional information can be found on our project page: szymanowiczs.github.io/viewset-diffusion.