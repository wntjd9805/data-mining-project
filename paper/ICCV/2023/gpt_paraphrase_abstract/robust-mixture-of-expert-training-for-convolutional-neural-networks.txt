The Sparsely-gated Mixture of Expert (MoE) is a promising deep model architecture that enables accurate and efficient model inference. However, little research has been done on the potential of MoE to enhance convolutional neural networks (CNNs) in terms of adversarial robustness. This paper aims to investigate how to make a CNN-based MoE model robust against adversarial attacks and whether it can be trained in the same way as a regular CNN model. Initial experiments show that the conventional adversarial training (AT) mechanism used for regular CNNs is not effective for robustifying an MoE-CNN. To understand this phenomenon, the robustness of an MoE-CNN is analyzed from two dimensions: the robustness of routers (gating functions that select data-specific experts) and the robustness of experts (the pathways defined by the subnetworks of the backbone CNN). It is observed that routers and experts are difficult to adapt to each other using conventional AT. Therefore, a new router-expert alternating Adversarial training framework called ADVMOE is proposed. The effectiveness of ADVMOE is demonstrated on four commonly-used CNN model architectures and four benchmark datasets. ADVMOE achieves a 1% to 4% improvement in adversarial robustness compared to the original dense CNN, while also reducing inference costs by over 50% thanks to the sparsity-gated MoE. The code for ADVMOE is available at https://github.com/OPTML-Group/Robust-MoE-CNN.