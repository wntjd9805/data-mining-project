The demand for high-quality videos has led to increased interest in high-resolution and high-dynamic range (HDR) imaging techniques. One crucial step in generating an HDR video from low dynamic range (LDR) images is motion compensation between frames. However, existing methods that use the optical flow algorithm often struggle with flow estimation errors in the presence of saturation or complex motions. To address this issue, we propose an end-to-end HDR video composition framework called LAN-HDR.LAN-HDR aligns LDR frames in the feature space and merges the aligned features to create an HDR frame, without relying on pixel-domain optical flow. The framework consists of two modules: an alignment module and a hallucination module. The alignment module aligns frames to adjacent references by evaluating luminance-based attention, disregarding color information. Meanwhile, the hallucination module generates sharp details, particularly for washed-out areas caused by saturation. The aligned and hallucinated features are blended adaptively to complement each other, resulting in a final HDR frame.To enhance temporal consistency and reduce flickering, we incorporate a temporal loss in addition to frame reconstruction losses during training. Our extensive experiments demonstrate that our method performs as well as or better than state-of-the-art methods on various benchmarks. For those interested, the codes for our framework are available at https://github.com/haesoochung/LAN-HDR.