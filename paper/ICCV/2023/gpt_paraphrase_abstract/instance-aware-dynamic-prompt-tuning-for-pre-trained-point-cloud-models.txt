Pre-trained point cloud models have been widely used in various 3D understanding tasks. However, the current approach of full fine-tuning in downstream tasks results in high storage overhead for model parameters, which hinders the efficiency of applying large-scale pre-trained models. This paper proposes a new approach called Instance-aware Dynamic Prompt Tuning (IDPT) to address this issue. Inspired by the success of visual prompt tuning, IDPT aims to strike a balance between performance and parameter efficiency. While static prompting methods show some effectiveness in transferring knowledge, they are susceptible to the diverse distribution of real-world point cloud data. To overcome this limitation, IDPT introduces a dynamic prompt generation module that can perceive the semantic features of each point cloud instance and generate adaptive prompt tokens to enhance the model's robustness. Experimental results demonstrate that IDPT outperforms full fine-tuning in most tasks while using only 7% of the trainable parameters, offering a promising solution for parameter-efficient learning in pre-trained point cloud models. The code for IDPT is available at https://github.com/zyh16143998882/ICCV23-IDPT.