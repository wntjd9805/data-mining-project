Overfitting to the source domain is a common issue in training deep neural networks using gradients. To address this, various regularization techniques have been introduced, including dropout. While dropout has shown improvements on benchmarks like ImageNet, its performance decreases when there is a domain shift in the test set, where the unseen data comes from a different distribution. In this paper, we propose a new approach to dropout mask construction based on the gradient-signal-to-noise ratio (GSNR) of the network's parameters. Instead of Bernoulli sampling, we discard parameters with high GSNR at each training step. Additionally, we use a meta-learning approach to automatically determine the optimal dropout ratio, eliminating the need for manual search. Our method is evaluated on standard domain generalization benchmarks and achieves competitive results in classification and face anti-spoofing tasks.