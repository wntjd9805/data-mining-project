The use of diffusion-based generative models has been successful in generating images based on text. However, applying these models to real-world video editing poses challenges due to the high level of randomness involved in the generation process. In this paper, we introduce FateZero, a method for editing real-world videos using text-based editing without the need for prompt training or specific masks. To ensure consistent video editing, we propose several techniques based on pre-trained models. Instead of using a straightforward inversion technique, our approach captures intermediate attention maps during inversion, preserving both structural and motion information. These maps are directly incorporated into the editing process, rather than being generated during denoising. To minimize semantic leakage from the source video, we fuse self-attentions with a blending mask obtained from cross-attention features of the source prompt. Additionally, we have improved the self-attention mechanism in the denoising UNet model by introducing spatial-temporal attention for frame consistency. Our method is the first to demonstrate the ability for zero-shot text-driven video style and local attribute editing using a trained text-to-image model. We also have enhanced zero-shot shape-aware editing capabilities based on the text-to-video model. Extensive experiments show that our method outperforms previous works in terms of temporal consistency and editing capability.