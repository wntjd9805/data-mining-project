Modeling neural radiance fields for fast-moving deformable objects solely from visual data is a difficult task due to the challenges posed by high deformation and low acquisition rates. To tackle this issue, we propose the utilization of event cameras, which capture visual changes rapidly and asynchronously. Our approach introduces a novel method for modeling deformable neural radiance fields using both RGB and event cameras. By leveraging the asynchronous stream of events and calibrated sparse RGB frames, our method optimizes the unknown camera pose at each event and integrates them into the radiance fields. This optimization process is carried out efficiently by collectively considering all events and actively sampling them during the learning phase. Experimental results on both synthetic and real-world datasets demonstrate the significant advantages of our proposed method compared to state-of-the-art techniques and baseline models. This research opens up a promising direction for modeling deformable neural radiance fields in dynamic real-world scenes. The code for our method is publicly available at: https://qimaqi.github.io/DE-NeRF.github.io/.