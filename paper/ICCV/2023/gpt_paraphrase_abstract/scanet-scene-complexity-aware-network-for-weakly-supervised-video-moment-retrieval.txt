Video moment retrieval is a task that involves finding specific moments in a video based on a given language query. Traditional methods for this task require annotating the temporal moments, which can be costly. As a result, researchers have focused on weakly-supervised VMR (wsVMR) systems. These systems generate multiple proposals as potential moment candidates and then select the most suitable one. However, current proposals in wsVMR systems do not take into account the varying number of scenes in each video. They are determined without considering the characteristics of the video. We believe that a retrieval system should be able to handle the complexities arising from different scene numbers in each video. To address this issue, we introduce a new retrieval system called Scene Complexity Aware Network (SCANet). SCANet measures the "scene complexity" of multiple scenes in a video and generates adaptive proposals that adapt to the varying complexities of scenes in each video. Our experimental results on three retrieval benchmarks (Charades-STA, ActivityNet, TVR) show that SCANet achieves state-of-the-art performance and demonstrates the effectiveness of incorporating scene complexity.