The combination of Bayesian optimization (BO) and Self-Distillation (SD) has shown great potential in improving model performance. BO suggests promising hyperparameter configurations based on observations from previous training trials, while SD transfers partial knowledge learned by the task model itself. However, both BO and SD only utilize partial knowledge from previous trials. To address this limitation, we propose the BOSS framework, which combines BO and SD to fully leverage the various knowledge gained from all training trials. BOSS suggests promising hyperparameter configurations through BO and carefully selects pre-trained models from previous trials for SD. In various tasks such as image classification, learning with noisy labels, semi-supervised learning, and medical image analysis, BOSS outperforms both BO and SD significantly. The code for BOSS is available at https://github.com/sooperset/boss.