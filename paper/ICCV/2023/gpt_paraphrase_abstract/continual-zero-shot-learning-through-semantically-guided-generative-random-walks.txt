This paper introduces a method called continual zero-shot learning (CZSL) to model the simultaneous processes of learning new concepts, remembering previous knowledge, and adapting to future tasks throughout a human's lifetime. Existing CZSL methods often rely heavily on unseen semantic information that may not be consistently accessible in real-world scenarios. To address this challenge, the paper proposes leveraging generative modeling to improve the understanding of the unseen visual space. The paper introduces generalization-bound tools and provides a theoretical explanation for the benefits of generative modeling in CZSL tasks. Based on this analysis, the paper proposes a learning algorithm that incorporates a novel semantically guided Generative RandomWalk (GRW) loss. This loss encourages the model to continually generate realistic and characterized samples to represent the unseen space. The proposed algorithm achieves state-of-the-art performance on various datasets, surpassing existing CZSL methods by 3-7%. The code for the algorithm is available at https://github.com/wx-zhang/IGCZSL.