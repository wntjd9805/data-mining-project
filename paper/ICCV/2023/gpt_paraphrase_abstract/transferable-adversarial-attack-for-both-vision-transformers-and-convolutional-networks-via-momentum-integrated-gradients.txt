The generation of transferable adversarial examples for Visual Transformers (ViTs) and Convolutional Neural Networks (CNNs) is challenging due to the distinct structures and mechanisms of these two backbone structures. This study introduces a new attack method called Momentum Integrated Gradients (MIG) that successfully attacks ViTs and exhibits impressive transferability across ViTs and CNNs. Instead of using regular gradients, the integrated gradients are employed to guide the generation of adversarial perturbations. The integrated gradients demonstrate higher similarity across models. The accumulated gradients are then obtained by combining the integrated gradients from previous iterations with the current ones in a momentum manner, and their sign is used to iteratively modify the perturbations. Extensive experiments demonstrate that MIG produces adversarial examples with stronger transferability, outperforming state-of-the-art methods for both CNN and ViT models.