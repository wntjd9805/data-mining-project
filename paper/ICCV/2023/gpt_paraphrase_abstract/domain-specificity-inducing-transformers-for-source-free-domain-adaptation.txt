Traditional methods for Domain Adaptation (DA) focus on learning feature representations that are invariant across different domains in order to improve performance on target domains. However, we argue that domain-specificity is also crucial, as models trained within a specific domain possess valuable domain-specific properties that can aid in adaptation. As a result, we propose a framework that facilitates the disentanglement and learning of both domain-specific and task-specific factors within a single model.Inspired by the success of vision transformers in various multi-modal vision problems, we discover that queries can be utilized to extract domain-specific factors. Therefore, we introduce a novel framework called Domain-Specificity inducing Transformer (DSiT) that disentangles and learns both domain-specific and task-specific factors.To achieve disentanglement, we propose the use of Domain-Representative Inputs (DRI) that contain domain-specific information. These inputs are used to train a domain classifier with a novel domain token. Importantly, we are the first to apply vision transformers to domain adaptation in a privacy-focused source-free setting. Our approach achieves state-of-the-art performance on benchmarks involving single-source, multi-source, and multi-target scenarios.