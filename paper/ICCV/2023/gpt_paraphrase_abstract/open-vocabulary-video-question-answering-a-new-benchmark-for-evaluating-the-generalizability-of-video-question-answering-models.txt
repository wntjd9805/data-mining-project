Video Question Answering (VideoQA) is a difficult task that involves complex reasoning using multiple modes of information. While multiple-choice VideoQA aims to predict the answer from a set of options, open-ended VideoQA aims to answer questions without any restrictions on the possible answers. However, most existing VideoQA models treat open-ended VideoQA as a classification task, where the answers are limited to a fixed set of frequent answers (e.g., top-1000 answers). This approach leads to bias towards frequent answers and fails to generalize to answers that are rare or unseen. To address this limitation, we propose a new benchmark called Open-vocabulary Video Question Answering (OVQA) that evaluates the generalizability of VideoQA models by considering rare and unseen answers.   To improve the generalization of the models, we introduce a novel Graph Neural Network (GNN)-based soft verbalizer. This verbalizer enhances the prediction of rare and unseen answers by aggregating information from similar words. We evaluate our approach by modifying existing open-ended VideoQA models (which are based on closed-vocabulary) and improve their performance by considering rare and unseen answers. Our ablation studies and qualitative analyses demonstrate that our GNN-based soft verbalizer significantly improves the model's performance, particularly on rare and unseen answers. We believe that our OVQA benchmark can be a valuable tool for evaluating the generalizability of VideoQA models and can inspire future research in this area. The code for our approach is available at https://github.com/mlvlab/OVQA.