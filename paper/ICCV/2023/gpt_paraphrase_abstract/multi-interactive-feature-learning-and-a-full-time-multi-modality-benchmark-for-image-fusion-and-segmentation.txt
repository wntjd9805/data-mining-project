This paper proposes a Multi-interactive Feature learning architecture called SegMiF for image fusion and segmentation tasks in autonomous driving and robotic operation. The SegMiF architecture consists of a fusion sub-network and a segmentation sub-network, with intermediate features bridging the two components to assist in knowledge transfer. A hierarchical interactive attention block ensures fine-grained mapping of information between the tasks, allowing for fully mutual-interactive modality/semantic features. A dynamic weight factor is introduced to automatically adjust task weights, balancing feature correspondence without requiring manual tuning. The paper also presents a multi-wave binocular imaging system and a benchmark dataset for image fusion and segmentation. Experimental results show that the proposed method produces visually appealing fused images and achieves an average 7.66% higher segmentation mIoU than state-of-the-art approaches in real-world scenes. The source code and benchmark dataset are available at https://github.com/JinyuanLiu-CV/SegMiF.