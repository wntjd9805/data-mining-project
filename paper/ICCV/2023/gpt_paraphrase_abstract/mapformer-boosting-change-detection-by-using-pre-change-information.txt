This study focuses on change detection in remote sensing imagery, which is important for various applications such as urban planning, disaster management, and climate research. Current methods for identifying areas of change often overlook the availability of semantic information from existing maps that describe features of the Earth's surface. The authors propose leveraging this additional information to improve change detection in bi-temporal images. They demonstrate that by simply integrating the semantic information through concatenation of latent representations, their approach outperforms state-of-the-art methods. Inspired by this finding, they introduce the new task of Conditional Change Detection, where pre-change semantic information is used in conjunction with bi-temporal images. To fully exploit this extra information, they propose a novel architecture called MapFormer, which incorporates a multi-modal feature fusion module that allows for feature processing based on the available semantic information. The authors also employ a supervised, cross-modal contrastive loss to guide the learning of visual representations. Their approach achieves significant improvements in change detection performance compared to existing methods, with absolute gains of 11.7% and 18.4% in terms of binary change IoU on the DynamicEarthNet and HRSCD datasets, respectively. Furthermore, they demonstrate the robustness of their approach to variations in the quality of the pre-change semantic information and the absence of pre-change imagery. The code for their method is publicly available on GitHub at https://github.com/mxbh/mapformer.