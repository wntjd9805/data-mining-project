Large-scale pre-trained Vision & Language (VL) models have achieved impressive results in various applications by allowing zero-shot open vocabulary reasoning using natural language prompts. However, recent studies have highlighted a fundamental weakness in these models. They struggle to understand Visual Language Concepts (VLC) beyond nouns, such as non-object words and compositional reasoning.To address this issue without compromising their zero-shot capabilities, we explore the use of purely synthetic data to train these models. We introduce Synthetic Visual Concepts (SyViC), a large-scale synthetic dataset and data generation codebase that generates additional data to enhance VLC understanding and compositional reasoning in VL models. Additionally, we propose a general strategy for VL finetuning using SyViC to achieve these improvements.Through extensive experiments on VL-Checklist, Winoground, and ARO benchmarks, we demonstrate that it is possible to enhance the VLC understanding of strong pre-trained VL models by leveraging synthetic data. For instance, we observe a 9.9% improvement on ARO and a 4.3% improvement on VL-Checklist, with less than a 1% drop in zero-shot accuracy. In conclusion, our work showcases the potential of using synthetic data and a finetuning strategy to address the limitations of large-scale pre-trained VL models in understanding VLC and performing compositional reasoning tasks.