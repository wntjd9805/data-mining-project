We propose a novel anomaly detection (AD) framework called FOcus-the-Discrepancy (FOD) that combines patch-wise representation discrepancies and patch-to-normal-patch correlations. We utilize the Transformer model to effectively model these two aspects. Our method introduces Intra-Inter-Correlation (I2Correlation) by renovating the self-attention maps in transformers. I2Correlation establishes intra- and inter-image correlations and highlights abnormal patterns. We employ RBF-kernel-based target-correlations for self-supervised learning to adaptively learn the intra- and inter-correlations. Additionally, we use an entropy constraint strategy to address optimization issues and enhance the distinguishability between normal and abnormal patterns. Our approach outperforms existing AD methods on three unsupervised real-world benchmarks. The code for our method is available at the provided GitHub link.