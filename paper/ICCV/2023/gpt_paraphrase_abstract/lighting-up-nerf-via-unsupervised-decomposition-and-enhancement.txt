This paper introduces a new approach called Low-Light NeRF (LLNeRF) that aims to enhance scene representation and synthesize normal-light views directly from low-light images. The Neural Radiance Field (NeRF) method, which is commonly used for view synthesis, struggles with low-light images due to their low pixel intensities, noise, and color distortion. Existing low-light image enhancement methods combined with NeRF do not solve the view inconsistency problem caused by the individual 2D enhancement process. LLNeRF addresses these issues by decomposing radiance field learning, allowing for joint optimization of illumination enhancement, noise reduction, and color correction with NeRF. The proposed method successfully generates novel view images with appropriate lighting, vivid colors, and details using a collection of camera-captured low dynamic range images from low-light scenes. Experimental results demonstrate that LLNeRF outperforms both existing low-light enhancement methods and NeRF methods.