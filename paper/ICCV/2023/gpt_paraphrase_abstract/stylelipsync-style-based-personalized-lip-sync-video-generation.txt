This paper introduces StyleLipSync, a personalized lip-sync video generative model that can create lip-synchronizing videos from any audio, regardless of the person's identity. The model utilizes a semantically rich latent space from a pre-trained StyleGAN to generate videos with different identities. To improve naturalness over frames, the model incorporates pose-aware masking, which dynamically locates the mask using a 3D parametric mesh predictor. Additionally, the paper proposes a few-shot lip-sync adaptation method that preserves lip-sync generalization while enhancing person-specific visual information. Experimental results show that the model can accurately generate lip-sync videos even with no prior training data, and it can enhance the characteristics of an unseen face using only a few seconds of target video.