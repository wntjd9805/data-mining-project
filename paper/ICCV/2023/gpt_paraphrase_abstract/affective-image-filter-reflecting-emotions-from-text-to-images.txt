Understanding and visually representing emotions in text is a difficult task that requires a deep understanding of natural language and high-quality image synthesis. In this study, we propose a new model called AffectiveImage Filter (AIF) that can comprehend abstract emotions in text and translate them into concrete images with appropriate colors and textures. Our model is built on a multi-modal transformer architecture, which combines both images and texts into tokens and incorporates emotional prior knowledge. We introduce several loss functions to grasp complex emotions and generate suitable visualizations. Additionally, we create a new dataset consisting of aesthetic images and emotional texts for training and evaluating the AIF model. To comprehensively assess its performance, we design four quantitative metrics and conduct a user study. The results demonstrate that our AIF model outperforms existing methods and can elicit specific emotional responses from human observers.