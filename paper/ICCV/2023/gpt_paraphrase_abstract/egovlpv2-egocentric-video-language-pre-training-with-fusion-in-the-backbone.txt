Video-language pre-training (VLP) has become increasingly important in various vision and language tasks. However, current ego-centric VLP frameworks have separate video and language encoders, limiting the development of a unified system. In this study, we present EgoVLPv2, the second generation of egocentric video-language pre-training, which improves upon the previous generation by incorporating cross-modal fusion directly into the video and language backbones. EgoVLPv2 achieves strong video-text representation during pre-training and utilizes cross-modal attention modules for different downstream tasks, reducing the need for extensive fine-tuning. Additionally, our proposed fusion in the backbone strategy is more lightweight and compute-efficient compared to adding fusion-specific layers. Extensive experiments on various video-language tasks demonstrate the effectiveness of EgoVLPv2, surpassing strong baselines and achieving state-of-the-art performance. More information about our project can be found at https://shramanpramanick.github.io/EgoVLPv2/.