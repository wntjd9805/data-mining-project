In the context of learning with noisy labels, the common practice is to consider small-loss data as correctly labeled. However, this approach may not effectively identify challenging examples with large losses, which are crucial for achieving optimal generalization performance. To address this issue, we introduce a new framework called Late Stopping, which takes advantage of the inherent robust learning ability of deep neural networks (DNNs) by prolonging the training process. Late Stopping gradually reduces the noisy dataset by removing highly probable mislabeled examples while preserving the majority of clean challenging examples in the training set. Our empirical observations indicate that mislabeled and clean examples exhibit differences in the number of epochs required for consistent and accurate classification, allowing us to identify and remove high-probability mislabeled examples. Experimental results on both benchmark-simulated and real-world noisy datasets demonstrate that our proposed method outperforms existing state-of-the-art approaches.