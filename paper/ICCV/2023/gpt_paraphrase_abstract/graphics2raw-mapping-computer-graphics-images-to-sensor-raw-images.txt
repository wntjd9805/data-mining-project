Computer graphics rendering platforms have made significant advancements in generating realistic images. This has led to the possibility of using computer-generated (CG) images as training data for high-level computer vision tasks like autonomous driving and semantic segmentation. However, CG images are not suitable for low-level vision tasks that involve processing RAW sensor images. This is because RAW images have specific color spaces and color casts caused by the sensor's response to lighting conditions, while CG images are rendered in a device-independent color space without these issues. To address this disparity, we propose a framework that accurately mimics RAW sensor images using CG images. Our approach allows for a one-to-many mapping, meaning a single CG image can be transformed to match multiple sensors and lighting conditions. Furthermore, our method only requires a small number of example RAW-DNG files from the target sensor as parameters for the mapping process. We compare our approach to alternative strategies and demonstrate that it produces more realistic RAW images and achieves better results in three low-level vision tasks: RAW denoising, illumination estimation, and neural rendering for night photography. As part of this work, we also provide a dataset of 292 realistic CG images for training low-light imaging models.