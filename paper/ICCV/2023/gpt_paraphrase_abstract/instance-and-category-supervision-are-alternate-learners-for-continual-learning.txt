Continual Learning (CL) involves developing complex behaviors by building upon previously acquired skills. However, current CL algorithms often suffer from class-level forgetting, where new knowledge quickly overwrites label information. To address this, recent self-supervised learning (SSL) techniques have been used to mine instance-level discrimination. However, previous research has highlighted that SSL objectives trade off between distortion invariance and preserving sample information, limiting instance-level discrimination. In this study, we reframe SSL from an information-theoretic perspective to separate the goal of instance-level discrimination and address the trade-off by promoting compact representations with maximum distortion invariance preservation. Based on this, we propose a novel alternate learning paradigm that combines instance-level and category-level supervision. This approach enhances robustness against forgetting and improves adaptation to each task. Extensive experiments on four benchmarks in both class-incremental and task-incremental settings validate our method, demonstrating its effectiveness and efficiency.