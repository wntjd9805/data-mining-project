This paper presents a new approach to detect out-of-distribution inputs in visual recognition models. The proposed method combines joint representation learning and statistical modeling to separate out-of-distribution data from in-distribution data. Instead of using auxiliary outlier data for training, the method learns a mixture of Gaussian models for each in-distribution category. Multiple Mahalanobis-based metrics are aggregated to create an in-distribution score function. The ImageNet-1k dataset is divided into ten random folds for evaluation, with one fold used as the in-distribution dataset and the others as out-of-distribution datasets. The method is also tested on seven popular benchmarks. Extensive experiments demonstrate that the proposed method outperforms current algorithms and maintains high efficiency in detecting out-of-distribution samples without compromising the discriminative ability of visual recognition models. Additionally, the visual representation obtained through the proposed method shows competitive performance compared to features learned by classical methods.