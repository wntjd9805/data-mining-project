Recent advancements in audio2mesh-based techniques have shown promising potential for speech-driven 3D facial animation tasks. However, there are several challenging issues that need to be addressed. One such challenge is the scarcity of data, which is inherent due to the difficulty of collecting 4D data. Additionally, existing methods often lack control over the animated face. To tackle these challenges, we propose a novel framework called Speech4Mesh. This framework aims to generate 4D talking head data and train the audio2mesh network using reconstructed meshes. In our approach, we first reconstruct the 4D talking head sequence using monocular videos. To accurately capture variations related to speech on the face, we utilize audio-visual alignment information through a contrastive learning scheme. After reconstructing the 4D data, we train the audio2mesh network, such as FaceFormer, using this generated data. To gain control over the animated talking face, we encode factors unrelated to speech, such as emotion, into an emotion embedding for manipulation. Additionally, we employ a differentiable renderer to ensure more accurate photometric details in the reconstruction and animation results.Empirical experiments demonstrate that our Speech4Mesh framework outperforms state-of-the-art reconstruction methods, particularly in the lower-face region. Moreover, it achieves better animation performance both perceptually and objectively after being pre-trained on the synthesized data. Furthermore, we validate that our framework allows explicit control over the emotion of the animated talking face.