This paper introduces OccFormer, a dual-path transformer network designed to effectively process 3D volumes for semantic occupancy prediction in autonomous driving. Unlike previous methods that utilized bird-eye-view representations, OccFormer utilizes a 3D semantic occupancy approach that provides structural information in the vertical direction. By decomposing the heavy 3D processing into local and global transformer pathways, OccFormer achieves long-range, dynamic, and efficient encoding of camera-generated 3D voxel features along the horizontal plane. Additionally, the paper proposes preserve-pooling and class-guided sampling techniques to address sparsity and class imbalance in the occupancy decoder. Experimental results demonstrate that OccFormer outperforms existing methods for semantic scene completion on the SemanticKITTI dataset and LiDAR semantic segmentation on the nuScenes dataset. The code for OccFormer is available at https://github.com/zhangyp15/OccFormer.