Multi-view camera-based 3D detection is a difficult problem in computer vision. Previous studies have tried using a pre-trained LiDAR detection model to improve camera-based detection. However, we believe that there is a significant difference between LiDAR and camera-based features, as they have distinct characteristics and come from different sources. To address this, we propose a method called Geometry Enhanced Masked Image Modeling (GeoMIM) that enhances the transfer of knowledge from the LiDAR model to the camera-based detection system. GeoMIM is a multi-camera vision transformer with Cross-View Attention (CVA) blocks that utilizes the LiDAR features as learning targets. During pretraining, GeoMIM's decoder incorporates a semantic branch for dense perspective-view features and a geometry branch for reconstructing dense perspective-view depth maps. The depth branch takes into account the camera's parameters to improve transferability. Extensive experiments show that GeoMIM surpasses existing methods on the nuScenes benchmark, achieving state-of-the-art performance in camera-based 3D object detection and segmentation.