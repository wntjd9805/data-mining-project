We present a new task called Multi3DRefer, which involves localizing multiple objects in real-world 3D scenes using natural language descriptions. Unlike previous tasks that focus on locating a single object, this task acknowledges the common need to identify multiple objects in various scenarios, such as visual navigation and object rearrangement. To facilitate this task, we introduce the Multi3DRefer dataset, which consists of 61,926 descriptions referring to 11,609 objects. Each description may reference zero, one, or multiple target objects. To enable further research in multi-modal 3D scene understanding, we propose a new evaluation metric and benchmark methods. Additionally, we improve the baseline performance by leveraging 2D features from CLIP, rendering object proposals online with contrastive learning. This approach surpasses the current state-of-the-art performance on the ScanRefer benchmark.