Our proposal, PromptStyler, aims to address source-free domain generalization in a joint vision-language space. We observe that a text feature can effectively represent relevant image features, and previous research has shown the cross-modal transferability of this joint space. PromptStyler synthesizes diverse styles using prompts without relying on images. It achieves this by learning to generate different style features through learnable style word vectors for pseudo-words. We ensure that the learned styles do not distort content information by enforcing proximity between style-content features and their corresponding content features in the joint space. After learning the style word vectors, we train a linear classifier using the synthesized style-content features. Remarkably, PromptStyler achieves state-of-the-art performance on PACS, VLCS, OfficeHome, and DomainNet datasets, even without using any images for training.