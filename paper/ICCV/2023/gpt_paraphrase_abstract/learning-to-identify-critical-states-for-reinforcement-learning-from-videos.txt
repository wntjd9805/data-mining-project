Recent research in deep reinforcement learning (DRL) has demonstrated that valuable information about effective policies can be derived from offline data, even when explicit information about executed actions is lacking. For instance, videos of humans or robots can contain implicit information about rewarding action sequences. However, a DRL machine must autonomously learn to identify and recognize relevant states, actions, and rewards in order to benefit from such videos. In this study, we propose a novel approach called Deep State Identifier that learns to predict returns from videos representing episodes, without relying on ground-truth annotations. Our method employs a mask-based sensitivity analysis to extract and identify critical states that are important for understanding and improving agent behavior. Through extensive experiments, we demonstrate the potential of our method in enhancing agent performance. The source code and generated datasets are publicly available at https://github.com/AI-Initiative-KAUST/VideoRLCS.