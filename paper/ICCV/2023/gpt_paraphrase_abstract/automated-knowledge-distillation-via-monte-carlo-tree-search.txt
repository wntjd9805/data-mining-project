This paper introduces Auto-KD, the first automated search framework for designing optimal knowledge distillation methods. Traditional distillation techniques require manual designs and extensive tuning, but Auto-KD addresses these issues by studying different distillers and finding ways to simplify and combine them. The authors create a uniform search space with advanced operations in transformations, distance functions, and hyperparameters. They also propose an effective search strategy using Monte Carlo tree search, updating the search space based on test loss and representation gap. To speed up the search process, they utilize offline processing, sparse training, and proxy settings. Auto-KD can be applied to multi-layer and multi-teacher scenarios with training-free weighted factors. The method is practical and shows promising results across various vision tasks, outperforming existing models. The code for Auto-KD is available at the provided GitHub link.