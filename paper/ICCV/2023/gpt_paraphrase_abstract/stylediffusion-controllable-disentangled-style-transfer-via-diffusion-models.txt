The problem of disentangling content and style (C-S) in style transfer is a significant challenge. Existing approaches, such as using explicit definitions or implicit learning, lack interpretability and control, resulting in mixed representations and unsatisfactory outcomes. In this study, we propose a novel C-S disentangled framework for style transfer that does not rely on previous assumptions. Our approach involves explicitly extracting content information and implicitly learning complementary style information, leading to interpretable and controllable C-S disentanglement and style transfer. We introduce a CLIP-based style disentanglement loss coupled with a style reconstruction prior to disentangle C-S in the CLIP image space. By incorporating the powerful capabilities of diffusion models in style removal and generation, our framework outperforms existing methods and offers flexible C-S disentanglement and trade-off control. Our research provides new insights into C-S disentanglement in style transfer and showcases the potential of diffusion models in learning well-disentangled C-S characteristics.