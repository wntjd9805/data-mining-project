3D pose estimation is a crucial task in computer vision that has many practical applications. However, accurately estimating the poses of multiple people from a single video (3DMPPE) is extremely challenging and has not yet been widely applied to real-world scenarios. We have identified three problems with current methods: they are not robust when faced with unseen views during training, they are vulnerable to occlusion, and they produce unstable output with excessive movement. To address these issues, we propose POTR-3D, the first sequence-to-sequence 2D-to-3D lifting model for 3DMPPE. This model is supported by a novel data augmentation strategy that is aware of the geometry of the scene, allowing it to generate diverse data with different viewpoints while considering occlusions and the ground plane. Through extensive experiments, we have demonstrated that our proposed model and data augmentation approach can robustly generalize to unseen views, accurately recover poses even in the presence of heavy occlusions, and generate smoother and more natural output. Our approach has been validated by achieving state-of-the-art performance on publicly available benchmarks, as well as by producing high-quality results on more challenging real-world videos. Interested individuals can find demonstration videos at https://www.youtube.com/@potr3d.