This paper introduces a novel defense strategy called FedCPA (Federated learning with Critical Parameter Analysis) to address poisoning attacks in federated learning systems. These attacks occur when malicious clients send false updates to the central server. Traditional defense strategies are not effective when dealing with non-IID (non-independent and identically distributed) data settings. FedCPA proposes an attack-tolerant aggregation method based on the observation that benign local models exhibit similar sets of top-k and bottom-k critical parameters, while poisoned models do not. By analyzing these critical parameters, FedCPA can identify and mitigate the impact of poisoned updates. The effectiveness of FedCPA is demonstrated through experiments conducted on various datasets under different attack scenarios. The results show that FedCPA outperforms existing defense strategies in defending against poisoning attacks in federated learning systems.