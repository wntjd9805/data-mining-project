The field of visual representation learning has made significant progress, but its application in robotics has been limited. Previous work used visual representations to enhance learning but relied on robot data to determine actions, which is costly. This study proposes a scalable alternative by utilizing visual representations to directly infer robot actions. By leveraging the relationships expressed in vision encoders as distances, the researchers develop an algorithm to acquire a distance function and dynamics predictor. The method outperforms traditional robot learning methods on various manipulation tasks and can generalize to new objects without using robot demonstrations during training.