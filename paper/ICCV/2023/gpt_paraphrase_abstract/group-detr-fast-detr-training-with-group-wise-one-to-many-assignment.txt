The Detection Transformer (DETR) is an end-to-end detection model that assigns one ground-truth object to one prediction without the need for post-processing. However, previous detection methods have shown success with assigning one ground-truth object to multiple predictions. This approach has proven challenging to apply to DETR training. This paper introduces Group DETR, a training approach that addresses this challenge by using a group-wise method for one-to-many assignment. This involves using multiple groups of object queries, performing one-to-one assignment within each group, and using separate decoder self-attention. This approach is similar to data augmentation with learned object query augmentation and simultaneously trains parameter-sharing networks of the same architecture. The inference process remains the same as traditional DETR, requiring only one group of queries without modifying the architecture. Group DETR is versatile and applicable to different DETR variants. Experimental results demonstrate that Group DETR significantly accelerates training convergence and improves the performance of various DETR-based models. The code for Group DETR is available at https://github.com/Atten4Vis/GroupDETR.