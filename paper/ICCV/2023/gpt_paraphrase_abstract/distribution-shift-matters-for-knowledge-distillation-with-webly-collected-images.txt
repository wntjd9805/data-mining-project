Knowledge distillation is a method used to train a smaller student network based on a larger, pre-trained teacher network. However, existing knowledge distillation techniques become impractical when the original training data is not available due to privacy concerns and data management issues. To address this limitation, data-free knowledge distillation approaches have been proposed, which gather training instances from the internet. However, these approaches often overlook the fact that the distribution of instances from the original training data and web-collected data may differ, which can impact the reliability of the student network. To overcome this challenge, we propose a new method called "Knowledge Distillation between Different Distributions" (KD3). KD3 consists of three components: 1) the dynamic selection of useful training instances from web-collected data based on the predictions of both the teacher and student networks, 2) the alignment of weighted features and classifier parameters of the two networks to facilitate knowledge transfer, and 3) the integration of a new contrastive learning block called MixDistribution, which generates perturbed data with a new distribution to align instances and enhance distribution-invariant learning by the student network. Extensive experiments conducted on various benchmark datasets demonstrate that our proposed KD3 outperforms existing data-free knowledge distillation methods.