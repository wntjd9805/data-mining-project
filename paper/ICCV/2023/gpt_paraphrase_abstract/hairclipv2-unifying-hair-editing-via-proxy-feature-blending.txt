In recent years, there have been significant advancements in hair editing techniques. Early methods relied on sketches or masks to specify editing conditions, allowing for fine-grained local control. However, these interaction modes are inefficient when editing conditions can be easily described through language or reference images. HairCLIP, a recent breakthrough in cross-modal models, introduced text-driven and reference-driven interaction modes for hair editing. However, it lacks support for fine-grained controls specified by sketches or masks. This paper presents HairCLIPv2, a new framework that aims to support all these interaction modes in a unified manner. HairCLIPv2 improves upon HairCLIP by preserving irrelevant attributes such as identity and background, and also supports unseen text descriptions. The key idea is to convert hair editing tasks into hair transfer tasks, with editing conditions represented by different proxies. The editing effects are applied to the input image by blending corresponding proxy features within the hairstyle or hair color feature spaces. In addition to unprecedented user interaction mode support, quantitative and qualitative experiments demonstrate the superior editing effects, preservation of irrelevant attributes, and visual naturalness achieved by HairCLIPv2. The code for HairCLIPv2 is available at https://github.com/wty-ustc/HairCLIPv2.