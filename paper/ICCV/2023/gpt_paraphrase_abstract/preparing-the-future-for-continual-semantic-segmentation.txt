This study focuses on addressing the challenge of Continual Semantic Segmentation (CSS) by introducing a new approach to overcome the difficulties faced by existing methods in learning new classes. The main problem in CSS is the rigidity-plasticity dilemma, which involves learning new knowledge while retaining old knowledge. Current approaches attempt to balance the learning of new and old classes during training on new data. However, this work aims to fundamentally avoid this dilemma rather than dealing with its complexities. The study reveals that the dilemma primarily arises from the greater fluctuation of knowledge for new classes, as they have never been learned before. Additionally, the available data in incremental steps is often insufficient, hindering the model's ability to learn distinctive features for both new and old classes. To tackle these challenges, a novel concept of pre-learning for future knowledge is introduced. This approach optimizes the feature space and output space for unlabeled data, enabling the model to acquire knowledge for future classes. Consequently, updating the model for new classes becomes as smooth as for old classes, effectively avoiding the rigidity-plasticity dilemma. Extensive experiments were conducted, and the results demonstrate a significant improvement in the learning of new classes compared to previous state-of-the-art methods.