Gestures are important non-verbal behaviors that accompany speech. Existing methods can generate gestures synchronized with speech rhythm, but they often lack the semantic context of the speech. However, semantic gestures play a crucial role in helping the audience understand the speech context in a more immersive environment. To address this, we present LivelySpeaker, a framework that generates co-speech gestures while being aware of the semantic content and offers various control options. Our approach consists of two stages: script-based gesture generation and audio-guided rhythm refinement. In the script-based stage, we use pre-trained CLIP text embeddings to generate gestures that align semantically with the script. Then, we propose a simple yet effective gesture generation backbone that is conditioned on audio signals and learns to produce realistic motions. We leverage this model to align the script-guided gestures with the audio signals, even in a zero-shot setting. Our two-stage generation framework enables applications such as changing the style of gestures, editing co-speech gestures through textual prompts, and controlling semantic awareness and rhythm alignment using guided diffusion. Extensive experiments demonstrate the superiority of our framework compared to other methods. Additionally, our core generative model achieves state-of-the-art performance on two benchmarks. We will release the code and model to facilitate future research.