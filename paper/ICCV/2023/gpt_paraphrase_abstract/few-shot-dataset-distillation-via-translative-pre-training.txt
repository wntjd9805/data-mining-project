This study focuses on few-shot dataset distillation, which involves synthesizing a distilled dataset using only a few or even a single neural network. The goal is to find a distillation space that allows for optimization of synthetic data in a more efficient manner. The researchers propose a quad-level optimization framework and a bi-level algorithm to learn this distillation space. However, the original algorithm has a memory issue, so the problem is converted to a first-order one using image translation. The translator is pre-trained on large datasets like ImageNet and only requires a few adaptation steps on the target dataset. Experimental results show that the pre-trained translator achieves comparable distillation performance with state-of-the-art methods, with a 15x acceleration. It also demonstrates satisfactory generalization performance across different datasets, storage budgets, and numbers of classes.