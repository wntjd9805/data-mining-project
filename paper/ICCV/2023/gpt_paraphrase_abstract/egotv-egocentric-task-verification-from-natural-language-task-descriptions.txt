In order to develop egocentric agents that can understand everyday tasks described in natural language, we introduce a benchmark and synthetic dataset called EgoTV. The objective of EgoTV is to verify the execution of tasks in egocentric videos based on their natural language descriptions. The dataset includes pairs of videos and task descriptions for multi-step tasks, which involve sub-task decompositions, state changes, object interactions, and sub-task ordering constraints. Additionally, EgoTV also provides abstracted task descriptions that offer partial details on task accomplishment methods. This dataset requires reasoning abilities in causal, temporal, and compositional aspects of both video and language modalities, which are lacking in existing datasets. We observe that current vision-language models struggle with the comprehensive reasoning required for task verification in EgoTV. To address this, we propose a novel approach called Neuro-Symbolic Grounding (NSG), which utilizes symbolic representations to capture the compositional and temporal structure of tasks. We demonstrate the capabilities of NSG in task tracking and verification using the EgoTV dataset and a real-world dataset derived from CrossTask. We have made the EgoTV and CrossTask datasets, as well as the NSG model, open-source for future research on egocentric assistive agents.