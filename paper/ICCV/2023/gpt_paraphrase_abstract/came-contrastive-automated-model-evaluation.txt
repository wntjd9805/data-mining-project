The AutoEval framework aims to evaluate machine learning models without relying on a labeled testing set. However, current methods heavily depend on computing distribution shifts between the unlabeled testing set and the training set, which hinders the implementation of this technology in real-world ML development. To overcome this obstacle, we propose a new framework called Contrastive Automatic Model Evaluation (CAME). CAME eliminates the need for the training set by utilizing a contrastive loss to analyze the relationship between model performance and a theoretical analysis. Through extensive empirical validation, we establish a predictable relationship between the two by deducing on the unlabeled testing set. CAME outperforms previous methods and achieves state-of-the-art results in AutoEval.