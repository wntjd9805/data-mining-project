Large-scale visual localization systems currently rely on 3D point clouds created from image collections using structure-from-motion. However, matching a query image's local features directly against the point cloud poses a challenge due to the scale of the nearest-neighbor search problem. To address this, recent approaches have proposed a hybrid method where a global embedding is used to retrieve a subset of database images, and the query's local features are only matched against this subset. Despite the need to compute two types of features for each query image, it has been widely believed that global embeddings are crucial for image retrieval in visual localization. In this paper, we challenge this assumption and introduce Constrained Approximate Nearest Neighbors (CANN), a joint solution that performs k-nearest-neighbor retrieval across both the geometry and appearance space using only local features. We establish the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and demonstrate how CANN improves visual localization. Our experiments on public localization benchmarks show that our method outperforms state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Furthermore, CANN is significantly faster in both index and query time compared to feature aggregation schemes for these datasets. We will release the code for our method.