This paper examines the issue of sparse supervision in the DETR model caused by a lack of positive query samples in one-to-one set matching. This sparse supervision hinders the encoder's feature learning and attention learning in the decoder. To address this problem, the authors propose a new training scheme called Co-DETR, which leverages versatile label assignment methods to improve the efficiency and effectiveness of DETR-based detectors. Co-DETR enhances the encoder's learning ability by training multiple parallel auxiliary heads supervised by one-to-many label assignments like ATSS and Faster RCNN. Additionally, the authors introduce extra customized positive queries by extracting positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. Importantly, Co-DETR introduces no additional parameters or computational cost to the original detector and does not require hand-crafted non-maximum suppression. The authors evaluate the effectiveness of Co-DETR on various DETR variants, achieving improved performance on COCO val and LVIS val datasets. Notably, with the ViT-L backbone, Co-DETR achieves superior results on COCO test-dev and LVIS val compared to previous methods, despite having smaller model sizes. The code for Co-DETR is available at https://github.com/Sense-X/Co-DETR.