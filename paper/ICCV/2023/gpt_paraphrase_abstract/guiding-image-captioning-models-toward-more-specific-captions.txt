The task of generating captions for images is typically done by matching them to reference image-caption pairs. However, the short length of reference captions in standard datasets may not be enough to uniquely identify the images. This problem is even more pronounced when models are trained using image-alt text pairs from the internet. This study demonstrates that it is possible to generate more specific captions with minimal changes to the training process. By implementing classifier-free guidance, the autoregressive captioning model is fine-tuned to estimate both conditional and unconditional distributions over captions. The guidance scale used during decoding allows for a trade-off between maximizing the probability of a caption given an image and the probability of an image given a caption. Compared to standard greedy decoding, using a guidance scale of 2 significantly improves metrics that don't rely on reference captions, such as CLIPScore and captionâ†’image retrieval performance. However, it worsens reference-based captioning metrics. The study also explores the use of language models to guide the decoding process, resulting in slight improvements over the reference-free vs. reference-based captioning metrics. Additionally, the quality of captions generated from a model trained solely on minimally curated web data is substantially improved.