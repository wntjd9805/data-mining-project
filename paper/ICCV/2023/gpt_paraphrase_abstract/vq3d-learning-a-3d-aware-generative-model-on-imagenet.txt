Recent research has demonstrated the feasibility of training generative models to create 3D content from 2D image datasets, specifically for single object classes like human faces, animal faces, or cars. However, these models struggle when applied to larger and more complex datasets. To address this issue and effectively model diverse and unconstrained image collections such as ImageNet, we propose a novel approach called VQ3D. This method incorporates a NeRF-based decoder into a two-stage vector-quantized autoencoder. In Stage 1, our model can reconstruct an input image and enable changes in camera position around the image. In Stage 2, it can generate entirely new 3D scenes. VQ3D successfully generates and reconstructs 3D-aware images from the extensive 1000-class ImageNet dataset, which contains 1.2 million training images. Moreover, it achieves a competitive ImageNet generation FID score of 16.8. For more information, please visit our project webpage at the provided URL.