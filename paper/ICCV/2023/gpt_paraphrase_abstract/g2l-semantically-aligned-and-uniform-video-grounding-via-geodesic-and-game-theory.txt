This study addresses the limitations of vanilla contrastive learning in video grounding and proposes a new framework called Geodesic and Game Localization (G2L) to overcome these limitations. The authors argue that vanilla contrastive learning fails to capture the correlations between temporally distant moments and leads to inconsistent video representations in the presence of semantic overlapping and sparse annotations. To address these issues, the G2L framework leverages geodesic distance to quantify the correlations among moments and guide the model in learning correct cross-modal representations. Additionally, the authors introduce the concept of semantic Shapley interaction based on geodesic distance sampling to achieve fine-grained semantic alignment in similar moments. The effectiveness of the proposed method is demonstrated through experiments on three benchmark datasets.