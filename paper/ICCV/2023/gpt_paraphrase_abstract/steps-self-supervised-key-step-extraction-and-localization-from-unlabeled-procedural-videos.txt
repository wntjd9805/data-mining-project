We aim to solve the problem of identifying important steps in unlabelled procedural videos, with the goal of enhancing job training and performance using Augmented Reality (AR) headsets. Our approach involves two main steps: representation learning and key step extraction. To achieve this, we introduce a training objective called Bootstrapped Multi-Cue Contrastive (BMC2) loss, which allows us to learn distinct representations for different steps without relying on labels. Unlike previous methods, we develop techniques to train a lightweight temporal module that utilizes readily available features for self-supervision. This allows us to effectively utilize multiple cues such as optical flow, depth, and gaze information to learn discriminative features for key steps, making our approach suitable for AR applications. Finally, we extract key steps using an adjustable algorithm that clusters the representations and samples. In comparison to previous approaches, our method shows significant improvements in localizing key steps and classifying procedural phases. Qualitative results demonstrate that the extracted key steps are meaningful and succinctly represent various steps in procedural tasks. The code for our approach is available at https://github.com/anshulbshah/STEPs.