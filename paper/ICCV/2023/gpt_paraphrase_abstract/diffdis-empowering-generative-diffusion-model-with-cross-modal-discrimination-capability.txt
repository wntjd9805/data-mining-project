Recently, there have been impressive advancements in large-scale diffusion models, such as Stable diffusion and DallE2, for image synthesis. At the same time, large-scale cross-modal pre-trained models like CLIP, ALIGN, and FILIP have proven to be effective for various tasks by aligning vision and language embeddings. This paper aims to explore the potential of combining generation and discrimination modeling. The proposed approach, called DiffDis, integrates cross-modal generative and discriminative pretraining into a single framework using the diffusion process. DiffDis approaches the image-text discriminative problem by treating it as a generative diffusion process, where the text embedding is generated from the text encoder conditioned on the image. To enable image-text discriminative learning, a dual-stream network architecture is introduced, which combines the noisy text embedding with latent image knowledge from different scales. Additionally, the generative and discriminative tasks can share the image-branch network structure in the multi-modality model, resulting in more efficient training. Through diffusion-based unified training, DiffDis achieves improved generation ability and cross-modal semantic alignment within a single architecture. Experimental results demonstrate that DiffDis outperforms single-task models in both image generation and image-text discriminative tasks, with notable improvements in zero-shot classification accuracy and zero-shot image synthesis FID. Figure 1 illustrates a comparison between our framework and single-task models, highlighting the integration of discriminative and generative tasks in the DiffDis framework.