Spiking neural networks (SNNs) have gained interest as energy-efficient alternatives to conventional neural networks (CNNs). To train deep SNN models, batch normalization (BN) techniques have been proposed. However, the complex dynamics of spiking neurons disrupt the regulated data flow after the BN layer before the nonlinear activation. To address this, we propose adding another BN layer, called MPBN, before the firing function to normalize the membrane potential again. To reduce the time cost, we introduce a training-inference-decoupled re-parameterization technique to fold the trained MPBN into the firing threshold. This technique eliminates any extra time burden during inference. Additionally, the MPBN can adopt an element-wise form, while BNs after the convolution layer can only use a channel-wise form. Experimental results demonstrate the effectiveness of the proposed MPBN on popular non-spiking static and neuromorphic datasets.