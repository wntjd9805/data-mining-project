This paper introduces IP-SLT, a straightforward yet efficient framework for translating sign language (SLT). IP-SLT utilizes a recurrent structure and improves the semantic representation of the input sign language video through iterative refinement. This approach imitates the process of human reading, where a sentence is repeatedly digested until complete understanding is achieved. IP-SLT consists of three main components: feature extraction, prototype initialization, and iterative prototype refinement. The initialization module generates an initial prototype based on the visual features extracted by the feature extraction module. Then, the iterative refinement module utilizes a cross-attention mechanism to refine the prototype by combining it with the original video features. Through repeated refinement, the prototype gradually reaches a more stable and accurate state, resulting in a smooth and appropriate translation. Additionally, to capture the sequential dependence of prototypes, an iterative distillation loss is proposed to transfer knowledge from the final iteration to the previous ones. Since the autoregressive decoding process is only executed once during inference, IP-SLT can enhance various SLT systems without significant overhead. Extensive experiments on public benchmarks are conducted to validate the effectiveness of IP-SLT.