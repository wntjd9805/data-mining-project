Fashion designers use fashion illustration as a means to convey their creative vision and bring their design ideas to life by showcasing how clothes interact with the human body. Computer vision can play a significant role in enhancing the fashion design process. While previous research has primarily focused on virtual try-on of garments, we propose a new task called multimodal-conditioned fashion image editing. This task involves generating fashion images that are centered around humans, guided by various multimodal prompts such as text, human body poses, and garment sketches. To address this problem, we introduce a novel architecture based on latent diffusion models, a previously unexplored approach in the fashion domain. As there are no existing datasets suitable for this task, we expand two existing fashion datasets, Dress Code and VITON-HD, by incorporating multimodal annotations collected through a semi-automatic process. Our experimental results on these newly augmented datasets demonstrate the effectiveness of our proposal, both in terms of realism and coherence with the provided multimodal inputs. To facilitate further research, we have made the source code and collected multimodal annotations publicly available at: https://github.com/aimagelab/multimodal-garment-designer.