This paper focuses on the problem of semi-supervised 3D object detection, which is important due to the high cost of annotating cluttered 3D indoor scenes. The authors propose a new approach based on the self-teaching framework, which has shown promising results in semi-supervised learning. However, adapting this framework to the detection problem is challenging because of the issue of proposal matching. Existing methods rely on two-stage pipelines that match proposals generated in the first stage, resulting in sparse training signals. In contrast, the authors propose a single-stage algorithm that allows for spatially dense training signals. A key challenge in this new design is the quantization error caused by point-to-voxel discretization, which leads to misalignment between transformed views in the voxel domain. To address this, the authors derive and implement closed-form rules that compensate for this misalignment on-the-fly. The experimental results demonstrate the effectiveness of the proposed approach, achieving a significant improvement in the Scan-Net mAP@0.5 metric from 35.2% to 48.5% using only 20% annotation. The codes and data used in the study are publicly available.