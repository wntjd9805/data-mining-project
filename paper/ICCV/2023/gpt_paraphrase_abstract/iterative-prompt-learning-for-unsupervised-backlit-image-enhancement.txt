We propose CLIP-LIT, a new method for enhancing backlit images using unsupervised learning. We utilize the Contrastive Language-Image Pre-Training (CLIP) approach to improve pixel-level image enhancement. CLIP helps differentiate between backlit and well-lit images and identify different luminance regions, which aids in optimizing the enhancement network. However, applying CLIP directly to enhancement tasks is challenging due to the lack of accurate prompts. To address this, we develop a prompt learning framework that learns prompt pairs by ensuring text-image similarity in the CLIP latent space. We train the enhancement network based on the similarity between the enhanced results and the initial prompt pair. We iteratively refine the prompt learning framework to reduce distribution gaps and enhance performance. Our method achieves visually pleasing results without the need for paired data, surpassing state-of-the-art methods in terms of visual quality and generalization ability.