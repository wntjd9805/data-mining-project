We present BiADT, a Bidirectional Alignment for domain adaptive Detection with Transformers, which aims to enhance the performance of cross-domain object detection. Existing methods that employ adversarial learning techniques typically use a gradient reverse layer (GRL) to reduce the domain gap between the source and target domains in terms of feature representations. However, applying GRL directly on global image or object representations may not be ideal since different image parts and objects may possess varying levels of domain-specific characteristics. In our proposed BiADT approach, we explicitly estimate token-wise domain-invariant and domain-specific features within the image and object token sequences. BiADT incorporates a novel deformable attention and self-attention mechanism, which facilitates bi-directional domain alignment and minimization of mutual information. These two objectives effectively diminish the domain gap in domain-invariant representations while simultaneously enhancing the distinctiveness of domain-specific features. Experimental results demonstrate that BiADT consistently achieves highly competitive performance compared to the state-of-the-art (SOTA) on various cross-domain detection tasks such as Cityscapes-to-FoggyCityscapes, Sim10K-to-Cityscapes, and Cityscapes-to-BDD100K. It outperforms the strong baseline method, AQT, by 2.0, 2.1, and 2.4 in mAP50, respectively. The implementation of BiADT is publicly available at https://github.com/helq2612/biADT.