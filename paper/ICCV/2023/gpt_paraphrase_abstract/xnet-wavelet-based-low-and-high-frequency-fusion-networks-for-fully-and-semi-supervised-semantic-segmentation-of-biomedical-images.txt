The advancement of deep neural networks (DNNs) has improved the fully- and semi-supervised semantic segmentation of biomedical images. However, most DNN models are designed to support either fully- or semi-supervised learning, limiting their capabilities. Additionally, few fully-supervised models consider the low frequency (LF) and high frequency (HF) information in images to enhance performance. In consistency-based semi-supervised models, artificially designed perturbations can introduce negative learning bias. To address these issues, we propose XNet, a wavelet-based LF and HF fusion model that supports both fully- and semi-supervised semantic segmentation. Our model outperforms existing models in both fully- and semi-supervised learning by extracting LF and HF information for consistency training, reducing the learning bias caused by artificial perturbations. Extensive experiments on two 2D and two 3D datasets demonstrate the effectiveness of our model. The code for our model is available at https://github.com/Yanfeng-Zhou/XNet.