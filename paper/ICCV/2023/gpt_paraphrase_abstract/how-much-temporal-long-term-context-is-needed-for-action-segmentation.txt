Modeling long-term context in videos is crucial for fine-grained tasks like temporal action segmentation. However, the amount of long-term temporal context required for optimal performance remains an open question. While transformers can capture this context, it becomes computationally prohibitive for long videos. Recent studies combine temporal convolutional networks with self-attentions computed only for a local temporal window to address this issue. Although these approaches yield good results, their performance is limited by their inability to capture the complete video context. This study aims to determine the necessary amount of long-term temporal context for temporal action segmentation by introducing a transformer-based model that utilizes sparse attention to capture the full video context. The proposed model is compared to the current state-of-the-art on three temporal action segmentation datasets (50Salads, Breakfast, and Assembly101). Experimental results indicate that modeling the full video context is essential for achieving the best performance in temporal action segmentation.