Current style transfer models are limited to either images or videos, requiring separate models and training processes for each domain. This paper introduces UniST, a Unified Style Transfer framework that can handle both images and videos. UniST utilizes a domain interaction transformer (DIT) which explores context information within each domain and allows for joint learning. This enables the incorporation of temporal information from videos into image style transfer and the integration of appearance texture from images into video style transfer, resulting in improved performance for both tasks. To address the computational burden of traditional multi-head self-attention, a simplified axial multi-head self-attention (AMSA) is proposed for DIT, which maintains style transfer performance while improving computational efficiency. Extensive experiments demonstrate that UniST outperforms state-of-the-art approaches in both image and video style transfer tasks. The code for UniST is available at https://github.com/NevSNev/UniST.