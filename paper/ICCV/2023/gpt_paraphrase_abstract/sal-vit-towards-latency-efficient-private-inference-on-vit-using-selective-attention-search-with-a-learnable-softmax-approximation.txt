Private inference (PI) has become a crucial aspect of machine learning inference as a service to ensure data and model privacy. However, current PI frameworks face challenges in terms of computational and communication overheads, especially for large models like vision transformers (ViTs). The main cause of this overhead is the encrypted softmax operation in each self-attention layer.   To address these challenges, we propose SAL-ViT, a solution that enhances the efficiency of PI in ViTs through two innovative techniques. The first technique is called learnable 2Quad (L2Q), which introduces learnable scaling and shifting parameters to the prior 2Quad softmax approximation. This enables improved accuracy while maintaining efficiency.   Furthermore, we observe that external attention (EA) has lower PI latency compared to self-attention (SA), albeit with a compromise in accuracy. To leverage the strengths of both EA and SA, we introduce a selective attention search (SAS) method. This method allows us to integrate lightweight EA ViTs with SA alternatives, maximizing accuracy through a constrained optimization procedure.   Extensive experiments demonstrate the effectiveness of SAL-ViT. On CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets, SAL-ViT achieves an average of 1.28×, 1.28×, and 1.14× lower PI latency, respectively, with 1.79%, 1.41%, and 2.08% higher accuracy compared to existing alternatives.