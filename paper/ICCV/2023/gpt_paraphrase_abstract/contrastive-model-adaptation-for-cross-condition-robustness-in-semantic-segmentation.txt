Standard methods for unsupervised domain adaptation adapt models from a source domain to a target domain using labeled source data and unlabeled target data together. However, in model adaptation, access to the labeled source data is not allowed, meaning only the source-trained model and unlabeled target data are available. This study focuses on adapting models from normal to adverse conditions for semantic segmentation, where image-level correspondences are available in the target domain. The target dataset consists of unlabeled pairs of street images taken at GNSS-matched locations, captured under normal and adverse conditions. The proposed method, called CMA, utilizes these image pairs to learn features that are invariant to the conditions using contrastive learning. CMA encourages the grouping of features based on their semantic content rather than the conditions under which they were captured. To establish accurate cross-domain semantic correspondences, the normal image is warped to match the viewpoint of the adverse image, and warp-confidence scores are used to create robust, aggregated features. The results show that CMA achieves state-of-the-art semantic segmentation performance for model adaptation on various normal-to-adverse adaptation benchmarks, such as ACDC and Dark Zurich. Additionally, CMA is evaluated on a newly obtained benchmark for adverse-condition generalization and demonstrates favorable results compared to standard unsupervised domain adaptation methods, despite the limitation of not having access to the source data. The code for CMA is available at https://github.com/brdav/cma.