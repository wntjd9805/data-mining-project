Tangent Model Composition (TMC) is a technique that combines component models that have been independently fine-tuned based on a pre-trained model. These component models are tangent vectors to the pre-trained model and can be added, scaled, or subtracted to support incremental learning, ensembling, or unlearning. At inference time, the component models are composed through scalar combination, reducing the computational cost of ensembling to that of a single model. TMC achieves a 4.2% improvement in accuracy compared to non-linearly fine-tuned ensembles, while reducing the inference cost by 2.5× to 10×, which scales linearly with the number of component models. Additionally, each component model can be forgotten at zero cost without any residual impact on the inference results. TMC can be used for continual fine-tuning without being constrained by sequential bias and can be executed concurrently on federated data. In experiments conducted on 3 benchmark datasets, TMC consistently outperforms recently published continual fine-tuning methods in task-incremental, class-incremental, and data-incremental settings, even without using a replay buffer. Although TMC is primarily designed for composing models local to a pre-trained embedding, it could potentially be extended to more general settings. The code for TMC is available at: https://github.com/tianyu139/tangent-model-composition.