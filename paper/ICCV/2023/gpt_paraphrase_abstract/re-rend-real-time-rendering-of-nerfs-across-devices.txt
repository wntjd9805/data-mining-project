This paper presents a new method called Re-ReND, which allows for real-time rendering of a pre-trained Neural Radiance Field (NeRF) on devices with limited resources. The proposed approach converts the NeRF into a representation that can be efficiently processed by standard graphics pipelines, enabling real-time performance. The method distills the NeRF by extracting the learned density into a mesh and factorizes the learned color information into a set of matrices representing the scene's light field. This factorization allows for inexpensive matrix multiplications to query the field, resulting in faster rendering compared to traditional radiance field methods. The proposed representation can be implemented using a fragment shader, making it compatible with standard rasterization frameworks. This flexible implementation allows for real-time rendering of NeRFs on various resource-constrained devices, including mobiles and AR/VR headsets. Experimental results show that Re-ReND achieves a significant increase in rendering speed (2.6-fold) compared to the current state-of-the-art methods, without any noticeable loss in quality.