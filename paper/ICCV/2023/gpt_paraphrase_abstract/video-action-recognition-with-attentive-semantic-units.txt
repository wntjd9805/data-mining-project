Visual-Language Models (VLMs) have greatly advanced the field of video action recognition. Previous studies have focused on adapting the visual branch of VLMs to learn video representations based on the semantics of action labels. However, we believe that there is untapped potential in VLMs. To maximize their effectiveness, we propose leveraging the semantic units (SU) present in action labels and their correlations with fine-grained items in frames for more accurate action recognition. SUs are extracted from language descriptions and encompass body parts, objects, scenes, and motions. To better align visual contents with SUs, we introduce a multi-region attention module (MRA) to the visual branch of VLMs. The MRA enables the perception of region-specific visual features, going beyond the original global feature representation. Our approach dynamically attends to and selects relevant SUs using the visual features of frames. These selected SUs are then used to decode spatiotemporal video representations through a cross-modal decoder. In summary, by using SUs as a medium, our method enhances discriminative ability and transferability. In fully-supervised learning, our approach achieved an impressive top-1 accuracy of 87.8% on the Kinetics-400 dataset. In K=2 few-shot experiments, we outperformed the previous state-of-the-art by 7.1% on HMDB-51 and 15.0% on UCF-101.