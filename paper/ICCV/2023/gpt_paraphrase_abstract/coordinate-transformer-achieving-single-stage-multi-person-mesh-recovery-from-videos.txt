The current methods for recovering multi-person 3D meshes from videos are limited by the lack of inter-person interactions and detection and tracking defects. To address these challenges, we propose a new approach called the Coordinate transFormer (Coord-Former) that models multi-person spatial-temporal relations and performs multi-mesh recovery in an end-to-end manner. Unlike existing methods that partition the feature map into coarse-scale patch-wise tokens, Coord-Former uses a Coordinate-Aware Attention mechanism to preserve pixel-level spatial-temporal coordinate information. We also introduce a BodyCenter Attention mechanism to fuse position information. Our experiments on the 3DPW dataset show that CoordFormer outperforms the previous state-of-the-art methods by significant margins in terms of MPJPE, PAMPJPE, and PVE metrics, while also being faster than recent video-based approaches. The code for CoordFormer is publicly available at https://github.com/Li-Hao-yuan/CoordFormer.