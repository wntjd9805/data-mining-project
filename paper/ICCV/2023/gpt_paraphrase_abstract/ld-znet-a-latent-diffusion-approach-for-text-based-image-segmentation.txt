Current large-scale pre-training tasks such as image classification, captioning, and self-supervised techniques do not adequately encourage the learning of semantic boundaries in objects. However, recent generative models that utilize text-based latent diffusion techniques may be able to learn these boundaries. This is because these models are required to generate detailed information about all objects in an image based on a text description. In this study, we propose a method for segmenting real and AI-generated images using latent diffusion models (LDMs) trained on extensive datasets from the internet. We demonstrate that the latent space of LDMs (referred to as z-space) is a superior input representation compared to other feature representations like RGB images or CLIP encodings for text-based image segmentation. By training segmentation models on the latent z-space, which encompasses various domains such as different art forms, cartoons, illustrations, and photographs, we are able to bridge the gap between real and AI-generated images. Our findings indicate that the internal features of LDMs contain valuable semantic information, and we introduce a technique called LD-ZNet to further enhance text-based segmentation performance. Overall, our results show up to a 6% improvement over standard baselines for text-to-image segmentation on natural images. Additionally, for AI-generated imagery, we achieve an improvement of nearly 20% compared to state-of-the-art techniques. More information about our project can be found at the following link: https://koutilya-pnvr.github.io/LD-ZNet/.