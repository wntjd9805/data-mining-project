Domain Adaptive Object Detection (DAOD) aims to improve the detection performance of target domains by reducing the feature distribution gap between the source and target domains. Current approaches use adversarial learning to align distributions in terms of categories, but this causes imbalanced object feature learning when objects are non-uniformly distributed at different scales. To address this, we propose a novel DAOD framework called CSDA that incorporates joint category and scale information. Our framework consists of two modules: 1) SGFF (Scale-Guided Feature Fusion) aligns category representations using discriminators at three scales to learn category-specific features, and 2) SAFE (Scale-Auxiliary Feature Enhancement) encodes scale coordinates into tokens and enhances category-specific features at different scales using self-attention. Our method, implemented on Faster-RCNN and FCOS detectors, achieves state-of-the-art results on three DAOD benchmarks.