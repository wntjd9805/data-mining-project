This paper introduces a new approach called Pose Transfer by Permuting Textures (PT2) for self-driven human pose transfer. The goal is to synthesize new views of a person for a given pose by disentangling pose and texture information. Previous methods achieved this through self-reconstruction, but they preserved some pose information that resulted in unwanted artifacts.   PT2 addresses this issue by disentangling pose from texture at the patch-level. It starts by removing pose from an input image and keeping only the texture information by permuting image patches. Then, the input image is reconstructed by sampling from the permuted textures, enabling patch-level disentanglement.   To enhance the quality of the reconstructed image and recover clothing shape information, PT2 utilizes encoders with multiple kernel sizes in a triple branch network. This helps reduce noise and improve the overall accuracy of the generated images.   The effectiveness of PT2 is demonstrated through extensive experiments on DeepFashion and Market-1501 datasets. PT2 outperforms other self-driven methods and even surpasses some fully-supervised methods in terms of automatic metrics. Additionally, a user study shows that images generated by PT2 are preferred in 68% of cases compared to self-driven approaches from previous work.   The code for PT2 is publicly available at https://github.com/NannanLi999/pt_square.