Current neural rendering methods for generating human avatars typically rely on dense input signals like video or multi-view images, or they utilize a learned prior from large-scale 3D human datasets to perform reconstruction with sparse-view inputs. However, these methods often struggle to achieve realistic reconstruction when only a single image is provided. To address this limitation and enable the efficient creation of animatable 3D humans, we propose a new method called ELICIT. Inspired by humans' ability to estimate body geometry and imagine clothing from a single image, ELICIT incorporates two priors: a 3D geometry prior based on a skinned vertex-based template model (SMPL), and a visual clothing semantic prior using CLIP-based pre-trained models. These priors work together to guide the optimization process and generate plausible content in invisible areas. By leveraging CLIP models, ELICIT can also generate text-conditioned unseen regions using text descriptions. Additionally, we introduce a segmentation-based sampling strategy to enhance visual details by refining different parts of the avatar locally. Through comprehensive evaluations on popular benchmarks like ZJU-MoCAP, Human3.6M, and DeepFashion, we demonstrate that ELICIT outperforms strong baseline methods for avatar creation with only a single image. The code for ELICIT is publicly available for research purposes at the following link: https://huangyangyi.github.io/ELICIT.