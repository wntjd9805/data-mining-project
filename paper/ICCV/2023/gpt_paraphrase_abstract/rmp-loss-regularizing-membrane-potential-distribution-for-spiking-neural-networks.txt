Spiking Neural Networks (SNNs) have gained significant attention due to their energy-saving capabilities by quantizing membrane potentials to transmit information. This approach replaces multiplications with additions, reducing energy consumption. However, quantization introduces errors, leading to information loss. To tackle this issue, we propose a regularizing membrane potential loss (RMP-Loss) technique that adjusts the potential distribution to a range close to the spikes, minimizing quantization error. Our method is simple to implement and train SNNs and consistently outperforms previous state-of-the-art methods across various network architectures and datasets.