We introduce SkeleTR, an innovative framework for skeleton-based action recognition that differs from previous approaches by focusing on general scenarios with variable numbers of people and diverse forms of interaction. Unlike prior work that emphasizes controlled environments, SkeleTR utilizes a two-stage approach. Firstly, it models the dynamics of intra-person skeletons using graph convolutions, and then it employs stacked Transformer encoders to capture important person interactions for action recognition in general scenarios. To address the issue of inaccurate skeleton associations, SkeleTR takes short skeleton sequences as input and increases their number. It offers a unified solution that can be directly applied to multiple skeleton-based action tasks, such as video-level action classification, instance-level action detection, and group-level activity recognition. Moreover, SkeleTR enables transfer learning and joint training across different action tasks and datasets, leading to improved performance. In evaluations on various skeleton-based action recognition benchmarks, SkeleTR achieves state-of-the-art performance.