Detecting image manipulation is crucial in order to prevent the spread of fake images and misinformation, which can have harmful effects on our daily lives. Image manipulation detection (IMD) is the main technique used to address these issues, but it faces challenges in two key areas. Firstly, there is data uncertainty, where the manipulated artifacts are often difficult for humans to identify, leading to noisy labels that can disrupt model training. Secondly, there is model uncertainty, where the same object may have different categories (tampered or not) due to manipulation operations, potentially confusing the model training process and resulting in unreliable outcomes. Previous research has primarily focused on addressing model uncertainty by designing intricate features and networks, but the problem of data uncertainty has been largely overlooked. In this paper, we tackle both problems by introducing an uncertainty-guided learning framework that utilizes a novel Uncertainty Estimation Network (UEN). The UEN is trained using dynamic supervision and generates estimated uncertainty maps to refine manipulation detection results, significantly reducing the learning difficulties. This is the first work to incorporate uncertainty modeling into IMD. Extensive experiments on various datasets demonstrate that our method achieves state-of-the-art performance, confirming its effectiveness and generalizability.