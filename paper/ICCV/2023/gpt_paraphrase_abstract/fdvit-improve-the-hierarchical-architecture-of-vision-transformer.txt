Transformer-based models in computer vision have shown great success, but their high computational costs limit their use on devices with limited resources. One reason for this is that vision transformers have redundant calculations due to the self-attention operation generating similar patches later in the network. Hierarchical architectures have been proposed to address this issue, but downsampling layers that shrink spatial dimensions by half result in excessive information loss. In this paper, we introduce FDViT, which improves the hierarchical architecture of vision transformers by using a flexible downsampling layer that can smoothly reduce the sizes of middle feature maps without being limited to integer stride. Additionally, a masked auto-encoder architecture is employed to facilitate training of the flexible downsampling layer and generate informative outputs. Experimental results on benchmark datasets demonstrate that our proposed method reduces computational costs, improves classification performance, and achieves state-of-the-art results. For instance, our FDViT-S model achieves a top-1 accuracy of 81.5%, which is 1.7 percentage points higher than the ViT-S model and reduces FLOPs by 39%.