Low-light conditions not only impair human vision but also negatively impact the performance of models in vision tasks. Previous works have made progress in day-night domain adaptation but rely on domain knowledge from nighttime datasets specific to the task. This paper introduces a more complex scenario called zero-shot day-night domain adaptation, which eliminates the need for nighttime data. Instead of focusing solely on image-level translation or model-level adaptation, this paper proposes a similarity min-max paradigm that integrates both approaches. At the image level, the method darkens images to increase the domain gap, while at the model level, it maximizes the feature similarity between darkened and normal-light images to improve model adaptation. This novel approach significantly enhances model generalizability, making it the first work to jointly optimize both levels. Extensive experiments validate the effectiveness and broad applicability of the proposed method across various nighttime vision tasks, including classification, semantic segmentation, visual place recognition, and video action recognition. A project page with further information is available at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.