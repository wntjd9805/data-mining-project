The existing methods for audio-driven talking-head synthesis have limitations due to their inflexibility and inefficiency, requiring expensive training to transfer emotions from guidance videos to talking-head predictions. To address this, we propose the Emotional Adaptation for Audio-driven Talking-head (EAT) method. Our approach transforms emotion-agnostic talking-head models into emotion-controllable ones in a cost-effective and efficient manner through parameter-efficient adaptations. We utilize a pretrained emotion-agnostic talking-head transformer and introduce three lightweight adaptations (Deep Emotional Prompts, Emotional Deformation Network, and Emotional Adaptation Module) to enable precise and realistic emotion controls. Through experiments, we demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks like LRW and MEAD. Moreover, our parameter-efficient adaptations show remarkable generalization ability, even in scenarios with limited emotional training videos.