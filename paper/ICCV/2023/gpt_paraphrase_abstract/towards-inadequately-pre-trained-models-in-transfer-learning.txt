Transfer learning is a popular approach in deep learning, especially in scenarios where there is limited annotation data available. Previous research has shown that better ImageNet pre-trained models have improved transferability to other tasks. However, this paper presents a contradictory finding. It shows that models at middle epochs, which are not fully trained, outperform fully trained models when used as feature extractors. On the other hand, the fine-tuning performance still improves with the performance of the source task. This indicates that there is no strong positive correlation between ImageNet accuracy and transfer learning performance. The paper conducts extensive analyses on the features of models before the softmax layer to provide explanations for this phenomenon. The findings suggest that models tend to initially learn spectral components related to large singular values during pre-training, and the residual components become more important during fine-tuning. The paper includes a toy experiment demonstrating transfer learning from a pre-trained ResNet18 model on CIFAR10 to a subset of MNIST. The experiment shows that the 5th-epoch model achieves the best performance as a feature extractor, suggesting that further pre-training on the source task may harm the feature quality for the target task. However, when fine-tuning the entire model, more adequate pre-training leads to higher transfer learning performance.