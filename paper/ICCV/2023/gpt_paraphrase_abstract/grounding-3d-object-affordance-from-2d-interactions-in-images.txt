The aim of grounding 3D object affordance is to locate regions in 3D space that represent the potential actions or interactions with objects. Previous studies have focused on connecting visual affordances with geometric structures by annotating interactive regions of interest on objects. However, this approach lacks generalization and fails to capture the essence of learning how to use object affordances. Humans can perceive object affordances in the physical world through demonstration images or videos, which inspired us to propose a new task: grounding 3D object affordance from 2D interactions in images. This task presents the challenge of anticipating affordance through interactions from different sources. To tackle this problem, we propose a novel network called Interaction-driven 3D Affordance Grounding Network (IAG), which aligns the region features of objects from different sources and models interactive contexts for 3D object affordance grounding. Additionally, we have created a Point-Image Affordance Dataset (PIAD) to support this task. Through comprehensive experiments on PIAD, we demonstrate the reliability of our proposed task and the superiority of our method. The project can be accessed at https://github.com/yyvhang/IAGNet.