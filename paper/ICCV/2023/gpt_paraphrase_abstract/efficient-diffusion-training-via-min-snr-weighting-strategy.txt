Denoising diffusion models have been widely used for generating images, but their training process often suffers from slow convergence. This paper identifies conflicting optimization directions between timesteps as one of the causes for this issue. To tackle this problem, the authors propose a new approach called Min-SNR-γ, which treats diffusion training as a multi-task learning problem. This method adjusts the loss weights of timesteps based on clamped signal-to-noise ratios, effectively resolving conflicts among timesteps. Experimental results show that the proposed approach significantly improves the convergence speed, being 3.4 times faster than previous weighting strategies. Additionally, it achieves a new record FID score of 2.06 on the ImageNet 256 × 256 benchmark using smaller architectures compared to previous state-of-the-art methods. The code for this approach is available at https://github.com/TiankaiHang/Min-SNR-Diffusion-Training.