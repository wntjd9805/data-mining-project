The conventional approach to video captioning involves sampling frames from a decoded video and then conducting further processes such as feature extraction and captioning model learning. However, this method may overlook important information in the videos and result in decreased performance. Additionally, the redundant information in the sampled frames can lead to inefficiency in video captioning inference. To address these issues, we propose a new approach that focuses on video captioning in the compressed domain. This approach offers several advantages over the existing pipeline. Firstly, the compressed video, which includes I-frames, motion vectors, and residuals, provides highly distinguishable information that can be leveraged for learning without the need for manual frame sampling. Secondly, the captioning model is more efficient in inference as it processes smaller and less redundant information. We introduce a simple yet effective end-to-end transformer model in the compressed domain for video captioning. Our method achieves state-of-the-art performance on various benchmarks while running nearly twice as fast as existing approaches. The code for our method is available at https://github.com/acherstyx/CoCap.