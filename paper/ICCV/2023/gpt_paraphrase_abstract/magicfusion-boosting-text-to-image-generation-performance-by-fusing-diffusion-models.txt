The rise of open-source AI communities has resulted in the development of numerous powerful text-guided diffusion models that are trained on different datasets. However, there has been limited research on combining these models to leverage their individual strengths. This study proposes a straightforward yet impactful technique called Saliency-aware Noise Blending (SNB) to enhance the performance of fused text-guided diffusion models in generating controllable outputs. The study reveals a strong correlation between the responses of classifier-free guidance and the saliency of generated images. Based on this finding, the proposed method relies on blending the predicted noises of two diffusion models in a saliency-aware manner, allowing each model to contribute based on its expertise. SNB does not require training and can be integrated into the sampling process of the Diffusion Model using Denoising Diffusion Probabilistic Models (DDIM). Moreover, it automatically aligns the semantic spaces of the two noise sources without the need for additional annotations like masks. Extensive experiments demonstrate the remarkable effectiveness of SNB in various applications. More information about the project can be found at https://magicfusion.github.io/.