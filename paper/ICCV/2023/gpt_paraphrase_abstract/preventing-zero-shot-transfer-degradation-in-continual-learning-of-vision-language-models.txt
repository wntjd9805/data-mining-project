Continual learning (CL) is a useful technique for adapting pre-trained vision-language models to new or under-trained data distributions without the need for re-training. However, when applying CL to the Contrastive Language-Image Pre-training (CLIP) model, we have observed a significant decline in its ability to transfer knowledge without any prior training. This is due to a phenomenon known as catastrophic forgetting. Existing CL methods can address this issue by replaying previous data, but they are unable to access the private pre-training dataset used for CLIP. Furthermore, while replaying data from previously learned tasks can improve their performance, it comes at the expense of zero-shot performance. To overcome these challenges, we propose a novel method called ZSCL that prevents the degradation of zero-shot transfer in the continual learning of vision-language models, both in the feature and parameter space. In the feature space, we introduce a reference dataset that facilitates distillation between the current and initial models. This reference dataset should possess semantic diversity but does not need to be labeled, seen in pre-training, or consist of matched image-text pairs. In the parameter space, we prevent significant shifts in parameters by averaging weights during training. We also introduce a more challenging benchmark called Multi-domain Task Incremental Learning (MTIL) to evaluate different methods. Unlike traditional class-incremental learning, the MTIL benchmark includes tasks from various domains, rather than being limited to a single dataset with class-separated tasks. Our proposed method outperforms other methods in both the traditional class-incremental learning setting and the MTIL benchmark, achieving an average score improvement of 9.7%. The code for our method can be found at https://github.com/Thunderbeee/ZSCL.