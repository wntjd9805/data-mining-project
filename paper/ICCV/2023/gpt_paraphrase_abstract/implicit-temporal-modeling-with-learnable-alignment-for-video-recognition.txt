Contrastive language-image pretraining (CLIP) has achieved impressive results in various image tasks. However, incorporating effective temporal modeling into CLIP remains a challenging and important problem. Existing approaches that combine spatial and temporal modeling compromise between efficiency and performance. While previous studies have focused on modeling temporal information using straight through tubes, we have discovered that simple frame alignment is sufficient without the need for temporal attention. In this paper, we propose a novel method called Implicit Learnable Alignment (ILA) that achieves high performance while minimizing temporal modeling efforts. Our approach predicts an interactive point in each frame and enhances the features around these points to implicitly align the frames. The aligned features are then pooled into a single token and used in subsequent spatial self-attention. By eliminating the need for costly or insufficient temporal self-attention, our method improves efficiency without sacrificing accuracy. Extensive experiments on benchmark datasets demonstrate the superiority and generality of our ILA module. Specifically, our ILA achieves a top-1 accuracy of 88.7% on the Kinetics-400 dataset with significantly fewer FLOPs compared to other state-of-the-art models like Swin-L and ViViT-H. The code for our method is publicly available at https://github.com/Francis-Rings/ILA.