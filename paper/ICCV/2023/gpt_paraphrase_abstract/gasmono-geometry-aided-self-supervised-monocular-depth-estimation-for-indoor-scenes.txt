This paper addresses the challenges of estimating depth in indoor scenes using self-supervised monocular techniques. These challenges arise from large rotations between frames and low texture. To overcome these challenges, the authors propose several approaches. Firstly, they use multi-view geometry to obtain coarse camera poses from monocular sequences, which helps in dealing with the rotation issue. However, they find that introducing these geometric coarse poses without refinement does not improve performance due to scale ambiguity across different scenes in the training dataset. To tackle this problem, the authors propose refining the poses during training through rotation and translation/scale optimization.Additionally, to mitigate the impact of low texture, the authors combine the global reasoning of vision transformers with an overfitting-aware, iterative self-distillation mechanism. This combination provides more accurate depth guidance from the network itself. The effectiveness of each component in the proposed framework is evaluated through experiments on NYUv2, ScanNet, 7scenes, and KITTI datasets. The results demonstrate that the framework achieves state-of-the-art performance in indoor self-supervised monocular depth estimation and exhibits outstanding generalization ability.The code and models for implementing the proposed framework are available at https://github.com/zxcqlf/GasMono.