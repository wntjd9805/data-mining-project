We introduce a new model called Tracking Any Point (TAP) that can effectively track any specified point on any physical surface in a video sequence. Our method consists of two stages: a matching stage that finds a suitable match for the query point in each frame, and a refinement stage that updates the trajectory and query features based on local correlations. Our model outperforms all baseline methods by a significant margin on the TAP-Vid benchmark, with an approximate 20% improvement in average Jaccard score on DAVIS. It also allows for fast inference on long and high-resolution videos, with the ability to track points faster than real-time on a modern GPU. Furthermore, we demonstrate a proof-of-concept diffusion model that can generate plausible animations by utilizing high-quality trajectories obtained from a large dataset. For additional information, including visualizations, source code, and pretrained models, please visit https://deepmind-tapir.github.io.