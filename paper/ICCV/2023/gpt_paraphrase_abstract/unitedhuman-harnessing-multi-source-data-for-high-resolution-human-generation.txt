Current methods in human generation struggle to accurately synthesize specific regions like faces and hands due to limited and low-resolution information in holistic human datasets. To address this issue, we propose a novel approach that utilizes multi-source datasets containing images of varying resolutions to train a high-resolution human generative model. However, multi-source data poses two challenges: the presence of non-aligned parts and different scales. To overcome these challenges, we introduce an end-to-end framework called UnitedHuman. This framework incorporates a Multi-Source Spatial Transformer that aligns the images spatially using a human parametric model. Additionally, we propose a continuous GAN with global-structural guidance and CutMix consistency to train the scale-invariant generative model using patches from different datasets. Our extensive experiments demonstrate that our model, trained on multi-source data, outperforms models trained on holistic datasets in terms of quality. For more information, please visit our project page: https://unitedhuman.github.io/.