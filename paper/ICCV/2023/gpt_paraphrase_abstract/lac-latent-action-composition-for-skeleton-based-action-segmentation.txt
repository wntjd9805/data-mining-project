Existing approaches to skeleton-based action segmentation face limitations in recognizing composable actions in untrimmed videos. These approaches extract local visual features from skeleton sequences and then use a temporal model to classify frame-wise actions. However, the visual features are inadequate in expressing composable actions, leading to limited performance. To address this, we propose a new self-supervised framework called Latent Action Composition (LAC). LAC incorporates a novel generation module that synthesizes new sequences by performing arithmetic operations on latent representations of multiple input skeleton sequences. These synthesized sequences, which exhibit diversity and complexity, are then used to learn visual representations of skeletons in both the sequence and frame spaces through contrastive learning. The resulting visual encoder is highly expressive and can be directly applied to action segmentation tasks without the need for additional temporal models. We conducted a transfer-learning study and demonstrated that representations learned from pre-trained LAC significantly outperform the state-of-the-art on TSU, Charades, and PKU-MMD datasets.