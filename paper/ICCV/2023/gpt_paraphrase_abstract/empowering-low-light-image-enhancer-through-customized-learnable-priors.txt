Current methods for enhancing low-light images using deep neural networks lack transparency and interpretability. Although some unfolding solutions have been proposed to address these issues, they rely on ambiguous and implicit priors. To improve the transparency of the deep unfolding paradigm, we propose a new approach that utilizes customized learnable priors. Inspired by the feature representation capabilities of Masked Autoencoder (MAE), we develop MAE-based illumination and noise priors from two perspectives: structure flow and optimization flow. The structure flow involves training the MAE to learn the illumination properties of a normal-light image and embedding it into the unfolding architecture. The optimization flow includes training the MAE to learn the gradient representation of a normal-light image and using it as a regularization term to reduce noise in the model output. These designs enhance the interpretability and representation capability of the model. Extensive experiments on multiple low-light image enhancement datasets demonstrate that our proposed paradigm outperforms state-of-the-art methods. The code for our approach is available at https://github.com/zheng980629/CUE.