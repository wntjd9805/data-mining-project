Efficient and accurate algorithms for human pose estimation in videos are crucial for various practical applications. However, existing methods often focus on temporal relations and overlook spatial attention, leading to suboptimal results. Additionally, the computational complexity of transformer-based models poses a challenge. To overcome these limitations, we propose MixSynthFormer, a transformer encoder-like model with MLP-based mixed synthetic attention. By incorporating both spatial and temporal attention, our model can accurately estimate human poses in videos using sparsely sampled frames. Moreover, the flexibility of our model allows for its application in other motion synthesis tasks. Extensive experiments on pose estimation, body mesh recovery, and motion prediction demonstrate the effectiveness and efficiency of MixSynthFormer. The code is available at https://github.com/ireneesun/MixSynthFormer.git.