This study focuses on transferring a pre-trained Generative Adversarial Network (GAN) from one image domain to another with minimal supervision, using just one reference image. The main challenge is to generate photo-realistic and diverse images that retain the characteristics of the target domain. Unlike previous approaches, this work proposes two lightweight modules in the generator and discriminator. The generator includes an attribute adaptor that preserves prior knowledge while maintaining synthesis quality and diversity. Additionally, the discriminator is equipped with an attribute classifier to ensure the generator captures the appropriate characteristics of the reference image. To address the limited diversity of training data, truncation is used in the training process to constrain the diversity of the latent space, making optimization easier. The proposed approach outperforms existing alternatives, especially in terms of synthesis diversity, even when there are large domain gaps. It also converges robustly within a few minutes for each experiment. The code and models for this approach are available at https://genforce.github.io/genda/.