Recent studies have utilized large-scale video datasets to train text-to-video (T2V) generators in order to replicate the success of text-to-image (T2I) generation. However, this approach is computationally expensive. This paper introduces a new T2V generation setting called "One-Shot Video Tuning," where only one text-video pair is used for training. The proposed model is built upon state-of-the-art T2I diffusion models that are pre-trained on extensive image data. Two key observations are made: 1) T2I models can generate still images representing verb terms, and 2) extending T2I models to generate multiple images concurrently shows excellent content consistency. To enhance the learning of continuous motion, a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy called "Tune-A-Video" are introduced. During inference, DDIM inversion is employed to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable performance of the proposed method across various applications.