The perception of sound source direction in a visual scene, known as sound source localization, is easily done by humans. Previous studies on learning-based sound source localization have primarily focused on the localization aspect of the problem. However, these studies have overlooked an important aspect of the problem, which is cross-modal semantic understanding. This understanding is crucial in comprehending audio-visual events that do not semantically match, such as silent objects or off-screen sounds. To address this, we propose a cross-modal alignment task that is combined with sound source localization to enhance the learning of the interaction between audio and visual modalities. Through this approach, we achieve high performance in both sound source localization and cross-modal retrieval, surpassing the current state-of-the-art methods. Our findings highlight the necessity of jointly addressing both tasks in order to achieve genuine sound source localization.