This study focuses on the task of text-driven human video generation, which involves creating a video sequence based on textual descriptions of a performer's appearance and motions. Unlike general text-driven video generation, this specific task requires maintaining the appearance of the synthesized human while performing complex motions. To address this, the researchers propose a method called Text2Performer that generates realistic human videos with articulated motions from texts.Text2Performer incorporates two novel designs. Firstly, it decomposes the VQVAE latent space into separate representations for human appearance and pose in an unsupervised manner. By doing so, the method ensures that the appearance is consistently maintained throughout the generated frames. Secondly, the researchers introduce a diffusion-based motion sampler called continuous VQ-diffuser. Unlike existing methods that operate in discrete space, continuous VQ-diffuser directly outputs continuous pose embeddings, enabling better motion modeling.To enhance temporal coherence, a motion-aware masking strategy is implemented to spatially and temporally mask the pose embeddings. Additionally, to facilitate research in text-driven human video generation, the researchers contribute a dataset called Fashion-Text2Video. This dataset contains manually annotated action labels and text descriptions.Extensive experiments demonstrate that Text2Performer generates high-quality human videos with diverse appearances and flexible motions, reaching resolutions of up to 512 Ã— 256. For more information, readers can visit the project page at https://yumingj.github.io/projects/Text2Performer.html.