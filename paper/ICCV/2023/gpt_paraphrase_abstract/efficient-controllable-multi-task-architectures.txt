Our objective is to develop a multi-task model that allows users to adjust the compute budget and task performance importance without retraining. This will optimize performance based on varying user needs without the need for extensive computational resources. Our proposed model consists of a shared encoder and task-specific decoders with adjustable channel widths. We control task importance by adjusting decoder capacities and overall computational cost by jointly adjusting the encoder capacity. This approach improves accuracy, gives better control over computational cost, and generates high-quality sub-architectures based on user constraints. We use a novel loss function called "Configuration-Invariant Knowledge Distillation" to ensure consistent representations across different width configurations. Additionally, we present a search algorithm that translates user constraints into runtime configurations for the encoder and decoders. The algorithm assigns a larger budget to the preferred task decoder while searching for an encoder configuration that enhances overall multi-task learning performance. Experimental results on three benchmarks demonstrate the advantages of our approach, including higher controllability and lower compute cost compared to previous methods.