This paper introduces AdaTrans, a new approach for disentangled and conditional face editing. Unlike previous methods that manipulate the latent space of StyleGAN using linear semantic directions, AdaTrans employs an adaptive nonlinear latent transformation. It breaks down the manipulation process into smaller steps, with the direction and size of each step conditioned on both the facial attributes and the latent codes. This enables AdaTrans to achieve targeted attribute manipulation while preserving other attributes. To ensure the transformed latent codes align with the distribution of latent codes, AdaTrans utilizes a predefined density model to maximize the likelihood of the transformed codes. In addition, the paper proposes a disentangled learning strategy within a mutual information framework, reducing the entanglement among attributes and minimizing the need for labeled data. AdaTrans offers controllable face editing with disentanglement, flexibility with non-binary attributes, and high fidelity. Extensive experiments demonstrate the effectiveness of AdaTrans compared to existing state-of-the-art methods, particularly in challenging scenarios with a large age gap and limited labeled examples. The source code for AdaTrans is available at https://github.com/Hzzone/AdaTrans.