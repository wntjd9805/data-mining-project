Vision in challenging weather conditions such as snow, rain, or fog is difficult due to scattering and attenuation, which significantly degrade image quality. However, it is crucial to address this issue in order to enable the operation of autonomous vehicles, drones, and robotic applications, as human performance is greatly affected in adverse weather. Previous research has focused on dehazing methods to remove weather-induced image degradation. However, most of these methods struggle to generalize from synthetic fully-supervised training approaches or generate high-quality results from real-world datasets that are not paired with clear weather conditions. Due to the limited availability of training data, which mainly consists of good weather conditions with adverse weather as an outlier, we propose an inverse rendering approach to reconstruct the scene content. Our method, ScatterNeRF, is a neural rendering technique that effectively renders foggy scenes and separates the fog-free background from the participating media. It leverages multiple views from a short automotive sequence for optimization, eliminating the need for a large training dataset. We introduce a disentangled representation for the scattering volume and the scene objects and learn the scene reconstruction using physics-inspired losses. We validate our method using multi-view In-the-Wild data and controlled captures in a large-scale fog chamber. The code and datasets for our method are available at https://light.princeton.edu/scatternerf.