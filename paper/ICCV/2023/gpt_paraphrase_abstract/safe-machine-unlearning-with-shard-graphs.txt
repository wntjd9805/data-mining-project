We propose a method called Synergy Aware Forgetting Ensemble (SAFE) that allows for adapting large models on diverse datasets while minimizing the cost of removing the influence of training samples. This process, known as selective forgetting or unlearning, typically involves partitioning the dataset into shards and training independent models on each shard, which are then combined through ensembling. However, increasing the number of shards reduces the cost of forgetting but also leads to higher inference cost and decreased accuracy, as the synergistic information between samples is lost during independent training. In contrast, SAFE introduces the concept of a shard graph, which incorporates limited information from other shards during training. This trade-off results in a modest increase in forgetting cost but a significant increase in accuracy, while still achieving complete removal of residual influence after forgetting. SAFE employs a lightweight system of adapters that can be trained while reusing most computations. This allows SAFE to be trained on smaller shards compared to current methods, reducing forgetting costs while maintaining high accuracy, as demonstrated on fine-grained computer vision datasets.