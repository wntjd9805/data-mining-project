Generating high-quality videos that are coherent and of extended duration is a difficult task. Researchers have used pretrained StyleGAN image generators and focused on motion generator design to tackle this challenge. The motion generator is trained using autoregressive methods with 3D convolutional discriminators to ensure motion coherence during video generation. This paper introduces a new motion generator design that utilizes a learning-based inversion network for GAN. The encoder in this method captures rich and smooth priors from encoding images to latents, allowing the generation of smooth future latents by modulating the inversion encoder temporally, with the guidance of the latent of an initially generated frame. This method benefits from sparse training and naturally constrains the generation space of the motion generator using the inversion network guided by the initial frame, eliminating the need for heavy discriminators. Additionally, this method supports style transfer through simple fine-tuning when the encoder is paired with a pretrained StyleGAN generator. Extensive experiments conducted on different benchmarks demonstrate the superiority of this method in generating long and high-resolution videos with decent single-frame quality and temporal consistency. The code for this method is available at https://github.com/johannwyh/StyleInV.