The rapid development of diffusion models has led to significant progress in image synthesis. Previous approaches have relied on pre-trained linguistic models, but these models often fail to accurately specify the spatial properties of an image, resulting in sub-optimal results for complex scene generation. In this paper, we propose a semantically controllable Layout-Aware diffusion model (LAW-Diffusion) to address this issue. Unlike previous methods that primarily focus on category-aware relationships, LAW-Diffusion incorporates a spatial dependency parser to encode location-aware semantic coherence across objects and generate scenes with perceptually harmonious object styles and contextual relations. We accomplish this by representing each object's regional semantics as an object region map and using a location-aware cross-object attention module to capture the spatial dependencies among these representations. To balance regional semantic alignment and texture fidelity, we introduce an adaptive guidance schedule for layout guidance. Additionally, LAW-Diffusion allows for instance reconfiguration while maintaining the other regions in a synthesized image through a layout-aware latent grafting mechanism. We propose a new evaluation metric called Scene Relation Score (SRS) to assess the plausibility of generated scenes and measure their rational and harmonious relations among contextual objects. Extensive experiments on COCO-Stuff and Visual-Genome datasets demonstrate that LAW-Diffusion achieves state-of-the-art generative performance, particularly in generating coherent object relations.