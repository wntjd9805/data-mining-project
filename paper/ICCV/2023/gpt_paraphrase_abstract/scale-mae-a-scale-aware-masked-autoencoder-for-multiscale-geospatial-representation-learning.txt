Large, pretrained models are commonly fine-tuned using augmented imagery to handle different conditions and scales. However, these models fail to capture scale-specific information in scale-dependent domains like remote sensing. To address this limitation, we propose Scale-MAE, a pretraining method that explicitly learns relationships between data at different scales. Scale-MAE pretrains a network by masking an input image at a known scale determined by the area of the Earth covered, rather than the image resolution. The masked image is then encoded using a ViT backbone and decoded through a bandpass filter to reconstruct low and high frequency images at different scales. Our experiments demonstrate that this approach leads to robust multiscale representations for remote sensing imagery. Compared to current state-of-the-art models, Scale-MAE achieves significant improvements in non-parametric kNN classification (2.4-5.6%) across eight remote sensing datasets. Additionally, it shows improved mIoU (0.9 to 1.7) for building segmentation transfer on the SpaceNet dataset across various evaluation scales.