Cross-modal alignment is a major challenge in Vision-and-Language Navigation (VLN) tasks. Previous studies have focused on mapping global instructions or sub-instructions to trajectories, but they often overlook the problem of achieving fine-grained alignment at the entity level. To address this issue, we propose a new pre-training paradigm called Grounded Entity-Landmark Adaptive (GELA) for VLN tasks. Our approach involves introducing grounded entity-landmark annotations into the Room-to-Room (R2R) dataset, creating GEL-R2R. We also employ three pre-training objectives: entity phrase prediction, landmark bounding box prediction, and entity-landmark semantic alignment. These objectives explicitly guide the learning of fine-grained cross-modal alignment between entity phrases and environmental landmarks. We evaluate our model on two benchmark tasks: VLN with descriptive instructions (R2R) and dialogue instructions (CVDN). Through comprehensive experiments, we demonstrate that our GELA model achieves state-of-the-art performance on both tasks, highlighting its effectiveness and generalizability.