Realistic human-centric rendering is important in computer vision and computer graphics. However, existing rendering datasets lack diversity in terms of outfit fabric, body-object interaction, and motion sequences, limiting their usefulness for real-world applications. In this study, we introduce DNA-Rendering, a large-scale dataset of human performance data for neural actor rendering. The dataset includes over 1500 human subjects, 5000 motion sequences, and 67.5 million frames of data. It covers a wide range of pose actions, body shapes, clothing, accessories, hairdos, and object interactions, capturing the variations in geometry and appearance from everyday life to professional occasions. DNA-Rendering also provides rich assets for each subject, such as 2D/3D human body keypoints, foreground masks, SMPLX models, cloth/accessory materials, multi-view images, and videos. These assets enhance the accuracy of current rendering methods for downstream tasks. The dataset was captured using a professional multi-view system with 60 synchronous cameras, ensuring high-quality resources for training and evaluation. Along with the dataset, we also present a benchmark for evaluating novel view synthesis, pose animation synthesis, and identity rendering methods. The dataset, code, and benchmarks will be publicly available at https://dna-rendering.github.io/.