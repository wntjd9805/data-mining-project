Meta-learning has the potential to address interference and forgetting in continual learning. However, existing algorithms have limitations such as expensive optimization processes and the need for new hyper-parameters. In this study, we propose a new meta-learning algorithm called SiM4C that focuses on minimizing forgetting and promoting forward transfer. SiM4C is a general and simple algorithm that is stable, has minimal computational overhead, and can be easily integrated with memory-based continual learning algorithms. Our experiments show that SiM4C outperforms previous meta-approaches in effectively learning on long task sequences. Furthermore, when integrated with existing memory-based algorithms, SiM4C achieves improved performance without introducing new hyper-parameters.