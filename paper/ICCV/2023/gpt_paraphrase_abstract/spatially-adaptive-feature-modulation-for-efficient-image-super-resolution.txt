We present a novel approach for efficient image super-resolution (SR) using a spatially-adaptive feature modulation (SAFM) mechanism. Existing deep learning-based models for SR are often large and complex, making them unsuitable for low-power devices with limited computational and memory resources. Our SAFM layer utilizes independent computations to learn multi-scale feature representations and combines these features for dynamic spatial modulation. Additionally, we introduce a convolutional channel mixer (CCM) to incorporate local contextual information and mix channels simultaneously. Our experimental results demonstrate that our proposed method is significantly smaller in size compared to state-of-the-art efficient SR methods like IMDN, while achieving comparable performance and requiring much less memory usage. The source codes and pre-trained models for our approach are available at the following link: https://github.com/sunny2109/SAFMN.