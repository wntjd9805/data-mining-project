Multi-task learning encounters two main challenges: the costly process of labeling all tasks and the need to balance the training progress of different tasks. In order to address the label annotation issue, we create a large-scale multi-task dataset by combining task-specific datasets, but the annotations for individual tasks are imbalanced, which can lead to an imbalance in training progress. To tackle this, we propose an achievement-based multi-task loss that adjusts the training speed based on the "achievement" metric, which measures the current accuracy relative to single-task accuracy. Additionally, we modify the multi-task loss by using a weighted geometric mean of individual task losses instead of a weighted sum, to prevent any single task from dominating the loss. In our experiments, we evaluate the accuracy and training speed of our proposed multi-task loss on the large-scale dataset and compare it to other recent multi-task losses. Our proposed loss achieves the highest multi-task accuracy without incurring any additional training time. Compared to single-task models, our approach improves object detection accuracy by 1.28%, semantic segmentation accuracy by 1.65%, and depth estimation accuracy by 1.18%, while reducing computations by 33.73%. The source code for our approach is available at https://github.com/samsung/Achievement-based-MTL.