Recent models for video grounding, based on the DETR framework, have been able to predict moment timestamps without the need for manual components like predefined proposals or non-maximum suppression. Instead, these models learn moment queries. However, these input-agnostic moment queries do not fully consider the temporal structure of the video, resulting in limited positional information. In this study, we propose an event-aware dynamic moment query that takes into account the specific content and positional information of the video. We introduce two levels of reasoning: event reasoning, which identifies distinctive event units in the video using a slot attention mechanism, and moment reasoning, which combines the moment queries with a given sentence through a gated fusion transformer layer. This allows for learning interactions between the moment queries and video-sentence representations to predict moment timestamps. Our experiments demonstrate that the event-aware dynamic moment queries are both effective and efficient, outperforming existing approaches on various video grounding benchmarks. The code for our model is publicly available at https://github.com/jinhyunj/EaTR.