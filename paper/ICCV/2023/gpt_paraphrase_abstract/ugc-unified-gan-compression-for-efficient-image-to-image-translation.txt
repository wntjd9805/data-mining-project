In recent years, Generative Adversarial Networks (GANs) have made significant progress in image-to-image translation. However, these GAN models require heavy computational costs and extensive training data. Existing efficient GAN learning techniques focus on either reducing computational costs or training data requirements. To address both aspects, we propose a new learning approach called Unified GAN Compression (UGC). UGC combines model-efficient and label-efficient learning by employing a semi-supervised-driven network architecture search and adaptive online semi-supervised distillation stages. This approach enables the creation of a flexible, label-efficient, and high-performing model. Extensive experiments show that UGC can achieve state-of-the-art lightweight models even with less than 50% of the original labels. For example, UGC can compress 40× MACs and achieve a FID score of 21.43 on the edges→shoes dataset with only 25% labels, outperforming the original model with 100% labels by 2.75 FID.