The task of Source-Free Video Unsupervised Domain Adaptation (SFVUDA) involves adapting an action recognition model from a labeled source dataset to an unlabeled target dataset without accessing the actual source data. Previous approaches have attempted to solve this task by using self-supervision techniques based on the temporal consistency of the target data. In this study, we take a different approach by utilizing "web-supervision" from Large Language-Vision Models (LLVMs), as LLVMs contain a wealth of world knowledge that is resilient to domain shifts. We demonstrate the effectiveness of integrating LLVMs for SFVUDA through a method called Domain Adaptation with Large Language-Vision models (DALL-V). DALL-V distills the world knowledge from LLVMs and combines it with information from the source model to create a student network specifically tailored for the target dataset. Despite its simplicity, DALL-V 1 achieves significant improvements over existing SFVUDA methods.