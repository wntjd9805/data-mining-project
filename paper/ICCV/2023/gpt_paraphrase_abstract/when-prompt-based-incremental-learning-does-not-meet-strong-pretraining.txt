Incremental learning is a technique used to address the issue of catastrophic forgetting in deep neural networks when learning sequential tasks. Prompt-based methods have shown impressive learning efficiency and performance by using a fixed backbone and learning task-specific prompts. However, these methods heavily rely on strong pre-training, typically on ImageNet-21k, and may struggle when faced with future tasks that differ significantly from the pre-training task.To overcome this limitation, we propose a learnable Adaptive Prompt Generator (APG). Our approach combines the prompt retrieval and prompt learning processes into a single learnable prompt generator. By doing so, we can optimize the entire prompting process to mitigate the negative effects caused by the gap between tasks. To ensure that our APG learns only effective knowledge, we maintain a knowledge pool that regularizes the APG using the feature distribution of each class.Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in exemplar-free incremental learning without strong pre-training. Furthermore, when strong pre-training is utilized, our method achieves comparable performance to existing prompt-based models, indicating that our approach can still benefit from pre-training. The code for our method is available at https://github.com/TOM-tym/APG.