The size of pre-trained language models (PLMs) is rapidly increasing, making full fine-tuning expensive in terms of both training and storage. In the context of vision-and-language (VL) tasks, parameter-efficient tuning (PET) techniques have been proposed to integrate modular modifications into PLMs, such as Adapters and LoRA. These techniques achieve comparable performance to full fine-tuning by tuning a small set of trainable parameters. However, excessive modular modifications and the neglect of the functionality gap between encoders and decoders can lead to a decrease in performance. Existing PET techniques, like VL-Adapter, fail to address these issues.To address these concerns, we introduce a framework called Vision-and-Language Parameter-Efficient Tuning (VL-PET). Our framework allows effective control over modular modifications through a novel granularity-controlled mechanism. By generating different granularity-controlled matrices, we can instantiate various model-agnostic VL-PET modules from our framework, achieving better efficiency and effectiveness trade-offs. We also propose lightweight PET module designs to improve VL alignment and modeling for encoders while maintaining text generation for decoders.We conducted extensive experiments on four image-text tasks and four video-text tasks to demonstrate the efficiency, effectiveness, and transferability of our VL-PET framework. Our VL-PETlarge with lightweight PET module designs outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37% (7.03%) with BART-base (T5-base) on image-text tasks. Additionally, we validate the enhanced effect of employing our VL-PET designs on existing PET techniques, leading to significant performance improvements.To access our code, please visit https://github.com/HenryHZY/VL-PET.