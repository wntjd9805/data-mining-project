Recent advancements in computer vision have seen the rise of Vision Transformers (ViTs), which have proven successful in various tasks. However, optimizing the performance and complexity of ViTs for efficient deployment on mobile devices has been a challenge. Several approaches have been proposed to accelerate attention mechanisms, improve inefficient designs, and incorporate lightweight convolutions suitable for mobile devices. Despite these efforts, ViTs and their variants still suffer from higher latency and more parameters compared to lightweight Convolutional Neural Networks (CNNs) like MobileNet, which is concerning for efficient deployment on resource-constrained hardware.  This study aims to address the central question of whether transformer models can achieve the speed and size of MobileNet while maintaining comparable performance. To accomplish this, the design choices of ViTs are revisited, and a new supernet with low latency and high parameter efficiency is proposed. Additionally, a novel fine-grained joint search strategy is introduced for transformer models, allowing for the optimization of both latency and number of parameters simultaneously.  The proposed models, referred to as EfficientFormerV2, demonstrate promising results. They achieve a 3.5% higher top-1 accuracy than MobileNetV2 on the ImageNet-1K dataset, while exhibiting similar latency and parameter count. This work showcases that properly designed and optimized vision transformers can achieve high performance even with the size and speed of MobileNet.