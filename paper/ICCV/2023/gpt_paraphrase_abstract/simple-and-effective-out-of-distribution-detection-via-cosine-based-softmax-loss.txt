We present a novel approach for detecting out-of-distribution (OOD) data in deep learning models during inference. Existing methods have limitations such as the need for additional data, input processing, or high computational cost. Moreover, these methods often require users to set hyperparameters, which greatly affect the detection rate. In contrast, our proposed method combines the feature norm and the Mahalanobis distance derived from classification models trained with cosine-based softmax loss. Our method is practical as it does not require extra data for training, is approximately three times faster during inference compared to methods involving input processing, and is easy to implement due to the absence of hyperparameters for OOD detection. Through experiments, we demonstrate that our method outperforms or is at least comparable to state-of-the-art OOD detection methods.