We introduce an automatic system that effectively eliminates clutter from 3D scenes and fills in the gaps with consistent geometry and texture. Our approach addresses two critical components: 3D segmentation based on shared properties and 3D inpainting. The conventional object categories in computer vision fail to capture the definition of 3D scene clutter, so we overcome this limitation by grouping noisy labels, utilizing virtual rendering, and applying an instance-level area-sensitive loss. Once the clutter is removed, we employ a merging technique to inpaint geometry and texture in the resulting gaps by combining RGB-D images. To ensure consistency across multiple views for mesh reconstruction, we develop innovative voting and pruning strategies. Our method surpasses baseline techniques in clutter segmentation and 3D inpainting, as demonstrated through experiments on the ScanNet and Matterport3D datasets. Both qualitative and quantitative evaluations validate the superiority of our approach. For further information, please visit our project page at https://weify627.github.io/clutter/.