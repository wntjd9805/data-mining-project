Semi-supervised learning (SSL) has gained popularity for its ability to train high-performance models using unlabeled data, reducing the cost of machine learning (ML). However, this paper reveals that SSL is vulnerable to strong poisoning attacks due to its reliance on non-inspected unlabeled data. While poisoning has been a problem in conventional supervised ML, the authors argue that it poses an even greater threat to SSL. To demonstrate this, they design a backdoor poisoning attack on SSL that can be executed by a weak adversary without prior knowledge of the target SSL pipeline. Unlike previous attacks on supervised ML that assume strong adversaries with impractical capabilities, this attack only poisons 0.2% of the unlabeled training data but still achieves a high success rate of misclassification (over 80%) on test inputs containing the backdoor trigger. The attack is effective across different benchmark datasets and SSL algorithms, even bypassing state-of-the-art defenses against backdoor attacks. These findings raise significant concerns about the security of SSL in real-world security critical applications.