This study focuses on the issues of severe misalignment between images and text and high redundancy in large-scale Vision-Language Pre-Training (VLP) datasets. To tackle these problems, a new Vision-Language learning algorithm called TL;DR is proposed. TL;DR aims to compress the existing VLP data into a smaller, yet high-quality set. The approach involves two main steps. Firstly, a codebook-based encoder-decoder captioner is developed to select representative samples. Secondly, a new caption is generated to complement the original captions for the selected samples, addressing the misalignment issue while maintaining uniqueness. Consequently, TL;DR allows for the reduction of the large dataset into a small set of high-quality data, which can be used as an alternative pre-training dataset. This algorithm significantly speeds up the time-consuming pretraining process. Notably, TL;DR can compress mainstream VLP datasets at a high ratio, such as reducing the well-cleaned CC3M dataset from 2.82M to 0.67M (approximately 24%) and noisy YFCC15M from 15M to 2.5M (approximately 16.7%). Extensive experiments conducted with three popular VLP models across seven downstream tasks demonstrate that the VLP model trained on the compressed dataset provided by TL;DR can achieve similar or even better results compared to training on the full-scale dataset.