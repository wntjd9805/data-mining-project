Researchers have been working on finding ways to reduce training costs in deep learning while still achieving good generalization across different datasets. One approach that has emerged is dataset distillation, which involves creating a smaller synthetic set that contains the same information as a larger real dataset, and can achieve similar test accuracy as a model trained on the entire dataset. However, previous methods of generating synthetic data have not been able to distribute and discriminate as well as the original training data, and they also require significant computational resources.  Despite some promising results, there is still a noticeable performance gap between models trained on condensed synthetic sets and those trained on the entire dataset. In this paper, we propose an efficient method called DatasetDistillation with Attention Matching (DataDAM) to address these challenges and achieve state-of-the-art performance while reducing training costs.  Our approach involves learning synthetic images by matching the spatial attention maps of real and synthetic data generated by different layers within a family of randomly initialized neural networks. We evaluate our method on several datasets, including CIFAR10/100, TinyImageNet, ImageNet-1K, and subsets of ImageNet-1K, and we consistently outperform previous methods in most settings. We achieve improvements of up to 6.5% on CIFAR100 and 4.1% on ImageNet-1K.  Furthermore, we demonstrate that our high-quality distilled images have practical benefits for other applications such as continual learning and neural architecture search. Overall, our approach shows great potential in reducing training costs while still maintaining strong performance in deep learning tasks.