This study introduces a novel video diffusion model that allows for content editing based on user-provided descriptions. Previous methods required re-training or error-prone propagation for each input, but our model addresses this issue by utilizing monocular depth estimates with varying levels of detail to disentangle structure and content. By incorporating joint video and image training, we are able to achieve explicit control over temporal consistency. Our experiments show that our model allows for fine-grained control over output characteristics, customization based on reference images, and is preferred by users.