This paper presents a system for understanding and synthesizing hand-object interactions from sparse views. The authors propose a two-stage approach, where they first learn the shape and appearance of hands and objects separately using neural representation. In the second stage, they use a rendering-based joint model fitting framework to analyze dynamic hand-object interactions, overcoming occlusion and deformation issues. The system also enables novel view synthesis. To ensure stable contact between the hand and object, the authors introduce a stable contact loss. Experimental results show that their method outperforms existing approaches. The code and dataset for this system are available on the project webpage.