Video Temporal Grounding (VTG) is crucial for video browsing on social media, as it involves linking specific video clips to language queries. However, existing methods in this field are limited in their ability to generalize to different VTG tasks and labels. In this paper, we introduce UniVTG, a unified framework that addresses this limitation. Firstly, we redefine and unify various VTG labels and tasks, allowing for a more comprehensive formulation. Additionally, we propose data annotation schemes to create scalable pseudo supervision for training. Secondly, we develop a flexible grounding model that effectively addresses each VTG task and utilizes the diverse labels. By doing so, we enhance the model's grounding abilities, including the ability to perform zero-shot grounding.Lastly, our unified framework enables us to leverage large-scale diverse labels for temporal grounding pretraining. This results in stronger grounding abilities and improved performance across tasks such as moment retrieval, highlight detection, and video summarization.We conduct extensive experiments on seven datasets to evaluate the effectiveness and flexibility of our proposed framework. The results demonstrate its superior performance compared to existing methods. The codes for our framework are publicly available at https://github.com/showlab/UniVTG.