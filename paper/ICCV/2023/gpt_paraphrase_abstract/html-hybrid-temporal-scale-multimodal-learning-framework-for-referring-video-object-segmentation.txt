The main challenge in Referring Video Object Segmentation (RVOS) is the diverse and flexible nature of object descriptions in the open world. Existing approaches often overlook the fact that different descriptions correspond to different temporal scales in the video, resulting in limited accuracy. To address this issue, we propose the Hybrid Temporal-scale Multimodal Learning (HTML) framework. This framework effectively aligns textual and visual features to uncover the core object semantics by learning multimodal interaction hierarchically across different temporal scales. Our approach incorporates a unique inter-scale multimodal perception module that dynamically connects language queries with visual features across different scales. This module reduces confusion by incorporating video context across scales. Through extensive experiments on well-known benchmarks such as Ref-Youtube-VOS, Ref-DAVIS17, A2D-Sentences, and JHMDB-Sentences, our HTML framework achieves state-of-the-art performance on all these datasets.