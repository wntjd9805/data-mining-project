Source-free domain adaptation (SFDA) is a widely used method for adapting a pre-trained model from a source domain to a target domain without accessing any source data. However, the existing literature on SFDA fails to address the security challenges that arise when the source domain owner is malicious. This study examines the impact of a malicious source adversary who can inject hidden malicious behavior, such as a backdoor or trojan, during source training and potentially transfer it to the target domain even after benign training by the target domain owner.Our investigation reveals that the unique challenges of SFDA, such as the absence of source data and target labels, make it difficult to defend against backdoor attacks using existing defenses. To overcome this issue, we propose a novel protection scheme called secure source-free domain adaptation (SSDA). SSDA employs static model compression of a pre-trained source model and a novel knowledge transfer scheme with a spectral-norm-based loss penalty for target training. These techniques are designed to suppress the malicious channels responsive to the backdoor during the adaptation stage. Additionally, knowledge transfer from an uncompressed auxiliary model helps to recover the benign test accuracy.Extensive evaluations on multiple datasets and domain tasks against recent backdoor attacks demonstrate that SSDA can successfully defend against strong backdoor attacks with minimal degradation in test accuracy compared to vulnerable baseline SFDA methods. The code for our proposed SSDA method is available at https://github.com/ML-Security-Research-LAB/SSDA.