Large-scale language-image models have significantly improved image generation capabilities, leading to the development of image editing methods that utilize text prompts for intuitive and versatile modifications. However, existing methods lack user-friendliness, generalization to larger domains, or high fidelity to the input image. To address these limitations, this paper presents an accurate and efficient inversion technique called Prompt Tuning Inversion for text-driven image editing. The proposed method consists of a reconstruction stage and an editing stage. In the reconstruction stage, the input image information is encoded into a learnable conditional embedding using Prompt Tuning Inversion. In the editing stage, a classifier-free guidance approach is employed to sample the edited image by linearly interpolating between the target embedding and the optimized one from the first stage. This technique ensures a balanced trade-off between editability and fidelity to the input image. For instance, the color of a specific object can be changed while preserving its original shape and background using only a target text prompt. Extensive experiments on ImageNet demonstrate the superior editing performance of the proposed method compared to state-of-the-art baselines.