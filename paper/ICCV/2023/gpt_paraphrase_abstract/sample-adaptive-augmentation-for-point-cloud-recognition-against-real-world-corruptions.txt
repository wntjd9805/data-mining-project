The need for robust 3D perception in the field of 3D vision has become crucial, particularly in the presence of corruption. Current data augmentation techniques often apply random transformations to all objects in a point cloud dataset, disregarding the structure of the samples. This approach can lead to excessive enhancement or degradation. To address this issue, we propose an alternative method called Adapt-Point, which performs sample-adaptive transformations based on the structure of each individual sample. Our approach utilizes an imitator, consisting of a Deformation Controller and a Mask Controller, to predict deformation parameters and generate per-point masks using intrinsic structural information from the input point cloud. Corruption simulations are then applied while a discriminator prevents excessive corruption that deviates from the original data distribution. Additionally, we incorporate a perception-guidance feedback mechanism to generate samples with appropriate difficulty levels. To overcome the lack of real-world corrupted point cloud data, we introduce a new dataset called ScanObjectNN-C, which closely resembles real-world environments compared to existing CAD datasets. Experimental results demonstrate that our method achieves state-of-the-art performance on various corruption benchmarks, including ModelNet-C, ScanObjectNN-C, and ShapeNet-C.