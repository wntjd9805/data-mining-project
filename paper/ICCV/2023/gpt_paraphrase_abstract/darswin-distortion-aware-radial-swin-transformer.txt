This paper introduces a new model called DarSwin, which is designed to adapt to the distortions produced by wide-angle lenses. Wide-angle lenses are commonly used in tasks that require a large field of view, but their significant distortions make it difficult for conventional models to effectively process wide-angle images. DarSwin leverages the physical characteristics of wide-angle lenses, specifically the radial distortion profile, to develop a distortion-aware transformer-based architecture. Unlike traditional transformer models, DarSwin incorporates radial patch partitioning, a distortion-based sampling technique for creating token embeddings, and angular position encoding for radial patch merging. The effectiveness of DarSwin is demonstrated through classification tasks using synthetically distorted ImageNet data. The experiments show that DarSwin can adapt to unseen distortions from different wide-angle lenses, outperforming other baseline models in terms of accuracy. The code and models for DarSwin are publicly available. The paper includes a visual comparison between Swin and DarSwin, highlighting the differences in their approach to handling wide-angle images. DarSwin's ability to perform radial transformations using distortion-aware radial patches enables better generalization across different lenses.