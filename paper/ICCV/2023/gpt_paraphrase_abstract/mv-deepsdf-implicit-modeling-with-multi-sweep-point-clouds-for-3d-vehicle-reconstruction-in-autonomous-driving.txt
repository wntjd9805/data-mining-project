Reconstructing 3D vehicles from sparse and noisy partial point clouds is crucial for autonomous driving. Existing methods designed for dense inputs with minimal noise cannot be directly applied to this problem. In this study, we propose a new framework called MV-DeepSDF, which utilizes multi-sweep point clouds to estimate the optimal shape representation using the Signed Distance Function (SDF). Unlike previous SDF-based methods that focus on single-view reconstruction and result in low fidelity, we analyze the consistency and complementarity of multi-sweeps in the latent feature space. We transform the problem of estimating implicit space shape into an element-to-set feature extraction problem and develop a new architecture that extracts individual element-level representations and aggregates them to generate a set-level predicted latent code. This set-level latent code represents the optimal 3D shape in the implicit space and can be decoded into a continuous SDF of the vehicle. Our approach effectively learns consistent and complementary information from multi-sweeps for accurate 3D vehicle reconstruction. We conducted thorough experiments on real-world autonomous driving datasets (Waymo and KITTI) and demonstrated the superior performance of our approach compared to state-of-the-art methods both qualitatively and quantitatively.