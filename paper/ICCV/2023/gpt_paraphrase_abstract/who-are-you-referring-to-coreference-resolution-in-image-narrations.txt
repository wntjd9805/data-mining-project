This paper focuses on extending coreference resolution to long-form narrations of visual scenes. To achieve this, the authors introduce a new dataset that includes annotated coreference chains and their bounding boxes. Unlike existing image-text datasets, this dataset includes coreferring expressions and labeled chains. The authors propose a new technique that uses weak supervision and prior linguistic knowledge to learn to identify coreference chains. This model outperforms several strong baselines in resolving coreferences. Additionally, the authors demonstrate that coreference resolution enhances the grounding of narratives in images.