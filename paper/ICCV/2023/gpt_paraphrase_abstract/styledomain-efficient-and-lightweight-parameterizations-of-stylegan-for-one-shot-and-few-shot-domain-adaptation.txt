Domain adaptation of GANs involves adjusting GAN models pretrained on a large dataset to a specific domain with limited samples. Although various methods address this problem, there are still unanswered questions. This paper conducts a comprehensive analysis of domain adaptation in GANs, focusing on the StyleGAN model. The study explores crucial components of StyleGAN responsible for adapting the generator to a new domain based on domain similarity. The findings propose efficient and lightweight parameterizations of StyleGAN for domain adaptation. Particularly, it reveals the existence of StyleSpace directions that are effective for adapting to similar domains. For dissimilar domains, the paper introduces Affine+ and AffineLight+ parameterizations, outperforming existing baselines in few-shot adaptation while utilizing fewer training parameters. Additionally, the paper examines StyleDomain directions and uncovers their surprising properties, which are then applied to domain mixing and cross-domain image morphing. The source code can be accessed at https://github.com/AIRI-Institute/StyleDomain.