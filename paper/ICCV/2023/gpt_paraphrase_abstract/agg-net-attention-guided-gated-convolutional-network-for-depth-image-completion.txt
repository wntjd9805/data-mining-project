Recently, the use of lightweight RGBD cameras for stereo vision has become widespread in various fields. However, these cameras, which rely on imaging principles such as TOF, structured light, or binocular vision, often capture invalid data such as weak reflection, boundary shadows, and artifacts. These invalid data can negatively impact subsequent work. To address this issue, we propose a new model called AGG-Net for depth image completion. AGG-Net utilizes an Attention Guided Gated-convolutional Network and can generate more accurate and reliable depth images from raw depth maps and corresponding RGB images. Our model employs a UNet-like architecture with two parallel branches for depth and color features. In the encoding stage, we introduce an Attention Guided Gated-Convolution module that merges depth and color features at different scales to mitigate the negative effects of invalid depth data during reconstruction. In the decoding stage, we present an Attention Guided Skip Connection module to prevent the introduction of excessive depth-irrelevant features during reconstruction. Experimental results show that our method surpasses state-of-the-art techniques on popular benchmarks such as NYU-Depth V2, DIML, and SUN RGB-D. The code for our method is available at https://github.com/htx0601/AGG-Net.