Vision-language pre-training methods have gained significant attention recently, aiming to simultaneously learn visual and textual features using transformer-based architectures. While existing approaches focus on aligning visual and textual features, there is limited exploration of strategies to enhance model robustness and convergence speed. In this study, we introduce a novel method called ViLTA, which consists of two components to improve the learning of fine-grained representations in image-text pairs. Firstly, for Masked Language Modeling (MLM), we propose a cross-distillation technique that generates soft labels to enhance model robustness. This addresses the issue of treating synonyms of masked words as negative samples in one-hot labels. Secondly, for Image-Text Matching (ITM), we utilize the current language encoder to synthesize challenging negative examples based on the language context, encouraging the model to learn high-quality representations. Through these techniques, ViLTA achieves superior performance on various vision-language tasks. Extensive experiments conducted on benchmark datasets validate the effectiveness of ViLTA and its promising potential for vision-language pre-training.