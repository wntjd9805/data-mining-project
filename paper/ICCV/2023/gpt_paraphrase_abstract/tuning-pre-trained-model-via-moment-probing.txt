In recent years, there has been a growing interest in efficiently fine-tuning large-scale pre-trained models. Linear probing (LP) is commonly used to exploit final representations for task-dependent classification. However, existing methods primarily focus on introducing a few learnable parameters and overlook the importance of the LP module. In this study, we propose a novel approach called Moment Probing (MP) to further explore the potential of LP. Unlike LP, which creates a linear classification head based on the mean of final features or classification tokens, MP performs a linear classifier on the feature distribution. This approach enhances representation ability by utilizing richer statistical information inherent in the features. We represent feature distribution using its characteristic function, approximated efficiently using first- and second-order moments of the features. Additionally, we introduce a multi-head convolutional cross-covariance (MHC3) method to compute second-order moments in an efficient and effective manner. To address the impact of MP on feature learning, we introduce a partially shared module called MP+ to learn two recalibrating parameters for backbones based on MP. Experimental results on ten benchmarks using various models demonstrate that our MP outperforms LP and is competitive with other approaches at a lower training cost. Moreover, our MP+ achieves state-of-the-art performance.