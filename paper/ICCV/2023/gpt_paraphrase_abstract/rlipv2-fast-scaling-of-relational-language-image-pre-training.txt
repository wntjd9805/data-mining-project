This study introduces RLIPv2, a model that improves upon the slow convergence of the RLIPv1 architecture and tackles the limited availability of scene graph data. RLIPv2 incorporates Asymmetric Language-Image Fusion (ALIF) to enable faster and deeper cross-modal fusion with sparsified language encoding layers. This approach achieves comparable or better performance than RLIPv1 but in a fraction of the time for pre-training and fine-tuning. To obtain scene graph data at scale, the study introduces a captioner (BLIP) and a designed Relation Tagger to extend object detection datasets with free-form relation labels. This allows for larger-scale relational pre-training. Experimental results on Human-Object Interaction Detection and SceneGraph Generation demonstrate that RLIPv2 outperforms state-of-the-art models in fully-finetuning, few-shot, and zero-shot settings. Notably, the largest RLIPv2 achieves 23.29mAP on HICO-DET without any fine-tuning, 32.22mAP with just 1% data, and 45.09mAP with 100% data. The code and models for RLIPv2 are publicly available at https://github.com/JacobYuan7/RLIPv2.