This abstract discusses the potential of generative pre-training in improving the performance of fundamental models in 2D and 3D vision. While generative pre-training has been successful in 2D vision, it has been limited in 3D vision due to reliance on Transformer-based backbones and the unordered nature of point clouds. The authors propose a new 3D-to-2D generative pre-training method that can be applied to any point cloud model. Their approach involves generating view images from different poses using the cross-attention mechanism, providing more precise supervision for 3D backbones to better understand the geometric structure and stereoscopic relationships of the point cloud. Experimental results show that their proposed method outperforms previous pre-training methods and is effective in enhancing architecture-oriented approaches. The authors achieve state-of-the-art performance in ScanObjectNN classification and ShapeNet-Part segmentation tasks when fine-tuning their method. The code for their approach is available at the provided GitHub link.