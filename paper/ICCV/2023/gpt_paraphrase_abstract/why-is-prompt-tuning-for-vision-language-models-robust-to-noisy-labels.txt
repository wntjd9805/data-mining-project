Vision-language models like CLIP are able to learn a generalized text-image representation through extensive training with large datasets. These models can then be adapted to new classification tasks using few-shot prompt tuning. Interestingly, we have discovered that this prompt tuning process is highly resilient to label noise. This has motivated us to investigate the underlying factors that contribute to the robustness of prompt tuning.To delve deeper into this property, we conducted a series of comprehensive experiments. Our findings indicate that the fixed classname tokens play a crucial role in regularizing the model optimization process, effectively reducing the impact of noisy samples on the gradients. Additionally, the powerful pre-trained image-text embedding, acquired from diverse and generic web data, provides valuable prior knowledge for image classification.Furthermore, we demonstrate that even noisy zero-shot predictions generated by CLIP can be utilized to fine-tune its own prompt. This approach significantly improves prediction accuracy in unsupervised settings. For those interested, the code for this project can be accessed at https://github.com/CEWu/PTNL.