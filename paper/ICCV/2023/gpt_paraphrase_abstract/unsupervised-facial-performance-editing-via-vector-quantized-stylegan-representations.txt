The development of virtual human avatar applications that are highly realistic has created a demand for photorealistic video face synthesis with the ability to control facial features through semantic editing. Although recent generative neural methods have made progress in synthesizing portrait videos, controlling specific facial aspects like the mouth interior and gaze at different levels of detail remains challenging. In this study, we propose a new framework for editing faces that combines a 3D face model with StyleGAN vector-quantization to learn multi-level semantic control over facial expressions. We demonstrate that by quantizing StyleGAN features, we can reveal more detailed semantic representations of facial attributes such as teeth and pupils, which are difficult to capture using 3D tracking techniques. By combining these representations with 3D tracking, we can train a generator to manipulate coarse expressions and finer facial attributes using self-supervision. Moreover, the learned representations can be used in conjunction with user-defined masks to create semantic segmentations that facilitate precise local control over specific facial attributes like the eyes and teeth. This enables a wide range of applications including face reenactment and visual expression manipulation in videos.