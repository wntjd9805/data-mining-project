Accurate detection and segmentation of affordances, with precise pixel-level localization, is crucial for complex systems like robots and assistive devices. We introduce a novel approach to affordance perception that allows for accurate multi-label segmentation. This approach leverages a 3D map of the environment to automatically extract grounded affordances from first-person interaction videos, ensuring pixel-level precision in locating the affordances. To facilitate research in this area, we create EPIC-Aff, the largest and most comprehensive dataset on affordances, based on the EPIC-Kitchen dataset. EPIC-Aff provides annotations for interaction-grounded, multi-label, metric, and spatial affordances.   Furthermore, we propose a new method for affordance segmentation using multi-label detection, enabling the coexistence of multiple affordances in the same space, particularly when they are associated with the same object. We explore various strategies for multi-label detection by employing different segmentation architectures. The experimental results emphasize the significance of multi-label detection in affordance segmentation.  Lastly, we demonstrate how our metric representation can be utilized to generate a map of interaction hotspots in spatial action-centric zones. This representation allows for task-oriented navigation based on the identified affordances. By leveraging our approach, systems can effectively navigate and interact with their surroundings.