Can we enhance self-supervised representation learning by utilizing the existing audiovisual information in videos? To address this query, we investigate different pre-training architectures and objectives in the masked autoencoding framework, inspired by the success of similar approaches in natural language and image comprehension. Our findings reveal significant advancements in audiovisual downstream classification tasks, surpassing the current state-of-the-art performance on VGGSound and AudioSet. Moreover, we demonstrate that our audiovisual pretraining scheme can be applied to multiple unimodal downstream tasks using a single pretrained model. Additionally, we showcase the transferability of our representations by achieving state-of-the-art audiovisual results on Epic Kitchens without specifically pre-training for this dataset.