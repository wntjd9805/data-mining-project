Supervised face clustering faces a significant challenge of large intra-class variance due to different face attributes like age, pose, and expression. Images of the same person but with different attributes are often grouped into separate sub-clusters. To tackle this issue, we introduce CLIP-Cluster, an attribute hallucination framework. CLIP-Cluster leverages the CLIP model to generate multiple representations for different attributes and combines them using neighbor-adaptive attention. Our framework includes a text-driven attribute hallucination module that uses natural language to generate new attributes for a given face image based on the image-language CLIP space. Additionally, we develop a neighbor-aware proxy generator that merges the features describing various attributes into a proxy feature. This proxy feature reduces intra-class variance by attending to hallucinated visual features and the source feature based on local neighbor information. We construct a graph using the proxy representations for subsequent clustering operations. Extensive experiments demonstrate that our approach surpasses state-of-the-art face clustering methods in both performance and efficiency.