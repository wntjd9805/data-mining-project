Recently, Diffusion Probabilistic Models (DPMs) have shown impressive results in generative tasks. However, the learned representations of pre-trained DPMs have not been fully understood. In this paper, we thoroughly investigate the representation power of DPMs and propose a novel method called RepFusion for transferring knowledge from generative DPMs to recognition tasks. Our study reveals that DPMs function as denoising autoencoders that balance representation learning and model capacity regularization. To implement RepFusion, we extract representations at different time steps from existing DPMs and use them as supervision for student networks. The optimal time step is determined through reinforcement learning. We evaluate our approach on various image classification, semantic segmentation, and landmark detection benchmarks, and demonstrate its superiority over state-of-the-art methods. Our findings highlight the potential of DPMs as a valuable tool for representation learning and provide insights into the broader applications of generative models. The code for RepFusion is available at https://github.com/Adamdad/Repfusion.