Traditional image stitching methods rely on complex geometric features to improve performance. However, these features are only suitable for specific scenes with well-defined geometric structures. On the other hand, deep stitching techniques can adaptively learn robust semantic features to handle adverse conditions, but they struggle with large-parallax cases. To address these challenges, we propose a parallax-tolerant unsupervised deep image stitching approach.   First, we introduce a robust and flexible warp that models image registration from global homography to local thin-plate spline motion. This warp ensures accurate alignment for overlapping regions and preserves the shape of non-overlapping regions through joint optimization of alignment and distortion.   To enhance the generalization capability, we devise a simple yet effective iterative strategy for warp adaptation in cross-dataset and cross-resolution applications.   Furthermore, to eliminate parallax artifacts, we propose a seamless composition of the stitched image using unsupervised learning for seam-driven composition masks.   Compared to existing methods, our solution is parallax-tolerant and does not require laborious designs of complicated geometric features for specific scenes. Extensive experiments demonstrate the superiority of our approach over state-of-the-art methods in both quantitative and qualitative evaluations. The code for our technique is available at https://github.com/nie-lang/UDIS2.