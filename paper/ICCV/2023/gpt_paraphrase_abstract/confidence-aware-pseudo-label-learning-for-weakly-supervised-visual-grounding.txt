Visual grounding is the process of identifying the target object in an image based on a natural language query. Traditional methods for labeling the position of the target object are time-consuming, so weakly supervised methods that only require image-sentence annotations during training have gained attention. These methods typically use pre-trained object detectors to generate region proposals and then select the best proposal based on cross-modal similarity or reconstruction loss. However, these methods often suffer from spurious associations and error propagation due to differences between modalities.   To address these limitations, we propose Confidence-aware Pseudo-label Learning (CPL). First, we use both uni-modal and cross-modal pre-trained models and employ conditional prompt engineering to generate multiple descriptive and diverse pseudo language queries for each region proposal. We then establish reliable cross-modal associations for model training based on the similarity between pseudo and real text queries. Additionally, we introduce a confidence-aware pseudo label verification module to reduce noise and the risk of error propagation during training.   We conducted experiments on five widely used datasets, and the results demonstrate that our proposed components significantly improve performance, achieving state-of-the-art results. The code for our approach can be found at https://github.com/zjh31/CPL.git.