Open-World Compositional Zero-Shot Learning (OW-CZSL) aims to recognize new compositions of attributes and objects that have been seen before. However, traditional methods built on the closed-world setting struggle in the unconstrained OW test space. Previous approaches have tried to address this issue by pruning compositions based on external knowledge or correlations in seen pairs, but these methods introduce biases that hinder generalization. To overcome these challenges, we propose a novel approach called Distilled Reverse Attention Network. Our method models attributes and objects separately, taking into account their contextuality and locality. We also introduce a reverse-and-distill strategy that learns disentangled representations of elementary components in the training data, guided by reverse attention and knowledge distillation. We evaluate our approach on three datasets and consistently achieve state-of-the-art performance. Our disentangling strategy for OW-CZSL is motivated by the idea that when the features of objects and attributes are disentangled, their residual features carry enough information to classify correctly, resulting in a large overlap between the object residuals and the attribute features. On the other hand, when the attribute-object features are entangled, the phenomena are reversed with little object information and a small overlap between the features.