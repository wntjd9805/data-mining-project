Despite the success of vision transformers (ViTs), their accuracy drops significantly when faced with common corruptions like noise or blur. We have observed that ViTs tend to rely on a few important tokens, a phenomenon known as token overfocusing. However, these tokens are not robust against corruptions, leading to inconsistent attention patterns. To address this issue, we propose two techniques. Firstly, our Token-aware Average Pooling (TAP) module encourages the local neighborhood of each token to participate in the attention mechanism. TAP learns average pooling schemes for each token, allowing potentially important tokens in the neighborhood to be considered adaptively. Secondly, we use our Attention Diversification Loss (ADL) to urge the output tokens to aggregate information from a diverse set of input tokens, instead of focusing on just a few. We achieve this by penalizing high cosine similarity between the attention vectors of different tokens. Through experiments, we apply our methods to various transformer architectures and significantly improve robustness. For instance, we enhance corruption robustness on ImageNet-C by 2.4% and accuracy by 0.4% using the state-of-the-art robust architecture FAN. Additionally, when fine-tuning on semantic segmentation tasks, we improve robustness on CityScapes-C by 2.4% and ACDC by 3.0%. Our code can be found at https://github.com/guoyongcs/TAPADL.