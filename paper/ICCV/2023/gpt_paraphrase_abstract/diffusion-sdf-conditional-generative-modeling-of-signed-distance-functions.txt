Probabilistic diffusion models have been successful in tasks like image synthesis, inpainting, and text-to-image conversion. However, their application in generating complex 3D shapes is still in its early stages. In this study, we introduce Diffusion-SDF, a generative model that focuses on shape completion, single-view reconstruction, and reconstruction of real-scanned point clouds. Our approach utilizes neural signed distance functions (SDFs) as a representation for 3D geometry, achieved through neural networks. Neural SDFs are implicit functions, and we tackle the challenge of learning their reverse neural network weights by employing a custom modulation module. Through extensive experiments, we demonstrate that our method can generate realistic outputs unconditionally and conditionally from incomplete inputs. This research extends the capabilities of diffusion models from learning explicit 2D representations to implicit 3D representations. The code for our approach is publicly available at https://github.com/princeton-computational-imaging/Diffusion-SDF.