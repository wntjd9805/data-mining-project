Vision-language pre-training (VLP) models are susceptible to adversarial attacks in multimodal tasks, and these attacks can be transferred to attack other black-box models. However, previous research has mainly focused on white-box attacks. This paper presents the first investigation into the transferability of adversarial attacks on recent VLP models. The study reveals that existing methods have lower transferability compared to their strong performance in white-box settings. This decrease in transferability is partly due to the under-utilization of cross-modal interactions. VLP models heavily rely on cross-modal interactions, and the alignments between different modalities are diverse. To address this issue, the paper proposes a highly transferable Set-level Guidance Attack (SGA) that effectively utilizes modality interactions and incorporates alignment-preserving augmentation with cross-modal guidance. Experimental results demonstrate that SGA can generate adversarial examples that transfer strongly across different VLP models in various vision-language tasks. Specifically, on image-text retrieval, SGA significantly improves the attack success rate for transfer attacks from the source model (ALBEF) to the target white-box model or black-box models (TCL) by a large margin, outperforming the state-of-the-art methods. The paper provides further details and discussions in Section 3. The code for SGA is available at https://github.com/Zoky-2020/SGA. Figure 1 compares the attack success rates of five different attacks on image-text retrieval. The first three columns represent attacks without cross-modal interactions, including image-only PGD attack, text-only BERT-Attack (BA), and combined separate unimodal attack (SA). The fourth column shows the state-of-the-art multimodal Co-Attack (CA) that uses single-pair cross-modal interactions. The last column presents the proposed SGA, which leverages multiple set-level cross-modal interactions and successfully attacks the white-box model while transferring to attack all black-box models with the highest success rate.