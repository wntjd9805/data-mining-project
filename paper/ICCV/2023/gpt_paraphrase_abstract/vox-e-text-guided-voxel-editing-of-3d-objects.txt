Large-scale models that use text guidance have become popular for generating diverse images that depict complex visual concepts. Recently, these models have also been used for text-to-3D synthesis. In this study, we propose a method that utilizes latent diffusion models to edit existing 3D objects. Our approach involves taking oriented 2D images as input and learning a grid-based volumetric representation of the object. To ensure that the representation aligns with a given text prompt, we employ a Score Distillation Sampling (SDS) loss, similar to existing text-to-3D methods. However, we face challenges when combining this diffusion-guided loss with an image-based regularization loss that aims to prevent significant deviations from the input object. This is because we need to achieve two conflicting goals based solely on 2D projections that capture both structure and appearance. To address this, we introduce a novel volumetric regularization loss that operates directly in 3D space, leveraging the explicit nature of our 3D representation to enforce correlation between the original and edited object's global structure. Additionally, we propose a technique to optimize cross-attention volumetric grids, which improves the precision of the edits' spatial extent. Through extensive experiments and comparisons, we demonstrate the effectiveness of our approach in generating a wide range of edits that were not achievable with previous methods.