Continual learning (CL) is a technique that allows models to adapt to new tasks and environments without forgetting previously learned knowledge. However, current CL setups do not consider the hierarchical relationships between old and new tasks, which is often present in real-world scenarios. This poses a challenge for traditional CL approaches. To address this challenge, we propose a new approach called hierarchical label expansion (HLE), which incorporates a multi-level hierarchical class incremental task configuration with an online learning constraint. In this configuration, a network first learns coarse-grained classes and then continually expands its knowledge to more fine-grained classes at various levels of hierarchy. To handle this setup, we introduce a rehearsal-based method that utilizes hierarchy-aware pseudo-labeling to incorporate hierarchical class information. We also propose a memory management and sampling strategy that selectively adopts samples of newly encountered classes. Our experiments demonstrate that our proposed method effectively utilizes hierarchy in the HLE setup, improving classification accuracy across all levels of hierarchies, regardless of depth and class imbalance ratio. Our method outperforms prior state-of-the-art works by significant margins, not only in the conventional disjoint, blurry, and i-Blurry CL setups but also in the HLE setup.