This study introduces a method called Knowledge Distillation (KD) that uses soft labels from a teacher to guide a student. In contrast, self-KD does not require a teacher and generates its own soft labels. The authors unify the formulations of KD and self-KD by decomposing the generic KD loss into a Normalized KD (NKD) loss and customized soft labels for target and non-target classes. The NKD loss normalizes the non-target logits to ensure their sum is equal, thus improving the use of soft labels for distillation. The authors also propose Universal Self-KD (USKD), which generates customized soft labels without a teacher. USKD smooths the target logit and uses the rank of intermediate features to generate soft non-target labels. For KD with teachers, NKD achieves state-of-the-art performance on CIFAR-100 and ImageNet. For self-KD without teachers, USKD is the first method that can be effectively applied to both CNN and ViT models, resulting in new state-of-the-art results on ImageNet. The code for this study is available at https://github.com/yzd-v/cls_KD.