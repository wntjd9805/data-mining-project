We propose UniTR, an efficient multi-modal backbone for outdoor 3D perception in autonomous driving systems. Current research in 3D perception relies on modality-specific approaches, resulting in increased computational overhead and inefficient collaboration between sensor data. UniTR addresses these issues by utilizing a modality-agnostic transformer encoder to handle view-discrepant sensor data, allowing for parallel representation learning and automatic cross-modal interaction without fusion steps. Additionally, we introduce a novel multi-modal integration strategy that considers both semantic-rich 2D perspective and geometry-aware 3D sparse neighborhood relations. UniTR supports various 3D perception tasks and achieves state-of-the-art performance on the nuScenes benchmark, with improved results in 3D object detection and BEV map segmentation. Furthermore, UniTR has lower inference latency and the code is available at https://github.com/Haiyang-W/UniTR.