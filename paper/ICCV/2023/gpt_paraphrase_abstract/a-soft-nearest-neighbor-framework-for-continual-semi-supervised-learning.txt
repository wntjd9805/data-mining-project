Current continual learning approaches rely on fully labeled data, which is unrealistic. This paper introduces an approach for continual semi-supervised learning, where only some data samples are labeled. The main challenge in this setting is the model's tendency to forget representations of unlabeled data and overfit the labeled samples. To address this, the authors utilize nearest-neighbor classifiers to partition the feature space and model the data distribution effectively. This non-parametric approach allows the model to learn a strong representation for the current task and extract relevant information from previous tasks. Through extensive experiments, the proposed method demonstrates superior performance compared to existing approaches, particularly in the continual semi-supervised learning paradigm. For instance, on CIFAR-100, the method outperforms others even with significantly less supervision (0.8% vs. 25% annotations). Furthermore, the method is effective for both low and high-resolution images and can be applied to complex datasets such as ImageNet-100. The source code for this method is publicly available at https://github.com/kangzhiq/NNCSL.