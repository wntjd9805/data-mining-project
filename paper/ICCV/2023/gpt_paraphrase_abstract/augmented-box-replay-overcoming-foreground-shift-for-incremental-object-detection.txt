This study focuses on the problem of catastrophic forgetting in incremental learning, specifically in the context of incremental object detection (IOD). While replaying stored samples from previous tasks is an effective method in incremental learning, it has not been successfully applied to IOD. The main reason for this is the issue of foreground shift, where the background of previous task images may contain foreground objects from the current task. To overcome this problem, the authors propose a novel method called Augmented BoxReplay (ABR) that only stores and replays foreground objects, thereby avoiding foreground shift. Additionally, they introduce an innovative loss function called Attentive RoI Distillation that uses spatial attention from region-of-interest (RoI) features to guide the current model's focus on the most important information from the old model. ABR not only reduces forgetting of previous classes but also maintains plasticity in current classes while significantly reducing storage requirements compared to standard image replay. The model's performance is validated through comprehensive experiments on Pascal-VOC and COCO datasets, demonstrating its state-of-the-art performance.