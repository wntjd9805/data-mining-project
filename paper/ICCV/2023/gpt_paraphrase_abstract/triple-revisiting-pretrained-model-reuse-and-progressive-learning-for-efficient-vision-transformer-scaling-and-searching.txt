One effective method to speed up the training of transformers is to utilize small pretrained models to initialize the transformer. These pretrained models already possess strong representation power, which aids in faster convergence of the model. Previous studies have developed expansion operators to scale up pretrained models to the desired model size before training. However, scaling a transformer in all dimensions simultaneously can lead to difficulties in preserving the functionality of the model. Additionally, maintaining the optimizer states for weights is crucial for successful model scaling, but the new weights introduced during expansion lack these states from the pretrained models. To tackle these challenges, we propose a method called TripLe. TripLe partially scales the model before training, allowing the remaining new parameters to grow during training by copying both the warmed-up weights and optimizer states from the existing weights. Consequently, the new parameters acquire their training states. Furthermore, by sequentially scaling the model width and depth, we can preserve the functionality of each expansion. We evaluate TripLe in both single-trial model scaling and multi-trial neural architecture search (NAS). Due to the rapid convergence of TripLe, the proxy accuracy achieved by TripLe provides a more accurate measure of model quality compared to training from scratch in multi-trial NAS. Experimental results demonstrate that TripLe outperforms both training from scratch and knowledge distillation (KD) in terms of training time and task performance. Moreover, TripLe can be combined with KD to further enhance task accuracy. In the case of NAS, the model obtained through TripLe surpasses DeiT-B in task accuracy while reducing the parameter size and FLOPs by 69%.