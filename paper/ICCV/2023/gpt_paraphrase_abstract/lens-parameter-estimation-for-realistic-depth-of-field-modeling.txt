We present a novel approach for estimating the depth of field effect in a single image. Unlike existing methods that estimate blur and/or depth on a per-pixel basis, our method utilizes a lens-based representation that incorporates two parameters: the blur factor and focus disparity. By incorporating a signed defocus representation, we achieve a more intuitive and linear representation. To solve this representation, we introduce a new weighting network. Additionally, our method ensures consistency between the estimated defocus blur, lens parameters, and depth map. We train our deep-learning-based model on a combination of real images with synthetic depth of field and fully synthetic images. These enhancements result in a more robust and accurate method, as evidenced by our state-of-the-art results. Our lens parametrization allows for various applications, including 3D staging in augmented reality environments and seamless object compositing.