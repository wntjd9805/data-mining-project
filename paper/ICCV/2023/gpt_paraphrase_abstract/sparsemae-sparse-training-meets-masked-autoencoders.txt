Masked Autoencoders (MAE) and its variations have been successful in pretraining large Vision Transformers (ViTs). However, the benefits of pretraining mechanisms are limited for small-scale models due to their capacity constraints. Sparse training, which involves pruning unimportant parameters, is a method used to transfer representations from large models to smaller ones. However, combining MAE fine-tuning with sparse training in a naive manner makes the network task-specific and leads to the loss of crucial task-agnostic knowledge necessary for model generalization.The objective of this study is to reduce the complexity of large vision transformers pretrained by MAE using sparse training. The paper presents an overview of different sparse training methods used to prune large vision transformers during both the MAE pretraining and finetuning stages, and discusses their limitations. To enhance learning of both task-agnostic and task-specific knowledge, the authors propose a novel two-stage sparse training method called SparseMAE. The method involves sparse pretraining, where a small-scale sub-network is dynamically pruned from a ViT-Base. During finetuning, the sparse sub-network adapts its topology connections based on the task-agnostic knowledge of the full model. Extensive experimental results demonstrate the effectiveness of SparseMAE and its superiority in improving small-scale vision transformers.The authors provide the code for SparseMAE at https://github.com/aojunzz/SparseMAE.