We utilize video data to reconstruct a neural volume that captures various aspects of the scene including color, density, scene flow, semantics, and attention information. By incorporating semantics and attention, we are able to differentiate prominent foreground objects from the background throughout the video. To address the issue of low resolution semantic and attention features, we employ pyramid computations that balance detail with whole-image context. Through optimization, we conduct saliency-aware clustering to break down the scene into distinct elements. For evaluation purposes, we annotate object masks in the NVIDIA Dynamic Scene and DyCheck datasets. Our findings demonstrate that our unsupervised approach can effectively decompose dynamic scenes and achieve comparable performance to supervised methods. Moreover, our method enhances foreground/background segmentation compared to recent static/dynamic split methods. To access further details, please visit our project webpage: https://visual.cs.brown.edu/saff.