We propose UHDNeRF, a new framework for synthesizing novel views of real-world scenes at ultra-high resolutions (e.g., 4K). Existing NeRF methods are not optimized for rendering at such high resolutions, resulting in blurry outputs that lose important details even when trained on 4K images. This is because the low-dimensional volumetric representation used by NeRF does not effectively capture the high-resolution inputs. To address this issue, we introduce an adaptive implicit-explicit scene representation. This approach combines an explicit sparse point cloud with an implicit volume to enhance the modeling of subtle details. We employ a frequency separation strategy, where the implicit volume represents the low-frequency properties of the scene, while the sparse point cloud reproduces high-frequency details. To fully exploit the information in the point cloud, we extract global structure features and local point-wise features for each sample located in the high-frequency regions. Additionally, we introduce a patch-based sampling strategy to reduce computational costs. Our method achieves high-fidelity rendering results, demonstrating its superiority in preserving high-frequency details in ultra-high-resolution scenarios compared to state-of-the-art NeRF-based solutions.