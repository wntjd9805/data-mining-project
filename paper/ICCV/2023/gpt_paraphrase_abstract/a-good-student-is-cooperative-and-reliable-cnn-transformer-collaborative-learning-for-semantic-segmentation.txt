This paper addresses the question of how to collaboratively learn convolutional neural network (CNN)-based and vision transformer (ViT)-based models for semantic segmentation. The authors propose an online knowledge distillation (KD) framework that simultaneously learns compact yet effective CNN-based and ViT-based models. The framework includes two key technical breakthroughs. Firstly, heterogeneous feature distillation (HFD) is introduced to improve consistency in low-layer feature space by mimicking heterogeneous features between CNNs and ViT. Secondly, bidirectional selective distillation (BSD) is proposed to enable the two student models to learn reliable knowledge from each other. BSD includes region-wise BSD, which determines the directions of knowledge transferred between corresponding regions in the feature space, and pixel-wise BSD, which discerns which prediction knowledge to transfer in the logit space. Extensive experiments on three benchmark datasets demonstrate that the proposed framework outperforms state-of-the-art online distillation methods and effectively facilitates collaborative learning between ViT-based and CNN-based models.