Spiking Neural Networks (SNNs) are promising models for energy-efficient neuromorphic computing. The backpropagation through time (BPTT) with surrogate gradients (SG) method has been successful in training non-differentiable SNNs. However, this method has limitations in terms of memory cost and training time. To address these issues, we propose the Spatial Learning Through Time (SLTT) method, which achieves high performance while significantly improving training efficiency compared to BPTT. Our approach involves ignoring unimportant routes in the computational graph during backpropagation, as we find that the backpropagation through the temporal domain contributes minimally to the final calculated gradients. This reduction in scalar multiplications leads to a smaller memory occupation that is not dependent on the total time steps. Additionally, we introduce a variant of SLTT, called SLTT-K, which allows backpropagation only at K time steps, further reducing the required number of scalar multiplications and making it independent of the total time steps.We conduct experiments on both static and neuromorphic datasets to evaluate the performance of our SLTT method. The results demonstrate superior training efficiency and performance compared to existing methods. Notably, our method achieves state-of-the-art accuracy on ImageNet, while reducing memory cost and training time by more than 70% and 50%, respectively, compared to BPTT. Our code is publicly available at https://github.com/qymeng94/SLTT.