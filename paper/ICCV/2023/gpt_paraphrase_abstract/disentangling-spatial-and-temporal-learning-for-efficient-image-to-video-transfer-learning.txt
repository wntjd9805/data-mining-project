Recently, there have been advancements in large-scale pre-trained language-image models, such as CLIP, which demonstrate remarkable abilities in understanding spatial content. However, applying these models to video recognition still faces challenges in effectively capturing temporal information. Current approaches involve incorporating tunable structures into or parallel to the pre-trained model. However, these methods either require resource-intensive back-propagation through the entire pre-trained model or are limited by the temporal reasoning capabilities of the pre-trained structure.In this study, we introduce DiST, a novel approach that disentangles the learning of spatial and temporal aspects in videos. DiST utilizes a dual-encoder structure, where a pre-trained foundation model serves as the spatial encoder, and a lightweight network is introduced as the temporal encoder. To combine spatio-temporal information, an integration branch is inserted between the encoders. This disentangled learning approach in DiST is highly efficient as it avoids the need for back-propagation with massive pre-trained parameters. Moreover, we empirically demonstrate that disentangled learning with an additional network for integration improves both spatial and temporal understanding.We conducted extensive experiments on five benchmark datasets, and the results show that DiST outperforms existing state-of-the-art methods by significant margins. When pre-training on the large-scale Kinetics-710 dataset, we achieve an impressive 89.7% accuracy on the Kinetics-400 dataset using a frozen ViT-L model, which confirms the scalability of DiST.For access to our codes and models, please visit https://github.com/alibaba-mmai-research/DiST.