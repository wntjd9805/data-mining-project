The recent development of camera-based Bird's-Eye-View (BEV) perception has been a viable option for cost-effective autonomous driving. However, current BEV-based multi-view 3D detectors tend to convert all image features into BEV features, disregarding the issue that a significant portion of background information may overshadow the object information. In this study, we introduce Semantic-Aware BEV Pooling (SA-BEVPool), which can eliminate background information based on the semantic segmentation of image features and convert image features into semantic-aware BEV features. Additionally, we propose BEV-Paste, an efficient data augmentation strategy that aligns well with semantic-aware BEV features. Furthermore, we develop a Multi-Scale Cross-Task (MSCT) head that combines task-specific and cross-task information to enhance the accuracy of depth distribution prediction and semantic segmentation, thereby improving the quality of semantic-aware BEV features. Finally, we integrate these modules into a novel multi-view 3D object detection framework called SA-BEV. Experimental results on nuScenes demonstrate that SA-BEV achieves state-of-the-art performance. The source code is available at https://github.com/mengtan00/SA-BEV.git.