Unsupervised domain adaptation involves transferring knowledge from a labeled source domain to an unlabeled target domain. However, obtaining abundant labeled data in real-world scenarios is challenging and expensive. To address this, recent research has focused on Few-shot Unsupervised Domain Adaptation (FUDA), where only a small number of source samples are labeled. These methods use self-supervised learning techniques for knowledge transfer. However, existing approaches often overlook the limited labeled data, which hinders reliable knowledge transfer. They also fail to consider the varying difficulty levels of target samples, leading to poor classification of challenging samples. To overcome these limitations, we propose a new method called Confidence-based Visual Dispersal Transfer (C-VisDiT) for FUDA. C-VisDiT includes a strategy that transfers only high-confidence source knowledge for model adaptation and a strategy that guides the learning of hard target samples based on easy ones. We evaluate C-VisDiT on benchmark datasets and compare it with state-of-the-art FUDA methods. The results demonstrate that C-VisDiT outperforms existing approaches significantly. The code for our method is available at https://github.com/Bostoncake/C-VisDiT.