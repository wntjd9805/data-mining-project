Casually captured Neural Radiance Fields (NeRFs) often suffer from artifacts and flawed geometry when rendered outside the input camera trajectory. Current evaluation protocols fail to address these issues as they only assess image quality at specific intervals during training. To address this limitation and facilitate the development and evaluation of new methods in novel-view synthesis, we introduce a new dataset and evaluation procedure. This involves recording two camera trajectories of the scene: one for training and another for evaluation. In this more challenging in-the-wild setting, we observe that existing hand-crafted regularizers are ineffective in removing artifacts and improving scene geometry. To overcome this, we propose a 3D diffusion-based method that utilizes local 3D priors and a novel density-based score distillation sampling loss. This approach discourages artifacts during NeRF optimization. Our experiments demonstrate that this data-driven prior successfully eliminates artifacts and enhances scene geometry for casually captured NeRFs.