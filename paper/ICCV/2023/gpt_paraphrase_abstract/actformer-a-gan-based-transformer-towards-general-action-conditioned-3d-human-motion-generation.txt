We propose a GAN-based Transformer model called ActFormer for generating 3D human motion, including both single-person and multi-person actions. Our approach combines the strengths of Transformer's spatio-temporal representation, GAN's generative modeling, and a Gaussian Process latent prior for temporal correlations. ActFormer can be extended to handle multi-person motions by modeling both temporal correlations and human interactions using Transformer encoders. We also introduce a new synthetic dataset of complex multi-person combat behaviors to support research on multi-person motion generation. Experimental results on multiple datasets demonstrate that our method outperforms existing approaches, making significant progress towards a general human motion generator. The project website can be accessed at https://liangxuy.github.io/actformer/.