Visual question answering involves predicting the answer to a question about an image. As different individuals may provide different answers to the same question, we seek to gain a better understanding of the reasons behind these answer variations. To address this, we introduce VQA-AnswerTherapy, the first dataset that visually grounds each unique answer to a visual question. We also propose two new problems: determining whether a visual question has a single answer grounding and localizing all answer groundings. We evaluate state-of-the-art algorithms on these problems to identify their strengths and limitations. The dataset and evaluation server can be accessed publicly at https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/. The answer format focuses solely on abstraction.