Despite advancements in generating high-quality images using diffusion models, the synthesis of photorealistic and temporally coherent animated frames is still in its early stages. While large-scale datasets for image generation are readily available, the collection of similar video data on such a scale remains challenging. Additionally, training a video diffusion model is computationally more demanding than its image counterpart. This study examines the feasibility of fine-tuning a pretrained image diffusion model with video data as a practical approach for video synthesis. The results reveal that simply extending the image noise prior to video diffusion yields subpar performance. However, by carefully designing a video noise prior, significantly better results are achieved. Through extensive experimentation, our model, named Preserve Your Own COrrelation (PYoCo), achieves state-of-the-art zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. Furthermore, it attains superior video generation quality on the smaller-scale UCF-101 benchmark using a 10Ã— smaller model and significantly less computation compared to previous approaches. Additional information about the project can be found at https://research.nvidia.com/labs/dir/pyoco/.