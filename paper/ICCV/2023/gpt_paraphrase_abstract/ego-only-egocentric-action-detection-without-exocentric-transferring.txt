We introduce a novel approach called Ego-Only, which allows for accurate action detection in egocentric videos without relying on any third-person transferring techniques. Previous studies have shown that training egocentric models from scratch is challenging, and transferring from third-person representations improves accuracy. However, in this paper, we challenge this belief and propose a strategy that enables effective training of egocentric models without third-person transferring. Our Ego-Only approach involves training a video representation using a masked autoencoder finetuned for temporal segmentation. The learned features are then used in an off-the-shelf temporal action localization method for action detection. We demonstrate that this approach achieves remarkably strong results on three well-known egocentric video datasets (Ego4D, EPIC-Kitchens-100, and Charades-Ego) without the need for third-person data. Ego-Only outperforms previous state-of-the-art methods that rely on third-person transferring, even when those methods use significantly more labels. Our approach sets new benchmarks for both action detection and action recognition in egocentric videos.