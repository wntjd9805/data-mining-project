This study presents a method for performing semantic segmentation of 3D point clouds using a feature synthesis approach. The goal is to enable the segmentation of objects that have not been seen before by generalizing from class-level semantic information. To achieve this, the authors aim to improve the correspondence between visual and semantic spaces by synthesizing diverse, generic, and transferable visual features. They propose a masked learning strategy to increase the diversity within the same class of visual features and enhance the separation between different classes. Additionally, they transform the visual features into a prototypical space to align them with the corresponding semantic space. To preserve the semantic-visual relationships, a consistency regularizer is developed. Experimental results on benchmarks such as Scan-Net, S3DIS, and SemanticKITTI demonstrate the effectiveness of the proposed approach in semantic segmentation. The code for the approach is available at the provided GitHub link.