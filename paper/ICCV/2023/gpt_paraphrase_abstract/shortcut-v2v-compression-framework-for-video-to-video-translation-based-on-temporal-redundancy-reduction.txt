Video-to-video translation is a technique used to generate video frames of a desired style or domain from an input video. However, current networks for video-to-video translation require extensive computational resources, which limits their practicality. To address this issue, we propose Shortcut-V2V, a compression framework that reduces computational requirements without sacrificing performance. Shortcut-V2V achieves this by approximating the intermediate features of a current frame using those of the previous frame, thereby avoiding full inference for every neighboring frame. Additionally, we introduce AdaBD, a new block that adaptively blends and deforms the features of neighboring frames, leading to more accurate predictions of intermediate features. We evaluate our framework using various video-to-video translation models and demonstrate its general applicability through quantitative and qualitative assessments. The results indicate that Shortcut-V2V achieves comparable performance to the original models while reducing computational cost by 3.2-5.7 times and memory usage by 7.8-44 times during testing. To access our code and videos, please visit our website: https://shortcut-v2v.github.io/.