The abstract discusses the challenges and solutions in incorporating continuous changes in scenes for novel view synthesis. It introduces a new dataset called World Across Time (WAT), which consists of scenes that change in appearance and geometry over time. To address this challenge, a method called CLNeRF is proposed, which combines continual learning with Neural Radiance Fields (NeRFs). CLNeRF utilizes generative replay and the Instant Neural Graphics Primitives (NGP) architecture to prevent catastrophic forgetting and efficiently update the model with new data. Trainable appearance and geometry embeddings are also added to NGP, allowing a single compact model to handle complex scene changes. CLNeRF outperforms other continual learning baselines across standard benchmarks and the WAT dataset. The source code, a demo, and the WAT dataset can be accessed at https://github.com/IntelLabs/CLNeRF.