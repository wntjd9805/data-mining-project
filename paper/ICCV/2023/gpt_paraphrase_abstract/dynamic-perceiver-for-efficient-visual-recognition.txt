Early exiting has emerged as a promising strategy for enhancing the efficiency of deep networks in making predictions. This involves structuring models with multiple classifiers, enabling the generation of predictions for "easy" samples at earlier stages, thereby eliminating the need for deeper layers. However, current multi-exit networks typically employ linear classifiers at intermediate layers, which forces low-level features to encode high-level semantics. This flawed design inevitably hampers the performance of later exits. To address this issue, we propose a novel architecture called Dynamic Perceiver (Dyn-Perceiver). Our approach decouples the feature extraction process from the early classification task using a dual-branch structure. One branch is responsible for extracting image features, while the other branch processes a latent code assigned for classification tasks. We incorporate bi-directional cross-attention layers to progressively merge information from both branches. Furthermore, early exits are exclusively placed within the classification branch, eliminating the requirement for linear separability in low-level features. Dyn-Perceiver serves as a versatile and adaptable framework that can be applied to various architectures. Through experiments in image classification, action recognition, and object detection, we demonstrate that our method significantly enhances the inference efficiency of different backbones, surpassing numerous competitive techniques across a wide range of computational budgets. Evaluation on both CPU and GPU platforms confirms the superior practical efficiency of Dyn-Perceiver. The code for our approach is available at https://www.github.com/LeapLabTHU/Dynamic_Perceiver.