Multi-task visual learning in computer vision has primarily focused on dense prediction, neglecting the 3D world and its multi-view structures. This approach also lacks versatility in generating new views. To address these limitations, we introduce the concept of multi-task view synthesis (MTVS), which treats multi-task prediction as a collection of new-view synthesis tasks for different scene properties, including RGB. To tackle MTVS, we propose a framework called Mu-vieNeRF that combines multi-task and cross-view knowledge to simultaneously synthesize multiple scene properties. Mu-vieNeRF incorporates the Cross-Task Attention (CTA) and Cross-View Attention (CVA) modules to effectively utilize information from various views and tasks. Through extensive evaluation on both synthetic and real-world datasets, we demonstrate that Mu-vieNeRF can generate different scene properties with high visual quality, surpassing traditional discriminative models in various scenarios. Additionally, we show that Mu-vieNeRF is compatible with a range of NeRF backbones, exhibiting universal applicability. The code for Mu-vieNeRF is publicly available at https://github.com/zsh2000/MuvieNeRF.