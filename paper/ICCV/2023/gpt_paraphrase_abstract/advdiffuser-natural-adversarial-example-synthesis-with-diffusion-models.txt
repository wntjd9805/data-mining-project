Previous research on adversarial examples has focused on fixed norm perturbation limits, which do not accurately represent how humans perceive perturbations. Recent studies have shifted towards natural unrestricted adversarial examples (UAEs) that exceed perturbation bounds but still appear semantically plausible. However, current methods that use GAN or VAE to generate UAEs result in low-quality and unnatural images due to the loss of high-level information. To address this issue, we propose a new method called AdvDiffuser, which utilizes diffusion models to synthesize natural UAEs. AdvDiffuser can generate UAEs from scratch or based on reference images. In order to create natural UAEs, we perturb predicted images to guide their latent code towards the adversarial sample space of a specific classifier. Additionally, we introduce adversarial inpainting based on class activation mapping to preserve the important regions of the image while perturbing less significant areas. Our experiments on CIFAR-10, CelebA, and ImageNet demonstrate that AdvDiffuser can successfully defeat the most robust models on the RobustBench leaderboard with close to 100% success rates. Furthermore, the UAEs generated by AdvDiffuser are not only more natural but also stronger compared to current state-of-the-art attacks. Specifically, compared to the GA-attack, the UAEs generated by AdvDiffuser have 6 smaller FID scores and exhibit a 0.28 higher SSIM metric, making them more perceptually stealthy. Finally, adversarial training with AdvDiffuser improves model robustness against attacks with unseen threat models, resulting in 1 smaller LPIPS perturbations and 2 3⇥⇠⇥ improvements.