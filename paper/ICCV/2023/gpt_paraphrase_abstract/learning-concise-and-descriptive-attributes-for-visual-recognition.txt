Recent advancements in foundation models have opened up new possibilities for interpretable visual recognition. One approach involves querying Large Language Models (LLMs) to obtain a set of attributes that describe each class, and then utilizing vision-language models to classify images based on these attributes. Previous studies have shown that querying a large number of attributes can yield competitive performance compared to image features. However, our own investigation on 8 datasets has uncovered that LLM-generated attributes in large quantities perform similarly to random words. This surprising discovery suggests that these attributes may contain significant noise. We propose a novel learning-to-search method to identify subsets of attributes that can maintain classification performance with much smaller sizes. As a result, our method achieves performance on par with massive LLM-generated attributes, such as 10k attributes for the CUB dataset, while only utilizing a total of 32 attributes to distinguish between 200 bird species. Additionally, our approach offers higher interpretability and interactivity for humans, as well as the ability to summarize knowledge for recognition tasks.