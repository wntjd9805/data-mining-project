This paper presents a new method for generating 3D-aware images by utilizing 2D diffusion models. The approach involves generating a set of 2D images from multiple views, and then using a sequential unconditional-conditional process to generate the final image. This approach enhances the generative modeling power of the method by leveraging 2D diffusion models. Additionally, depth information from monocular depth estimators is incorporated to create training data for the conditional diffusion model using still images. The method is trained on a large-scale unstructured 2D image dataset, specifically ImageNet, which has not been addressed by previous methods. The results show that the method produces high-quality images that outperform previous approaches. Furthermore, the method is capable of generating images with large view angles, even when the training images are diverse and unaligned, collected from real-world environments.