Network calibration is crucial for accurate predictions in deep neural networks used in real-world systems. Existing approaches use mixup to calibrate network predictions during training but fail to address the issue of inaccurate label mixtures in mixup. In this paper, we propose RankMixup, a novel framework that tackles the problem of label mixtures in network calibration. We suggest using an ordinal ranking relationship between raw and mixup-augmented samples as a supervisory signal for calibration instead of label mixtures. We hypothesize that the network should have higher confidence in raw samples compared to augmented ones. To implement this, we introduce a mixup-based ranking loss (MRL) that enforces lower confidence for augmented samples while maintaining the ranking relationship. Additionally, we leverage the ranking relationship among multiple mixup-augmented samples to enhance calibration capability. We expect augmented samples with higher mixing coefficients to have higher confidence and vice versa. We introduce a novel loss, M-NDCG, to align the order of confidences with the mixing coefficients. Extensive experiments on standard benchmarks demonstrate the effectiveness of RankMixup in network calibration.