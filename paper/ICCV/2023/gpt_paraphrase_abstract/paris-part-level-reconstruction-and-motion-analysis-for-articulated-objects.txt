We propose a method for simultaneously reconstructing the individual parts of articulated objects and estimating their motion parameters. By analyzing multi-view images of the object in two different articulation states, we separate the movable parts from the static ones and reconstruct their shape and appearance while predicting the motion parameters. Our approach, called PARIS, is a self-supervised architecture that learns implicit models for shape and appearance at the part level. It optimizes the motion parameters without the need for 3D supervision, motion annotations, or semantic labeling. In comparison to existing methods that utilize 3D point clouds, our method demonstrates better generalization across object categories and outperforms baseline models. We achieve significant improvements in reconstruction accuracy, reducing the Chamfer-L1 distance by 3.94 (45.2%) for objects and 26.79 (84.5%) for parts. Additionally, our motion estimation achieves an error rate of 5% across 10 object categories.