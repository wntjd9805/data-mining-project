In this study, the authors address the issue of driver distraction as a major cause of severe traffic accidents in recent years. They highlight the lack of comprehensive perception datasets that restrict the development of vision-driven driver monitoring systems, leading to compromised road safety and traffic security. To overcome this limitation, the authors present the AssIstive Driving pErception dataset (AIDE), which takes into account contextual information both inside and outside the vehicle in naturalistic scenarios. AIDE enables holistic driver monitoring through three key features: multi-view settings of driver and scene, multi-modal annotations of face, body, posture, and gesture, and four practical task designs for driving understanding. To thoroughly explore the capabilities of AIDE, the authors provide experimental benchmarks using three baseline frameworks and a variety of methods. They also introduce two fusion strategies to gain new insights into learning effective multi-stream/modal representations. The authors systematically investigate the importance and rationality of the key components in AIDE and the benchmarks. The project link for AIDE is provided as https://github.com/ydk122024/AIDE.