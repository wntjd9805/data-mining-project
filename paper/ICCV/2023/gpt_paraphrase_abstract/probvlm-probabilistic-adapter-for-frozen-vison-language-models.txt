Large-scale vision-language models, such as CLIP, have been successful in finding connections between images and text. However, the current deterministic mapping process used by these models poses a problem. It assigns a single vector in the embedding space to an image or text sample, disregarding the fact that multiple samples can represent the same concept in the real world. This lack of ambiguity in the embedding space is not reflective of reality. To address this issue, we introduce ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs. This estimation is achieved through inter/intra-modal alignment in a post-hoc manner, without the need for large-scale datasets or extensive computing. We evaluate ProbVLM on challenging datasets (COCO, Flickr, CUB, and Oxford-flowers) and compare it to other methods. Our results demonstrate that ProbVLM outperforms these methods in terms of estimating multi-modal embedding uncertainties and improving retrieval tasks. Additionally, we explore two real-world downstream tasks for VLMs: active learning and model selection. In both cases, we show that the estimated uncertainty provided by ProbVLM aids in these tasks. Finally, we introduce a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model. The code for ProbVLM is available at https://github.com/ExplainableML/ProbVLM.