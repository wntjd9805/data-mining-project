The ability to perform various visual tasks in dynamic scenes is a unique feature of human perception. Despite advancements in image and video recognition through representation learning, current research mainly focuses on designing specialized networks for individual or simple combinations of tasks. This study aims to create a unified model for multiple image and video recognition tasks in autonomous driving, considering the diverse input and output structures involved. To facilitate this exploration, a new challenge called Video Task Decathlon (VTD) is introduced, consisting of ten representative tasks related to object classification, segmentation, localization, and association. The proposed unified network, VTDNet, utilizes a single structure and set of weights for all ten tasks. VTDNet employs task interaction stages to exchange information between similar tasks and task groups. Due to practical limitations in labeling all tasks on every frame and the performance degradation associated with joint training, a Curriculum training, Pseudo-labeling, and Fine-tuning (CPF) scheme is designed to successfully train VTDNet on all tasks and mitigate performance loss. With the CPF scheme, VTDNet outperforms its single-task counterparts on most tasks while only requiring 20% of the overall computations. This research presents a promising direction for exploring the integration of perception tasks in autonomous driving.