Transformer-based methods have become highly successful in 2D and 3D detection tasks in images. However, the tokenization process used before the attention mechanism in these methods causes a loss of spatial information. To address this issue, positional encoding has been identified as a crucial component for improving the performance of these methods. Recent research has demonstrated that encoding based on samples of 3D viewing rays can enhance the quality of multi-camera 3D object detection. Building on this, we propose the use of 3D point positional encoding (3DPPE) in the Transformer decoder for 3D detection. Instead of relying solely on rays, we believe that 3D point locations can offer more valuable information. To implement 3DPPE, we leverage predicted depth values to approximate the actual point positions, even though 3D measurements are not available during monocular 3D object detection inference. Our hybrid-depth module combines direct and categorical depth estimation to refine the depth of each pixel. Despite the approximation, our approach achieves impressive results, with 46.0 mAP and 51.4 NDS on the competitive nuScenes dataset, outperforming encodings based on ray samples. The code for our method can be accessed at https://github.com/drilistbox/3DPPE.