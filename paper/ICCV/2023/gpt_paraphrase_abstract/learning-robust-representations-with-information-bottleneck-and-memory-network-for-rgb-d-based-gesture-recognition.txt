Current RGB-D-based gesture recognition methods have shown promise, but often fail to consider the interference of task-irrelevant cues such as illumination and background. These irrelevant factors are learned alongside the predictive ones, which hampers accurate recognition. This paper proposes an analytical framework to address this issue. Drawing on the Information Bottleneck theory, two rules (Sufficiency and Compactness) are derived to create a new information-theoretic loss function. This loss function promotes a more sufficient and compact representation by mitigating the impact of gesture-irrelevant information. To emphasize predictive information, a memory network is integrated using a content-based and contextual memory addressing scheme. This scheme weakens nuisances while preserving task-relevant information, guiding the refinement of the feature representation. Experimental results on three public datasets demonstrate that our approach achieves a better feature representation and outperforms state-of-the-art methods. The code for our method is available at: https://github.com/Carpumpkin/InBoMem.