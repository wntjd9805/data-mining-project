Recent advancements in video and 3D understanding have led to the emergence of novel 4D spatio-temporal methods that combine these concepts. One such method is the Ego4D Episodic Memory Benchmark, which introduced a task called Visual Queries with 3D Localization (VQ3D). The objective of this task is to determine the 3D position of a query object's center in relation to the camera pose of a query frame, using an egocentric video clip and an image crop of the query object. Current approaches to VQ3D involve converting the 2D localization results of the Visual Queries with 2D Localization (VQ2D) task into 3D predictions. However, we highlight that the limited number of camera poses resulting from camera re-localization in previous VQ3D methods significantly hampers their success rate.In this study, we present a pipeline called EgoLoc that effectively combines 3D multiview geometry with 2D object retrieval from egocentric videos. Our approach involves estimating more accurate camera poses and aggregating multi-view 3D displacements by leveraging the confidence of 2D object detection. This significantly improves the success rate of object queries and achieves a state-of-the-art performance in the VQ3D task, with an overall success rate of up to 87.12%. We provide a comprehensive empirical analysis of the VQ3D task and existing solutions, and identify the remaining challenges in VQ3D. The code for our approach is available at https://github.com/Wayne-Mai/EgoLoc.