This paper introduces a novel method called Minimum Spanning Tree (MST) compression for compressing and accelerating binary neural networks (BNNs) in order to address the computational burden associated with wider and deeper networks. The proposed architecture leverages the observation that an output channel in a binary convolution can be computed using another output channel and XNOR operations with different weights. The method involves constructing a fully connected graph with vertices representing output channels and calculating the distance between vertices based on the difference in weights used for those outputs. The Minimum Spanning Tree (MST) of the graph with the minimum depth is then used to reorder output calculations, aiming to reduce computational cost and latency. Additionally, a new learning algorithm is proposed to minimize the total MST distance during training. Experimental results on benchmark models demonstrate that the proposed method achieves significant compression ratios with minimal accuracy drops, making it a promising approach for resource-constrained edge-computing devices.