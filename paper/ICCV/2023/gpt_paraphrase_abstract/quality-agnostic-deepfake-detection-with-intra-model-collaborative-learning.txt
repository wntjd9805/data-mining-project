The rise of deepfake technology has raised concerns regarding security threats and the spread of fake information. While there has been significant research on deepfake detection, the challenge of detecting low quality and different qualities of deepfakes simultaneously remains. Most current approaches use a single model for detecting specific types of deepfake video quality, which is computationally expensive and impractical for real-world deployment. In this study, we propose a universal collaborative learning framework called QAD, which allows for the effective and simultaneous detection of different qualities of deepfakes. Our approach maximizes the dependency between intermediate representations of images from different quality levels using the Hilbert-Schmidt Independence Criterion. We also introduce an Adversarial Weight Perturbation module to enhance the model's robustness against image corruption and improve overall performance. Through extensive experiments on seven deepfake datasets, we demonstrate that our QAD model outperforms previous state-of-the-art benchmarks.