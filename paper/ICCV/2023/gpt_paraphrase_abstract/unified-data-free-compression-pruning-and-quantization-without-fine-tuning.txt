This study introduces a novel framework called Unified Data-Free Compression (UDFC) that addresses the issue of reducing inference time and memory usage in neural networks without requiring access to the original training dataset. Existing methods often rely on fine-tuning the model with the original dataset, which is resource-intensive and raises privacy concerns for sensitive data. While some data-free methods have been proposed, they typically focus on either pruning or quantization separately, neglecting the potential benefits of combining the two techniques. In contrast, UDFC simultaneously performs pruning and quantization without any data or fine-tuning process. The framework assumes that the information lost during compression (e.g., due to pruning or quantization) can be reconstructed by linearly combining other channels. Based on this assumption, UDFC derives a reconstruction form to restore the compressed network's information loss. The study mathematically formulates the reconstruction error between the original and compressed networks and deduces a closed-form solution. The UDFC framework is evaluated on a large-scale image classification task, demonstrating significant improvements across various network architectures and compression methods. For instance, compared to the state-of-the-art method using 30% pruning ratio and 6-bit quantization on ResNet-34, UDFC achieves a 20.54% accuracy improvement on the ImageNet dataset. The code for UDFC will be made available.