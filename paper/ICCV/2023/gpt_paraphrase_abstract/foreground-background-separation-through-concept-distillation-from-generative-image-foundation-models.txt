Curating datasets for object segmentation is a challenging task. However, the development of large-scale pre-trained generative models has greatly improved the quality and ease of conditional image generation. In this study, we propose a new approach that allows the generation of general foreground-background segmentation models using simple textual descriptions, without the need for segmentation labels. We utilize pre-trained latent diffusion models to automatically create weak segmentation masks for concepts and objects. These masks are then employed to fine-tune the diffusion model for an inpainting task, which enables precise removal of the object while generating a synthetic foreground and background dataset. Our method outperforms previous techniques in both discriminative and generative performance, and it narrows the gap with fully supervised training without requiring pixel-wise object labels. We showcase the results on segmenting various objects (humans, dogs, cars, birds) and demonstrate a use case in medical image analysis. The code for our method is accessible at https://github.com/MischaD/fobadiffusion.