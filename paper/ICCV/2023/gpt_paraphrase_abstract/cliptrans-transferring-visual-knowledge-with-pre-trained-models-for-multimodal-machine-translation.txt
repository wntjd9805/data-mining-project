There is a growing interest in developing multimodal machine translation (MMT) systems that enhance neural machine translation (NMT) by incorporating visual knowledge. However, training powerful MMT models from scratch is challenging due to the lack of annotated multilingual vision-language data, particularly for low-resource languages. Although there are multilingual pre-trained models for NMT and multimodal pre-trained models for vision-language tasks, these models are not directly applicable to MMT because they do not provide aligned multimodal multilingual features. To address this issue, we propose CLIPTrans, which combines the independently pre-trained multimodal M-CLIP and the multilingual mBART models. We align their embedding spaces by conditioning mBART on M-CLIP features using a lightweight mapping network. We train CLIPTrans in a two-stage pipeline, starting with image captioning and then moving to the translation task. Experimental results demonstrate the effectiveness of CLIPTrans, achieving state-of-the-art performance on standard benchmarks with an average BLEU score improvement of +2.67. The code for CLIPTrans can be found at www.github.com/devaansh100/CLIPTrans.