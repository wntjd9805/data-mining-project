This paper introduces a novel image matching method called Occ2Net, which addresses the issue of occlusion relations between objects in various visual applications. Most existing methods do not consider these occlusion relations caused by camera motion and scene structure. Occ2Net uses a 3D occupancy model to represent occlusion relations and infer matching points in occluded regions. The Occupancy Estimation (OE) module simplifies the creation of a multi-view 3D representation by incorporating inductive bias. Additionally, the method includes an Occlusion-Aware (OA) module that utilizes attention layers and rotation alignment to enable matching between occluded and visible points. The performance of Occ2Net is evaluated on real-world and simulated datasets, demonstrating its superiority over state-of-the-art methods, particularly in occlusion scenarios. The schematic diagram in Figure 1 illustrates the matching process for occluded regions, showing that Occ2Net can successfully match objects that are occluded in one image but visible in another.