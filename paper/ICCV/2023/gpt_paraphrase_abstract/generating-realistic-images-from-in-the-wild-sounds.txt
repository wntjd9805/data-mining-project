Generating images from wild sounds is a challenging task due to the lack of paired datasets and the significant differences between sound and images. Previous studies have only focused on limited categories or music. This paper presents a novel approach to generate images from in-the-wild sounds. The approach involves converting sound into text using audio captioning, utilizing audio attention and sentence attention to capture the rich characteristics of sound and visualize it, and employing direct sound optimization with CLIPscore and AudioCLIP to generate images using a diffusion-based model. Experimental results demonstrate that our model can generate high-quality images from wild sounds and outperforms baseline methods in both quantitative and qualitative evaluations on wild audio datasets.