The recent development of large-scale text-to-image diffusion models has significantly enhanced our ability to generate images based on text prompts. These models have demonstrated impressive compositional generalization skills and have been primarily used for sampling purposes. However, in addition to generating images, diffusion models can also provide conditional density estimates, which have utility beyond image generation. This study explores the potential of leveraging the density estimates from text-to-image diffusion models, such as StableDiffusion, for zero-shot classification without the need for additional training. The proposed approach, called Diffusion Classifier, achieves excellent results on various benchmarks and outperforms alternative methods of knowledge extraction from diffusion models. While there is still a gap between generative and discriminative approaches in zero-shot recognition tasks, the diffusion-based approach exhibits stronger multimodal compositional reasoning abilities compared to competing discriminative approaches. Furthermore, the study utilizes Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet, demonstrating their performance comparable to state-of-the-art discriminative classifiers and robustness to distribution shift. Overall, these findings represent a step towards the utilization of generative models rather than discriminative models for downstream tasks. For detailed results and visualizations, please visit our website at diffusion-classifier.github.io/.