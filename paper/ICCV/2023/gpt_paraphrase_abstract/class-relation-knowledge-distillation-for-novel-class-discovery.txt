We address the issue of discovering new classes without supervision using labeled data from known classes. The main challenge is transferring knowledge from known-class data to learning novel classes. Previous methods focus on creating a shared representation space for knowledge transfer and neglect modeling class relations. To overcome this, we introduce a representation of class relations for novel classes based on predicted class distributions from a model trained on known classes. However, we find that this class relation representation becomes less informative during typical discovery training. To prevent this loss of information, we propose a novel knowledge distillation framework that uses the class-relation representation to regularize the learning of novel classes. Additionally, to allow for flexible knowledge distillation, we develop a learnable weighting function that promotes knowledge transfer based on the semantic similarity between the novel and known classes. We evaluate our method on several benchmarks, including CIFAR100, Stanford Cars, CUB, and FGVC-Aircraft datasets, and our results show that our approach outperforms previous state-of-the-art methods by a significant margin. The code for our method is available.