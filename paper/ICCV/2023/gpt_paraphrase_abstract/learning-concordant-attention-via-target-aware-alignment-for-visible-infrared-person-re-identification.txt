This study addresses the issue of misalignment in existing paradigms for Visible-Infrared Person Re-identification (VI Re-ID) due to the distribution gap between heterogeneous data. The authors propose a new framework called Concordant Attention Learning (CAL) that aims to learn semantic-aligned representations for VI Re-ID. CAL incorporates the Target-aware Concordant Alignment paradigm, which enables adaptive attention adjustment based on the target image being aligned. This is achieved by utilizing discriminative clues from the modality counterpart and employing effective modality-agnostic correspondence searching strategies. To ensure semantic concordance during cross-modal retrieval, the authors introduce MatchDistill, a method that matches attention patterns and learns their underlying semantic correlations through bipartite-graph-based similarity modeling and cross-modal knowledge exchange. Extensive experiments on VI Re-ID benchmark datasets validate the effectiveness and superiority of CAL.