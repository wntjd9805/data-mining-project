This abstract discusses Composed Image Retrieval (CIR) and proposes a new task called Zero-Shot CIR (ZS-CIR) that eliminates the need for labeled training datasets. The authors introduce a method called SEARLE, which utilizes CLIP token embedding space to map visual features of a reference image into a pseudo-word token and combines it with a relative caption. They also present a benchmarking dataset called CIRCO, which is the first dataset for CIR that includes multiple ground truths for each query. Experimental results demonstrate that SEARLE outperforms existing baselines on popular CIR datasets FashionIQ and CIRR, as well as on CIRCO. The dataset, code, and model are available for public use on GitHub.