We propose a framework called PRANC that allows for significant reduction in the size of deep models. PRANC achieves this by representing a deep model as a combination of randomly initialized and frozen models in the weight space. These randomly initialized models serve as the basis for the deep model, and during training, we search for local minima within the subspace spanned by these basis networks. By using a single scalar seed and learned linear mixture coefficients, the original deep model can be reconstructed. This approach addresses the challenge of efficiently storing and communicating deep models in various scenarios such as multi-agent learning, continual learners, federated systems, and edge computing.  In our study, we apply PRANC to condense image classification models and compress images by compacting their associated implicit neural networks. Our experimental results demonstrate that PRANC outperforms baseline methods by a large margin when compressing deep models by almost 100 times for image classification tasks. Additionally, we show that PRANC enables memory-efficient inference by generating layer-wise weights on the fly. The source code for PRANC is available at: https://github.com/UCDvision/PRANC.