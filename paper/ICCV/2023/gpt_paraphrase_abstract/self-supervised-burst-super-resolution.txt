We propose a new self-supervised training method for burst super-resolution that eliminates the need for carefully tuning synthetic data simulation pipelines. Unlike weakly-paired training strategies that require noisy smartphone burst photos paired with clean references from a DSLR camera, our approach is more scalable and avoids color mismatch. We introduce a self-supervised objective that uses a forward imaging model to recover high-resolution images from aliased high frequencies in the burst, without manual tuning of the model's parameters. Our strategy is robust to dynamic scene motion, allowing for training with in-the-wild data. Experimental results on real and synthetic data demonstrate that our self-supervised approach achieves comparable or better quality than fully-supervised baselines trained with synthetic data or weakly-paired ground-truth. The effectiveness of our strategy is demonstrated across four different burst super-resolution architectures.