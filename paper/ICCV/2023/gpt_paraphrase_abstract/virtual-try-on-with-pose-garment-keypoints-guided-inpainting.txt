Virtual try-on is a technology that allows consumers to virtually try on garments for online shopping without physically wearing them. Existing image-based virtual try-on methods often result in distorted garment shapes and loss of pattern details. In this study, we propose a method that uses human pose and garment keypoints to guide the inpainting process for virtual try-on. By predicting garment keypoints at the target pose and using them as guide information, we are able to accurately preserve the shapes and patterns of the garments. Our method also utilizes a semantic-conditioned inpainting scheme to generate the final try-on image. We conducted experiments on the VITON-HD dataset and compared our results with prior methods. The qualitative and quantitative results demonstrate that our method outperforms existing methods across different image resolutions. The code repository for our method is available at https://github.com/lizhi-ntu/KGI.