Model calibration often involves optimizing certain parameters, such as temperature, based on an objective function like negative log-likelihood. However, a crucial aspect that has been overlooked is the influence of the calibration set difficulty on the objective function. Specifically, the ratio of misclassified to correctly classified samples plays a significant role. If the difficulty level of a test set differs greatly from that of the calibration set, a phenomenon known as out-of-distribution (OOD) data occurs. In this case, the optimal calibration parameters for the two datasets would be different, resulting in a suboptimal calibrator on the OOD test set and reduced calibration performance.To address this issue, we propose a simple and effective method called adaptive calibrator ensemble (ACE) for calibrating OOD datasets, which typically have higher difficulty levels than the calibration set. ACE involves training two calibration functions: one for in-distribution data (low difficulty) and another for severely OOD data (high difficulty). To achieve desirable calibration on a new OOD dataset, ACE utilizes an adaptive weighting method that balances between these two extreme functions. When implemented, ACE generally enhances the performance of several state-of-the-art calibration schemes on various OOD benchmarks. Importantly, this improvement does not come at the expense of in-distribution calibration performance.For more information and access to the project, please visit our project website: https://github.com/insysgroup/Adaptive-Calibrators-Ensemble.git.