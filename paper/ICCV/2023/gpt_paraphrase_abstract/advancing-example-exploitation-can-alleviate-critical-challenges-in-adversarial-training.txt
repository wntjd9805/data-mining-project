Deep neural networks have made significant advancements in various tasks, but they are vulnerable to adversarial examples, which are created by adding perturbations to original data. Adversarial training (AT) has emerged as the most effective defense mechanism against adversarial examples. Recent studies have emphasized the importance of exploiting examples, where the learning intensity of the model is adjusted for specific examples to enhance AT approaches. However, these studies employ different and contradictory analysis methodologies, leading to potential confusion in future research. To address this issue, we present a comprehensive summary of representative strategies that focus on exploiting examples within a unified framework. Additionally, we investigate the role of examples in AT and identify that examples contributing primarily to accuracy or robustness are distinct. Building on this finding, we propose a novel idea for example exploitation that can further enhance the performance of advanced AT methods. This idea suggests that critical challenges in AT, such as the trade-off between accuracy and robustness, robust overfitting, and catastrophic overfitting, can be simultaneously alleviated from an example-exploitation perspective. The code for implementing this idea is available at https://github.com/geyao1995/advancing-example-exploitation-in-adversarial-training.