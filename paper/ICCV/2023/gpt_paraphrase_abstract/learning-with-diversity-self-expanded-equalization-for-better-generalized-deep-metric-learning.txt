Improving generalization ability is crucial in deep metric learning (DML). While current DML methods focus on enhancing model robustness against category shift to perform well on unseen categories, domain shift is also prevalent in real-world scenarios. Thus, effectively generalizing the DML model to both unseen categories and domains remains a challenging yet necessary problem. In this study, we propose a novel technique called self-expanded equalization (SEE) to address this issue. Our approach involves utilizing a "min-max" strategy along with a proxy-based loss to augment a diverse set of out-of-distribution samples, thereby expanding the range of the original training data. To fully leverage the implicit cross-domain relationships between the source and augmented samples, we introduce a domain-aware equalization module. This module regularizes the feature distribution in the metric space, inducing a domain-invariant distance metric. Extensive experiments conducted on two benchmarks and a large-scale multi-domain dataset demonstrate the superiority of our SEE method compared to existing DML techniques.