Transfer learning is important in computer vision tasks as it allows the use of pre-trained deep learning models. However, selecting the best pre-trained model for a specific task remains a challenge. Current methods for measuring transferability rely on statistical correlations between features and labels, but they do not consider the impact of representation dynamics during fine-tuning, making their results unreliable, particularly for self-supervised models. In this paper, we propose a physics-inspired approach called PED to overcome these challenges. We frame model selection as a potential energy problem and model the interaction forces that affect fine-tuning dynamics. By capturing the motion of dynamic representations to reduce potential energy within a force-driven physical model, we obtain a more accurate and stable measure of transferability. Experimental results on 10 tasks and 12 self-supervised models show that our approach can improve existing ranking techniques and reveal insights into the transfer learning mechanism. The code is available at https://github.com/lixiaotong97/PED.