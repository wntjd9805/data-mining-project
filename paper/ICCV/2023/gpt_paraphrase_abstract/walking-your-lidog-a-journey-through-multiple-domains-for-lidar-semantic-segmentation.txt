The ability to deploy safe robots in various environments is crucial for developing intelligent agents. While there has been significant progress in LiDAR semantic segmentation within specific domains, it remains unclear if these methods can generalize across different domains. To address this, we introduce a new experimental setup to study domain generalization for LiDAR semantic segmentation (DG-LSS). Our findings reveal a significant performance gap between models trained on the source dataset and those trained on the target domain. To bridge this gap, we propose a novel method specifically designed for DG-LSS. This method enhances a sparse-convolutional encoder-decoder 3D segmentation network with a dense 2D convolutional decoder that learns to classify a birds-eye view of the point cloud. By incorporating this auxiliary task, the 3D network can learn robust features that are transferable across domains and resilient to changes in sensor placement and resolution. Our method outperforms all baselines, achieving a higher mean Intersection over Union (mIoU) on the target domain. We hope that this work inspires the community to develop and evaluate future models in cross-domain conditions.