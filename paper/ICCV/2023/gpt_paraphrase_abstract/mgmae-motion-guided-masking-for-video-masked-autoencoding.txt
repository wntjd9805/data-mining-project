This paper introduces a motion guided masking strategy to improve the performance of video masked autoencoding (VideoMAE). The authors highlight that motion is a crucial aspect in videos and should be considered during masked pre-training. Their motion guided masking incorporates motion information to create a temporally consistent masking volume. This enables tracking of unmasked tokens over time and sampling of temporal consistent cubes from videos. By aligning these unmasked tokens, the authors address the issue of information leakage and promote learning of more useful structure information in the motion guided masked autoencoder (MGMAE). The MGMAE is implemented with an efficient optical flow estimator and a backward masking map warping strategy. Experimental results on Something-Something V2 and Kinetics-400 datasets demonstrate the superior performance of MGMAE compared to the original VideoMAE. The authors also provide visualization analysis to showcase the motion-adaptive sampling of temporal consistent cubes, which enhances the effectiveness of video pre-training.