Temporal sentence grounding (TSG) is the task of locating a specific moment in an untrimmed video based on a given natural language query. While fully supervised methods for TSG require laborious timestamp annotations, weakly supervised methods have been found to have a significant performance gap. In this study, we aim to reduce annotation cost while maintaining competitive performance compared to fully supervised methods. We explore a recently proposed approach called glance-supervised temporal sentence grounding, which only requires single frame annotation (referred to as glance annotation) for each query.  To achieve our goal, we propose a framework called Dynamic Gaussian prior based Grounding (D3G). D3G consists of two modules: Semantic Alignment Group Contrastive Learning (SA-GCL) and Dynamic Gaussian prior Adjustment (DGA). SA-GCL leverages Gaussian prior and semantic consistency to sample reliable positive moments from a 2D temporal map, aligning the positive sentence-moment pairs in the joint embedding space. DGA is designed to address annotation bias resulting from glance annotation and handle complex queries with multiple events. It dynamically adjusts the distribution to approximate the ground truth of target moments.  We conducted extensive experiments on three challenging benchmarks, and the results demonstrate the effectiveness of D3G. It outperforms existing weakly supervised methods by a large margin and significantly reduces the performance gap compared to fully supervised methods. The code for D3G is available at https://github.com/solicucu/D3G.