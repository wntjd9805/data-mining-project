Vision-language models like CLIP have improved the accuracy of open-vocabulary object detection, where the detector is trained on known categories but needs to identify new categories. Current methods utilize CLIP's zero-shot recognition ability to align object-level embeddings with text embeddings of categories. However, we have noticed that using CLIP for object-level alignment leads to overfitting to known categories, resulting in poor performance for similar new categories that are recognized as known categories. To address this issue, we have identified that the loss of crucial fine-grained local image details prevents existing methods from achieving strong generalization from known to new categories. Therefore, we propose a solution called Early Dense Alignment (EDA) that bridges the gap between generalizable local details and object-level predictions. In EDA, we use object-level supervision to learn dense-level alignment instead of object-level alignment, preserving the fine-grained local details. Through extensive experiments, we have demonstrated that our approach outperforms competing methods under the same strict conditions and without utilizing external training resources. Specifically, we have achieved an improvement of +8.4% in novel box AP50 on COCO and +3.9% in rare mask AP on LVIS.