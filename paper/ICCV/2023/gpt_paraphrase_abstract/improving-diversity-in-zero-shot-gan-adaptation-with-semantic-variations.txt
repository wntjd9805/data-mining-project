Training deep generative models typically requires a large amount of data. To address the high cost of data collection, zero-shot GAN adaptation aims to use well-trained generators to generate images of a new target domain without additional training samples. However, this approach often leads to a loss of image diversity, known as mode collapse, when only a single representative text feature is available instead of real images. To tackle this issue, we propose a novel method that identifies semantic variations of the target text in the CLIP space. By exploring diverse semantic variations while controlling the deviation of semantic information, we can address mode collapse. We introduce a directional moment loss that matches the image and text direction distributions, elastic weight consolidation, and a relation consistency loss to preserve valuable content information from the source domain. Through extensive experiments, we demonstrate the effectiveness of our proposed methods in ensuring sample diversity in various zero-shot GAN adaptation scenarios. We also conduct ablation studies to validate the impact of each proposed component. Notably, our model achieves state-of-the-art performance in terms of both diversity and quality in zero-shot GAN adaptation.