Visual navigation in household robots requires the ability to understand the semantics and spatial structure of the environment. Existing methods primarily rely on pre-training visual backbones with independent images or self-supervised learning, but they overlook the importance of spatial relationships in navigation. To address this, we propose a new method called Ego2-Map, which leverages the natural tendency of humans to build cognitive maps during navigation. Our approach contrasts the agent's egocentric views with semantic maps, using a visual transformer as the backbone encoder and training the model with data from Habitat-Matterport3D environments. By transferring information from the map to the agent's egocentric representations, Ego2-Map enhances navigation by incorporating objects, structure, and transitions. Experimental results demonstrate that our learned representations outperform existing pre-training methods in object-goal navigation. Additionally, our representations significantly improve vision-and-language navigation in continuous environments, achieving state-of-the-art results. Specifically, we achieve 47% success rate (SR) and 41% success weighted by path length (SPL) on the test server.