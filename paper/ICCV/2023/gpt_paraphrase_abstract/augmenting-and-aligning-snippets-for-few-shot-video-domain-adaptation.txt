To enhance the transferability and robustness of video models across different video tasks and environments, Video Unsupervised Domain Adaptation (VUDA) has been introduced. However, current VUDA methods rely on a large amount of high-quality unlabeled target data, which may not be available in real-world situations. Therefore, we propose a more realistic scenario called Few-Shot Video-based Domain Adaptation (FSVDA), where video models are adapted with only a few target video samples. While some methods have addressed Few-Shot Domain Adaptation (FSDA) in images and FSVDA, they primarily rely on spatial augmentation for expanding the target domain and statistical alignment at the instance level. However, videos contain valuable temporal and semantic information that should be fully considered during target domain augmentation and alignment in FSVDA. To address this, we introduce a novel approach called SSA2lign, which tackles FSVDA at the snippet level. In SSA2lign, the target domain is expanded through a simple snippet-level augmentation, followed by attentive alignment of snippets both semantically and statistically. The semantic alignment of snippets is conducted from multiple perspectives. Empirical results demonstrate that SSA2lign achieves state-of-the-art performance on various cross-domain action recognition benchmarks. The code for SSA2lign can be found at: https://github.com/xuyu0010/SSA2lign.