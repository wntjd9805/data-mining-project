The study addresses the issue of retrieving partially relevant untrimmed videos with query input, which is a more practical but challenging task compared to previous works that assume pre-trimmed videos. The authors propose a Dual Learning framework with Dynamic Knowledge Distillation (DL-DKD) to tackle this task. DL-DKD leverages the knowledge of a large vision-language model as the teacher to guide a student model. The knowledge distillation process involves an inheritance student branch that absorbs knowledge from the teacher model and an exploration student branch that incorporates task-specific information. A dynamic knowledge distillation strategy is implemented to adjust the learning effect of each student branch during training. Experimental results show that the proposed model achieves state-of-the-art performance on PRVR datasets.