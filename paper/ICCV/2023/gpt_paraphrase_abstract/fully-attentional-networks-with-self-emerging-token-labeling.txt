Recent research suggests that Vision Transformers (ViTs) have shown resilience in handling out-of-distribution scenarios. Notably, the Fully Attentional Network (FAN), a collection of ViT backbones, has achieved state-of-the-art robustness. This paper focuses on revisiting FAN models and enhancing their pre-training through a self-emerging token labeling (STL) framework. Our approach involves a two-stage training process. Initially, we train a FAN token labeler (FAN-TL) to generate meaningful patch token labels. Subsequently, we train a FAN student model using both the token labels and the original class label. By incorporating the proposed STL framework, our best model based on FAN-L-Hybrid, comprising 77.3M parameters, attains an 84.8% Top-1 accuracy and 42.1% mCE on ImageNet-1K and ImageNet-C datasets. Furthermore, it achieves new state-of-the-art results on ImageNet-A (46.1%) and ImageNet-R (56.6%), surpassing the original FAN counterpart significantly, without the need for additional data. Additionally, our framework exhibits substantial performance improvement in downstream tasks like semantic segmentation, with up to a 1.7% increase in robustness compared to the counterpart model.