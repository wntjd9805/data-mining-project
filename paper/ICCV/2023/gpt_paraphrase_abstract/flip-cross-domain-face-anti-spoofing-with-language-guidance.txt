Face anti-spoofing, also known as presentation attack detection, is a crucial aspect of face recognition systems used in security-sensitive applications. However, existing methods for face anti-spoofing have limited effectiveness when confronted with unseen types of spoofing, different camera sensors, and varying environmental conditions. Recently, vision transformer (ViT) models have shown promise in the field of face anti-spoofing due to their ability to capture long-range dependencies among image patches. Nonetheless, these models often require adaptive modules or auxiliary loss functions to adapt pre-trained ViT weights learned on large-scale datasets like ImageNet. In this study, we demonstrate that initializing ViTs with multimodal pre-trained weights, such as those from CLIP, enhances the generalizability of face anti-spoofing, aligning with the zero-shot transfer capabilities of vision-language pre-trained models. To address the challenge of robust cross-domain face anti-spoofing, we propose a novel approach that grounds visual representations using natural language. Specifically, we show that aligning the image representation with a collection of class descriptions based on natural language semantics improves the generalizability of face anti-spoofing, particularly in scenarios with limited data. Furthermore, we introduce a multimodal contrastive learning strategy to further enhance feature generalization and bridge the gap between source and target domains. Through extensive experiments conducted on three standard protocols, we demonstrate that our method surpasses state-of-the-art approaches, achieving superior zero-shot transfer performance compared to "adaptive ViTs" with five-shot transfer. The code for our method can be found at: https://github.com/koushiksrivats/FLIP.