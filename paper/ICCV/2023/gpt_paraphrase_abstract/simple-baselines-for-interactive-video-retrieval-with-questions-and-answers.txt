Up until now, video retrieval systems have primarily been designed for a "single-shot" scenario where users submit queries in isolation, disregarding any past interactions with the system. However, there has been a recent resurgence of interest in interactive systems that aim to enhance retrieval. Unfortunately, existing approaches to interactive video retrieval are complex and only provide limited improvements in performance. In this study, we address this issue by proposing several straightforward yet effective baselines for interactive video retrieval using a question-answering approach. To simulate user interactions, we utilize a VideoQA model, which allows us to study the interactive retrieval task even without access to ground truth dialogue data. Through experiments conducted on MSR-VTT, MSVD, and AVSD datasets, we demonstrate that our framework, employing question-based interaction, significantly enhances the performance of text-based video retrieval systems. Interested individuals can access the code for our framework at https://github.com/kevinliang888/IVR-QA-baselines.