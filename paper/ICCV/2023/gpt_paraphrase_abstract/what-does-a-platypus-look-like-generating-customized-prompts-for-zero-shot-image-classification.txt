Open-vocabulary models have emerged as a promising approach for image classification, allowing for classification among any set of categories specified with natural language prompts. Typically, these prompts consist of hand-written templates completed with category names. This study presents a simple method called CuPL (Customized Prompts via Language models) to generate more accurate prompts without relying on explicit knowledge of the task domain or extensive manual sentence construction. CuPL combines open-vocabulary models with large language models (LLMs) to generate descriptive sentences that highlight important distinguishing characteristics of image categories. By incorporating this knowledge into the classification process, the model assigns greater importance to these features when making predictions. The results show that this straightforward and general approach improves accuracy on various zero-shot image classification benchmarks, including a one percentage point gain on ImageNet. Importantly, this baseline method requires no additional training and remains completely zero-shot. The schematic of the method illustrates the steps involved, from LLM-generated captions to the use of these captions as prompts for classification. The output of the method is focused on abstraction, providing a concise answer format.