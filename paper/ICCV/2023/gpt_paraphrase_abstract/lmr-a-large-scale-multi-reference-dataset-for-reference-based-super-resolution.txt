Reference-based super-resolution (RefSR) is a widely accepted method that produces better results than single image super-resolution (SISR) by using similar high-quality images as references. While it is intuitive that more references would lead to improved performance, previous RefSR methods have focused only on training with a single reference image. This creates a mismatch between training and testing scenarios, as multiple reference images are often available in practical applications. This mismatch is due to the lack of publicly available multi-reference SR training datasets, which hampers research on multi-reference super-resolution. To address this issue, we have developed a large-scale dataset called LMR, which consists of 112,142 groups of 300x300 training images. This dataset is 10 times larger than the existing largest RefSR dataset and includes images of larger sizes. Each group in the LMR dataset is accompanied by 5 reference images with varying levels of similarity. Additionally, we propose a new baseline method for multi-reference super-resolution called MRefSR. This method incorporates a Multi-Reference Attention Module (MAM) for feature fusion of any number of reference images, and a Spatial Aware Filtering Module (SAFM) for selecting the fused features. Our proposed MRefSR approach achieves significant improvements over state-of-the-art methods in both quantitative and qualitative evaluations. The code and data for our approach are available at the following link: https://github.com/wdmwhh/MRefSR.