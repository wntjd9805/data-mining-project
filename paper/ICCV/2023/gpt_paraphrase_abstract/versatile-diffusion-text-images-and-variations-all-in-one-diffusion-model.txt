Recent advancements in diffusion models have made significant progress in various generation tasks. However, current approaches primarily focus on extending capabilities and improving performance rather than enhancing capacity, resulting in the need for separate models for different tasks. In this study, we introduce Versatile Diffusion (VD), a multi-task multimodal network that expands the existing single-flow diffusion pipeline. VD is capable of handling multiple flows, including text-to-image, image-to-text, and variations, within a single unified model. The design of VD incorporates a pipeline framework with shareable and swappable layer modules, enabling crossmodal generality beyond images and text. Through extensive experiments, we demonstrate that VD surpasses baseline approaches and achieves competitive quality in all of its base tasks. Additionally, VD enables novel extensions such as disentangling style and semantics, dual- and multi-context blending, and more. The success of our multi-flow multimodal framework over images and text may serve as inspiration for further universal AI research based on diffusion models. The code and models for VD are openly available at https://github.com/SHI-Labs/Versatile-Diffusion.