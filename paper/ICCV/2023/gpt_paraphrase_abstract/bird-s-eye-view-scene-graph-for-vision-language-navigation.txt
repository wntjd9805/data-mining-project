Vision-language navigation (VLN) has made significant progress in recent years, with agents being able to navigate 3D environments based on human instructions. However, current agents rely on panoramic observations, which limits their ability to perceive the 3D scene geometry and often leads to ambiguous choices in selecting the correct panoramic view. To overcome these limitations, we propose a novel approach called BEV Scene Graph (BSG). BSG utilizes multi-step Bird's Eye View (BEV) representations to encode the layout and geometric cues of indoor environments, guided by 3D detection. During navigation, BSG constructs a local BEV representation at each step and maintains a global scene map based on BEV, organizing all collected local representations according to their topological relationships. Leveraging BSG, the agent predicts both a local BEV grid-level decision score and a global graph-level decision score, along with a sub-view selection score on panoramic views, to improve the accuracy of action prediction. Our experimental results demonstrate that our approach outperforms state-of-the-art methods on three benchmark datasets (REVERIE, R2R, and R4R), highlighting the potential of BEV perception in VLN.