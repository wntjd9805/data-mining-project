The focus of understanding egocentric videos is on modeling interactions between the hand and objects. While models that use RGB frames as input perform well, their performance can be further enhanced by incorporating additional input modalities such as object detections, optical flow, and audio. However, the inclusion of modality-specific modules adds complexity that hinders practical deployment. This study aims to maintain the performance of a multi-modal approach while using only RGB frames as input during inference. The results show that models trained by multi-modal teachers achieve higher accuracy and better calibration compared to models trained with ground truth labels in a uni- or multi-modal manner. To address challenges in applying multi-modal knowledge distillation, a principled framework is adopted. Additionally, the study demonstrates a reduction in computational complexity while maintaining high performance with fewer input views. The code for this work is available at: https://github.com/gorjanradevski/multimodal-distillation.