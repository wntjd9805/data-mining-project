Current approaches combining Neural Architecture Search (NAS) and quantization have been successful in automatically designing low-FLOPs INT8 quantized neural networks (QNN). However, directly applying NAS to design accurate QNN models that achieve low latency on real-world devices leads to subpar performance. The main reason for the poor INT8 latency is the issue of unfriendly quantization, where the choices of operators and configurations in existing search spaces result in varying quantization efficiency and slower INT8 inference speed. To address this challenge, we propose SpaceEvo, an automatic method that designs a dedicated, quantization-friendly search space for each target hardware. SpaceEvo leverages the idea of automatically searching for hardware-preferred operators and configurations to construct the search space, guided by a metric called Q-T score that measures the quantization-friendliness of a candidate search space. Additionally, we train a quantized-for-all supernet using our discovered search space, allowing the searched models to be deployed without the need for extra retraining or quantization. Our discovered models, SEQnet, achieve state-of-the-art INT8 quantized accuracy on ImageNet under various latency constraints, surpassing prior art CNNs by up to 10.1% accuracy improvement. Extensive experiments on real devices demonstrate that SpaceEvo consistently outperforms manually-designed search spaces, achieving up to 2.5 times faster speed while maintaining the same level of accuracy.