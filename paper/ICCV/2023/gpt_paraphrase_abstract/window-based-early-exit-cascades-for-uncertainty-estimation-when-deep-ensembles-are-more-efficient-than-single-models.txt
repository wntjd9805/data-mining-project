Deep Ensembles are a widely criticized method in deep learning due to their computational expense. However, recent research challenges this notion by demonstrating that ensembles can be more computationally efficient than scaling single models within the same architecture family. This efficiency is achieved by using an early-exit approach to cascade ensemble members. In this study, we explore the application of this efficiency to tasks related to uncertainty estimation. By focusing on binary classification tasks, we propose a novel approach of passing only samples close to the binary decision boundary to later cascade stages. Our experiments on large-scale datasets and various network architectures show that this window-based early-exit approach achieves a better trade-off between uncertainty computation and performance compared to scaling single models. For instance, a cascaded EfficientNet-B2 ensemble achieves similar coverage at 5% risk as a single EfficientNet-B4 with less than 30% of the computational cost. We also observe that cascades/ensembles provide more reliable improvements on out-of-distribution data compared to scaling models up. The code for this study is available at the following GitHub repository: [link].