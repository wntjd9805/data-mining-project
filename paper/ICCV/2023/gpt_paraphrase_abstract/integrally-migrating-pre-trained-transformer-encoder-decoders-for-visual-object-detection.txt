We propose a method called integrally migrating pre-trained transformer encoder-decoders (imTED) to improve the generalization capacity of object detectors. While current object detectors benefit from pre-trained backbone networks, other components such as the detector head and the feature pyramid network (FPN) are trained from scratch, limiting their generalization. In imTED, we fully pre-train the feature extraction path by migrating pre-trained transformer decoders to the detector head and removing the randomly initialized FPN. We also introduce a multi-scale feature modulator (MFM) to enhance scale adaptability. These design choices reduce randomly initialized parameters and unify detector training with representation learning. Experiments on the MS COCO object detection dataset demonstrate that imTED consistently outperforms other detectors by approximately 2.4 average precision (AP). Additionally, without any additional enhancements, imTED achieves a state-of-the-art improvement of up to 7.6 AP in few-shot object detection. The code for imTED is available at https://github.com/LiewFeng/imTED.