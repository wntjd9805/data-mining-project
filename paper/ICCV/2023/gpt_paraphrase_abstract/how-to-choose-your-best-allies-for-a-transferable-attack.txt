The transferability of adversarial examples is a critical concern in ensuring the security of deep neural networks. Adversarial examples that can deceive one model and fool another model pose a significant threat. While the Attack Success Rate is commonly used to evaluate transferability, it alone does not provide a comprehensive assessment. This paper introduces a novel methodology that focuses on the level of distortion to evaluate transferability. This approach reveals that randomly selecting the source model for a transferable attack can result in poorer performance compared to a black box attack. To address this issue, we propose a selection mechanism called FiT, which efficiently identifies the optimal source model with minimal preliminary queries to the target. Our experimental results demonstrate that FiT is highly effective in selecting the best source model for various scenarios, including single-model attacks, ensemble-model attacks, and multiple attacks.