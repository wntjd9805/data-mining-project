Generating realistic images with control over camera pose and scene contents is important for applications like AR/VR and simulation. While progress has been made in 3D-aware generative models, most methods focus on object-centric images and cannot generate urban scenes with free camera viewpoint control and scene editing. To address this challenge, we propose UrbanGIRAFFE. Our approach uses a coarse 3D panoptic prior that includes the distribution of layout for stuff and countable objects to guide a 3D-aware generative model. Our model is compositional and controllable as it breaks down the scene into stuff, objects, and sky. We utilize a conditioned stuff generator that incorporates coarse semantic and geometry information using semantic voxel grids. The object layout prior allows us to learn an object generator from cluttered scenes. With appropriate loss functions, our approach enables photorealistic 3D-aware image synthesis with diverse controllability, including large camera movement, stuff editing, and object manipulation. We validate the effectiveness of our model on synthetic and real-world datasets, including the challenging KITTI-360 dataset.