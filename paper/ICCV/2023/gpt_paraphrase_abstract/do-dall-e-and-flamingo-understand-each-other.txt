The field of multimodal research, which focuses on understanding and creating both images and text, has made significant progress. This progress can be seen in the development of advanced models like Flamingo and DALL-E, which excel at image captioning and text-to-image generation respectively. An interesting question in this field is whether these models can understand each other. To investigate this, we propose a task where Flamingo generates a description for an image, and DALL-E uses that description to synthesize a new image. We argue that if the generated image is similar to the original image, it indicates that these models understand each other. We specifically examine the relationship between the quality of image reconstruction and the quality of text generation. Our findings show that an optimal description is one that produces a generated image similar to the original. Based on these findings, we propose a unified framework to fine-tune the text-to-image and image-to-text models. The reconstruction component serves as a regularization loss to guide the tuning of the models. We conduct extensive experiments on various datasets using different image captioning and image generation models, and our results validate our findings and demonstrate the effectiveness of our proposed framework. Due to the unavailability of DALL-E and Flamingo, we utilize Stable Diffusion and BLIP for the rest of our work. For more information, please visit our project website: https://dalleflamingo.github.io.