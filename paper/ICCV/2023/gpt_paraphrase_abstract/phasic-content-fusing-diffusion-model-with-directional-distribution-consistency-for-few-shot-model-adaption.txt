Training a generative model with a limited number of samples is difficult. Current methods rely on few-shot model adaption, but when there is very little data (less than 10 samples), the generative network tends to overfit and lose content quality. To address this, we propose a new approach called phasic content fusing few-shot diffusion model with a directional distribution consistency loss. This approach targets different learning objectives at different stages of the diffusion model training. We use a phasic training strategy with content fusion to help the model learn content and style information when the number of samples (t) is large, and learn local details of the target domain when t is small. This improves the model's ability to capture content, style, and local details. Additionally, we introduce a new directional distribution consistency loss that ensures more efficient and stable consistency between the generated and source distributions compared to previous methods, preventing overfitting. We also propose a cross-domain structure guidance strategy to enhance structure consistency during domain adaptation. Theoretical analysis, qualitative, and quantitative experiments demonstrate that our approach outperforms state-of-the-art methods in few-shot generative model adaption tasks. The source code for our approach is available at the given URL.