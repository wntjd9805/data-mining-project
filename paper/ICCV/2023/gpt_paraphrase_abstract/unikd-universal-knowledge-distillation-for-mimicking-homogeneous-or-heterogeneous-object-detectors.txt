Knowledge distillation (KD) is a widely used technique to enhance the performance of lightweight object detectors. However, existing methods mainly focus on feature-based distillation, where the student detector mimics the features of a homogeneous teacher detector. This approach fails when trying to distill knowledge from a heterogeneous teacher due to a significant semantic gap, limiting the practical applications of KD. Bridging this gap currently requires time-consuming and experience-dependent algorithm design.To address this issue, we propose Universal Knowledge Distillation (UniKD) that introduces Adaptive Knowledge Extractors (AKEs) with deformable cross-attention as additional decoder heads. In UniKD, the AKEs are pretrained on the output of the teacher detector, infusing its content and positional knowledge into a fixed set of knowledge embeddings. These fixed AKEs are then attached to the student detector's backbone to encourage the absorption of the teacher's knowledge.In this query-based distillation approach, detection-relevant information can be dynamically aggregated into a set of knowledge embeddings and transferred between different detectors. If the teacher model is too large for online inference, its output can be pre-stored on disk to save computation overhead, which is more storage-efficient than feature-based methods. Extensive experiments demonstrate that UniKD can be applied to any homogeneous or heterogeneous teacher-student pairs and outperforms conventional feature-based KD.In summary, our proposed UniKD approach overcomes the limitations of previous feature-based KD methods by introducing AKEs and enabling the transfer of knowledge between different detectors. It is a versatile and effective method that can be applied to various teacher-student pairs, yielding significant performance improvements.