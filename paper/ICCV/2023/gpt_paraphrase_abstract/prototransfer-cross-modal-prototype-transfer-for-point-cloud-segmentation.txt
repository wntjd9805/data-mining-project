Knowledge transfer from multi-modal data, such as LiDAR points and images, to a single LiDAR modality is a promising approach for point cloud semantic segmentation in autonomous driving. However, existing methods primarily focus on distilling knowledge from aligned point-pixel fusion features, neglecting a large number of unmatched image pixels and underutilizing unmatched LiDAR points. This paper proposes a novel approach called ProtoTransfer, which fully leverages image representations and transfers the learned multi-modal knowledge to all point cloud features. The approach involves constructing a class-wise prototype bank from strictly-aligned fusion features and encouraging all point cloud features to learn from these prototypes during model training. Additionally, a pseudo-labeling scheme is employed to exploit unmatched point and pixel features. These features are accumulated into the class-wise prototype bank using a carefully designed fusion strategy. The proposed approach achieves superior performance compared to state-of-the-art methods on two large-scale benchmarks, nuScenes and SemanticKITTI, and ranks 2nd on the nuScenes Lidarseg challenge leaderboard.