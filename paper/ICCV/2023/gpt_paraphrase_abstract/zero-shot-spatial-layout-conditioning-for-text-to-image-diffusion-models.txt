Large-scale models that convert text into images have greatly advanced generative image modeling and offer a user-friendly interface for image generation. However, expressing spatial constraints, such as object placement, through text is challenging, and existing text-based image generation models struggle to accurately follow such instructions. This paper introduces ZestGuide, a novel approach that enables precise spatial control over generated content by associating text with specific segments on the image canvas. Unlike other methods, ZestGuide does not require additional training and can be seamlessly integrated into pre-trained text-to-image diffusion models. It utilizes implicit segmentation maps extracted from cross-attention layers to align the generated content with input masks. Experimental results demonstrate that ZestGuide produces high-quality images that accurately align with input segmentations, outperforming previous methods both quantitatively and qualitatively. Even when compared to the previous state-of-the-art model, Paint with Words, which also employs zero-shot segmentation conditioning, ZestGuide achieves a significant improvement of 5 to 10 mIoU points on the COCO dataset while maintaining similar FID scores.