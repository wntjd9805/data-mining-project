In recent years, Transformer networks have started replacing traditional convolutional neural networks (CNNs) in computer vision due to their ability to capture global information and adapt to different inputs. However, the computational complexity of softmax-attention, which is used in Transformers, limits their application in image dehazing tasks, especially for high-resolution images. To address this issue, we propose a new variant of the Transformer called MB-TaylorFormer. This variant uses Taylor expansion to approximate the softmax-attention, resulting in linear computational complexity. Additionally, we introduce a multi-scale attention refinement module to correct any errors caused by the Taylor expansion. Furthermore, our proposed Transformer utilizes a multi-branch architecture with multi-scale patch embedding. This embedding method incorporates features from different scales using overlapping deformable convolutions. The multi-scale patch embedding is designed based on three key ideas: varying receptive field sizes, multi-level semantic information, and flexible receptive field shapes. The MB-TaylorFormer model effectively embeds coarse to fine features during the patch embedding stage and captures long-distance pixel interactions without incurring excessive computational costs. Experimental results on various dehazing benchmarks demonstrate that MB-TaylorFormer achieves state-of-the-art performance while maintaining low computational burden. The source code and pre-trained models for MB-TaylorFormer are available at https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer.