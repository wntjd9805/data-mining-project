Personalized federated learning (PFL) aims to address the issue of non-independent and identically distributed (non-IID) data among clients by allowing each client to train a personalized model while collaborating with others. The traditional approach in PFL is to localize sensitive parameters, such as classifier layers, to prevent the negative impact of non-IID data. However, we argue that this approach is too conservative and propose a new guideline for client collaboration in PFL.  Our guideline considers not only the sensitivity to non-IID data but also the similarity of data distribution when deciding which parameters should be localized. We believe that even if certain parameters are easily influenced by non-IID data, they can still be beneficial if shared with clients that have similar data distribution. By allowing clients to share more parameters with others, our approach leads to improved model performance.  In addition to the guideline, we introduce a new PFL method called FedCAC. This method utilizes a quantitative metric to evaluate the sensitivity of each parameter to non-IID data and carefully selects collaborators based on this evaluation. Experimental results demonstrate that FedCAC enables clients to share more parameters with others, resulting in superior performance compared to state-of-the-art methods, especially in scenarios with diverse data distributions.  To facilitate the implementation of our approach, we have integrated the code into our FL training framework, which can be accessed at https://github.com/kxzxvbk/Fling.