Traditional methods for learning with noisy labels have been successful in dealing with datasets that have artificially injected noise. However, these methods are not effective in handling real-world noise. Meta-learning has been increasingly used in various fields of machine learning, where researchers have used auxiliary clean datasets to correct the training labels. However, existing approaches for meta-label correction are not fully utilizing their potential. In this study, we propose an Enhanced Meta Label Correction (EMLC) approach for the learning with noisy labels (LNL) problem. We reevaluate the meta-learning process and introduce faster and more accurate meta-gradient derivations. We also propose a new teacher architecture specifically designed for the LNL problem, with novel training objectives. EMLC surpasses previous approaches and achieves state-of-the-art results in standard benchmarks. Particularly, EMLC improves the performance on the real-world dataset Clothing1M by 1.52%, while requiring only half the time per epoch compared to the baseline approach. Additionally, EMLC demonstrates faster convergence of the meta-objective.