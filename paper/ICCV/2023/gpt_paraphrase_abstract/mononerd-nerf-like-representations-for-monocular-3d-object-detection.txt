In the realm of monocular 3D detection, it is a common practice to utilize scene geometry to improve the performance of detectors. However, existing methods often explicitly incorporate these geometric clues by estimating a depth map and projecting it into 3D space. This explicit approach results in sparse 3D representations due to the increased dimensionality, leading to significant loss of information, particularly for distant and occluded objects. To address this issue, we propose MonoNeRD, a novel detection framework that can infer dense 3D geometry and occupancy. Our method models scenes using Signed Distance Functions (SDF), which enables the creation of dense 3D representations. We treat these representations as Neural Radiance Fields (NeRF) and utilize volume rendering to recover RGB images and depth maps. This work is the first to introduce volume rendering for monocular 3D detection, showcasing the potential of implicit reconstruction for image-based 3D perception. Extensive experiments conducted on the KITTI-3D benchmark and Waymo Open Dataset validate the effectiveness of MonoNeRD. The codes for implementing MonoNeRD are publicly available at https://github.com/cskkxjk/MonoNeRD.