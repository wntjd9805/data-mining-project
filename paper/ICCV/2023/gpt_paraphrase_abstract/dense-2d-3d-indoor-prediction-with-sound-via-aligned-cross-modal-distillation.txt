Sound plays an important role in our spatial reasoning in everyday life. To enable deep networks to have the same ability, we tackle the challenge of predicting indoor environments using sound in both 2D and 3D through cross-modal knowledge distillation. Our proposed framework, called Spatial Alignment via Matching (SAM), establishes local connections between visual and audio modalities to transfer knowledge. SAM combines audio features with visually coherent spatial embeddings to address inconsistencies in different layers of a student model. Our approach is not dependent on a specific input representation, allowing for flexibility in input shapes and dimensions without sacrificing performance. We introduce a new benchmark called Dense Auditory Prediction of Surroundings (DAPS), and we are the first to address dense indoor prediction of omnidirectional environments in both 2D and 3D using audio observations. Our distillation framework consistently achieves state-of-the-art performance in tasks such as audio-based depth estimation, semantic segmentation, and challenging 3D scene reconstruction, using various metrics and backbone architectures.