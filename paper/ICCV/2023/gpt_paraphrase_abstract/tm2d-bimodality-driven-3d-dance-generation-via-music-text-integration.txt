We present a new method for generating 3D dance movements that combine text and music modalities. Unlike previous approaches that only use music as a guide, our objective is to create more diverse dance movements by incorporating the instructive information from the text. However, the lack of paired motion data with both music and text poses a challenge for integrating the two modalities. To overcome this, we propose using a 3D human motion VQ-VAE to map the motion data from both datasets into a latent space of quantized vectors. This effectively combines the motion tokens from the two datasets with different distributions for training. Additionally, we introduce a cross-modal transformer to incorporate text instructions into the motion generation architecture, while still maintaining the performance of music-conditioned dance generation. To evaluate the quality of the generated motion, we introduce two new metrics: Motion Prediction Distance (MPD) and Freezing Score (FS), which measure the coherence and freezing percentage of the motion. Extensive experiments demonstrate that our approach can generate realistic and coherent dance movements based on both text and music, while achieving comparable performance to using each modality separately. The code for our method is available at the provided URL.