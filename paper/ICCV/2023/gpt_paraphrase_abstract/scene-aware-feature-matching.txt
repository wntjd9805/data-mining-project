Current methods for matching features in images focus on matching individual points, but they lack a deeper understanding of the overall scene. This leads to a significant decrease in performance when dealing with challenging scenes that have large changes in viewpoint and lighting. In order to address this issue, we propose a new model called SAM, which uses attentional grouping to guide the matching of scene-aware features. SAM handles features at multiple levels, including image tokens and group tokens, using attention layers. It also groups the image tokens using a token grouping module. Our model can be trained using only ground-truth matches and produces reliable grouping results. With the guidance of scene-aware grouping, SAM is not only more accurate and robust, but also more interpretable than traditional feature matching models. We conducted extensive experiments on various applications, including homography estimation, pose estimation, and image matching, which demonstrate that our model achieves state-of-the-art performance.