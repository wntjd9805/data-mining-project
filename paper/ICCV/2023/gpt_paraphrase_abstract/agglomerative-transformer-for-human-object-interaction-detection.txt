We propose AGER, an agglomerative Transformer that allows Transformer-based human-object interaction (HOI) detectors to effectively utilize additional instance-level cues in a single-stage and end-to-end approach. AGER achieves this by clustering patch tokens and aligning cluster centers to instances using textual guidance. This approach offers two main advantages. Firstly, it enhances the extraction of various instance-level cues, resulting in improved HOI detection performance with a state-of-the-art mean average precision (mAP) of 36.75 on the HICO-Det dataset. Secondly, the dynamical clustering mechanism of AGER enables the generation of instance tokens during the feature learning process of the Transformer encoder, eliminating the need for an additional object detector or instance decoder. This allows for the extraction of desired extra cues in an efficient manner, reducing GFLOPs by 8.5% and increasing FPS by 36% compared to a vanilla DETR-like pipeline without extra cue extraction. The code for AGER can be found at https://github.com/six6607/AGER.git.