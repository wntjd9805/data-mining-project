This paper focuses on the challenge of tracking objects using both RGB videos and event data. Instead of using a complex fusion network, the authors explore the potential of a pre-trained vision Transformer (ViT). They propose a training augmentation strategy that involves randomly masking certain tokens to encourage interaction between different modalities. To address network oscillations caused by the masking strategy, they introduce an orthogonal high-rank loss to regularize the attention matrix. Experimental results show that their training techniques significantly improve the performance of state-of-the-art trackers in terms of tracking precision and success rate. The authors believe that their approach provides valuable insights into leveraging pre-trained ViTs for modeling cross-modal data. The code for their method is publicly available at https://github.com/ZHU-Zhiyu/High-Rank_RGB-Event_Tracker.