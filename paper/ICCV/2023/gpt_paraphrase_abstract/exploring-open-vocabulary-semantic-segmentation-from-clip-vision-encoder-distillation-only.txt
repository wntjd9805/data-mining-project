Semantic segmentation is an important task in computer vision that involves dividing images into meaningful regions at the pixel level. However, current methods rely on costly human annotations for training, which limits their scalability to large, unlabeled datasets. To overcome this challenge, we propose a new method called ZeroSeg. ZeroSeg utilizes a pretrained vision-language model, such as CLIP vision encoder, to train semantic segmentation models without the need for human annotations. Although vision-language models have extensive knowledge of visual concepts, it is difficult to apply this knowledge to semantic segmentation because they are typically trained at the image level. ZeroSeg addresses this issue by distilling the visual concepts learned by the vision-language model into segment tokens, which represent localized regions in the target image. We evaluate ZeroSeg on popular segmentation benchmarks, including PASCAL VOC 2012, PASCALContext, and COCO, using a zero-shot approach. Our method achieves state-of-the-art performance compared to other zero-shot segmentation methods, while also performing competitively against strongly supervised methods. Additionally, we demonstrate the effectiveness of ZeroSeg in open-vocabulary segmentation through human studies and qualitative visualizations. The code for ZeroSeg is publicly available at https://github.com/facebookresearch/ZeroSeg.