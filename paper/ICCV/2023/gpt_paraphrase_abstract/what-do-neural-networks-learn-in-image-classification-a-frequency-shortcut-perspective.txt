This study explores the mechanisms of representation learning in neural networks (NNs) through frequency analysis. While most research focuses on regression tasks, this study investigates the learning dynamics of NNs for classification tasks. Synthetic datasets with biased frequency bands are used to experimentally demonstrate that NNs tend to find simple solutions for classification, with the initial learning depending on the distinctive frequency characteristics, whether they are low- or high-frequencies. This phenomenon is further confirmed using natural images. A metric is proposed to measure class-wise frequency characteristics, and a method is introduced to identify frequency shortcuts, which can be either texture-based or shape-based depending on what simplifies the objective. The transferability of frequency shortcuts is validated on out-of-distribution (OOD) test sets, showing that they can be transferred across datasets and are not fully avoidable by larger model capacity and data augmentation. The study recommends future research to focus on effective training schemes that mitigate frequency shortcut learning. The codes and data used in this study are available at https://github.com/nis-research/nn-frequency-shortcuts.