Weakly supervised object localization (WSOL) is a difficult task in which object localization models are learned from image category labels. Existing methods that train activation models in a discriminative manner often overlook less discriminative but still important object parts. In this research, we introduce a novel approach called the generative prompt model (GenPromp) that addresses this issue. GenPromp is the first generative pipeline that localizes less discriminative object parts by treating WSOL as a conditional image denoising procedure. During training, GenPromp converts image category labels into learnable prompt embeddings. These embeddings are then used to conditionally recover the input image with noise and learn representative embeddings. During inference, GenPromp combines the representative embeddings with discriminative embeddings obtained from a pre-trained vision-language model. This combination enhances both the representative and discriminative capacity of the embeddings. The combined embeddings are used to generate multi-scale attention maps, which assist in localizing the full extent of the object. Experimental results on CUB-200-2011 and ILSVRC datasets demonstrate that GenPromp outperforms the best discriminative models by 5.2% and 5.6% in terms of Top-1 localization accuracy. This establishes a strong baseline for WSOL using a generative approach. The code for GenPromp is publicly available at https://github.com/callsys/GenPromp.