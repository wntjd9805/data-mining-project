Current methods for 3D lane detection from monocular images in autonomous driving rely on building 3D surrogates from front-view image features and camera parameters. However, the depth ambiguity in monocular images leads to misalignment between the surrogate feature map and the original image, making accurate lane detection difficult. To address this issue, we propose a new approach called LATR, which is an end-to-end 3D lane detector that utilizes 3D-aware front-view features without transformed view representation. LATR detects 3D lanes using cross-attention based on query and key-value pairs, constructed using our lane-aware query generator and dynamic 3D ground positional embedding. Each query is generated based on 2D lane-aware features and incorporates a hybrid embedding to enhance lane information. Additionally, 3D space information is injected as positional embedding from an iteratively-updated 3D ground plane. LATR outperforms previous state-of-the-art methods on both synthetic Apollo and realistic OpenLane and ONCE-3DLanes datasets, achieving significant improvements in terms of F1 score on OpenLane. The code for LATR will be made available at https://github.com/JMoonr/LATR.