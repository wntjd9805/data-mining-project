This paper focuses on improving the pre-training of medical visual-language models by incorporating domain-specific knowledge from paired image-text reports in radiology. The paper presents several contributions: Firstly, instead of processing the raw reports directly, a novel triplet extraction module is used to extract medical-related information, simplifying language complexity and improving supervision signals. Secondly, a novel triplet encoding module is proposed, which utilizes a knowledge base to query entity translation and leverage domain knowledge in the medical field, establishing relationships between medical entities in the language embedding space. Thirdly, a Transformer-based fusion model is introduced to spatially align entity descriptions with visual signals at the image patch level, enabling medical diagnosis. Lastly, comprehensive experiments are conducted to validate the effectiveness of the proposed architecture, and the model is benchmarked on various public datasets including ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax, COVIDx CXR-2, COVID Rural, and Edema Severity. The model outperforms previous methods in disease classification and grounding tasks, both in zero-shot and fine-tuning settings.