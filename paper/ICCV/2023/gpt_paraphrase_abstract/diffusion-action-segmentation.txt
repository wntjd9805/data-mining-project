Temporal action segmentation in long-form videos is a crucial task that has been traditionally approached using iterative refinement models. In this study, we propose a new framework based on denoising diffusion models that follows the same iterative refinement paradigm. Our framework generates action predictions iteratively by using random noise and video features as conditions. To improve the modeling of key characteristics of human actions, such as position prior, boundary ambiguity, and relational dependency, we introduce a unified masking strategy for the conditioning inputs. We conduct extensive experiments on three benchmark datasets (GTEA, 50Salads, and Breakfast) and compare our method with state-of-the-art approaches. The results demonstrate that our generative approach achieves superior or comparable performance, highlighting the effectiveness of our framework for action segmentation. The code for our method is available at tinyurl.com/DiffAct.