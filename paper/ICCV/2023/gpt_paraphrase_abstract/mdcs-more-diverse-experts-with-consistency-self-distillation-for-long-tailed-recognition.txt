Recently, there have been significant advancements in long-tail recognition (LTR) through the use of multi-expert methods. However, there are two areas that require further improvement: the inclusion of more diverse experts and the reduction of model variance. Previous methods have not effectively addressed these issues. In order to bridge this gap, we propose a new approach called More Diverse experts with Consistency Self-distillation (MDCS). MDCS consists of two main components: Diversity Loss (DL) and Consistency Self-distillation (CS). DL promotes diversity among experts by ensuring they focus on different categories. To reduce model variance, we use KL divergence to distill richer knowledge from weakly augmented instances for self-distillation of the experts. We also introduce Confident Instance Sampling (CIS) to select correctly classified instances for CS, avoiding biased or noisy knowledge. Our analysis and ablation study demonstrate that our method effectively increases expert diversity, reduces model variance, and improves recognition accuracy compared to previous approaches. Furthermore, DL and CS are mutually reinforcing and dependent on each other: expert diversity benefits from CS, and CS relies on DL for significant results. Experimental results show that MDCS outperforms the state-of-the-art by 1% to 2% on popular long-tailed benchmarks such as CIFAR10-LT, CIFAR100-LT, ImageNet-LT, Places-LT, and iNaturalist 2018. The code for our method is available at https://github.com/fistyee/MDCS.