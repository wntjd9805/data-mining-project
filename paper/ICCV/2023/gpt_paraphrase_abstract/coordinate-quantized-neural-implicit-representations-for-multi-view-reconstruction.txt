Significant progress has been made in recent years in the field of learning neural implicit representations from multi-view images for 3D reconstruction. The use of sinusoidal functions as positional encodings alongside coordinates has been instrumental in capturing high frequency details with coordinate-based neural networks. However, the inclusion of high frequency positional encodings has resulted in unstable optimization, leading to noisy reconstructions and artifacts in empty spaces. To address this issue, we propose a method to learn neural implicit representations with quantized coordinates, which reduces uncertainty and ambiguity during optimization. Instead of using continuous coordinates, we discretize them into discrete coordinates by employing nearest interpolation among quantized coordinates. These quantized coordinates are obtained by discretizing the field at an extremely high resolution. We utilize these discrete coordinates and their positional encodings to learn implicit functions through volume rendering. This approach significantly reduces variations in the sample space and increases the number of multi-view consistency constraints on intersections of rays from different views. As a result, the inference of implicit functions becomes more effective. Importantly, the use of quantized coordinates does not introduce any additional computational burden and is compatible with the latest methods in the field. Our experiments using widely accepted benchmarks demonstrate the superiority of our approach over the state-of-the-art techniques. The code for our method is publicly available at https://github.com/MachinePerceptionLab/CQ-NIR.