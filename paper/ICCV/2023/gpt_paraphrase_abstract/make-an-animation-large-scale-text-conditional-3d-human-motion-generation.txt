Text-guided human motion generation has received significant attention due to its wide-ranging applications in animation and robotics. The use of diffusion models has improved the quality of generated motions. However, current approaches are limited by their reliance on small-scale motion capture data, resulting in subpar performance. This paper introduces Make-An-Animation, a model that leverages large-scale image-text datasets to learn diverse poses and prompts, leading to substantial performance enhancements compared to previous works. Make-An-Animation is trained in two stages: first, on a curated dataset of (text, static pseudo-pose) pairs extracted from image-text datasets, and second, on motion capture data with additional layers to model temporal aspects. Unlike previous diffusion models, Make-An-Animation employs a U-Net architecture, similar to recent text-to-video generation models. Our model achieves state-of-the-art performance in text-to-motion generation, as demonstrated by human evaluation of motion realism and alignment with input text. Generated samples can be accessed at https://azadis.github.io/make-an-animation.