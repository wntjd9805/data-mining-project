We present a Diffusion-augmented Generative model (DG3D) for generating high-quality 3D textured meshes at low cost. Many virtual reality applications require large amounts of 3D content, necessitating efficient and affordable modeling tools. However, directly generating textured meshes poses challenges due to the instability and incompleteness of a hybrid framework that involves converting between 2D features and 3D space. To address these difficulties, DG3D incorporates a diffusion-based augmentation module into the min-max game between the 3D tetrahedral mesh generator and 2D renderings discriminators. This integration stabilizes network optimization and prevents mode collapse in vanilla GANs. We also propose using multi-modal renderings in discrimination to enhance the aesthetics and completeness of the generated textures. Through extensive experiments on public benchmarks and real scans, we demonstrate that DG3D significantly outperforms existing state-of-the-art methods, achieving improvements of 5% to 40% in FID-3D score and 5% to 10% in geometry-related metrics. The code for DG3D is publicly available at https://github.com/seakforzq/DG3D.