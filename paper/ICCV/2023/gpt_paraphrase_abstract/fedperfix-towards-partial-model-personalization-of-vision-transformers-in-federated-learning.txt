Personalized Federated Learning (PFL) has emerged as a promising solution for decentralized learning in heterogeneous data environments. To enhance the efficiency of PFL, researchers have proposed partial model personalization, which selectively updates local model parameters rather than aggregating all of them. However, previous studies on partial model personalization have primarily focused on Convolutional Neural Networks (CNNs), leaving a gap in understanding its application to other popular models like Vision Transformers (ViTs). This research aims to address this gap by investigating the potential for partial model personalization in ViT models. The study empirically evaluates the sensitivity of each type of layer to data distribution. The findings indicate that the self-attention layer and the classification head are the most sensitive parts of a ViT. Building on these insights, the researchers propose a novel approach called FedPerfix. This approach utilizes plugins to transfer information from the aggregated model to the local client as personalization. The proposed approach is evaluated on CIFAR-100, OrganAM-NIST, and Office-Home datasets. The results demonstrate the effectiveness of FedPerfix in improving the model's performance compared to several advanced PFL methods. The code for FedPerfix is publicly available at https://github.com/imguangyu/FedPerfix.