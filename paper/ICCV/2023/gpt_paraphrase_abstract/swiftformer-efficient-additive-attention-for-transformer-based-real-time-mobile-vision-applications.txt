Self-attention is widely used in vision applications to capture global context. However, its computational complexity limits its use in real-time and resource-constrained mobile devices. Although hybrid approaches combining convolutions and self-attention have been proposed, the expensive matrix multiplication operations in self-attention remain a bottleneck. This study presents a novel efficient additive attention mechanism that replaces quadratic matrix multiplications with linear element-wise multiplications. Our design demonstrates that the key-value interaction can be replaced with a linear layer without sacrificing accuracy. Unlike previous methods, our efficient self-attention formulation can be used at all network stages. We introduce a series of models called "Swift-Former" using our proposed efficient additive attention, which achieves state-of-the-art performance in terms of accuracy and mobile inference speed. Our smaller variant achieves 78.5% top-1 ImageNet-1K accuracy with only 0.8 ms latency on iPhone 14, surpassing the accuracy and speed of MobileViT-v2. Our code and models can be found at: https://tinyurl.com/5ft8v46w.