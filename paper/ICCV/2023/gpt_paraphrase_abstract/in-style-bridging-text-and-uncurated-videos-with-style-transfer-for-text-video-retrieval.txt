Large-scale noisy web image-text datasets have proven effective for training robust vision-language models. However, when it comes to video retrieval, these models still need to be fine-tuned using hand-curated text-video pairs in order to adapt to different styles of video descriptions. To overcome this challenge without relying on annotated pairs, we propose a new approach called text-video retrieval with uncurated and unpaired data. This approach utilizes only text queries and uncurated web videos during training, without any paired text-video data. Our proposed method, In-Style, learns the style of the text queries and transfers it to uncurated web videos. Additionally, we demonstrate that a single model can be trained with multiple text styles to improve generalization. To achieve this, we introduce a multi-style contrastive training procedure that enhances generalizability across multiple datasets. We evaluate our model's retrieval performance on multiple datasets, showcasing the advantages of our style transfer framework in the new task of uncurated and unpaired text-video retrieval. Furthermore, our approach improves the state-of-the-art performance on zero-shot text-video retrieval.