Score-based Generative Models (SGMs) have achieved remarkable results in various tasks. However, the current design of the forward diffusion process in SGMs is largely unexplored and often relies on physical heuristics or simplifications. Drawing from scalable Bayesian posterior samplers, we present a comprehensive method for formulating forward processes in SGMs that ensures convergence to the desired target distribution. Our approach reveals that several existing SGMs can be understood as specific instances of our framework. Using this method, we introduce Phase Space Langevin Diffusion (PSLD), which employs score-based modeling within an augmented space enriched by auxiliary variables similar to physical phase space. Our empirical results demonstrate that PSLD outperforms competing methods in terms of sample quality and speed-quality trade-off on established image synthesis benchmarks. Notably, PSLD achieves sample quality comparable to state-of-the-art SGMs (FID: 2.10 for unconditional CIFAR-10 generation). Furthermore, we show the applicability of PSLD in conditional synthesis using pre-trained score networks, providing a promising alternative as an SGM backbone for future advancements. The code and model checkpoints for PSLD can be accessed at https://github.com/mandt-lab/PSLD.