This paper aims to localize action instances in a long untrimmed query video using shorter support videos that represent a common action with unknown class information. The key challenge is to identify reliable temporal cues from the limited support videos. To address this, we propose an attention mechanism based on cross-correlation. This mechanism transforms the support videos to match the context of the query video, highlighting relevant frames and suppressing irrelevant ones. We also summarize sub-sequences of support video frames to capture coarse temporal dynamics and propagate this information to the fine-grained support video features through cross-attention. The cross-attentions are applied to each support video individually, balancing their heterogeneity and compatibility. Finally, the resulting support video features are used to attend to candidate instances in the query video. Additionally, we develop a relational classifier based on the representations of the query and support videos. Our approach achieves state-of-the-art performance on benchmark datasets (ActivityNet1.3 and THUMOS14) and we conduct extensive analysis of each component.