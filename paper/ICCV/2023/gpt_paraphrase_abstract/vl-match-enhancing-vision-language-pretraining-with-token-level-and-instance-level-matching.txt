Vision-Language Pretraining (VLP) has greatly improved the performance of vision-language tasks that involve matching images and texts. This paper introduces VL-Match, a Vision-Language framework that enhances token-level and instance-level matching. At the token level, VL-Match includes a Vision-Language Replaced Token Detection task that promotes interaction between text tokens and images. The text encoder of VLP acts as a generator, producing a corrupted text, while the multimodal encoder of VLP serves as a discriminator, predicting whether each token in the corrupted text matches the image.At the instance level, VL-Match incorporates an Image-Text Matching task that determines whether an image-text pair is matched. To generate more challenging negative text samples, a novel bootstrapping method is proposed. These negative samples differ from the positive ones only at the token level, forcing the network to detect subtle differences between images and texts.Importantly, VL-Match achieves superior performance on all image-text retrieval tasks compared to previous state-of-the-art models, despite having fewer parameters.In summary, VL-Match enhances token-level and instance-level matching in vision-language tasks. It introduces a Vision-Language Replaced Token Detection task to improve interaction between text tokens and images. Additionally, a novel bootstrapping method is used to generate challenging negative text samples for the Image-Text Matching task. VL-Match outperforms previous state-of-the-art models with fewer parameters.