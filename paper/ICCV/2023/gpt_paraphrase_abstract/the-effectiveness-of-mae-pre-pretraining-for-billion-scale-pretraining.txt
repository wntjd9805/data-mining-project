This study challenges the conventional pretrain-then-finetune approach in computer vision for visual recognition tasks. Typically, advanced foundation models are pretrained using large-scale weakly supervised datasets comprising billions of images. In this research, we propose an additional pre-pretraining stage that is straightforward and utilizes the self-supervised MAE technique to initialize the model. Although previous studies have shown that MAE scales with the size of models, our findings demonstrate that it also scales with the size of the training dataset. As a result, our MAE-based pre-pretraining method is suitable for training foundation models as it scales with both model and data size. The introduction of pre-pretraining consistently enhances both the model convergence and the downstream transfer performance across various model scales (ranging from millions to billions of parameters) and dataset sizes (ranging from millions to billions of images). We assess the effectiveness of pre-pretraining on 10 different visual recognition tasks, including image classification, video recognition, object detection, low-shot classification, and zero-shot recognition. Notably, our largest model achieves new state-of-the-art results on iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on Food-101 (96.2%). This study highlights the significant role of model initialization, even in web-scale pretraining using billions of images.