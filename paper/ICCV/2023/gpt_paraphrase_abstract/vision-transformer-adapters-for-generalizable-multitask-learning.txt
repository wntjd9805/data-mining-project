We present the first multitasking vision transformer adapters that learn task affinities capable of being applied to new tasks and domains. These adapters, integrated into a standard vision transformer backbone, can efficiently handle multiple dense vision tasks without requiring excessive parameters. Unlike other multitasking transformers, our approach does not necessitate retraining or fine-tuning when incorporating new tasks or domains. We introduce a task-adapted attention mechanism within our adapter framework that combines gradient-based and attention-based task similarities. The learned task affinities generalize well to zero-shot task transfer, unsupervised domain adaptation, and generalization to novel domains without the need for fine-tuning. Our experimental results demonstrate superior performance compared to both convolutional neural network-based and vision transformer-based multitasking methods. Further details can be found on our project page at https://ivrl.github.io/VTAGML.