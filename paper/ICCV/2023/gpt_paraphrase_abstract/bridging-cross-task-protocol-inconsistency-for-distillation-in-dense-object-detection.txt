Knowledge distillation (KD) has shown promise in training compact models for dense object detection. However, the commonly used softmax-based distillation method overlooks the absolute classification scores for each category. Consequently, the optimal distillation loss does not necessarily result in optimal classification scores for dense object detectors. This inconsistency in the cross-task protocol is particularly problematic for dense object detectors due to the imbalanced nature of foreground categories. To address this issue, we propose a new distillation method that incorporates cross-task consistent protocols specifically tailored for dense object detection.  To tackle the problem of cross-task protocol inconsistency in classification distillation, we approach it by formulating the classification logit maps in both the teacher and student models as multiple binary-classification maps. We then apply a binary-classification distillation loss to each map. For localization distillation, we introduce an IoU-based Localization Distillation Loss that is independent of specific network structures and can be compared to existing localization distillation losses. Our proposed method is simple yet effective, and experimental results demonstrate its superiority over existing methods. The code for our method is available at https://github.com/TinyTigerPan/BCKD.