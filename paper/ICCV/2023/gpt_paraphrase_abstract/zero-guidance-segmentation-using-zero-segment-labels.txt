The CLIP model has allowed for the development of new applications, such as open-vocabulary segmentation, which can locate segments based on arbitrary text queries. This research aims to determine if it is possible to discover semantic segments without user guidance or predefined classes, and automatically label them using natural language. A novel problem called zero-guidance segmentation is proposed, along with a baseline approach that utilizes the DINO and CLIP models without fine-tuning or a segmentation dataset. The method involves segmenting an image into smaller segments, encoding them in CLIP's visual-language space, translating them into text labels, and merging similar segments together. The challenge lies in encoding a visual segment in a way that incorporates both global and local context information for recognition. The main contribution is a new attention-masking technique that balances these two contexts by analyzing CLIP's attention layers. Several metrics are also introduced for evaluating this new task. The method demonstrates the ability to accurately locate specific objects, such as the Mona Lisa painting, in a crowded museum setting. Additional results can be found at the provided website.