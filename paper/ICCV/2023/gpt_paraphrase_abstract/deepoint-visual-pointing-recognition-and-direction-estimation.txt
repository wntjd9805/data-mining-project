This paper presents a novel approach for automatically recognizing and estimating the direction of pointing gestures. The authors introduce the DP Dataset, a large-scale dataset consisting of over 2 million frames of 33 individuals performing pointing gestures in various styles. Each frame in the dataset is annotated with information about the timing and 3D direction of the pointing. To analyze this dataset, the authors propose a new deep network model called DeePoint, which is based on the Transformer architecture. Unlike previous methods that focus solely on the hands, DeePoint takes into account the spatio-temporal coordination of the entire body. The authors conduct extensive experiments to evaluate the accuracy and efficiency of DeePoint and demonstrate its effectiveness. They believe that both the DP Dataset and DeePoint will provide a solid foundation for understanding human intentions through visual cues.