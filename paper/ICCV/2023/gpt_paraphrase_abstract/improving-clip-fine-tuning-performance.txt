CLIP models have achieved remarkable accuracy in zero-shot recognition, but their performance in fine-tuning for downstream vision tasks is not optimal. On the other hand, masked image modeling (MIM) performs exceptionally well in fine-tuning despite not using semantic labels during training. These two tasks have different components, including image-level targets versus token-level targets, cross-entropy loss versus regression loss, and full-image inputs versus partial-image inputs. To bridge the differences, we propose a classical feature map distillation framework that combines the semantic capability of CLIP models and the key elements of MIM. Experimental results show that this approach significantly improves the fine-tuning performance of CLIP models on various downstream vision tasks. We also observe that the approach generates new CLIP representations with similar diagnostic properties to MIM. Moreover, the feature map distillation approach can be applied to other pre-training models, such as DINO, DeiT, and SwinV2-G, achieving a new record of 64.2 mAP on COCO object detection with a 1.1 improvement. The code and models for this approach are available at https://github.com/SwinTransformer/Feature-Distillation.