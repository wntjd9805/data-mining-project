Sign Language Translation (SLT) is a complex task that involves converting visual-gestural language into text. Many previous methods use gloss sequences as an intermediate representation to aid SLT, making it a two-stage process of sign language recognition (SLR) followed by sign language translation (SLT). However, the scarcity of gloss-annotated sign language data and the limitations of gloss representation have hindered the progress of SLT. To overcome this challenge, we propose a novel approach called Gloss-Free SLT based on Visual-Language Pretraining (GFSLT-VLP). This approach leverages pre-trained models to improve SLT without relying on gloss annotations. Our approach consists of two main stages: (i) integrating Contrastive Language-Image Pre-stages (CLIP) with masked self-supervised learning to bridge the semantic gap between visual and textual representations, and (ii) constructing an end-to-end architecture that inherits the parameters of the pre-trained Visual Encoder and Text Decoder. The combination of these designs forms a robust sign language representation and significantly enhances gloss-free sign language translation. Our approach achieves unprecedented improvements in BLEU-4 score on the PHOENIX14T and CSL-Daily datasets compared to state-of-the-art gloss-free SLT methods. Additionally, our approach achieves competitive results on the PHOENIX14T dataset when compared to gloss-based methods. The proposed approach excludes gloss information throughout the training and inference process, resulting in a more streamlined and effective SLT system.