Autoregressive approaches are increasingly used to serialize observed variables based on specific criteria. Neural Processes (NPs) model variable distribution as a continuous function and offer fast solutions for various tasks through a meta-learning framework. This study introduces an autoregressive-based framework for NPs, capitalizing on their autoregressive properties. By leveraging the autoregressive stacking effects of different variables, this framework enhances the representation of the latent distribution and refines local and global relationships within the positional representation using a sliding window mechanism. Autoregression improves function approximations in a stacked manner, thereby increasing the optimization upper bound. This framework is referred to as Autoregressive Neural Processes (AENPs) or Conditional Autoregressive Neural Processes (CAENPs). Unlike traditional NP models, AENPs address both local and global considerations by capturing contextual relationships in the deterministic path and introducing sliding window attention and global attention to reconcile these relationships in the context sample points. Autoregressive constraints exist between multiple latent variables in the latent paths, creating a complex global structure that enables the model to learn complex distributions. The effectiveness of NPs or CFANPs models is demonstrated through experiments on 1D data, Bayesian optimization, and 2D data.