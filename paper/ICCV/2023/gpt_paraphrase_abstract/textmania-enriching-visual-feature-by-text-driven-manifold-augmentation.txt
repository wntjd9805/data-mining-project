We propose TextManiA, a method for enhancing visual feature spaces using text. TextManiA enriches visual data by introducing semantic perturbations within classes using visually mimetic words, known as attributes. Our approach is based on the hypothesis that general language models like BERT and GPT contain some visual information even without being trained on visual data. To test this hypothesis, TextManiA transfers pre-trained text representations from a large language encoder to the target visual feature space during training. Our extensive analysis suggests that the language encoder does indeed include visual information that can be used to enhance visual representation. Our experiments demonstrate that TextManiA is particularly effective in scenarios with limited samples and class imbalance, as well as in evenly distributed data. We also show that TextManiA is compatible with label mix-based approaches for evenly distributed scarce data.