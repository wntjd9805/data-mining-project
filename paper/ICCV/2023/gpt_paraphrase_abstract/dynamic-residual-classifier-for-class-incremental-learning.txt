The rehearsal strategy is commonly used to address the issue of catastrophic forgetting in class incremental learning (CIL), where limited exemplars from previous tasks are preserved. However, the imbalanced sample numbers between old and new classes can lead to biased classifier learning. Existing CIL methods utilize long-tailed (LT) recognition techniques, such as adjusted losses and data re-sampling methods, to handle the data imbalance within each increment task. This study demonstrates the dynamic nature of data imbalance in CIL and proposes a new approach called Dynamic Residual Classifier (DRC) to tackle this challenging scenario. DRC is based on a recent advancement in residual classifiers and utilizes branch layer merging to address the problem of model growth. Furthermore, DRC is compatible with different CIL pipelines and significantly enhances their performance. When combined with the model adaptation and fusion (MAF) pipeline, this method achieves state-of-the-art results on both conventional CIL and LT-CIL benchmarks. Extensive experiments are conducted to provide a detailed analysis, and the code for DRC is publicly available.