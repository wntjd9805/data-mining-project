We present a new representation called OrderedAtomic Activity for understanding interactive scenarios. This representation breaks down each scenario into a series of ordered atomic activities, where each activity includes an action, the actors involved, and the temporal sequence of events. This design also helps identify important interactive relationships, such as yielding. The action is a high-level semantic motion pattern that is based on the road topology, which we divide into zones and corners with unique identifiers. For instance, a group of pedestrians crossing in front is represented as C1 â†’ C4: P+. We have created a large-scale dataset called OATS1, consisting of 1026 video clips (approximately 20 seconds each) recorded at intersections in the San Francisco Bay Area. Each clip is labeled with the proposed language, resulting in 59 activity categories and 6512 annotated activity instances. We propose three detailed tasks for scenario understanding: multilabel atomic activity recognition, activity order prediction, and interactive scenario retrieval. To address these tasks, we introduce a framework based on Graph Convolutional Networks that models the appearance and motion of traffic participants. Our framework outperforms existing methods in these tasks. However, we also observe that the current methods do not achieve satisfactory performance, indicating the need for the research community to develop new algorithms for these tasks in order to improve interactive scenario understanding.