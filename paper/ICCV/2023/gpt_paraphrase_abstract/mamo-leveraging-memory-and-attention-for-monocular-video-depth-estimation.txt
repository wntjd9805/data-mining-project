We propose MAMo, a new framework for estimating depth in monocular videos. MAMo enhances single-image depth estimation networks by incorporating temporal information, resulting in more accurate depth predictions. This is achieved by introducing a memory component that stores visual and displacement information from previous frames. The memory allows the depth network to refer to relevant features from the past when predicting depth for the current frame. We employ an attention-based approach to process memory features, using self-attention to learn the spatio-temporal relation between visual and displacement memory tokens. These features are then combined with current visual features through cross-attention and passed to a decoder for depth prediction. Extensive experiments on benchmark datasets demonstrate that MAMo consistently improves monocular depth estimation networks and achieves state-of-the-art accuracy. Notably, MAMo provides higher accuracy and lower latency compared to existing cost-volume-based video depth models.