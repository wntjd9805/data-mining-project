Unsupervised learning of visible-infrared person re-identification (USL-VI-ReID) is a challenging task that can address the issue of expensive cross-modality annotations. Previous research has focused on handling the cross-modality discrepancy in unsupervised conditions. However, these approaches overlook the fact that USL-VI-ReID involves a hierarchical discrepancy, including camera variation and modality discrepancy, which leads to clustering inconsistencies and ambiguous cross-modality label association. To tackle these problems, we propose a hierarchical framework for USL-VI-ReID that learns a grand unified representation (GUR). The GUR framework consists of two main components. Firstly, it adopts a bottom-up domain learning strategy with a cross-memory association embedding module to explore the hierarchical domains, such as intra-camera, inter-camera, and inter-modality domains. This allows for the learning of a unified and robust representation against hierarchical discrepancy. Secondly, to unify the identities across modalities, we develop a cross-modality label unification module. This module constructs a cross-modality affinity matrix to propagate labels between the two modalities, while maintaining the label structure within each modality using a homogeneous structure matrix. Extensive experiments demonstrate that our GUR framework outperforms existing USL-VI-ReID methods and even surpasses some supervised counterparts.