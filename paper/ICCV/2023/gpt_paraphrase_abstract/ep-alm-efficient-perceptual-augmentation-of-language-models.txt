Large Language Models (LLMs) have made a remarkable impact with their impressive capabilities at large scales. Similarly, transformer models like ViT have achieved top performance in challenging vision benchmarks. While unimodal models have seen abundant success, the question arises whether we need to follow the same approach for multimodal tasks. Rather than creating new models, we propose enhancing Language Models with perception. However, existing methods for adapting pretrained models for vision-language tasks have limitations in efficiency. They rely on numerous parameters, large multimodal pretraining, encoders trained on massive image-text datasets, and impose significant inference overhead. Moreover, these approaches mainly focus on Zero-Shot and In Context Learning, neglecting direct finetuning. To address these issues, we explore the minimum computational effort required to adapt unimodal models for multimodal tasks. We introduce a challenging setup and various approaches that efficiently adapt pretrained unimodal models. Our approach, eP-ALM, surpasses other baselines on VQA and Captioning tasks across Image, Video, and Audio modalities by freezing over 99% of parameters, training only one linear projection layer, and appending just one trainable token. The code for our approach is available at https://github.com/mshukor/eP-ALM.