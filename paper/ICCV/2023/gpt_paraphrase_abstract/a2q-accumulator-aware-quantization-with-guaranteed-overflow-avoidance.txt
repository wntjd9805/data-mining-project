We introduce a new weight quantization method called accumulator-aware quantization (A2Q) for training quantized neural networks (QNNs) to prevent overflow when using low-precision accumulators during inference. A2Q utilizes a formulation inspired by weight normalization to limit the â„“1-norm of model weights based on derived accumulator bit width bounds. This promotes unstructured weight sparsity, ensuring overflow avoidance while training QNNs for low-precision accumulation. We demonstrate the effectiveness of A2Q on computer vision tasks in deep learning, showing that it can train QNNs with low-precision accumulators while maintaining competitive model accuracy compared to a floating-point baseline. We evaluate the impact of A2Q on both general-purpose platforms and programmable hardware, with a primary focus on FPGA deployment due to their ability to fully exploit custom accumulator bit widths. Our experiments reveal that the choice of accumulator bit width significantly affects the resource efficiency of FPGA-based accelerators. On average, A2Q achieves up to a 2.3x reduction in resource utilization compared to 32-bit accumulator counterparts, while maintaining 99.2% of the floating-point model accuracy.