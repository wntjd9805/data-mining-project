Recent advancements in vision-only perception models for autonomous driving have shown promising results by encoding multi-view image features into Bird's-Eye-View (BEV) space. However, the transformation of image features into the BEV coordinate frame remains a critical step and the main bottleneck in these methods. Existing approaches either rely on non-parametric depth distribution modeling, which consumes significant memory, or ignore geometry information altogether.  In contrast, this study proposes a solution that utilizes parametric depth distribution modeling for feature transformation. The process involves lifting 2D image features to the 3D space defined for the ego vehicle using a predicted parametric depth distribution for each pixel in each view. Subsequently, the 3D feature volume is aggregated based on the occupancy of the 3D space derived from depth, resulting in the BEV frame. The transformed features can then be utilized for downstream tasks such as object detection and semantic segmentation.  Another issue addressed in this paper is the hallucination problem present in existing semantic segmentation methods, which do not consider visibility information. This problem can pose significant challenges for subsequent modules like control and planning. To mitigate this issue, the proposed method provides depth uncertainty and reliable visibility-aware estimations. Additionally, the study introduces a novel visibility-aware evaluation metric, which, when taken into account, helps mitigate the hallucination problem.  Extensive experiments conducted on object detection and semantic segmentation using the nuScenes datasets demonstrate that the proposed method outperforms existing approaches in both tasks. This research was conducted during an internship at NVIDIA.