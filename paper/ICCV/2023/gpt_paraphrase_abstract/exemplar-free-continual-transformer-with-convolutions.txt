Continual Learning (CL) is the process of training a machine learning model sequentially, allowing it to learn new information while retaining previously learned tasks without access to previous training data. While CL has gained significant interest, most recent approaches in computer vision have focused on convolutional architectures. However, with the emergence of vision transformers, it is important to explore their potential for CL. Although some recent approaches have been proposed for CL with vision transformers, they either store training instances of previous tasks or rely on task identifiers during testing, which can be restrictive.To address these limitations, this paper presents a novel exemplar-free approach called ConTraCon for class/task incremental learning. ConTraCon does not require task identifiers during inference and eliminates the need for storing previous training instances. The proposed approach utilizes the transformer architecture and involves re-weighting the key, query, and value weights of the multi-head self-attention layers of a transformer trained on a similar task. This re-weighting is achieved using convolution, allowing the approach to maintain low parameter requirements for each task. Additionally, an image augmentation-based entropic task identification approach is employed to predict tasks without relying on task identifiers during inference.Experiments conducted on four benchmark datasets demonstrate that the proposed ConTraCon approach outperforms several competitive approaches while requiring fewer parameters.