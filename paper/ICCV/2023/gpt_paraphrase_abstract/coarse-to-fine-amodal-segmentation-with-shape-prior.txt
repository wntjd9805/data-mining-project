We propose a novel approach called Coarse-to-Fine Segmentation (C2F-Seg) for addressing the challenging task of amodal object segmentation. Our method progressively models the segmentation by reducing the learning space from pixel-level image space to vector-quantized latent space. This allows us to handle long-range dependencies and learn a coarse-grained amodal segment from visual features and visible segments. However, the latent space lacks detailed information, making it difficult to provide precise segmentation. To overcome this, we introduce a convolution refine module that injects fine-grained information and improves the precision of amodal object segmentation based on visual features and coarse-predicted segmentation. To facilitate research in this field, we create a synthetic amodal dataset named MOViD-A, suitable for both image and video amodal object segmentation. Our model is extensively evaluated on benchmark datasets, KINS and COCO-A, and our empirical results demonstrate the superiority of C2F-Seg. Furthermore, we showcase the potential of our approach for video amodal object segmentation on FISHBOWL and MOViD-A. More information can be found on our project page: https://jianxgao.github.io/C2F-Seg.