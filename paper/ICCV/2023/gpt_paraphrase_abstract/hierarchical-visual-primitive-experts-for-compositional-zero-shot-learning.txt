Compositional zero-shot learning (CZSL) is a task that involves recognizing unseen compositions using prior knowledge of known primitives. However, existing CZSL methods face challenges in understanding the contextuality between attributes and objects, ensuring discriminability of visual features, and dealing with the long-tailed distribution of real-world compositional data. To address these issues, we present a simple and scalable framework called Composition Transformer (CoT). CoT utilizes object and attribute experts in a unique way to generate representative embeddings, employing a hierarchical visual network. The object expert extracts representative object embeddings from the final layer in a bottom-up manner, while the attribute expert generates attribute embeddings in a top-down manner using an object-guided attention module that explicitly models contextuality. Additionally, to overcome biased predictions caused by imbalanced data distribution, we introduce a simple minority attribute augmentation (MAA) technique that synthesizes virtual samples by mixing two images and oversampling minority attribute classes. Our proposed method achieves state-of-the-art performance on multiple CZSL benchmarks, including MIT-States, C-GQA, and VAW-CZSL. Furthermore, we demonstrate that CoT improves visual discrimination and effectively addresses model bias resulting from imbalanced data distribution. The code for our method is publicly available at https://github.com/HanjaeKim98/CoT.