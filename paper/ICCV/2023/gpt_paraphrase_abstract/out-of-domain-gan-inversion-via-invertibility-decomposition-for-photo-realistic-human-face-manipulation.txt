Generative Adversarial Networks (GAN) inversion is limited in its ability to accurately recreate images due to Out-Of-Domain (OOD) areas, such as backgrounds and accessories. To enhance the fidelity of GAN inversion, it is necessary to detect and blend these OOD areas with the input image. Previous methods have used reconstruction error to predict an "invertibility mask" that identifies the OOD areas, but this approach is often inaccurate due to the influence of reconstruction error in the In-Domain (ID) areas. In this study, we propose a new framework for improving the fidelity of human face inversion by decomposing input images into ID and OOD partitions using invertibility masks. Unlike previous methods, our invertibility detector is trained concurrently with a spatial alignment module. By iteratively aligning generated features to the input geometry and reducing the reconstruction error in the ID regions, we make the OOD areas more distinguishable and accurately predictable. We further enhance our results by blending the OOD areas from the input image with the ID GAN inversion results. Our method produces photo-realistic outcomes for real-world human face image inversion and manipulation. Extensive experiments demonstrate that our approach outperforms existing methods in terms of GAN inversion quality and attribute manipulation. The code for our method is available at: AbnerVictor/OOD-GAN-inversion.