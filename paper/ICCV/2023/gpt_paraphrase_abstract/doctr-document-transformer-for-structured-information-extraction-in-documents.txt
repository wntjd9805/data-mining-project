We propose a novel approach for extracting structured information from visually rich documents. Existing methods, such as IOB tagging and graph-based formulations, have limitations in terms of reliance on text ordering or decoding complex graphs. In contrast, inspired by anchor-based object detectors in computer vision, we represent entities as anchor words and bounding boxes, while entity linking is represented as associations between anchor words. This approach is more robust to text ordering and results in a compact graph for entity linking. To implement this formulation, we introduce a DocumentTransformer (DocTr) that detects and associates entity bounding boxes in visually rich documents. Additionally, we employ a simple pre-training strategy to improve entity detection in the context of language. Our evaluations on three structured information extraction benchmarks demonstrate the efficacy of our proposed approach, which outperforms existing solutions.