We present a new method called LiDAR-UDA for LiDAR segmentation, which is a two-stage self-training-based Unsupervised Domain Adaptation (UDA) technique. Existing self-training methods rely on a model trained on labeled source data to generate pseudo labels for target data and improve predictions through fine-tuning. However, these methods struggle with domain shifts caused by variations in LiDAR sensor configurations between the source and target domains. To address this issue, we propose two approaches: 1) LiDAR beam subsampling, which randomly drops beams to simulate different LiDAR scanning patterns, and 2) cross-frame ensembling, which leverages temporal consistency across consecutive frames to generate more reliable pseudo labels. Our method is straightforward, widely applicable, and does not result in any additional inference cost. We evaluate our approach on multiple public LiDAR datasets and demonstrate that it achieves superior performance compared to state-of-the-art methods, with an average mIoU improvement of more than 3.9% across all scenarios. The code for our method will be made available at https://github.com/JHLee0513/lidar_uda.