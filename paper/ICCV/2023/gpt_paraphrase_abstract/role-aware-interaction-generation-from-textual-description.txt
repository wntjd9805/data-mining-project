This research addresses the issue of generating interaction between two human actors based on textual descriptions. The researchers argue that certain interactions, known as asymmetric interactions, involve a relationship between an actor and a receiver, and their movements differ significantly depending on their assigned roles. However, existing studies on interaction generation focus on learning the correspondence between a single label and the combined motions of both actors, disregarding the differences in individual roles.   To tackle this problem, the researchers propose a novel approach called role-aware interaction generation. This approach allows for the designation of roles before generating the interaction. The researchers translate the text of the asymmetric interactions into active and passive voice to ensure consistency with each role. They develop a model that learns to generate the motions of the designated role, ensuring a mutually consistent interaction. By treating individual motions separately, the model can be pretrained using single-person motion data, leading to more accurate interactions.  Furthermore, the researchers introduce a method inspired by Permutation Invariant Training (PIT) to automatically learn which action corresponds to an actor or a receiver without the need for additional annotation. They also identify cases where existing evaluation metrics fail to accurately assess the quality of generated interactions. To address this issue, they propose a new metric called Mutual Consistency.  Experimental results demonstrate the effectiveness of the proposed method and the importance of the new metric. The researchers provide the code for their approach on GitHub.