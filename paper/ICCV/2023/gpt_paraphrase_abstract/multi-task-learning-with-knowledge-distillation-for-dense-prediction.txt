While multi-task learning (MTL) is an attractive area of research, it is often more challenging to train compared to single-task learning. Incorporating knowledge distillation into MTL to enhance training efficiency and model performance remains a difficult problem. In this study, we present a novel knowledge distillation approach for MTL of dense prediction, guided by two design principles. Firstly, to optimize memory and training efficiency, we employ a single robust multi-task model as the teacher during training, instead of using multiple teachers as commonly done in existing studies. Secondly, we utilize a less sensitive Cauchy-Schwarz (CS) divergence instead of the Kullback-Leibler (KL) divergence and introduce a CS distillation loss accordingly. By using the less sensitive divergence, our knowledge distillation approach with an alternative match captures inter-task and intra-task information between the teacher model and the student model for each task, enabling the acquisition of "dark knowledge" for effective distillation. We conducted extensive experiments on dense prediction datasets, including NYUD-v2 and PASCAL-Context, encompassing various vision tasks such as semantic segmentation, human parts segmentation, depth estimation, surface normal estimation, and boundary detection. The results demonstrate a significant improvement in model performance and practical inference efficiency achieved by our proposed method.