Large Pre-trained Transformers have a unique ability to learn in-context without gradient updates. This ability has been extended to the vision-language domain by incorporating visual information into language models. However, these methods may have issues such as template sensitivity and hallucination, and the computational demands of these large models are significant. In order to address these challenges, we propose a framework called Self-supervised IN-Context learning (SINC). SINC utilizes a meta-model to learn from self-supervised prompts and tailored demonstrations, enabling in-context learning without relying solely on the intrinsic ability of large language models. The learned models can then be applied to downstream tasks for on-the-fly in-context predictions. Our extensive experiments demonstrate that SINC outperforms gradient-based methods in various vision-language tasks under few-shot settings. Additionally, the design of SINC allows us to explore the benefits of in-context learning across different tasks and analyze the essential components for its emergence in the vision-language domain.