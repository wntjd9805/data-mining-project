Large vision-language models (VLMs) like CLIP have been successful in various tasks but struggle with compositional concepts like counting. To address this limitation, we propose a simple yet effective method to enhance the quantitative understanding of VLMs without compromising their overall performance. We introduce a counting-contrastive loss that is used to fine-tune a pre-trained VLM alongside its original objective. This loss is applied to counterfactual examples, where an image is paired with a caption containing an incorrect object count. By encouraging discrimination between the correct caption and its counterfactual variant, our loss serves as a hard negative example. This is the first approach to extend CLIP's capabilities to object counting. Additionally, we present a new benchmark called "CountBench" to evaluate object counting capabilities. Our approach outperforms state-of-the-art baseline models on this benchmark. Moreover, we demonstrate the usefulness of our counting-aware CLIP model in image retrieval and text-conditioned image generation, showing that our model can produce more reliable counts of objects compared to existing models.