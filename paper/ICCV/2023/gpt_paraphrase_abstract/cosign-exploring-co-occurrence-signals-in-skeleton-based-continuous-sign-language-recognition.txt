This paper introduces a novel approach called CoSign for Continuous Sign Language Recognition (CSLR) that focuses on utilizing co-occurrence signals and exploring the potential of skeleton data. Co-occurrence signals, such as hand shape, facial expression, and lip pattern, are crucial in CSLR. Skeleton data, which are more efficient and concise than RGB data, are often overlooked but have great potential for co-occurrence exploration in CSLR.   CoSign utilizes a group-specific Graph Convolutional Network (GCN) to optimize the knowledge of each co-occurrence signal. It also employs a complementary regularization technique to prevent complex co-adaptation across different signals. Additionally, CoSign adopts a two-stream framework to gradually combine both static and dynamic information in the skeleton data.   The effectiveness of CoSign is validated through experiments on three public CSLR datasets (PHOENIX14, PHOENIX14-T, and CSL-Daily). The results demonstrate that CoSign achieves competitive performance compared to recent video-based approaches, while also reducing computation costs during training.