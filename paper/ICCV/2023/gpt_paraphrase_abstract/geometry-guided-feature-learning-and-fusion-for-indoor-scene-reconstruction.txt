This paper proposes a new approach to 3D scene reconstruction that incorporates geometry at multiple levels. Unlike current methods that only consider geometry at the feature level, our approach utilizes geometry-guided feature learning, feature fusion, and network supervision. Firstly, geometry-guided feature learning encodes view-dependent information by incorporating geometric priors. Secondly, a geometry-guided adaptive feature fusion generates weights for multiple views based on the geometric priors. Finally, a consistent 3D normal loss is designed to enforce local constraints by considering the consistency between 2D and 3D normals. Extensive experiments on the Scan-Net dataset demonstrate that our volumetric methods with the proposed geometry integration mechanism outperform existing methods both quantitatively and qualitatively. Furthermore, our methods also exhibit good generalization on the 7-Scenes and TUM RGB-D datasets.