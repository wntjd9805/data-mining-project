Domain adaptation (DA) is a technique used to transfer knowledge from a fully labeled source domain to a target domain with limited or no labeled data, while accounting for differences between the two domains. Semi-supervised learning-based (SSL) approaches, which utilize pseudo labeling, have gained popularity in DA. However, these methods heavily rely on the source domain to generate pseudo labels for the target domain, leading to bias from the source data. Additionally, they often ignore the class distribution bias in the target domain, further degrading performance. To address these issues, we propose a method called GeT, which aims to learn a non-biased target embedding distribution with high-quality pseudo labels. GeT utilizes an online target generative classifier to incorporate the target distribution into distinct Gaussian components weighted by their class priors, reducing source data bias and enhancing target class discriminability. We also introduce a structure similarity regularization framework to mitigate target class distribution bias and improve discriminability. Experimental results demonstrate the effectiveness of GeT, showing consistent improvements in various DA scenarios, including those with class distribution bias. Our code is publicly available at the provided link.