Pre-trained models, such as those trained on ImageNet, are widely used for transfer learning. However, it has been observed that a single pre-trained feature does not perform well across different tasks. This is because each pre-training strategy has a specific bias that may not be suitable for all downstream tasks. Additionally, the augmentations used in training lead to features that are invariant to spatial and appearance transformations, making them less effective for tasks that require sensitivity to these factors.  To address this limitation, we propose a new feature that supports diverse downstream tasks by providing a diverse range of sensitivities and invariances. Drawing inspiration from Quality-Diversity in evolution, we design a pre-training objective that emphasizes both high quality and diversity in features, where diversity is defined based on transformation invariances. Our framework can be applied to both supervised and self-supervised pre-training, resulting in a small ensemble of features.  We also demonstrate how downstream tasks can easily select their preferred invariances from this ensemble, making the approach efficient and adaptable. Through empirical and theoretical analysis, we show that our representation and transfer learning approach is effective for diverse downstream tasks.  To facilitate reproducibility, we have made our code available at https://github.com/ruchikachavhan/quality-diversity-pretraining.git.