Instance embedding discrimination is crucial in connecting instances over time for online video instance segmentation (VIS). Currently, online VIS methods use contrastive items (CIs) sourced from a single reference frame, which we believe is inadequate for achieving highly discriminative embeddings. To address this, we propose a training strategy called Consistent Training for Online VIS (CTVIS) that aligns the training and inference pipelines by replicating the inference phase during training. CTVIS constructs CIs by utilizing the momentum-averaged embedding, memory bank storage mechanisms, and introducing noise to relevant embeddings. This enables a reliable comparison between current and historical instance embeddings, improving the modeling of VIS challenges like occlusion, re-identification, and deformation. Empirically, CTVIS outperforms state-of-the-art VIS models by up to +5.0 points on three VIS benchmarks: YTVIS19 (55.1% AP), YTVIS21 (50.1% AP), and OVIS (35.5% AP). Additionally, we observe that pseudo-videos generated from images can train robust models that surpass fully-supervised ones.