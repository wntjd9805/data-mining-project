Recent advancements in zero-shot video object segmentation (ZVOS) methods have focused on integrating appearance and motion information through feature fusion modules. These modules are designed to be applied uniformly across multiple feature stages. However, our initial experiments have shown that while concatenating the two modality features and using vanilla Transformers for feature fusion can improve performance, it comes at the cost of extensive computation.Upon further analysis, we have discovered that the attention dependencies learned by Transformers in different stages exhibit distinct properties. In the low-level stages, the attention dependencies are global and independent of specific queries, while in the high-level stages, they are semantic-specific. Motivated by these observations, we propose two variants of Transformers for ZVOS: Context-Sharing Transformer (CST) and Semantic Gathering-Scattering Transformer (SGST).CST focuses on learning global-shared contextual information within image frames with a lightweight computation approach. On the other hand, SGST models the semantic correlation separately for the foreground and background, reducing computation costs through a soft token merging mechanism. By employing CST and SGST for low-level and high-level feature fusions, respectively, we establish a level-isomerous Transformer framework for the ZVOS task.Compared to the baseline approach that utilizes vanilla Transformers for multi-stage fusion, our proposed method significantly increases speed by a factor of 13Ã— while achieving new state-of-the-art ZVOS performance. The code for our method is available at the following GitHub repository: https://github.com/DLUT-yyc/Isomer.