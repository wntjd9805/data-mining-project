Current face swap models still have issues with either leaking target identity or failing to preserve target non-identity attributes in the final results. These problems are caused by two common design flaws in previous models: (1) relying on a single compressed encoder to represent both semantic-level non-identity attributes (such as pose) and pixel-level non-facial region details, which is contradictory; (2) heavily depending on long skip-connections between the encoder and the final generator, resulting in leakage of target face identity. To address these issues, we propose a new face swap framework called "WSC-swap" that eliminates skip connections and utilizes two target encoders to separately capture pixel-level non-facial region attributes and semantic non-identity attributes in the face region. To enhance disentanglement learning for the target encoder, we employ identity removal loss through adversarial training (GAN) and non-identity preservation loss using prior 3DMM models. Extensive experiments on FaceForensics++ and CelebA-HQ datasets demonstrate that our approach outperforms previous methods across various metrics, including a novel metric for measuring identity consistency that has been overlooked in previous studies.