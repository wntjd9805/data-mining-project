Increasing the window size in Transformer-based image super-resolution models has been found to improve performance but also leads to high computational overhead. This paper introduces SRFormer, a method that combines large window self-attention with reduced computational burden. The core of SRFormer is the permuted self-attention (PSA), which balances channel and spatial information for self-attention. The PSA is simple and can be easily applied to existing super-resolution networks. SRFormer achieves a higher PSNR score on the Urban100 dataset compared to SwinIR, while using fewer parameters and computations. This approach provides a useful tool for future research in super-resolution model design. The code for SRFormer is available at https://github.com/HVision-NKU/SRFormer.