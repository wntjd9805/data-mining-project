This paper presents a method for localizing steps in instructional videos using a language knowledge base as a source of step descriptions. The model learns to align steps in videos by matching frames, narrations, and step descriptions. The alignment is achieved through direct alignment of step descriptions to frames and indirect alignment through composing steps-to-narrations with narrations-to-video correspondences. The approach performs global temporal grounding of all steps in an article by utilizing order information and is trained with step pseudo-labels. To evaluate the model, a new benchmark called HT-Step is introduced, which involves manually annotating a subset of HowTo100M1 with steps from wikiHow articles. The results of experiments on this benchmark and zero-shot evaluations on CrossTask demonstrate that the multi-modality alignment approach outperforms several baselines and prior works. Additionally, the module for matching narration-to-video surpasses the state of the art on the HTM-Align narration-video alignment benchmark.