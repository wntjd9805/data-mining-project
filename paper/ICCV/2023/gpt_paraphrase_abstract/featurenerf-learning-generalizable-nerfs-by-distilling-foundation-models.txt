Recent research has shown promising results in using generalizable NeRFs for synthesizing novel views from limited image data. However, these models have not been widely applied to other tasks beyond synthesis, such as semantic understanding and parsing. In this paper, we propose a new framework called FeatureNeRF, which aims to learn generalizable NeRFs by distilling knowledge from pre-trained vision foundation models. By leveraging 2D pre-trained models and neural rendering techniques, FeatureNeRF is able to project these models into 3D space and extract deep features for 3D query points using NeRF MLPs. This enables the mapping of 2D images to continuous 3D semantic feature volumes, which can be utilized for various downstream tasks. To evaluate the effectiveness of FeatureNeRF, we conduct experiments on tasks including 2D/3D semantic keypoint transfer and 2D/3D object part segmentation. The results of our extensive experiments demonstrate that FeatureNeRF serves as an effective generalizable 3D semantic feature extractor. For more information, please visit our project page at https://jianglongye.com/featurenerf/.