Recently, multiple teacher-based knowledge distillations have been explored in knowledge distillation. However, the effectiveness of these approaches is sometimes limited because immature teachers may transfer false knowledge to the student. To address this issue and make better use of multiple networks, we propose dividing the networks into teacher and student groups. The student group consists of immature networks that need to learn from the teacher's knowledge, while the teacher group comprises selected networks capable of successful teaching. Our approach introduces an online role change strategy, where the top-ranked networks in the student group can promote to the teacher group in each iteration. By training the teacher group using the error samples from the student group, we refine the teacher group's knowledge and successfully transfer collaborative knowledge from the teacher group to the student group. We demonstrate the superiority of our method on CIFAR-10, CIFAR-100, and ImageNet datasets, achieving high performance. Additionally, we validate the generality of our approach by applying it to different backbone architectures such as ResNet, WRN, VGG, Mobilenet, and Shufflenet.