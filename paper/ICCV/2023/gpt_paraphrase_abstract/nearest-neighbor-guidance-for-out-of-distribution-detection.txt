Detecting samples that are outside the known distribution is crucial for machine learning models operating in open-world environments. Classifier-based scores are commonly used for this purpose as they can detect out-of-distribution samples with high accuracy. However, these scores often suffer from overconfidence issues, resulting in misclassification of out-of-distribution samples that are far from the known distribution. In order to address this challenge, we propose a method called Nearest Neighbor Guidance (NNGuide) that incorporates the boundary geometry of the data manifold into the classifier-based score. NNGuide reduces the overconfidence of the classifier on out-of-distribution samples while maintaining its ability to detect fine-grained details. We conducted extensive experiments on ImageNet out-of-distribution detection benchmarks, including scenarios where the known data underwent natural distribution shifts. The results show that NNGuide significantly improves the performance of the base detection scores, achieving state-of-the-art results on metrics such as AUROC, FPR95, and AUPR.