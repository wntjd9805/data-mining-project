Neural fields, which use neural networks to represent signals, offer advantages over traditional discrete representations. They are continuous, scale well with increasing resolution, and can be differentiated multiple times. However, optimizing separate neural fields for each signal is inefficient and fails to utilize shared information or structures among signals. Existing methods address this as a meta-learning problem, using gradient-based meta-learning or hypernetworks. Instead, we propose a new paradigm that treats the large-scale training of neural representations as part of a partially-observed neural process framework. By leveraging neural process algorithms, we demonstrate that our approach outperforms current gradient-based meta-learning and hypernetwork approaches.