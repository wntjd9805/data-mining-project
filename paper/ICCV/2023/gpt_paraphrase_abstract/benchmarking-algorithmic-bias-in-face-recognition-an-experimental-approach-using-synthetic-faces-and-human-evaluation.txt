We present a novel approach to measuring bias in face recognition systems through experimentation. Current methods rely on observational datasets collected in real-world scenarios, which only allow for correlational conclusions. In contrast, our experimental method allows for causal conclusions by manipulating individual attributes while keeping others constant. We generate synthetic faces using a neural face generator and modify specific attributes of interest. Human observers provide ground truth evaluations on the similarity of synthetic image pairs. We validate our method by quantitatively assessing race and gender biases in three research-grade face recognition models. Our synthetic pipeline reveals that these algorithms have lower accuracy when recognizing faces from Black and East Asian populations. Additionally, our method can measure the impact of attribute changes on face identity distances reported by these models. We have made our large synthetic dataset, consisting of 48,000 synthetic face image pairs and 555,000 human annotations, available to researchers working in this important field.