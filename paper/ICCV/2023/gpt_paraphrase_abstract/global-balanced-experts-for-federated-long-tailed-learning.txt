Federated learning (FL) is a popular method for distributed machine learning that allows collaborative training of a global model across multiple devices without sharing local data. However, the presence of long-tailed data can negatively impact the model's performance in real-world FL applications. Existing rebalance strategies are ineffective for addressing the federated long-tailed issue when using local label distribution as the class prior at the clients' side. To address this problem, we propose a new framework called Global Balanced Multi-Expert (GBME). GBME optimizes a balanced global objective without requiring additional information beyond the standard FL pipeline. It achieves this by deriving a proxy from the accumulated gradients uploaded by the clients after local training. This proxy is shared by all clients as the class prior for rebalance training. It also guides the client grouping to train a multi-expert model, which aggregates knowledge from different clients through an ensemble of experts corresponding to different client groups.To enhance privacy preservation, we introduce the GBME-p algorithm, which guarantees the prevention of privacy leakage from the proxy. We conducted extensive experiments on long-tailed decentralized datasets to evaluate the effectiveness of GBME and GBME-p. Both methods outperformed state-of-the-art techniques. The code for GBME and GBME-p is available at [link].