Estimating depths in 360-degree images is challenging due to the distorted field-of-view. Convolutional neural networks (CNN) are not effective for this task. While a transformer with global attention improves depth estimation, it is computationally inefficient. Local attention is needed, but a specific strategy is required to address the distorted geometry and limited receptive field of equirectangular images. Existing approaches only address one of these issues, leading to unsatisfactory results. In this paper, we propose EGformer, a transformer that is biased towards equirectangular geometry. EGformer enables efficient extraction of equirectangular geometry-aware local attention with a large receptive field. We utilize the equirectangular geometry as a bias for local attention, rather than trying to reduce distortion. Our approach achieves the best depth outcomes with low computational cost and fewest parameters compared to recent studies on depth estimation in 360-degree images. This demonstrates the effectiveness of our proposed methods.