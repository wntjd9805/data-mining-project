Visual Speech Recognition (VSR) is a challenging task that requires deeper reasoning over video sequences. Current approaches in VSR heavily rely on labeled data for training, limiting their ability to generalize well beyond the training set and perform well in out-of-distribution scenarios. In contrast to previous methods that use complex training procedures and architectures, we propose a simple approach called Lip2Vec. This approach involves learning a prior model that maps the encoded latent representations of lip sequences to corresponding audio latents, which are invariant enough for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. Our proposed model achieves competitive results on the LRS3 dataset, with a Word Error Rate (WER) of 26. Unlike state-of-the-art approaches, our model also performs reasonably well on the VoxCeleb2-en test set. By treating VSR as an ASR task, we believe that we can bridge the performance gap between the two and enable more flexible formulations of lip reading.