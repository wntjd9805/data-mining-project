The use of multi-modal data for scene understanding is crucial in various applications, such as autonomous navigation. However, existing models face challenges in adapting to changing data distributions without the need for extensive data annotation. These models typically assume the availability of source data and paired multi-modal data for training, which may not always be feasible due to privacy, security, or economic concerns. Additionally, relying solely on paired multi-modal data collection ignores the potential benefits of using pre-trained uni-modal models that are widely available.To address these limitations, we propose a novel approach that adapts a set of independently trained uni-modal models to a target domain with unlabeled multi-modal data, without requiring access to the original source dataset. Our approach employs a switching framework that selects between two cross-modal pseudo-label fusion methods - agreement filtering and entropy weighting - based on the estimated domain gap. We evaluate our method on the semantic segmentation problem and conduct experiments in seven challenging adaptation scenarios.The results demonstrate the effectiveness of our approach, as it achieves comparable or even superior performance compared to methods that assume access to source data. Specifically, our method achieves an improvement in mIoU (mean Intersection over Union) of up to 12% over competing baselines. We have made our code publicly available at https://github.com/csimo005/SUMMIT.