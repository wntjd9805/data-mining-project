We propose a new method to accurately estimate camera rotation in crowded real-world scenes using handheld monocular video. Existing methods lack both high accuracy and acceptable speed in this specific setting. To address this, we create a new dataset and benchmark with rigorously verified ground truth on 17 video sequences. The commonly used methods for wide baseline stereo and autonomous driving perform poorly on monocular video due to their reliance on specific sensor setups, motion models, or local optimization strategies. Additionally, commonly used robustification techniques like RANSAC become prohibitively slow for dynamic scenes. To overcome these limitations, we introduce a novel generalization of the Hough transform on SO(3) to efficiently and robustly find the camera rotation that best matches the optical flow. Our method significantly outperforms other comparable fast methods, reducing error by almost 50% and achieving higher accuracy regardless of speed. This represents a significant advancement in the field of computer vision, particularly for crowded scenes. The code and dataset are available at the provided website.