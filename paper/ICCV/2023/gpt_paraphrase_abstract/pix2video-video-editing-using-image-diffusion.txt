Image diffusion models have become the leading image generator models due to their ability to produce high-quality and diverse images. These models are particularly useful for tasks such as inverting real images and generating images based on text inputs, making them attractive for image editing applications. In this study, we explore the use of pre-trained image models for text-guided video editing, with the main challenge being to preserve the content of the source video while achieving the desired edits. Our method involves two simple steps: firstly, we employ a pre-trained structure-guided image diffusion model to perform text-guided edits on a reference frame; secondly, we gradually propagate these changes to future frames using self-attention feature injection, adapting the denoising step of the diffusion model. We further consolidate the changes by adjusting the latent code of each frame before continuing the process. Importantly, our approach does not require any training and can be applied to a wide range of edits. To evaluate our method, we conduct extensive experiments and compare it against four prior and parallel efforts. Our results demonstrate that our approach enables realistic text-guided video edits without the need for computationally intensive preprocessing or video-specific fine-tuning.