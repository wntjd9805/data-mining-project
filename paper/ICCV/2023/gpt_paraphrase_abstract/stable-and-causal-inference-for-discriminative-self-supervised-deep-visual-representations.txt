Discriminative self-supervised methods have made significant progress in enhancing visual tasks by learning data encoders that are robust to distortions/augmentations. However, these methods often result in unstable learned representations that hinder downstream performance. This study takes a causal perspective to analyze the reasons behind this instability and proposes solutions to overcome it. Inspired by prior works, our approach focuses on demixing ground truth causal sources to some extent. Unlike previous methods, we introduce our solutions during the inference process rather than the training process to improve time efficiency. Through experiments on controlled and realistic image datasets, we demonstrate the effectiveness of our proposed solutions, which involve using controlled synthetic data to temper a linear transformation.