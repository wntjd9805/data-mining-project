The study focuses on the problem of realistic visual question answering (RVQA), where a model needs to distinguish unanswerable questions (UQs) from answerable ones (AQs). The researchers identify two issues in current RVQA research: the presence of too many easy UQs in datasets and the requirement of a large number of annotated UQs for training. To address the first issue, a new testing dataset called RGQA is proposed, which combines AQs from an existing VQA dataset with approximately 29,000 human-annotated UQs. These UQs are generated using two approaches: CLIP-based and Perturbation-based, resulting in both fine-grained and coarse-grained image-question pairs. To tackle the second issue, an unsupervised training approach is introduced. This approach involves using pseudoUQs by randomly pairing images and questions, incorporating an RoI Mixup procedure to generate more fine-grained pseudo UQs, and employing model ensembling to improve model confidence. Experimental results demonstrate that the use of pseudo UQs significantly outperforms existing RVQA baselines, and the incorporation of RoI Mixup and model ensembling further enhances performance. Additionally, a human evaluation highlights a performance gap between humans and models, suggesting the need for further RVQA research. The researchers have made their code and dataset available on GitHub.