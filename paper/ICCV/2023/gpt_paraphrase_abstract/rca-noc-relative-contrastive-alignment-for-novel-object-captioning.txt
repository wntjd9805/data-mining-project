This paper presents a new method for generating captions for novel objects using a technique called relative contrastive learning. The approach focuses on aligning visual and semantic aspects by maximizing compatibility between image regions and object tags in a contrastive manner. To achieve this, the authors augment the tags for each image using positive and negative pairs obtained from foundation models like CLIP. The rank of each augmented tag in a list is used as a relative relevance label to contrast the top-ranked tags with lower-ranked tags. This learning objective encourages the top-ranked tags to be more compatible with their image and text context, enhancing the discriminative ability of the learned multi-modality representation. The proposed approach, called RCA-NOC, is evaluated on two datasets and outperforms existing methods by a significant margin, demonstrating its effectiveness in improving vision-language representation for novel object captioning.