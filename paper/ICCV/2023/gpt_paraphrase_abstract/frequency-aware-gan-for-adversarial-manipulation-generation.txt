The increasing concerns surrounding image manipulation techniques stem from their potential to cause morality and security issues. While several methods have been proposed to detect these manipulations and have shown promising results, they are susceptible to adversarial attacks. This study introduces the Adversarial Manipulation Generation (AMG) task to investigate the vulnerability of image manipulation detectors.To accomplish this, the study proposes an optimal loss function and expands existing attacks to generate adversarial examples. It is noted that current spatial attacks result in a significant decline in image quality, primarily due to the loss of high-frequency detailed components. Building upon this observation, a novel adversarial attack is developed, which combines spatial and frequency features within the GAN architecture to generate adversarial examples. Additionally, an encoder-decoder architecture with skip connections is designed to preserve fine details.The proposed method is evaluated on three image manipulation detectors (FCN, ManTra-Net, and MVSS-Net) using three benchmark datasets (DEFACTO, CASIAv2, and COVER). The experimental results demonstrate that the method generates adversarial examples quickly (0.01s per image), maintains better image quality (with a PSNR 30% higher than spatial attacks), and achieves a high attack success rate. Furthermore, the examples generated by AMG are observed to deceive both classification and segmentation models, indicating better transferability across different tasks.