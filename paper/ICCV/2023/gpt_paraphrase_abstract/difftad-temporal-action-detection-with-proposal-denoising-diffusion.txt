We present DiffTAD, a new approach for temporal action detection (TAD) using denoising diffusion. DiffTAD takes random temporal proposals as input and accurately generates action proposals for untrimmed long videos. Unlike previous methods that rely on discriminative learning, DiffTAD adopts a generative modeling perspective. This is achieved by diffusing ground-truth proposals to random ones through a forward/noising process and then learning to reverse the noising process through a backward/denoising process. To facilitate training, we incorporate the denoising process into the Transformer decoder, using a temporal location query design that ensures faster convergence. Additionally, we propose a cross-step selective conditioning algorithm to accelerate inference. Extensive evaluations on ActivityNet and THUMOS datasets demonstrate that DiffTAD outperforms existing alternatives. The code for DiffTAD is available at https://github.com/sauradip/DiffusionTAD.