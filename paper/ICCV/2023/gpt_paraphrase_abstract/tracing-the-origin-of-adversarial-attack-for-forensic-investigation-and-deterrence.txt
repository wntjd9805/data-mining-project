Deep neural networks can be attacked by adversarial examples, where certain inputs are intentionally crafted to mislead the model. In this paper, we aim to investigate and trace these attacks back to their source, specifically the model from which the adversarial examples originated. By developing techniques to trace the attack, we can enhance forensic investigation of such incidents and potentially deter future attacks.We focus on a scenario where a machine learning model is distributed to multiple buyers, with each buyer receiving a slightly different copy of the model that performs the same classification task. A malicious buyer generates adversarial examples using their copy (referred to as Mi) and uses them to attack other copies of the model. Our objective is to identify the specific copy (Mi) responsible for generating the adversarial examples.To address this problem, we propose a two-stage framework called separate-and-trace. In the model separation stage, we generate multiple copies of the model, each with unique features injected into them. This ensures that adversarial examples generated from each copy possess distinct and traceable features. We achieve this through a parallel structure that pairs a unique tracer with the original classification model in each copy. We also employ a training method based on variational autoencoders (VAEs) to facilitate this process.In the tracing stage, we utilize the adversarial examples and a few candidate models to identify the likely source (Mi). By analyzing the output logits from each tracer, which are influenced by the unique features induced by the tracer, we can effectively trace the potential adversarial copy. Our empirical results demonstrate the feasibility of tracing the origin of adversarial examples, and our proposed framework can be applied to various architectures and datasets.In summary, our research focuses on tracing the source of adversarial attacks in deep neural networks. Through a two-stage framework, we generate copies of the model with unique features and utilize these features to trace the origin of adversarial examples. Our approach shows promising results and can be applied to different models and datasets.