We propose a task called Visual Planning for Assistance (VPA) to advance multi-modal AI assistants capable of guiding users in achieving complex goals. VPA involves creating a plan based on a concise natural language goal and a video of the user's progress. The plan must consider the user's actions so far and relate them to the requirements of the goal. This task requires dealing with long video histories and complex action dependencies.  To address these challenges, we divide VPA into video action segmentation and forecasting. We experiment with formulating the forecasting step as a multi-modal sequence modeling problem, utilizing pre-trained language models as the sequence model. This approach, known as Visual Language Model based Planner (VLaMP), outperforms baseline methods across various metrics that measure plan quality. We also conduct comprehensive ablations to evaluate the individual contributions of language pre-training, visual observations, and goal information.  We have made all the data, model checkpoints, and training code publicly available.