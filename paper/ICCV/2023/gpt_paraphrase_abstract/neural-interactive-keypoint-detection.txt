This study introduces Click-Pose, an interactive key-point detection framework that utilizes user feedback to improve the accuracy and efficiency of 2D keypoint annotation. By combining a neural keypoint detector with user corrections, Click-Pose reduces labeling costs by more than 10 times compared to manual-only annotation. The framework incorporates a pose error modeling strategy that trains the model to reconstruct correct poses using ground truth data and typical pose errors. Additionally, Click-Pose includes an interactive human-feedback loop that allows users to correct predicted keypoints with minimal clicks, facilitating efficient annotation. The effectiveness of Click-Pose is demonstrated through experiments in various scenarios, including in-domain, out-of-domain scenes, and keypoint adaptation tasks. Results show that Click-Pose requires only 1.97 and 6.45 NoC@95 (at precision 95%) on COCO and Human-Art datasets, respectively, reducing efforts by 31.4% and 36.3% compared to the state-of-the-art model (ViT-Pose) with manual correction. Furthermore, Click-Pose outperforms the previous end-to-end model by 1.4 AP on COCO and 3.0 AP on Human-Art datasets even without user clicks.