Recent advancements in video representation learning and pre-trained vision-language models have led to significant improvements in self-supervised video object localization. This study proposes a novel approach that combines slot attention and text assignment to localize objects in videos. The slot attention approach is used to initially locate objects, followed by assigning text to the identified slots. To achieve this, an unsupervised method is employed to extract semantic information from the pre-trained CLIP model. The resulting video object localization is entirely unsupervised, except for the implicit annotation provided by CLIP. Notably, this is the first unsupervised approach that achieves promising results on regular video benchmarks.