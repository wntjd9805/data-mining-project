Video-language pre-trained models have achieved impressive results in video question-answering tasks, but training large-scale video-based models is costly compared to image-based models due to the length of video sequences. In order to address this issue, we propose a method called Tem-Adapter that leverages knowledge from image-based pre-training while considering the differences between image and video domains. Our approach includes a Temporal Aligner and a Semantic Aligner to learn temporal dynamics and complex semantics. Unlike traditional pre-trained knowledge adaptation methods that focus solely on the downstream task, our Temporal Aligner incorporates a language-guided autoregressive task that predicts future states based on historical clues and language guidance. Additionally, we introduce a Semantic Aligner that uses a template to fuse question and answer pairs as event descriptions and learns a Transformer decoder with the entire video sequence as guidance. We evaluate Tem-Adapter and various pre-train transferring methods on two VideoQA benchmarks, and our method demonstrates significant performance improvement. The answer format outputs only the abstraction.