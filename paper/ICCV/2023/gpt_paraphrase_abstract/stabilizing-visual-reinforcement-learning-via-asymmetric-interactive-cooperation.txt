Vision-based reinforcement learning relies on representation encoders to abstract observation states. However, increasing the parameters of convolutional neural networks (CNNs) for supervised computer vision tasks does not benefit reinforcement learning with temporal-difference (TD) losses in complex environments. This is due to the training instability caused by the oscillating self-overfitting of the heavy-optimizable encoder. When forced to fit sensitive TD targets, the parameters experience serious oscillation, leading to uncertain drifting of the latent state space. Consequently, these perturbations affect policy learning. To address this issue, we propose an asymmetric interactive cooperation approach involving a heavy-optimizable encoder and a supportive light-optimizable encoder. This approach combines the highly discriminative capability of the heavy-optimizable encoder with the training stability of the light-optimizable encoder. Additionally, we introduce a greedy bootstrapping optimization to separate visual perturbations from policy learning. This involves training the representation and policy alternately until they are sufficiently trained. We demonstrate the effectiveness of our method in utilizing larger visual models through experiments in the first-person highway driving task in the CARLA and Vizdoom environments.