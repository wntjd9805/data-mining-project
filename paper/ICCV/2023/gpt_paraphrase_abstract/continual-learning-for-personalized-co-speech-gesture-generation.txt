Co-speech gestures play a crucial role in human communication and are important for personalized chat agents to generate. Previous models for gesture generation assumed that data for each speaker is available all at once and in large quantities. However, in practical scenarios, speaker data is received sequentially and in small amounts as the agent personalizes with more speakers, resembling a continual learning paradigm. Although recent works have made progress in adapting to low-resource data, they tend to completely forget the gesture styles of the initial speakers they were trained on. Additionally, prior generative continual learning approaches do not explore the multimodal aspect of this space, resulting in limited research in this area.  This paper explores this new paradigm and introduces C-DiffGAN, an approach that continually learns new speaker gesture styles using only a few minutes of per-speaker data while retaining previously learned styles. Drawing inspiration from previous continual learning works, C-DiffGAN promotes knowledge retention by firstly generating reminiscences of previous low-resource speaker data and then crossmodally aligning to them in order to mitigate catastrophic forgetting. Through standard continual learning measures, we provide quantitative evidence of improved performance and reduced forgetting compared to strong baselines. Moreover, a qualitative user study confirms that our method produces more natural and style-preserving gestures. The code and videos related to this work can be accessed at https://chahuja.com/cdiffgan.