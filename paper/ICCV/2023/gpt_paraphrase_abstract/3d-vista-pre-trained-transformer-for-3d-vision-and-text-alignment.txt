The field of 3D vision-language grounding (3D-VL) aims to connect the 3D physical world with natural language, which is crucial for achieving embodied intelligence. Current models in this field rely on complex modules, auxiliary losses, and optimization techniques, highlighting the need for a simpler and unified model. In this study, we introduce 3D-VisTA, a pre-trained Transformer model for 3D Vision and Text Alignment that can be easily adapted to different tasks. 3D-VisTA utilizes self-attention layers for both single-modal modeling and multi-modal fusion, without the need for task-specific designs. To improve its performance on 3D-VL tasks, we create ScanScribe, a large-scale dataset containing 2,995 RGB-D scans and 278K scene descriptions. 3D-VisTA is pre-trained on ScanScribe using masked language/object modeling and scene-text matching. It achieves state-of-the-art results on various 3D-VL tasks, including visual grounding, dense captioning, question answering, and situated reasoning. Additionally, 3D-VisTA demonstrates strong performance even with limited annotations during task fine-tuning, making it highly data-efficient.