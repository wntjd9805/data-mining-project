This study focuses on the task of temporal grounding in videos, which involves identifying a specific segment of a video that matches a given query description. While there has been progress in temporal grounding for short-form videos, such as those lasting only a few minutes, the task becomes more challenging for long videos that span hours. One common approach is to use a sliding window, but this method is inefficient and inflexible due to the limited number of frames captured within the window.To address this challenge, the authors propose an end-to-end framework for fast temporal grounding in long videos. Their approach allows for modeling an hours-long video with a single network execution, resulting in a highly efficient pipeline. The framework is designed in a coarse-to-fine manner, where context knowledge is first extracted from non-overlapping video clips (anchors), and then supplemented with detailed content knowledge from anchors that strongly respond to the query.One advantage of this approach is its ability to capture long-range temporal correlation by treating the entire video as a whole. This facilitates more accurate temporal grounding. Experimental results on the MAD and Ego4d datasets, which consist of long-form videos, demonstrate that the proposed method outperforms existing state-of-the-art approaches. It achieves significantly higher efficiency, with speed improvements of 14.6× and 102.8× on the respective datasets. The project can be accessed at https://github.com/afcedf/SOONet.git.