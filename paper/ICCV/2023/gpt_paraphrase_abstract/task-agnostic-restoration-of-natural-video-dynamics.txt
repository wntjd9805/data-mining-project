Many video restoration and translation tasks process each frame independently, ignoring the temporal connection between frames. This often leads to temporal inconsistencies in the final output. State-of-the-art techniques that address these inconsistencies require unprocessed videos to restore temporal consistency, which can compromise the translation effect. We propose a framework that learns to infer and utilize consistent motion dynamics from inconsistent videos to reduce temporal flicker while preserving perceptual quality for neighboring and distant frames. Our framework achieves state-of-the-art results on benchmark datasets, DAVIS and videvo.net, processed by various image processing applications. The code and trained models are available at https://github.com/MKashifAli/TARONVD.