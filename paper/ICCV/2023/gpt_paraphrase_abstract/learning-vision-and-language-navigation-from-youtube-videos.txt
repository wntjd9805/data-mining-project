This study focuses on vision-and-language navigation (VLN), which involves an embodied agent navigating in realistic 3D environments using natural language instructions. Current VLN methods have limitations, such as training on small-scale environments and unreasonable path-instruction datasets, which hinders their ability to generalize to unseen environments. Although there are extensive house tour videos on YouTube that provide real navigation experiences and layout information, they have not been explored for VLN purposes. To address this, the study proposes creating a large-scale dataset by extracting reasonable path-instruction pairs from house tour videos and pre-training the agent using this dataset. However, constructing these pairs and extracting layout knowledge from raw and unlabeled videos present challenges. To overcome these challenges, an entropy-based method is used to construct the nodes of a path trajectory, and an action-aware generator is employed to generate instructions from unlabeled trajectories. Additionally, a trajectory judgment pretext task is devised to encourage the agent to extract layout knowledge. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on two popular benchmarks, R2R and REVERIE. The code for this study is available at https://github.com/JeremyLinky/YouTube-VLN.