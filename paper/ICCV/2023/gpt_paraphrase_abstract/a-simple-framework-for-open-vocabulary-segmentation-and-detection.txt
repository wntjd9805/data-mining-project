We introduce OpenSeeD, a framework that combines segmentation and detection datasets to learn jointly. To overcome differences in vocabulary and annotation granularity, we employ a pre-trained text encoder to encode visual concepts and establish a shared semantic space. This approach yields promising results compared to models trained solely on segmentation tasks. However, we identify two discrepancies: task discrepancy (different requirements for segmentation and detection) and data discrepancy (varying spatial granularity of box and mask annotations). To address these issues, we propose decoupled decoding to reduce interference between foreground/background and conditioned mask decoding to generate masks for given boxes. Our model, which incorporates these techniques, is trained on COCO and Objects365 datasets. After pre-training, it demonstrates competitive or superior zero-shot transferability for both segmentation and detection. OpenSeeD outperforms state-of-the-art methods for open-vocabulary instance and panoptic segmentation on five datasets, as well as previous work for open-vocabulary detection on LVIS and ODinW. In specific tasks, our model achieves new state-of-the-art performance for panoptic segmentation on COCO and ADE20K, and instance segmentation on ADE20K and Cityscapes. Notably, OpenSeeD is the first to explore joint training on segmentation and detection, and we hope it serves as a strong baseline for developing a single model for both tasks in the open world. The code will be made available at https://github.com/IDEA-Research/OpenSeeD.