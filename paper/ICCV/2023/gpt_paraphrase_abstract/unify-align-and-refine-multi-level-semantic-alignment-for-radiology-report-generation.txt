Automatic generation of radiology reports has gained significant attention in research due to its potential to reduce the workload of radiologists. However, aligning images with their corresponding reports and matching image patches with keywords remains a challenging task. In this study, we propose a novel approach called Unify, Align and then Refine (UAR) to address this challenge. The UAR approach utilizes three modules: Latent Space Unifier (LSU), Cross-modal Representation Aligner (CRA), and Text-to-Image Refiner (TIR). The LSU module unifies multimodal data into discrete tokens, allowing for the learning of common knowledge across modalities. The CRA module learns discriminative features using orthonormal basis and a dual-gate mechanism, and then aligns visual and textual representations globally using a triplet contrastive loss. The TIR module enhances token-level local alignment by calibrating text-to-image attention with a learnable mask. Additionally, we employ a two-stage training procedure to gradually learn cross-modal alignments at different levels, simulating the workflow of radiologists. Our experiments on the IU-Xray and MIMIC-CXR benchmark datasets demonstrate the superiority of the UAR approach over other state-of-the-art methods.