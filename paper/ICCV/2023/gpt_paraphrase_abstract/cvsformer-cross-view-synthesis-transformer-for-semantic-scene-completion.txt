Semantic scene completion (SSC) involves accurately understanding the geometric and semantic relationships between objects in a 3D scene in order to reason about occluded objects. Existing SSC methods use voxelization of 3D objects, allowing deep 3D convolutional networks (3DCNNs) to learn these object relationships. However, current networks lack controllable kernels to model object relationships across multiple views, where relevant views provide information about occluded objects.   In this research, we propose a solution called Cross-View Synthesis Transformer (CVSformer), which consists of Multi-View Feature Synthesis and Cross-View Transformer. In the Multi-View Feature Synthesis, we use a set of 3D convolutional kernels with different rotations to compute multi-view features for each voxel. The Cross-View Transformer employs cross-view fusion to comprehensively learn cross-view relationships, which provide valuable information for enhancing individual view features. These enhanced features are then used to predict the geometric occupancies and semantic labels of all voxels.   We evaluated CVSformer on public datasets and achieved state-of-the-art results. The code for CVSformer is available at https://github.com/donghaotian123/CVSformer.