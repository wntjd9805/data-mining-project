We present a new tracking framework called Aba-ViTrack, which utilizes pure vision transformer models (ViTs) for UAV tracking. While discriminative correlation filters (DCF)-based trackers and lightweight convolutional neural network (CNN)-based trackers are commonly used in UAV tracking, the potential of ViTs has not been explored in this context. ViTs have been shown to outperform CNNs in image classification tasks. Our proposed Aba-ViTrack integrates feature learning and template-search coupling into an efficient one-stream ViT, eliminating the need for a heavy relation modeling module. Additionally, we introduce an adaptive and background-aware token computation method in Aba-ViT to reduce inference time. This method discards tokens based on learned probabilities, prioritizing background tokens over target tokens. Extensive experiments on six UAV tracking benchmarks demonstrate that Aba-ViTrack achieves state-of-the-art performance in UAV tracking. The code for Aba-ViTrack is available at https://github.com/xyyang317/Aba-ViTrack.