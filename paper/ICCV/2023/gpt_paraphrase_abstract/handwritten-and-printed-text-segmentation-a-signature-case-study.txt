Handwritten text overlapping with printed text poses challenges during the optical character recognition (OCR) and digitization process of documents, impacting downstream NLP tasks. Previous studies have focused on either binary classification of handwritten text or three-class segmentation (handwritten, printed, and background pixels). However, these approaches assign overlapping pixels to only one class, neglecting the other. To overcome this limitation, our research introduces innovative methods for segmenting handwritten and printed text. Our goal is to accurately identify and separate text from different classes, with a particular emphasis on improving segmentation performance for overlapping sections. To facilitate this task, we present a new dataset called SignaTR6K, which comprises real legal documents. Additionally, we propose a novel model architecture specifically designed for handwritten and printed text segmentation. Our best configuration outperforms previous studies on two different datasets, achieving a 17.9% and 7.3% improvement in IoU scores. The SignaTR6K dataset can be downloaded from this link: https://forms.office.com/r/2a5RDg7cAY.