The current focus in developing more efficient vision transformers is on reducing the computational burden of self attention modules by adopting sparse attention or local attention windows. In contrast, we propose a different approach to enhance the performance of transformer-based architectures by densifying the attention pattern. Our method, called forward cross attention for hybrid vision transformer (FcaFormer), utilizes tokens from previous blocks in the same stage to improve performance. This is achieved through two key components: learnable scale factors (LSFs) and a token merge and enhancement module (TME). The LSFs enable efficient processing of cross tokens, while the TME generates representative cross tokens. By integrating these components, FcaFormer enhances the interactions of tokens across blocks with different semantics and promotes greater information flow to lower levels. We have designed a series of FcaFormer models based on forward cross attention that strike the optimal balance between model size, computational cost, memory cost, and accuracy. For instance, our FcaFormer achieves an 83.1% top-1 accuracy on Imagenet with only 16.3 million parameters and approximately 3.6 billion MACs, without requiring knowledge distillation for training. This represents a nearly 50% reduction in parameters and computational costs, while outperforming distilled EfficientFormer by 0.7% in terms of accuracy. The code for FcaFormer is available at https://github.com/hkzhang-git/FcaFormer.