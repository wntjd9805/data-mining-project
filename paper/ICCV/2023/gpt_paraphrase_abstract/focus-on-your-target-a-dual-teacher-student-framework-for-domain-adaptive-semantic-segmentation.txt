We examine unsupervised domain adaptation (UDA) for semantic segmentation. Currently, the prevalent approach for UDA involves self-training, which grants the model two capabilities: (i) acquiring reliable semantics from labeled images in the source domain, and (ii) adapting to the target domain by generating pseudo labels for unlabeled images. We discover that adjusting the proportion of training samples from the target domain strengthens or weakens the "learning ability" while the "adapting ability" moves in the opposite direction, indicating a conflict between these abilities, particularly for a single model. To address this issue, we propose a new framework called dual teacher-student (DTS) and equip it with a bidirectional learning strategy. By increasing the proportion of target-domain data, the second teacher-student model learns to prioritize the target domain while the first model remains unaffected. DTS can be easily integrated into existing self-training approaches. In a standard UDA situation involving training on synthetic labeled data and real unlabeled data, DTS consistently outperforms the baselines and achieves new state-of-the-art results of 76.5% and 75.1% mIoUs on GTAv→Cityscapes and SYNTHIA→Cityscapes, respectively. The implementation can be accessed at https://github.com/xinyuehuo/DTS.