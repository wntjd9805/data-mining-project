In the task of incremental object detection (IOD), it is necessary for a model to continuously learn from newly added data. However, directly fine-tuning a well-trained detection model for a new task can lead to a significant decrease in performance on old tasks, which is known as catastrophic forgetting. Previous approaches, such as knowledge distillation, have been effective in mitigating this issue by transferring information from the old model to the new one. However, existing methods in feature distillation primarily focus on low-level feature information and do not adequately consider the importance of high-level semantic information.In this study, we identify the cause of catastrophic forgetting in the IOD task as the destruction of the semantic feature space. To address this, we propose a novel method that dynamically distills both semantic and feature information using a Transformer-based detector. Our method preserves between-class discriminativeness by distilling class-level semantic distance and feature distance among different categories. Additionally, it maintains within-class consistency by distilling instance-level semantic information and feature information within each category.We conduct extensive experiments on the Pascal VOC and MS COCO benchmarks to evaluate the performance of our method. The results demonstrate that our approach outperforms all previous state-of-the-art CNN-based methods under various experimental scenarios. Notably, we achieve a remarkable mean average precision (mAP) improvement from 36.90% to 39.80% in a one-step IOD task.To visually illustrate the effect of our method, we provide a visualization of the semantic feature space of old categories before and after adding new categories. The figures demonstrate that catastrophic forgetting leads to a loss of within-class consistency and between-class discriminativeness. However, when using our method, the semantic feature space maintains these important characteristics, as evidenced by the bottom three figures in the visualization.In conclusion, our proposed method effectively mitigates catastrophic forgetting in the IOD task by preserving both within-class consistency and between-class discriminativeness. The experimental results demonstrate the superiority of our approach compared to previous methods, highlighting its potential for incremental learning in object detection.