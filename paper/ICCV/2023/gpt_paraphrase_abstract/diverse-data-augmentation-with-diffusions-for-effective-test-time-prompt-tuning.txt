Recent years have seen the emergence of pre-trained vision-language models, such as CLIP, that have shown promising performance on various downstream tasks. This paper focuses on the concept of test-time prompt tuning (TPT), which involves learning adaptive prompts for each test sample from a new domain. Existing TPT methods rely on data augmentation and confidence selection, but these techniques have limitations. Traditional data augmentation techniques lack data diversity, while entropy-based confidence selection alone does not guarantee accurate predictions. To address these issues, a new TPT method called DiffTPT is proposed. DiffTPT utilizes pre-trained diffusion models to generate diverse and informative data. It incorporates augmented data from both conventional methods and pre-trained stable diffusion to enhance the model's adaptability to unknown test data. Additionally, a cosine similarity-based filtration technique is introduced to ensure the fidelity of the generated data by selecting data that is more similar to the single test sample. Experimental results on test datasets with distribution shifts and unseen categories demonstrate that DiffTPT improves zero-shot accuracy by an average of 5.13% compared to the current state-of-the-art TPT method.