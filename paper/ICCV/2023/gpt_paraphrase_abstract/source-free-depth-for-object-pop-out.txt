Depth cues are important for visual perception, but directly measuring depth is often impractical. However, modern learning-based methods can infer depth maps from images taken in natural environments. In this study, we utilize these depth inference models to perform object segmentation based on the objects' "pop-out" prior in 3D. This prior assumes that objects are located on the background surface, allowing us to reason about objects in 3D space. By adapting the inferred depth maps, we can localize objects using only 3D information. To achieve this separation, we rely on weak supervision from the segmentation mask to learn about the contact surface between objects and the background. This intermediate representation of the contact surface enables us to reason about objects purely in 3D, improving the transfer of depth knowledge into semantics. Our adaptation method only requires the depth model and does not rely on the source data used for training, making the learning process efficient and practical. We conducted experiments on eight datasets for salient object detection and camouflaged object detection, consistently demonstrating the advantages of our method in terms of performance and generalizability. The source code for our method is publicly available at https://github.com/Zongwei97/PopNet.