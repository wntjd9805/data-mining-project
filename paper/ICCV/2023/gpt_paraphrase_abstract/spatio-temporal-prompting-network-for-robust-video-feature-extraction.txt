The degradation of frame quality is a significant challenge in video understanding. Recent approaches have used transformer-based integration modules to compensate for the loss of information in deteriorated frames. However, these modules are heavy and complex, and they are tailored specifically for each task, making generalization difficult. In this paper, we propose a unified framework called Spatio-Temporal Prompting Network (STPN) that efficiently extracts robust and accurate video features by dynamically adjusting input features in the backbone network. STPN predicts video prompts containing spatio-temporal information from neighboring frames, which are then added to the patch embeddings of the current frame to update the input for video feature extraction. Additionally, STPN can be easily applied to various video tasks as it does not contain task-specific modules. Despite its simplicity, STPN achieves state-of-the-art performance on widely-used datasets for video object detection, video instance segmentation, and visual object tracking. The code is available at https://github.com/guanxiongsun/STPN.