Contrastive Language-Image Pretraining has become a popular method for training vision and text encoders using uncurated image-text pairs from the web. Recent efforts have introduced additional supervision terms to improve data efficiency by using randomly augmented views of the image. However, as the image augmentation process is unaware of the corresponding text, it can lead to misalignments between the image and text during training. Previous approaches either ignored this discrepancy or used external models to mitigate the impact of misalignments. In contrast, we propose a new metric learning approach called "Misalign, Contrast then Distill (MCD)" that leverages these misalignments as an additional training source. Unlike previous methods that treated augmented images and their text counterparts as simple positive pairs, MCD predicts the degree of misalignment caused by the augmentation. Our extensive experiments demonstrate that MCD achieves state-of-the-art transferability in multiple classification and retrieval tasks.