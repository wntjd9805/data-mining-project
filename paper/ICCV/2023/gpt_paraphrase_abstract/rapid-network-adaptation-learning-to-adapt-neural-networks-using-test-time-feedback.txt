We present a novel approach for adapting neural networks to changes in data distribution during testing. Instead of relying on robustness mechanisms during training, we propose a closed-loop system that utilizes feedback signals during testing to adapt the network in real-time. We achieve this by implementing a learning-based function that acts as an optimized for the network. Our proposed method, called Rapid Network Adaptation (RNA), is highly flexible and significantly faster compared to existing methods. We conduct extensive experiments using various adaptation signals, target tasks, datasets, and distribution shifts, demonstrating the generality, efficiency, and flexibility of our approach. Our evaluations across different datasets and tasks, such as depth estimation, optical flow, semantic segmentation, and classification, as well as distribution shifts like cross-datasets and common corruptions in both 2D and 3D, show promising results.