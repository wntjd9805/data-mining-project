The task of image segmentation, which involves separating different objects in images based on textual descriptions, has numerous applications in computer vision. However, manually labeling training data for this task is extremely expensive, resulting in a shortage of labeled data for training purposes. In order to address this problem, we propose a weakly supervised learning approach that relies solely on text descriptions of training images for guidance.To achieve this, we introduce a novel model that identifies semantic entities within input images and combines these entities to predict the mask of the referred object based on the given text query. Additionally, we propose a new loss function that enables training of the model without any further supervision. Our approach was evaluated on four publicly available benchmarks for referring image segmentation, and it consistently outperformed existing methods for the same task as well as recent open-vocabulary segmentation models across all benchmarks.