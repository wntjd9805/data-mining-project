Recent advancements in text-to-image generation models have shown impressive capabilities in producing high-quality images. However, most of the research has focused on synthesizing images using only text prompts. Although some studies have explored the use of other modalities as conditions, this often necessitates a large amount of paired data, such as box/mask-image pairs, and extensive fine-tuning time to train the models effectively. Acquiring such paired data is time-consuming, labor-intensive, and limited to a closed set, which poses a potential bottleneck for applications in an open world.  To address this issue, this study proposes a training-free approach to control objects and contexts in synthesized images based on simple user-provided conditions, such as a box or scribble. The aim is to overcome the aforementioned challenges by integrating three spatial constraints – Inner-Box, Outer-Box, and Corner Constraints – into the denoising step of diffusion models. Importantly, this integration requires no additional training or massive amounts of annotated layout data.  Extensive experiments have been conducted to evaluate the effectiveness of the proposed constraints. The results demonstrate that these constraints enable precise control over the content and location of objects in the synthesized images while maintaining the ability of diffusion models to generate diverse concepts with high fidelity. This approach offers a promising solution for generating images that adhere to specific spatial conditions without the need for extensive training or paired data.