This study presents Scenimefy, a new framework for automatically rendering high-quality anime scenes from complex real-world images. Previous attempts have struggled to produce satisfactory results in terms of consistent semantic preservation, evident stylization, and fine details. Scenimefy addresses these challenges by employing a semi-supervised image-to-image translation approach. It utilizes structure-consistent pseudo paired data derived from a semantic-constrained StyleGAN, which leverages model priors like CLIP. Additionally, segmentation-guided data selection is applied to obtain high-quality pseudo supervision. The framework also incorporates a patch-wise contrastive style loss to enhance stylization and fine details. Furthermore, a new high-resolution anime scene dataset is contributed to support future research in this area. Extensive experiments demonstrate the superiority of Scenimefy over existing baselines in terms of both perceptual quality and quantitative performance. More information about the project can be found at https://yuxinn-j.github.io/projects/Scenimefy.html.