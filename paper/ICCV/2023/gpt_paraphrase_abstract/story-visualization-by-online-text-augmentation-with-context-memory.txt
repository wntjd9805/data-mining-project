The task of story visualization (SV), which involves generating images from text descriptions, is challenging due to the need to both capture visual details and encode long-term context. Previous approaches have mainly focused on generating relevant images for individual sentences, making it difficult to generate contextually convincing images that align with the entire paragraph. In this study, we propose a new memory architecture for the Bi-directional Transformer framework. We also introduce online text augmentation, which generates multiple pseudo-descriptions to provide additional supervision during training and improve generalization to variations in language during inference. Our method outperforms existing approaches on popular SV benchmarks, such as Pororo-SV and Flintstones-SV, achieving better scores in metrics like FID, character F1, frame accuracy, BLEU-2/3, and R-precision. Importantly, our method achieves these results with similar or lower computational complexity.