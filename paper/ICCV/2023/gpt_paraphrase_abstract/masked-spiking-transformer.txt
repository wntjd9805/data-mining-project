The combination of Spiking Neural Networks (SNNs) and Transformers has garnered considerable attention due to their potential for high energy efficiency and performance. However, current research in this area often relies on direct training, which can result in suboptimal performance. To address this, we propose leveraging the benefits of the ANN-to-SNN conversion method to merge SNNs and Transformers, leading to significantly enhanced performance compared to existing SNN models. Additionally, inspired by the synaptic failures observed in the nervous system, which reduce the transmission of spikes across synapses, we introduce a novel framework called Masked Spiking Transformer (MST). This framework incorporates a Random Spike Masking (RSM) technique to eliminate redundant spikes and decrease energy consumption without compromising performance. Our experiments demonstrate that the proposed MST model achieves a noteworthy 26.8% reduction in power consumption when the masking ratio is 75%, while maintaining the same level of performance as the unmasked model. The code can be accessed at: https://github.com/bic-L/Masked-Spiking-Transformer.