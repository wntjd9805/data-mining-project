Vision-language cross-modal learning schemes have significantly improved the performance of text-video retrieval. However, existing solutions often only align global video-level and sentence-level features, neglecting the intrinsic video-text relations. This means that a text description may only correspond to a specific part of the video, both spatially and temporally. To address this issue, we propose a text-video learning framework that incorporates progressive spatio-temporal prototype matching.  Our framework consists of two phases: object-phrase prototype matching and event-sentence prototype matching. In the object-phrase prototype matching phase, we generate spatial prototypes by predicting key patches or words. These prototypes are then aggregated into object or phrase prototypes. By optimizing the local alignment between these prototypes, our model is able to capture fine-grained spatial details.  In the event-sentence prototype matching phase, we introduce a temporal prototype generation mechanism. This mechanism associates intra-frame objects and captures inter-frame temporal relations. The progressively generated event prototypes provide a diverse representation of semantic events in videos, enabling dynamic matching.  Through comprehensive experiments, we have validated the effectiveness of our method. Our approach consistently outperforms state-of-the-art methods on four video retrieval benchmarks.