Anomaly detection is a critical task in machine learning that involves identifying abnormal samples in test data by learning patterns from a set of normal training samples. However, existing anomaly detection methods assume that the training and test data come from the same distribution, which is not always the case in real-world applications. This is because test data can have significant distribution shifts due to natural variations such as lighting conditions, object poses, or background appearances. As a result, current anomaly detection methods are ineffective in handling such cases.In this study, we address the problem of anomaly detection under distribution shift and establish performance benchmarks on four widely-used datasets for generalization and out-of-distribution tasks. We find that simply adapting state-of-the-art methods for out-of-distribution generalization to anomaly detection settings does not work effectively due to the lack of labeled anomaly data.To overcome this challenge, we propose a novel robust approach for anomaly detection that can handle diverse distribution shifts. Our method minimizes the distribution gap between in-distribution and out-of-distribution normal samples in both the training and inference stages in an unsupervised manner. We conduct extensive empirical experiments on the four datasets and demonstrate that our approach significantly outperforms existing anomaly detection methods and out-of-distribution generalization methods when dealing with data containing various distribution shifts. Moreover, our method maintains high detection accuracy on in-distribution data.We have made our code and data available at https://github.com/mala-lab/ADShift.