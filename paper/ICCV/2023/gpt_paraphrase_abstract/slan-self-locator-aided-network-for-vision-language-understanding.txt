Learning the complex relationship between vision and language is crucial for achieving accurate results in vision-language tasks. However, extracting relevant image regions based on textual information for semantic alignment remains a challenge. Existing methods either rely on inefficient region proposal modules or require scarce grounding data to pre-train detectors, limiting their effectiveness. To address these limitations, we propose the Self-LocatorAided Network (SLAN), which eliminates the need for additional gold data. SLAN includes a region filter and a region adaptor that work together to identify and update key regions of interest based on textual guidance. By establishing detailed region-word alignments, SLAN can be applied to various downstream tasks. In evaluations, SLAN achieves competitive performance in five vision-language understanding tasks, surpassing previous state-of-the-art methods. Moreover, it demonstrates strong zero-shot and fine-tuned transferability in two localization tasks. The code for SLAN is publicly available at https://github.com/scok30/SLAN.