The Contrastive Language-Image Pre-training (CLIP) technique has gained popularity for various vision tasks. However, existing methods for improving its performance through few-shot learning are either limited or have too many learnable parameters. In this paper, we propose APE, an Adaptive Prior refinement method for CLIP's pre-trained knowledge, which achieves better accuracy with efficient computation. By refining the prior knowledge using a module, we analyze the differences between classes in the downstream data and separate the domain-specific knowledge from the CLIP-extracted cache model. We introduce two variants of the model, a training-free APE and a training-required APE-T. We leverage the relationships between the test image, prior cache model, and textual representations, and only train a lightweight category-residual module. Both APE and APE-T achieve state-of-the-art performance, surpassing the second-best method by +1.59% and +1.99% respectively, using 16 shots and significantly fewer learnable parameters. The code for APE is available at https://github.com/ yangyangyang127/APE.