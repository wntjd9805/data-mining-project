This paper focuses on developing rotation-equivariant neural networks for 4D panoptic segmentation in autonomous driving. The task involves recognizing semantic classes and object instances on the road using LiDAR scans, as well as assigning consistent IDs to instances over time. The authors propose that the driving scenario's symmetry to rotations on the ground plane can be leveraged for better generalization and robust feature learning. They review object instance clustering strategies and reframe the centerness-based and offset-based approaches as predictions of invariant scalar fields and equivariant vector fields. They also design invariant and equivariant layers to address other sub-tasks. The authors evaluate their equivariant models on the SemanticKITTI benchmark and demonstrate that they achieve higher accuracy with lower computational costs compared to non-equivariant models. Additionally, their method outperforms existing approaches and secures the top position on the SemanticKITTI 4D Panoptic Segmentation leaderboard.