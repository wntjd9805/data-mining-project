This study focuses on the problem of side-view depth estimation for autonomous vehicles. While most research in 3D perception for autonomous vehicles has focused on frontal-view information, it has been found that more than half of fatal accidents occur due to side impacts. To address this issue, the study investigates the use of monocu-lar ﬁsheye cameras, which provide wide Field of View (FoV) information. However, ﬁsheye cameras mainly observe road areas, resulting in severe distortion on object areas like vehicles or pedestrians. To overcome this, the study proposes a new ﬁsheye depth estimation network called SlaBins. The network infers an accurate and dense depth map by utilizing a geometric property of road environments, which is that most objects are standing (orthogonal) on the road. The study introduces a slanted multi-cylindrical image (MCI) representation, which allows the description of distance as a radius to a cylindrical layer orthogonal to the ground, regardless of the camera viewing direction. Based on this representation, the study estimates a set of adaptive bins and a per-pixel probability map for depth estimation. By combining this with the estimated slanted angle of the viewing direction, the network directly infers a dense and accurate depth map for ﬁsheye cameras. Experimental results show that SlaBins outperforms existing methods in both qualitative and quantitative evaluation on the SynWoodScape and KITTI-360 depth datasets. More information about the project can be found at https://syniez.github.io/SlaBins/.