This paper presents a novel approach to reconstructing the shape and appearance of an object in the real world, as well as the surrounding illumination, based on 2D images. The proposed method combines neural-based object reconstruction with physics-based inverse rendering (PBIR) to achieve accurate and efficient results.   The pipeline begins by using a neural SDF (Signed Distance Function) to reconstruct the shape of the object, which may not be perfect but produces high-quality results. Next, a neural material and lighting distillation stage is introduced to predict the material properties and illumination of the object.   In the final stage, the initial predictions from the neural stages are refined using PBIR to obtain a final reconstruction of the object's shape, material, and illumination. The experimental results demonstrate that the proposed pipeline outperforms existing methods in terms of both quality and performance. The code for this pipeline can be found at https://neural-pbir.github.io/.