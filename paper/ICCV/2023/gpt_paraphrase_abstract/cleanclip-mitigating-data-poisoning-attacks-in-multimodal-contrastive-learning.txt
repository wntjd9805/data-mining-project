Multimodal contrastive pretraining has been widely used to train models like CLIP, but it has been found that these models are susceptible to backdoor attacks. Injecting just a few poisoned examples can manipulate the model's behavior by creating false associations between a backdoor trigger and the target label. To address this issue, we propose CleanCLIP, a finetuning framework that weakens these spurious associations by independently realigning the representations of each modality. By combining multimodal contrastive and unimodal self-supervised objectives, we can significantly reduce the impact of backdoor attacks. Moreover, supervised finetuning on labeled image data can completely remove the backdoor trigger from the vision encoder. Our empirical results demonstrate that CleanCLIP maintains performance on normal examples while effectively erasing various backdoor attacks in multimodal contrastive learning. The code and pre-trained checkpoints are available at https://github.com/nishadsinghi/CleanCLIP.