Recent advancements in Neural Radiance Fields (NeRF) have resulted in impressive improvements in high-quality view synthesis. However, NeRF's rendering efficiency is hindered by the need for multiple network forwardings to render a single pixel. Existing approaches to address this issue either reduce the required samples or optimize the implementation to accelerate network forwarding. Nevertheless, the problem of multiple sampling persists due to the inherent representation of radiance fields. In contrast, Neural Light Fields (NeLF) tackle this problem by querying only one network forwarding per pixel, but at the cost of requiring significantly larger network capacities to achieve comparable visual quality to NeRF. In this study, we propose a new representation called Neural Radiance Distribution Field (NeRDF) that aims to enable efficient real-time view synthesis. Our approach utilizes a small network similar to NeRF while maintaining rendering speed with a single network forwarding per pixel, similar to NeLF. We accomplish this by modeling the radiance distribution along each ray using frequency basis and predicting frequency weights using the network. Pixel values are then computed through volume rendering on radiance distributions. Experimental results demonstrate that our proposed method offers a better balance between speed, quality, and network size compared to existing methods. Specifically, we achieve a speed-up of approximately 254 times compared to NeRF, while maintaining a similar network size and only experiencing a marginal decline in performance. Further details and results can be found on our project page at yushuang-wu.github.io/NeRDF.