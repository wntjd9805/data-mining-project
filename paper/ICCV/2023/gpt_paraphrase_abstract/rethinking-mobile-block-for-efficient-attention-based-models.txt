This paper focuses on the development of lightweight models for dense predictions, with a focus on trading off parameters, FLOPs (floating-point operations per second), and performance. The Inverted Residual Block (IRB) is commonly used in lightweight Convolutional Neural Networks (CNNs), but attention-based studies have not recognized a counterpart for this. In this work, we propose a new perspective by combining the efficient IRB infrastructure with effective components of the Transformer model. This allows us to extend the CNN-based IRB to attention-based models and abstract a lightweight MetaMobile Block (MMB) for model design. By following a simple yet effective design criterion, we derive a modern Inverted Residual Mobile Block (iRMB) and use it to build a ResNet-like Efficient Model (EMO) for downstream tasks. Our extensive experiments on ImageNet-1K, COCO2017, and ADE20K benchmarks demonstrate that our EMO outperforms state-of-the-art methods. For example, our EMO-1M/2M/5M achieves Top-1 accuracies of 71.5, 75.1, and 78.4, respectively, surpassing equal-order CNN-/Attention-based models. Additionally, our EMO trades off parameters, efficiency, and accuracy well, running 2.8-4.0 times faster than EdgeNeXt on the iPhone14.