Remote photoplethysmography (rPPG) is an area of research that focuses on non-invasively monitoring vital signs using cameras. Previous studies have proposed supervised methods for this purpose, but recent research has shifted towards contrastive-based self-supervised methods. However, these methods often struggle to handle interferences like head motions, facial dynamics, and video compression, resulting in the learning of irrelevant periodicities. To overcome this limitation, we propose two improvements. Firstly, we enhance the self-supervised learning process by introducing more reliable and explicit contrastive constraints. Secondly, we propose a novel learning strategy that seamlessly combines self-supervised constraints with pseudo-supervisory signals from traditional unsupervised methods. Additionally, we develop a co-rectification technique to mitigate the negative effects of noisy pseudo-labels. Experimental results demonstrate the superiority of our methodology compared to representative models when applied to small, high-quality datasets like PURE and UBFC-rPPG. Notably, our method outperforms prevailing self-supervised techniques and achieves remarkable alignment with state-of-the-art supervised methods on large-scale challenging datasets like VIPL-HR and V4V, all without requiring any annotation cost.