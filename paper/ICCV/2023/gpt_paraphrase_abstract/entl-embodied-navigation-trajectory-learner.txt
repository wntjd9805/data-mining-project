We present the Embodied Navigation Trajectory Learner (ENTL), a technique for extracting extended sequence representations in the field of embodied navigation. Our method combines world modeling, localization, and imitation learning into a single task of sequence prediction. By training our model with vector-quantized predictions of future states based on current states and actions, we achieve competitive performance on navigation tasks using significantly less data compared to strong baselines. Additionally, our ENTL model can perform auxiliary tasks such as localization and future frame prediction, which serves as a proxy for world modeling. Importantly, our approach does not rely on explicit reward signals during pre-training, making the resulting model applicable to multiple tasks and environments. The code for our method is available at https://github.com/klemenkotar/ENTLFigure 1. Our proposed architecture allows for the extraction of comprehensive sequence representations in the context of embodied navigation. This architecture facilitates the sharing of a spatio-temporal transformer-based backbone across various tasks, including navigation, localization, and future frame prediction.