The success of Vision Transformers (ViTs) is often attributed to the multi-head self-attention (MSA) mechanism that allows for global interactions at each layer. Previous studies have acknowledged the importance of long-range dependency in MSA's effectiveness. In this research, we investigate the role of MSA in terms of density, specifically focusing on the spatial interactions of learned attention maps. Surprisingly, our initial analysis reveals that these attention maps exhibit dense interactions rather than sparse ones. This goes against the norm as dense attention maps are more challenging for the model to learn due to softmax. We interpret this unusual behavior as a strong preference of ViT models for dense interactions. To address this, we introduce a method called Context Broadcasting (CB), where we manually incorporate dense uniform attention into each layer of the ViT models. Our study demonstrates that CB serves as a substitute for dense attention, reducing the density of the original attention maps while still adhering to softmax in MSA. Furthermore, we show that the inclusion of CB leads to increased capacity and generalizability of the ViT models, without incurring significant costs (only requiring 1 line of code and no additional parameters).