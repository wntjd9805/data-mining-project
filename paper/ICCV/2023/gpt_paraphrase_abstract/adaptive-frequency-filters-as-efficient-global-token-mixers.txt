Recent advancements in vision transformers, large-kernel CNNs, and MLPs have been successful in various vision tasks by effectively fusing information at a global level. However, deploying these models efficiently, particularly on mobile devices, presents significant challenges due to the computational costs associated with self-attention mechanisms, large kernels, and fully connected layers. In this study, we address these challenges by applying the conventional convolution theorem to deep learning and discover that adaptive frequency filters can serve as efficient global token mixers. Based on this insight, we introduce the Adaptive Frequency Filtering (AFF) token mixer, a neural operator that transfers a latent representation to the frequency domain using a Fourier transform and performs semantic-adaptive frequency filtering through elementwise multiplication. This operation is mathematically equivalent to token mixing in the original latent space, with a dynamically sized convolution kernel matching the spatial resolution of the latent representation. We leverage AFF token mixers as the primary neural operators to construct a lightweight neural network called AFFNet. Through extensive experiments, we demonstrate the effectiveness of our proposed AFF token mixer and show that AFFNet achieves superior accuracy and efficiency trade-offs compared to other lightweight network designs in various visual tasks, including visual recognition and dense prediction tasks. The code for our approach is publicly available at https://github.com/microsoft/TokenMixers.