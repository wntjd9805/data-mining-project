Monocular depth estimation, a challenging task due to the lack of information in 2D images, differs from other tasks like classification and segmentation. Existing backbones borrowed from other tasks do not effectively handle different environmental information, leading to limited depth accuracy. To address this, we propose a Direction-aware Cumulative Convolution Network (DaCCN) that enhances depth feature representation. Our approach includes a direction-aware feature extraction module that adjusts feature representation in each direction, enabling the encoding of various information types. Additionally, we introduce a cumulative convolution technique to efficiently aggregate important environmental information. Experimental results on popular benchmarks (KITTI, Cityscapes, and Make3D) demonstrate significant improvements, establishing our method as the new state-of-the-art with all three types of self-supervision. The implementation code is available at https://github.com/wencheng256/DaCCN.