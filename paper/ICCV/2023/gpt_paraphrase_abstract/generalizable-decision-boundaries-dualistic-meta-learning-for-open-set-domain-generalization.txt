Domain generalization (DG) is a technique used to address the problem of domain shift, which occurs when there are statistical differences between the source and target domains. However, current methods often fail to consider a common scenario where the source and target domains have different classes. To address this limitation, open set domain generalization (OSDG) has emerged as a more practical approach for recognizing unseen classes in unseen domains. One intuitive approach is to use multiple one-vs-all classifiers to define decision boundaries for each class and reject outliers as unknown. However, this approach is often biased towards positive class boundaries due to significant class imbalance, leading to misclassification of known samples in the unseen target domain. In this paper, we propose a new meta-learning-based framework called MEDIC (dualistic MEta-learning with joint DomaIn-Class matching) that simultaneously considers gradient matching for inter-domain and inter-class splits to achieve a balanced and generalizable boundary for all tasks. Experimental results demonstrate that MEDIC outperforms previous methods in open set scenarios and maintains competitive close set generalization ability. The code for MEDIC is available at https://github.com/zzwdx/MEDIC.