Learning discriminative features from a small number of labeled samples for identifying new classes in skeleton-based action recognition has gained attention. Previous approaches focus on learning action-specific embeddings using intra-skeleton or inter-skeleton spatial associations, resulting in less discriminative representations. To overcome these shortcomings, we propose the Parallel Attention Interaction Network (PAINet), which incorporates two complementary branches to strengthen the match by considering both inter-skeleton and intra-skeleton correlations. Our approach includes a topology encoding module that utilizes topology and physical information to improve the modeling of interactive parts and joint pairs in both branches. In the Cross Spatial Alignment branch, we employ a spatial cross-attention module to establish joint associations across sequences and introduce a directional Average Symmetric Surface Metric to locate the closest temporal similarity. Simultaneously, the Cross Temporal Alignment branch incorporates a spatial self-attention module to aggregate spatial context within sequences and a temporal cross-attention network to correct misalignment temporally and calculate similarity. Extensive experiments conducted on three skeleton benchmarks (NTU-T, NTU-S, and Kinetics) demonstrate the effectiveness of our framework, consistently outperforming state-of-the-art methods.