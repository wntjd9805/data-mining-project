This study introduces a new approach to self-supervised learning (SSL) that incorporates variational inference to not only learn representations but also representation uncertainties. SSL is a method of learning representations without labels by maximizing the similarity between different augmented views of an image. On the other hand, variational autoencoder (VAE) is an unsupervised representation learning method that uses variational inference to train a probabilistic generative model. While both VAE and SSL can learn representations without labels, their relationship has not been explored previously. This study clarifies the theoretical relationship between SSL and variational inference and proposes a novel method called variational inference SimSiam (VI-SimSiam). VI-SimSiam leverages variational inference to interpret SimSiam and define the distribution of the latent space, allowing it to predict representation uncertainty. Experimental results demonstrate that VI-SimSiam can effectively learn uncertainty by comparing input images and predicted uncertainties. Furthermore, the study presents a relationship between estimated uncertainty and classification accuracy.