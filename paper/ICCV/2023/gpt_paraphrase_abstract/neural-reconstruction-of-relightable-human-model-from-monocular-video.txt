Creating cost-effective relightable and animatable human characters from monocular video is a crucial task for digital human modeling and virtual reality applications. However, this task is complex due to the intricate articulation motion, wide range of ambient lighting conditions, and pose-dependent clothing deformations. In this paper, we present a new self-supervised framework that utilizes a monocular video of a moving human to generate a 3D neural representation capable of being rendered with different poses under any lighting conditions. Our framework decomposes dynamic humans into neural fields in a canonical space, considering geometry and spatially varying BRDF material properties. Moreover, we introduce pose-driven deformation fields that enable mapping between canonical space and observation. By leveraging the proposed appearance decomposition and deformation fields, our framework learns in a self-supervised manner. Ultimately, through pose-driven deformation, recovered appearance, and physically-based rendering, the reconstructed human figure becomes relightable and can be controlled by novel poses. We demonstrate significant performance improvements compared to previous methods and showcase compelling examples of relighting from monocular videos of moving humans in challenging, uncontrolled capture scenarios.