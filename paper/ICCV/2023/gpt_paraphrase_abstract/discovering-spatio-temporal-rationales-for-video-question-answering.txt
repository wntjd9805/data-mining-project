This paper aims to address the challenges of complex video question answering (VideoQA) tasks, which involve analyzing long videos with multiple objects and events occurring at different times. The key to overcoming this challenge is to identify the crucial moments and objects related to the questions asked. To achieve this, the authors propose a Spatio-Temporal Rationalization (STR) method, which is a selection module that adaptively identifies the question-critical moments and objects through cross-modal interaction. These identified moments and objects serve as grounded rationales to support the reasoning behind the answers. Building upon the STR method, the authors introduce TranSTR, a neural network architecture inspired by Transformers. TranSTR utilizes STR as its core component and introduces a novel answer interaction mechanism to coordinate the decoding of answers. Experimental results on four datasets demonstrate that TranSTR achieves state-of-the-art performance, particularly on the NExT-QA and Causal-VidQA datasets, where it outperforms the previous state-of-the-art by 5.8% and 6.8% respectively. The authors also conduct extensive studies to validate the importance of STR and the proposed answer interaction mechanism. The success of TranSTR and the comprehensive analysis provided in this paper are expected to inspire further research and advancements in the field of complex VideoQA. The authors plan to release the code for TranSTR at the specified GitHub repository.