This paper presents a novel approach to video compression called model-based video compression (MVC). Unlike traditional methods that rely on signal prediction theory and compress video frames individually, MVC considers scenes as the fundamental units for compression. The proposed MVC framework models the intensity variation of the entire video sequence within one scene, aiming to find non-redundant representations rather than reducing redundancy through spatio-temporal predictions. This is achieved by employing an implicit neural representation as the basic modeling architecture. To enhance the efficiency of video modeling, the paper introduces context-related spatial positional embedding and frequency domain supervision for spatial context enhancement. For capturing temporal correlation, a scene flow constrain mechanism and temporal contrastive loss are designed. Experimental results demonstrate that the proposed method achieves up to a 20% reduction in bitrate compared to the latest video coding standard H.266 and is more efficient in decoding than existing video coding strategies.