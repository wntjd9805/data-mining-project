This paper introduces HM-ViT, a novel framework for multi-agent hetero-modal cooperative perception in Vehicle-to-Vehicle (V2V) collaborations. Unlike previous works that focused on homogeneous traffic, HM-ViT allows agents with different sensor modalities to collaborate and enhance perception performance. By effectively fusing features from multi-view images and LiDAR point clouds, HM-ViT can predict 3D objects in highly dynamic V2V scenarios with varying numbers and types of agents. Experimental results on the OPV2V dataset demonstrate that HM-ViT outperforms state-of-the-art methods in V2V hetero-modal cooperative perception. The code for HM-ViT is available at https://github.com/XHwind/HM-ViT.