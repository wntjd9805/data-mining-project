The field of automatic 3D content creation has made significant progress recently, thanks to the availability of pre-trained language models and image diffusion models. This progress has led to the emergence of text-to-3D content creation as a prominent topic. However, existing methods in this field have limitations in terms of geometry recovery and photorealistic rendering, as they use implicit scene representations that couple geometry and appearance through volume rendering. These methods are not effective in generating high-quality 3D assets.To address these limitations, we propose a new method called Fantasia3D for high-quality text-to-3D content creation. The key aspect of Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we use a hybrid scene representation and encode the surface normal extracted from this representation as input for the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function (BRDF) into the text-to-3D task. This allows us to learn the surface material for achieving photorealistic rendering of the generated surface.Our disentangled framework is more compatible with popular graphics engines, enabling various functionalities such as relighting, editing, and physical simulation of the generated 3D assets. We have conducted comprehensive experiments that demonstrate the advantages of our method compared to existing approaches in different text-to-3D task settings.For more information about our project and to access the source codes, please visit our project page at https://fantasia3d.github.io/.