Neural image compression methods have made significant advancements in recent years, but their high computational complexity compared to traditional codecs limits their practical implementation. This study aims to bridge this gap by using shallow or linear decoding transforms, which reduces decoding complexity. To compensate for the decrease in compression performance, more powerful encoder networks and iterative encoding are employed, taking advantage of the asymmetrical computation budget between encoding and decoding. The authors theoretically formalize this approach and provide experimental results that establish a new trade-off between rate-distortion and decoding complexity for neural image compression. The achieved rate-distortion performance is comparable to the established mean-scale hyperprior architecture but at a much lower decoding complexity, reducing it by 80% overall or over 90% for the synthesis transform alone. The code for this study can be found at https://github.com/mandt-lab/shallow-ntc.