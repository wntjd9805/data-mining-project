This paper introduces Tube-Link, a versatile framework for accurate video segmentation in diverse scenarios. The framework utilizes a near-online approach where a short subclip is inputted and corresponding spatial-temporal tube masks are generated. To improve the modeling of cross-tube relationships, tube-level linking is performed using attention along the queries. Additionally, temporal contrastive learning is employed to enhance instance-wise discriminative features for tube-level association. The flexibility and efficiency of the approach allow for varying subclip lengths based on dataset or scenario requirements. Tube-Link outperforms existing specialized architectures on five video segmentation datasets, achieving significant improvements in performance. Specifically, it achieves nearly 13% relative improvements on VIPSeg and 4% improvements on KITTI-STEP compared to the strong baseline Video K-Net. When using a ResNet50 backbone on Youtube-VIS-2019 and 2021, Tube-Link increases IDOL by 3% and 4% respectively. The code for Tube-Link is available at https://github.com/lxtGH/Tube-Link.