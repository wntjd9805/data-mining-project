This study addresses the issue of incomplete 3D scans of real scenes due to limited resolution and mutual occlusion between objects. Existing methods for scene completion struggle to capture long-range spatial context, leading to unsatisfactory results. To overcome this problem, the researchers propose a novel approach called the Dual-Scale Transformer Network (DST-Net). This network efficiently utilizes both long-range and short-range spatial context information to enhance the quality of 3D scene completion. To reduce the computational cost of extracting long-range features, DST-Net adopts a self-supervised two-stage completion strategy. In the first stage, the input scene is divided into blocks and completion is performed on each block individually. In the second stage, the blocks are merged and refined as a whole to improve completeness. Additionally, the researchers introduce a contrastive attention training strategy to encourage the transformers to learn distinctive features for better scene completion. Experimental results on multiple datasets demonstrate that the proposed method produces superior completion results compared to state-of-the-art methods in both quantitative and qualitative evaluations.