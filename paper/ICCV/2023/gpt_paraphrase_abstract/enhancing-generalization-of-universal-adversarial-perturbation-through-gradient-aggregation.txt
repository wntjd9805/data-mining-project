Universal adversarial perturbations (UAP) can deceive deep neural networks in a way that is not specific to individual instances, making them more challenging to address compared to instance-specific adversarial examples. This paper focuses on the problems of UAP generation methods, specifically the gradient vanishing issue with small-batch stochastic gradient optimization and the local optima problem with large-batch optimization. To overcome these challenges, a solution called Stochastic Gradient Aggregation (SGA) is proposed. SGA involves multiple iterations of inner pre-search using small-batch training, followed by aggregating all inner gradients into a one-step gradient estimation to enhance stability and reduce quantization errors. Extensive experiments on the ImageNet dataset demonstrate that SGA significantly improves the generalization ability of UAP and outperforms other state-of-the-art methods. The code for SGA is available at https://github.com/liuxuannan/Stochastic-Gradient-Aggregation.