The use of large pre-trained vision-and-language models has led to significant progress in Vision-and-Language Navigation (VLN) tasks. However, fully fine-tuning these models for each VLN task is costly due to their size. Parameter-Efficient Transfer Learning (PETL) has emerged as a research hotspot, showing potential in efficiently tuning large pre-trained models for common computer vision (CV) and natural language processing (NLP) tasks by only adjusting a minimal set of parameters. However, applying existing PETL methods to the more challenging VLN tasks may result in performance degradation. To address this, we introduce VLN-PETL, the first study exploring PETL methods for VLN tasks. VLN-PETL includes two PETL modules: Historical Interaction Booster (HIB) and Cross-modal Interaction Booster (CIB). These modules are combined with existing PETL methods to create the integrated VLN-PETL. Extensive experiments on four mainstream VLN tasks demonstrate the effectiveness of VLN-PETL, achieving comparable or better performance than full fine-tuning and outperforming other PETL methods with promising margins. The source code for VLN-PETL is available at [source code URL].