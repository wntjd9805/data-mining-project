In recent years, there has been a growing trend in deep learning algorithms to train large scale models using big datasets and high parameter counts. However, the robustness of these models in real-world settings has not been thoroughly explored. This study aims to address this issue by benchmarking the performance of large scale models under various perturbations and datasets that simulate real-world shifts. The results show that these models exhibit a decline in performance under these shifts. It is also noted that existing methods of fine-tuning the entire model for robustness may not be feasible due to the size of the networks and the risk of forgetting important characteristics. Therefore, a simple and cost-effective solution is proposed, inspired by knowledge transfer literature. This method involves robustifying smaller models at a lower computational cost and using them as teachers to fine-tune a fraction of the large scale networks, reducing the overall computational overhead. The proposed method is evaluated using different vision perturbations and transfer learning scenarios on various datasets. The benchmark results demonstrate that our method efficiently induces robustness in these large scale models, requiring less time and preserving the transfer learning and zero-shot properties of the original model. None of the existing methods achieve these results.