This paper explores the possibility of learning high-level skills required for computer vision tasks from text data and transferring them to vision tasks without visual training data. The approach involves leveraging the joint embedding space of contrastively trained vision and language encoders. The study analyzes the impact of systematic differences between embedding spaces for different modalities in contrastive models and proposes strategies to mitigate this concern. The models developed using only text training data are evaluated on four representative tasks, namely image captioning, visual entailment, visual question answering, and visual news captioning. The results show that these models perform comparably to models trained on images, with significant improvements in captioning and visual entailment by over 9 points, and surpassing all prior work on visual news by over 30 points. Additionally, the paper presents various stylistic image captioning models trained solely on text data from books, the web, or language models, without any image or human-curated language data. The answer format of the abstract only includes the core meaning.