Neural network (NN)-based image compression methods have shown impressive results compared to traditional methods. However, most studies have focused on non-scalable image compression, while spatially scalable compression has received less attention. This paper introduces a new NN-based method called COMPASS that supports arbitrary-scale spatial scalability. COMPASS has a flexible structure where the number of layers and their scale factors can be determined during inference. To reduce redundancy between adjacent layers, COMPASS uses an inter-layer arbitrary scale prediction method called LIFF based on implicit neural representation. A combined RD loss function is proposed to effectively train multiple layers. Experimental results demonstrate that COMPASS achieves significant BD-rate gain compared to SHVC and the state-of-the-art NN-based spatially scalable method for various scale factors. COMPASS also exhibits comparable or better coding efficiency than single-layer coding for different scale factors.