Deep neural networks (DNNs) are highly valuable due to their commercial benefits and the high demand for costly annotation and computation resources. In order to protect the copyright of DNNs, a popular method known as backdoor-based ownership verification has emerged. This method allows the model owner to embed a specific backdoor behavior into the model before releasing it, thereby watermarking it. By examining the presence of this behavior, the model owner can identify if a suspicious third-party model has been "stolen" from them. However, these watermarks have been found to be vulnerable to removal attacks, including fine-tuning. To further investigate this vulnerability, we explore the parameter space and discover the existence of watermark-removed models in close proximity to the watermarked one. These watermark-removed models can be easily exploited by removal attacks. Building on this discovery, we propose a mini-max formulation to locate these watermark-removed models and recover their watermark behavior. Through extensive experiments, we demonstrate that our method enhances the robustness of model watermarking against parametric changes and numerous watermark removal attacks. The codes necessary for reproducing our main experiments can be found at https://github.com/GuanhaoGan/robust-model-watermarking.