We introduce Text2Tex, an innovative approach to creating high-quality textures for 3D meshes based on given text prompts. Our method utilizes inpainting in a pre-trained depth-aware image diffusion model to progressively generate detailed textures from multiple viewpoints. To prevent inconsistencies and distortions in the textures across views, we dynamically divide the rendered view into a generation mask that indicates the generation status of each visible texel. This partitioned view representation guides the depth-aware inpainting model in generating and updating partial textures for the corresponding regions. Additionally, we propose an automated scheme for generating a sequence of views to determine the most suitable view for updating the partial texture. Through extensive experiments, we demonstrate that our method outperforms existing text-driven approaches and GAN-based methods.