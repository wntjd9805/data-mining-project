This study focuses on guided depth map super-resolution (GDSR), which involves increasing the resolution of low-resolution (LR) depth maps using high-resolution (HR) RGB images from the same scene. The key challenge is effectively extracting shared and private RGB/depth features. Additionally, the study addresses issues such as blurry edges, noisy surfaces, and over-transferred RGB texture. The proposed solution, the Spherical Space feature Decomposition Network (SSDNet), tackles these problems. It utilizes Restormer block-based RGB/depth encoders to extract local-global features and maps them to the spherical space for feature separation and alignment. The shared RGB features are fused with depth features to accomplish GDSR. To further address the detail issues, a spherical contrast refinement (SCR) module is introduced. This module employs contrastive learning in the spherical feature space to improve the accuracy of patch classification and feature alignment. Extensive experiments demonstrate the effectiveness of the proposed method on four test datasets and its ability to generalize to real-world scenes. The code for this method is publicly available at https://github.com/Zhaozixiang1228/GDSR-SSDNet.