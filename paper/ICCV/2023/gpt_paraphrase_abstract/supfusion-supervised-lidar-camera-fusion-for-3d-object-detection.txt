The task of LiDAR-Camera fusion-based 3D detection is crucial for autonomous driving. While there have been numerous approaches developed in recent years that show promising performance in comparison to single-modal detectors, they often lack effective supervision for the fusion process. To address this issue, this paper proposes a novel training strategy called SupFusion, which introduces auxiliary feature-level supervision to enhance LiDAR-Camera fusion and improve detection performance. The strategy includes a data enhancement method called Polar Sampling, which densifies sparse objects and trains an assistant model to generate high-quality features for supervision. These features are then used to train the LiDAR-Camera fusion model, optimizing the fusion feature to simulate the generated high-quality features. Additionally, a simple yet effective deep fusion module is proposed, which consistently outperforms previous fusion methods when used with the SupFusion strategy. This approach offers several advantages, including the ability to boost LiDAR-Camera detection performance without incurring extra inference costs, and the continuous improvement of the detector's abilities through the deep fusion module. Extensive experiments demonstrate the effectiveness of SupFusion and the deep fusion module, with around a 2% improvement in 3D mAP on the KITTI benchmark using multiple LiDAR-Camera 3D detectors. The code for this work is available at the provided GitHub link.