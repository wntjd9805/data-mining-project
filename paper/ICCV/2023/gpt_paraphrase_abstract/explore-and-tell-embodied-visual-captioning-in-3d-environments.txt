Existing visual captioning models have achieved impressive results, but they assume that the captured image provides a complete view of the scene. However, in real-world situations, a single image may not offer an optimal viewpoint, leading to a lack of detailed scene understanding. To address this limitation, we propose a new task called Embodied Captioning, which empowers visual captioning models with navigation abilities to actively explore the scene and reduce visual ambiguity from suboptimal viewpoints. Our approach involves an agent starting at a random viewpoint and navigating the environment to gather information from different perspectives, ultimately generating a comprehensive paragraph describing all objects in the scene. To facilitate this task, we create the ET-Cap dataset using the Kubric simulator, which includes 10,000 3D scenes with cluttered objects and three annotated paragraphs per scene. We introduce the Cascade Embodied Captioning model (CaBOT) consisting of a navigator and a captioner to tackle this task. The navigator predicts suitable actions to take in the environment, while the captioner generates a paragraph description based on the entire navigation trajectory. Extensive experiments demonstrate that our model outperforms carefully designed baselines. We provide access to our dataset, codes, and models at https://aim3-ruc.github.io/ExploreAndTell.