Recent advancements in text-to-video retrieval systems have focused on utilizing pre-trained models trained on large-scale image-text pairs. However, these methods have primarily prioritized the video modality and neglected the audio signal. A recent development by ECLIPSE has improved text-to-video retrieval by creating an audiovisual video representation. However, the main objective of this task is to capture the audio and video information that is relevant to the text query, rather than simply improving audio and video alignment. To address this limitation, we propose TEFAL, a method that produces audio and video representations conditioned on the text query. Instead of using a single audiovisual attention block that may suppress audio information, our approach incorporates two independent cross-modal attention blocks that allow the text to attend to the audio and video representations separately. We evaluate our method on four benchmark datasets (MSR-VTT, LSMDC, VATEX, and Charades) that include audio, and consistently achieve better performance compared to state-of-the-art methods. This can be attributed to the additional text-query-conditioned audio representation, which complements the text-query-conditioned video representation.