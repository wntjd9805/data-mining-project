This study addresses the issue of referring image segmentation, which involves segmenting the target object referred to in a natural language expression. Previous methods have relied on the assumption that one sentence describes one target in the image, which is not realistic in real-world scenarios. Consequently, these methods fail when expressions refer to no objects or multiple objects. To overcome this limitation, the authors propose a Dual Multi-Modal Interaction (DMMI) Network that consists of two decoder branches, allowing information flow in both directions. The text-to-image decoder utilizes text embedding to query visual features and localize the corresponding target, while the image-to-text decoder reconstructs the erased entity-phrase based on the visual feature. This approach encourages visual features to contain critical semantic information about the target entity, thereby supporting accurate segmentation in the text-to-image decoder. Furthermore, the authors introduce a new dataset called Ref-ZOM, which includes image-text pairs under different settings to provide a more realistic challenge. Extensive experiments demonstrate that their method achieves state-of-the-art performance on various datasets, and the Ref-ZOM-trained model performs well with different types of text inputs. The codes and datasets used in this study are available at the provided GitHub link.