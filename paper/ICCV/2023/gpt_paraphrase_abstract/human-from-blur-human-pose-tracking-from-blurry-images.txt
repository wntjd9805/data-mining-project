We propose a novel approach for estimating 3D human poses from blurry images. Our method addresses the challenge of image deblurring by using a 3D human model, a texture map, and a sequence of poses to represent human motion. We model the blurring process using temporal image aggregation. By utilizing a differentiable renderer, we can solve the inverse problem by backpropagating the pixel-wise re-projection error to recover the most accurate representation of human motion that explains the input images. We also introduce additional regularization terms to improve the results. To the best of our knowledge, this is the first method to tackle this problem. Our approach consistently outperforms other methods on significantly blurry inputs, as they lack the essential functionalities that our method combines, such as sub-frame accurate image deblurring and explicit 3D modeling of non-rigid human motion.