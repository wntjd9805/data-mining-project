The use of Vision Transformer (ViT) based Vision-Language Pre-training (VLP) models has shown impressive results in different tasks. However, the long sequences of visual tokens fed into ViT can lead to inefficient and ineffective training. Current approaches try to address this challenge by either extracting patches at the bottom level within ViT or abstracting patches at the top level outside ViT, but they do not balance training efficiency and effectiveness well. Taking inspiration from text summarization in natural language processing, we propose a new approach called BUS (Bottom-Up Patch Summarization) that combines bottom-level extraction and top-level abstraction to efficiently learn a concise summary of long visual token sequences. In our approach, we incorporate a Text-Semantics-Aware Patch Selector (TSPS) into the ViT backbone to extract visual tokens at a coarse-grained level. We then add a flexible Transformer-based Patch Abstraction Decoder (PAD) on top of the backbone for higher-level visual abstraction. This collaboration between bottom-up extraction and top-level abstraction allows BUS to achieve high training efficiency while maintaining or even improving effectiveness. We evaluate our approach on various visual-language understanding and generation tasks and demonstrate competitive performance in downstream tasks while increasing training efficiency by 50%. Additionally, our model achieves state-of-the-art performance on many downstream tasks by increasing input image resolution without increasing computational costs compared to baselines.