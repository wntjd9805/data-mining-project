Classifier guidance is a technique that uses the gradients of an image classifier to control the generation and editing of images. However, current methods for classifier guidance have limitations, such as the need to train new models or using approximations that result in inaccurate gradients and suboptimal control. In this study, we propose a new method called Direct Optimization of Diffusion Latents (DOODL) that overcomes these limitations. DOODL optimizes diffusion latents based on the gradients of a pre-trained classifier, using an invertible diffusion process for efficient backpropagation. We demonstrate the effectiveness of DOODL compared to one-step classifier guidance in various scenarios, including improving image generations from complex prompts, expanding vocabulary in Stable Diffusion, enabling image-conditioned generation, and enhancing image aesthetics. Our results show that DOODL outperforms existing methods in terms of computational and human evaluation metrics.