Federated learning (FL) is a novel method for training machine learning models on decentralized devices. However, the simultaneous execution of multiple FL tasks can overwhelm devices with limited resources. This study introduces a new FL system called MAS (Merge and Split) that effectively coordinates and trains multiple FL tasks concurrently. MAS merges the tasks into a single FL task with a multi-task architecture and then splits it into multiple tasks based on their affinities. The split tasks are trained using parameters from the merged task. Experimental results show that MAS outperforms other methods, reducing training time by 2Ã— and energy consumption by 40%. This research aims to inspire further exploration and optimization of training simultaneous FL tasks.