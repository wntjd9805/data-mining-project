This paper presents a novel approach for estimating joint attention in a single image. Unlike previous work that focuses solely on gaze-related attributes, our method takes into account the locations and actions of individuals as contextual cues to weight these attributes. Additionally, we explicitly model the interactions among all of these attributes in our approach.To achieve this, we propose a Transformer-based attention network that encodes joint attention as low-dimensional features. We introduce a specialized MLP head with positional embedding to the Transformer, allowing it to predict the pixelwise confidence of joint attention and generate a confidence heatmap. This pixelwise prediction improves the accuracy of the heatmap by addressing the problem of predicting a high-dimensional heatmap from low-dimensional features, which can be ill-posed.Moreover, we enhance the estimated joint attention by integrating it with general image-based attention estimation. Through comparative experiments, our method demonstrates superior performance to state-of-the-art methods in terms of quantitative metrics. The code for our approach is available at https://github.com/chihina/PJAE.