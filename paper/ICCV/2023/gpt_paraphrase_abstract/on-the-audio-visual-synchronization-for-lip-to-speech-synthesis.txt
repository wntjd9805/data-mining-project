Most lip-to-speech (LTS) synthesis models are trained and evaluated under the assumption that the audio-video pairs in the dataset are well synchronized. However, we have found that commonly used datasets like GRID, TCD-TIMIT, and Lip2Wav can suffer from data asynchrony issues, leading to inaccurate evaluation with conventional time alignment-sensitive metrics such as STOI, ESTOI, and MCD. Additionally, training an LTS model with such datasets can cause the generated speech and input video to be out of sync. To address these problems, we have developed a time-alignment frontend for the metrics to ensure accurate evaluation. Additionally, we propose a synchronized lip-to-speech (SLTS) model with an automatic synchronization mechanism (ASM) that corrects data asynchrony and penalizes model asynchrony during training. We have conducted evaluations using artificial and popular audiovisual datasets, and our approach has shown superior performance compared to existing state-of-the-art models across various evaluation metrics.