Large vision-language models are widely applicable but require expensive training that only large institutions can afford. This paper introduces a more efficient approach called Curation in Training (CiT), which sacrifices generality for efficiency. CiT incorporates a data objective into training and automatically generates high-quality data to accelerate contrastive image-text training. It eliminates the need for an offline data filtering pipeline, allowing for the use of diverse data sources, including raw image-text pairs from the web. CiT consists of two loops: an outer loop that curates the training data and an inner loop that consumes the curated data. The text encoder connects these two loops. By leveraging metadata such as class names and a large pool of image-text pairs, CiT selects relevant training data by measuring the similarity between their text embeddings and the embeddings of the metadata. Experimental results demonstrate that CiT can significantly speed up training, particularly when dealing with large amounts of raw data.