Current neural network pruning methods heavily rely on manually designed criteria and structures, leading to dependence on heuristics and expert knowledge. This paper proposes a principled and unified framework based on Information Bottleneck (IB) theory to address this issue by introducing an automatic pruning approach. The channel pruning problem is formulated from an IB perspective, and the IB principle is implemented by solving a Hilbert-Schmidt Independence Criterion (HSIC) Lasso problem under certain conditions. Theoretical guidance is used to develop an automatic pruning scheme by searching for global penalty coefficients. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on various benchmark networks and datasets. For instance, with VGG-16, it achieves a 60% reduction in FLOPs (floating point operations) by removing 76% of the parameters, with a 0.40% improvement in top-1 accuracy on CIFAR-10. Similarly, with ResNet-50, it achieves a 56% reduction in FLOPs by removing 50% of the parameters, with a small loss of 0.08% in top-1 accuracy on ImageNet. The code for this method is available at https://github.com/sunggo/APIB.