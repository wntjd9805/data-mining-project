Recent studies have suggested using feature learning methods for video analysis tasks in order to identify corresponding pixels from different frames. These methods aim to learn distinct representations for matching pixels, particularly through self-supervised learning. However, these approaches struggle with tiny or single-pixel visual targets. Traditionally, pixel-wise correspondences were associated with optical flows, but this approach lacks robustness in real-world videos and only provides deterministic correspondences.To address this issue, we propose a holistic framework for learning features that establish pixel-wise correspondences. Our approach is inspired by optical flows and self-supervised feature learning. We utilize both labeled synthetic videos and unlabeled real-world videos to learn fine-grained representations. Additionally, we employ an adversarial learning scheme to enhance the generalization ability of the learned features. Furthermore, we introduce a coarse-to-fine framework to optimize computational efficiency.Through experiments on various correspondence-based tasks, our method demonstrates superior performance compared to state-of-the-art methods in terms of accuracy and efficiency.