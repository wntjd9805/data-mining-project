Current supervised text recognition methods struggle with handling complex text images due to their heavy reliance on annotated real images. While large-scale synthetic text images have been used to reduce this dependence, there is still a domain gap that limits recognition performance. To address this issue, we propose a self-supervised learning approach that explores robust text feature representations on unlabeled real images. Existing self-supervised methods use sequence-to-sequence representation learning by splitting visual features along the horizontal axis, which restricts the flexibility of augmentations. Our novel method, called Character-to-Character Distillation (CCD), overcomes this limitation by enabling versatile augmentations for general text representation learning. CCD achieves this by delineating the character structures in unlabeled real images through a self-supervised character segmentation module. It then enriches the diversity of local characters while maintaining their pairwise alignment under flexible augmentations using a transformation matrix between two augmented views. Experimental results demonstrate that CCD outperforms existing methods, achieving state-of-the-art results in text recognition, text segmentation, and text super-resolution. The average performance gains are 1.38% in text recognition, 1.7% in text segmentation, 0.24 dB (PSNR) in text super-resolution, and 0.0321 (SSIM) in text super-resolution. The code for CCD is available at https://github.com/TongkunGuan/CCD.