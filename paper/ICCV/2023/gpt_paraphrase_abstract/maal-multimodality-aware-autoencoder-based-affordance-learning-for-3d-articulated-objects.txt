Inferring affordance for 3D articulated objects is a challenging problem that is crucial for the application of robots in real-world scenarios. This task involves determining where and how to act on these objects. Current approaches typically process multi-modal inputs with early fusion and use critic networks to generate scores. However, this approach lacks sufficient multi-modal learning ability and requires inefficient iterative training. To address these limitations, this paper introduces a new method called Multimodality-Aware Autoencoder-based affordance Learning (MAAL). MAAL is a pipeline that can be trained in one go and only requires a small number of positive samples. The key component of MAAL is the MultiModal Energized Encoder (MME), which enables better multi-modal learning by comprehensively modeling all multi-modal inputs and capturing the interactions between robots and objects. Experimental results using the PartNet-Mobility dataset demonstrate the effectiveness of MAAL in learning multi-modal data and solving the 3D articulated object affordance problem.