This paper introduces a method called HyperReenact for neural face reenactment, which aims to generate realistic talking head images of a specific person by using a target facial pose. Existing face reenactment methods often produce visual artifacts, especially when dealing with extreme head pose changes or require expensive fine-tuning to maintain the source identity. To overcome these limitations, the proposed method leverages a pretrained StyleGAN2 generator to invert real images into their latent space. A hypernetwork is then used to refine the source identity characteristics and perform facial pose re-targeting, eliminating the need for external editing methods that typically introduce artifacts. The method operates under the one-shot setting, using only a single source frame, and allows for cross-subject reenactment without subject-specific fine-tuning. The effectiveness of the proposed method is demonstrated through quantitative and qualitative comparisons with state-of-the-art techniques on the VoxCeleb1 and VoxCeleb2 benchmarks. The results show that the HyperReenact approach produces artifact-free images and exhibits robustness even under extreme head pose changes. The code and pretrained models are made publicly available at: https://github.com/StelaBou/HyperReenact.