Adversarial training (AT) is a widely used method for enhancing the resilience of deep neural networks. However, recent findings have revealed a new type of attack that exploits the vulnerability of adversarially trained networks by manipulating the intermediate layers to produce incorrect predictions. This indicates that the current approach to adversarial training may have limitations in terms of the perturbation search space. To understand the effectiveness of this intermediate-layer attack, we analyze the forward propagation process and identify a phenomenon called the Clustering Effect. This effect suggests that the intermediate-layer representations of neural networks tend to be similar for samples with the same label in the training set. We provide a theoretical proof of the existence of the Clustering Effect using Information Bottleneck Theory. Interestingly, we observe that the intermediate-layer attack violates the Clustering Effect in adversarial training. Motivated by these observations, we propose a regularization method called sufficient adversarial training (SAT) that expands the perturbation search space during training. We also establish a robustness bound for neural networks through rigorous mathematical analysis. Experimental evaluations demonstrate that SAT outperforms other state-of-the-art AT methods in defending against adversarial attacks on both output and intermediate layers. Additional details, including our code and Appendix, are available at https://github.com/clustering-effect/SAT.