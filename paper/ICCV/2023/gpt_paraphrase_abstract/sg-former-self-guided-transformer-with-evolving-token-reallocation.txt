The Vision Transformer has shown impressive success in various vision tasks, but its high computational cost limits its ability to handle large feature maps. Previous approaches have tried to reduce this cost by using either local self-attentions or global self-attentions with shorter sequence lengths, resulting in coarse granularity. In this paper, we introduce a new model called Self-guided Transformer (SG-Former), which combines global self-attention with adaptive fine granularity. Our approach uses a significance map, estimated through hybrid-scale self-attention, to redistribute tokens based on the importance of each region. This allows for fine-grained attention in salient regions while sacrificing efficiency and global receptive fields in minor regions. The SG-Former outperforms state-of-the-art models, achieving high accuracy on ImageNet-1K, CoCo, and ADE20K datasets, with lower computational costs and fewer parameters. The code for SG-Former is available at https://github.com/OliverRensu/SG-Former.