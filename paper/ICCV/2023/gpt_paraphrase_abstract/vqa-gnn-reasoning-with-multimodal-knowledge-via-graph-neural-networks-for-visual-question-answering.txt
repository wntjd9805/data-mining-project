Visual question answering (VQA) necessitates systems to engage in concept-level reasoning by integrating unstructured and structured multimodal knowledge. Existing approaches typically combine a scene graph and a concept graph by connecting corresponding visual and concept nodes, and incorporate the QA context representation for question answering. However, these methods only perform unidirectional fusion from unstructured to structured knowledge, limiting their ability to capture joint reasoning across different modalities of knowledge. To enhance expressive reasoning, we propose VQA-GNN, a novel VQA model that enables bidirectional fusion between unstructured and structured multimodal knowledge to obtain unified knowledge representations. Our approach connects the scene graph and concept graph through a super node representing the QA context and employs a new multimodal GNN technique for inter-modal message passing to bridge representational gaps. On challenging VQA tasks (VCR and GQA), our method surpasses strong baseline VQA methods by 3.2% on VCR (Q-AR) and 4.6% on GQA, demonstrating its efficacy in concept-level reasoning. Ablation studies further confirm the effectiveness of bidirectional fusion and multimodal GNN in unifying unstructured and structured multimodal knowledge. The answer format only outputs the abstraction.