This study focuses on weakly-supervised point cloud semantic segmentation, where there are sparse annotations available (less than 0.1% of points labeled). The goal is to reduce the cost of dense annotations. However, it is challenging to extract both contextual and object information from sparsely-annotated points for scene understanding. Inspired by masked modeling in image and video representation learning, the researchers aim to apply this concept to learn contextual information from sparsely-annotated point clouds. However, directly applying masked modeling to 3D point clouds with sparse annotations is not straightforward. Two main challenges are identified: effectively masking out informative visual context from 3D point clouds and fully exploiting sparse annotations for context modeling. To address these challenges, the researchers propose a method called Contextual Point Cloud Modeling (CPCM), which consists of a region-wise masking strategy and a contextual masked training method. The region-wise masking continuously masks the point cloud in geometric space to create a meaningful masked prediction task for context learning. The contextual masked training disentangles the learning of supervised segmentation and unsupervised masked context prediction to effectively learn from the limited labeled points and mass unlabeled points, respectively. Experimental results on the ScanNet V2 and S3DIS benchmarks demonstrate that CPCM outperforms state-of-the-art methods.