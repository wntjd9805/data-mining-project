Existing unsupervised video deraining methods are not effective in capturing the complex spatio-temporal properties of rain, resulting in unsatisfactory outcomes. This paper proposes a new approach by incorporating a bio-inspired event camera into the unsupervised video deraining pipeline. This integration allows for the capture of high temporal resolution information and the modeling of intricate rain characteristics. The proposed method consists of two modules: an asymmetric separation module and a cross-modal fusion module. The separation module is responsible for segregating rain-background layer features, while the fusion module enhances positive aspects and suppresses negative aspects from a cross-modal perspective. To enhance network training, a cross-modal contrastive learning method is employed, which utilizes complementary information from event cameras and explores the differences and similarities between rain-background layers in different domains. This encourages the network to focus on the distinct characteristics of each layer and learn a more discriminative representation. Additionally, a real-world dataset comprising rainy videos and events is constructed using a hybrid imaging system. Extensive experiments demonstrate that our method outperforms existing approaches on both synthetic and real-world datasets.