This paper introduces a new problem called Cloth2Body, which aims to generate 3D human body meshes from 2D clothing images. Unlike previous methods, Cloth2Body addresses challenges related to partial observation of the input and the diverse range of possible outputs. These challenges include accurately locating and posing human bodies within clothing, estimating body shapes across different clothing types, and generating diverse and plausible results from 2D clothing images. To tackle these challenges, the authors propose an end-to-end framework that accurately estimates 3D body mesh parameters from 2D clothing images. The framework utilizes kinematics-aware pose estimation and inverse kinematics to improve pose estimation accuracy. An adaptive depth trick is also introduced to align the re-projected 3D mesh with the 2D clothing image. Physics-informed shape estimation is used to estimate body shape parameters based on partial body measurements from RGB images, enabling better alignment and flexible user editing. Lastly, an evolution-based pose generation method inspired by genetic algorithms is implemented to generate diverse and reasonable poses during inference. Experimental results on synthetic and real-world data demonstrate that the proposed framework achieves state-of-the-art performance in recovering natural and diverse 3D body meshes that align well with clothing.