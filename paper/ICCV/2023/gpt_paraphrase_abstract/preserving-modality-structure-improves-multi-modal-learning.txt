We introduce a new method called Semantic-Structure-Preserving Consistency, which aims to enhance the generalizability of self-supervised learning on large-scale multi-modal datasets. Existing approaches in this field have successfully learned joint embeddings without human annotations, enabling zero-shot cross-modal tasks. However, these methods struggle to generalize to out-of-domain data due to the lack of consideration for the semantic structure present in modality-specific embeddings.   To address this issue, our proposed approach preserves the modality-specific relationships in the joint embedding space. We achieve this by learning multiple anchors and representing the multifaceted relationship between samples based on their relationship with these anchors. We introduce a novel algorithm called Multi-Assignment Sinkhorn-Knopp to assign multiple anchors to each sample.   Our experiments demonstrate that our approach effectively learns semantically meaningful anchors in a self-supervised manner. Additionally, our evaluation on the MSR-VTT and YouCook2 datasets shows that our multi-anchor assignment solution achieves state-of-the-art performance and generalizes well to both in-domain and out-of-domain datasets. The code for our approach is available at the following link: [GitHub link].