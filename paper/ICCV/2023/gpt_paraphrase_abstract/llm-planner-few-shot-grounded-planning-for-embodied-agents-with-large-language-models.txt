This study focuses on utilizing large language models (LLMs) as a planner for embodied agents in visually-perceived environments, enabling them to understand and execute complex tasks through natural language instructions. The limitations of current methods, such as high data requirements and poor sample efficiency, hinder the development of adaptable agents capable of learning new tasks quickly. To address this, we propose a new approach called LLM-Planner, which leverages the capabilities of large language models to perform few-shot planning for embodied agents. Additionally, we suggest a straightforward yet effective method to enhance LLMs by incorporating physical grounding, enabling them to generate and update plans based on the current environment. Through experiments conducted on the ALFRED dataset, our method demonstrates highly competitive few-shot performance. Despite using less than 0.5% of paired training data, LLM-Planner achieves comparable performance to recent baselines trained on the complete dataset. Conversely, existing methods struggle to successfully complete tasks under the same few-shot setting. Our research paves the way for the development of versatile and sample-efficient embodied agents capable of rapid task learning.