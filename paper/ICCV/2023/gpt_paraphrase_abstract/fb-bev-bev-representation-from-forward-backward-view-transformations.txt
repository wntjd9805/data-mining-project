The View Transformation Module (VTM) is a crucial component in camera-based Bird-Eye-View (BEV) perception systems, as it facilitates the transformation between multi-view image features and BEV representation. Currently, the two prominent paradigms for VTM are forward projection and backward projection. Forward projection, exemplified by Lift-Splat-Shoot, produces sparsely projected BEV features without post-processing. On the other hand, backward projection, represented by BEV-Former, tends to generate false-positive BEV features due to incorrect projections caused by the lack of depth utilization. To overcome these limitations, we propose a novel forward-backward view transformation module that combines the strengths of both methods. Our approach, implemented as FB-BEV, achieves a state-of-the-art result of 62.4% NDS on the nuScenes test set. The popularity of BEV-based 3D detection models has increased due to their comprehensive representation abilities for multi-camera inputs, improving the performance of vision-only and multi-modality perception models for autonomous driving. These models typically consist of an image backbone, a View Transformation Module, and a detection head. The VTMs are responsible for projecting multi-view camera features onto the BEV plane, and they can be categorized into forward projection and backward projection methods.