This paper introduces a new task called zero-shot text-to-video generation and proposes a low-cost approach that leverages existing text-to-image synthesis methods to generate videos without the need for training or optimization. The approach enriches the latent codes of the generated frames with motion dynamics to maintain consistency in the global scene and background over time. It also incorporates a new cross-frame attention mechanism to preserve the context, appearance, and identity of the foreground object. Experimental results demonstrate that this approach achieves high-quality and consistent video generation with minimal overhead. Additionally, the proposed method can be applied to other tasks such as conditional and content-specialized video generation and instruction-guided video editing. Despite not being trained on additional video data, the method performs comparably or better than recent approaches. The code for this approach is publicly available.