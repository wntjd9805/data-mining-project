Online continual learning (CL) is a method of learning that focuses on acquiring new knowledge while also retaining previously learned information from non-stationary data streams. However, in this dynamic training environment, the model tends to forget previously learned knowledge and becomes biased towards new tasks. To tackle this issue, we propose a Continual Bias Adaptor (CBA) module that enhances the classifier network's ability to adapt to drastic changes in distribution during training. This adaptation ensures that the classifier network can effectively consolidate previously learned tasks in a stable manner. Notably, in the testing stage, the CBA module can be removed without incurring any additional computational cost or memory overhead. We provide theoretical insights into why our proposed method is effective in mitigating catastrophic distribution shifts and validate its effectiveness through extensive experiments using four rehearsal-based baselines and three public continual learning benchmarks.