We propose UniT3D, a unified transformer-based architecture for simultaneously solving 3D visual grounding and dense captioning. Previous attempts at connecting these tasks have been task-specific and have not explicitly depicted their shared nature. UniT3D learns a strong multimodal representation through supervised joint pre-training with bidirectional and seq-to-seq objectives. Its generic architecture allows for expansion of the pre-training scope to benefit 3D vision-language tasks. Extensive experiments show that UniT3D achieves significant improvements in 3D dense captioning and visual grounding.