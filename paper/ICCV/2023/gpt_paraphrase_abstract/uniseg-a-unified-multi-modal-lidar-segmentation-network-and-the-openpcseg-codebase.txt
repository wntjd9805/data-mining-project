This paper introduces UniSeg, a unified multi-modal LiDAR segmentation network that combines RGB images with three views of point clouds to achieve semantic segmentation and panoptic segmentation simultaneously. The network utilizes the Learnable cross-Modal Association (LMA) module to fuse voxel-view and range-view features with image features, leveraging the semantic information of images and remaining robust to calibration errors. Additionally, the enhanced voxel-view and range-view features are transformed to the point space and further fused adaptively using the Learnable cross-View Association (LVA) module. UniSeg achieves promising results in three benchmark datasets, ranking first in two challenges. The paper also presents the OpenPCSeg codebase, the largest and most comprehensive outdoor LiDAR segmentation codebase, which includes reproducible implementations of popular LiDAR segmentation algorithms. The codebase will be publicly available at https://github.com/PJLab-ADG/PCSeg.