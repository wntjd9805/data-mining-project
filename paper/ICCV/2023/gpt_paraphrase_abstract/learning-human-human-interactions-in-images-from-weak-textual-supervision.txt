Previous research has treated human interactions as categorical and overlooked the wide range of possible interactions. In this study, we propose a new approach to learning human-human interactions from a single still image, allowing for more flexibility in modeling different situations and relationships between people. Due to the lack of specifically labeled data for this task, we utilize knowledge distillation by training a captioning model with synthetic caption data generated by a large language model without explicit guidance. Our results demonstrate that the pseudo-labels derived from this method effectively train the captioning model to comprehend human-human interactions in images. Various metrics measuring textual and semantic fidelity, as well as factual accuracy of our predictions, validate the success of our approach. Additionally, our method outperforms state-of-the-art models in image captioning and situation recognition for this task. We plan to make our code and pseudo-labels publicly available, along with a manually-curated test set called "Waldo and Wenda" for evaluating still image human-human interaction understanding.