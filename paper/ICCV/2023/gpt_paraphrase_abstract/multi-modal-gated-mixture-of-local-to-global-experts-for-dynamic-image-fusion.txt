The objective of infrared and visible image fusion is to combine information from multiple sources to improve performance in various tasks compared to using a single modality. However, existing methods often overlook the dynamic changes in reality, resulting in the loss of visible texture in good lighting conditions and infrared contrast in low lighting conditions. To address this issue, we propose a dynamic image fusion framework called MoE-Fusion, which utilizes a multi-modal gated mixture of local-to-global experts. Our model consists of a Mixture of Local Experts (MoLE) and a Mixture of Global Experts (MoGE) guided by a multi-modal gate. The MoLE focuses on learning multi-modal local features to retain local information in a sample-adaptive manner, while the MoGE emphasizes global information to enhance overall texture detail and contrast. Extensive experiments demonstrate that our MoE-Fusion approach outperforms existing methods by effectively preserving multi-modal image texture and contrast through dynamic learning. Moreover, it achieves superior performance in detection tasks. The code for our method is publicly available on GitHub (https://github.com/SunYM2020/MoE-Fusion).