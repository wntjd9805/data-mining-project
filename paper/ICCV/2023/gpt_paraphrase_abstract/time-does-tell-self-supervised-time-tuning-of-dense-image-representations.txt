Spatially dense self-supervised learning is an emerging field with potential applications in unsupervised segmentation and pretraining for dense downstream tasks. However, the temporal information present in videos has been largely overlooked. To bridge this gap, our paper proposes a novel approach called time-tuning, which incorporates temporal consistency into dense self-supervised learning. Unlike methods designed solely for images, our approach not only enhances representation quality for videos but also for images. We achieve this by fine-tuning image-pretrained models using a new self-supervised temporal-alignment clustering loss on unlabeled videos. This enables the transfer of high-level information from videos to image representations. Our method, time-tuning, outperforms the state-of-the-art by 8-10% in unsupervised semantic segmentation for videos and matches it for images. We believe that our approach opens up possibilities for further scaling of self-supervised learning by leveraging the abundant availability of videos. The implementation of our method can be accessed at: https://github.com/SMSD75/Timetuning.