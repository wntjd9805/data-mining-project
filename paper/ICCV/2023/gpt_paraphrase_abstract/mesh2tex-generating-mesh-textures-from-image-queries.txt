Recent progress has been made in learning neural representations that describe the geometry of objects. However, the generation of textured objects for downstream applications and 3D rendering is still in its early stages. Reconstructing textured geometry from real object images is particularly challenging, as the reconstructed geometry is often imprecise, making realistic texturing difficult. To address this issue, we introduce Mesh2Tex, a method that learns a realistic object texture manifold by utilizing a hybrid mesh-neural-field texture representation. This texture representation allows for the efficient encoding of high-resolution textures as a neural field in the barycentric coordinate system of the mesh faces. By leveraging collections of 3D object geometry and photorealistic RGB images, Mesh2Tex learns the texture manifold, enabling the generation of object textures that match input RGB images for a given 3D object geometry.One key advantage of Mesh2Tex is its robustness in challenging real-world scenarios where the mesh geometry only approximates the underlying geometry in the RGB image. This robustness allows for the generation of realistic object textures that accurately represent observed real-world environments, surpassing previous state-of-the-art approaches.For more information, please visit the project page at alexeybokhovkin.github.io/mesh2tex/.