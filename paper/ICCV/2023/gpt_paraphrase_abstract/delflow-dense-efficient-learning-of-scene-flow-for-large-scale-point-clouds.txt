The sparsity of point clouds and the density of image pixels pose challenges for feature fusion in point-wise scene flow estimation. Existing methods have limitations in predicting scene flow from entire point clouds due to memory inefficiency and heavy overhead. To address these issues, we propose regularizing raw points into a dense format by storing 3D coordinates in 2D grids. This dense representation preserves most points, improves efficiency, and enables effective feature fusion. We also introduce a warping projection technique to mitigate information loss caused by multiple points being mapped into one grid during cost volume computation. Our method outperforms prior approaches on the FlyingThings3D and KITTI datasets, as demonstrated by sufficient experiments. The source codes for our method will be made available on GitHub.