We present a new method for unsupervised object localization that can explain model predictions without the need for additional finetuning. Existing methods in this area often rely on class-agnostic activation maps or self-similarity maps, but these maps have limitations in explaining how the model makes predictions. In our approach, we propose a simple yet effective method based on representer point selection, where the model's predictions are represented as a linear combination of representer values of training points. By selecting the most important examples for the model's predictions, our method offers insights into how the model predicts the foreground object, providing relevant examples and their importance. Our method surpasses the current state-of-the-art unsupervised and self-supervised object localization methods on various datasets, and even outperforms recent weakly supervised and few-shot methods. The code for our method can be found at: https://github.com/yeonghwansong/UOLwRPS.