This paper introduces a new approach to curriculum learning for the efficient training of visual backbones, such as vision Transformers. Traditional deep networks require costly training procedures to achieve superior performance. The proposed approach is inspired by the learning dynamics of deep networks, which show that during early training stages, models primarily learn to recognize easier-to-learn discriminative patterns within each example, such as lower-frequency components of images and the original information before data augmentation. Based on this observation, the authors propose a curriculum that gradually introduces more difficult patterns while always leveraging all training data at each epoch. To implement this curriculum, the authors introduce a cropping operation in the Fourier spectrum of inputs, enabling the model to efficiently learn from only the lower-frequency components. They also demonstrate that exposing the features of original images is equivalent to adopting weaker data augmentation. By integrating these techniques and designing a curriculum learning schedule with a greedy-search algorithm, they develop an approach called EfficientTrain, which is simple, general, and surprisingly effective. EfficientTrain significantly reduces the wall-time training cost of popular models like ResNet, ConvNeXt, DeiT, PVT, Swin, and CSWin by more than 1.5 times on ImageNet-1K/22K without sacrificing accuracy. The approach is also effective for self-supervised learning, such as MAE, and the code is available at the provided GitHub link.