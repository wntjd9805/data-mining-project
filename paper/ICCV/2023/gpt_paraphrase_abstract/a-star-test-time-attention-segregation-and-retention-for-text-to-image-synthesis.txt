Recent advancements in text-to-image generative models have resulted in the development of highly effective methods for generating creative images from text. However, these models have certain limitations that need to be addressed. Upon analyzing the cross-attention representations of these models, we have identified two main issues. Firstly, when the text prompts contain multiple concepts, there is a considerable overlap in the pixel-space (i.e., same spatial regions) between different pairs of concepts. Consequently, the model struggles to distinguish between these concepts, leading to the neglect of one of them in the final generated image. Secondly, although these models initially attempt to capture all the concepts during the denoising process, as indicated by cross-attention maps, this knowledge is not retained by the end of denoising. This loss of knowledge results in inaccurate image generation.To tackle these challenges, we propose two innovative attention-based loss functions that significantly enhance the performance of pretrained baseline text-to-image diffusion models during testing. Firstly, our attention segregation loss reduces the overlap in cross-attention maps between different concepts in the text prompt. This reduction minimizes confusion and conflicts among concepts, ensuring that all concepts are captured accurately in the generated image. Secondly, our attention retention loss explicitly enforces the retention of cross-attention information for all concepts throughout the denoising process. This retention helps prevent information loss and preserves all concepts in the final generated image.We have conducted extensive experiments using the proposed loss functions on various text prompts. The results demonstrate that our approach produces generated images that are semantically much closer to the input text compared to the baseline text-to-image diffusion models.