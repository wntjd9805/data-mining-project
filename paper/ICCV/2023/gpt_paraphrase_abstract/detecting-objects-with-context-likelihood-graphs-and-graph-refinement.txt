This paper aims to improve object detection by considering the interrelationships between objects. Unlike existing methods that learn objects and relations separately, the authors propose a novel approach of learning the joint distribution of object-relation pairs. They create a graphical representation called a context-likelihood graph, which is generated using inter-object relation priors and initial class predictions. The authors use an energy-based modeling technique to learn the joint distribution, allowing for iterative sampling and refinement of the context-likelihood graph. By learning the distribution jointly, they are able to generate a more accurate graph representation, leading to improved object detection performance. The authors conduct experiments on the Visual Genome and MS-COCO datasets, comparing their method to DETR and Faster-RCNN as well as alternative methods that model object interrelationships separately. Their approach consistently outperforms these detectors and methods, particularly benefiting rare object classes. The method is detector agnostic, end-to-end trainable, and achieves promising results.