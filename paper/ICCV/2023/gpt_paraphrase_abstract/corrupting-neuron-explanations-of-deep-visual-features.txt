The lack of transparency in the behavior of deep neural networks (DNNs) has led to the development of explainability methods. However, there are concerns about the reliability and robustness of these methods. In this study, we conducted the first analysis of the robustness of Neuron Explanation Methods using a standardized approach. Our findings reveal that these explanations can be significantly distorted by random noise and carefully crafted perturbations in the input data. Even a small amount of random noise with a standard deviation of 0.02 can alter the assigned concepts of up to 28% of neurons in the deeper layers. Additionally, we introduced a new corruption algorithm that can manipulate the explanation of over 80% of neurons by contaminating less than 10% of the input data. This raises concerns about the trustworthiness of Neuron Explanation Methods in critical real-world applications that require safety and fairness.