Human-Object Interaction (HOI) detection in videos, particularly in open set scenarios, presents significant challenges. Existing approaches focus on static images and closed settings, where all HOI classes are provided in the training data. However, in open set scenarios, HOI detectors need to be able to recognize unseen HOIs and capture temporal contextual information accurately. To address these challenges, we propose ACoLP, an Action-centric Chain-of-LookPrompting model for open set video HOI detection. ACoLP treats actions as carriers of semantics in videos, capturing essential semantic information across frames. To ensure generalizability to unseen classes, we introduce a chain-of-look prompting scheme inspired by natural language processing. This scheme decomposes prompt generation into a series of intermediate visual reasoning steps, enabling our model to capture complex visual reasoning processes underlying HOI events. Experimental results on two video HOI datasets, VidHOI and CAD120, demonstrate that ACoLP performs competitively in closed settings and outperforms existing methods by a significant margin in open set settings. Our code is available at https://github.com/southnx/ACoLP.