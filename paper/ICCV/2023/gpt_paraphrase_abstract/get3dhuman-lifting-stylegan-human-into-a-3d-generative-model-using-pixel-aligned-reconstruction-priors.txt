The ability to quickly generate high-quality 3D digital humans is crucial for various applications. While recent advancements in differentiable rendering have allowed for the training of 3D generative models without relying on 3D ground truths, there is still room for improvement in terms of both the accuracy and diversity of the generated 3D humans. This paper introduces Get3DHuman, a novel framework that enhances the realism and diversity of the generated outcomes using a limited amount of 3D ground-truth data. The key idea is to leverage human-related priors learned from 2D human generators and 3D reconstructors. By connecting the latent space of Get3DHuman with that of StyleGAN-Human using a specially-designed prior network, the framework maps the input latent code to shape and texture feature volumes obtained from the pixel-aligned 3D reconstructor. These outcomes from the prior network serve as supervisory signals for the main generator network. To ensure effective training, three tailored losses are proposed for the generated feature volumes and intermediate feature maps. Extensive experiments demonstrate that Get3DHuman outperforms other state-of-the-art approaches and can be applied to shape interpolation, shape re-texturing, and single-view reconstruction through latent inversion.