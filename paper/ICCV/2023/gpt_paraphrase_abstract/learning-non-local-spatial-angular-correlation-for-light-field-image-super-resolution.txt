Leveraging spatial-angular correlation is essential for enhancing the resolution of light field (LF) images. However, this task is challenging due to the non-local nature caused by the differences among LF images. Although deep neural networks (DNNs) have been developed for LF image super-resolution and have shown improved performance, existing methods struggle to effectively utilize the long-range spatial-angular correlation, particularly when dealing with scenes that have significant disparity variations. In this study, we propose a straightforward yet efficient approach to learn the non-local spatial-angular correlation for LF image super-resolution. Our method involves using the epipolar plane image (EPI) representation to project the 4D spatial-angular correlation onto multiple 2D EPI planes. Subsequently, we develop a Transformer network that employs iterative self-attention operations to model the dependencies between each pair of EPI pixels and learn the spatial-angular correlation. This allows our method to incorporate information from all angular views and achieve a global receptive field along the epipolar line. We extensively evaluate our method through experiments and provide insightful visualizations to validate its effectiveness. Comparative results on five publicly available datasets demonstrate that our approach not only achieves state-of-the-art super-resolution performance but also exhibits robustness to disparity variations. The code for our method is publicly available at https://github.com/ZhengyuLiang24/EPIT.