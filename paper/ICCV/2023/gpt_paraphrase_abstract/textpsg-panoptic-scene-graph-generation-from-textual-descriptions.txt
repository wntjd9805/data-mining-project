The Panoptic Scene Graph is a method used for understanding scenes comprehensively. However, previous approaches require large amounts of annotated data, which is time-consuming and costly to obtain. To overcome this limitation, we propose a new problem called Caption-to-PSG, which aims to generate Panoptic Scene Graphs solely from textual descriptions. We leverage the abundant image-caption data available on the Web to generate these graphs. This problem is challenging due to the absence of location priors, explicit links between visual regions and textual entities, and predefined concept sets. To address these challenges, we introduce a framework called TextPSG, which consists of four modules: a region grouper, an entity grounder, a segment merger, and a label generator. These modules utilize novel techniques to group image pixels into segments, align visual segments with language entities, learn segment similarity, and learn object semantics and relation predicates. Our framework outperforms baseline methods and demonstrates strong robustness. We conduct thorough experiments to validate our design choices and provide insights for future research. The code, data, and results of our project can be accessed on our project page: https://vis-www.cs.umass.edu/TextPSG.