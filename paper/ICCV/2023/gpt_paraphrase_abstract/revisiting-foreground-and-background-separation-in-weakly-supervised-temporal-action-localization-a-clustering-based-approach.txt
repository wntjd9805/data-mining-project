Weakly-supervised temporal action localization involves localizing action instances in videos using only video-level action labels. Current methods use a localization-by-classification approach, optimizing snippet-level predictions with a video classification loss. However, this approach struggles with the disparity between classification and detection, leading to inaccurate separation of foreground and background snippets. To address this issue, we propose a new method that leverages unsupervised snippet clustering instead of relying heavily on the video classification loss. Our method includes two main components: a snippet clustering component that groups snippets into latent clusters and a cluster classification component that classifies each cluster as foreground or background. Since there are no ground-truth labels for training these components, we introduce a unified self-labeling mechanism based on optimal transport to generate high-quality pseudo-labels that align with plausible prior distributions. This ensures accurate association of snippet cluster assignments with their foreground and background labels, improving the separation. We evaluate our method on three benchmarks (THUMOS14, ActivityNet v1.2, and v1.3) and achieve promising performance while being more lightweight than previous methods. Code can be found at https://github.com/Qinying-Liu/CASE.