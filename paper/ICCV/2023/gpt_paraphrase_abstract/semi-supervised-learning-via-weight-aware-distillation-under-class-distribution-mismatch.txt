We introduce a new approach called Weight-Aware Distillation (WAD) to address the problem of Semi-Supervised Learning (SSL) under class distribution mismatch. In this problem, unlabeled data contains unknown categories that are not present in the labeled data. Traditional SSL methods suffer from reduced performance when faced with this mismatch because the instances with unknown categories negatively impact the target classifier. Through mathematical reasoning, we demonstrate that the SSL error under class distribution mismatch consists of two components: pseudo-labeling error and invasion error. These two errors jointly limit the SSL population risk. To mitigate this error, we propose the WAD framework, which selectively transfers knowledge from unsupervised contrastive representation to the target classifier using weights. WAD incorporates adaptive weights and high-quality pseudo-labels to target instances by leveraging point mutual information (PMI) in the representation space. This allows WAD to maximize the utilization of unlabeled data and filter out unknown categories. Theoretical analysis confirms that WAD has a tight upper bound on population risk under class distribution mismatch. Experimental evaluations on benchmark datasets (CIFAR10 and CIFAR100) and an artificial cross-dataset demonstrate that WAD outperforms five state-of-the-art SSL approaches and one standard baseline. The code for WAD is available at https://github.com/RUC-DWBI-ML/research/tree/main/WAD-master.