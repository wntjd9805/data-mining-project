Current event-based motion deblurring techniques have limitations in practical usage, such as assuming the same spatial resolution of inputs and specific blurriness distributions. This study aims to overcome these limitations and improve the generalizability of event-based deblurring in real-world scenarios. To achieve this, a scale-aware network is proposed that allows flexible input spatial scales and can learn from different temporal scales of motion blur. A two-stage self-supervised learning scheme is developed to fit real-world data distribution. By considering the relative blurriness, the proposed approach efficiently restores brightness and structure in latent images and generalizes deblurring performance to handle varying spatial and temporal scales of motion blur. The method is extensively evaluated and demonstrates remarkable performance. Additionally, a real-world dataset consisting of multi-scale blurry frames and events is introduced to facilitate further research in event-based deblurring. The Multi-Scale Real-world Blurry Dataset (MS-RBD) and the Pytorch implementation of the proposed method can be accessed at: https://github.com/XiangZ-0/GEM.