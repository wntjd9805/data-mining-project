Generating a new view of a scene from a single input image is a challenging task due to uncertainty caused by unobserved elements and limited field-of-view. To address this, we propose a novel generative model that can produce a sequence of realistic images aligned with a specified camera trajectory. Our approach utilizes an autoregressive conditional diffusion-based model to interpolate visible scene elements and extrapolate unobserved regions consistently. We introduce a new metric, the thresholded symmetric epipolar distance (TSED), to measure the consistency of generated views. Unlike previous methods, our approach not only produces high-quality images but also maintains consistency with desired camera poses. More information can be found on our project page.