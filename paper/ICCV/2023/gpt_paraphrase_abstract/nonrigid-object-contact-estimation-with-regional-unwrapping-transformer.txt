The vision and robotics community often focuses on acquiring contact patterns between hands and rigid objects using learning-based methods. However, these methods face challenges when dealing with nonrigid objects, as the existing contact representation is limited by the object's geometry. This leads to unordered contact neighborhoods and difficulties aligning contact features with image cues. To address this, we introduce a new hand-object contact representation called RUPs (Region Unwrapping Profiles). RUPs unwrap the estimated hand-object surfaces into multiple high-resolution 2D regional profiles. This region grouping strategy aligns with the hand's bone division, which serves as the foundation for a composite contact pattern. Using the RUPs representation, our Regional Unwrapping Transformer (RUFormer) learns correlation priors across regions from monocular inputs and predicts corresponding contact and deformed transformations. Our experiments show that this framework accurately estimates deformed degrees and transformations, making it suitable for both nonrigid and rigid contact scenarios.