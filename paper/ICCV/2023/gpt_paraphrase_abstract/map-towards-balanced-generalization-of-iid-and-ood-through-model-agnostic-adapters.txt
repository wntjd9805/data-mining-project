In recent years, deep learning has achieved significant success but has mainly relied on the assumption of independent and identically distributed (IID) data. This limits its application in more challenging scenarios where the data is out-of-distribution (OOD). While several methods have been proposed to address this issue and have shown good performance on OOD data, we have discovered through experimentation that these methods sacrifice the performance on IID data. This finding, which we call the IID-OOD dilemma, highlights the need for a deep learning model that can balance generalization between IID and OOD scenarios, especially in real-world applications where distribution shifts can be uncertain and minor. To tackle this problem, we investigate the challenge of balancing IID and OOD generalizations and introduce a novel method called Model Agnostic adaPters (MAP). This method is designed to be reliable and effective for real-world data that is agnostic to distribution shifts. Our approach involves incorporating the inductive bias of IID into OOD methods by using auxiliary adapter layers. We employ bilevel optimization to explicitly model and optimize the relationship between the OOD model and auxiliary adapter layers. Additionally, we provide a first-order approximation to reduce computational time. Experimental results on six datasets demonstrate the effectiveness of MAP in improving IID performance while also achieving good OOD performance.