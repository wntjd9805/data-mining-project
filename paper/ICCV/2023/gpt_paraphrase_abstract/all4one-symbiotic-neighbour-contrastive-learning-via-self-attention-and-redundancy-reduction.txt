Nearest neighbour-based methods have been successful in self-supervised learning (SSL) due to their strong generalization abilities. However, their computational efficiency decreases when multiple neighbours are used. This paper introduces a new SSL approach called All4One, which addresses this issue by reducing the distance between neighbour representations using centroids created through a self-attention mechanism. All4One combines Centroid Contrasting, Neighbour Contrasting, and Feature Contrasting objectives. The use of centroids allows for learning contextual information from multiple neighbours, while neighbour contrast enables direct learning from neighbours and feature contrast enables learning unique representations. All4One outperforms popular instance discrimination approaches by more than 1% on linear classification evaluation and achieves state-of-the-art results on benchmark datasets. It is also robust to embedding dimensionalities and augmentations, surpassing NNCLR and Barlow Twins by more than 5% in low dimensionality and weak augmentation settings. The source code for All4One is available on GitHub. The simplified architecture of All4One is depicted in Figure 1. It utilizes three objective functions to contrast different representations: the Centroid objective contrasts contextual information from multiple neighbours, the Neighbour objective ensures diversity, and the Feature contrast objective measures correlation and increases independence of generated features.