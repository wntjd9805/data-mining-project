We suggest the utilization of Masked Autoencoders (MAEs) as effective learners for Class Incremental Learning (CIL), a method used to learn new classes sequentially without forgetting previous knowledge. MAEs were originally designed for unsupervised learning to acquire useful representations and can be easily combined with supervised loss for classification. Additionally, MAEs can reconstruct original input images from randomly chosen patches, allowing for more efficient storage of exemplars from past tasks in CIL. To enhance the quality of reconstructed images and the stability of representations, we propose a bilateral MAE framework that incorporates image-level and embedding-level fusion. Through experiments conducted on CIFAR-100, ImageNet-Subset, and ImageNet-Full datasets, we demonstrate that our approach outperforms the current state-of-the-art methods. The code for our approach is accessible at https://github.com/scok30/MAE-CIL.