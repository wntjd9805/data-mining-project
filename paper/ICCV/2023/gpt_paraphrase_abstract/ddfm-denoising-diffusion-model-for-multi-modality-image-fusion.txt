We propose a novel algorithm for multi-modality image fusion that combines different modalities while retaining their unique features. Our approach is based on the denoising diffusion probabilistic model (DDPM) to leverage strong generative priors and address challenges associated with GAN-based methods. We formulate the fusion task as a conditional generation problem under the DDPM sampling framework, which is divided into an unconditional generation subproblem and a maximum likelihood subproblem. The latter is modeled hierarchically with latent variables and inferred using the expectation-maximization (EM) algorithm. By integrating the inference solution into the diffusion sampling iteration, our method can generate high-quality fused images with natural image generative priors and cross-modality information. Importantly, our approach only requires an unconditional pre-trained generative model and does not require fine-tuning. Extensive experiments demonstrate promising fusion results in infrared-visible image fusion and medical image fusion. The code for our method is available at https://github.com/Zhaozixiang1228/MMIF-DDFM.