Most studies on nighttime semantic segmentation focus on domain adaptation techniques using image input. However, due to the limitations of conventional cameras in capturing details and boundaries in low-light conditions, structural information is often lost. This paper introduces a novel unsupervised Cross-Modality Domain Adaptation (CMDA) framework that leverages multi-modality information (images and events) for nighttime semantic segmentation, using labels only from daytime images. The CMDA framework incorporates an Image Motion-Extractor and an Image Content-Extractor to bridge the gap between different modalities and domains. Additionally, the paper introduces the first image-event nighttime semantic segmentation dataset. Experimental results on both public image datasets and the proposed image-event dataset demonstrate the effectiveness of the proposed approach. The code, models, and dataset are made available on GitHub. The paper also highlights the limitations of frame-based cameras in capturing nighttime details and the benefits of using event cameras with high dynamic range. The proposed CMDA framework improves semantic segmentation results in nighttime images by incorporating the event modality for the first time.