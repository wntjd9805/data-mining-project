This paper introduces a new framework called DK-DETR for open-vocabulary object detection (OVOD). Unlike existing methods that rely on pre-trained vision-language models (VLMs), DK-DETR proposes a simple yet effective approach to distill knowledge from VLMs to a DETR-like detector. The framework includes two innovative distillation schemes: semantic knowledge distillation (SKD) and relational knowledge distillation (RKD). SKD explicitly transfers semantic knowledge, while RKD leverages implicit relationship information between objects.To fully utilize the rich knowledge from the VLM, DK-DETR incorporates a distillation branch with auxiliary queries, which helps mitigate the negative impact on base categories. By integrating SKD and RKD into the distillation branch, DK-DETR significantly improves the detection performance of novel categories without disturbing the detection of base categories.Extensive experiments conducted on LVIS and COCO datasets demonstrate that DK-DETR outperforms existing OVOD methods when only base-category supervision is available. The code and models for DK-DETR are publicly available at https://github.com/hikvision-research/opera.