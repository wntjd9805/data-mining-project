This paper aims to enable machines to understand and interact with multiple potential objects in a similar way that humans do. We propose a transformer-based model that predicts the 3D location, physical properties, and affordance of objects. To train and validate our approach, we gather a dataset consisting of Internet videos, egocentric videos, and indoor images. Our model demonstrates high performance on our data and shows good generalization to robotics data.