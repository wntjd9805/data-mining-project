Dataset distillation is a technique used to create smaller datasets from larger ones in order to save storage and training costs. Current methods for synthesizing these smaller datasets focus on matching synthetic images to the original ones based on gradients, embedding distributions, or training trajectories. However, the strategy for selecting the original images to match is typically limited to random sampling, which overlooks the evenness of the sample distribution and may result in noisy or biased matching targets. Additionally, random sampling does not effectively capture sample diversity, leading to optimization instability and reduced training efficiency. To address these issues, we propose a new matching strategy called Dataset distillation byREpresentAtive Matching (DREAM), which selects only representative original images for matching. DREAM can easily be integrated into existing dataset distillation frameworks and significantly reduces the number of distilling iterations by more than 8 times without any decrease in performance. With sufficient training time, DREAM achieves state-of-the-art performances and provides significant improvements.