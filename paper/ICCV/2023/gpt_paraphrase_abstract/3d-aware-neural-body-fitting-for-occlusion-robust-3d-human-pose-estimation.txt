Regression-based methods for estimating 3D human pose from 2D images using deep networks have achieved impressive results on standard benchmarks. However, these methods suffer from degraded performance when faced with occlusion. Alternatively, optimization-based methods fit a parametric body model to 2D features iteratively, which can make them more robust to occlusion but introduces a 2D-3D ambiguity. Building on the success of generative models in rigid object pose estimation, we propose a novel approach called 3D-aware Neural Body Fitting (3DNBF) for accurate and occlusion-resistant 3D human pose estimation. Our method leverages a generative model that uses deep features based on a volumetric human representation with Gaussian ellipsoidal kernels emitting 3D pose-dependent feature vectors. These neural features are trained using contrastive learning to acquire 3D-awareness, thereby addressing the 2D-3D ambiguity. Experimental results demonstrate that 3DNBF outperforms other methods on both occluded and standard benchmarks. The code for our approach is publicly available at https://github.com/edz-o/3DNBF.