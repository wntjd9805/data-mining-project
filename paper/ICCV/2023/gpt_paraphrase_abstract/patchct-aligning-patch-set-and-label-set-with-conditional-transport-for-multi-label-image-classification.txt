This paper explores the semantic consistency between visual patches and linguistic labels in the context of multi-label image classification. Previous studies have used cross-modal attention-based approaches to align these representations, but they required complex operations and specially designed modules. The authors propose using conditional transport theory to minimize the cost of bidirectional transport between image and label representations. The images and labels are processed by modality-specific encoders, treating each image as a combination of patch embeddings and label embeddings. Conditional transport is then used to align these semantic sets by defining forward and backward navigators. This approach allows for the interpretation and visualization of learned prototypes. Experimental results on three public image benchmarks demonstrate that the proposed model consistently outperforms previous methods.