Anomaly detection (AD) is crucial in safety-critical applications to identify samples that deviate from the training distribution. While recent attempts using self-supervised learning have shown promise by creating virtual outliers, their training objectives do not align closely with AD requirements. This paper proposes a new approach called Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation (UniCon-HA) that addresses these shortcomings. UniCon-HA explicitly focuses on both a concentrated inlier distribution and a dispersed outlier distribution. It achieves this through supervised and unsupervised contrastive losses, which encourage the concentration of inliers and the dispersion of virtual outliers, respectively. To deal with the potential introduction of outliers during contrastive data augmentation, a soft mechanism is introduced to re-weight each augmented inlier based on its deviation from the inlier distribution, ensuring a purified concentration. Additionally, an easy-to-hard hierarchical augmentation strategy is adopted, inspired by curriculum learning, to promote a higher concentration. Contrastive aggregation is performed at different depths of the network based on the strengths of data augmentation. The proposed method is evaluated in three AD settings (unlabeled one-class, unlabeled multi-class, and labeled multi-class) and consistently outperforms other competitors.