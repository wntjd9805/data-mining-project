We present Encyclopedic-VQA, a large-scale dataset for visual question answering (VQA) that focuses on detailed properties of fine-grained categories and instances. The dataset consists of 221,000 unique question-answer pairs, with each pair matched to up to five images, resulting in a total of 1 million VQA samples. Additionally, our dataset includes a controlled knowledge base sourced from Wikipedia, which provides evidence to support each answer. Our empirical findings reveal that our dataset poses a significant challenge for large vision-language models, as they perform poorly on it. Even the state-of-the-art model, PaLI, achieves only 13.0% accuracy on our dataset compared to its high performance on OK-VQA. Furthermore, we conduct experiments that demonstrate the potential for improvement in answering our encyclopedic questions by augmenting large models with a mechanism to retrieve relevant information from the knowledge base. An oracle experiment with perfect retrieval achieves 87.0% accuracy on the single-hop portion of our dataset, while an automatic retrieval-augmented prototype achieves 48.8%. We believe that our dataset will facilitate future research on retrieval-augmented vision-language models. The dataset focuses primarily on providing abstract answers.