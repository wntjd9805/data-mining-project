Diffusion probabilistic models have been successful in generating images based on text guidance. However, generating 3D shapes is challenging due to the lack of sufficient data and the inherent ambiguity of text-based descriptions. This paper proposes a model that combines hand-drawn sketches and textual descriptions to generate colored point clouds. The model uses a joint diffusion process to transform point coordinates and colors into a Gaussian distribution, and then learns the reverse diffusion process to reconstruct the desired shape and color. To improve the sketch-text embedding, the model incorporates a capsule attention network. The model also employs staged diffusion to generate shapes and assign colors based on appearance prompts, while preserving precise shapes from earlier stages. The flexibility of the model allows it to be extended to other tasks like appearance re-editing and part segmentation. Experimental results demonstrate that the model outperforms current state-of-the-art methods in point cloud generation.