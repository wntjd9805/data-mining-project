Recent advancements in latent diffusion have paved the way for a new approach to generating realistic images from tactile signals. While previous methods have only addressed specific aspects of visuo-tactile synthesis, and have fallen short compared to cross-modal synthesis in other domains, our model leverages these advancements to tackle a range of visuo-tactile synthesis tasks. Notably, we surpass previous work in the tactile-driven stylization problem, where an image is manipulated to match a touch signal, and we achieve this without the need for additional scene information. Furthermore, our model successfully addresses two novel synthesis challenges: generating images without the presence of the touch sensor or the hand holding it, and estimating an image's shading based on its reflectance and touch. For more information, please refer to our project page: https://fredfyyang.github.io/vision-from-touch/.