Recent research has shown that self-attentions, which are used in models, function as low-pass filters rather than convolutions. It has been found that enhancing the high-pass filtering ability of these self-attentions improves the overall performance of the models. However, contrary to this notion, we have conducted an investigation on existing convolution-based models using spectral analysis and have discovered that improving the low-pass filtering in convolution operations also leads to performance enhancements. Based on this observation, we propose the utilization of optimal token mixers that capture balanced representations of both high- and low-frequency components to further enhance model performance. To validate this hypothesis, we decompose visual features into the frequency domain and combine them in a balanced manner. To address this, we replace the challenge of achieving balance with a mask filtering problem in the frequency domain. Furthermore, we introduce a novel token-mixer called SPAM and leverage it to develop a MetaFormer model known as SPANet. Experimental results demonstrate that our proposed method effectively achieves this balance and that the balanced representations of both high- and low-frequency components significantly improve model performance across various computer vision tasks. The code for our approach is publicly available at https://doranlyong.github.io/projects/spanet/.