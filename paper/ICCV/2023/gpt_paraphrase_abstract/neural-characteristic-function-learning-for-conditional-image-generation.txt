The development of conditional generative adversarial networks (cGANs) has revolutionized the way we generate and control data by learning joint distributions using adversarial techniques. However, cGANs have faced criticism for their flawed discrepancy measure between distributions, resulting in training issues such as mode collapse and instability. To overcome this problem, we propose a new approach called conditional characteristic function generative adversarial network (CCF-GAN). The CCF-GAN utilizes characteristic functions (CFs) to accurately measure the distance between joint distributions, providing a theoretical foundation. We demonstrate that the difference between CFs is a comprehensive and optimization-friendly measure of discrepancy. To address the computational challenge of calculating CF differences, we introduce neural CF (NCF), a neural network that efficiently minimizes an upper bound of the difference. Using NCF, we establish the CCF-GAN framework, which decomposes CFs of joint distributions and enables the learning of data distribution and auxiliary information with classified importance. Our experimental results on both synthetic and real-world datasets confirm the superior performance of CCF-GAN in terms of generation quality and stability.