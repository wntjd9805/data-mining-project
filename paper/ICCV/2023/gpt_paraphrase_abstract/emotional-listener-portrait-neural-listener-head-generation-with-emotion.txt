Generating non-verbal behaviors of a listener in response to a speaker's information is a challenge due to the unpredictable nature of facial expressions, which are influenced by the emotions and attitudes of both individuals. To address this problem, we introduce the Emotional Listener Portrait (ELP) model. ELP represents each facial motion as a combination of discrete motion-codewords and explicitly models the probability distribution of these motions under different emotions during a conversation. The ELP model can automatically generate diverse and natural responses by sampling from the learned distribution, as well as generate controlled responses with a predetermined attitude. Our ELP model outperforms previous methods according to various quantitative metrics.