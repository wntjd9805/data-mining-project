Visible-infrared person re-identification (VI-ReID) is a challenging task due to differences between visible and infrared modalities and variations within classes. Existing methods mainly focus on learning representations that are shared across modalities, which neglects modality-specific and identity-aware information that is valuable for Re-ID. To address these issues, we propose a novel Modality Unifying Network (MUN) for VI-ReID. First, we generate an auxiliary modality by combining a cross-modality learner and an intra-modality learner, which captures both modality-specific and modality-shared representations to reduce variations. Second, we introduce an identity alignment loss function to align identity centers across the three modalities and discover discriminative feature representations. Third, we introduce a modality alignment loss to reduce the distribution distance between visible and infrared images through modality prototype modeling. Extensive experiments on multiple datasets show that our method outperforms current state-of-the-art methods significantly.