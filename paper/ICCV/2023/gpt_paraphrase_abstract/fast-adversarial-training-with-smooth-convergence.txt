Fast adversarial training (FAT) is a technique that enhances the ability of neural networks to withstand adversarial attacks. However, previous FAT approaches have encountered a major problem called catastrophic overfitting, which occurs when the models become highly vulnerable to attacks during training. To address this issue, we examined the training process of existing FAT methods and identified a correlation between catastrophic overfitting and the presence of outliers in the loss convergence. Therefore, we propose that a stable FAT process should exhibit a moderately smooth loss convergence. To achieve this, we introduce a new constraint called ConvergeSmooth, which limits the difference in loss between consecutive training epochs. We also introduce a convergence stride in ConvergeSmooth to balance the trade-off between convergence and smoothing. Additionally, we devise a weight centralization technique that does not require any additional hyperparameters apart from the loss balance coefficient. These methods are not specific to any particular attack and can improve the training stability of various FAT techniques. Extensive experiments conducted on popular datasets demonstrate that our proposed methods effectively prevent catastrophic overfitting and outperform all previous FAT approaches. The code for our methods is available at https://github.com/FAT-CS/ConvergeSmooth.