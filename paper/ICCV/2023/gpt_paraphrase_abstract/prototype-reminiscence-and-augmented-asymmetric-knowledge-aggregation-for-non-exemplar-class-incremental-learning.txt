Non-exemplar class-incremental learning (NECIL) is a method that enables deep models to learn new classes without storing old class samples, while still maintaining existing knowledge. Typically, NECIL methods store prototypical representations to prevent catastrophic forgetting when learning new classes. However, as the model continuously learns new knowledge, these stored representations may not accurately represent the properties of old classes when knowledge updates occur.   To address this issue, we propose a new prototype reminiscence mechanism. This mechanism combines previous class prototypes with incoming new class features, allowing for the dynamic reshaping of old class feature distributions. As a result, the decision boundaries of previous tasks are preserved. Additionally, we introduce an augmented asymmetric knowledge aggregation approach to enhance model generalization for both new and old classes. This approach aggregates the overall knowledge of the current task while extracting valuable knowledge from past tasks, along with self-supervised label augmentation.  Experimental results on three benchmarks demonstrate the superior performance of our approach compared to state-of-the-art methods.