Accurate calibration of cameras is crucial for accurate 3D perception. However, traditional calibration methods require known geometric targets. In this study, we propose a self-calibration approach that can infer camera intrinsics in real-world scenarios using monocular videos. We achieve this by explicitly modeling projection functions and multi-view geometry and leveraging deep neural networks for feature extraction and matching. To accomplish this, we integrate bundle adjustment into deep learning models and introduce a self-calibrating bundle adjustment layer. This layer optimizes camera intrinsics using Gau√ü-Newton steps and can be adapted to different camera models without re-training. We implemented this layer in the DROID-SLAM system, resulting in a model called DroidCalib, which achieves state-of-the-art calibration accuracy on various datasets. Our results demonstrate that DroidCalib can generalize to unseen environments and different camera models, even with significant lens distortion. This approach enables 3D perception tasks without prior knowledge of the camera. The code for DroidCalib is available at https://github.com/boschresearch/droidcalib.