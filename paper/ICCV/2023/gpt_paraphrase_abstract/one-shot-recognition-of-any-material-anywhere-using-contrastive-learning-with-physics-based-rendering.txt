The recognition of materials and their states through visual cues is crucial for understanding various aspects of the world, such as determining the level of food preparation, the presence of rust on metal, or the occurrence of a chemical reaction. However, existing image recognition methods are limited in their ability to handle the wide range of material states that exist. To address this limitation, we introduce MatSim, which is the first dataset and benchmark designed for computer vision-based recognition of material similarities and transitions. The focus of MatSim is to identify any material under any conditions using just one or a few example images.  The dataset comprises both synthetic and natural images. Synthetic images were created using a vast collection of textures, objects, and environments generated by computer graphics artists. These synthetic images include mixtures and gradual transitions between materials to enable the system to learn cases with smooth transitions between states, such as food being gradually cooked. Additionally, images with materials inside transparent containers were rendered to support applications in the beverage and chemistry lab domains.  The dataset was used to train a Siamese network, which can identify the same material in different objects, mixtures, and environments. The network generates a descriptor that can be utilized to identify the states of materials and their subclasses using just a single image. Furthermore, we introduce a novel benchmark for few-shot material recognition, which includes natural images from diverse fields such as food and beverage states, ground types, and other practical scenarios. Our experiments demonstrate that a network trained on the MatSim synthetic dataset outperforms state-of-the-art models like Clip on this benchmark and achieves satisfactory results on other unsupervised material classification tasks.  To facilitate further research and development, we have made the MatSim dataset, generation code, and trained models available at the following link: https://github.com/ZuseZ4/MatSim-Dataset-Generator-Scripts-And-Neural-net. The authors of this work are affiliated with Vector Institute, University of Toronto, Karlsruhe Institute of Technology, and Innoviz, and can be contacted at the provided email addresses. Figure 1 illustrates the MatSim benchmark, showcasing the ability to identify materials from various perspectives using just one or a few natural images, enabling few-shot learning. The figure presents the top-1 results achieved by ConvNeXt trained on the MatSim dataset and pretrained Clip H14 on material classes that were not seen during training. Only a selection of samples is displayed.