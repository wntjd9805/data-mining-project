We propose a Multimodal Interlaced Transformer (MIT) for weakly supervised point cloud segmentation, considering both 2D and 3D data. Previous methods required additional 2D annotations for integrating 2D and 3D features. However, due to the high cost of point cloud annotations, there is a need for effective fusion based on weakly supervised learning. Our transformer model includes two encoders and one decoder, using only scene-level class tags. The encoders generate self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder performs interlaced 2D-3D cross-attention and fusion of implicit 2D and 3D features. We alternate the roles of queries and key-value pairs in the decoder layers, iteratively enriching the 2D and 3D features. Experimental results on the S3DIS and ScanNet benchmarks demonstrate the superior performance of our method compared to existing weakly supervised point cloud segmentation methods. More information about the project can be found at https://jimmy15923.github.io/mit_web/.