Test-time adaptation methods often rely on a model's predictions to adapt a pretrained model to an unlabeled target domain. However, these methods can be hindered by noisy signals caused by incorrect or open-set predictions. To mitigate this issue, we propose a simple yet effective sample selection method inspired by an empirical finding. While entropy minimization aims to increase the confidence values of predicted labels, we observed that noisy samples tend to show decreased confidence values. Therefore, we filter out samples whose confidence values are lower in the adapted model compared to the original model, as they are likely to be noisy. This approach improves the long-term adaptation performance of existing test-time adaptation methods in image classification and semantic segmentation tasks. For example, we achieved a 49.4% reduction in error rates with TENT in image classification and an 11.7% increase in mIoU with TENT in semantic segmentation.