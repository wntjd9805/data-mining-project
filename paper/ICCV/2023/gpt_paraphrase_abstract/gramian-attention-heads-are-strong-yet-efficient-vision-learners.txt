We present a new architecture design that improves expressiveness by incorporating multiple classification heads instead of using channel expansion or additional building blocks. Our approach uses attention-based aggregation, leveraging pairwise feature similarity to enhance the lightweight heads with minimal resource overhead. To strengthen class tokens, we compute the Gramian matrices in an attention layer for each head. This allows the heads to learn more distinctive representations and improve their aggregation capabilities. Additionally, we propose a learning algorithm that encourages the heads to complement each other by reducing correlation in the aggregation process. Our models surpass state-of-the-art CNNs and ViTs in terms of accuracy-throughput trade-off on ImageNet-1K and demonstrate impressive performance across various downstream tasks, including COCO object instance segmentation, ADE20k semantic segmentation, and fine-grained visual classification datasets. We validate the effectiveness of our framework through practical experiments and support it with a generalization error bound. The code for our approach is publicly available at: https://github.com/Lab-LVM/imagenet-models.