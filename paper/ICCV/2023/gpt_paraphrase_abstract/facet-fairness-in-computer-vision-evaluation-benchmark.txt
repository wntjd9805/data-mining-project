Computer vision models have demonstrated disparities in performance based on attributes such as gender and skin tone. However, there has been a lack of a unified approach to quantifying these differences in common computer vision tasks. To address this, we introduce FACET (FAirness in Computer Vision EvaluaTion), a comprehensive benchmark consisting of 32k images for image classification, object detection, and segmentation. Each image in FACET is manually annotated by expert reviewers, including person-related attributes like skin tone and hair type, bounding boxes, and fine-grained person-related classes. Using FACET, we evaluate state-of-the-art vision models and gain insights into performance disparities and challenges across sensitive demographic attributes. By examining single and multiple demographics attributes, we find that classification, detection, segmentation, and visual grounding models exhibit performance disparities across demographic attributes and intersections of attributes. These disparities indicate that not all individuals in datasets are treated fairly and equitably in vision tasks. We expect that the use of our benchmark will contribute to the development of fairer and more robust vision models. FACET is publicly available at https://facet.metademolab.com.