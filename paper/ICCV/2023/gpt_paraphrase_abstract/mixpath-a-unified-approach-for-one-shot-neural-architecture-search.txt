Blending multiple convolutional kernels is beneficial in neural architecture design, but current methods for searching multi-path structures are limited. To address this, we propose training a one-shot multi-path supernet to accurately evaluate candidate architectures. We observe that feature vectors from multiple paths are nearly multiples of those from a single path, which disrupts the supernet training and ranking. To mitigate this issue, we introduce a novel mechanism called Shadow Batch Normalization (SBN) to regulate the disparate feature statistics. Extensive experiments demonstrate that SBNs stabilize optimization and improve ranking performance. Our unified multi-path approach, named MixPath, generates a series of models that achieve state-of-the-art results on ImageNet.