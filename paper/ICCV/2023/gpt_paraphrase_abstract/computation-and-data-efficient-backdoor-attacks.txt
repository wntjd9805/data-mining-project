Backdoor attacks on deep neural network (DNN) models have been extensively studied, with various techniques proposed for different domains such as language processing and transfer learning. The most common method of embedding a backdoor into a DNN model is by poisoning the training data. However, existing approaches randomly select samples from the benign training set for poisoning, without considering the individual contribution of each sample to the backdoor effectiveness. This leads to suboptimal attacks.  A recent study introduced the use of forgetting scores to measure the importance of each poisoned sample and filter out redundant data for effective backdoor training. However, this method is empirically designed and time-consuming as it requires multiple training stages for data selection. To address these limitations, we propose a novel confidence-based scoring methodology that efficiently measures the contribution of each poisoning sample based on the distance posteriors. We also introduce a greedy search algorithm to find the most informative samples for backdoor injection more promptly.  Experimental evaluations on both 2D image and 3D point cloud classification tasks demonstrate that our approach achieves comparable or even superior performance compared to the forgetting score-based method, while only requiring a few extra epochs of computation during the standard training process. The code for our approach can be accessed at https://github.com/WU-YU-TONG/computational_efficient_backdoor.