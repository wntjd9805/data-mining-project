Deep clustering is a powerful method for simultaneously improving representation learning and exploring data distribution. However, it often leads to a trivial solution where all instances collapse to the same features. To address this, a two-stage training strategy is proposed that involves pre-training for representation learning and fine-tuning for clustering. Additionally, one-stage methods focus on representation learning and use constraints to prevent collapsing. Despite their success, there is a need for a suitable learning objective for deep clustering. This study introduces a stable cluster discrimination task called SeCu, which addresses the instability of the prevalent discrimination task used in supervised learning. A new hardness-aware clustering criterion is also proposed. Furthermore, the study investigates a global entropy constraint for cluster assignments. Experimental results on benchmark datasets and ImageNet demonstrate that SeCu achieves state-of-the-art performance, highlighting the effectiveness of one-stage deep clustering.