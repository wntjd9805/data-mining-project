The literature extensively covers the topic of reconstructing 3D objects from a single 2D image, but this method relies on depth supervision during training, which limits its practicality. To address this limitation, we propose a self-supervised monocular scene reconstruction technique called SceneRF. Unlike previous methods, SceneRF only requires posed image sequences for training, eliminating the need for depth supervision. Inspired by recent advancements in neural radiance fields (NeRF), we optimize a radiance field by incorporating explicit depth optimization and a novel probabilistic sampling strategy that efficiently handles large scenes. During inference, a single input image is sufficient to generate new depth views, which are then fused together to reconstruct the 3D scene. Through comprehensive experiments, we demonstrate that SceneRF outperforms all baseline methods in terms of synthesizing novel depth views and reconstructing scenes, as evidenced by its superior performance on indoor BundleFusion and outdoor SemanticKITTI datasets. The code for SceneRF is available at https://astra-vision.github.io/SceneRF.