We propose a method to accelerate transformer-based 3D object detectors for real-time safety-critical systems like autonomous vehicles. Our approach involves halting tokens at different layers based on their contribution to the detection task, allowing for efficient and accurate model deployment. Although token halting is non-differentiable, we enable differentiable end-to-end learning through a equivalent differentiable forward-pass. Additionally, our framework allows halted tokens to be reused to enhance the model's predictions via a token recycling mechanism. Compared to existing approaches, our method significantly improves the trade-off between efficiency and accuracy. By halting tokens and increasing model capacity, we enhance the performance of the baseline model without increasing latency on the Waymo Open Dataset.