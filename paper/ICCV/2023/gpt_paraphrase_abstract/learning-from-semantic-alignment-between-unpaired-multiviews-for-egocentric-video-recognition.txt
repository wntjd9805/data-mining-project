We propose Semantics-based Unpaired Multiview Learning (SUM-L) to address the challenge of learning comprehensive multiview representations with varying cross-view semantic information. Our approach involves creating cross-view pseudo-pairs and aligning views invariantly using the semantic information of videos. To enhance the efficiency of multiview learning, we also align video and text data for first-person and third-person videos, leveraging semantic knowledge to improve video representations. Our framework demonstrates effectiveness through extensive experiments on benchmark datasets, outperforming existing view-alignment methods in more challenging scenarios. The code for our method is available at https://github.com/wqtwjt1996/SUM-L.