Current models for bird's-eye view (BEV) perception lack the ability to generate realistic and coherent semantic map layouts and do not consider uncertainties caused by partial sensor information. This study introduces MapPrior, a new BEV perception framework that combines a traditional discriminative model with a learned generative model for semantic map layouts. MapPrior produces more accurate, realistic, and uncertainty-aware predictions. The model is evaluated on the nuScenes benchmark and outperforms the strongest competing method in both camera- and LiDAR-based BEV perception, as demonstrated by improved MMD and ECE scores. Additionally, MapPrior can be used to continuously generate layouts through unconditional sampling.