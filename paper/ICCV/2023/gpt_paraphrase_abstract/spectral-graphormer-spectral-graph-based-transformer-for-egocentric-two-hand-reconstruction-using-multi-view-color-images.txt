We introduce a new transformer-based framework for reconstructing two high-quality hand models from multi-view RGB images. Unlike existing methods that focus on regressing hand model parameters from single images, our approach tackles the more challenging task of directly regressing the absolute root poses of two hands with extended forearms at high resolution from an egocentric viewpoint. To address the limitations of current datasets, which either lack egocentric viewpoints or background variations, we create a large-scale synthetic dataset with diverse scenarios and collect a real dataset from a multi-calibrated camera setup to validate our proposed multi-view image feature fusion strategy. To ensure realistic reconstructions, we propose two strategies: (i) a coarse-to-fine spectral graph convolution decoder that smoothens the meshes during upsampling, and (ii) an optimization-based refinement stage during inference to prevent self-penetrations. Through comprehensive quantitative and qualitative evaluations, we demonstrate that our framework produces realistic two-hand reconstructions and showcases the generalization of synthetic-trained models to real data, as well as their potential for real-time AR/VR applications.