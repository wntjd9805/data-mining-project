Generating human-like motions based on text input is a challenging task that requires aligning the motions with the input while also adhering to human capabilities and physical laws. While diffusion models have made progress, their application in discrete spaces has not been thoroughly explored. Current methods often overlook the varying significance of different motions and treat them uniformly, disregarding the fact that not all motions hold the same relevance to a given textual description. Some motions are more salient and informative and should be given priority during generation. To address this issue, we propose a Priority-Centric Motion Discrete Diffusion Model (M2DM). Our model incorporates a Transformer-based VQ-VAE to derive a concise, discrete representation of motion. It utilizes a global self-attention mechanism and a regularization term to prevent code collapse. Additionally, we introduce a motion discrete diffusion model that employs a novel noise schedule determined by the significance of each motion token in the sequence. This approach preserves the most salient motions during the reverse diffusion process, resulting in more semantically rich and varied motions. We also develop two strategies to determine the importance of motion tokens, considering both textual and visual indicators. Through comprehensive experiments on the HumanML3D and KIT-ML datasets, we demonstrate that our model outperforms existing techniques in terms of fidelity and diversity, especially for complex textual descriptions.