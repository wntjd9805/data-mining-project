Current methods for generating photorealistic and multi-view 3D consistent images using 3D-aware GANs face challenges in achieving both simultaneously. CNN-based 2D super-resolution techniques can enhance photorealism but compromise strict 3D consistency. On the other hand, learning high-resolution 3D representations for direct rendering compromises image quality. In this paper, we propose a novel learning strategy called 3D-to-2D imitation. This strategy allows a 3D-aware GAN to generate high-quality images while maintaining strict 3D consistency by having the images synthesized by the generator's 3D rendering branch imitate those generated by its 2D super-resolution branch. We also introduce 3D-aware convolutions into the generator to improve 3D representation learning, further enhancing image generation quality. Our method achieves FID scores of 5.4 and 4.3 on 512 resolution FFHQ and AFHQ-v2 Cats, respectively, outperforming existing 3D-aware GANs using direct 3D rendering and approaching the previous state-of-the-art method that leverages 2D super-resolution. Project website: https://seanchenxy.github.io/Mimic3DWeb.