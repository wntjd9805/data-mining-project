We propose a novel method for learning motion-focused video representations through self-supervision. Unlike existing approaches that focus on minimizing distances between temporally augmented videos with high spatial similarity, we suggest learning similarities between videos that have similar local motion dynamics but different appearances. To achieve this, we introduce synthetic motion trajectories called tubelets by adding motion patterns beyond what is present in the pretraining data, using techniques like scaling and rotation. Our approach demonstrates remarkable data efficiency, as it maintains performance even when using only 25% of the pre-training videos. We validate our method through experiments conducted on 10 diverse downstream settings, which showcase our competitive performance and generalizability to new domains and fine-grained actions. The code for our method is available at https://github.com/fmthoker/tubelet-contrast.