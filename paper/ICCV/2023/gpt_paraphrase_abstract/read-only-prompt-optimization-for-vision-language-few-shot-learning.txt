In recent years, researchers have found that prompt tuning is effective in adapting pre-trained vision-language models for specific tasks. This involves introducing learnable prompts while keeping the pre-trained weights unchanged. However, using learnable prompts can impact the internal representation within the self-attention module, leading to performance variability and limited generalization in scenarios with limited data.  To address these challenges, we propose a new approach called Read-only Prompt Optimization (RPO). RPO utilizes masked attention to prevent any shift in the internal representation of the pre-trained model. Additionally, to enhance the optimization process of RPO, the read-only prompts are initialized based on special tokens of the pre-trained model.  Extensive experiments demonstrate that RPO outperforms existing methods like CLIP and CoCoOp in terms of generalization from base-to-new tasks and across different domains. RPO also exhibits better robustness and achieves improved generalization in extremely data-deficient settings. Furthermore, RPO enhances parameter efficiency and computational overhead.  For those interested in implementing RPO, the code is available at https://github.com/mlvlab/RPO.