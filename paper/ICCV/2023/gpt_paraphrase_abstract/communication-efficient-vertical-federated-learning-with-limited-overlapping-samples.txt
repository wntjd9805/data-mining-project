Federated learning is a collaborative approach that allows clients to train a global model without sharing their local data. Vertical federated learning (VFL) addresses scenarios where clients have different feature spaces but share some overlapping samples. However, existing VFL methods have high communication costs and struggle to handle limited overlapping samples commonly found in real-world situations. To address these challenges, we propose a practical VFL framework called one-shot VFL, which simultaneously solves the communication bottleneck and limited overlapping samples problem using semi-supervised learning. Additionally, we introduce few-shot VFL to further improve accuracy with just one additional communication round between the server and clients. In our framework, clients only need to communicate with the server once or a few times. We evaluate the proposed VFL framework on image and tabular datasets and achieve significant improvements in accuracy, surpassing state-of-the-art VFL methods. Our methods reduce communication costs by over 330Ã— compared to existing approaches when evaluated on CIFAR-10. The code for our framework is available at https://nvidia.github.io/NVFlare/research/one-shot-vfl.