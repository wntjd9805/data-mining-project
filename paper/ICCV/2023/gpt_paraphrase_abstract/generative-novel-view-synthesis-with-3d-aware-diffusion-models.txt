We introduce a diffusion-based model that can generate 3D-aware novel views using just one input image. Our model is able to sample from the range of possible renderings that align with the input image, and can produce diverse and believable novel views even in situations where there is ambiguity. We achieve this by incorporating geometry priors in the form of a 3D feature volume into existing 2D diffusion backbones. This feature volume captures the distribution of potential scene representations and enhances our model's ability to generate novel renderings that are consistent with the input view. Moreover, our method can also generate 3D-consistent sequences in an autoregressive manner. We showcase state-of-the-art results on synthetic renderings and room-scale scenes, as well as impressive outcomes for challenging real-world objects.