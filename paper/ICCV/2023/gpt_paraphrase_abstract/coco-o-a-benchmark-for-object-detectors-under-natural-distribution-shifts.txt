The issue of object detection losing effectiveness when faced with image inputs that have natural distribution shifts has led to a focus on the robustness of detectors under Out-Of-Distribution (OOD) inputs. However, current benchmark datasets for OOD robustness lack universality and are not suitable for general detectors built on common tasks like COCO. To address this, we introduce COCO-O, a test dataset based on COCO that includes 6 types of natural distribution shifts. COCO-O exhibits a significant 55.7% relative performance drop on a Faster R-CNN detector due to its large distribution gap with training data. We utilize COCO-O to assess the credibility of improvements in over 100 modern object detectors, and unfortunately, most classic detectors do not demonstrate strong OOD generalization. We also investigate the impact of architecture design, augmentation, and pre-training techniques on robustness, revealing that the backbone is the most important component for robustness and that end-to-end detection transformer designs do not enhance robustness, and may even reduce it. Additionally, large-scale foundation models have shown significant progress in robust object detection. Our COCO-O dataset aims to provide a comprehensive testbed for studying the robustness of object detection.