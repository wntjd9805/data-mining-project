Despite efforts to develop cross-lingual translation techniques, such as machine translation and audio speech translation, to overcome language barriers in multi-media communications, there is a lack of research on visual speech. This is primarily due to the absence of datasets containing visual speech and translated text pairs. To address this gap, we introduce AVMuST-TED, the first dataset for Audio-Visual Multilingual Speech Translation, derived from TED talks. However, visual speech is not as easily distinguishable as audio speech, making it challenging to map source speech phonemes to target language text. To overcome this challenge, we propose MixSpeech, a framework that utilizes audio speech to improve the training of visual speech tasks. To further minimize the gap between modalities and improve knowledge transfer, we suggest using mixed speech, which combines audio and visual streams, and implementing a curriculum learning strategy to adjust the mixing ratio as needed. MixSpeech improves speech translation in noisy environments, resulting in higher BLEU scores (+1.4 to +4.2) for four languages in AVMuST-TED. Additionally, MixSpeech achieves state-of-the-art performance in lip reading on CMLR (11.1%), LRS2 (25.5%), and LRS3 (28.0%).