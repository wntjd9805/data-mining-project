In traditional monocular 3D human pose estimation, a machine learning model is used to predict the most likely 3D pose for a given input image. However, this approach can be problematic as a single image can have multiple plausible solutions for the 2D-3D lifting step, leading to overconfident predictions. To address this issue, we propose DiffPose, a conditional diffusion model that predicts multiple hypotheses for a given input image. Unlike similar approaches, our diffusion model is straightforward and does not require intensive hyperparameter tuning or complex network structures. It also avoids problems like mode collapse and unstable training.Additionally, we address the problem of over-simplification in the intermediate representation of common two-step approaches. These approaches first estimate a distribution of 2D joint locations using joint-wise heatmaps and then use the maximum argument of these heatmaps for the 3D pose estimation. However, this simplification removes valid information about possibly correct but labeled unlikely joint locations. In our proposed method, we represent the heatmaps as a set of 2D joint candidate samples to retain this information. To extract the original distribution from these samples, we introduce an embedding transformer that conditions the diffusion model.Through experiments, we demonstrate that DiffPose outperforms the state of the art in multi-hypothesis pose estimation. It achieves a 3-5% improvement for simple poses and significantly outperforms existing methods for highly ambiguous poses.