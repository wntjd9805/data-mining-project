Active learning (AL) is a technique used to select the most valuable data samples from a pool of unlabeled data and annotate them to expand the labeled dataset. Uncertainty-based methods, which focus on selecting the most uncertain samples, have been effective in improving model performance. However, these methods often overlook the training dynamics (TD) of the model, which refers to the ever-changing behavior of the model during optimization through stochastic gradient descent. Other research areas have shown that TD provides important insights into measuring data uncertainty. In this study, we present theoretical and empirical evidence to support the use of the ever-changing model behavior instead of relying solely on fully trained model snapshots. We propose a new AL method called Training Dynamics for Active Learning (TiDAL) that efficiently predicts the training dynamics of unlabeled data to estimate their uncertainty. Experimental results demonstrate that TiDAL outperforms or performs comparably to state-of-the-art AL methods on both balanced and imbalanced benchmark datasets. These methods estimate data uncertainty using only static information after model training.