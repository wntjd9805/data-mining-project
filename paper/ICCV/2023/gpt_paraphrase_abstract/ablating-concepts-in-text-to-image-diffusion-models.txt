Large-scale text-to-image models are capable of producing high-quality images and have strong compositional abilities. However, these models are often trained on vast amounts of Internet data, which may include copyrighted content, licensed images, and personal photos. Moreover, these models have been observed to mimic the style of different artists or memorize specific training samples. The challenge is to remove copyrighted concepts or images from the model without having to retrain it entirely. To address this issue, we propose an efficient method that involves ablating concepts in the pretrained model, thereby preventing the generation of specific target concepts. Our algorithm learns to match the distribution of images corresponding to a desired style, instance, or text prompt with the distribution of images associated with an anchor concept. This effectively prevents the model from generating the target concepts based on the provided text input. Extensive experiments demonstrate that our method successfully eliminates the generation of the ablated concept while preserving closely related concepts within the model.