Vision transformers have achieved impressive results in various visual tasks, but they suffer from high computational complexity, especially in dense prediction tasks like semantic segmentation. While removing less attentive tokens has been explored for image classification, it cannot be directly applied to semantic segmentation, where dense prediction is required for every patch. In this study, we propose a Dynamic Token Pruning (DToP) method that leverages the early exit of tokens to address this issue. Inspired by the coarse-to-fine segmentation process in human vision, we divide the commonly used auxiliary-loss-based network architecture into multiple stages, where each auxiliary block assesses the difficulty level of each token. By predicting the easier tokens in advance, we can reduce computational costs without completing the entire forward pass. Additionally, we retain the k highest confidence tokens for each semantic category to preserve representative context information. This approach allows the computational complexity to adapt to the difficulty of the input, resembling how humans perform segmentation. Experimental results demonstrate that our DToP architecture reduces computational costs by an average of 20% to 35% for existing semantic segmentation methods based on plain vision transformers, without sacrificing accuracy. The code for our method is available at: https://github.com/zbwxp/Dynamic-Token-Pruning.