We present a straightforward and effective framework, called DDP, for dense visual predictions using the conditional diffusion pipeline. Our method adopts a "noise-to-map" generative approach, gradually removing noise from a random Gaussian distribution guided by the input image. DDP seamlessly integrates the denoising diffusion process into the modern perception pipeline, making it applicable to various dense prediction tasks such as semantic segmentation and depth estimation without requiring task-specific design or architecture customization. DDP offers advantages like dynamic inference and uncertainty awareness, in contrast to previous single-step discriminative methods. We demonstrate DDP's superior performance on three representative tasks, including semantic segmentation, BEV map segmentation, and depth estimation, across six diverse benchmarks. Without any additional techniques, DDP achieves state-of-the-art or competitive results compared to specialized methods on each task. Our aim is for DDP to serve as a strong foundation and facilitate future research in this field.