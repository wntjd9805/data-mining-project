The abstract discusses the limitations of existing LiDAR-camera 3D object detection methods and proposes a new method called SparseFusion. The existing methods either find dense candidates or generate dense representations, which is noisy and inefficient since objects only occupy a small part of a scene. SparseFusion exclusively uses sparse candidates and representations to address this issue. It utilizes parallel detectors in LiDAR and camera modalities to obtain sparse candidates for fusion. The camera candidates are transformed into the LiDAR coordinate space, and a lightweight self-attention module is used to fuse the multi-modality candidates in a unified 3D space. To prevent negative transfer between modalities, semantic and geometric cross-modality transfer modules are applied before modality-specific detectors. SparseFusion achieves state-of-the-art performance on the nuScenes benchmark while also being the fastest method, outperforming those with stronger backbones. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of the proposed modules and overall method pipeline. The code for SparseFusion will be publicly available at the provided GitHub link.