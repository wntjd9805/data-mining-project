We introduce a method to effectively and efficiently adapt a vanilla VisionTransformer (ViT) that has been pre-trained using masked image modeling (MIM) for object detection. Our approach is based on two key observations. Firstly, we find that a MIM pre-trained vanilla ViT encoder performs surprisingly well in object-level recognition scenarios, even when only a fraction of the input embeddings (25% to 50%) are randomly sampled. Secondly, to create multi-scale representations for object detection from a single-scale ViT, we replace the pre-trained patchify stem with a compact convolutional stem that is randomly initialized. The intermediate features of this stem naturally serve as higher resolution inputs for a feature pyramid network, eliminating the need for further upsampling or manipulations. In our proposed detector, called MIMDET, the pre-trained ViT is used as the third stage of the backbone, resulting in a hybrid architecture combining ConvNet and ViT. MIMDET outperforms other hierarchical architectures such as SwinTransformer, MViTv2, and ConvNeXt in COCO object detection and instance segmentation. It also achieves better results compared to the previous best adapted vanilla ViT detector, while converging 2.8 times faster. Our code and pre-trained models are available at https://github.com/hustvl/MIMDet.