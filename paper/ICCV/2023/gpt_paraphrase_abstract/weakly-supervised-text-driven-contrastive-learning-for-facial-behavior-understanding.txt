Contrastive learning has shown promise in learning robust representations using unlabeled data. However, creating effective positive-negative pairs for contrastive learning on facial behavior datasets is challenging because these pairs often contain subject-ID information, and randomly constructed pairs may separate similar facial images due to limited subject diversity. To address this issue, we propose utilizing activity descriptions, a type of coarse-grained information available in some datasets, which can provide high-level semantic information about image sequences but has been overlooked in previous studies. Our approach, called Contrastive Learning with Text-Embedded Framework for Facial behavior understanding (CLEF), consists of two stages. In the first stage, we employ a weakly-supervised contrastive learning method that constructs positive-negative pairs using coarse-grained activity information to learn representations. In the second stage, we focus on training facial expression recognition or facial action unit recognition by maximizing similarity between the image and the corresponding text label names. CLEF achieves state-of-the-art performance on three in-the-lab datasets for facial action unit recognition and three in-the-wild datasets for facial expression recognition.