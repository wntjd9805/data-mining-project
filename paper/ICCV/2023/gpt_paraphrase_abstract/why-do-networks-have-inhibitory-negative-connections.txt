The presence of inhibitory connections in brains and negative weights in deep networks can be explained by their role in increasing representation capacity. Representing functions is the main purpose of both natural intelligence in the brain and artificial intelligence in deep networks. In the absence of negative weights, neural networks with activation functions that do not decrease cannot universally approximate functions. While this may seem intuitive, there is currently no formal theory in machine learning or neuroscience that explains the significance of negative weights in terms of representation capacity. Additionally, non-negative deep networks are unable to represent certain geometric properties of the representation space. Understanding these insights can lead to the development of more efficient biological and machine learning systems by incorporating more sophisticated inductive priors on the distribution of weights.