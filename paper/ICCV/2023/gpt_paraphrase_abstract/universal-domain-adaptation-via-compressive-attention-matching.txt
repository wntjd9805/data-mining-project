Universal domain adaptation (UniDA) is a technique that aims to transfer knowledge from a source domain to a target domain without prior knowledge of the label set. However, determining whether target samples belong to common categories is a challenge. Existing methods rely on sample features to make judgments, which results in limited accuracy as they overemphasize global information and overlook crucial local objects in images. To address this issue, we propose a Universal Attention Matching (UniAM) framework that leverages the self-attention mechanism in vision transformers to capture important object information. Our framework introduces a novel approach called Compressive Attention Matching (CAM) to compressively represent attentions and explore core information. CAM also incorporates a residual-based measurement to determine the commonness of samples. By utilizing this measurement, UniAM achieves Common Feature Alignment (CFA) and Target Class Separation (TCS) on a domain-wise and category-wise basis. Notably, UniAM is the first method to directly utilize attention in vision transformers for classification tasks. Extensive experiments demonstrate that UniAM outperforms current state-of-the-art methods on various benchmark datasets.