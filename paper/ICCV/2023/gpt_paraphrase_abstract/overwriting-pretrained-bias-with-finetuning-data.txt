Transfer learning is advantageous because it allows models pretrained on large datasets to be adapted for smaller, more specific datasets. However, there is a concern that these pretrained models may contain biases that can transfer to the adapted model. This study examines bias in two forms: spurious correlations between the target task and a sensitive attribute, and underrepresentation of certain groups in the dataset. The results show that pretrained models can indeed inherit biases when fine-tuned, but these biases can be mitigated through minor adjustments to the fine-tuning dataset, without significantly impacting performance. This suggests that careful curation of the fine-tuning dataset is crucial for reducing biases in downstream tasks, and can even compensate for biases present in the pretrained model.