Recent advancements in zero-shot image recognition have shown that vision-language models have the ability to acquire comprehensive visual representations that contain semantic information. However, understanding an image involves not only recognizing its content but also determining the specific locations of objects within the image and grouping visually related parts together. In this study, we investigate the extent to which vision-language models can accurately identify object locations and group visually related parts. Our findings reveal that existing models, which are trained using contrastive losses and large web-based datasets, lack precise object localization information. To address this limitation, we propose a minimal set of modifications that enable models to learn both semantic and spatial information effectively. We evaluate the performance of our proposed model through various metrics such as zero-shot image recognition, unsupervised bottom-up and top-down semantic segmentations, and robustness analyses. Our results demonstrate that our model achieves state-of-the-art performance in unsupervised segmentation and exhibits unique robustness to spurious correlations in datasets designed to assess the causal behavior of vision models.