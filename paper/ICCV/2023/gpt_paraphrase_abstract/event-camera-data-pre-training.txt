This paper presents a pre-trained neural network designed to handle event camera data. The proposed model is based on a self-supervised learning framework, utilizing paired event camera data and natural RGB images for training. The model consists of three interconnected modules: 1. A set of event data augmentations that generate meaningful event images, which are used for self-supervised training. 2. A conditional masking strategy that selects informative event patches from event images. This encourages the model to understand the spatial layout of a scene and speeds up the training process. 3. A contrastive learning approach that enforces the similarity of embeddings between matching event images and between paired event and RGB images. To prevent the collapse of event image embeddings, an embedding projection loss is proposed. Additionally, a probability distribution alignment loss is introduced to ensure consistency between event images and their corresponding RGB images in the feature space. The superior performance of our method is demonstrated through transfer learning experiments on downstream tasks. For instance, we achieve a top-1 accuracy of 64.83% on the N-ImageNet dataset. The code for our method is publicly available at https://github.com/Yan98/Event-Camera-Data-Pre-training.