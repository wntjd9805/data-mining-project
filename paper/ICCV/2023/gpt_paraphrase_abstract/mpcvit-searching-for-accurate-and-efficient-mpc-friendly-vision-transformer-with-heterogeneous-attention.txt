This study introduces a new secure multi-party computation (MPC) framework called MPCViT, specifically designed for deep learning inference on encrypted data. While existing neural network architectures like Vision Transformers (ViTs) are not optimized for MPC and result in significant latency overhead, this paper identifies Softmax as the main cause of this latency due to its high communication complexity. However, the authors propose that Softmax can be replaced or linearized without compromising model accuracy.   To address this issue, the authors propose MPCViT, which is an MPC-friendly version of ViT that enables accurate and efficient inference in MPC. They conduct a systematic evaluation of Softmax attention and other attention variants, leading to the development of a heterogeneous attention optimization space. Additionally, they introduce an MPC-aware neural architecture search algorithm for fast Pareto optimization.  To further enhance inference efficiency, the authors propose MPCViT+, which optimizes not only the Softmax attention but also other network components such as GeLU and matrix multiplication. Through extensive experiments, the authors demonstrate that MPCViT achieves higher accuracy with reduced latency compared to baseline ViT, MPCFormer, and THE-X on the Tiny-ImageNet dataset. MPCViT+ also achieves a better Pareto front compared to MPCViT.  The code and models for evaluation can be found at https://github.com/PKU-SEC-Lab/mpcvit.