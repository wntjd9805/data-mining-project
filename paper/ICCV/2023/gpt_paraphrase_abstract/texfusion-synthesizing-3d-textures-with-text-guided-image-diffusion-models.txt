We introduce TexFusion, a novel approach for synthesizing textures for 3D geometries using large-scale text-guided image diffusion models. Unlike previous methods that use slow and unreliable optimization processes, TexFusion utilizes a 3D-consistent generation technique specifically designed for texture synthesis. This technique involves sampling from regular diffusion models on different 2D rendered views. By employing latent diffusion models, applying the diffusion model's denoiser on a set of 2D renders, and aggregating the denoising predictions on a shared latent texture map, TexFusion generates final output RGB textures by optimizing an intermediate neural color field. We extensively validate TexFusion and demonstrate its ability to efficiently generate diverse, high-quality, and globally coherent textures. Our method achieves state-of-the-art text-guided texture synthesis performance using only image diffusion models, without relying on ground truth 3D textures for training. This versatility makes TexFusion applicable to a wide range of geometry and texture types. We anticipate that TexFusion will advance AI-based texturing of 3D assets for applications in virtual reality, game design, simulation, and more.