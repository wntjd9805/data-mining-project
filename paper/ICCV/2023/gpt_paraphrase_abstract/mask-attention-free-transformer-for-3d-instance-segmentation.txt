Transformer-based methods have become dominant in 3D instance segmentation, with mask attention being commonly used. However, the mask attention pipeline often leads to slow convergence due to low-recall initial instance masks. In this study, we propose abandoning mask attention and instead using an auxiliary center regression task. By using center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To achieve this, we introduce a series of position-aware designs. Firstly, we learn a spatial distribution of 3D locations as the initial position queries, which are densely spread across the 3D space, enabling easy capture of objects with high recall. Additionally, we introduce relative position encoding for cross-attention and iterative refinement for more accurate position queries. Experimental results demonstrate that our approach achieves faster convergence compared to existing methods, sets a new state-of-the-art on the ScanNetv2 3D instance segmentation benchmark, and performs superiorly on various datasets. The code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.