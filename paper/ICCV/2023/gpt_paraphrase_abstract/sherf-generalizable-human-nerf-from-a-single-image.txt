The existing methods for reconstructing 3D humans rely on multiple 2D images or videos captured from fixed camera views, which limits their applicability to real-world scenarios where human images are often captured from random camera angles. In this study, we propose SHERF, a novel approach for recovering animatable 3D humans from a single input image. SHERF utilizes a canonical space to extract and encode 3D human representations, enabling rendering and animation from various views and poses. To ensure high-quality synthesis of novel views and poses, the encoded representations should capture both global appearance and local fine-grained textures. To address this, we introduce a bank of 3D-aware hierarchical features, including global, point-level, and pixel-aligned features, which facilitate informative encoding. Global features enhance the information extracted from the single input image, while point-level features provide clues about the 3D human structure, and pixel-aligned features preserve fine-grained details. We also propose a feature fusion transformer to effectively integrate the hierarchical feature bank. Extensive experiments on multiple datasets demonstrate that SHERF achieves state-of-the-art performance in terms of generalizability for novel view and pose synthesis. The code for SHERF is available at *Equal contribution.