Many studies have focused on explaining how deep models make predictions, but they often fail to address more complex causal questions such as why one prediction is made instead of another or why two predictions differ. To fill this gap, we propose a new approach called Counterfactual Contrastive Explanation (CCE), which uses a positive-negative saliency-based explanation scheme to visually and intuitively explain these causal questions. Our approach generates contrastive examples using a content-aware counterfactual perturbing algorithm, allowing us to derive positive and negative saliency maps that explain why one class (P) is predicted rather than another class (Q). Our counterfactual perturbation method adheres to the principles of validity, sparsity, and data distribution closeness. Additionally, by adjusting the perturbation objective, our framework can accommodate different causal questions. Extensive experiments demonstrate the effectiveness and superior performance of CCE on various interpretability metrics. A user study confirms that CCE increases user confidence compared to standard saliency map baselines.