This paper introduces a new framework called TransHuman for training conditional Neural Radiance Fields (NeRF) to generate realistic human renderings from multi-view videos. Previous methods have used a SparseConvNet (SPC)-based representation to process the painted SMPL, but this approach has limitations. The SPC-based representation operates in a volatile observation space, leading to misalignment between training and inference stages, and it lacks global relationships between human parts. TransHuman addresses these issues by learning the painted SMPL in a canonical space and capturing global relationships with transformers. The framework consists of three main components: Transformer-based Human Encoding (TransHE), Deformable Partial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI). TransHE uses transformers to process the painted SMPL in the canonical space and capture global relationships. DPaRF associates each output token with a deformable radiance field to encode the query point in the observation space. FDI integrates fine-grained information from reference images. Experimental results on ZJU-MoCap and H36M datasets demonstrate that TransHuman achieves state-of-the-art performance with high efficiency. The project page can be found at https://pansanity666.github.io/TransHuman/.