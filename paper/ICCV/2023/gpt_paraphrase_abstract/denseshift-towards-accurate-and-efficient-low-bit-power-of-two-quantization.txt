Efficiently deploying deep neural networks on low-resource edge devices is difficult due to their increasing resource requirements. To overcome this challenge, researchers have suggested multiplication-free neural networks, such as Power-of-Two quantization or Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift networks are less accurate than their full-precision counterparts, mainly due to limited weight range encoding schemes and quantization loss. In this study, we propose DenseShift networks, which greatly enhance the accuracy of Shift networks and achieve competitive performance with full-precision networks in vision and speech applications. Additionally, we introduce a method to deploy an efficient DenseShift network using non-quantized floating-point activations, resulting in a 1.6Ã— speed-up compared to existing methods. Our research demonstrates that zero-weight values in low-bit Shift networks do not contribute to model capacity and negatively impact inference computation. To address this issue, we propose a zero-free shifting mechanism that simplifies inference and increases model capacity. We also propose a sign-scale decomposition design to enhance training efficiency and a low-variance random initialization strategy to improve the model's transfer learning performance. Through extensive experiments on various computer vision and speech tasks, we show that DenseShift outperforms existing low-bit multiplication-free networks and achieves competitive performance with full-precision networks. Furthermore, our proposed approach exhibits strong transfer learning performance without any drop in accuracy. The code for our study is available on GitHub. Figure 1 showcases benchmark low-bit DenseShift networks compared to state-of-the-art low-bit Shift networks on ImageNet using the ResNet-18 model architecture.