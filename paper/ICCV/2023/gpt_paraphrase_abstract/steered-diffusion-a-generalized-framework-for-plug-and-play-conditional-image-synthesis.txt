Plug-and-play generation with conditional generative models requires large annotated training sets for high-quality synthesis. To address this limitation, researchers have explored the use of predefined or pretrained models that are not explicitly trained for the generative task but can guide the process through language or other means. However, such guidance is typically effective only for synthesizing high-level semantics and not for fine-grained details in tasks like image-to-image translation.To overcome this limitation and leverage the fine-grained generative control offered by diffusion-based generative models, we propose Steered Diffusion. This framework enables photorealistic zero-shot conditional image generation using a diffusion model trained for unconditional generation. The key concept is to steer the image generation of the diffusion model during inference by designing a loss function based on a pre-trained inverse model that characterizes the conditional task. This loss function influences the sampling trajectory of the diffusion process. Our framework allows for the incorporation of multiple conditions during inference.We conducted experiments using steered diffusion on various tasks including inpainting, colorization, text-guided semantic editing, and image super-resolution. The results demonstrate significant qualitative and quantitative improvements compared to state-of-the-art diffusion-based plug-and-play models, with minimal additional computational cost. This work was conducted during an internship at MERL.