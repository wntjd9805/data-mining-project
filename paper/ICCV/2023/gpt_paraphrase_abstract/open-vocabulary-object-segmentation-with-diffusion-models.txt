The objective of this study is to extract the relationship between visual and textual information from a pre-trained text-to-image diffusion model. Specifically, the authors aim to generate segmentation maps that simultaneously depict images and segmentation masks for the corresponding visual entities described in the text prompt. The authors propose several contributions: (i) integrating a grounding module with the Stable Diffusion model to align the visual and textual embedding space using a limited number of object categories; (ii) developing an automated pipeline for creating a dataset consisting of {image, segmentation mask, text prompt} triplets to train the grounding module; (iii) assessing the performance of open-vocabulary grounding on images generated by the text-to-image diffusion model and demonstrating its ability to accurately segment objects beyond the categories seen during training; (iv) utilizing an enhanced diffusion model to construct a synthetic semantic segmentation dataset and showcasing competitive performance on the zero-shot segmentation benchmark through training a standard segmentation model on this dataset. These findings pave the way for leveraging the powerful diffusion model for discriminative tasks.