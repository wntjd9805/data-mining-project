Privacy-preserving federated learning allows multiple users to collaboratively train a model with the help of a central server. The server's role is limited to learning the final aggregated result, which ensures that the users' private training data remains undisclosed during the individual model updates. However, this approach also opens up the possibility of malicious users compromising the model's accuracy without detection, which is known as Byzantine attacks. Existing defenses against such attacks rely on robust rank-based statistics, such as setting bounds through the median of updates, to identify malicious updates. However, incorporating privacy-preserving rank-based statistics, especially those based on the median, is challenging and inefficient in terms of secure implementation due to the need for sorting individual updates.To address this issue, we introduce the first private robustness check that utilizes high break point rank-based statistics on aggregated model updates. By leveraging randomized clustering, we significantly enhance the scalability of our defense mechanism while upholding privacy. We utilize the derived statistical bounds in zero-knowledge proofs to identify and eliminate malicious updates without revealing the private user updates. Our innovative framework, zPROBE, enables secure and resilient federated learning in the face of Byzantine attacks. We demonstrate the effectiveness of zPROBE through experiments on various computer vision benchmarks. Empirical evaluations confirm that zPROBE offers a low overhead solution to defend against state-of-the-art Byzantine attacks while maintaining privacy.