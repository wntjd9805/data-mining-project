This study focuses on object understanding in egocentric visual data, which is an important research area in egocentric vision. However, existing object datasets are either not egocentric or have limitations in terms of object categories, visual content, and annotation detail. To address this, the researchers introduce EgoObjects, a large-scale egocentric dataset for fine-grained object understanding. The Pilot version of the dataset includes over 9,000 videos collected from participants in 50+ countries using wearable devices. It also contains over 650,000 object annotations from 368 different categories, with each object annotated at the instance level. The dataset was designed to capture objects under various background complexities, surrounding objects, distance, lighting, and camera motion. Alongside data collection, the researchers developed a multi-stage federated annotation process to accommodate the dataset's growth. To facilitate research on EgoObjects, the researchers present four benchmark tasks related to egocentric object understanding, including instance-level and category-level object detection, as well as two novel continual learning object detection tasks. The dataset and API can be accessed at https://github.com/facebookresearch/EgoObjects.