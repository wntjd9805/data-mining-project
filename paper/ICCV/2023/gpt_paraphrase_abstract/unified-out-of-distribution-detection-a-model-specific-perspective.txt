The goal of out-of-distribution (OOD) detection is to identify test examples that do not belong to the training distribution and are therefore difficult to predict accurately. However, most existing works focus only on detecting OOD examples that come from semantic shift, such as unseen categories, while ignoring other possible causes like covariate shift. This paper introduces a new framework that takes a broader perspective on OOD detection. Instead of specifically targeting examples from a particular cause, we propose detecting examples that a deployed machine learning model is unable to predict correctly. In other words, whether a test example should be detected and rejected or not depends on the specific model being used. This framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and addresses the concern of applying machine learning models in uncontrolled environments. We conduct an extensive analysis using various models, sources of OOD examples, and OOD detection approaches. Through this analysis, we gain insights into improving and understanding OOD detection in uncontrolled environments.