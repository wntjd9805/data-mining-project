We introduce a technique for teaching machines to comprehend and represent the fundamental spatial understanding of various human-object interactions in a 3D environment without the need for external supervision. This task is challenging due to the existence of different patterns of interactions that can be considered human-like and natural, while the human pose and object geometry can vary even within similar interactions. This diversity makes it difficult to annotate 3D interactions and hinders the scalability of supervised learning approaches. Our approach involves training the machine using multiple 2D images captured from different viewpoints, where humans interact with similar objects. By leveraging a generative model that can produce high-quality 2D images based on text prompts, we can generate a large amount of diverse data for learning the spatial relationship between humans and objects during interactions. Despite the synthesized images not being perfect, we demonstrate that they are sufficient for learning the 3D human-object spatial relations. We present several strategies to effectively utilize the synthesized images, including leveraging a generative image model for learning 3D spatial relations, reasoning about spatial relations from inconsistent 2D cues in a self-supervised manner using 3D occupancy reasoning with pose canonicalization, employing semantic clustering to differentiate between different types of interactions with the same object types, and introducing a novel metric to evaluate the quality of 3D spatial learning in interactions.