The commonly used approach of large-scale pre-training followed by fine-tuning in object detection algorithms has limitations that affect the detector's performance, generalization ability, and convergence speed. This paper addresses these issues by proposing AlignDet, a unified pre-training framework that can be applied to different detectors to alleviate the discrepancies. AlignDet divides the pre-training process into two stages: image-domain pre-training, which optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training, which learns instance-level semantics and task-aware concepts to initialize the parts outside the backbone. By incorporating self-supervised pre-trained backbones, all modules can be pre-trained in an unsupervised paradigm. Extensive experiments show that AlignDet achieves significant improvements in various protocols, including detection algorithm, model backbone, data setting, and training schedule. For instance, AlignDet enhances FCOS by 5.3 mAP, RetinaNet by 2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP with fewer epochs.