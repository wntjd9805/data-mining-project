Deep Learning models have achieved impressive results in various vision tasks. However, they often struggle when faced with domain shifts during testing. To address this issue, researchers have developed Test-time training (TTT) methods. These methods involve training on a secondary task alongside the main task, which serves as a self-supervised proxy task during testing.   This study proposes a new unsupervised TTT technique that focuses on maximizing Mutual Information between multi-scale feature maps and a discrete latent representation. This technique can be incorporated into standard training as an auxiliary clustering task. The experimental results demonstrate competitive classification performance on popular test-time adaptation benchmarks. The code for this technique can be found at: https://github.com/dosowiechi/ClusT3.git.