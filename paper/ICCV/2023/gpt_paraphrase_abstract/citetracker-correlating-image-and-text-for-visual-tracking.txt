Conventional visual tracking methods rely on a single image patch as the reference for tracking targets. However, this approach has limitations as images alone may not provide a complete and precise understanding of the target object due to their inherent ambiguity. This makes it challenging to accurately track targets with significant variations. To address this issue, we introduce the CiteTracker, a novel method that leverages the connection between images and text to enhance target modeling and inference in visual tracking. Our approach involves converting the target image patch into a descriptive text that contains class and attribute information, providing a more comprehensive reference for the target. Additionally, we incorporate a dynamic description module that can adapt to target variations, resulting in a more effective target representation. To establish the correlation between the target description and the search image, we employ an attention-based correlation module that generates correlated features for target state reference. We extensively evaluate our proposed algorithm on five diverse datasets, and the results demonstrate its superiority over state-of-the-art methods. We will make the source code and trained models available at https://github.com/NorahGreen/CiteTracker.