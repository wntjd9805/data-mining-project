Vision-and-language navigation (VLN) allows an agent to follow natural language instructions to navigate in 3D environments. Current approaches for representing the previously visited environment in VLN typically use recurrent states, topological maps, or top-down semantic maps. In contrast, we introduce the Grid Memory Map (GridMM), a top-down egocentric and dynamically growing structure that organizes the visited environment. This approach projects historical observations onto a unified grid map in a top-down view, providing a better representation of spatial relations. Additionally, we propose an instruction relevance aggregation method to capture detailed visual information in each grid region. Our method outperforms existing approaches in both discrete and continuous environments, as demonstrated through extensive experiments on various datasets. The source code for our method is publicly available at https://github.com/MrZihan/GridMM.