This study addresses the challenges faced by vision transformers (ViTs) when distilling knowledge from convolutional neural networks (CNNs). While the use of CNNs enhances ViT performance by leveraging their image-friendly local-inductive bias, it also presents two problems. Firstly, the distinct network designs of CNNs and ViTs result in different semantic levels of intermediate features, making spatial-wise knowledge transfer methods inefficient. Secondly, distilling knowledge from CNNs hinders network convergence in later training stages as ViT's ability to integrate global information is suppressed by CNN's local-inductive-bias supervision.  To overcome these challenges, the authors propose Cumulative Spatial Knowledge Distillation (CSKD). This approach distills spatial-wise knowledge to all patch tokens of ViT by directly utilizing the corresponding spatial responses of CNNs, eliminating the need for intermediate features. Additionally, CSKD introduces a Cumulative Knowledge Fusion (CKF) module that incorporates the global response of CNNs and progressively emphasizes its importance during training. By applying CKF, CNN's local-inductive bias is effectively utilized in the early training period, while ViT's global capability is fully exploited in the later stages.  The superiority of CSKD is demonstrated through extensive experiments and analysis on ImageNet-1k and downstream datasets. The provided code for CSKD implementation can be found at https://github.com/Zzzzz1/CSKD.