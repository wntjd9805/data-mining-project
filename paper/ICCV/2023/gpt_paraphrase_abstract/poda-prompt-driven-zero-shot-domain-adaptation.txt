This study introduces a new approach called "Prompt-driven Zero-shot Domain Adaptation" in the field of computer vision. The goal is to adapt a model trained on a source domain using only a general description of the target domain in natural language, referred to as a prompt. The researchers utilize a pretrained vision-language model called CLIP to optimize the transformation of source features towards the target text embedding while preserving their content and semantics. This is achieved through a technique called Prompt-driven Instance Normalization (PIN). The study also demonstrates that these prompt-driven augmentations can be applied to perform zero-shot domain adaptation for semantic segmentation. Experimental results indicate that the proposed method outperforms CLIP-based style transfer baselines and even surpasses one-shot unsupervised domain adaptation on various datasets for different tasks such as semantic segmentation, object detection, and image classification. The code for this method is made available on GitHub.