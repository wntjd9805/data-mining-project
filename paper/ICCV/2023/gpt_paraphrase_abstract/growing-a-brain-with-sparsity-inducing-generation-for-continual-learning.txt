Current deep neural networks face the issue of catastrophic forgetting in continual learning, where they tend to lose information about previously learned tasks when optimizing for new tasks. Recent strategies have attempted to retain old knowledge by isolating important parameters for previous tasks. However, relying on fixed old knowledge may hinder the capture of novel representations. To address this limitation, we propose a framework that evolves previously allocated parameters by incorporating the knowledge of new tasks. Our approach consists of two different networks: a base network that learns knowledge of sequential tasks, and a sparsity-inducing hypernetwork that generates parameters for each time step to evolve old knowledge. These generated parameters transform the old parameters of the base network to reflect the new knowledge. We design the hypernetwork to generate sparse parameters based on task-specific information and the structural information of the base network. We evaluate our approach on class-incremental and task-incremental learning scenarios, specifically for image classification and video action recognition tasks. Experimental results consistently demonstrate that our proposed method outperforms a wide range of continual learning approaches in these scenarios by evolving old knowledge.