Federated learning (FL) is a promising technique for safeguarding data privacy in authentication systems. However, challenges such as limited communication rounds, scarce representation, and scalability hinder its potential. To address these issues, we propose ProtoFL, an unsupervised federated learning method that utilizes Prototypical Representation Distillation to enhance the representation capability of a global model and reduce communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. This is the first study to investigate the use of FL for enhancing one-class classification performance. We conducted extensive experiments on popular benchmarks, including MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to showcase the superior performance of our approach compared to existing methods in the literature.