The traditional method for retrieving videos based on text queries involves aligning visual and textual information at either a coarse-grained or fine-grained level. However, accurately retrieving the correct video based on a text query is often difficult because it requires reasoning about both high-level (scene) and low-level (object) visual clues and how they relate to the text query. In order to address this challenge, we propose a model called UCOFIA (Unified Coarse-to-fine Alignment) that captures cross-modal similarity information at different levels of granularity. To mitigate the impact of irrelevant visual clues, we also incorporate an Interactive Similarity Aggregation module (ISA) that considers the importance of different visual features when aggregating the cross-modal similarity. Furthermore, we employ the Sinkhorn-Knopp algorithm to normalize the similarities at each level before summing them, thereby addressing issues of over- and under-representation at different levels. By considering the cross-modal similarity at different levels of granularity, UCOFIA allows for the effective unification of multi-grained alignments. In empirical evaluations, UCOFIA outperforms previous state-of-the-art CLIP-based methods on multiple video-text retrieval benchmarks, achieving improvements of 2.4%, 1.4%, and 1.3% in text-to-video retrieval R@1 on MSR-VTT, Activity-Net, and DiDeMo, respectively. The code for UCOFIA is publicly available at https://github.com/Ziyang412/UCoFiA.