Zero-shot point cloud segmentation is a technique that aims to enable deep learning models to recognize objects in point clouds that were not seen during training. Current approaches focus on transferring knowledge from labeled seen classes to unlabeled unseen classes. This is typically done by aligning visual features with semantic features derived from word embeddings, using annotations from the seen classes. However, point clouds lack the necessary information to fully match with semantic features. Previous research has not adequately explored the rich appearance information present in images, which can complement the textureless point clouds. In light of this, we propose a new multi-modal zero-shot learning method that effectively utilizes the complementary information from both point clouds and images to improve visual-semantic alignment. We conducted extensive experiments on two popular benchmarks, SemanticKITTI and nuScenes, and our method outperformed state-of-the-art approaches by achieving a 52% and 49% improvement on average for unseen class mIoU, respectively.