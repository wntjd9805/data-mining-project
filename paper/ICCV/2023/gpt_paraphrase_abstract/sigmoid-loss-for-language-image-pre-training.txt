We present a straightforward sigmoid loss function for image-text pre-training that differs from the conventional contrastive learning method. Our proposed loss function operates exclusively on image-text pairs and does not require a global perspective on pairwise similarities for normalization. This allows for the use of larger batch sizes while also achieving better performance with smaller batch sizes. By utilizing just four TPUv4 chips, we successfully train a Base CLIP model with a batch size of 4,000 and a Large LiT model with a batch size of 20,000. The latter achieves an impressive 84.5% ImageNet zero-shot accuracy within a two-day training period. This separation of batch size from the loss function enables us to investigate the influence of examples versus pairs and negative to positive ratio. Additionally, we push the boundaries by testing a batch size of one million, but discover that the benefits of increasing batch size diminish rapidly. Ultimately, a more reasonable batch size of 32,000 proves to be sufficient. Our research aims to inspire further advancements in enhancing the quality and efficiency of language-image pre-training.