This paper introduces a robust framework called MetaBEV, which aims to address the challenges of sensor corruptions and failures in autonomous driving vehicles. The framework utilizes inputs from multiple sensors such as LiDAR and cameras, and is designed to handle extreme real-world environments where sensor data may be corrupted or missing.MetaBEV consists of modal-specific encoders that process signals from the different sensors. It then initializes a set of dense Bird's Eye View (BEV) queries, referred to as meta-BEV. These queries are iteratively processed by a BEV-Evolving decoder, which selectively combines deep features from LiDAR, cameras, or both modalities. The updated BEV representations are utilized for various 3D prediction tasks.To address the performance drop in multi-task joint learning, a new structure called M2oE is introduced in MetaBEV. This structure helps alleviate the impact of distinct tasks on each other.The evaluation of MetaBEV is conducted on the nuScenes dataset, focusing on 3D object detection and BEV map segmentation tasks. The experiments demonstrate that MetaBEV outperforms previous methods by a significant margin, both on full and corrupted sensor modalities. For example, when the LiDAR signal is missing, MetaBEV improves detection NDS by 35.5% and segmentation mIoU by 17.7% compared to the vanilla BEVFusion model. Similarly, when the camera signal is absent, MetaBEV achieves 69.2% NDS and 53.7% mIoU, surpassing previous works that rely on full-modalities. Furthermore, MetaBEV performs well in both canonical perception and multi-task learning settings, setting a new state-of-the-art for nuScenes BEV map segmentation with a mIoU of 70.4%.