Existing text-to-image diffusion models have been successful in generating high-quality images from text prompts. However, these models have limitations when it comes to handling multiple personalized subjects and the risk of overfitting. Additionally, their large number of parameters makes them inefficient for model storage. To overcome these challenges, we propose a new approach called SVDiff. This approach involves fine-tuning the singular values of weight matrices, resulting in a more compact and efficient parameter space that reduces the risk of overfitting and language-drifting. We also introduce a data-augmentation technique called Cut-Mix-Unmix to improve the quality of multi-subject image generation. Furthermore, we present a simple text-based image editing framework. Compared to existing methods, our SVDiff method has a significantly smaller model size, with approximately 2,200 times fewer parameters than vanilla DreamBooth. This makes our method more practical for real-world applications.