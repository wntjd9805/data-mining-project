Source-free domain adaptation (SFDA) is a technique used to adapt a model trained on a labeled source domain to an unlabeled target domain. Currently, large-data pre-trained networks are employed to initialize the source model during training and then discarded. However, this approach can lead to overfitting to the source data distribution and a loss of relevant knowledge for the target domain. In order to address this issue, we propose integrating the pre-trained network into the target adaptation process. This pre-trained network possesses diverse features that are crucial for generalization and offers an alternative perspective on features and classification decisions compared to the source model. Our proposal involves distilling valuable information from the target domain through a co-learning strategy, which enhances the quality of target pseudolabels used for fine-tuning the source model. We evaluated our approach on four benchmark datasets and observed that it improves adaptation performance while being compatible with existing SFDA methods. Furthermore, by leveraging modern pre-trained networks with stronger representation learning capabilities in the co-learning strategy, we achieved even better performance.