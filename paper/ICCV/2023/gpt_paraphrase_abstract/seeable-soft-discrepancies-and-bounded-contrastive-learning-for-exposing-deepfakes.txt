Modern deepfake detectors have made significant progress in detecting manipulated images when training and test images come from the same dataset. However, these detectors often perform poorly when applied to deepfakes created using unknown techniques. In this study, we propose a new deepfake detector called SeeABLE, which addresses this issue by treating the detection problem as an out-of-distribution task. SeeABLE generates local image perturbations, referred to as soft-discrepancies, and uses a novel regression-based bounded contrastive loss to push these perturbed faces towards predefined prototypes. To enhance SeeABLE's ability to detect unknown deepfake types, we generate a diverse set of soft discrepancies and train the detector to identify both the modified part of the face and the type of alteration. Through rigorous experiments on widely-used deepfake datasets, we demonstrate that SeeABLE outperforms other state-of-the-art detectors and exhibits strong generalization capabilities. The source code for SeeABLE is available on the anonymous-author-sub GitHub repository.