This paper presents a solution to the difficult problem of wide-angle novel view synthesis from a single image, also known as wide-angle 3D photography. Existing methods fail to handle large-region occlusion and blending of foreground layers into background inpainting. To address these issues, we propose Diffuse3D, which incorporates a pre-trained diffusion model for global synthesis and modifies the model to incorporate depth-aware inference. We achieve this by altering the convolution mechanism during the denoising process, injecting depth information using bilateral kernels to consider correlations among pixels. This ensures that foreground regions are not included in background inpainting and only pixels with similar depth are utilized. Additionally, we introduce a global-local balancing approach to enhance contextual understanding. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods, particularly in wide-angle scenarios. Furthermore, our method is a plug-and-play module that can be seamlessly integrated with any diffusion model and does not require training. The code for our method is available at https://github.com/yutaojiang1/Diffuse3D.