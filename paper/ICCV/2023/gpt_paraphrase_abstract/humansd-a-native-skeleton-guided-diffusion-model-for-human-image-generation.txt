Controllable human image generation (HIG) has various practical applications. Current methods like ControlNet and T2I-Adapter add a learnable branch to a pre-trained stable diffusion (SD) model to enforce different conditions, including skeleton guidance. However, this plug-and-play approach faces challenges due to conflicts between the original images from the SD branch and the given condition. In this study, we propose a native skeleton-guided diffusion model called HumanSD for controllable HIG. Instead of using dual-branch diffusion for image editing, we fine-tune the original SD model using a novel heatmap-guided denoising loss. This approach strengthens the given skeleton condition during model training and mitigates forgetting effects. HumanSD is fine-tuned on three large-scale human-centric datasets, including two newly established ones. Experimental results demonstrate that HumanSD outperforms ControlNet in terms of pose control and image quality, particularly with sophisticated skeleton guidance. The code and data for HumanSD are available at: https://idea-research.github.io/HumanSD/.