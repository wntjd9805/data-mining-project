Knowledge-based visual question answering (VQA) involves answering questions that require knowledge beyond the image itself. Large language models (LMs) like GPT-3 are valuable for this task due to their strong knowledge retrieval and reasoning abilities. However, previous approaches have relied on captioning models to convert images into text, which often fail to capture important visual details necessary for accurate answers. To overcome this challenge, we propose PROMPTCAP, a captioning model that serves as a better bridge between images and LMs. Unlike generic captions, PROMPTCAP uses a natural-language prompt to guide the generation of captions, specifying which visual entities should be described. To train PROMPTCAP, we use examples synthesized with GPT-3 and existing datasets, avoiding the need for additional annotation. We evaluate PROMPTCAP's performance in a pipeline where GPT-3 is prompted with image captions for VQA. PROMPTCAP significantly outperforms generic captions and achieves state-of-the-art accuracy on knowledge-based VQA tasks. Additionally, zero-shot results on WebQA demonstrate PROMPTCAP's ability to generalize to unseen domains. The answer format of PROMPTCAP focuses on abstraction, providing concise and relevant answers to questions.