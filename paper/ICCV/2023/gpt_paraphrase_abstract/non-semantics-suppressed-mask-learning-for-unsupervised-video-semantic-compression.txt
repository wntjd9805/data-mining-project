Most video compression methods prioritize improving the visual quality of decoded videos rather than ensuring semantic completeness, which can negatively impact downstream video analysis tasks such as action recognition. This paper introduces a new approach to unsupervised video semantic compression, where video semantics are compressed in a task-agnostic manner. The proposed method, called the Semantic-Mining-then-Compensation (SMC) framework, enhances the plain video codec by incorporating powerful semantic coding capabilities. The framework is optimized using unlabeled video data by masking a portion of the compressed video and reconstructing the masked regions from the original video. This approach draws inspiration from recent masked image modeling (MIM) methods. While the MIM scheme learns generalizable semantic features, its generative learning paradigm may also cause the coding framework to memorize non-semantic information at the expense of additional bit costs. To address this issue, the proposed method explicitly reduces the entropy of non-semantic information in the decoded video features by formulating it as a parametrized Gaussian Mixture Model conditioned on the extracted video semantics. Extensive experiments demonstrate that the proposed approach outperforms previous traditional, learnable, and perceptual quality-oriented video codecs across three video analysis tasks and seven datasets.