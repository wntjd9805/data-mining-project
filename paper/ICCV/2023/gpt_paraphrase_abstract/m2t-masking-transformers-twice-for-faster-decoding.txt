We present a novel approach to neural image compression using bidirectional transformers trained for masked token prediction. Previous models utilizing these transformers focused on image generation by iteratively sampling groups of masked tokens based on uncertainty. In contrast, we demonstrate that predefined deterministic schedules perform equally well or even better for image compression. This finding allows us to incorporate masked attention during training and activation caching during inference, resulting in significantly faster models without sacrificing bitrate.