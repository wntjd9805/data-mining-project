This study investigates the presence of "gender artifacts" in large-scale visual datasets, which are visual cues correlated with gender that can be learned by image classifiers. The researchers examine various datasets and find that gender artifacts are pervasive, appearing in different aspects such as color channels, pose, and location of people. Previous methods that attempt to remove gender biases from these datasets actually remove more information from the scene than just gender. The study concludes that it is largely infeasible to completely eliminate these artifacts, as some of them may be essential for downstream tasks like object recognition. Instead, researchers and practitioners should be aware of the gendered distribution within datasets and develop fairness-aware methods that are robust to these biases.