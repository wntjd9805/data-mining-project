Task-Free Continual Learning (TFCL) is a learning approach that aims to acquire new concepts from a continuous stream of data without any task information. The Dynamic Expansion Model (DEM) has shown promise in TFCL by dynamically increasing the model's capacity to handle changes in the data distribution. However, existing methods only consider recognizing input shifts as a signal for expansion, disregarding the correlation between new data and previously learned knowledge. This leads to the addition and training of unnecessary parameters. In this study, we propose a novel and effective framework for TFCL that addresses these limitations. Our framework dynamically expands the architecture of a DEM model using a self-assessment mechanism that evaluates the diversity of knowledge among existing experts as expansion signals. This ensures that additional underlying data distributions can be learned with a compact model structure. To manage the memory buffer, we introduce a novelty-aware sample selection approach that forces the newly added expert to learn novel information from the data stream, further enhancing the diversity among experts. Additionally, we propose the use of knowledge transfer in TFCL, allowing the reuse of previously learned representation information for learning new incoming data. This aspect has not been explored in prior research. The expansion and training of the DEM model are regularized through a gradient updating mechanism, gradually exploring positive forward transfer and improving performance. Empirical results on TFCL benchmarks demonstrate that our proposed framework outperforms the state-of-the-art methods while utilizing a reasonable number of parameters. The code for our framework is available at the following link: https://github.com/dtuzi123/SEDEM/.