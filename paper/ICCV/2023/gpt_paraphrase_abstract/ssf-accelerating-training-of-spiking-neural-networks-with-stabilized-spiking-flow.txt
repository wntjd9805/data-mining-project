Surrogate gradient (SG) is a highly effective method for training spiking neural networks (SNNs) to achieve classification performance similar to artificial neural networks. However, SG suffers from the drawback of time-consuming training. This paper presents a formal analysis of the backward process of classic SG, revealing that the accumulation of membrane potential over time leads to exponential growth in training time. To address this issue, a new approach called Stabilized Spiking Flow (SSF) is proposed to accelerate the training of SG-based SNNs. SSF works by averaging the input and output activations of each spiking neuron over time, resulting in stabilized input and output. Instead of backpropagating all errors associated with the current neuron and entangled in the time domain, an auxiliary gradient is directly propagated from the stabilized output to input through a devised relationship mapping. The SSF method is also compatible with different neuron models. Extensive experiments conducted on both static and neuromorphic datasets demonstrate that SNNs trained with the SSF approach achieve comparable performance to the original counterparts, while significantly reducing the training time. Notably, when the number of time steps is 80, the SSF approach speeds up the training process of state-of-the-art SNN models by up to 10 times.