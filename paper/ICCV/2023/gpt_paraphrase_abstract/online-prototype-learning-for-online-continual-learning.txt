This paper addresses the problem of online continual learning (CL), which involves learning from a continuous stream of data while adapting to new information and preventing catastrophic forgetting. Previous methods have focused on storing samples or distilling knowledge to combat forgetting, but this paper takes a different approach by investigating the issue of shortcut learning. Shortcut learning refers to the limitations of online CL models, where learned features may be biased, not generalizable to new tasks, and hinder knowledge distillation. To overcome this, the authors propose the online prototype learning (OnPro) framework. OnPro incorporates online prototype equilibrium to learn representative and discriminative features, achieving a balance between separating seen classes and learning new ones. Additionally, an adaptive prototypical feedback mechanism is introduced to identify easily misclassified classes and enhance their boundaries. Experimental results on benchmark datasets demonstrate the superior performance of OnPro compared to existing methods. The source code for OnPro is available at the provided GitHub link.