Neural Architecture Search (NAS) is a method used to automatically discover the best neural network architectures efficiently. Zero-Shot NAS is a promising technique that predicts the accuracy of potential architectures without training. However, current proxies used in Zero-Shot NAS lack consistency across different search spaces and do not prioritize generalization. Recent studies have shown that the gradient signal-to-noise ratio (GSNR) is correlated with network generalization performance. In this paper, we not only provide the probability that higher GSNR at network initialization leads to better generalization, but also theoretically prove that GSNR improves convergence. We introduce the 両-based gradient signal-to-noise ratio (両-GSNR) as a proxy for Zero-Shot NAS to predict network accuracy at initialization. Our extensive experiments in various search spaces demonstrate that 両-GSNR outperforms previous proxies in terms of ranking consistency. Additionally, 両-GSNR-based Zero-Shot NAS achieves exceptional performance when directly searching for optimal architectures in different search spaces and datasets. The source code can be found at https://github.com/Sunzh1996/Xi-GSNR.