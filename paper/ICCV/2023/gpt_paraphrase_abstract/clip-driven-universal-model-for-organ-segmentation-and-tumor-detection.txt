Automated organ segmentation and tumor detection have greatly benefited from the availability of public datasets. However, existing models are limited by the small size and partially labeled nature of these datasets, as well as a lack of investigation into diverse types of tumors. As a result, these models can only segment specific organs or tumors and do not consider the anatomical structure semantics or have the ability to be extended to new domains.  To address these limitations, we propose the CLIP-Driven Universal Model, which incorporates text embedding learned from Contrastive Language-Image Pre-training (CLIP) into segmentation models. This CLIP-based label encoding captures anatomical relationships, allowing the model to learn a structured feature embedding and successfully segment 25 organs and 6 types of tumors.   We developed the proposed model using an assembly of 14 datasets, with a total of 3,410 CT scans used for training. We then evaluated the model on 6,162 external CT scans from 3 additional datasets. Our model achieved first place on the Medical Segmentation Decathlon (MSD) public leaderboard and achieved state-of-the-art results on Beyond The Cranial Vault (BTCV).  Furthermore, the Universal Model is computationally more efficient, being 6 times faster compared to dataset-specific models. It also demonstrates better generalization to CT scans from different sites and exhibits stronger transfer learning performance on novel tasks.