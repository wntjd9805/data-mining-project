We present a novel framework for self-supervised learning in point cloud videos, focusing on both object-centric and scene-centric data. Existing methods typically learn representations at the clip or frame level, resulting in limited capture of detailed semantics. Instead, we propose a unified self-supervised approach that performs contrastive learning at the point level. Additionally, we introduce a new pretext task that aligns superpoints semantically, enhancing the ability of representations to capture semantic cues at various scales. Moreover, as dynamic point clouds often exhibit high redundancy in the temporal dimension, conducting contrastive learning directly at the point level can lead to an abundance of undesired negatives and inadequate modeling of positive representations. To address this issue, we propose a selection strategy that preserves appropriate negatives and utilizes high-similarity samples from other instances as positive supplements. Our extensive experiments demonstrate that our method surpasses supervised counterparts across various downstream tasks and exhibits excellent transferability of the learned representations.