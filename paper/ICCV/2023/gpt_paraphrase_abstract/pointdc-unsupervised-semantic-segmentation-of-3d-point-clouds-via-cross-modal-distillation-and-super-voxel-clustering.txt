This paper presents the first attempt at fully unsupervised semantic segmentation of point clouds, aiming to identify semantically meaningful objects without any annotations. Previous unsupervised methods for 2D images fail in the context of point clouds due to clustering and irregularity ambiguities. To address these challenges, the authors propose a novel framework called PointDC, which consists of two steps: Cross-Modal Distillation (CMD) and Super-Voxel Clustering (SVC). In CMD, multi-view visual features are projected back to 3D space and aggregated into a unified point feature to improve point representation training. In SVC, point features are aggregated into super-voxels and subjected to an iterative clustering process to classify semantic classes. PointDC1 outperforms prior unsupervised methods, achieving significant improvements in both the ScanNet-v2 and S3DIS semantic segmentation benchmarks (+18.4 mIoU and +11.5 mIoU, respectively).