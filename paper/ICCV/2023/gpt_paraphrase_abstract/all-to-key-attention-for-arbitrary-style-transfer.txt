Attention-based arbitrary style transfer methods have shown promise in generating detailed and vibrant local style features. However, these methods typically employ an all-to-all attention mechanism, which matches each position of the content features with all positions of the style features. This approach often leads to distorted style patterns and has computational limitations. To address these issues, we propose a novel all-to-key attention mechanism in this paper. Instead of matching content features with all style features, our approach matches content features with stable key positions in the style features. This new mechanism aligns better with the characteristics of style transfer. Our method incorporates two types of attention: distributed attention and progressive attention. Distributed attention focuses on key style representations that capture the style distribution of local regions, while progressive attention pays attention to coarse-grained regions before refining the attention to key positions in fine-grained regions. Our module, called StyA2K, demonstrates exceptional performance in maintaining semantic structure and producing consistent style patterns. We conducted qualitative and quantitative comparisons with state-of-the-art methods, which confirmed the superior performance of our approach. The codes and models for our method are available at https://github.com/LearningHx/StyA2K.