A key challenge for autonomous agents, like robots, is to maintain a consistent understanding of the world over time and space, even when faced with occlusions and unfamiliar views. Training these agents to develop a versatile neural representation without supervision remains an open question. We propose framing the training objective as a patch retrieval problem, wherein given an image patch from one view, we aim to retrieve all patches from other views that correspond to the same real-world location. However, this objective does not promote the reusability of features across different scenes. To address this, we carefully construct the retrieval set by excluding patches that correspond to distant locations, striking a balance between retrieval and reusability. Additionally, we can control the scale of learned features by adjusting the spatial tolerance for considering a retrieval as positive. We optimize for smooth Average Precision (AP) as a unified objective, which also helps in selecting landmarks or keypoints with high AP. Our approach enables the creation of sparse, multi-scale, semantic spatial maps comprising easily identifiable landmarks. These maps have various applications, including landmark retrieval, localization, semantic segmentation, and instance segmentation.