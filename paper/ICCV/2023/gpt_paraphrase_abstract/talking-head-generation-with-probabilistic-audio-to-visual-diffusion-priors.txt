We present a new approach to generating realistic talking head videos from audio inputs. Unlike previous methods that require additional sources of information to control the synthesis process, we sample various facial movements (such as pose, expression, blink, and gaze) to match the input audio while maintaining realistic lip synchronization and overall naturalness. Our approach utilizes a newly proposed audio-to-visual diffusion prior that is trained on the mapping between audio and non-lip visual representations. One advantage of our framework is its ability to generate diverse facial motion sequences for the same audio clip, making it user-friendly for real-world applications. Through extensive evaluations, we demonstrate that our diffusion prior performs better than auto-regressive priors on various metrics. Additionally, our system achieves competitive results in terms of audio-lip synchronization while effectively sampling lip-irrelevant facial motions that are visually pleasing and semantically aligned with the audio input.