Recent advancements in semantic segmentation models trained using image-level text supervision have shown promise in challenging real-world scenarios. However, these models still struggle with achieving precise pixel-level semantic alignment and accurate object masks. To address this issue, we introduce MixReorg, a novel pre-training approach for semantic segmentation. MixReorg enhances a model's ability to reorganize mixed patches from different images, considering both local visual relevance and global semantic coherence. Our method involves generating fine-grained patch-text pairs data by mixing image patches while preserving the correspondence between patches and text. The model is then trained to minimize the segmentation loss of the mixed images and the contrastive losses of the original and restored features. By employing MixReorg as a mask learner, traditional text-supervised semantic segmentation models can acquire highly generalizable pixel-semantic alignment capabilities, which are crucial for open-world segmentation. After training with large-scale image-text data, MixReorg models can directly segment visual objects of any category without the need for further fine-tuning. Our proposed framework demonstrates impressive performance on popular zero-shot semantic segmentation benchmarks. It outperforms GroupViT by significant margins of 5.0%, 6.2%, 2.5%, and 3.4% mIoU on PASCAL VOC2012, PASCAL Context, MS COCO, and ADE20K, respectively.