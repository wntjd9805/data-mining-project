Most self-supervised approaches to representation learning utilize a cross-view consistency objective, which involves maximizing the similarity between augmented views of the same image. However, a recent method called NNCLR takes a different approach by using positive pairs from different images obtained through nearest neighbor bootstrapping in a contrastive setting. Surprisingly, when incorporating nearest neighbor bootstrapping into a self-distillation scheme, there is a decline in performance or even a collapse, unlike the contrastive learning setting that relies on negative samples. We investigate this unexpected behavior and propose a solution. Our solution involves adaptively bootstrapping neighbors based on the estimated quality of the latent space. Experimental results demonstrate consistent improvements compared to the naive bootstrapping approach and the original baselines. Our approach also leads to improved performance across various self-distillation method/backbone combinations and standard downstream tasks. The code for our approach is publicly available at https://github.com/tileb1/AdaSim.