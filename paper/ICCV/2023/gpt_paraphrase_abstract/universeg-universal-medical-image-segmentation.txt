Deep learning models have become the dominant approach for segmenting medical images. However, these models often struggle to generalize to new segmentation tasks involving different anatomies, image modalities, or labels. To address this issue, researchers typically need to train or fine-tune models for each new task, which is time-consuming and challenging for clinical researchers with limited resources and expertise. In this study, we propose UniverSeg, a method that can solve unseen medical segmentation tasks without the need for additional training. UniverSeg utilizes a novel mechanism called CrossBlock to generate accurate segmentation maps based on a query image and a set of example image-label pairs that define the new segmentation task. To enable generalization to new tasks, we have curated and standardized a collection of 53 open-access medical segmentation datasets, called MegaMedical, consisting of over 22,000 scans. We trained UniverSeg on this diverse dataset, encompassing various anatomies and imaging modalities. Our results demonstrate that UniverSeg outperforms several other methods on unseen tasks. We also provide a thorough analysis and insights into the proposed system. The source code and model weights for UniverSeg are freely available at https://universeg.csail.mit.edu.