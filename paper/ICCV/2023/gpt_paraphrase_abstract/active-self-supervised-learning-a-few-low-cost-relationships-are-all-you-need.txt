Self-Supervised Learning (SSL) is a popular method for learning representations from unlabeled data. However, it requires knowing which samples are similar, known as positive views. This limitation is typically addressed by applying data augmentations to the same input. In this research, we introduce Positive Active Learning (PAL) as a more generalized approach to address this issue. PAL incorporates an oracle that queries semantic relationships between samples. This approach has three main benefits. First, it offers a theoretically grounded learning framework beyond SSL, based on similarity graphs, that can be extended to supervised and semi-supervised learning. Second, it allows the integration of prior knowledge, such as observed labels, into SSL losses without altering the training process. Third, it provides an active learning framework that enables the annotation of datasets at low cost, bridging the gap between theory and practice. PAL achieves this by using non-expert-friendly queries about semantic relationships between inputs.