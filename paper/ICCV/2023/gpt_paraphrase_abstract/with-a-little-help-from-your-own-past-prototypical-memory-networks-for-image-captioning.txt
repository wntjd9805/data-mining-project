Currently, image captioning tasks rely on Transformer-based architectures to extract semantic information from images and generate coherent descriptions. However, the attention operator in these architectures only considers a weighted summation of projections from the current input sample, thereby disregarding relevant semantic information from other samples. In this study, we propose a network that incorporates a prototypical memory model to perform attention over activations obtained from processing other training samples. Our memory model represents the distribution of past keys and values using discriminating and compact prototype vectors. We evaluate the performance of our model on the COCO dataset, comparing it to carefully designed baselines and state-of-the-art approaches. We also analyze the role of each proposed component. Our results show that our approach improves the performance of an encoder-decoder Transformer by 3.7 CIDEr points, both in cross-entropy training and fine-tuning with self-critical sequence training. The source code and trained models are available at: https://github.com/aimagelab/PMA-Net.