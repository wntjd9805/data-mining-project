Limited supervised data in 3D human action recognition poses a challenge in leveraging the modeling potential of powerful networks like transformers. To address this issue, researchers have explored self-supervised pre-training strategies. This study demonstrates that explicit contextual motion modeling plays a crucial role in learning effective feature representation for 3D action recognition, as opposed to the common practice of performing masked self-component reconstruction in human joints. To this end, the Masked Motion Prediction (MAMP) framework is proposed. MAMP takes a masked spatio-temporal skeleton sequence as input and predicts the corresponding temporal motion of the masked human joints. Additionally, MAMP incorporates motion information as an empirical semantic richness prior to guide the masking process, enhancing attention to semantically rich temporal regions. Extensive experiments conducted on NTU-60, NTU-120, and PKU-MMD datasets validate that MAMP pre-training significantly improves the performance of the vanilla transformer, achieving state-of-the-art results without additional enhancements. The MAMP source code is available at https://github.com/maoyunyao/MAMP.