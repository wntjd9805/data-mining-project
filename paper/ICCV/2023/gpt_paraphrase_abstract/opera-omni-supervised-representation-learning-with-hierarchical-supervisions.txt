The pretrain-finetune approach in modern computer vision has been successful in self-supervised learning, which outperforms supervised learning in terms of transferability. However, considering the availability of abundant labeled data, a question arises: how can we train a better model by combining both self and full supervision signals? This paper proposes a solution called Omni-suPErvised Representation leArning with hierarchical supervisions (OPERA) that addresses this issue. The authors offer a comprehensive perspective on supervisions from labeled and unlabeled data, and present a unified framework for fully supervised and self-supervised learning. They extract a series of hierarchical proxy representations for each image and apply self and full supervisions to these proxy representations. Extensive experiments conducted on convolutional neural networks and vision transformers demonstrate the outstanding performance of OPERA in various tasks such as image classification, segmentation, and object detection.