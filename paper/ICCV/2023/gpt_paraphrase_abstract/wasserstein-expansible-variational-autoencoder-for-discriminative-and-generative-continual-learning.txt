Task-Free Continual Learning (TFCL) is a challenging learning approach where a model is trained on evolving data distributions without any knowledge of the tasks involved. This paper introduces the Wasserstein Expansible Variational Autoencoder (WEVAE) as a solution to the problem of redundancy in probabilistic representations within Varia-tional Autoencoder (VAE) mixtures. Existing methods fail to consider the similarity among mixture components, leading to components learning similar tasks. The proposed WEVAE evaluates the statistical similarity between new data and existing mixture components, using this information to determine when to expand the model. This approach prevents unnecessary model expansion while ensuring diverse knowledge among the trained components. Additionally, the paper presents an energy-based sample selection method that assigns high energies to novel samples and low energies to samples similar to the model's existing knowledge. Extensive empirical studies on supervised and unsupervised benchmark tasks demonstrate the superior performance of WEVAE compared to other methods. The code for WEVAE is available at https://github.com/dtuzi123/WEVAE/.