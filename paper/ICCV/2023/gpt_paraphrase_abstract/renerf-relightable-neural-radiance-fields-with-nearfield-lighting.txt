Recent advancements in radiance fields and volumetric inverse rendering have enabled the creation of data-driven models that can generate highly realistic images from novel viewpoints. However, these models are typically limited in their ability to change the scene lighting, as it is "baked" into the model. Other methods that capture limited lighting variation or make restrictive assumptions about the scene also exist. These limitations make it difficult to apply these models to arbitrary materials and complex 3D environments with unique lighting conditions.  In this paper, we focus on capturing high-fidelity assets for neural relighting in controlled studio settings, without the need for a dense light stage. Instead, we utilize a small number of area lights commonly used in photogrammetry. Our proposed method, called ReNeRF, is a relightable radiance field model that leverages image-based relighting. This approach allows us to capture global light transport for arbitrary objects without relying on complex and error-prone simulations. As a result, our method is simple and provides full control over both the viewpoint and lighting, without making oversimplified assumptions about how light interacts with the scene.  Furthermore, ReNeRF does not rely on the typical assumption of distant lighting. During the training process, we explicitly consider the distance between 3D points in the volume and the point samples on the light sources. This allows us to achieve better generalization to novel and continuous lighting directions, including nearfield lighting effects.  Overall, our proposed method overcomes the limitations of existing techniques by providing a relightable radiance field model that can be applied to a wide range of materials and complex 3D environments. It offers full control over viewpoint and lighting, without relying on oversimplified assumptions or complex simulations.