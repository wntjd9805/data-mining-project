This paper addresses the challenge of accurately estimating rotation in 6D object pose estimation from RGB-D object observations. The difficulty lies in learning in the non-linear space of SO(3). To overcome this challenge, the authors propose a novel rotation estimation network called VI-Net. The key idea of VI-Net is to decouple the rotation into a viewpoint rotation and an in-plane rotation. The network consists of two branches: a V-Branch for learning the viewpoint rotation through binary classification on spherical signals, and an I-Branch for estimating the in-plane rotation by transforming the signals to view from the zenith direction. To process the spherical signals, a Spherical Feature Pyramid Network is constructed using a novel design of SPAtial Spherical Convolution (SPA-SConv). This design addresses the boundary problem of spherical signals and enables viewpoint-equivariant feature extraction through symmetric convolutional operations. The proposed VI-Net is applied to the task of category-level 6D object pose estimation without available CAD models. Experimental results on benchmarking datasets demonstrate the effectiveness of the method, with significant improvement over existing approaches in terms of high precision.