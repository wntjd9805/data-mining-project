Recent research has demonstrated the ability to invert real images into StyleGAN's latent space, allowing for various edits due to the rich feature representations of well-trained GAN models. However, inverting erased images poses a more challenging task for realistic inpaintings and edits. In this study, we propose a solution by learning an encoder and a mixing network to combine encoded features from erased images with StyleGAN's mapped features from random samples. To ensure the utilization of both inputs, the networks are trained with generated data using a novel setup. Additionally, higher-rate features are utilized to prevent color inconsistencies between the inpainted and unerased regions. Extensive experiments are conducted, comparing our method with state-of-the-art inversion and inpainting techniques. The results, evaluated through qualitative metrics and visual comparisons, demonstrate significant improvements.