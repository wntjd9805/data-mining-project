Federated learning (FL) on mobile edge devices is a promising distributed learning method for mobile applications. However, it faces challenges such as high training latency, heterogeneous training data, and hardware differences in mobile edge devices. To address these challenges, we propose a new FL scheme called "workie-talkie" FL with contrastive regularization (FedCR). FedCR accelerates training by overlapping local computing and wireless communications, reducing latency and eliminating straggler issues. We also introduce class-wise contrastive regularization to handle model staleness and data heterogeneity. Additionally, we combine contrastive regularization with subnetworks to accommodate hardware differences in edge devices. Our experiments in a FL testbed demonstrate that FedCR outperforms existing FL approaches on various datasets and models.