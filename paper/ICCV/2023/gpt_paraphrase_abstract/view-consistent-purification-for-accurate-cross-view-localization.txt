This paper presents a new method for accurately localizing outdoor robotics using a variable number of onboard cameras and easily accessible satellite images. Existing cross-view localization methods have limitations in dealing with noise sources like moving objects and seasonal variations. Our method is the first sparse visual-only approach that improves perception in dynamic environments by identifying consistent key points and their corresponding deep features from ground and satellite views. It also eliminates off-the-ground objects and establishes a homography transformation between the two views. Additionally, our method incorporates a spatial embedding technique that utilizes camera intrinsic and extrinsic information to reduce ambiguity in visual matching, resulting in better feature matching and overall pose estimation accuracy. The method is highly adaptable and resilient to environmental changes, only requiring geo-poses as ground truth. Extensive experiments on the KITTI and Ford Multi-AV Seasonal datasets demonstrate that our method surpasses current state-of-the-art methods, achieving median spatial accuracy errors below 0.5 meters in lateral and longitudinal directions, as well as a median orientation accuracy error below 2.1 degrees.