Current multi-expert ensemble models for long-tailed learning face challenges in effectively addressing the imbalance factor of the entire dataset and predicting unseen classes. To overcome these issues, we propose a novel method called Local and Global Logit Adjustments (LGLA). LGLA learns experts that cover all classes and increases the discrepancy among them through logit adjustments. It comprises two main components: the Class-aware Logit Adjustment (CLA) strategy and the Adaptive Angular Weighted (AAW) loss. The CLA strategy trains multiple experts on subsets using Local Logit Adjustment (LLA), while also training one expert for the inversely long-tailed distribution through Global Logit Adjustment (GLA). The AAW loss further improves accuracy by employing adaptive hard sample mining for different experts. Our experiments on popular long-tailed benchmarks demonstrate the superiority of LGLA over state-of-the-art methods.