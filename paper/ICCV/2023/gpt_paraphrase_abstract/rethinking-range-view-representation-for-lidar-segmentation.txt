Autonomous driving perception relies heavily on LiDAR segmentation. Point- or voxel-based methods have gained popularity due to their superior performance compared to traditional range view representation. However, there are several challenges in effectively learning from range view projections, such as the "many-to-one" mapping, semantic incoherence, and shape deformation. To address these obstacles, we propose RangeFormer, a comprehensive framework that encompasses innovative designs in network architecture, data augmentation, and post-processing. RangeFormer enhances the learning and processing of LiDAR point clouds from the range view. Additionally, we introduce a Scalable Training from Range view (STR) strategy that enables training on low-resolution 2D range images while maintaining satisfactory 3D segmentation accuracy. Remarkably, our range view method surpasses the point, voxel, and multi-view fusion counterparts in the benchmark tests for LiDAR semantic and panoptic segmentation (e.g., SemanticKITTI, nuScenes, and ScribbleKITTI). This achievement is the first of its kind and demonstrates the superior performance of our approach in these competitive benchmarks.