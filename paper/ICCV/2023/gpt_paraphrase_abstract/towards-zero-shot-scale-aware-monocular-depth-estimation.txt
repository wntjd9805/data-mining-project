Monocular depth estimation is a challenge due to its ambiguity in scale, requiring supervision to generate metric predictions. However, existing models are limited to specific geometries and cannot transfer scales across different domains. To address this, recent studies have focused on relative depth and zero-shot transfer. In this research, we propose a new framework called ZeroDepth that can accurately predict metric scale for diverse test images from various domains and camera parameters. This is achieved by incorporating input-level geometric embeddings, allowing the network to learn a scale prior for objects. Additionally, we decouple the encoder and decoder stages using a variational latent representation conditioned on single frame information. We evaluated ZeroDepth on both outdoor (KITTI, DDAD, nuScenes) and indoor (NYUv2) benchmarks, and our approach achieved a new state-of-the-art performance in both settings using a single pre-trained model. This outperformed methods that rely on in-domain training data and require test-time scaling for metric estimation. Further details can be found on our project page: https://sites.google.com/view/tri-zerodepth.