In practical machine learning applications, the issue of imbalanced class distribution often arises, particularly in long-tailed recognition scenarios. Existing methods to address this problem, such as data reweighing, resampling, and supervised contrastive learning, have limitations. These methods introduce imbalance between instances of the head class and tail class, which can overlook the underlying semantic structures of the head class and exacerbate biases in the tail class. To overcome these drawbacks, we propose a new approach called "subclass-balancing contrastive learning (SBCL)." This method clusters each head class into multiple subclasses that have similar sizes as the tail classes. By doing so, we ensure that the representations capture the two-layer class hierarchy between the original classes and their subclasses. The clustering is performed in the representation space and is updated during training, preserving the semantic substructures of the head classes. Furthermore, our approach avoids overemphasizing tail class samples, allowing each individual instance to contribute equally to representation learning. As a result, SBCL achieves both instance- and subclass-balance, while also learning the original class labels through contrastive learning among subclasses from different classes. We evaluate SBCL on various long-tailed benchmark datasets and demonstrate that it achieves state-of-the-art performance. Additionally, we conduct extensive analyses and ablation studies to verify the advantages of SBCL. Our code for implementing SBCL is available at https://github.com/JackHck/subclass-balancing-contrastive-learning.