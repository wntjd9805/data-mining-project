Video action segmentation is the process of categorizing frames or short snippets of a video into predefined action categories. While there have been advancements in this field, many current approaches still rely on segmenting frames individually, resulting in fragmented results. To address this issue, we propose a novel approach centered around contextually refined temporal keypoints.Our method identifies a set of sparse, over-complete temporal keypoints using non-local visual cues. Each keypoint represents a potential action segment candidate. We then enhance these initial keypoints through iterative refining and reassembling operations. We believe that optimal temporal keypoints should collectively resemble the true ground-truth structure, so we introduce a module that conducts graph matching between the keypoint-derived graph and a reference graph constructed from accurate annotations. This module learns structural features to further refine the initial keypoints.We also apply a set of predefined rules to reassemble all temporal keypoints. The resulting unfiltered temporal keypoints are used to generate the final action segments. We evaluate our approach on three video benchmarks (50salads, GTEA, and Breakfast) and consistently achieve substantial improvements over existing methods. Our approach demonstrates superiority in video action segmentation, with F1@50 scores of 79.5%, 83.4%, and 60.5% for each benchmark, compared to previous state-of-the-art scores of 78.5%, 79.8%, and 57.4% respectively.