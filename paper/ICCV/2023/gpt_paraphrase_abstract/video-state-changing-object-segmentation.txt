This paper introduces a new benchmark called Video State-Changing Object Segmentation (VSCOS) to address the challenge of understanding and interacting with object state changes in video. Existing benchmarks in Video Object Segmentation (VOS) do not focus on this crucial problem. The VSCOS benchmark is constructed by selecting videos that involve state changes from existing datasets. In order to be annotation-efficient, only the first and last frames of training videos are annotated, which differs from conventional VOS. Additionally, an open-vocabulary setting is included to evaluate the generalization to new types of objects or state changes. State-of-the-art VOS models struggle with state-changing objects and lose track after the state changes. The main difficulties of the VSCOS task are analyzed and three technical improvements are identified: fine-tuning strategies, representation learning, and integrating motion information. Applying these improvements results in a strong baseline for consistently segmenting state-changing objects. The VSCOS benchmark and baseline methods are publicly available on GitHub at https://github.com/venom12138/VSCOS.