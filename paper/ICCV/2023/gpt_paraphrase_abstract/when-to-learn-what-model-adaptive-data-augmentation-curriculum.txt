We propose a new approach called "Model-Adaptive Data Augmentation (MADAug)" for improving the generalization of neural networks. Unlike existing methods, MADAug dynamically adjusts the data augmentation policy for each input image based on the training stage. This creates a curriculum of data augmentations that is optimized for better generalization. We train the augmentation policy using a bi-level optimization scheme, aiming to minimize the validation-set loss of a model trained with the policy-produced augmentations. We evaluate MADAug on various image classification tasks and network architectures, comparing it to existing data augmentation approaches. MADAug outperforms or matches other baselines and exhibits better fairness by improving performance across all classes, especially the difficult ones. Additionally, MADAug's learned policy performs well when transferred to fine-grained datasets. The auto-optimized policy in MADAug gradually introduces increasing perturbations, forming a curriculum from easy to hard. Our code is available at https://github.com/JackHck/MADAug.