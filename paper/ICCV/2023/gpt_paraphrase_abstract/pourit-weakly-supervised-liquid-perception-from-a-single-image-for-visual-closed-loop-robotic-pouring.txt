This paper introduces a framework called PourIt! for robotic pouring tasks that aims to overcome the limitations of existing methods in liquid perception. Current approaches rely on labeled data for training, which is time-consuming and requires human labor. PourIt! proposes a simple data collection pipeline that reduces the need for pixel-wise annotations by using image-level labels. A binary classification model is trained to generate Class Activation Maps (CAM) that focus on the visual difference between images with and without liquid drops. A feature contrast strategy is employed to improve the quality of the CAM, ensuring accurate coverage of the liquid regions. The container pose is then utilized to recover the 3D point cloud of the detected liquid region. Additionally, the liquid-to-container distance is calculated to enable visual closed-loop control of the physical robot. To evaluate the effectiveness of PourIt!, a novel dataset called PourIt! dataset is contributed. Results obtained from this dataset and experiments with a physical Franka robot demonstrate the utility and effectiveness of the proposed method. The dataset, code, and pre-trained models will be made available on the project page.