Training data for video segmentation is costly to annotate, which hinders the application of end-to-end algorithms to new video segmentation tasks, particularly in scenarios where a large vocabulary is not available. To address this issue, we propose a decoupled video segmentation approach called DEVA. DEVA consists of task-specific image-level segmentation and class/task-agnostic bi-directional temporal propagation. By adopting this design, we only require an image-level model for the specific task, which is more affordable to train, and a universal temporal propagation model that is trained once and can be applied across different tasks. To effectively combine these two modules, we employ bi-directional propagation to fuse segmentation hypotheses from various frames in a (semi-)online manner, resulting in a coherent segmentation. Our experiments demonstrate that this decoupled formulation performs favorably compared to end-to-end approaches in various data-scarce tasks such as large-vocabulary video panoptic segmentation, open-world video segmentation, referring video segmentation, and unsupervised video object segmentation. The code for DEVA is available at: hkchengrex.github.io/Tracking-Anything-with-DEVA.