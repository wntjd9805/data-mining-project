Many studies have focused on creating effective embedding spaces for predicting single-label objects in images. However, in reality, objects often have multiple specific attributes, such as shape, color, and length, each composed of various classes. To apply models in real-world scenarios, it is crucial to be able to distinguish and identify the granular components of an object. The conventional approach of embedding multiple attributes into a single network often leads to entanglement, where fine-grained features of each attribute cannot be separated. To address this issue, we propose a Conditional Cross-Attention Network that generates disentangled multi-space embeddings for specific attributes using a single backbone. Firstly, we use a cross-attention mechanism to merge and switch the information of specific attributes, which we validate through visualizations. Secondly, we introduce the vision transformer to fine-grained image retrieval, presenting a simple yet effective framework compared to existing methods. Unlike previous studies that showed varying performance based on benchmark datasets, our proposed method consistently achieves state-of-the-art performance on the FashionAI, DARN, DeepFashion, and Zappos50K benchmark datasets.