This study investigates the core issue of multi-layer generator models in the acquisition of hierarchical representations. These models, composed of multiple layers of latent variables arranged in a top-down structure, aim to learn different levels of data abstraction. However, the conventional approach of parameterizing these latent variables as Gaussian can be insufficient in capturing complex abstractions, leading to limited success in hierarchical representation learning. Conversely, the energy-based (EBM) prior is known to excel at capturing data regularities, but it often lacks the hierarchical structure needed to capture diverse levels of hierarchical representations. To address this, the paper proposes a joint latent space EBM prior model that incorporates multi-layer latent variables for effective hierarchical representation learning. The authors introduce a variational joint learning scheme that seamlessly integrates an inference model to facilitate efficient inference. Experimental results demonstrate that the proposed joint EBM prior is both effective and expressive in capturing hierarchical representations and modeling data distribution.