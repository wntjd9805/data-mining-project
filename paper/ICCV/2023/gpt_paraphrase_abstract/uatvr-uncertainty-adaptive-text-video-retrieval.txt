The rise of web videos and large-scale vision-language pre-training models like CLIP has sparked interest in retrieving videos based on text instructions. This is typically done by aligning text-video pairs in an embedding space and creating cross-modal interactions for semantic correspondence. However, the optimal combinations of entities in different granularities for cross-modal queries are not well-studied, especially for modalities with hierarchical semantics such as video and text.To address this issue, we propose a new approach called Uncertainty-Adaptive Text-Video Retrieval (UATVR). UATVR treats each search as a distribution matching process, using additional learnable tokens in the encoders to aggregate multi-grained semantics for flexible high-level reasoning. In the refined embedding space, text-video pairs are represented as probabilistic distributions, and prototypes are sampled for matching evaluation.We conducted comprehensive experiments on four benchmarks and found that UATVR outperformed existing methods, achieving state-of-the-art results on MSR-VTT (50.8%), VATEX (64.5%), MSVD (49.7%), and DiDeMo (45.8%). The code for UATVR is available at the following GitHub repository: https://github.com/bofang98/UATVR.