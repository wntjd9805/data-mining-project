Deep Image Manipulation Localization (IML) models face challenges due to limited training data and therefore heavily rely on pre-training. However, we propose that contrastive learning is a more suitable approach to address the data insufficiency problem in IML. Contrastive learning requires the creation of mutually exclusive positive and negative samples. In the case of IML, we encounter three types of image patches: tampered, authentic, and contour patches. While tampered and authentic patches are naturally mutually exclusive, contour patches contain both tampered and authentic pixels, making them non-mutually exclusive. Simply discarding these contour patches leads to significant performance degradation as they are crucial for learning outcomes. To overcome this dilemma, we introduce the Non-mutually exclusive Contrastive Learning (NCL) framework. In NCL, we establish a pivot structure with dual branches that alternates the role of contour patches between positives and negatives during training. We also design a pivot-consistent loss to prevent spatial corruption caused by the role-switching process. By doing so, NCL leverages the advantages of self-supervised learning to address data insufficiency while maintaining a high manipulation localization accuracy. Extensive experiments demonstrate that NCL achieves state-of-the-art performance on all five benchmarks without the need for pre-training and exhibits greater robustness on unseen real-life samples. The implementation of NCL can be found at https://github.com/Knightzjz/NCL-IML.