The process of locally editing 3D objects based on text is challenging due to the difficulty of blending the original object with the desired changes without distorting its form. To overcome this challenge, we propose a new model called Blending-NeRF, which consists of two NeRF networks: a pre-trained NeRF and an editable NeRF. We also introduce new blending operations that enable Blending-NeRF to accurately edit specific regions of the object specified by the text. By utilizing CLIP, a pretrained vision-language aligned model, we guide Blending-NeRF to add new objects with different colors and densities, modify textures, and remove parts of the original object. Through extensive experiments, we demonstrate that Blending-NeRF can generate natural and localized edits to 3D objects based on various text prompts.