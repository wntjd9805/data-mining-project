This paper presents a novel approach called Sound Localization from Motion (SLfM), which involves estimating camera rotation and localizing sound sources based on the consistent changes in images and sounds as we rotate our heads. The approach utilizes self-supervision to learn these tasks, where a visual model predicts camera rotation from image pairs, and an audio model predicts sound source direction from binaural sounds. The models are trained to generate predictions that align with each other. During testing, the models can be deployed independently. To tackle the challenging problem, the paper proposes a method for learning an audio-visual representation through cross-view binauralization, which estimates binaural sound from one view using images and sound from another view. The proposed model achieves accurate rotation estimation on both real and synthetic scenes and competes with state-of-the-art self-supervised approaches in sound source localization. The paper provides further details and results on the project website: https://ificl.github.io/SLfM.