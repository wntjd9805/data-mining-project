In recent advancements in multi-modal 3D object detection, a fusion technique called Bird-Eye-View (BEV) based fusion has been utilized to combine LiDAR point clouds and camera images in a shared BEV space. However, the process of transforming camera images to BEV is challenging due to the inherent uncertainty in depth estimation for each pixel, resulting in misalignment between these two modalities. Additionally, this transformation also causes distortion in the projection of camera image features in BEV.   To address these issues, this paper introduces a new approach called Object-centric Fusion (ObjectFusion) that eliminates the need for camera-to-BEV transformation during fusion. Instead, ObjectFusion focuses on aligning object-centric features across different modalities for 3D object detection. The proposed method involves learning modality-specific feature maps (voxel, BEV, and image features) from LiDAR point clouds, BEV projections, and camera images. A heatmap-based proposal generator is then used to generate 3D object proposals from the BEV features. These proposals are subsequently reprojected back to voxel, BEV, and image spaces. By utilizing voxel and RoI pooling techniques, object-centric features are generated for each modality, ensuring spatial alignment. Finally, the object-centric features from all three modalities are fused at the object level and fed into the detection heads for further processing.  Extensive experiments conducted on the nuScenes dataset demonstrate the effectiveness of ObjectFusion. The proposed approach achieves a mean average precision (mAP) of 69.8% on the nuScenes validation set, outperforming the previous BEV fusion technique (BEVFusion) by 1.3%.