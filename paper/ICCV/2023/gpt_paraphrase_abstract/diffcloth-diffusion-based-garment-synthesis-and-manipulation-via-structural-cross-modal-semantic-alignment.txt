Fashion designers can benefit from cross-modal garment synthesis and manipulation, which allows them to generate and modify garments using flexible linguistic interfaces. However, existing approaches for generating garment images aligned with text prompts and manipulating them have limitations. These approaches neglect the structural correspondence between visual and textual representations in fashion design. To address this, we propose DiffCloth, a diffusion-based pipeline for cross-modal garment synthesis and manipulation. DiffCloth aligns cross-modal semantics by formulating a bipartite matching problem between linguistic attribute-phrases and visual garment parts. We use constituency parsing and semantic segmentation to obtain these parts. To address attribute confusion, we introduce a semantic-bundled cross-attention mechanism that preserves spatial structure similarities between attribute adjectives and part nouns in each attribute-phrase. DiffCloth also allows for manipulation of generated results by replacing attribute-phrases in the text prompts. We identify manipulation-irrelevant regions using blended masks obtained from the bundled attention maps of the attribute-phrases and keep them unchanged. Our extensive experiments on the CM-Fashion benchmark demonstrate that DiffCloth achieves state-of-the-art garment synthesis results by leveraging structural information and supports flexible manipulation with region consistency.