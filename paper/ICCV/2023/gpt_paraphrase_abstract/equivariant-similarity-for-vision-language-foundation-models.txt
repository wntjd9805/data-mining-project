This study examines the concept of equivariance in vision-language foundation models (VLMs), specifically focusing on the multimodal similarity function. The objective of this function is not only to train the models but also to support downstream tasks. Existing image-text similarity objectives categorize matched pairs as similar and unmatched pairs as dissimilar. However, equivariance requires similarity to accurately vary based on semantic changes, allowing VLMs to generalize better to nuanced and unseen multimodal compositions. Modeling equivariance is challenging because collecting ground truth data for semantic changes is difficult. For instance, it is unclear how much similarity changes when an image-text pair about a dog is altered to a cat. To address this issue, we propose EQSIM, a regularization loss that can be efficiently calculated from any two matched training pairs and easily incorporated into existing image-text retrieval fine-tuning. Additionally, we introduce EQBEN, a new benchmark dataset designed to evaluate the equivariance of VLMs. Unlike existing evaluation sets, EQBEN focuses on "visual-minimal change". Extensive experiments demonstrate the lack of equivariance in current VLMs and validate the effectiveness of EQSIM.