This paper introduces a novel ground embedding module for monocular depth estimation, aiming to address the limitations of current algorithms in real-world scenarios. The proposed module decouples camera parameters from pictorial cues, promoting generalizability. By generating ground depth and incorporating it into the depth prediction process, the module optimally combines ground depth with residual depth using a ground attention mechanism. The ground embedding module is lightweight and flexible, making it easy to integrate into different depth estimation networks. Experimental results demonstrate that the proposed approach achieves state-of-the-art performance on popular benchmarks and significantly improves generalization across various cross-domain tests.