This study introduces a new approach to speech-driven 3D facial animation that overcomes the challenges of high supervision costs and ambiguity in mapping speech to lip movements. The proposed framework consists of a Speech-to-Image Transcoder and a Face-to-Geometry Regressor. The Transcoder learns a common representation space for speech and image domains, allowing for the transformation of speech into visually consistent facial images. The Regressor reconstructs 3D facial meshes from these transformed images. Both modules require minimal training data, eliminating the need for expensive supervision. Additionally, the joint learning scheme incorporates detailed visual features into speech encoding, enabling the transformation of subtle speech variations into nuanced lip movements and improving the fidelity of 3D face reconstructions. As a result, the direct mapping of speech-to-animation becomes less ambiguous, leading to coherent and high-fidelity generation of lip motion. Extensive experiments demonstrate that this approach produces competitive results compared to supervised methods.