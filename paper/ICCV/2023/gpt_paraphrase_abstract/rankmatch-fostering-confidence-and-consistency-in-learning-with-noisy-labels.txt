Learning with noisy labels (LNL) is a challenging problem in weakly-supervised learning. Current approaches use sample selection and small-loss criteria to mitigate noisy labels. However, the one-dimensional loss metric is too simplistic and can lead to classification errors during sample selection. To address this, we propose RankMatch, a novel LNL framework that considers confidence and consistency.   In terms of confidence, we introduce a new sample selection strategy based on confidence representation voting. This strategy increases the quantity of selected samples without sacrificing labeling accuracy.   Regarding consistency, we argue that the rank of principal features is a more reliable indicator than the commonly used feature distance metric. Based on this, we propose rank contrastive loss, which enhances the consistency of similar samples irrespective of their labels and aids in feature representation learning.   We conducted experiments on noisy versions of CIFAR-10, CIFAR-100, Clothing1M, and WebVision datasets. The results demonstrate the superiority of our approach compared to existing state-of-the-art methods.