This paper introduces a new approach to lip reading, specifically designed for low-resource languages, which have not been adequately addressed in previous research. Developing lip reading models for low-resource languages is challenging due to the lack of sufficient video-text paired data for training the models to accurately capture lip movements and language. To overcome this challenge, we propose a method that learns general speech knowledge from a high-resource language by predicting speech units. Since different languages share some common phonemes, the general speech knowledge acquired from one language can be applied to other languages. Additionally, we propose a Language-specific Memory-augmented Decoder (LMDecoder) to learn language-specific knowledge, which involves modeling language. LMDecoder stores language-specific audio features in memory banks and can be trained using audio-text paired data, which is more readily available than video-text paired data. By utilizing the learned language-specific knowledge, the input speech units can be transformed into language-specific audio features and translated into texts. By combining the general speech knowledge and language-specific knowledge, we can effectively develop lip reading models even for low-resource languages. The proposed method is evaluated through extensive experiments conducted in English, Spanish, French, Italian, and Portuguese.