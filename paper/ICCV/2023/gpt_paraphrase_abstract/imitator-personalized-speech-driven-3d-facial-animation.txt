Speech-driven 3D facial animation has been extensively explored in various fields such as gaming, character animation, virtual reality, and telepresence systems. However, existing methods that deform the face topology to synchronize with the input audio do not take into account the individual's unique speaking style and facial characteristics, resulting in unrealistic and inaccurate lip movements. To overcome this limitation, we introduce Imitator, a novel approach for speech-driven facial expression synthesis. Our method learns identity-specific details from a short input video and generates facial expressions that match the speaking style and facial idiosyncrasies of the target actor. To achieve this, we train a style-agnostic transformer on a large dataset of facial expressions, which serves as a prior for audio-driven facial expressions. This prior is then used to optimize for the identity-specific speaking style based on a short reference video. To train the prior, we propose a new loss function that detects bilabial consonants, ensuring plausible lip closures and enhancing the realism of the generated expressions. Through extensive experiments and user studies, we demonstrate that our approach significantly improves Lip-Sync accuracy by 49% compared to existing methods. Moreover, our method successfully produces expressive facial animations from input audio while preserving the actor's unique speaking style. More details and results can be found on our project page: https://balamuruganthambiraja.github.io/Imitator.