Binarizing convolutional neural networks has been extensively studied for compressing model size, reducing energy consumption, and speeding up inference. However, there is limited research on binarizing vision Transformers, which are crucial for recent advancements in visual recognition. In this study, we address two challenges to advance Binary Vision Transformers (BiViT). Firstly, the traditional binary method overlooks the long-tailed distribution of softmax attention, leading to significant binarization errors in the attention module. To overcome this, we propose Softmax-aware Binarization, which dynamically adjusts to the data distribution and reduces binarization errors. Secondly, to retain the information from pretrained models and maintain accuracy, we introduce a Cross-layer Binarization scheme that separates the binarization of self-attention and multi-layer perceptrons (MLPs). We also propose Parameterized Weight Scales, which incorporate learnable scaling factors for weight binarization. Overall, our approach outperforms existing methods by 19.8% on the TinyImageNet dataset and achieves a competitive 75.6% Top-1 accuracy over the Swin-S model on ImageNet. Additionally, in COCO object detection, our method achieves an mAP of 40.8 with a Swin-T backbone using the Cascade MaskR-CNN framework.