The current focus of video captioning efforts is on describing individual videos, but there is a growing need for captioning videos in groups. This study introduces a new task called group video captioning, which involves inferring the desired content from a group of target videos and describing it using a group of related reference videos. The task requires the model to summarize the target videos effectively and accurately describe the distinguishing content compared to the reference videos. This becomes more challenging as the length of the videos increases. To address this problem, the study proposes an efficient relational approximation (ERA) to identify shared content among videos, with complexity proportional to the number of videos. Additionally, a contextual feature refinery is introduced, which uses intra-group self-supervision to capture contextual information and refine common properties. Two group video captioning datasets derived from YouCook2 and ActivityNet Captions are also constructed for experimentation. The results of the experiments demonstrate the effectiveness of the proposed method for this new task.