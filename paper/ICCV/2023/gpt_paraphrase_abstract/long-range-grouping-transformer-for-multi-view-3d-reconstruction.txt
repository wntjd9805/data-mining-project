Transformer networks have shown superior performance in various computer vision tasks. However, in multi-view 3D reconstruction algorithms, self-attention processing becomes challenging due to the intricate image tokens containing massive information from multiple views. Existing methods address this issue by either compressing the token number or discarding attention operations between tokens from different views, which negatively impacts performance. To overcome this problem, we propose the long-range grouping attention (LGA) approach based on the divide-and-conquer principle. LGA groups tokens from all views for separate attention operations, enabling macro representation for each view and ensuring diverse feature learning among different groups. We establish an effective and efficient encoder that utilizes LGA for inter-view feature connections and standard self-attention for intra-view feature extraction. Additionally, we design a novel progressive upsampling decoder for high-resolution voxel generation. Based on these contributions, we introduce a powerful transformer-based network called LRGT. Experimental results on ShapeNet demonstrate that our method achieves state-of-the-art accuracy in multi-view reconstruction. The code is available at https://github.com/LiyingCV/Long-Range-Grouping-Transformer.