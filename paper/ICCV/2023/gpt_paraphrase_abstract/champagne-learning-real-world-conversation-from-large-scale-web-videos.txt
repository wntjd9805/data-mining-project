Most neural conversational models are limited to text-based conversations, disregarding the importance of visual information such as body gestures and physical behavior in conveying meaning. In this study, we introduce CHAMPAGNE, a generative model that takes into account visual contexts in conversations. To train CHAMPAGNE, we create YTD-18M, a large dataset of 18 million video-based dialogues collected from web videos. To ensure the quality of the dataset, we utilize a pretrained language model to convert error-prone automatic transcripts into a cleaner dialogue format while preserving the original meaning. Human evaluation shows that YTD-18M is more sensible and specific compared to prior resources with fewer dialogues, while still incorporating visual information. Our experiments demonstrate that CHAMPAGNE successfully learns to conduct conversations using YTD-18M and achieves state-of-the-art performance on four vision-language tasks related to real-world conversations when fine-tuned. We provide the data, models, and code for CHAMPAGNE at https://seungjuhan.me/champagne.