Vision-Language Pretraining (VLP) has been successful in improving performance on various tasks by training models on large datasets offline. However, as real-world data continues to grow, this approach becomes unsustainable as models lack the ability to continually learn and accumulate knowledge. Most studies on continual learning are limited to single-modal classification, and existing multimodal datasets do not simulate scenarios with non-stationary data streams. To address these limitations and support the study of Vision-Language Continual Pretraining (VLCP), we introduce a comprehensive benchmark dataset called P9D. This dataset consists of over one million product image-text pairs from 9 industries, where each industry represents an independent task that supports continual learning and reflects the long-tail nature of real-world data. We thoroughly analyze the characteristics and challenges of VLCP and propose a new algorithm called Compatible momentum contrast with Topology Preservation (CTP). The CTP algorithm allows the model to absorb knowledge from both the current and previous-task models to update modal features flexibly. Additionally, Topology Preservation enables the transfer of embedding knowledge across tasks while maintaining the flexibility of feature adjustment. Experimental results demonstrate that our method outperforms other baselines and does not require extensive training. The P9D dataset and codes are available at https://github.com/KevinLight831/CTP.