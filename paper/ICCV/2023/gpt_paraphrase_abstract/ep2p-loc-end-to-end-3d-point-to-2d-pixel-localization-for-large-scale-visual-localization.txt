Visual localization is the process of determining the position and orientation of a camera in a 3D environment based on a 2D image. While 3D point clouds are increasingly used to create accurate reference maps, matching these points with pixels in 2D images remains challenging. Existing methods that learn to match features from both modalities struggle with low inlier rates, and alternative approaches that bypass this issue through classification suffer from poor refinement. In this study, we propose EP2P-Loc, a novel visual localization method that addresses these problems and allows for end-to-end training of pose estimation. To increase the number of inliers, we develop a simple algorithm to remove invisible 3D points in the image and establish 2D-3D correspondences without relying on keypoint detection. To optimize memory usage and search complexity, we adopt a coarse-to-fine approach that involves extracting patch-level features from 2D images, performing 2D patch classification on each 3D point, and obtaining precise 2D pixel coordinates through positional encoding. Additionally, we introduce a differentiable Perspective-n-Point (PnP) algorithm for end-to-end training, which is a first in this field. Through experiments on large-scale indoor and outdoor benchmarks, including 2D-3D-S and KITTI, we demonstrate that our method outperforms existing visual localization and image-to-point cloud registration methods, establishing a new state-of-the-art.