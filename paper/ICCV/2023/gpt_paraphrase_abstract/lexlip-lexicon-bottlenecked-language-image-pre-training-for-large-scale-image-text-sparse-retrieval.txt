Image-text retrieval (ITR) is the process of retrieving images or texts that correspond to a query from the opposite modality. The traditional approach to ITR involves encoding images and texts into dense representations using dual-stream encoders. However, this method is not efficient for large-scale scenarios as it results in slow retrieval speeds. To overcome this limitation, we propose a new approach called sparse retrieval, which utilizes sparse representations in the vocabulary space for images and texts. By leveraging bag-of-words models and efficient inverted indexes, we are able to significantly reduce retrieval latency.One challenge in implementing sparse retrieval is representing continuous image data in a sparse vocabulary space. To address this, we introduce a novel pre-training framework called Lexicon-Bottlenecked Language-Image Pre-Training (LexLIP). This framework learns importance-aware lexicon representations by using lexicon-bottlenecked modules between the dual-stream encoders and weakened text decoders. This allows us to create continuous bag-of-words bottlenecks and learn distributions of lexicon importance.Through pre-training with same-scale data, LexLIP achieves state-of-the-art performance on two ITR benchmarks, MSCOCO and Flickr30k. Additionally, in large-scale retrieval scenarios, LexLIP outperforms CLIP with a retrieval speed that is 5.8 times faster and requires 19.1 times less index storage memory. Furthermore, LexLIP surpasses CLIP in 8 out of 10 zero-shot image classification tasks.In summary, we propose a sparse retrieval paradigm for ITR that utilizes sparse representations in the vocabulary space to improve retrieval speeds. Our pre-training framework, LexLIP, addresses the challenge of representing continuous image data in a sparse vocabulary space. The results demonstrate that LexLIP achieves superior performance compared to existing methods in terms of retrieval speed, index storage memory, and zero-shot image classification tasks.