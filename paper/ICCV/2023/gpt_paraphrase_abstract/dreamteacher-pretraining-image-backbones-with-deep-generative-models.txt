This study introduces a framework called DreamTeacher, which utilizes generative networks for pre-training image backbones. The framework aims to distill knowledge from trained generative models into standard image backbones that are well-designed for specific perception tasks. Two types of knowledge distillation are investigated: 1) transferring learned generative features onto target image backbones instead of pretraining them on labeled datasets like ImageNet, and 2) distilling labels obtained from generative networks with task heads onto target backbones' logits. The researchers conducted extensive analyses using various generative models, dense prediction benchmarks, and different pre-training methods. The empirical results demonstrate that DreamTeacher outperforms existing self-supervised representation learning methods consistently. Pre-training on unsupervised ImageNet using DreamTeacher leads to significant improvements in downstream datasets compared to pre-training on ImageNet classification alone. This highlights the potential of generative models, particularly diffusion generative models, as a promising approach for representation learning on large and diverse datasets without the need for manual annotation.