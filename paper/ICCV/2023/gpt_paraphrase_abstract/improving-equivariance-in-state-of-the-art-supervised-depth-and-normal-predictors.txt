Current state-of-the-art depth and surface normal predictors lack the equivariant property to cropping-and-resizing, despite their strong performance. This problem persists even with crop-and-resize data augmentation during training. To address this issue, we propose an equivariant regularization technique that includes an averaging procedure and a self-consistency loss. This technique explicitly promotes cropping-and-resizing equivariance in both CNN and Transformer architectures. Importantly, our approach does not add extra computational cost during testing and significantly enhances the supervised and semi-supervised learning performance of dense predictors on Taskonomy tasks. Additionally, when fine-tuned with our loss on unlabeled images, the accuracy and equivariance of the state-of-the-art depth and normal predictors are improved when evaluated on NYU-v2.