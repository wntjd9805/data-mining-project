This paper focuses on developing a knowledge distillation (KD) method for compact 3D detectors. It is observed that existing KD methods are only effective when the teacher model and student counterpart have similar intermediate feature representations. However, these methods are less effective for extreme-compact 3D detectors due to the significant representation disparity caused by the sparsity and irregularity of 3D point clouds. To address this issue and reduce the performance gap between compact students and over-parameterized teachers, a novel representation disparity-aware distillation (RDD) method is proposed. RDD is built from the perspective of information bottleneck (IB) and effectively minimizes the disparity in features and logits between student and teacher proposal region pairs. Extensive experiments demonstrate the superiority of RDD over existing KD methods. For instance, RDD increases the mAP of CP-Voxel-S to 57.1% on the nuScenes dataset, surpassing the teacher's performance while utilizing only 42% FLOPs.