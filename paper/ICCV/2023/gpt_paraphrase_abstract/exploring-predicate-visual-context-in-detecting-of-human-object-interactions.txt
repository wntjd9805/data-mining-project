The DETR framework has become the dominant approach for human-object interaction (HOI) recognition, particularly in two-stage transformer-based HOI detectors. These detectors are highly effective and efficient in training. However, they rely on object features that lack detailed contextual information, disregarding pose and orientation details in favor of visual cues related to object identity and box extremities. Consequently, they struggle to recognize complex or ambiguous interactions. This study aims to address these issues by conducting visualizations and carefully designed experiments. We explore the best way to reintroduce image features through cross-attention. By improving query design, extensively exploring keys and values, and incorporating box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) surpasses state-of-the-art methods on the HICO-DET and V-COCO benchmarks while maintaining low training cost.