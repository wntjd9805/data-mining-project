The development of pre-training methods that combine 3D vision and language is hindered by limited training data. Recent attempts to transfer vision-language pre-training methods to 3D vision have been limited by the domain gap between 3D and images. In order to overcome this limitation, we propose CLIP2Point, a pre-training method that uses contrastive learning to transfer CLIP to the 3D domain and adapt it to point cloud classification. We introduce a new depth rendering setting that improves visual effects and generate a dataset of image-depth pairs for pre-training. The pre-training scheme of CLIP2Point combines cross-modality learning to capture expressive visual and textual features with intra-modality learning to enhance the invariance of depth aggregation. We also propose a novel dual-path structure called GatedDual-Path Adapter (GDPA) for efficient adaptation of pre-training knowledge to downstream tasks. Experimental results demonstrate that CLIP2Point effectively transfers CLIP knowledge to 3D vision and outperforms other 3D transfer learning and pre-training networks in zero-shot, few-shot, and fully-supervised classification tasks. The code for CLIP2Point is available at: [link].