Prompt learning has become an efficient approach to adapt pre-trained vision-language models for different tasks. Current methods like CoOp and ProDA use soft prompts to learn task-specific prompts. However, the recent CoCoOp approach combines image semantics with prompts of different labels, which weakens class discrimination. To address this, we propose a class-aware text prompt (CTP) that enriches prompts with label-related image information, effectively incorporating image semantics without introducing ambiguities. Additionally, we propose text-guided feature tuning (TFT) to make the image branch focus on class-related representations instead of preserving complete image representations. A contrastive loss is used to align augmented text and image representations for downstream tasks. Our method outperforms existing approaches, including CoCoOp, achieving significant improvements in classification benchmarks. Specifically, we achieve an average improvement of 4.03% on new classes and 3.19% on harmonic-mean over eleven benchmarks.