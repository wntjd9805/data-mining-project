Closed-set 3D perception models that are trained on a predetermined set of object categories may not be suitable for safety critical applications like autonomous driving, as they may encounter new object types after deployment. This paper introduces a multi-modal auto labeling pipeline capable of generating amodal 3D bounding boxes and tracklets to train models on open-set categories without 3D human labels. The pipeline utilizes motion cues from point cloud sequences and freely available 2D image-text pairs to detect and track all traffic participants. Unlike previous studies, which can only provide class-agnostic auto labels for moving objects, our method can handle both static and moving objects in an unsupervised manner and can output open-vocabulary semantic labels due to the proposed vision-language knowledge distillation. Experimental results on the Waymo Open Dataset demonstrate that our approach significantly outperforms previous work in various unsupervised 3D perception tasks.