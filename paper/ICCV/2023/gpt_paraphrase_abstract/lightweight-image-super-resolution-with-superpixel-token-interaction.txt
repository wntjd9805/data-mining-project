Current methods for single-image super-resolution (SISR) using transformer-based models have yielded impressive results. However, the computational cost of the self-attention mechanism when applied to the entire image is high. To overcome this, existing approaches divide low-resolution input images into patches, which are processed separately and then combined to generate high-resolution images. However, this conventional patch division approach is coarse and lacks interpretability, leading to artifacts and interference in attention operations. To address these challenges, we propose a new method called Super Token Interaction Network (SPIN). SPIN utilizes superpixels to cluster similar local pixels, creating interpretable local regions, and employs intra-superpixel attention to enable interaction between local information. This approach is interpretable as it only complements similar regions while excluding dissimilar ones. Additionally, we introduce a superpixel cross-attention module to facilitate information propagation through the surrogate representation of superpixels. Extensive experiments demonstrate that our SPIN model outperforms state-of-the-art SR methods in terms of accuracy and computational efficiency. The code for SPIN is available at https://github.com/ArcticHare105/SPIN.