Diffusion models, which use deep neural networks for noise estimation, have had success in image synthesis. However, the slow inference, high memory usage, and computational intensity of the noise estimation model hinder the efficient use of diffusion models. Post-training quantization (PTQ), a compression method commonly used for other tasks, does not work well on diffusion models. To address this, we propose a new PTQ method specifically designed for the unique pipeline and architecture of diffusion models. Our method compresses the noise estimation network to speed up the generation process. The main challenge in quantizing diffusion models is the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers in the noise estimation network. We solve these challenges by incorporating timestep-aware calibration and split short-cut quantization. Experimental results show that our method can quantize full-precision unconditional diffusion models into 4-bit without compromising performance. Compared to traditional PTQ, our method only causes a small change in Fr√©chet Inception Distance (FID) of at most 2.34, whereas traditional PTQ results in changes greater than 100. Our approach can also be applied to text-guided image generation, enabling stable diffusion with 4-bit weights and high generation quality.