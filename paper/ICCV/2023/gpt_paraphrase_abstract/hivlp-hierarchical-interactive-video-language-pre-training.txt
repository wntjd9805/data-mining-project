Video-Language Pre-training (VLP) is a popular area of study in deep learning. However, compared to image-language pre-training, VLP has fallen behind due to the limited availability of video-text pairs. In this research, we train a VLP model using a combination of image-text and video-text pairs, which outperforms pre-training with only video-text pairs. Existing methods typically use cross-attention between visual and textual tokens to model cross-modal interaction. However, these visual features are either low resolution and lack detailed information or high resolution without high-level semantics. To address this issue, we propose Hierarchical interactive Video-Language Pre-training (HiVLP) which efficiently utilizes a hierarchical visual feature group for multi-modal cross-attention during pre-training. In this hierarchical framework, low-resolution features focus on global high-level semantic information, while high-resolution features capture fine-grained details. As a result, HiVLP effectively learns both global and fine-grained representations, improving the alignment between video and text inputs. Additionally, we introduce a hierarchical multi-scale vision contrastive loss for self-supervised learning to enhance their interaction. Experimental results demonstrate that HiVLP achieves state-of-the-art performance in text-video retrieval, video-text retrieval, and video captioning tasks.