Referring to the task of video object segmentation (RVOS), which requires annotated data, we face challenges when minimal annotations are available for new scenes. To address this issue, we propose a model with a newly designed cross-modal affinity (CMA) module based on a Transformer architecture. This module quickly learns new semantic information and enables the model to adapt to different scenarios. We define this problem as few-shot referring video object segmentation (FS-RVOS) and create a benchmark to encourage research in this direction. The benchmark covers various situations to simulate real-world scenarios. Extensive experiments demonstrate that our model performs well with only a few samples, outperforming existing methods on the benchmark. On Mini-Ref-YouTube-VOS, our model achieves an average performance of 53.1 J and 54.8 F, which is 10% better than the baselines. Additionally, on Mini-Ref-SAIL-VOS, our model achieves impressive results of 77.7 J and 74.8 F, significantly surpassing the baselines. Our code is publicly available at https://github.com/hengliusky/Few_shot_RVOS.