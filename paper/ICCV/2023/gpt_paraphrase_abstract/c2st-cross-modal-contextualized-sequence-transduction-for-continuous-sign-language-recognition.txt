Continuous Sign Language Recognition (CSLR) is a process of converting sign language in a video into written words or glosses. The current mainstream approach to CSLR involves using a spatial module to learn visual representation, a temporal module to analyze the temporal information of the video frames, and the connectionist temporal classification (CTC) loss to align video features with gloss sequences. However, this approach disregards the language prior embedded in the gloss sequence and does not consider the contextualization of glosses during alignment learning. To address these limitations, we propose a new method called Cross-modal Contextualized Sequence Transduction (C2ST) for CSLR. Our method effectively incorporates the knowledge of gloss sequence into the video representation learning and sequence transduction process. We introduce a cross-modal context learning framework that extracts linguistic features from gloss sequences using a language model and integrates them with visual features for video modeling. Additionally, we introduce a contextualized sequence transduction loss that considers the contextual information of gloss sequences during label prediction, without assuming independence between the glosses. Our approach achieves state-of-the-art results on three widely used sign language recognition datasets: Phoenix-2014, Phoenix-2014-T, and CSL-Daily. On CSL-Daily, we achieve a significant improvement of 4.9% in Word Error Rate compared to the best published results.