Currently, text-to-image synthesis is popular among researchers and the general public. However, the security of these models has been overlooked. Many text-guided image generation models rely on pre-trained text encoders from external sources, assuming that these models will behave as expected. Unfortunately, this is not always the case. In this study, we introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a significant tampering risk. Our attacks involve making slight alterations to an encoder, making it difficult to detect any suspicious behavior in image generations with clean prompts. By inserting a single character trigger into the prompt, such as a non-Latin character or emoji, the adversary can manipulate the model to generate images with predefined attributes or follow a hidden, potentially malicious description. We provide empirical evidence of the effectiveness of our attacks on StableDiffusion and emphasize that the injection process of a single backdoor can be completed in less than two minutes. Additionally, our approach can also be used to make an encoder forget phrases related to certain concepts, improving the safety of image generation. Interested readers can access our source code at https://github.com/LukasStruppek/Rickrolling-the-Artist.