The deployment of large vision-language models on resource-constrained devices and time-sensitive tasks is impractical due to their size and computational requirements. Model distillation, which involves creating smaller models with similar performance to larger models, offers a solution. This study explores the distillation of visual representations from large teacher models into lightweight student models using a smaller dataset. It focuses on the challenging problem of open-vocabulary out-of-distribution (OOD) generalization, which has been overlooked in previous research on model distillation. Two principles are proposed to improve the student models' OOD generalization: (1) better imitation of the teacher's visual representation space and alignment with the teacher's vision-language coherence, and (2) enhancement of the teacher's language representations with informative semantic attributes to effectively differentiate between labels. Several metrics are proposed and experiments are conducted to evaluate the techniques. The results demonstrate significant improvements in zero-shot and few-shot student performance on open-vocabulary OOD classification, validating the effectiveness of the proposed approaches. The code for this study is available at the provided link.