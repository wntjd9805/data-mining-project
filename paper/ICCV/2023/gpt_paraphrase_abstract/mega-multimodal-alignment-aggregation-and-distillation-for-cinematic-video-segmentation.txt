Prior studies have examined the segmentation of cinematic videos into scenes and narrative acts, but have overlooked the crucial task of aligning and merging multiple modalities to effectively process long-form videos (> 60min). This paper introduces Multimodal alignmEnt aGgregation and distillAtion (MEGA) as a solution for segmenting cinematic long videos. MEGA addresses this challenge by leveraging various media modalities, coarsely aligning inputs of different lengths and modalities using alignment positional encoding. To maintain temporal synchronization while reducing computation, an enhanced bottleneck fusion layer is introduced, which utilizes temporal alignment. Moreover, MEGA employs a novel contrastive loss to synchronize and transfer labels across modalities, allowing for act segmentation based on labeled synopsis sentences on video shots. Experimental results demonstrate that MEGA outperforms state-of-the-art methods on the MovieNet dataset for scene segmentation (with an improvement in Average Precision of +1.19%) and on the TRI-POD dataset for act segmentation (with a Total Agreement improvement of +5.51%).