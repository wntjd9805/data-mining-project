3D pose transfer is a difficult task that involves transferring the pose of one object onto another while preserving the identity of the target object. Existing methods often rely on keypoint annotations to establish correspondence between the source and target objects. Some recent methods allow for end-to-end learning of correspondence but require the desired final output as ground truth for supervision. Unsupervised methods have also been proposed, but they still require ground truth correspondence between the source and target inputs. In this study, we propose a new self-supervised framework for 3D pose transfer that can be trained without any correspondence labels in unsupervised, semi-supervised, or fully supervised settings. Our framework incorporates two contrastive learning constraints in the latent space: a mesh-level loss for capturing global patterns such as pose and identity, and a point-level loss for distinguishing local semantics. We demonstrate that our method achieves state-of-the-art results in supervised 3D pose transfer and comparable results in unsupervised and semi-supervised settings. Furthermore, our approach is applicable to unseen human and animal data with complex topologies.