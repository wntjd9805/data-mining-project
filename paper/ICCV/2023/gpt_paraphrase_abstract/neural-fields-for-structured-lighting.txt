We introduce a method that combines neural radiance fields and structured light imaging to improve image formation. Existing neural models that rely on depth sensors to capture scene geometry can be prone to errors or failures. Instead of relying on processed depth maps, our approach explicitly models the raw structured light images. This allows for the estimation of accurate depth maps, even for objects with complex material properties. Additionally, the raw structured light images provide radiometric cues that enable the prediction of surface normals and the decomposition of scene appearance into direct, indirect, and ambient components. We evaluate our method on various real and synthetic scenes and successfully decompose scenes into their constituent components for novel views.