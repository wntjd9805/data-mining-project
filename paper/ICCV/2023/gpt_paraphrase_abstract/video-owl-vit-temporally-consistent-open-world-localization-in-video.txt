We propose a method to adapt pretrained open-world image models for object localization in videos. Understanding the open visual world is important for various vision tasks. While contrastive pre-training has shown significant improvements for image-level tasks, it is more challenging to apply pretrained models for structured tasks like object localization, especially in videos with limited task-specific data. To address this, we extend the OWL-ViT open-vocabulary detection model by adding a transformer decoder for video adaptation. The decoder recurrently propagates object representations through time by using output tokens from one frame as object queries for the next. Our model is trainable on video data and provides improved temporal consistency compared to tracking-by-detection methods, while retaining the open-world capabilities of the backbone detector. We evaluate our model on the TAO-OW benchmark and demonstrate successful transfer of open-world capabilities learned from large-scale image-text pretraining to diverse video-based open-world localization.