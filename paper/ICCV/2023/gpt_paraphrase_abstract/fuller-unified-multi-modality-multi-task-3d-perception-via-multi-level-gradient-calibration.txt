In the context of 3D autonomous driving, there is a growing interest in multi-modality fusion and multi-task learning to enhance prediction accuracy and manage computational resources. However, simply extending existing frameworks to multi-modality multi-task learning is ineffective and can lead to biased results and conflicts between tasks. Previous approaches have manually coordinated the learning process using empirical knowledge, but this may not yield optimal results. To address this issue, we propose a new and straightforward learning framework that calibrates gradients at multiple levels across tasks and modalities during optimization. Specifically, we calibrate the gradients produced by task heads at the last layer of the shared backbone to alleviate task conflicts. Additionally, we calibrate the magnitudes of these calibrated gradients before propagating them to the modality branches of the backbone, ensuring that downstream tasks give equal attention to different modalities. Our experiments on the nuScenes benchmark dataset demonstrate the effectiveness of our method, with significant improvements in map segmentation and 3D detection accuracy. This advancement contributes to the application of multi-modality fusion and multi-task learning in the field of 3D autonomous driving. We also explore the connections between modalities and tasks.