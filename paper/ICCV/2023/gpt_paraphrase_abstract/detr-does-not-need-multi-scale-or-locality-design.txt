This paper introduces an enhanced version of the DETR detector that maintains a simple design by utilizing a single-scale feature map and global cross-attention calculations without specific locality constraints. This is in contrast to previous DETR-based detectors that reintroduce architectural biases of multi-scale and locality in the decoder. The authors demonstrate that two straightforward techniques are surprisingly effective in compensating for the absence of multi-scale feature maps and locality constraints. The first technique involves incorporating a box-to-pixel relative position bias term into the cross-attention formulation. This term guides each query to focus on the corresponding object region while also allowing for encoding flexibility. The second technique involves using masked image modeling (MIM)-based backbone pre-training, which helps the model learn representations with precise localization abilities and is crucial for addressing the reliance on multi-scale feature maps. By incorporating these techniques and leveraging advancements in training and problem formulation, the improved "plain" DETR detector achieves exceptional improvements over the original DETR. When pre-trained on the Object365 dataset with a Swin-L backbone, it achieves an accuracy of 63.9 mAP, which is highly competitive with state-of-the-art detectors that heavily depend on multi-scale feature maps and region-based feature extraction. The code for the improved DETR is available at https://github.com/impiga/Plain-DETR.