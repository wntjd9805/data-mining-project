We introduce FerKD, an innovative and efficient knowledge distillation framework that incorporates partial soft-hard label adaptation and a region-calibration mechanism. Our approach is based on the observation that standard data augmentations transform inputs into diverse conditions, including easy positives, hard positives, and hard negatives. In traditional distillation frameworks, these transformed samples are treated equally based on their predictive probabilities from pretrained teacher models. However, relying solely on these predictions neglects the reliability of the soft label predictions. To address this, we propose a new scheme that calibrates the less-confident regions by using softened hard groundtruth labels as context. Our approach involves the processes of mining hard regions and calibration. Empirical results demonstrate that this method significantly improves convergence speed and final accuracy. Furthermore, we discover that a consistent mixing strategy can stabilize the distributions of soft supervision by mixing similar regions within the same image. We introduce a stabilized SelfMix augmentation that reduces variation in the mixed images and corresponding soft labels. FerKD is a well-designed learning system that eliminates several heuristics and hyperparameters from the previous FKD solution. Importantly, it achieves remarkable improvements on ImageNet-1K and downstream tasks. For example, FerKD outperforms FKD and FunMatch by significant margins, achieving 81.2% on ImageNet-1K with ResNet-50. With better pre-trained weights and larger architectures, our finetuned ViT-G14 achieves 89.9%. Our code is available at the following GitHub link: https://github.com/szq0214/FKD/tree/main/FerKD.