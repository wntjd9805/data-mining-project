Recent studies have relied heavily on dense annotations for detecting dynamic facial action units (AUs), but manual annotations are time-consuming and costly. Existing semi-supervised learning methods overlook the consistency, extensibility, and adaptability of structural knowledge across spatial-temporal domains. Additionally, their reliance on offline design and excessive parameters hampers learning efficiency. To address these issues, we propose a lightweight and online semi-supervised framework called Knowledge-Spreader (KS) to learn AU dynamics with sparse annotations. By formulating semi-supervised learning as a Progressive Knowledge Distillation (PKD) problem, we aim to infer cross-domain information, particularly from spatial to temporal domains, by consolidating knowledge granularity within a Teacher-Students Network. KS uses sparsely annotated key-frames to learn AU dependencies as privileged knowledge. The model then spreads the learned knowledge to unlabeled neighbors by applying knowledge distillation and pseudo-labeling, completing the temporal information as expanded knowledge. We term this progressive knowledge distillation "Knowledge Spreading," enabling our model to learn spatial-temporal knowledge from video clips with only one label. Extensive experiments demonstrate that KS achieves competitive performance compared to state-of-the-art methods using only 2% labels on BP4D and 5% labels on DISFA datasets. We also tested KS on our newly developed large-scale comprehensive emotion database BP4D++, which addresses the scarcity of annotations and identities by providing a considerable number of well-synchronized and aligned sensor modalities.