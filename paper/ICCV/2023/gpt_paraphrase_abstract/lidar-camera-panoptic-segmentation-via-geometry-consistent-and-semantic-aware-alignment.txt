We introduce LCPS, the first LiDAR-Camera Panoptic Segmentation network for the challenging task of 3D panoptic segmentation. This task requires both semantic segmentation and instance segmentation. While LiDAR data provides valuable information, we believe that images can complement it with rich texture, color, and discriminative details for improved performance. However, fusing LiDAR and camera data remains a difficult problem. Our approach consists of three stages of LiDAR-Camera fusion: 1) The Asynchronous Compensation Pixel Alignment (ACPA) module addresses coordinate misalignment caused by asynchronous problems between sensors.2) The Semantic-Aware Region Alignment (SARA) module extends point-pixel mapping to semantic relationships, allowing one-to-many mappings.3) The Point-to-Voxel feature Propagation (PVP) module integrates geometric and semantic fusion information for the entire point cloud.Our fusion strategy enhances the performance of the LiDAR-only baseline by approximately 6.9% in terms of PQ on the NuScenes dataset. We validate the effectiveness of our framework through extensive quantitative and qualitative experiments. The code for our framework will be made available at https://github.com/zhangzw12319/lcps.git.