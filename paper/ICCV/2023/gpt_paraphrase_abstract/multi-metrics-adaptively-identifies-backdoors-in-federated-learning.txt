Federated learning (FL) is a decentralized and privacy-preserving approach that is susceptible to backdoor attacks, where the resulting model is manipulated by adversaries. Existing defense methods based on statistical differences are only effective against specific attacks, especially when the malicious gradients are similar to benign ones or the data is non-independent and identically distributed (non-IID). This paper reevaluates distance-based defense methods and finds that Euclidean distance loses significance in high dimensions and a single metric cannot identify diverse characteristics of malicious gradients. To address this, the paper proposes a defense strategy that uses multiple metrics and dynamic weighting to adaptively identify backdoors. The defense does not rely on predefined assumptions about attack settings or data distributions and has minimal impact on benign performance. Comprehensive experiments on different datasets and attack settings show that the proposed method achieves the best defensive performance, with the lowest backdoor accuracy of 3.06% under the challenging Edge-case PGD attack. The experiments also demonstrate the method's adaptability to different non-IID degrees without compromising benign performance.