Gaze target detection involves predicting where a person is looking in an image and the likelihood that their gaze is outside of the scene. Previous approaches focused on regressing a heatmap of the gaze location but neglected to establish connections between individuals and the objects they are looking at. To address this, this study introduces a Transformer-based architecture that automatically detects objects, including heads, in the scene and establishes associations between each head and the gazed object. This approach provides a comprehensive and explainable analysis of gaze, including the gaze target area, gaze pixel point, the class, and the image location of the gazed object. The proposed method achieves state-of-the-art results on various evaluation metrics, including a 2.91% increase in AUC, a 50% reduction in gaze distance, and a 9% improvement in out-of-frame average precision for gaze target detection. Additionally, it achieves an 11-13% improvement in average precision for the classification and localization of the gazed objects. The code for this method is publicly available.