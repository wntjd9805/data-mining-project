We propose a new method called LoLep for accurately representing scenes and generating improved novel views from a single RGB image. The challenge lies in regressing appropriate plane locations without depth information. To overcome this, we divide the disparity space into bins and use a disparity sampler to regress local offsets for multiple planes within each bin. However, using only this sampler leads to non-convergence, so we introduce two optimizing strategies that consider different disparity distributions in datasets. We also propose an occlusion-aware reprojection loss as a simple yet effective geometric supervision technique. Additionally, we incorporate a self-attention mechanism to enhance occlusion inference and introduce a Block-Sampling Self-Attention (BS-SA) module to address the issue of applying self-attention to large feature maps. Our approach achieves state-of-the-art results on various datasets, outperforming MINE with an LPIPS reduction of 4.8% to 9.0% and an RV reduction of 74.9% to 83.5%. We also demonstrate the benefits of our method on real-world images.