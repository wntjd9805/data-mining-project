Point cloud registration is a crucial task in computer vision, involving the estimation of the rigid transformation between two unaligned scans. While previous approaches have focused on supervised registration, these methods have practical limitations. In recent years, the use of inexpensive RGB-D sensors has enabled the development of unsupervised registration techniques. However, most existing unsupervised methods employ a cascaded design or unidirectional fusion of RGB-D data, failing to fully exploit the complementary information present in the data. To address this limitation, we propose a novel network that implements multi-scale bidirectional fusion between RGB images and point clouds derived from depth images. This bidirectional fusion enables the effective leveraging of visual and geometric features at multiple scales, resulting in more distinctive deep features for correspondence estimation and ultimately improving the accuracy of our registration approach. Through extensive experiments on benchmark datasets such as ScanNet and 3DMatch, we demonstrate that our method achieves state-of-the-art performance. The code for our approach will be made publicly available at https://github.com/phdymz/PointMBF.