We present a technique for modifying NeRF scenes using textual instructions. Our approach involves using an image-conditioned diffusion model called InstructPix2Pix to edit input images while optimizing the underlying scene. This results in a refined 3D scene that adheres to the specified editing instructions. We show that our method can successfully edit real-world scenes on a large scale and achieve more realistic and targeted edits compared to previous approaches. For visual demonstrations of our results, please visit our project website at https://instruct-nerf2nerf.github.io.