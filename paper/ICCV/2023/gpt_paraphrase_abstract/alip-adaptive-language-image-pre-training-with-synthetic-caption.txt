The Contrastive Language-Image Pre-training (CLIP) method has improved the performance of vision-language tasks by using a large dataset of image-text pairs from the web. However, the presence of noise and unmatched pairs in this web data can negatively impact the learning process. To address this issue, we use the OFA model to generate synthetic captions that focus on the image content. These captions provide additional useful information for pre-training. We then propose the Adaptive Language-Image Pre-training (ALIP) model, which combines supervision from both raw text and synthetic captions. The Language Consistency Gate (LCG) and Description Consistency Gate (DCG) dynamically adjust the weights of samples and image-text/caption pairs during training. Additionally, the adaptive contrastive loss reduces the impact of noise data and improves the efficiency of pre-training. Experimental results demonstrate that ALIP achieves state-of-the-art performance on various downstream tasks, such as image-text retrieval and linear probe. To facilitate future research, we have released the code and pre-trained models on https://github.com/deepglint/ALIP.