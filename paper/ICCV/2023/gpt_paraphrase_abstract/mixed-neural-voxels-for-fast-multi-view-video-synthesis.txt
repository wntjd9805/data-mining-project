Synthesizing high-quality videos from real-world multi-view input is a difficult task due to the complexities of real-world environments and dynamic movements. Prior research using neural radiance fields has shown promising results in reconstructing dynamic scenes, but training these models on real-world scenes is time-consuming. In this study, we propose a new method called MixVoxels that efficiently represents dynamic scenes, allowing for faster training and rendering. MixVoxels represents dynamic scenes as a combination of static and dynamic voxels and processes them using different networks. By using a lightweight model to compute static voxels, which are dominant in many daily scenes, we significantly reduce the computational load. To differentiate between static and dynamic voxels, we introduce a novel variation field that estimates the temporal variance of each voxel. For dynamic representations, we design an inner product time query method that efficiently queries multiple time steps, crucial for capturing high-dynamic movements. With just 15 minutes of training on 300-frame videos, MixVoxels achieves higher peak signal-to-noise ratio (PSNR) compared to previous methods. In terms of rendering, MixVoxels can generate a novel view video at 1K resolution and 37 frames per second (fps). The source code and trained models are available at https://github.com/fengres/mixvoxels.