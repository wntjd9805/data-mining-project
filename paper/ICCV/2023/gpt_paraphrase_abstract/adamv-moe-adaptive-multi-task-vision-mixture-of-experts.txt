The use of Sparsely activated Mixture-of-Experts (MoE) has shown promise in multi-task learning (MTL). Instead of combining all tasks into one model, MoE separates the parameter space and utilizes only the relevant model pieces for each task, leading to more stable training and efficient inference. However, current MoE approaches use a fixed network capacity for all tasks, which can result in overfitting simple tasks or underfitting challenging scenarios. To address this issue, we propose an adaptive MoE framework called AdaMV-MoE for multi-task vision recognition. AdaMV-MoE automatically determines the number of activated experts for each task based on training dynamics, eliminating the need for manual tuning of model size. We evaluate our proposal on ImageNet classification and COCO object detection & instance segmentation, which are known to be difficult to learn together due to their differences. Extensive experiments using various vision transformers demonstrate that AdaMV-MoE outperforms MTL with a shared backbone and the recent state-of-the-art MoE approach. The code for AdaMV-MoE is available online at the provided GitHub link.