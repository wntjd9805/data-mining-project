A variety of widely used pre-trained models signify a significant advancement in artificial intelligence. However, the existing methods for fine-tuning these models are insufficient in addressing the negative transfer effects caused by rare and spuriously correlated features. To overcome this limitation, we introduce a new approach called Concept-Tuning, which refines feature representations at the patch level, with each patch encoding a specific concept. Concept-Tuning mitigates the impact of rare and spuriously correlated features by maximizing the mutual information between examples within the same category and applying front-door adjustment through attention neural networks. Our experimental results consistently demonstrate that Concept-Tuning outperforms previous fine-tuning methods across multiple datasets, pre-training strategies, network architectures, and sample sizes. The improvements achieved by Concept-Tuning range from 0% to 4.76%.