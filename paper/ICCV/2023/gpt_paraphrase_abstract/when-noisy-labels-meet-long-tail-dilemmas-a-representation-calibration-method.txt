Large-scale datasets in real-world scenarios often suffer from noisy labels and class imbalance, which negatively impact the generalization of trained models. This paper aims to address the problem of learning with noisy labels on long-tailed data by proposing a method called representation calibration (RCAL). Unlike previous approaches that rely on impractical assumptions, RCAL leverages unsupervised contrastive learning to extract representations and assumes that, in the absence of incorrect labeling and class imbalance, the representations of instances in each class follow a multivariate Gaussian distribution. Based on this assumption, RCAL recovers the underlying representation distributions from the polluted ones caused by mislabeled and class-imbalanced data. It then samples additional data points from the recovered distributions to improve generalization. Additionally, by incorporating representation learning with contrastive learning, the robustness of the representations enhances the performance of the classifier during training. The paper provides theoretical insights into the effectiveness of the proposed representation calibration method and validates its superiority through experiments on various benchmarks.