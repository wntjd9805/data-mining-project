We introduce CROSSLOC3D, an innovative approach to 3D place recognition that addresses the challenge of matching point sets captured from different sources. These sources may vary in terms of accuracy, distance, and perspective. Our method tackles the representation gap between points from different sources by utilizing multi-grained features and selecting convolution kernel sizes that capture the most prominent features. Inspired by diffusion models, we employ an iterative refinement process that gradually brings the embedding spaces of different sources closer to a single canonical space, facilitating better metric learning. Additionally, we present CS-CAMPUS3D, a novel dataset containing aerial and ground LiDAR scans, which exhibits representation gaps, variations in point density, viewpoint differences, and noise patterns. The results demonstrate that our CROSSLOC3D algorithm outperforms existing methods, achieving an improvement of 4.74% - 15.37% in terms of top 1 average recall on the CS-CAMPUS3D benchmark. Furthermore, our algorithm achieves performance comparable to state-of-the-art 3D place recognition methods on the Oxford RobotCar dataset. The code and CS-CAMPUS3D benchmark can be accessed on github.com/rayguan97/crossloc3d.