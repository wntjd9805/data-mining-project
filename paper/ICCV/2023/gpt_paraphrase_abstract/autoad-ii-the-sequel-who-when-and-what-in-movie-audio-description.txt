Audio Description (AD) involves creating descriptions of visual content for visually impaired audiences. It is particularly challenging for movies as the descriptions must coincide with pauses in dialogue, identify characters by name, and enhance comprehension of the storyline. In order to address these challenges, we propose a novel approach for automatically generating movie AD. Our model utilizes CLIP visual features of frames, the cast list, and the temporal locations of speech to answer the 'who', 'when', and 'what' questions.   To address the 'who' question, we introduce a character bank that includes the character's name, the actor who played the role, and a CLIP feature of their face. This character bank improves the accuracy of character identification in the generated AD.   For the 'when' question, we explore different models to determine whether an AD should be generated for a specific time interval. These models analyze the visual content of the interval and its neighboring frames to make this decision.  For the 'what' question, we develop a new vision-language model that incorporates the character proposals from the character bank. This model utilizes cross-attention to consider the visual features and generates AD text. We demonstrate that our model outperforms previous architectures for AD text generation through a fair comparison.  In summary, our proposed model for automatically generating movie AD combines CLIP visual features, the cast list, and temporal speech locations to address the challenges of character identification, determining when AD should be generated, and generating accurate AD text.