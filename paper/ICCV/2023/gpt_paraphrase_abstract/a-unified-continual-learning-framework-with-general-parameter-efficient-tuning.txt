The "pre-training â†’ downstream adaptation" approach has advantages and challenges for Continual Learning. Current state-of-the-art is achieved through Parameter-Efficient-Tuning (PET) adaptation, but it is limited to Transformers and only explores prompt. This paper introduces a unified CL framework called Learning-Accumulation-Ensemble (LAE), which incorporates PET methods such as Adapter, LoRA, or Prefix to adapt pre-trained models to downstream tasks with fewer parameters and resources. LAE has three novel designs: 1) Learning, where the pre-trained model tunes an online PET module for the new task, along with adaptation speed calibration; 2) Accumulation, where task-specific knowledge learned by the online PET module is accumulated into an offline PET module through momentum update; 3) Ensemble, where two experts with online/offline PET modules are constructed for prediction ensemble during inference. LAE is compatible with various PET methods and shows strong CL capability. For instance, LAE with Adapter PET outperforms prior state-of-the-art by 1.3% and 3.6% in last-incremental accuracy on CIFAR100 and ImageNet-R datasets, respectively. The code is available at https://github.com/gqk/LAE.