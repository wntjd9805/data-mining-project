Accurate depth estimation from panoramic images is important for various assistive technologies, but it faces limitations in terms of performance and ambiguity. To address these challenges, we propose a self-supervised approach that utilizes the holistic view provided by panoramas. We construct a 3D point cloud from depth predictions and generate synthetic panoramas to fine-tune the network with geometric consistency. By minimizing the discrepancy between 3D structures estimated from synthetic images, we enhance performance in robot navigation and map-free localization. Our calibration method broadens the applicability of panorama-based machine vision systems in different external conditions.