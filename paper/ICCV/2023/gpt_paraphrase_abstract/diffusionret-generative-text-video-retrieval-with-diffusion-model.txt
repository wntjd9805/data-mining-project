Current text-video retrieval models focus on maximizing the conditional likelihood and overlook the underlying data distribution, making it difficult to identify out-of-distribution data. To address this, we propose a generative approach that models the correlation between text and video as their joint probability. Our diffusion-based framework, Diffusion-Ret, gradually generates the joint distribution from noise. During training, DiffusionRet is optimized from both generative and discriminative perspectives, leveraging the strengths of both methods. We conducted extensive experiments on various benchmarks, demonstrating the superior performance of our method. Remarkably, DiffusionRet also performs well in out-domain retrieval settings. This work provides fundamental insights into related fields and the code is available at https://github.com/jpthu17/DiffusionRet.