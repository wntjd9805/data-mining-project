Recent advancements in Neural Radiance Fields (NeRF) have made it possible to reconstruct high-quality 3D faces and synthesize new views. However, existing methods for manipulating these reconstructed faces rely on labor-intensive tasks like providing semantic masks and manually searching for attributes, making them unsuitable for non-expert users. In contrast, our approach aims to enable face manipulation with just a single text input. We achieve this by training a scene manipulator called a latent code-conditional deformable NeRF, which can control face deformations using a latent code. However, using a single latent code to represent scene deformations is not ideal for capturing local deformations observed in different instances. To address this, we propose a Position-conditional Anchor Compositor (PAC) that learns to represent manipulated scenes using spatially varying latent codes. We then optimize the renderings of these scenes with the scene manipulator to have a high cosine similarity with a target text in the CLIP embedding space, enabling text-driven manipulation. To the best of our knowledge, our approach is the first to tackle text-driven manipulation of NeRF-reconstructed faces. Extensive experiments, comparisons, and analyses demonstrate the effectiveness of our approach.