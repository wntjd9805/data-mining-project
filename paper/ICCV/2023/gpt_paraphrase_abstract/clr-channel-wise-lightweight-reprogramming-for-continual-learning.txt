Continual learning is the process of continuously acquiring knowledge over multiple tasks, similar to how humans learn. However, a major challenge in this process is the occurrence of catastrophic forgetting, where previously learned tasks are forgotten after learning new ones. To address this issue, we propose a method called Channel-wise Lightweight Reprogramming (CLR) for convolutional neural networks (CNNs). CLR enables a CNN model trained on an old task to be "reprogrammed" to solve a new task using a lightweight and inexpensive reprogramming parameter.  CLR helps achieve a better balance between stability and plasticity in solving continual learning problems. To maintain stability and retain the ability to perform previous tasks, we use a task-agnostic immutable parameter set as a shared "anchor." We then introduce task-specific lightweight reprogramming parameters to reinterpret the outputs of the immutable parts, allowing for the integration of new knowledge. We only train the lightweight reprogramming parameters for each new task, making them exclusive to each task and immune to catastrophic forgetting. To minimize the parameter requirement for reprogramming, we focus on adjusting essential kernels and learning channel-wise linear mappings from anchor parameters to task-specific domain knowledge. For general CNNs, the increase in parameters with CLR is less than 0.6% for any new task.  In our experiments, we demonstrate that our method outperforms 13 state-of-the-art continual learning approaches on a challenging sequence of 53 image classification datasets. The code and data for our method are available in the provided link.