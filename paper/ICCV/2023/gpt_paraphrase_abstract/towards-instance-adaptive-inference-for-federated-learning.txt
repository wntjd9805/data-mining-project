Federated learning (FL) is a distributed learning approach where multiple clients collaborate to learn a global model. However, the performance of the global model is often hindered by differences in data distribution among the clients, both between and within clients. This paper introduces a new FL algorithm called FedIns, which addresses the issue of intra-client data heterogeneity by enabling instance-adaptive inference. Instead of using large instance-adaptive models, FedIns utilizes a parameter-efficient fine-tuning method called scale and shift deep features (SSF). Each client trains an SSF pool, which is then aggregated on the server side to maintain low communication costs. For each instance, the best-matched SSF subsets are dynamically selected from the pool and combined to create an adaptive SSF specific to that instance. This approach reduces both intra-client and inter-client heterogeneity. Experimental results demonstrate that FedIns outperforms other state-of-the-art FL algorithms, achieving a 6.64% improvement with less than 15% communication cost on Tiny-ImageNet.