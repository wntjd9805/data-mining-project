There is a belief that generation can aid in understanding visual data. To explore this idea, we reconsider the use of generative pre-training for visual representations, taking into account recent interest in denoising diffusion models. Although pre-training directly with diffusion models does not yield strong representations, we propose a method called DiffMAE. DiffMAE conditions diffusion models on masked input and formulates them as masked autoencoders. Our approach has several advantages: it serves as a strong initialization for recognition tasks, enables high-quality image inpainting, and can be extended to video with excellent classification accuracy. We also conduct a thorough analysis of the advantages and disadvantages of design choices and establish connections between diffusion models and masked autoencoders. For more information, visit our project page.