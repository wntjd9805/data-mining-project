We introduce a method for efficiently creating high-resolution 3D-aware images that maintain consistency across multiple viewpoints. Our approach, called GMNR, utilizes a novel module called α-guided view-dependent representation (α-VdR) to learn view-dependent information. The α-VdR module efficiently computes the view-dependent representation by learning coefficients for viewing direction and position through an α-guided pixel sampling technique. Additionally, we propose a view-consistency loss to ensure photometric similarity across multiple views. The GMNR model can generate high-resolution 3D-aware images that are view-consistent across different camera poses while maintaining computational efficiency during training and inference. Experimental results on three datasets demonstrate the effectiveness of our proposed modules, showing improved generation quality and inference time compared to existing methods. Our GMNR model achieves a frame rate of 17.6 FPS on a single V100 GPU and generates 1024 × 1024 pixel images. The code for our model is available at https://github.com/VIROBO-15/GMNR.