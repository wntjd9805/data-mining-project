We present SMAUG, a efficient pre-training framework for video-language models that addresses the high computation requirements of traditional methods. SMAUG utilizes masked autoencoders as its foundation, but unlike previous approaches that only mask textual inputs, our strategy considers both visual and textual modalities. This leads to better alignment between modalities and reduces pre-training costs. Additionally, we introduce a space-time token sparsification module that selectively chooses important spatial regions and temporal frames for pre-training using contextual information. By combining these techniques, our method achieves competitive performance on text-to-video retrieval and video question answering tasks, while significantly reducing pre-training costs by at least 1.9Ã—. For instance, SMAUG only requires approximately 50 NVIDIA A6000 GPU hours for pre-training, while achieving competitive performance on these tasks across six popular benchmarks.