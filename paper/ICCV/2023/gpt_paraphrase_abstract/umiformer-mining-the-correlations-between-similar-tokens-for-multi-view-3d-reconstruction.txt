In recent years, breakthroughs in video tasks have been achieved through the use of vision transformers and spatial-temporal decoupling for feature extraction. However, multi-view 3D reconstruction faces challenges in inheriting these successes due to the lack of clear associations between unstructured views. Unlike videos, there is no usable prior relationship among the images. To address this issue, we propose a novel transformer network called UMIFormer for Unstructured Multiple Images. UMIFormer utilizes transformer blocks for intra-view encoding and designed blocks for token rectification, which mines correlations between similar tokens from different views to achieve decoupled inter-view encoding. The tokens acquired from different branches are then compressed into a fixed-size compact representation while preserving rich information for reconstruction by leveraging token similarities. Our empirical demonstration on ShapeNet confirms that our decoupled learning method is adaptable for unstructured multiple images. Furthermore, our experiments show that our model outperforms existing state-of-the-art methods by a significant margin. The code for UMIFormer is available at https://github.com/GaryZhu1996/UMIFormer.