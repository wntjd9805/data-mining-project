The performance of vision-language models like CLIP can be enhanced by incorporating semantic knowledge from large language models (LLMs) like GPT-3. This study examines this behavior and proposes WaffleCLIP, a framework for zero-shot visual classification that replaces LLM-generated descriptors with random character and word descriptors. Without relying on external models, WaffleCLIP achieves similar performance improvements on various visual classification tasks. It serves as a cost-effective alternative and a sanity check for future LLM-based vision-language model extensions. Extensive experiments are conducted to analyze the impact and limitations of introducing additional semantics with LLM-generated descriptors. The study demonstrates that leveraging semantic context through querying LLMs for high-level concepts can effectively resolve potential ambiguities in class names. The code for WaffleCLIP can be accessed at: https://github.com/ExplainableML/WaffleCLIP.