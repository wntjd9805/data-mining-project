Federated learning (FL) is a distributed machine learning framework that maintains privacy by allowing multiple clients to collaborate on training a global model without sharing their local data. However, gradient inversion attacks can compromise client privacy by analyzing the shared model updates. Previous attacks assume knowledge of the local learning rates of clients, but we have discovered that using uniformly distributed random learning rates does not significantly affect the accuracy of the global model. Additionally, personalizing learning rates can address the drift issue caused by non-IID data. We also provide a convergence guarantee for FedAvg with uniformly perturbed learning rates. To counter gradient inversion attacks, we propose a defense called learning rate perturbation (LRP), which involves adding random noise to each client's learning rate. For classification tasks, we modify LRP into ada-LRP by personalizing the expectation of each local learning rate. Experimental results demonstrate that our defenses effectively enhance privacy preservation against existing gradient inversion attacks, with LRP outperforming five baseline defenses against a state-of-the-art attack. Furthermore, our defenses only lead to minor accuracy reductions (less than 0.5%) in the global model, making them suitable for real-world applications.