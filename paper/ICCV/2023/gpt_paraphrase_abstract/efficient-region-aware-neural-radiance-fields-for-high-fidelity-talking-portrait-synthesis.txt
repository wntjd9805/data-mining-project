In this study, we propose ER-NeRF, a new architecture for talking portrait synthesis that combines fast convergence, real-time rendering, and high performance with a small model size. Our approach focuses on leveraging the different contributions of spatial regions to improve talking portrait modeling. We introduce a compact and expressive NeRF-based Tri-Plane Hash Representation to enhance the accuracy of dynamic head reconstruction by eliminating empty spatial regions using three plane hash encoders. To handle speech audio, we introduce a Region Attention Module that uses an attention mechanism to generate region-aware condition features. Unlike existing methods that use MLP-based encoders to implicitly learn cross-modal relations, our attention mechanism explicitly connects audio features to spatial regions, capturing the priors of local motions. Additionally, we propose an Adaptive Pose Encoding technique to optimize the separation of head and torso by mapping the complex head pose transformation into spatial coordinates. Our extensive experiments demonstrate that our method produces high-fidelity talking portrait videos with audio-lips synchronization, realistic details, and high efficiency compared to previous methods. The code for our approach is available at https://github.com/Fictionarry/ER-NeRF.