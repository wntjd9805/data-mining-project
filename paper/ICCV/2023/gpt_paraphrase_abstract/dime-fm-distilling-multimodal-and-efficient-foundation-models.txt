Large Vision-Language Foundation Models (VLFM) like CLIP, ALIGN, and Florence have shown impressive performance on various tasks but are challenging to use in practical applications due to their size, latency, and fixed architectures. However, training smaller custom VLFMs for resource-limited applications using public and smaller-scale data has proven to be difficult. To address this, we propose a new distillation mechanism called DIME-FM that enables the transfer of knowledge from large VLFMs to smaller, customized models using a small amount of inexpensive, unpaired images and sentences.In our study, we demonstrate the effectiveness of DIME-FM by transferring knowledge from the pre-trained CLIP-ViT-L/14 model to a ViT-B/32 model using only 40 million public images and 28.4 million unpaired public sentences. The resulting model, named "Distill-ViT-B/32," performs on par with the CLIP-ViT-B/32 model that was pre-trained on a private WiT dataset consisting of 400 million image-text pairs. Distill-ViT-B/32 achieves similar results in terms of zero-shot and linear-probing performance on both ImageNet and the ELEVATER benchmarks, which include 20 image classification tasks. Furthermore, it exhibits comparable robustness when evaluated on five datasets with natural distribution shifts from ImageNet.For more information and access to the code, please visit our project page.