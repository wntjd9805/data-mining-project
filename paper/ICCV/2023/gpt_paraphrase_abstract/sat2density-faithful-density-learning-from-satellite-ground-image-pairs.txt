This paper aims to create a precise 3D representation of satellite images by using pairs of satellite-ground images. The main focus is on the challenging task of generating 3D-aware ground-views from a satellite image. Inspired by the density field representation used in volumetric neural rendering, a new approach called Sat2Density is proposed. Sat2Density leverages the characteristics of ground-view panoramas for both sky and non-sky regions to learn accurate density fields of 3D scenes from a geometric standpoint. Unlike other methods that require additional depth information during training, Sat2Density can automatically learn precise and faithful 3D geometry using density representation alone, without the need for depth supervision. This advancement significantly enhances the synthesis of ground-view panoramas. Furthermore, this study offers a fresh geometric perspective for understanding the relationship between satellite and ground-view images in 3D space.