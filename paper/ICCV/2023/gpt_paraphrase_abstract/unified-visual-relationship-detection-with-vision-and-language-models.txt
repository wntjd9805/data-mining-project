This research focuses on training a single visual relationship detector that predicts over multiple datasets, even when there are inconsistent taxonomies. This is particularly challenging in visual relationship detection when second-order visual semantics are involved. To overcome this challenge, we propose a novel method called UniVRD, which leverages vision and language models (VLMs) to unify visual relationship detection. VLMs provide well-aligned image and text embeddings, allowing similar relationships to be optimized for semantic unification. Our bottom-up design allows the model to benefit from training with both object detection and visual relationship datasets. Experimental results on human-object interaction detection and scene-graph generation demonstrate the competitive performance of our model. UniVRD achieves a 38.07 mAP on HICO-DET, surpassing the current best bottom-up HOI detector by 14.26 mAP. Importantly, we show that our unified detector performs as well as dataset-specific models in terms of mAP and achieves further improvements with increased model scale. Our code will be publicly available on GitHub.