Federated Learning (FL) is a decentralized approach to machine learning where multiple clients collaborate to train neural networks without sharing their local data, ensuring data privacy. However, real-world FL applications face challenges due to distribution shifts across the local datasets of individual clients. These shifts can affect the aggregation of the global model or lead to convergence towards suboptimal local solutions. While previous efforts have addressed distribution shifts in the label space, the challenge of divergent feature distributions among clients' local data remains relatively unexplored. This challenge can significantly impact the performance of the global model in the FL framework. To address this problem, we propose Federated Representation Augmentation (FRAug). FRAug utilizes a shared embedding generator to capture client consensus and generates synthetic embeddings. These embeddings are then transformed into client-specific representations using a locally optimized RTNet, augmenting the training space of each client. Our empirical evaluation on three public benchmarks and a real-world medical dataset demonstrates the effectiveness of FRAug, surpassing the current state-of-the-art FL methods for feature distribution shifts such as PartialFed and FedBN.