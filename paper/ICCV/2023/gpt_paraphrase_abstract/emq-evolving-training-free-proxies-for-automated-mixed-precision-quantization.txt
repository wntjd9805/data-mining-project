Mixed-Precision Quantization (MQ) offers a trade-off between accuracy and complexity in models. Traditional search methods for optimizing per-layer bit-width configurations in MQ require time-consuming candidate training. Recently, training-free approaches have been introduced to improve search efficiency by utilizing MQ proxies. However, the relationship between these proxies and quantization accuracy is not well understood. To address this gap, we present MQ-Bench-101, a benchmark dataset that includes different bit configurations and quantization results. We find that the existing training-free proxies exhibit weak correlations on MQ-Bench-101. To overcome this, we develop an automatic search framework called Evolving proxies for Mixed-precision Quantization (EMQ) using evolutionary algorithms. We create a comprehensive search space that includes the existing proxies and conduct an evolution search to identify the best correlated MQ proxy. To enhance search efficiency and prevent premature convergence, we employ a diversity-prompting selection strategy and compatibility screening protocol. Our EMQ framework enables the generation of proxies without the need for extensive tuning or expert knowledge. Extensive experiments on ImageNet using various ResNet and MobileNet models demonstrate that EMQ achieves superior performance compared to state-of-the-art mixed-precision methods, while significantly reducing costs. The code for our framework will be made available.