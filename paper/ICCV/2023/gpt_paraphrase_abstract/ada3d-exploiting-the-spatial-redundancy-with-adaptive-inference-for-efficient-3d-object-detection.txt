Voxel-based methods have become the leading approach for 3D object detection in autonomous driving. However, their high computational and memory requirements make them challenging to implement in resource-constrained vehicles. This is largely due to the presence of redundant background points in Lidar point clouds, which leads to spatial redundancy in both 3D voxel and BEV map representations. To address this issue, we propose a new adaptive inference framework called Ada3D. Ada3D effectively reduces spatial redundancy by filtering out redundant input, guided by a lightweight importance predictor and the unique properties of Lidar point clouds. Additionally, we maintain the sparsity of BEV features by introducing Sparsity Preserving Batch Normalization. With Ada3D, we achieve a 40% reduction in 3D voxels and decrease the density of 2D BEV feature maps from 100% to 20% without sacrificing accuracy. Moreover, Ada3D significantly reduces the computational and memory costs of the model, achieving a 5× reduction. It also improves GPU latency by 1.52× / 1.45× and GPU peak memory optimization by 1.5× / 4.5× for the 3D and 2D backbone respectively.