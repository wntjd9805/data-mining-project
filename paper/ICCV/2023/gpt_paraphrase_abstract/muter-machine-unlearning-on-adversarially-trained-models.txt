Machine unlearning is a task that involves removing the impact of certain training data from a trained model when data deletion requests are made. This task aligns with data regulations that emphasize the Right to be Forgotten. While various unlearning methods have been proposed, they primarily focus on standard training models and do not apply to adversarial training models (ATMs). ATMs are popular defenses against adversarial examples but pose challenges due to the bi-level optimization involved in the training process. This complexity complicates the measurement of data influence for deletion, making unlearning from ATMs more difficult than standard model training. To address this, a new approach called MUter is introduced in this paper. MUter employs a closed-form unlearning step based on a total Hessian-related measure of data influence, which avoids mis-capturing the influence associated with the indirect Hessian part. Additionally, MUter reduces computational costs by introducing approximations and conversions to avoid the most computationally demanding aspects of Hessian inversions. Experimental validation on four datasets using linear and neural network models demonstrates the efficiency and effectiveness of MUter.