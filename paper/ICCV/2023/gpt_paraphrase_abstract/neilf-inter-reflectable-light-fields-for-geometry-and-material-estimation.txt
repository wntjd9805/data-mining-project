We introduce a new differentiable rendering framework that can estimate the geometry, material, and lighting of a scene from multiple images. Unlike previous methods that rely on simplified environment maps or flash-lights, our approach incorporates a neural incident light field and a neural radiance field to model the lighting of a static scene. By combining these fields and considering inter-reflections between surfaces, we are able to separate the scene's geometry, material, and lighting in a physically-based manner. Our framework can also be easily applied to other systems using neural radiance fields. Additionally, our method not only decomposes the outgoing radiance into incident lights and surface materials, but also enhances the reconstruction detail of the neural surface. We demonstrate the effectiveness of our approach on various datasets, achieving state-of-the-art results in terms of geometry reconstruction, material estimation, and novel view rendering fidelity.