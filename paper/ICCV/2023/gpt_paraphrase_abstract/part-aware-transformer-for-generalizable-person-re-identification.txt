Domain generalization person re-identification (DG-ReID) is a task that involves training a model on source domains and ensuring that it performs well on unseen domains. While Vision Transformer models generally exhibit better generalization ability than common CNN networks when faced with distribution shifts, Transformer-based ReID models often suffer from overfitting to domain-specific biases due to the supervised learning strategy employed on the source domain. This study observes that while global images of different IDs should possess distinct features, their similar local parts (such as a black backpack) are not constrained by this requirement. Motivated by this observation, the researchers propose a pure Transformer model called Part-aware Transformer for DG-ReID. This model includes a proxy task called Cross-ID Similarity Learning (CSL), which aims to extract local visual information shared by different IDs. By only focusing on the visual similarity of the parts, regardless of the ID labels, this proxy task enables the model to learn generic features and mitigate the impact of domain-specific biases. Additionally, the researchers introduce Part-guided Self-Distillation (PSD) based on the local similarity obtained in CSL to further enhance the generalization of global features. Experimental results demonstrate that their method achieves state-of-the-art performance in most DG ReID scenarios. The code for their approach is available at a designated GitHub repository. The accompanying Figure 1 provides a visual comparison of different Transformers applied to DG ReID, showcasing the superior performance of Vision transformers over CNNs even with fewer parameters. The figure also presents attention maps for the "class token" in both the source domain (MSMT) and the target domain (Market), illustrating that although the attention to discriminative information is limited in the target domain, the proposed ViT backbone with fused attention results from shallow layers helps address this limitation.