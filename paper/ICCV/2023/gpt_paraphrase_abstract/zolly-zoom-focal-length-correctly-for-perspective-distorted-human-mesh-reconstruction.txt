Current 3D human mesh reconstruction methods struggle with calibrating single-view RGB images taken in real-world settings. They often rely on a fixed focal length or estimate it based on the background context, which fails to address the distortion of the torso, limbs, hands, and face caused by perspective camera projection when the camera is close to the subject. These assumptions about focal length can lead to incorrect projection matrices and hinder the reconstruction task. In response, we propose Zolly, the first 3DHMR method specifically designed for perspective-distorted images. Our approach involves analyzing the main cause of perspective distortion, which we identify as the relative position of the human body to the camera center. We introduce a new camera model and a novel 2D representation called the distortion image, which captures the dense distortion scale of the human body. Instead of relying on environmental context, we estimate the distance using distortion scale features. We then combine these features with image features to reconstruct the body mesh. To ensure accurate projection matrices and body positioning, we incorporate both perspective and weak-perspective projection loss. To facilitate research in this area, we introduce the synthetic dataset PDHuman and extend two real-world datasets specifically tailored for perspective-distorted images. Our extensive experiments demonstrate that Zolly outperforms existing methods on both perspective-distorted datasets and the standard benchmark (3DPW). The code and dataset for Zolly will be made available at https://wenjiawang0312.github.io/projects/zolly/.