Data heterogeneity poses a challenge to federated learning (FL) by causing biased classifiers in local models, which hinder performance. Previous attempts to address this issue through classifier calibration have been insufficient in improving the poor feature representations caused by training-time classifier biases. To effectively resolve the classifier bias problem in FL, a thorough understanding of the classifier mechanisms is necessary. Recent advancements in neural collapse have revealed that classifiers and feature prototypes collapse into an optimal structure known as a simplex equiangular tight frame (ETF) under ideal training conditions. Based on this insight, we propose a solution to FL's classifier bias problem by incorporating a synthetic and fixed ETF classifier during training. This optimal classifier structure allows all clients to learn unified and optimal feature representations, even with highly heterogeneous data. We have developed several modules to better adapt the ETF structure in FL, achieving both high generalization and personalization. Extensive experiments demonstrate that our approach outperforms existing methods on benchmark datasets such as CIFAR-10, CIFAR-100, and Tiny-ImageNet. The source code for our method is available at https://github.com/ZexiLee/ICCV-2023-FedETF.