This study introduces Environment Agnostic Reinforcement Learning (EAR) as a compact framework for generalizing vision-based deep reinforcement learning (RL) to handle changes in dynamic environments. The challenge lies in adapting an agent to unfamiliar environments due to the high-dimensional nature of visual input. To address this, the proposed EAR extracts Environment-agnostic features (EAFs) using three novel objectives: feature factorization, reconstruction, and episode-aware state shifting. By focusing only on vital features, policy learning is achieved. EAR is a simple, single-stage method with low model complexity and fast inference time, ensuring high reproducibility. It achieves state-of-the-art performance in the DeepMind ControlSuite and DrawerWorld benchmarks. The code for EAR is available at: https://github.com/doihye/EAR.