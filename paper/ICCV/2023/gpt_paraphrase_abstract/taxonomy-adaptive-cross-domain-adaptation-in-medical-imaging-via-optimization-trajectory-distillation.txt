The success of automated medical image analysis relies on having large-scale training sets that are annotated by experts. Unsupervised domain adaptation (UDA) has emerged as a promising approach to address the challenge of limited labeled data. However, existing UDA methods assume a closed-set adaptation setting, where the source and target domains have identical label sets, which is not realistic in clinical practice where new classes often exist across datasets due to taxonomic inconsistency. Although some methods have been proposed to handle both domain shifts and incoherent label sets, they fail to consider the common characteristics of these two issues and do not take into account the learning dynamics during network training.In this study, we introduce a novel approach called optimization trajectory distillation to tackle these technical challenges from a new perspective. Our approach leverages the low-rank nature of the gradient space and incorporates a dual-stream distillation algorithm to regularize the learning dynamics of insufficiently annotated domains and classes. We use external guidance from reliable sources to guide the learning process. By addressing the issue of inadequate navigation during network optimization, which is a major obstacle in the taxonomy adaptive cross-domain adaptation scenario, our approach overcomes this limitation.We extensively evaluate our proposed method on various tasks that have clinical and open-world significance. The results demonstrate the effectiveness and improvements of our approach compared to previous methods. To facilitate reproducibility, we have made the code available at https://github.com/camwew/TADA-MI.