Developing agents that can understand and follow text instructions in their surroundings is a challenging task, especially when it comes to locating objects in the real world. This capability is essential for glass-devices or autonomous robots. However, existing datasets for referring expression comprehension tasks primarily rely on web-crawled data and do not accurately represent the diverse structures found in the real world. To address this limitation, a large-scale egocentric video dataset called Ego4D was introduced. Ego4D covers various real-world scenes from around the world, including indoor and outdoor situations such as shopping, cooking, walking, talking, and manufacturing. Building upon this dataset, we created a comprehensive video-based referring expression comprehension dataset called RefEgo. This dataset consists of over 12k video clips and 41 hours of annotation for referring expression comprehension. In our experiments, we combined state-of-the-art 2D referring expression comprehension models with an object tracking algorithm to track referred objects in videos, even under challenging conditions such as when the object goes out-of-frame or when multiple similar objects are present.