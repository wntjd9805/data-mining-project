One commonly used method in deep metric learning is a convolutional neural network followed by global average pooling (GAP). Although simple, GAP is highly effective in aggregating information. The reason for its effectiveness could be that each feature vector represents a different semantic entity, and GAP can be seen as a convex combination of these entities. Building on this idea, we introduce a learnable generalized sum pooling method (GSP) that improves upon GAP in two ways. Firstly, GSP allows for the selection of a subset of semantic entities, enabling the model to ignore irrelevant information. Secondly, GSP learns the weights corresponding to the importance of each entity. We formulate this as an entropy-smoothed optimal transport problem, which is a strict generalization of GAP. We demonstrate that this optimization problem has analytical gradients, making it suitable for direct learning. To aid in the learning process of GSP, we propose a zero-shot loss. Extensive evaluations on four popular metric learning benchmarks show the effectiveness of our method. The code for our GSP-DML Framework is available for use.