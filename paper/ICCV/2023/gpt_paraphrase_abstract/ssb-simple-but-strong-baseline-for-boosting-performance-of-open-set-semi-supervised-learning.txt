Semi-supervised learning (SSL) methods are effective in utilizing unlabeled data to improve model generalization. However, these models often struggle in open-set scenarios, where the unlabeled data includes outliers from novel categories not present in the labeled set. In this study, we examine the challenging and realistic open-set SSL setting, aiming to accurately classify inliers while detecting outliers. Initially, the inlier classifier is trained solely on inlier data. However, we discover that incorporating high-confidence pseudo-labeled data, regardless of their inlier or outlier status, significantly enhances inlier classification performance. To prevent adverse effects between inlier classification and outlier detection, we propose using non-linear transformations to separate the features used in the multi-task learning framework. Moreover, we introduce pseudo-negative mining to further improve outlier detection. Combining these three components, we develop a Simple but Strong Baseline (SSB) for open-set SSL. Experimental results demonstrate that SSB outperforms existing methods by a significant margin in terms of both inlier classification and outlier detection performance. The code for SSB will be made available at https://github.com/YUE-FAN/SSB.