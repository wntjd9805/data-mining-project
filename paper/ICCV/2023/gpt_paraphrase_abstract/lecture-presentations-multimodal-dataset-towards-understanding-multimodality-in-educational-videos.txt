This abstract discusses the use of slide presentations in educational videos and the importance of their multimodal nature in effectively transferring knowledge to students. The authors introduce the Lecture Presentations Multimodal (LPM) Dataset, which contains aligned slides and spoken language from over 180 hours of video and 9000+ slides across various subjects. They propose three research tasks to evaluate vision-language models' understanding of multimodal content: figure-to-text retrieval, text-to-figure retrieval, and generation of slide explanations. The authors provide manual annotations and establish baselines for these tasks. Comparing baselines and human student performances, they find that current vision-language models struggle with weak crossmodal alignment, learning novel visual mediums, technical language, and long-range sequences. They introduce PolyViLT, a novel multimodal transformer trained with a multi-instance learning loss, which outperforms current approaches for retrieval. The abstract concludes by highlighting the challenges and opportunities in multimodal understanding of educational presentation videos.