We present a lightweight dynamic local and global self-attention network (DLGSANet) for image super-resolution. Our approach combines the advantages of Transformers with low computational costs. Inspired by Transformers, we introduce a multi-head dynamic local self-attention (MHDLSA) module to efficiently extract local features. Unlike existing Transformers that consider all similarities between queries and keys for feature aggregation, we propose a sparse global self-attention (SparseGSA) module to select the most relevant similarity values. This allows us to better utilize useful global features for image reconstruction. We integrate the MHDLSA and SparseGSA modules into a hybrid dynamic-Transformer block (HDTB) for both local and global feature exploration. To simplify network training, we organize the HDTBs into a residual hybrid dynamic-Transformer group (RHDTG). By embedding the RHDTGs into an end-to-end trainable network, we demonstrate that our method achieves competitive performance with fewer network parameters and lower computational costs compared to state-of-the-art approaches. For more details, please refer to our website: https://neonleexiang.github.io/DLGSANet/.