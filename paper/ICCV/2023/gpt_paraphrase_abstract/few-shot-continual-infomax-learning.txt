This paper introduces the Few-shot Continual Infomax Learning (FCIL) framework, which enables a deep neural network to continually learn new concepts from a sequential stream of few-shot data. FCIL addresses the issue of catastrophic forgetting by utilizing transfer entropy and feature embedding infomax. The feature embedding infomax transfers the encoding capability of the base network to learn the feature embedding of novel classes, while the transfer entropy preserves the relational structure of learned knowledge. The effectiveness of FCIL is demonstrated through evaluations on CIFAR100, miniImageNet, and CUB200 datasets, where it outperforms state-of-the-art methods in the few-shot continual learning task.