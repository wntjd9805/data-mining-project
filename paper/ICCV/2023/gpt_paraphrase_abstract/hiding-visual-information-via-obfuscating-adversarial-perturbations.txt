The increasing leakage and misuse of visual information have raised concerns regarding security and privacy, leading to the development of information protection methods. Existing approaches primarily focus on de-identifying data against deep learning models but fail to adequately protect the inherent visual information. To address this gap, we propose an Adversarial Visual Information Hiding (AVIH) method inspired by Type-I adversarial attacks. AVIH generates obfuscating adversarial perturbations to obscure visual information while ensuring that the hidden objectives are correctly predicted by models. Importantly, our method does not modify the parameters of the applied model, making it adaptable to different scenarios. Experimental results on recognition and classification tasks demonstrate that AVIH effectively hides visual information without significantly impacting model performance. Source code is available at https://github.com/suzhigangssz/AVIH.