Event cameras are a type of camera that record changes in light intensity, allowing them to capture fast-moving objects without motion blur. They have the advantage of operating at a much higher frequency compared to traditional frame-based cameras. Recent studies have focused on training neural networks to predict optical flow from events captured by event cameras. However, these studies have limited the flow prediction to a fixed interval, such as 10 Hz, which does not effectively utilize the fast speed capabilities of event cameras, which can operate up to 3 kHz.This research presents a method for achieving temporally dense flow estimation at 100 Hz using two different types of recurrent networks: Long-short term memory (LSTM) and spiking neural network (SNN). The LSTM model, based on the popular EV-FlowNet, demonstrates the efficiency of the training method by producing optical flow 10 times more frequently compared to existing models. Additionally, the estimated flows have 13% lower errors than predictions from the baseline EV-FlowNet.The SNN model, constructed with leaky integrate and fire neurons, efficiently captures the temporal dynamics of the flow estimation. The inherent recurrent dynamics of SNN result in significant parameter reduction compared to the LSTM model. Furthermore, the event-driven computation of the spiking model contributes to its efficiency.This research was supported by various organizations, including the Center for Brain-inspired Computing (C-BRIC), DARPA, Semiconductor Research Corporation (SRC), National Science Foundation, DoD Vannevar Bush Fellowship, and IARPA MicroE4AI. The code for this research is available at the provided link.