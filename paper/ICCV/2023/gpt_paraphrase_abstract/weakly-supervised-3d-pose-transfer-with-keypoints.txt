The primary difficulties in 3D pose transfer are the lack of paired training data with different characters performing the same pose, the challenge of separating pose and shape information from the target mesh, and the difficulty in applying the transfer to meshes with varying topologies. To address these challenges, we propose a new weakly-supervised keypoint-based framework. Our approach utilizes a topology-agnostic keypoint detector and inverse kinematics to calculate transformations between the source and target meshes. Unlike previous methods, our framework only requires supervision on the keypoints and can be applied to meshes with different topologies. Additionally, it is shape-invariant for the target, allowing extraction of pose-only information from the target meshes without transferring shape information. We also introduce a cycle reconstruction technique for self-supervised pose transfer, eliminating the need for ground truth deformed meshes with the same pose and shape as the target and source. We evaluate our approach on human and animal datasets and achieve superior performance compared to unsupervised methods and comparable performance to fully supervised methods. We also test our approach on the challenging Mixamo dataset to demonstrate its ability to handle meshes with different topologies and complex clothing. Cross-dataset evaluation further confirms the strong generalization ability of our approach. Our source code is available at the following link: https://github.com/jinnan-chen/3D-Pose-Transfer.