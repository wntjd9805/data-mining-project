Unsupervised methods for learning about objects in a scene can divide the scene into separate entities without needing specific information about their location. These methods are promising for reducing the amount of annotation required in multiple-object tracking (MOT) pipelines. However, they have two drawbacks: objects are often split into parts, and they are not consistently tracked over time. In contrast, current state-of-the-art models achieve high accuracy and temporal consistency by using supervised object detection with additional ID labels for association over time. This paper presents a video object-centric model for MOT that addresses these limitations. The model includes an index-merge module that converts object-centric slots into detection outputs and an object memory module that constructs complete object prototypes to handle occlusions. By leveraging object-centric learning, our approach only requires sparse detection labels (0%-6.25%) for object localization and feature binding, eliminating the need for ID labels. Through our self-supervised Expectation-Maximization-inspired loss for object association, we significantly narrow the performance gap between existing object-centric models and fully supervised state-of-the-art methods, surpassing several unsupervised trackers. The code for our approach can be found at https://github.com/amazon-science/object-centric-multiple-object-tracking.