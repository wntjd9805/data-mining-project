In order to create more realistic virtual reality and augmented reality experiences, it is important to be able to accurately control 3D avatars. While current technology only allows for real-time tracking of the head and hands, a neural network trained on large amounts of motion data has the potential to solve this problem. We propose a two-stage framework that can generate accurate and smooth full-body motions using only the tracking signals from the head and hands. Our framework incorporates joint-level features and uses spatial and temporal transformer blocks to capture correlations between joints. We also introduce a set of loss terms to ensure a high level of control over the avatar's movements. Through extensive experiments, we demonstrate that our approach outperforms existing methods in terms of accuracy and smoothness.