Learning 3D shape representation for deformable objects is a significant challenge in computer vision. Current methods often require additional annotations specific to a particular domain, such as skeleton poses for humans or animals, resulting in increased annotation effort and error accumulation. Moreover, these approaches are limited to specific domains. To address these limitations, we propose a novel self-supervised approach that learns a neural implicit shape representation for deformable objects. Our method can represent shapes using a template shape and dense correspondence in 3D, without the need for prior knowledge of skeleton and skinning weight annotations. Instead, our approach only requires a collection of shapes represented in signed distance fields. To handle large deformations, we constrain the learned template shape in the same latent space as the training shapes. We also introduce a new formulation of local rigid constraints that enforce rigid transformations in local regions and address local reflection issues. Additionally, we present a hierarchical rigid constraint to reduce ambiguity caused by joint learning of template shape and correspondences. Extensive experiments demonstrate that our model can effectively represent shapes with large deformations. Furthermore, we show that our shape representation supports two common applications, texture transfer, and shape editing, with competitive performance. The code and models for our approach are available at https://iscas3dv.github.io/deformshape.