Recent advancements in 3D generative models have been significant, but training these models across different domains is challenging and requires a large amount of training data and knowledge of pose distribution. Text-guided domain adaptation methods have been used to adapt the generator to target domains using text prompts, eliminating the need for extensive data collection. However, current text-to-image diffusion models face issues such as shape-pose trade-off, pose bias, and instance bias in the target domain, resulting in inferior 3D shapes, low text-image correspondence, and low intra-domain diversity in generated samples. To address these issues, we propose a novel pipeline called PODIA-3D for pose-preserved text-to-image diffusion-based domain adaptation in 3D generative models. Our approach incorporates a pose-preserved text-to-image diffusion model that can handle significant domain changes with high-level noise. We also introduce specialized-to-general sampling strategies to enhance the details of the generated samples. Furthermore, to overcome the instance bias, we introduce a text-guided debiasing method that improves intra-domain diversity. Our method successfully adapts 3D generators across substantial domain gaps, as demonstrated by qualitative results and a user study. Our approach outperforms existing 3D text-guided domain adaptation methods in terms of text-image correspondence, realism, diversity of rendered images, and sense of depth in the generated 3D shapes.