We present DualMind, a versatile agent designed to address the challenges faced by current methods in decision-making tasks. Unlike existing approaches that suffer from overfitting and rely on task-specific fine-tuning, DualMind utilizes a novel "Dual-phase" training strategy inspired by human learning. Initially, the model learns fundamental knowledge through a self-supervised objective tailored for control tasks. Then, it learns to make decisions in different contexts by imitating behaviors based on given prompts. Notably, DualMind can handle tasks across various domains, scenes, and embodiments with a single set of model weights. It can also execute zero-shot prompting without task-specific fine-tuning. Extensive experiments on MetaWorld and Habitat demonstrate the superior generalizability of DualMind, surpassing other generalist agents by more than 50% and 70% on Habitat and MetaWorld, respectively. Achieving a success rate of 90% on over 30 tasks in MetaWorld, DualMind proves its effectiveness. The source code is available at https://github.com/yunyikristy/DualMind.