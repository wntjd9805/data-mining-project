This paper introduces a new vision Transformer called Scale-Aware Modulation Transformer (SMT) that efficiently handles various downstream tasks by combining convolutional networks and vision Transformers. The SMT incorporates two novel designs: the Multi-Head Mixed Convolution (MHMC) module, which captures multi-scale features and expands the receptive field, and the Scale-Aware Aggregation (SAA) module, which enables information fusion across different heads. These modules enhance convolutional modulation. Additionally, the paper proposes an Evolutionary Hybrid Network (EHN) that effectively simulates the shift from capturing local to global dependencies as the network becomes deeper. Experimental results demonstrate that SMT outperforms existing state-of-the-art models across a wide range of visual tasks. For instance, SMT achieves top-1 accuracies of 82.2% and 84.3% on ImageNet-1K with different computational costs. After pretrained on ImageNet-22K, it achieves top-1 accuracies of 87.1% and 88.1% when fine-tuned with different resolutions. In object detection and semantic segmentation tasks, SMT also outperforms comparable models. The code for SMT is available at https://github.com/AFeng-x/SMT.