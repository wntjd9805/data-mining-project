In recent years, there has been a growing interest in applying stylization techniques to 3D scenes using a reference style image, specifically on neural radiance fields (NeRF). While stylizing NeRF directly ensures consistent appearance across different views, it is challenging to transfer patterns from the style image to different parts of the NeRF scene. To address this, we propose a stylization framework for NeRF based on local style transfer. We employ a hash-grid encoding to learn the embedding of appearance and geometry components, enabling us to control stylization to some extent. Stylization is achieved by optimizing the appearance branch while keeping the geometry branch fixed. To facilitate local style transfer, we introduce a new loss function that utilizes a segmentation network and bipartite matching to establish correspondences between the style image and content images obtained from volume rendering. Our experiments demonstrate that our method produces realistic stylization results in novel view synthesis, while also allowing flexibility in manipulating and customizing region correspondences.