Sequentially learning multiple pretext tasks based on their difficulties is proposed in this paper to enhance the performance of unsupervised video anomaly detection. Instead of concurrently learning multiple pretext tasks, which often leads to sub-optimal solutions and performance drops, the authors suggest starting with easier tasks and gradually moving towards more challenging ones. This approach aims to reduce the difficulties of learning and prevent convergence to sub-optimal solutions. The authors specifically devise a sequential learning order for three commonly used pretext tasks: frame prediction, frame reconstruction, and frame-order classification. Additionally, a new contrastive loss is introduced to improve the discriminative ability of learned representations by separating normal and pseudo-abnormal samples. Extensive experiments conducted on three datasets validate the effectiveness of the proposed method.