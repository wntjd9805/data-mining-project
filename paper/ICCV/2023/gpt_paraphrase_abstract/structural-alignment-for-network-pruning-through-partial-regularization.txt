This paper presents a new method for reducing the computational and storage costs of Convolutional Neural Networks (CNNs) through channel pruning. Existing pruning methods often remove redundant structures directly, resulting in a significant performance loss. To address this issue, the proposed method trains a target sub-network during model training and utilizes it to guide the learning of model weights through partial regularization. This sub-network is efficiently optimized using an architecture generator. Moreover, the paper introduces the proximal gradient for the proposed partial regularization to facilitate the structural alignment process. These designs help reduce the gap between the pruned model and the sub-network, thereby improving pruning performance. Empirical results demonstrate that the sub-network obtained using this method outperforms the one-shot pruning approach. Extensive experiments on CIFAR-10 and ImageNet datasets using ResNets and MobileNet-V2 confirm the state-of-the-art performance achieved by the proposed method.