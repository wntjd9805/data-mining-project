This paper presents a new method for pruning Deep Neural Networks (DNNs) that focuses on minimizing output distortion while adhering to a target pruning ratio constraint. The approach considers the influence of all layers and introduces a layer-adaptive pruning scheme. The authors discover and utilize an important property of output distortion caused by pruning weights on multiple layers, allowing them to formulate the pruning as a combinatorial optimization problem. They efficiently solve this problem using dynamic programming, achieving linear time complexity and making the optimization algorithm fast and feasible for CPUs. Extensive experiments on the ImageNet and CIFAR-10 datasets demonstrate the superiority of this approach compared to existing methods. The proposed method achieves significant improvements in top-1 accuracy for various DNN architectures, such as ResNet-32, VGG-16, and DenseNet-121. On ImageNet, the method outperforms other techniques, achieving higher top-1 accuracy for VGG-16 and ResNet-50. These results showcase the effectiveness and practicality of the proposed approach for enhancing DNN performance through layer-adaptive weight pruning. The code for this method will be available at https://github.com/Akimoto-Cris/RD_VIT_PRUNE.