This study focuses on continuous sign language recognition (CSLR), a task that involves recognizing continuous signs from videos without prior knowledge of the temporal boundaries between consecutive signs. The lack of data poses a significant challenge to CSLR progress. Existing approaches typically train CSLR models on small monolingual corpora, which are much smaller than those used in speech recognition. This study explores the possibility of using multilingual sign language corpora to enhance monolingual CSLR. The research is based on the observation of cross-lingual signs, which are signs from different sign languages that share similar visual signals, such as hand shape and motion. The main idea is to identify cross-lingual signs in one sign language and utilize them as auxiliary training data to improve the recognition capability of another sign language. To achieve this, two sign language dictionaries containing isolated signs from two datasets are built. Sign-to-sign mappings between the two sign languages are then identified using an optimized isolated sign language recognition model. Finally, a CSLR model is trained using a combination of the target data with original labels and the auxiliary data with mapped labels. Experimental results show that this approach achieves state-of-the-art performance on two widely-used CSLR datasets: Phoenix-2014 and Phoenix-2014T.