Large deep learning models have achieved significant success in recent years. However, training these models is challenging due to high computational costs, slow convergence, and overfitting issues. This paper proposes a new approach called Deep Incubation to address these challenges. The approach involves dividing large models into smaller sub-modules that can be trained separately and then seamlessly assembled. The main challenge is ensuring compatibility between the independently trained sub-modules. To tackle this, the paper introduces a global, shared meta model that links all the modules together. This meta model is designed to be small and have minimal computational overhead. The paper also presents a module incubation algorithm, which trains each sub-module to replace the corresponding component of the meta model and complete a given learning task. Despite its simplicity, the proposed approach effectively encourages each sub-module to understand its role in the target large model, enabling smooth collaboration after assembly. Empirical results show that the method outperforms end-to-end training in established settings and provides performance gains in downstream tasks such as object detection and image segmentation. The code for this approach is available at https://github.com/LeapLabTHU/Deep-Incubation.