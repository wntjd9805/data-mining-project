Recently, there have been concerns about the vulnerability of skeleton-based human activity recognition methods to adversarial attacks. However, these attacks typically require either full knowledge of the victim (white-box attacks), access to training data (transfer-based attacks), or frequent model queries (black-box attacks). These requirements are quite restrictive, leading to questions about the extent of the vulnerability. In this paper, we aim to demonstrate that the vulnerability does indeed exist. We introduce a new attack task called hard no-box attack, where the attacker has no access to the victim's model, training data, or labels. To accomplish this, we first learn a motion manifold and define an adversarial loss to compute a novel gradient for the attack called skeleton-motion-informed (SMI) gradient. Unlike existing gradient-based attack methods, our gradient considers the motion dynamics and is not based on the assumption that each dimension in the data is independent. The SMI gradient can augment various gradient-based attack methods, resulting in a new family of no-box attack methods. Through extensive evaluation and comparison, we demonstrate that our approach poses a significant threat to existing classifiers. We also show that the SMI gradient enhances the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box scenarios.