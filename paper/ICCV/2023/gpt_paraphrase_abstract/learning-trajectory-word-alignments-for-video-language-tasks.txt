Current Video-Language BERT models (VDL-BERTs) lack the ability to capture the trajectory characteristic of objects in videos, as they primarily focus on spatial contexts and ignore significant temporal contexts. To address this issue, we introduce a new model called TW-BERT, which incorporates a trajectory-to-word (T2W) attention mechanism to learn Trajectory-Word alignment for video-language tasks. Additionally, existing VDL-BERTs often sample a fixed number of frames, disregarding the varying graininess of different trajectories. To overcome this limitation, we propose the Hierarchical Frame-Selector (HFS) module, which selectively chooses frames based on text context during fine-tuning. By integrating T2W attention and HFS, our TW-BERT achieves state-of-the-art performance in text-to-video retrieval tasks and performs comparably to VDL-BERTs trained on larger datasets in video question-answering tasks. The code for TW-BERT will be provided in the supplementary material.