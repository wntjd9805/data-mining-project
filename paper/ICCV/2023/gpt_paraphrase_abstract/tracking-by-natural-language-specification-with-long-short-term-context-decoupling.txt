The main challenge in Tracking by Natural Language Specification (TNL) is predicting the movement of the target object using two different types of information: a static textual description of the video's main characteristics (long-term context) and an image patch containing the object and its surroundings from the current frame (search area). Many current methods struggle with how to use and combine these two types of information, as they may sometimes be inconsistent and lead to conflicts. To address this problem, we propose DecoupleTNL, which incorporates a video clip with short-term context information into the TNL framework and explores ways to reduce the impact of inconsistent visual representation and linguistic information. We introduce two jointly optimized tasks: short-term context-matching, which gathers dynamic short-term context information, and long-term context-perceiving, which extracts static long-term context information. We then design a long short-term modulation module to integrate both types of context information for accurate tracking. Extensive experiments on three tracking benchmark datasets demonstrate the superiority of DecoupleTNL.