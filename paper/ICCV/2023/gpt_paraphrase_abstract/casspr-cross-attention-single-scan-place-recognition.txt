Place recognition is crucial for autonomous robots and self-driving vehicles. The current best-performing methods for this task utilize LiDAR point clouds, either by using point-based or voxel-based structures. Voxel-based approaches excel in capturing spatial context across different scales, but they lack the local precision of point-based methods. Consequently, existing methods struggle with accurately identifying subtle geometric features in sparse single-shot LiDAR scans. To address these limitations, we propose CASSPR, a novel approach that combines point-based and voxel-based methods using cross attention transformers. CASSPR incorporates a sparse voxel branch to extract and aggregate information at lower resolution, as well as a point-wise branch to capture fine-grained local details. By utilizing queries from one branch to match structures in the other, CASSPR ensures that both branches contribute equally to generating comprehensive descriptors of the point cloud. Through extensive experiments on various datasets (Oxford RobotCar, TUM, USyd), CASSPR outperforms existing models by a significant margin. For example, it achieves an impressive AR@1 of 85.6% on the TUM dataset, surpassing the previous leading model by approximately 15%. The code for CASSPR is publicly available.