We aim to generate realistic talking head videos by animating a human face in a still image using motion information from a target-driving video. However, generating dynamic poses and expressions from a static image is challenging because the image lacks sufficient appearance information for occluded regions and delicate expression variations. This often leads to artifacts and lower quality generation. To address this issue, we propose a novel approach called MCNet that utilizes a global facial representation space and an implicit identity representation conditioned memory compensation network. Our network module learns a unified spatial facial meta-memory bank from all training samples, which provides facial structure and appearance priors to compensate for the limitations of the source image. Additionally, we introduce a query mechanism based on implicit identity representations learned from the source image keypoints. This mechanism enhances the retrieval of correlated information from the memory bank for better compensation. Extensive experiments on VoxCeleb1 and CelebV datasets demonstrate that MCNet outperforms previous state-of-the-art methods in terms of generating high-fidelity and representative talking head videos.