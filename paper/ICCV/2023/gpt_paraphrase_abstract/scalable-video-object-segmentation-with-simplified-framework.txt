This paper introduces a new framework called Simplified VOS (SimVOS) for video object segmentation (VOS). Existing methods for VOS rely on hand-crafted modules that perform feature extraction and matching separately, but these designs do not adequately capture target interaction and limit dynamic target-aware feature learning. To address these limitations, SimVOS utilizes a single transformer backbone, specifically a scalable Vision Transformer (ViT), to simultaneously extract and match features between query and reference frames. This approach enables SimVOS to learn improved target-aware features for accurate mask prediction. Additionally, SimVOS can directly leverage well-pretrained ViT backbones, bridging the gap between VOS and large-scale self-supervised pre-training. To balance performance and speed, within-frame attention and a new token refinement module are introduced to enhance running speed and save computational cost. Experimental results demonstrate that SimVOS achieves state-of-the-art performance on popular video object segmentation benchmarks, including DAVIS-2017, DAVIS-2016, and YouTube-VOS 2019, without relying on synthetic video or BL30K pre-training. The code and models for SimVOS are publicly available at https://github.com/jimmy-dq/SimVOS.git.