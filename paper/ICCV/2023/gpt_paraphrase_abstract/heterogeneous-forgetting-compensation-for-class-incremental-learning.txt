Class-incremental learning (CIL) methods have been successful in sequentially learning new classes while mitigating catastrophic forgetting of old categories. However, existing methods overlook the fact that different old classes may have varying rates of forgetting, leading to suboptimal compensation for forgetting. To address this issue, we propose the Heterogeneous Forgetting Compensation (HFC) model. HFC tackles heterogeneous forgetting by considering both the representation and gradient aspects. Firstly, we introduce a task-semantic aggregation block that combines local category information to learn task-shared global representations, thus alleviating the impact of forgetting heterogeneity on the representation. Additionally, we introduce two plug-and-play losses: a gradient-balanced forgetting compensation loss and a gradient-balanced relation distillation loss. These losses account for forgetting heterogeneity by balancing the gradients and ensuring consistent relations between old and new classes. Experimental results on various datasets demonstrate the effectiveness of our HFC model. The implementation code can be found at https://github.com/JiahuaDong/HFC.