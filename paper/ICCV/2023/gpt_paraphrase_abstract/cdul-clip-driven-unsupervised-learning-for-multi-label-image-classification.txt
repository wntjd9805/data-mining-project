This paper introduces a novel unsupervised learning approach for multi-label image classification without the need for annotations. The proposed method consists of three stages: initialization, training, and inference. In the initialization stage, the CLIP model is utilized to extend its capabilities for multi-label predictions by aggregating global and local image-text similarity. This is achieved by breaking down each image into snippets and generating similarity vectors for both the entire image and each snippet using CLIP. A similarity aggregator is then employed to combine the global and local similarity vectors.   In the training stage, the aggregated similarity scores are used as initial pseudo labels. An optimization framework is proposed to train the parameters of the classification network and refine the pseudo labels for unobserved labels. During inference, only the classification network is utilized to predict the labels of input images.  Extensive experiments demonstrate the effectiveness of the proposed method, surpassing the performance of state-of-the-art unsupervised methods on MS-COCO, PASCAL VOC 2007, PASCAL VOC 2012, and NUS datasets. Furthermore, the method achieves comparable results to weakly supervised classification methods.