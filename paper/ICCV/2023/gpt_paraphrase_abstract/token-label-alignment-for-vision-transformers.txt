Data mixing strategies, such as CutMix, have been successful in enhancing the performance of convolutional neural networks (CNNs). These strategies involve combining two images during training and assigning them a mixed label. However, we have observed a fluctuation phenomenon in the input tokens that hampers the effectiveness of data mixing strategies, particularly in vision transformers (ViTs). The contributions of input tokens vary as they propagate forward, resulting in a different mixing ratio in the output tokens. Consequently, the original data mixing strategy computes inaccurate training targets, leading to less effective training. To tackle this issue, we propose a method called token-label alignment (TL-Align) that establishes a correspondence between transformed tokens and their original counterparts, ensuring that each token maintains its label. We leverage the computed attention at each layer to efficiently achieve token-label alignment, incurring minimal additional training costs. Through extensive experiments, we demonstrate that our approach significantly enhances the performance of ViTs across various tasks, including image classification, semantic segmentation, object detection, and transfer learning. The code for our method is available at: https://github.com/Euphoria16/TL-Align.