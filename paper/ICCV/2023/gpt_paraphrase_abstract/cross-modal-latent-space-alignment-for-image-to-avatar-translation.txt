We propose a new technique for generating vectorized avatars automatically using a single portrait image. Unlike existing methods that rely on image-to-image translation, our approach overcomes limitations in 3D rendering, animation, and video. We utilize modality-specific autoencoders trained on large-scale unpaired portraits and parametric avatars. By training an alignment module on a smaller dataset, we establish a mapping between the two modalities. This cross-modal latent space preserves facial identity, resulting in visually appealing and high-fidelity avatars. Our method is resolution-independent, offering versatility and applicability across various scenarios. Quantitative and qualitative evaluations support the superiority of our approach.