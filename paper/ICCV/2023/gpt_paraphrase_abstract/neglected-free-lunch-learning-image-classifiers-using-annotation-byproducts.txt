Supervised learning of image classifiers involves creating a parametric model fÎ¸ by using pairs of images and their corresponding labels. However, this approach overlooks valuable auxiliary information obtained during the annotation process, such as the time-series of mouse traces and clicks made during image selection. We propose that utilizing these annotation byproducts, denoted as Z, can provide approximate human attention that helps the model focus on relevant cues in the foreground. This reduces the impact of spurious correlations and discourages shortcut learning. To validate this idea, we introduce ImageNet-AB and COCO-AB, which are enriched versions of the ImageNet and COCO training sets that include sample-wise annotation byproducts. We refer to this new training paradigm as learning using annotation byproducts (LUAB). By incorporating a multitask loss that regresses Z along with Y (the original labels), we demonstrate that LUAB significantly improves the generalizability and robustness of the trained models. Importantly, this approach does not incur any additional annotation costs compared to traditional supervised learning. The ImageNet-AB and COCO-AB datasets can be accessed at github.com/naver-ai/NeglectedFreeLunch.