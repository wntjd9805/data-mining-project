Pre-trained models and grid-based models have been successful in various tasks in the field of Document AI. However, when it comes to document layout analysis (DLA), existing pre-trained models primarily rely on either textual or visual features, even in multi-modal settings. On the other hand, grid-based models for DLA, although multi-modal, do not fully utilize the benefits of pre-training. In order to make the most of multi-modal information and exploit pre-training techniques for improved representation in DLA, this paper introduces VGT, a two-stream Vision Grid Transformer. This model incorporates the Grid Transformer (GiT), which is pre-trained to understand 2D token-level and segment-level semantics. Additionally, the paper presents a new dataset called D4LA, which is the most diverse and detailed manually-annotated benchmark for document layout analysis to date. Experimental results demonstrate that the proposed VGT model achieves state-of-the-art performance on DLA tasks, surpassing previous benchmarks such as PubLayNet, DocBank, and D4LA. The code, models, and D4LA dataset will be publicly available.