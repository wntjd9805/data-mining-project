Token-based masked generative models are becoming popular due to their fast inference time through parallel decoding. However, current token-based approaches suffer from suboptimal generation performance as they sample multiple tokens simultaneously without considering their dependencies. This empirical study investigates this issue and proposes a learnable sampling model called Text-Conditioned Token Selection (TCTS), which selects optimal tokens based on localized supervision using text information. TCTS not only improves image quality but also enhances the semantic alignment between generated images and provided texts. To further enhance image quality, a cohesive sampling strategy called Frequency Adaptive Sampling (FAS) is introduced, which divides tokens into groups based on self-attention maps. The efficacy of TCTS combined with FAS is validated across various generative tasks, demonstrating its superiority over baselines in terms of image-text alignment and image quality. Furthermore, this text-conditioned sampling framework reduces the original inference time by over 50% without modifying the original generative model.