In recent studies, methods have been proposed to combine RGB images and point clouds in order to estimate both 2D optical flow and 3D scene flow. However, the performance of these methods is limited by the low sampling rates of conventional RGB cameras and LiDAR sensors, particularly in highly dynamic scenes. In contrast, event cameras can capture intensity changes with a high temporal resolution, providing additional dynamic information of the observed scenes. This paper introduces a multi-stage multimodal fusion model called RPEFlow, which incorporates RGB images, point clouds, and events for joint optical flow and scene flow estimation. The model includes an attention fusion module that utilizes a cross-attention mechanism to explore the cross-modal correlation between 2D and 3D branches. Additionally, a mutual information regularization term is introduced to explicitly model the complementary information of the three modalities for effective multimodal feature learning. The paper also presents a new synthetic dataset for further research. Experimental results on both synthetic and real datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art methods. The code and dataset are available at https://npucvr.github.io/RPEFlow.