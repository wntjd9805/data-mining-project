We introduce a unified approach for addressing various video tasks related to human movement by utilizing large and diverse datasets. Our approach involves a pretraining phase where a motion encoder is trained to reconstruct the underlying 3D motion from incomplete 2D observations. The resulting motion representations incorporate knowledge about the geometric, kinematic, and physical aspects of human motion, which can be easily transferred to multiple tasks. To implement the motion encoder, we utilize a Dual-stream Spatio-temporal Transformer (DSTformer) neural network, which effectively captures long-range spatio-temporal relationships among skeletal joints. This network achieves the lowest error in 3D pose estimation when trained from scratch. Additionally, our framework achieves state-of-the-art performance in all three tasks by fine-tuning the pre-trained motion encoder with a simple regression head consisting of 1-2 layers. This demonstrates the versatility of the acquired motion representations. The source code and models are available at https://motionbert.github.io/.