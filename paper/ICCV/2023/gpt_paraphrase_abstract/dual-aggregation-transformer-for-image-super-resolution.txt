Transformer has become popular in low-level vision tasks, particularly in image super-resolution (SR). These networks use self-attention along spatial or channel dimensions and have shown impressive performance. Building on this concept, we propose a new Transformer model called Dual Aggregation Transformer (DAT) for image SR. DAT aggregates features across both spatial and channel dimensions, both in inter-block and intra-block manners. To achieve this, we apply spatial and channel self-attention alternatively in consecutive Transformer blocks. This approach allows DAT to capture global context and perform inter-block feature aggregation. Additionally, we introduce the adaptive interaction module (AIM) and the spatial-gate feed-forward network (SGFN) for intra-block feature aggregation. AIM complements the self-attention mechanisms from corresponding dimensions, while SGFN incorporates non-linear spatial information in the feed-forward network. Extensive experiments demonstrate that our DAT model outperforms existing methods in image SR. The code and models for DAT are available at https://github.com/zhengchen1999/DAT.