Skeleton-based action recognition has become a popular research topic, but current approaches are limited in fully utilizing semantic relations between actions. For instance, the difference between hand gestures like "make victory sign" and "thumb up" lies in the movement of hands, which is not captured by traditional one-hot classification methods. To address this issue, we propose a novel approach called Generative Action-description Prompts (GAP) for skeleton-based action recognition. Our approach leverages a pre-trained language model to automatically generate text descriptions for body part movements in actions. We then use a multi-modal training scheme, where the text encoder generates feature vectors for different body parts and supervises the skeleton encoder for learning action representations. Experimental results demonstrate that our GAP method outperforms various baseline models without incurring extra computation costs during inference. In fact, GAP achieves new state-of-the-art results on popular skeleton-based action recognition benchmarks, including NTU RGB+D, NTU RGB+D 120, and NW-UCLA. The source code for our approach is publicly available at https://github.com/MartinXM/GAP.