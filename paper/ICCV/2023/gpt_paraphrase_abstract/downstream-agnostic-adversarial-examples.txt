Self-supervised learning typically involves using a large amount of unlabeled data to pre-train an encoder that can serve as a general-purpose feature extractor. This allows downstream users to simply fine-tune the encoder and benefit from the advantages of a "large model." However, the security of pre-trained encoders, particularly when they are publicly available for commercial use, has not been thoroughly explored.In this paper, we introduce AdvEncoder, which is the first framework for generating universal adversarial examples that are agnostic to downstream tasks, based on a pre-trained encoder. The goal of AdvEncoder is to create a universal perturbation or patch that can deceive all downstream tasks that utilize the victim pre-trained encoder. Unlike traditional adversarial example techniques, the pre-trained encoder only produces feature vectors rather than classification labels.To generate adversarial examples, we leverage high-frequency component information in the images to guide the process. We then develop a generative attack framework that learns the distribution of the attack surrogate dataset to enhance the success rates and transferability of the adversarial perturbations/patches. Our experiments demonstrate that an attacker can successfully target downstream tasks without any knowledge of the pre-training dataset or the downstream dataset.Furthermore, we propose four defenses specifically designed for pre-trained encoders, and our results confirm the effectiveness of these defenses. Overall, our findings highlight the attack potential of AdvEncoder. The code for AdvEncoder is publicly available at: https://github.com/CGCL-codes/AdvEncoder.