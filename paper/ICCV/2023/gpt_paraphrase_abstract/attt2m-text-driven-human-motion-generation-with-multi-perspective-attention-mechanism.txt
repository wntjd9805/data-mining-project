In recent years, there has been a significant research focus on generating 3D human motion based on textual descriptions. However, this task presents challenges due to the complex nature of human motion and the difficulty in learning the relationship between text and motion. To address these challenges, we propose a two-stage method called AttT2M. This method incorporates a multi-perspective attention mechanism, including body-part attention and global-local motion-text attention. The body-part attention focuses on the motion embedding perspective, utilizing a body-part spatio-temporal encoder to learn a more expressive latent space. The global-local motion-text attention, on the other hand, aims to learn the cross-modal relationship between motion and text at both the sentence-level and word-level. The final step involves generating the text-driven motion using a generative transformer. We conducted extensive experiments on HumanML3D and KIT-ML datasets and compared our method to the current state-of-the-art approaches. The results show that our method outperforms existing approaches in terms of qualitative and quantitative evaluation, achieving fine-grained synthesis and action-to-motion generation. The code for our method is available at https://github.com/ZcyMonkey/AttT2M.