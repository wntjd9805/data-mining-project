Current face recognition systems that are considered state-of-the-art rely heavily on large amounts of labeled training data. However, due to privacy concerns, the available data is often limited to celebrity web crawls, which in turn limits the number of identities that can be used for training. On the other hand, there has been a recent trend in the industry towards self-supervised learning techniques, which has prompted researchers to explore how these techniques can be applied to face recognition.One popular method for improving the training dataset is to augment it with samples generated from generative models, while ensuring that the identity of the individuals is preserved. In this study, the researchers demonstrate that a simple approach, which involves fine-tuning the pSp encoder for StyleGAN, leads to better facial recognition results compared to training on synthetic face identities. Additionally, the researchers have created two large-scale unlabeled datasets, AfricanFaceSet-5M and AsianFaceSet-3M, consisting of 5 million and 3 million images of different people respectively. Pretraining on these datasets improves the recognition of the respective ethnicities, as well as others. Furthermore, combining all the unlabeled datasets results in the highest performance increase.The self-supervised strategy proposed in this study proves to be particularly useful when there is a limited amount of labeled training data available. This can be advantageous for more specific face recognition tasks and when privacy concerns are a priority. The researchers evaluated their approach using a standard dataset called RFW and a new benchmark called RB-WebFace. All the code and data used in this study have been made publicly available for further research.To access the code and data, visit https://github.com/seva100/stylegan-for-facerec.