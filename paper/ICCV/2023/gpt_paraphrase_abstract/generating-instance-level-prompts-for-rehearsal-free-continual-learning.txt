We present a new method called Domain-Adaptive Prompt (DAP) that utilizes Vision Transformers (ViT) for continual learning. Prompt-based continual learning has gained attention recently for its rehearsal-free nature. The prompt pool, which is central to prompt-based continual learning, is crucial for effectively utilizing the pre-trained ViT backbone in a sequence of tasks. However, we have observed that the use of a prompt pool creates a domain scalability issue between pre-training and continual learning. This problem arises because the prompt pool encodes group-level instructions. To address this problem, we propose DAP, a pool-free approach that generates an appropriate prompt on an individual basis at inference time. We optimize an adaptive prompt generator that generates fine-grained instructions specific to each input, enhancing model flexibility and reducing forgetting. Our experiments on seven datasets, which vary in domain similarity to ImageNet, demonstrate that DAP outperforms state-of-the-art prompt-based methods. The code for DAP is publicly available at https://github.com/naver-ai/dap-cl.