We propose DySample, a highly efficient dynamic upsampler that is lightweight and achieves impressive performance gains. Existing kernel-based dynamic upsamplers like CARAFE, FADE, and SAPA introduce significant workload due to time-consuming dynamic convolution and the use of additional sub-networks for generating dynamic kernels. Moreover, FADE and SAPA require high-resolution feature guidance, which limits their application scenarios. To address these concerns, we bypass dynamic convolution and approach upsampling from the perspective of point sampling, which is more resource-efficient and can be easily implemented using the standard built-in function in PyTorch. We initially present a basic design and then demonstrate how to progressively enhance its upsampling behavior to develop our new upsampler, DySample.Compared to previous kernel-based dynamic upsamplers, DySample does not require a customized CUDA package and has significantly fewer parameters, FLOPs (floating-point operations), GPU memory usage, and latency. In addition to its lightweight characteristics, DySample outperforms other upsamplers in five dense prediction tasks, including semantic segmentation, object detection, instance segmentation, panoptic segmentation, and monocular depth estimation.The code for DySample is available at https://github.com/tiny-smart/dysample.