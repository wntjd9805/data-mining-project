Self-supervised representation learning involves withholding a part of the data and tasking the network to predict it. Data augmentation, particularly tool masking, is commonly used to create an information gap. Tool masking withholds content along the sequential dimension (e.g., spatial in images, temporal in audio, and syntactic in language). This study explores the orthogonal channel dimension for data augmentation by utilizing precision redundancy. Each channel's data is quantized using a non-uniform quantizer, with the quantized value randomly sampled within randomly sampled quantization bins. Quantization is similar to channel-wise masking as it removes information within each bin but preserves information across bins. The proposed approach outperforms existing generic data augmentation methods and performs similarly to modality-specific augmentations. The approach is evaluated on various data modalities, such as vision, audio, 3D point clouds, and the DABS benchmark. The code for the approach can be found at https://github.com/microsoft/random_quantize.