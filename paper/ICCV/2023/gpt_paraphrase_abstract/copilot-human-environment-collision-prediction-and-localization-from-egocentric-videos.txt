The ability to predict collisions between humans and their environment is crucial for applications like virtual reality, augmented reality, and wearable assistive robotics. This study introduces the challenging task of forecasting collisions in various environments using multi-view egocentric videos from body-mounted cameras. To tackle this problem, a transformer-based model called COPILOT is proposed. COPILOT simultaneously predicts collisions and localizes them by utilizing a unique 4D space-time-viewpoint attention mechanism that aggregates information from multi-view inputs. To facilitate research in this area, a synthetic data generation framework is developed to create egocentric videos of virtual humans moving and colliding in diverse 3D environments. This framework generates a large-scale dataset consisting of 8.6 million egocentric RGBD frames. Extensive experiments demonstrate that COPILOT can generalize to both synthetic and real-world scenes. The study also shows that the outputs of COPILOT can be effectively used for collision avoidance through simple closed-loop control. For more information, please visit the project webpage at https://sites.google.com/stanford.edu/copilot.