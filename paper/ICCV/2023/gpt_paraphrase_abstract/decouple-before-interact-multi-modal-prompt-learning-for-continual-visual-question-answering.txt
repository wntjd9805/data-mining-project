In the real world, it is important for a Visual Question Answering (VQA) model to be able to provide accurate answers to new questions and images in a continuous setting, known as CL-VQA. However, current approaches to CL-VQA only consider either the vision or language aspect and apply uni-modal continual learning strategies, which is not ideal. This limited approach leads to restricted evaluations and poor performance due to the neglect of interactions between modalities. To address these challenges, we propose a comprehensive formulation for CL-VQA that combines both vision and language. Based on this formulation, we introduce a novel approach called TRIPLET, which utilizes a pre-trained vision-language model and incorporates decoupled prompts and prompt interaction strategies to capture the complex interactions between modalities. The decoupled prompts have learnable parameters that are separated for different aspects, while the prompt interaction strategies model the interactions between inputs and prompts. Furthermore, we create two CL-VQA benchmarks to provide a more comprehensive evaluation. Extensive experiments show that our TRIPLET approach outperforms existing methods in both uni-modal and multi-modal continual learning settings for CL-VQA.