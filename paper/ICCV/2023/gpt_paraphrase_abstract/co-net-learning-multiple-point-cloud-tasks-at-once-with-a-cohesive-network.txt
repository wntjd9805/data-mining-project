We introduce CO-Net, a comprehensive framework that collectively optimizes multiple point cloud tasks across different datasets. Our approach maintains high storage efficiency by combining models with shared parameters into a single model. To achieve effective feature extraction, we utilize a residual MLP (Res-MLP) block and scale it accordingly to meet the requirements of various tasks. Additionally, we propose a novel nested layer-wise processing policy that determines the optimal architecture for each task, balancing the sharing and non-sharing of parameters within each layer of the block. This policy addresses the challenges of multi-task learning on point cloud data, such as varying model topologies and conflicting gradients from different dataset domains. Furthermore, we introduce a sign-based gradient surgery technique to enhance the training of CO-Net, emphasizing the use of shared parameters and ensuring thorough optimization of each task. Experimental results demonstrate that CO-Net significantly reduces computation and storage costs compared to previous methods, while outperforming them in terms of performance. We also show that CO-Net enables incremental learning and prevents catastrophic amnesia when adapting to new point cloud tasks.