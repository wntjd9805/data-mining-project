Simulation is a vital tool for advancing the development of self-driving systems. One crucial aspect of this is simulating realistic and diverse human behavior. In this study, we propose a comprehensive framework for learning realistic human dynamics from real driving scenarios, bridging the gap between real and simulated human behavior in safety-critical applications. We demonstrate that current methods are not effective in driving scenarios where video data is recorded from moving vehicles and humans are often partially or fully hidden. Additionally, existing methods often overlook the overall scene in which humans are situated, leading to motion artifacts such as foot sliding or ground penetration. To tackle this challenge, we suggest an approach that combines physics with a reinforcement learning-based motion controller to learn human dynamics for driving scenarios. Our framework can simulate plausible human dynamics that accurately replicate observed human motions and fill in motions for concealed body parts, while enhancing the physical plausibility of the entire motion sequence. Experiments conducted on the demanding Waymo Open Dataset reveal that our method significantly outperforms state-of-the-art motion capture approaches in recovering high-quality, physically plausible, and scene-aware human dynamics.