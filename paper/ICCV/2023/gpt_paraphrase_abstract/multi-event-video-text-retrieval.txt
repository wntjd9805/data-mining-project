Video-Text Retrieval (VTR) is an important task in handling the vast amount of video-text data available on the Internet. Many existing approaches use a two-stream Vision-Language model to learn a joint representation of video-text pairs. However, these models assume a one-to-one correspondence between videos and texts, which is not always practical. In reality, videos often contain multiple events while texts are specific to single events. This gap between the training objective and real-world applications can lead to degraded performance. To address this, we propose the Multi-event Video-Text Retrieval (MeVTR) task, which focuses on scenarios where videos contain multiple events. We introduce a simple model called Me-Retriever that incorporates key event video representation and a new MeVTR loss. Through extensive experiments, we demonstrate that our framework outperforms other models in both Video-to-Text and Text-to-Video tasks, providing a strong baseline for the MeVTR task. We believe this work will serve as a solid foundation for future research in this area. The code is available at https://github.com/gengyuanmax/MeVTR.