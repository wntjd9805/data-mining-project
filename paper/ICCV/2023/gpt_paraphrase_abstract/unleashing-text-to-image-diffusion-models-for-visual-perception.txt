This paper introduces a new framework called VPD (Visual Perception with pre-trained Diffusion models) that utilizes the semantic information of a pre-trained text-to-image diffusion model in visual perception tasks. Instead of using the pre-trained denoising autoencoder in a diffusion-based pipeline, the authors propose using it as a backbone to study how to fully leverage the learned knowledge. The denoising decoder is prompted with appropriate textual inputs and the text features are refined with an adapter to align with the pre-trained stage, allowing the visual contents to interact with the text prompts. Cross-attention maps between the visual and text features are also employed to provide explicit guidance. The authors demonstrate that vision-language pre-trained diffusion models can be adapted faster to downstream visual perception tasks using VPD compared to other pre-training methods. The effectiveness of VPD is validated through extensive experiments on semantic segmentation, referring image segmentation, and depth estimation. Notably, VPD achieves impressive results on NYUv2 depth estimation and RefCOCO-val referring image segmentation benchmarks, setting new records. The code for VPD is available at https://github.com/wl-zhao/VPD.