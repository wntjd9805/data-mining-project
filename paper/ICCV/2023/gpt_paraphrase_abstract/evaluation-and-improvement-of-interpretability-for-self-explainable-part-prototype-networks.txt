Part-prototype networks such as ProtoPNet, ProtoTree, and ProtoPool have gained attention for their interpretability and comparable accuracy. However, recent studies have found that interpretability from prototypes is fragile due to a semantic gap between similarities in the feature space and the input space. To address this challenge, we propose two evaluation metrics, "consistency score" and "stability score," to quantitatively assess the interpretability of part-prototype networks. These metrics measure the consistency of explanations across images and the robustness of explanations against perturbations, both crucial for practical use. Additionally, we introduce an enhanced part-prototype network with a shallow-deep feature alignment module and a score aggregation module to improve interpretability. Through systematic evaluation experiments and in-depth discussions, we uncover the interpretability of existing part-prototype networks. Our model outperforms state-of-the-art approaches in terms of accuracy and interpretability, as demonstrated on three benchmarks across nine architectures. The code for our model is available at https://github.com/hqhQAQ/EvalProtoPNet.