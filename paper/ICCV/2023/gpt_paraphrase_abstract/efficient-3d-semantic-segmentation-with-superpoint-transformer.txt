We present a new transformer architecture called superpoint-based transformer for efficient semantic segmentation of large-scale 3D scenes. Our method includes a fast algorithm to divide point clouds into a hierarchical superpoint structure, resulting in preprocessing that is 7 times faster than existing superpoint-based approaches. Additionally, we utilize a self-attention mechanism to capture the relationships between superpoints at different scales, achieving state-of-the-art performance on three challenging benchmark datasets: S3DIS, KITTI-360, and DALES. Our approach has only 212k parameters, making it up to 200 times more compact than other state-of-the-art models while maintaining similar performance. Furthermore, our model can be trained on a single GPU in just 3 hours for a fold of the S3DIS dataset, which is significantly fewer GPU-hours compared to the best-performing methods. Access to our code and models can be found at github.com/drprojects/superpoint_transformer.