We introduce a new image-text pretraining method called Contrastive Feature Masking Vision Transformer (CFM-ViT) for open-vocabulary object detection (OVD). Our approach combines the masked autoencoder (MAE) objective with contrastive learning to enhance representation for localization tasks. Unlike traditional MAE, we perform reconstruction in the joint image-text embedding space rather than the pixel space, leading to improved understanding of region-level semantics. Additionally, we propose Positional Embedding Dropout (PED) to address scale variation by randomly dropping out positional embeddings during pretraining. PED enhances detection performance and allows for the use of a frozen ViT backbone as a region classifier, preserving open-vocabulary knowledge during detection fine-tuning. CFM-ViT achieves a state-of-the-art 33.9 APr on the LVIS open-vocabulary detection benchmark, surpassing the best approach by 7.6 points and demonstrating better zero-shot detection transfer. Furthermore, CFM-ViT exhibits strong image-level representation, outperforming the state of the art on 8 out of 12 metrics in zero-shot image-text retrieval benchmarks.