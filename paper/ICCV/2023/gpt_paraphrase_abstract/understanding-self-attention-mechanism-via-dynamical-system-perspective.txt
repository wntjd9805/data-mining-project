The self-attention mechanism (SAM) has been widely used in artificial intelligence and has proven to enhance the performance of different models. However, current explanations for its effectiveness are mainly based on intuition and experience, lacking a direct modeling approach. In this paper, we address this issue by applying the dynamical system perspective to residual neural networks and demonstrating the presence of intrinsic stiffness phenomenon (SP) in high-performance neural networks. We argue that the ability of neural networks to measure SP at the feature level is crucial for achieving high performance and poses a challenge in training. Drawing parallels to the adaptive step-size method used in solving stiff ordinary differential equations (ODEs), we show that SAM acts as a stiffness-aware step size adaptor that improves the model's representational ability by refining stiffness estimation and generating adaptive attention values. This provides a new understanding of why and how SAM benefits model performance. Our novel perspective also explains the lottery ticket hypothesis in SAM, allows for the design of new quantitative metrics for representational ability, and inspires a new theoretic-inspired approach called StepNet. Extensive experiments on popular benchmarks demonstrate that StepNet accurately extracts fine-grained stiffness information, accurately measures SP, and leads to significant improvements in various visual tasks.