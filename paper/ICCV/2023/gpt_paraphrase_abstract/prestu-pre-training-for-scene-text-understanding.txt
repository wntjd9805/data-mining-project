Vision-and-language (V&L) models often struggle to recognize and reason about text embedded in visual inputs. This may be due to the lack of inclusion of this ability in V&L pre-training methods. To address this issue, we propose a new pre-training approach called PRESTU, specifically designed for scene-text understanding (STU). PRESTU incorporates OCR-aware pre-training objectives, which encourage the model to identify text in an image and establish connections with the rest of the image content. We implement PRESTU using a transformer-based encoder-decoder architecture and train it on large-scale image-text datasets containing scene text extracted from an OCR system. Through empirical evaluations, we demonstrate the effectiveness of PRESTU on eight visual question answering and four image captioning benchmarks. The output generated by the answer format solely focuses on the abstraction.