Recent advancements in video recognition models have incorporated Transformer models to capture long-range spatio-temporal context. However, these models come with a high computational cost due to their reliance on self-attention. On the other hand, convolutional designs offer a more efficient alternative but lack the ability to model long-range dependencies effectively. To address this limitation, we propose Video-FocalNet, a novel architecture for video recognition that combines the benefits of both local and global context modeling. Video-FocalNet utilizes a spatio-temporal focal modulation architecture, which reverses the interaction and aggregation steps of self-attention to improve efficiency. We implement these steps using computationally less expensive convolution and element-wise multiplication operations, achieving comparable performance to self-attention at a lower computational cost. Through extensive experimentation, we demonstrate that our parallel spatial and temporal encoding design is the optimal choice for focal modulation-based spatio-temporal context modeling. In evaluations on five large-scale datasets (Kinetics-400, Kinetics-600, SS-v2, Diving-48, and ActivityNet-1.3), Video-FocalNets outperform state-of-the-art transformer-based models while maintaining computational efficiency. Our code and models are publicly available for further research and implementation.