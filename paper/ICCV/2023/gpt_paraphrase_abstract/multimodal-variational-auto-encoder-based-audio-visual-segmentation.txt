We propose an Explicit Conditional Multimodal Variational Auto-Encoder (ECMVAE) for audio-visual segmentation (AVS) that aims to segment sound sources in video sequences. Existing AVS methods primarily use implicit feature fusion strategies, where models are trained to fit the discrete samples in the dataset. However, due to limited and less diverse datasets, these methods often yield unsatisfactory results. In contrast, our approach addresses this problem from an effective representation learning perspective by explicitly modeling the contribution of each modality.  We observe that audio contains critical category information about the sound producers, while visual data provides potential sound producer(s). The shared information between the two corresponds to the target sound producer(s) depicted in the visual data. Consequently, cross-modal shared representation learning becomes crucial for AVS. To accomplish this, our ECMVAE factorizes the representations of each modality into a modality-shared representation and a modality-specific representation. We apply an orthogonality constraint between the shared and specific representations to preserve the exclusive attributes of the factorized latent code. Additionally, we introduce a mutual information maximization regularizer to extensively explore each modality.  Our approach is evaluated quantitatively and qualitatively on the AVSBench, demonstrating its effectiveness. It achieves a new state-of-the-art for AVS, with a 3.84 mIOU performance improvement on the challenging MS3 subset for multiple sound source segmentation.