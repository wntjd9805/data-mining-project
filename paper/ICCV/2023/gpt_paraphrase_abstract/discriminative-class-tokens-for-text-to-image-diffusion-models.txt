Advancements in text-to-image diffusion models have allowed for the creation of high-quality images with increased diversity. However, these images often lack subtle details and are prone to errors due to ambiguity in the input text. One solution to address these issues is to train diffusion models using class-labeled datasets. However, this approach has drawbacks, including the limited size of supervised datasets compared to larger scraped text-image datasets used for training text-to-image models, which affects the quality and diversity of generated images. Additionally, using hard-coded labels as input restricts control over the generated images.To overcome these limitations, we propose a non-invasive fine-tuning technique that leverages the expressive potential of free-form text while ensuring high accuracy through signals from a pretrained classifier. Our approach involves iteratively modifying the embedding of an added input token in a text-to-image diffusion model to steer generated images towards a specific target class based on the guidance of a classifier. Compared to previous fine-tuning methods, our technique is fast and does not require a collection of in-class images or retraining of a noise-tolerant classifier.We extensively evaluate our method and demonstrate that the generated images are more accurate and of higher quality compared to standard diffusion models. Furthermore, our approach can be used to augment training data in low-resource settings and provides insights into the data used to train the guiding classifier. The code for our method is available at https://github.com/idansc/discriminative_class_tokens.