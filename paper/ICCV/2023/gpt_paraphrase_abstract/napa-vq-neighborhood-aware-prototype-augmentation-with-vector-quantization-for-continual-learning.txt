Catastrophic forgetting is a problem faced by deep neural networks when they lose old knowledge upon acquiring new knowledge. Existing solutions to this issue involve storing exemplars, but this may not be feasible in applications with memory limitations or privacy constraints. Non-Exemplar based Class Incremental Learning (NECIL) has been proposed as an alternative, but it struggles to differentiate between old and new classes, leading to overlapping feature representations. To address this, we propose NAPA-VQ, a framework that reduces class overlap in NECIL. Inspired by Neural Gas, we use topological relationships in the feature space to identify neighboring classes that are likely to be confused with each other. This information is used to enforce separation between these neighboring classes and generate old class representative prototypes, which aid in obtaining a discriminative decision boundary between old and new classes. Our experiments on CIFAR-100, TinyImageNet, and ImageNet-Subset show that NAPA-VQ outperforms existing NECIL methods in terms of accuracy and forgetting. The average improvement is 5%, 2%, and 4% in accuracy and 10%, 3%, and 9% in forgetting, respectively. The code for NAPA-VQ can be found at https://github.com/TamashaM/NAPA-VQ.git.