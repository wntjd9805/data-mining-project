Zero-shot image captioning using pre-trained vision-language models (VLMs) and large language models (LLMs) has made significant progress. However, these methods are prone to modality bias induced by LLMs, resulting in the generation of descriptions that include non-existent objects. To address this issue, we propose ViECap, a transferable decoding model that utilizes entity-aware decoding to generate descriptions in both seen and unseen scenarios. ViECap incorporates entity-aware hard prompts to guide LLMs' attention towards the visual entities present in the image, ensuring coherent caption generation across diverse scenes. By incorporating entity-aware hard prompts, ViECap maintains its performance when transferring from in-domain to out-of-domain scenarios. Extensive experiments show that ViECap achieves state-of-the-art cross-domain captioning and performs competitively in-domain compared to previous VLMs-based zero-shot methods. The code for ViECap is available at: https://github.com/FeiElysia/ViECap.