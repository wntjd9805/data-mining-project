While previous NeRF-based methods have achieved impressive results in synthesizing novel views of humans, they often require a large number of images or views for training. In this study, we propose a new type of NeRF called ActorsNeRF, which can be animated. ActorsNeRF is first pretrained on a diverse range of human subjects and then fine-tuned using a small number of monocular video frames featuring a new actor with unseen poses. To improve the model's generalization capabilities, we incorporate two human priors that capture the variations in human appearance, shape, and pose. These priors are applied in the encoded feature space, where different human subjects are aligned in a category-level canonical space, and the same human across different frames are aligned in an instance-level canonical space for rendering. Through quantitative and qualitative evaluations, we demonstrate that ActorsNeRF outperforms existing state-of-the-art methods in few-shot generalization to new people and poses across multiple datasets. Further details can be found on our project page: https://jitengmu.github.io/ActorsNeRF/.