This paper addresses the limitations of diffusion-based methods in editing objects in videos while preserving their appearance over time. The authors propose a solution by introducing temporal dependency to existing text-driven diffusion models. They develop an inter-frame propagation mechanism that utilizes layered representations to propagate appearance information from one frame to the next. This mechanism forms the basis of a text-driven video editing framework called StableVideo, which enables consistency-aware video editing. The authors conduct extensive experiments to demonstrate the effectiveness of their approach, showing superior qualitative and quantitative results compared to state-of-the-art video editing methods. The code for their approach is made available.