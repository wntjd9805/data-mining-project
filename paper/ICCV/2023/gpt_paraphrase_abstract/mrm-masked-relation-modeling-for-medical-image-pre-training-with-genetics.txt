Current deep learning methods for automatic multi-modal medical diagnosis require a large number of expert annotations, which is time-consuming and impractical. Pre-training techniques based on masked image modeling (MIM) have shown promising results in learning meaningful representations from unlabeled data and transferring them to other tasks. However, these methods have primarily focused on natural images and overlook the unique characteristics of medical data, leading to unsatisfactory generalization performance in medical diagnosis.  To address this issue, this paper introduces a masked relation modeling (MRM) framework that leverages genetics to enhance image pre-training. Unlike previous MIM methods that mask input data, which results in the loss of disease-related semantics, the MRM framework incorporates relation masking. This involves masking token-wise feature relations in both self- and cross-modality levels, preserving the original semantics and enabling the model to learn rich disease-related information.  Additionally, the MRM framework proposes relation matching to align the sample-wise relations between intact and masked features. By encouraging global constraints in the feature space, relation matching enhances semantic relation modeling, thereby improving feature representation.  Extensive experiments demonstrate the effectiveness of the proposed MRM framework, achieving state-of-the-art performance in various downstream diagnosis tasks. The framework is simple yet powerful and can be accessed through the provided GitHub repository (https://github.com/CityU-AIM-Group/MRM).