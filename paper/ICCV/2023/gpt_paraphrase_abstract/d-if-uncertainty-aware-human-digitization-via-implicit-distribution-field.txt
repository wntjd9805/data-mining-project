The creation of realistic virtual humans is important in various industries such as metaverse, intelligent healthcare, and self-driving simulation. However, effectively generating these virtual humans at a large scale with high levels of realism is still a challenge. The use of deep implicit function has introduced a new era in image-based 3D clothed human reconstruction, allowing for accurate shape recovery with fine details. Existing approaches typically locate the surface by determining the implicit value for each point. However, it is questionable whether all points should be treated equally regardless of their proximity to the surface. In this study, we propose a new approach that replaces the deterministic implicit value with an adaptive uncertainty distribution, which takes into account the distance of each point to the surface. This transition from "value to distribution" leads to significant improvements compared to existing methods. Our qualitative results demonstrate that the models trained using our uncertainty distribution loss can capture more intricate wrinkles and realistic limbs. The code and models are available on GitHub for research purposes.