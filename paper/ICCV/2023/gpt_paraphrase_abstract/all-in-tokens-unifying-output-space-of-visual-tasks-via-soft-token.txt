We introduce AiT, a unified output representation for various computer vision tasks, which is an important step towards developing general-purpose vision task solvers. Despite the challenges presented by the complex and task-specific outputs, we demonstrate the potential of using a discrete representation (VQ-VAE) to model the dense outputs of multiple computer vision tasks as a sequence of discrete tokens. This is inspired by the ability of VQ-VAE to effectively capture structures that span multiple pixels using a limited number of discrete codes. To achieve this, we propose a modified, shallower architecture for VQ-VAE that enhances efficiency without compromising prediction accuracy. Additionally, our approach incorporates uncertainty into the decoding process by employing a soft fusion of the codebook entries, resulting in a more stable training process and improved prediction accuracy. Through our evaluation on depth estimation and instance segmentation tasks, involving both continuous and discrete labels, we demonstrate that AiT outperforms other unified models. The code and models for AiT can be accessed at https://github.com/SwinTransformer/AiT.