Knowledge distillation (KD) is a commonly used technique in unsupervised anomaly detection (AD). The idea is to train a student model to produce representations of typical patterns, called "normality", and identify any discrepancies between the student and teacher models as anomalies. However, this approach suffers from a problem known as "normality forgetting". Even when trained on anomaly-free data, the student model is still able to reconstruct anomalous representations and is sensitive to fine patterns in normal data that were present during training. To address this issue, we propose a new framework called Memory-guided Knowledge-Distillation (MemKD) that adjusts the normality of student features in detecting anomalies.   In MemKD, we introduce a normality recall memory (NR Memory) that enhances the normality of student-generated features by recalling stored normal information. This ensures that the representations do not exhibit anomalies and accurately describe fine patterns. Additionally, we employ a normality embedding learning strategy to facilitate information learning for the NR Memory. This strategy creates a set of normal exemplars that the NR Memory can use to memorize prior knowledge from anomaly-free data and recall it when processing query features.   Through comprehensive experiments on five benchmark datasets, we demonstrate that MemKD achieves promising results in anomaly detection.