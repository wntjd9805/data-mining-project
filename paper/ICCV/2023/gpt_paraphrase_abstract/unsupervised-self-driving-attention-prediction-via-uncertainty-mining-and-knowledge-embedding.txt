The prediction of attention regions of interest is a crucial but difficult task for self-driving systems. Current methods rely on obtaining large-scale labeled traffic datasets, which is time-consuming. Additionally, the significant difference between natural scenes and traffic scenes in these datasets limits the effectiveness of model training. To overcome these challenges, we propose an unsupervised approach that utilizes uncertainty modeling and driving knowledge integration to predict self-driving attention. Our method includes an Uncertainty Mining Branch (UMB) that identifies similarities and differences among multiple pseudo-labels generated from models pre-trained on natural scenes by actively measuring uncertainty. Furthermore, our Knowledge Embedding Block (KEB) bridges the gap between domains by incorporating driving knowledge to adaptively refine the generated pseudo-labels. Our method achieves quantitative and qualitative results that are equivalent to or even more impressive than fully-supervised state-of-the-art approaches on three public datasets. This demonstrates the effectiveness of our proposed method and the potential of this research direction. The code for our method is available at https://github.com/zaplm/DriverAttention. Figure 1 illustrates our unsupervised self-driving attention prediction model, which avoids relying on ground truth labels from traffic datasets. Instead, it utilizes pseudo-labels generated from models pre-trained on natural scenes and refines the results through uncertainty mining and knowledge embedding. The pre-training stage is represented by the red dashed line, the training process is represented by the black dashed line, and the testing process is represented by the black solid line.