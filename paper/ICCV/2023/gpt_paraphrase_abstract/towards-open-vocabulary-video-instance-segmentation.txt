Video Instance Segmentation (VIS) is a technique used to identify and classify objects in videos, but it has limitations when it comes to handling new categories of objects. To address this issue, we propose a new task called Open-Vocabulary Video Instance Segmentation. This task involves segmenting, tracking, and classifying objects in videos, including categories that were not seen during training. To facilitate research in this area, we have created a dataset called Large-Vocabulary Video Instance Segmentation (LV-VIS), which contains well-annotated objects from 1,196 different categories. This dataset is significantly larger than existing datasets and allows for better evaluation and comparison of algorithms. Additionally, we have developed an efficient architecture called OV2Seg, which combines memory mechanisms with the Transformer model to achieve Open-Vocabulary VIS in real-time. Our experiments on LV-VIS and other datasets demonstrate the strong ability of OV2Seg to generalize to new categories. The dataset and code for OV2Seg are available for researchers to use.