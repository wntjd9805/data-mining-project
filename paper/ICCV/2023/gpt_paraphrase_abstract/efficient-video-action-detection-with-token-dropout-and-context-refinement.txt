We present an end-to-end framework called EVAD that efficiently detects actions in videos using vision transformers (ViTs). EVAD incorporates two specialized designs to enhance video action detection. Firstly, we introduce spatiotemporal token dropout, which preserves tokens related to actor motions from keyframes and drops out irrelevant tokens in the video clip. Secondly, we refine scene context by utilizing the remaining tokens to improve actor identity recognition. Our action detector expands the region of interest into the temporal domain, and the refined spatiotemporal actor identity representations are generated using a decoder with attention mechanism. EVAD achieves high efficiency while maintaining accuracy, as demonstrated on three benchmark datasets (AVA, UCF101-24, JHMDB). Compared to the vanilla ViT backbone, EVAD reduces overall GFLOPs by 43% and improves real-time inference speed by 40% with no performance degradation. Additionally, even at similar computational costs, EVAD improves performance by 1.1 mAP when using higher resolution inputs. The code for EVAD is available at https://github.com/MCG-NJU/EVAD.