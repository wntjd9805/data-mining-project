The challenge of integrating spatiotemporal features from different modalities into a single feature is a key issue in vision-based action recognition. This study introduces a novel approach called the 3D deformable transformer, which addresses this challenge by incorporating adaptive spatiotemporal receptive fields and a cross-modal learning scheme. The 3D deformable transformer consists of three attention modules: 3D deformability, local joint stride, and temporal stride attention. The cross-modal tokens are inputted into the 3D deformable attention module to generate a cross-attention token that reflects spatiotemporal correlation. The local joint stride attention combines spatial attention and pose tokens, while the temporal stride attention reduces the number of input tokens in the attention module and supports temporal expression learning without using all tokens simultaneously. The deformable transformer iterates L times and utilizes the last cross-modal token for classification. Experimental results on the NTU60, NTU120, FineGYM, and PennAction datasets demonstrate that the proposed 3D deformable transformer outperforms or achieves comparable results to pre-trained state-of-the-art methods, even without pre-training. Additionally, the study presents the potential for explainable action recognition by visualizing the important joints and correlations using spatial joint and temporal stride attention.