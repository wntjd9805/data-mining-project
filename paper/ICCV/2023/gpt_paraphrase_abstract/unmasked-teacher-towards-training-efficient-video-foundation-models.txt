Video Foundation Models (VFMs) have been underexplored due to computational constraints and limited data availability. Existing VFMs rely on Image Foundation Models (IFMs), which struggle to adapt to the video domain. Although VideoMAE has successfully trained a robust ViT with limited data, its low-level reconstruction presents challenges in convergence and conflicts with high-level cross-modal alignment. This study proposes a training-efficient approach for temporal-sensitive VFMs that combines the advantages of existing methods. To improve data efficiency, we mask out most low-semantics video tokens while selectively aligning the unmasked tokens with IFM, acting as the UnMasked Teacher (UMT). By incorporating semantic guidance, our method enables faster convergence and enhances multi-modal compatibility. Employing a progressive pre-training framework, our model demonstrates proficiency in various tasks such as scene-related, temporal-related, and complex video-language understanding. Remarkably, our scratch-built ViT-L/16 achieves state-of-the-art performance on diverse video tasks, employing only publicly available sources for pre-training within a span of 6 days using 32 A100 GPUs.