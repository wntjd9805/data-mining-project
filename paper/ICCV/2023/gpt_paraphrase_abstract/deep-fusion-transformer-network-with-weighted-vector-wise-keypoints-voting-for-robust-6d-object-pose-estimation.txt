The efficient integration of color and depth modalities is a critical challenge in estimating the pose of 6D objects from a single RGBD image. This study addresses this problem by introducing a novel Deep Fusion Transformer (DFTr) block that aggregates cross-modality features to improve pose estimation. Unlike existing fusion methods, the proposed DFTr leverages semantic similarity to better model cross-modality semantic correlation, resulting in improved integration of globally enhanced features from different modalities for better information extraction. To enhance robustness and efficiency, a weighted vector-wise voting algorithm is introduced, which uses a non-iterative global optimization strategy for precise 3D keypoint localization while achieving near real-time inference. Extensive experiments demonstrate the effectiveness and strong generalization capability of the proposed 3D keypoint voting algorithm. Results on four widely-used benchmarks indicate that our method outperforms state-of-the-art approaches by a significant margin. The code for our method is available at https://github.com/junzastar/DFTr Voting.