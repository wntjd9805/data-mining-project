Recently, there have been advancements in text-to-image generation models, particularly DALL-E and its diffusion model variants, which have demonstrated impressive results in producing realistic images. However, there is a lack of comprehensive evaluation methods for these models. In this study, we focus on assessing the visual reasoning abilities and social biases of various text-to-image models, including both multimodal transformer language models and diffusion models.   To evaluate visual reasoning skills, we introduce PAINTSKILLS, a diagnostic evaluation dataset that measures three key skills: object recognition, object counting, and spatial relation understanding. Despite the models' ability to generate high-quality images, there is still a significant gap between their performance and the ideal accuracy in object counting and spatial relation understanding.  Additionally, we analyze the gender and skin tone biases present in the generated images by examining their distribution across different professions and attributes. Our findings reveal that recent text-to-image models learn specific biases related to gender and skin tone from the web image-text pairs they are trained on.  We believe that this research will be instrumental in guiding future advancements in text-to-image generation models, particularly in improving their visual reasoning skills and promoting socially unbiased representations.