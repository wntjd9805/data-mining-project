This paper introduces I-ViT, an integer-only quantization scheme for Vision Transformers (ViTs). ViTs have shown impressive performance in computer vision tasks, but their deployment on edge devices is challenging due to their high storage and computational requirements. Quantization is a promising solution to reduce model complexity, and the dyadic arithmetic pipeline enables efficient integer-only inference. However, dyadic arithmetic is not applicable to the non-linear components in ViTs, posing a challenge for integer-only inference. To address this issue, I-ViT proposes a quantization scheme that allows ViTs to perform the entire computational graph of inference using integer arithmetic and bit-shifting, without any floating-point operations. Linear operations in ViTs follow the integer-only pipeline with dyadic arithmetic, while non-linear operations are approximated using lightweight integer-only arithmetic methods. Shiftmax and ShiftGELU are introduced as integer bit-shifting techniques to approximate non-linear operations like Softmax, GELU, and LayerNorm.The proposed I-ViT scheme is evaluated on various benchmark models, and the results demonstrate that integer-only INT8 quantization achieves comparable or slightly higher accuracy compared to full-precision (FP) models. Additionally, the authors utilize TVM for practical hardware deployment on GPU's integer arithmetic units, resulting in a 3.72∼4.11× inference speedup compared to the FP model. The code for both Pytorch and TVM implementations is made available on GitHub.Overall, I-ViT presents an integer-only quantization scheme for ViTs, addressing the challenges of deploying these models on edge devices while maintaining performance. The experimental results highlight the effectiveness of the proposed approach and its potential for practical hardware implementation.