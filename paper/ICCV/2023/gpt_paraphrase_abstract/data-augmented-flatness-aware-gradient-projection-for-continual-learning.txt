Continual learning (CL) aims to learn new tasks without forgetting previously learned ones. However, existing CL methods that rely on gradient projection suffer from poor performance on new tasks due to overly strong projection constraints. In this study, we propose a new approach called Data Augmented Flatness-aware Gradient Projection (DFGP) to address this issue. DFGP consists of three modules: data and weight perturbation, flatness-aware optimization, and gradient projection. Firstly, we perturb the task data and current weights to identify the worst-case scenario for task loss. Then, we optimize the loss and flatness of the loss surface based on the raw and worst-case perturbed data, resulting in a flatness-aware gradient. Finally, we update the network using the flatness-aware gradient in directions orthogonal to the subspace of old tasks. Experimental results on four datasets demonstrate that our method improves the flatness of the loss surface and enhances the performance of new tasks, achieving state-of-the-art accuracy across all tasks.