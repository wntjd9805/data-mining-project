The Neural Radiance Field (NeRF) is a method for generating novel views of a scene using implicit scene representation. However, NeRF suffers from scalability issues as it requires densely sampled images for each new scene. Previous studies have tried to address this problem by combining NeRF with the Multi-View Stereo (MVS) technique, but this approach still requires a complex fine-tuning process for new scenes. Without this fine-tuning, the rendering quality of the synthesized views drops significantly, particularly in high-frequency features.To overcome these limitations, we propose WaveNeRF, which combines wavelet frequency decomposition with MVS and NeRF to achieve high-quality synthesis without per-scene optimization. WaveNeRF preserves high-frequency information by integrating the discrete wavelet transform into the MVS process, explicitly separating high-frequency information. This disentangled frequency information is then injected into NeRF using a hybrid neural renderer, resulting in faithful representation of high-frequency details. Additionally, a frequency-guided sampling strategy is designed to reduce artifacts in high-frequency regions.Extensive experiments on three well-known benchmarks demonstrate that WaveNeRF outperforms existing methods in generalizable radiance field modeling, even when only three input images are provided.