Prompt-based learning has made significant progress in image class-incremental learning, but there is still a lack of exploration in the video domain. To address this gap, this paper proposes a space-time prompting approach called ST-Prompt, which utilizes a powerful image-language pre-trained model called CLIP to enable video class-incremental learning (VCIL). ST-Prompt consists of two types of prompts: task-specific prompts and task-agnostic prompts. The task-specific prompts aim to overcome the issue of catastrophic forgetting by learning multi-grained prompts, including spatial prompts, temporal prompts, and comprehensive prompts, to accurately identify tasks. On the other hand, the task-agnostic prompts maintain a globally-shared prompt pool that allows the exchange of contexts between frames, enabling the pre-trained image models to have temporal perception abilities. This approach allows ST-Prompt to transfer knowledge from image-language pre-trained models to the VCIL task with only a small set of prompts that need to be optimized. The effectiveness of ST-Prompt is evaluated through extensive experiments on three standard benchmarks. The results demonstrate that ST-Prompt outperforms state-of-the-art VCIL methods, achieving a significant improvement of 9.06% on the HMDB51 dataset under the 1 Ã— 25 stage setting.