HighData formatFixed-pointModel quantization has emerged as a mandatory tech-nique for efﬁcient inference with advanced Deep NeuralNetworks (DNN) by representing model parameters with fewer bits. Nevertheless, prior model quantization ei-ther suffers from the inefﬁcient data encoding method thus leading to noncompetitive model compression rate, or re-quires time-consuming quantization aware training pro-cess. In this work, we propose a novel Adaptive Floating-Point (AFP) as a variant of standard IEEE-754 ﬂoating-point format, with ﬂexible conﬁguration of exponent and mantissa segments. Leveraging the AFP for model quanti-zation (i.e., encoding the parameter) could signiﬁcantly en-hance the model compression rate without accuracy degra-dation and model re-training. We also want to highlight that our proposed AFP could effectively eliminate the com-putationally intensive de-quantization step existing in the dynamic quantization technique adopted by the famous ma-chine learning frameworks (e.g., pytorch, tensorRT, etc.).Moreover, we develop a framework to automatically opti-mize and choose the adequate AFP conﬁguration for each layer, thus maximizing the compression efﬁcacy. Our exper-iments indicate that AFP-encoded ResNet-50/MobileNet-v2 only has ∼0.04/0.6% accuracy degradation w.r.t its full-precision counterpart.It outperforms the state-of-the-art works by 1.1% in accuracy using the same bit-width while reducing the energy consumption by 11.2×, which is quite impressive for inference. Code is released at: https://github.com/MXHX7199/ICCV_2021_AFP 