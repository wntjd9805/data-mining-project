Physical-world adversarial attacks based on universal adversarial patches have been proved to be able to mis-lead deep convolutional neural networks (CNNs), expos-ing the vulnerability of real-world visual classification sys-tems based on CNNs. In this paper, we empirically reveal and mathematically explain that the universal adversarial patches usually lead to deep feature vectors with very large norms in popular CNNs.Inspired by this, we propose a simple yet effective defending approach using a new feature norm clipping (FNC) layer which is a differentiable module that can be flexibly inserted in different CNNs to adaptively suppress the generation of large norm deep feature vectors.FNC introduces no trainable parameter and only very low computational overhead. However, experiments on multiple datasets validate that it can effectively improve the robust-ness of different CNNs towards white-box universal patch attacks while maintaining a satisfactory recognition accu-racy for clean samples. 