This paper aims to explain adversarial attacks in terms of how adversarial perturbations contribute to the attack-ing task. We estimate attributions of different image re-gions to the decrease of the attacking cost based on theShapley value. We deÔ¨Åne and quantify interactions among adversarial perturbation pixels, and decompose the en-tire perturbation map into relatively independent pertur-bation components. The decomposition of the perturba-tion map shows that adversarially-trained DNNs have more perturbation components in the foreground than normally-trained DNNs. Moreover, compared to the normally-trainedDNN, the adversarially-trained DNN have more compo-nents which mainly decrease the score of the true category.Above analyses provide new insights into the understanding of adversarial attacks. 