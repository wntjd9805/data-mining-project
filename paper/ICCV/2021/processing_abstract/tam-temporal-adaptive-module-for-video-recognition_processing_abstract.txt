Video data is with complex temporal dynamics due to various factors such as camera motion, speed variation, and different activities. To effectively capture this diverse motion pattern, this paper presents a new temporal adap-tive module (TAM) to generate video-specific temporal ker-nels based on its own feature map. TAM proposes a unique two-level adaptive modeling scheme by decoupling the dy-namic kernel into a location sensitive importance map and a location invariant aggregation weight. The importance map is learned in a local temporal window to capture short-term information, while the aggregation weight is gener-ated from a global view with a focus on long-term struc-ture. TAM is a modular block and could be integrated into 2D CNNs to yield a powerful video architecture (TANet) with a very small extra computational cost. The exten-sive experiments on Kinetics-400 and Something-Something datasets demonstrate that our TAM outperforms other tem-poral modeling methods consistently, and achieves the state-of-the-art performance under the similar complex-ity. The code is available at https://github.com/ liu-zhy/temporal-adaptive-module. 