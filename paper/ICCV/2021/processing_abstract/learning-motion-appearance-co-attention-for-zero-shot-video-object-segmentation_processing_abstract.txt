How to make the appearance and motion information in-teract effectively to accommodate complex scenarios is a fundamental issue in flow-based zero-shot video object seg-In this paper, we propose an Attentive Multi-mentation.Modality Collaboration Network (AMC-Net) to utilize ap-pearance and motion information uniformly. Specifically,AMC-Net fuses robust information from multi-modality fea-tures and promotes their collaboration in two stages. First, we propose a Multi-Modality Co-Attention Gate (MCG) on the bilateral encoder branches, in which a gate func-tion is used to formulate co-attention scores for balancing the contributions of multi-modality features and suppress-ing the redundant and misleading information. Then, we propose a Motion Correction Module (MCM) with a visual-motion attention mechanism, which is constructed to em-phasize the features of foreground objects by incorporat-ing the spatio-temporal correspondence between appear-ance and motion cues. Extensive experiments on three pub-lic challenging benchmark datasets verify that our proposed network performs favorably against existing state-of-the-art methods via training with fewer data. The code is released at https://github.com/isyangshu/AMC-Net. 