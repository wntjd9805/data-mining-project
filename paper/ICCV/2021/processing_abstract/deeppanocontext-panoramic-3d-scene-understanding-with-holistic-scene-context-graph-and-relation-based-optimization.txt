Panorama images have a much larger field-of-view thus naturally encode enriched scene context information com-pared to standard perspective images, which however is not well exploited in the previous scene understanding methods.In this paper, we propose a novel method for panoramic 3D scene understanding which recovers the 3D room lay-out and the shape, pose, position, and semantic category for each object from a single full-view panorama image. In order to fully utilize the rich context information, we design a novel graph neural network based context model to pre-dict the relationship among objects and room layout, and a differentiable relationship-based optimization module to optimize object arrangement with well-designed objective functions on-the-fly. Realizing the existing data are either with incomplete ground truth or overly-simplified scene, we present a new synthetic dataset with good diversity in room layout and furniture placement, and realistic image quality for total panoramic 3D scene understanding. Experiments demonstrate that our method outperforms existing methods on panoramic scene understanding in terms of both geome-try accuracy and object arrangement. Code is available at https://chengzhag.github.io/publication/dpc. 