The recently proposed Visual image Transformers (ViT) with pure attention have achieved promising performance on image recognition tasks, such as image classiﬁcation.However, the routine of the current ViT model is to main-tain a full-length patch sequence during inference, which is redundant and lacks hierarchical representation. To this end, we propose a Hierarchical Visual Transformer (HVT) which progressively pools visual tokens to shrink the se-quence length and hence reduces the computational cost, analogous to the feature maps downsampling in Convolu-tional Neural Networks (CNNs). It brings a great beneﬁt that we can increase the model capacity by scaling dimen-sions of depth/width/resolution/patch size without introduc-ing extra computational complexity due to the reduced se-quence length. Moreover, we empirically ﬁnd that the av-erage pooled visual tokens contain more discriminative in-formation than the single class token. To demonstrate the improved scalability of our HVT, we conduct extensive ex-periments on the image classiﬁcation task. With compara-ble FLOPs, our HVT outperforms the competitive baselines on ImageNet and CIFAR-100 datasets. Code is available at https://github.com/MonashAI/HVT. 