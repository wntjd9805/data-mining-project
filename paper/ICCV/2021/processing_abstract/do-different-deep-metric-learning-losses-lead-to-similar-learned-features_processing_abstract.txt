Recent studies have shown that many deep metric learn-ing loss functions perform very similarly under the same experimental conditions. One potential reason for this un-expected result is that all losses let the network focus on similar image regions or properties. In this paper, we in-vestigate this by conducting a two-step analysis to extract and compare the learned visual features of the same model architecture trained with different loss functions: First, we compare the learned features on the pixel level by corre-lating saliency maps of the same input images. Second, we compare the clustering of embeddings for several image properties, e.g. object color or illumination. To provide in-dependent control over these properties, photo-realistic 3D car renders similar to images in the Cars196 dataset are generated. In our analysis, we compare 14 pretrained mod-els from a recent study and find that, even though all mod-els perform similarly, different loss functions can guide the model to learn different features. We especially find differ-ences between classification and ranking based losses. Our analysis also shows that some seemingly irrelevant prop-erties can have significant influence on the resulting em-bedding. We encourage researchers from the deep metric learning community to use our methods to get insights into the features learned by their proposed methods.Figure 1. Given the standard DML setting with a neural network that maps input images to an embedding space (grey box), we pro-pose two analysis methods. First, we identify pixels that are im-portant for the network to create the embedding. We then com-pare them qualitatively and quantitatively between loss functions.Second, we investigate the influence of image properties on the clustering behavior in the embedding space and compare them be-tween loss functions. 