Image captioning is an important task for benchmarking  visual reasoning and for enabling accessibility for people  with  vision  impairments.  However,  as  in  many  machine  learning  settings,  social  biases  can  infuence  image  cap-tioning  in  undesirable  ways.  In  this  work,  we  study  bias  propagation  pathways  within  image  captioning,  focusing  specifcally  on  the  COCO  dataset.  Prior  work  has  ana-lyzed gender bias in captions using automatically-derived  gender  labels;  here  we  examine  racial  and  intersectional  biases using manual annotations.  Our frst contribution is  in annotating the perceived gender and skin color of 28,315  of  the  depicted  people  after  obtaining  IRB  approval.  Us-ing these annotations, we compare racial biases present in  both manual and automatically-generated image captions. We  demonstrate  differences  in  caption  performance,  sen-timent,  and word choice between images of lighter versus  darker-skinned people.  Further,  we fnd the magnitude of  these  differences  to  be  greater  in  modern  captioning  sys-tems compared to older ones, thus leading to concerns that  without  proper  consideration  and  mitigation  these  differ-ences  will  only  become  increasingly  prevalent.  Code  and  data is available at https://princetonvisualai.  github.io/imagecaptioning-bias/.  