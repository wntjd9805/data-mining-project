Deepfakes (”deep learning” + ”fake”) are videos syn-thetically generated with AI algorithms. While they could be entertaining, they could also be misused for falsifying speeches and spreading misinformation. The process to cre-ate deepfakes involves both visual and auditory manipula-tions. Exploration on detecting visual deepfakes has pro-duced a number of detection methods as well as datasets, while audio deepfakes (e.g. synthetic speech from text-to-speech or voice conversion systems) and the relationship between the video and audio modalities have been relatively neglected. In this work, we propose a novel visual / audi-tory deepfake joint detection task and show that exploiting the intrinsic synchronization between the visual and audi-tory modalities could benefit deepfake detection. Experi-ments demonstrate that the proposed joint detection frame-work outperforms independently trained models, and at the same time, yields superior generalization capability on un-seen types of deepfakes. 