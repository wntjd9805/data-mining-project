Given only a few available images for a novel unseen category, few-shot image generation aims to generate more data for this category. Previous works attempt to glob-ally fuse these images by using adjustable weighted coef-ﬁcients. However, there is a serious semantic misalignment between different images from a global perspective, mak-ing these works suffer from poor generation quality and di-versity. To tackle this problem, we propose a novel Local-Fusion Generative Adversarial Network (LoFGAN) for few-shot image generation.Instead of using these available images as a whole, we ﬁrst randomly divide them into a base image and several reference images. Next, LoFGAN matches local representations between the base and refer-ence images based on semantic similarities, and replaces the local features with the closest related local features. In this way, LoFGAN can produce more realistic and diverse images at a more ﬁne-grained level, and simultaneously enjoy the characteristic of semantic alignment. Further-more, a local reconstruction loss is also proposed, which can provide better training stability and generation quality.We conduct extensive experiments on three datasets, which successfully demonstrates the effectiveness of our proposed method for few-shot image generation and downstream vi-sual applications with limited data. Code is available at https://github.com/edward3862/LoFGAN-pytorch. 