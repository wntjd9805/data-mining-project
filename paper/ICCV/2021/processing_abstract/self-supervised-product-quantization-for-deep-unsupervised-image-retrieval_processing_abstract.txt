Supervised deep learning-based hash and vector quan-tization are enabling fast and large-scale image retrieval systems. By fully exploiting label annotations, they are achieving outstanding retrieval performances compared to the conventional methods. However, it is painstak-ing to assign labels precisely for a vast amount of train-ing data, and also, the annotation process is error-prone.To tackle these issues, we propose the first deep unsu-pervised image retrieval method dubbed Self-supervisedProduct Quantization (SPQ) network, which is label-free and trained in a self-supervised manner. We design a CrossQuantized Contrastive learning strategy that jointly learns codewords and deep visual descriptors by comparing in-dividually transformed images (views). Our method ana-lyzes the image contents to extract descriptive features, al-lowing us to understand image representations for accurate retrieval. By conducting extensive experiments on bench-marks, we demonstrate that the proposed method yields state-of-the-art results even without supervised pretraining. 