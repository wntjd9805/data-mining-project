Deep neural networks have achieved remarkable perfor-mance on a range of classification tasks, with softmax cross-entropy (CE) loss emerging as the de-facto objective func-tion. The CE loss encourages features of a class to have a higher projection score on the true class-vector compared to the negative classes. However, this is a relative constraint and does not explicitly force different class features to be well-separated. Motivated by the observation that ground-truth class representations in CE loss are orthogonal (one-hot encoded vectors), we develop a novel loss function termed ‘Orthogonal Projection Loss’ (OPL) which imposes orthogonality in the feature space. OPL augments the prop-erties of CE loss and directly enforces inter-class separation alongside intra-class clustering in the feature space through orthogonality constraints on the mini-batch level. As com-pared to other alternatives of CE, OPL offers unique ad-vantages e.g., no additional learnable parameters, does not require careful negative mining and is not sensitive to the batch size. Given the plug-and-play nature of OPL, we eval-uate it on a diverse range of tasks including image recog-nition (CIFAR-100), large-scale classification (ImageNet), domain generalization (PACS) and few-shot learning (mini-ImageNet, CIFAR-FS, tiered-ImageNet and Meta-dataset) and demonstrate its effectiveness across the board. Further-more, OPL offers better robustness against practical nui-sances such as adversarial attacks and label noise. Code is available at: https://github.com/kahnchana/opl. 