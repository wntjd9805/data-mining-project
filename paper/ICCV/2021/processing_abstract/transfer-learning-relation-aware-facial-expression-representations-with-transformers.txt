Facial expression recognition (FER) has received in-creasing interest in computer vision. We propose the Trans-FER model which can learn rich relation-aware local rep-resentations. It mainly consists of three components: Multi-Attention Dropping (MAD), ViT-FER, and Multi-head Self-Attention Dropping (MSAD). First, local patches play an important role in distinguishing various expressions, how-ever, few existing works can locate discriminative and di-verse local patches. This can cause serious problems when some patches are invisible due to pose variations or view-point changes. To address this issue, the MAD is proposed to randomly drop an attention map. Consequently, models are pushed to explore diverse local patches adaptively. Sec-ond, to build rich relations between different local patches, the Vision Transformers (ViT) are used in FER, called ViT-FER. Since the global scope is used to reinforce each local patch, a better representation is obtained to boost the FER performance. Thirdly, the multi-head self-attention allowsViT to jointly attend to features from different information subspaces at different positions. Given no explicit guid-ance, however, multiple self-attentions may extract similar relations. To address this, the MSAD is proposed to ran-domly drop one self-attention module. As a result, mod-els are forced to learn rich relations among diverse local patches. Our proposed TransFER model outperforms the state-of-the-art methods on several FER benchmarks, show-ing its effectiveness and usefulness.*The first two authors contributed equally. This work was done whenFanglei Xue and Qiangchang Wang were interns at IDL, Baidu Research.â€ Corresponding authorFigure 1. Attention visualizations [5] on two example images: Sur-prise (Row 1) and Anger (Row 2). Column 1: Original images.Column 2: Attention visualizations of our ViT-FER model. Col-umn 3: Attention visualization of our TransFER model. 