We present pure-transformer based models for video classiﬁcation, drawing upon the recent success of such mod-els in image classiﬁcation. Our model extracts spatio-temporal tokens from the input video, which are then en-coded by a series of transformer layers. In order to han-dle the long sequences of tokens encountered in video, we propose several, efﬁcient variants of our model which fac-torise the spatial- and temporal-dimensions of the input. Al-though transformer-based models are known to only be ef-fective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough abla-tion studies, and achieve state-of-the-art results on multiple video classiﬁcation benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. 