Capsule Networks (CapsNets) create internal represen-tations by parsing inputs into various instances at differ-ent resolution levels via a two-phase process â€“ part-whole transformation and hierarchical component routing. Since both of these internal phases are computationally expen-sive, CapsNet have not found wider use. Existing varia-tions of CapsNets mainly focus on performance compari-son with the original CapsNet, and have not outperformedCNN-based models on complex tasks. To address the limita-tions of the existing CapsNet structures, we propose a novelPrediction-Tuning Capsule Network (PT-CapsNet), and also introduce fully connected PT-Capsules (FC-PT-Caps) and locally connected PT-Capsules (LC-PT-Caps). Differ-ent from existing CapsNet structures, our proposed model (i) allows the use of capsules for more difficult vision tasks and provides wider applicability; and (ii) provides better than or comparable performance to CNN-based baselines on these complex tasks. In our experiments, we show ro-bustness to affine transformations, as well as the lightweight and scalability of PT-CapsNet via constructing larger and deeper networks and performing comparisons on classifica-tion, semantic segmentation and object detection tasks. The results show consistent performance improvement and sig-nificant parameter reduction compared to various baseline models. Code is available at https://github.com/Christinepan881/PT-CapsNet.git. 