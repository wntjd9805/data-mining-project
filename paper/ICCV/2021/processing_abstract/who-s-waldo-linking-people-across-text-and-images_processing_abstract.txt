We present a task and benchmark dataset for person-centric visual grounding, the problem of linking between people named in a caption and people pictured in an im-age. In contrast to prior work in visual grounding, which is predominantly object-based, our new task masks out the names of people in captions in order to encourage meth-ods trained on such image–caption pairs to focus on con-textual cues, such as the rich interactions between mul-tiple people, rather than learning associations between names and appearances. To facilitate this task, we introduce a new dataset, Who’s Waldo, mined automatically from image–caption data on Wikimedia Commons. We propose aTransformer-based method that outperforms several strong baselines on this task, and release our data to the research community to spur work on contextual models that consider both vision and language. Code and data are available at: https://whoswaldo.github.io 