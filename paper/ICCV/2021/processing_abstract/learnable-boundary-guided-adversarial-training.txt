Previous adversarial training raises model robustness under the compromise of accuracy on natural data. In this paper, we reduce natural accuracy degradation. We use the model logits from one clean model to guide learning of an-other one robust model, taking into consideration that logits from the well trained clean model embed the most discrim-inative features of natural data, e.g., generalizable classi-fier boundary. Our solution is to constrain logits from the robust model that takes adversarial examples as input and makes it similar to those from the clean model fed with cor-responding natural data. It lets the robust model inherit the classifier boundary of the clean model. Moreover, we ob-serve such boundary guidance can not only preserve high natural accuracy but also benefit model robustness, which gives new insights and facilitates progress for the adversar-ial community. Finally, extensive experiments on CIFAR-10,CIFAR-100, and Tiny ImageNet testify to the effectiveness of our method. We achieve new state-of-the-art robustness onCIFAR-100 without additional real or synthetic data with auto-attack benchmark 1. Our code is available at https://github.com/dvlab-research/LBGAT. 