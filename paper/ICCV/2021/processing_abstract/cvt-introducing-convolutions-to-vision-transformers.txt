We present in this paper a new architecture, named Con-volutional vision Transformer (CvT), that improves VisionTransformer (ViT) in performance and efficiency by intro-ducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifica-tions: a hierarchy of Transformers containing a new convo-lutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural net-works (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits ofTransformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting ex-tensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transform-ers and ResNets on ImageNet-1k, with fewer parame-ters and lower FLOPs.In addition, performance gains are maintained when pretrained on larger datasets (e.g.ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 ac-curacy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial com-ponent in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher res-olution vision tasks. Code will be released at https://github.com/microsoft/CvT. 