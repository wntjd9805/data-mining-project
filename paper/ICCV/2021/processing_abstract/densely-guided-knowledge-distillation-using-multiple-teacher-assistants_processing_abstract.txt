With the success of deep neural networks, knowledge dis-tillation which guides the learning of a small student net-work from a large teacher network is being actively stud-ied for model compression and transfer learning. How-ever, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ.In this paper, we propose a densely guided knowledge distillation using mul-tiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifi-cally, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previ-ous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teach-ing of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources.We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and Ima-geNet. We also achieved significant performance improve-ments with various backbone architectures such as ResNet,WideResNet, and VGG. 1Figure 1. Problem definition of the large gap between a teacher and a student network. (a) In general, the difference between layers at KD is approximately 1.8 times, but (b) we are interested in the challenging problem of layer differences of more than 5 times. For solving this problem, TAKD [23] has been proposed.However, (c) TAKD has a fundamental limitation such as the er-ror avalanche problem. Assuming that a unique error occurs one by one when a higher-level teacher assistant (TA) teaches a lower-level TA. The error case continues to increase whenever teaching more TAs. Meanwhile, in (d), the proposed densely guided knowl-edge distillation can be relatively free from this error avalanche problem because it does not teach TAs at each level alone. 