Training vision-based Autonomous driving models is a challenging problem with enormous practical implications.One of the main challenges is the requirement of storage and processing of vast volumes of (possibly redundant)In this paper, we study the problem driving video data. of data-efficient training of autonomous driving systems.We argue that in the context of an edge-device deploy-ment, multi-criteria online video frame subset selection is an appropriate technique for developing such frameworks.We study existing convex optimization based solutions and show that they are unable to provide solution with high weightage to loss of selected video frames. We design a novel multi-criteria online subset selection algorithm, TM-COSS, which uses a thresholded concave function of selec-tion variables. Extensive experiments using driving simula-tor CARLA show that we are able to drop 80% of the frames, while succeeding to complete 100% of the episodes. We also show that TMCOSS improves performance on the cru-cial affordance “Relative Angle” during turns, on inclusion of bucket-specific relative angle loss (BL), leading to selec-tion of more frames in those parts. TMCOSS also achieves an 80% reduction in number of training video frames, on real-world videos from the standard BDD and Cityscapes datasets, for the tasks of drivable area segmentation, and semantic segmentation. 