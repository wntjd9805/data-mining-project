The key point of language-guided person search is to construct the cross-modal association between visual and textual input. Existing methods focus on designing mul-timodal attention mechanisms and novel cross-modal loss functions to learn such association implicitly. We propose a representation learning method for language-guided person search based on color reasoning (LapsCore).It can ex-plicitly build a ﬁne-grained cross-modal association bidi-rectionally. Speciﬁcally, a pair of dual sub-tasks, image colorization and text completion, is designed. In the for-mer task, rich text information is learned to colorize gray images, and the latter one requests the model to understand the image and complete color word vacancies in the cap-tions. The two sub-tasks enable models to learn correct alignments between text phrases and image regions, so that rich multimodal representations can be learned. Extensive experiments on multiple datasets demonstrate the effective-ness and superiority of the proposed method. 