Recently, machine learning models have demonstrated to be vulnerable to backdoor attacks, primarily due to the lack of transparency in black-box models such as deep neural networks. A third-party model can be poisoned such that it works adequately in normal conditions but behaves mali-ciously on samples with specific trigger patterns. However, the trigger injection function is manually defined in most ex-isting backdoor attack methods, e.g., placing a small patch of pixels on an image or slightly deforming the image before poisoning the model. This results in a two-stage approach with a sub-optimal attack success rate and a lack of com-plete stealthiness under human inspection.In this paper, we propose a novel and stealthy back-door attack framework, LIRA, which jointly learns the op-timal, stealthy trigger injection function and poisons the model. We formulate such an objective as a non-convex, constrained optimization problem. Under this optimization framework, the trigger generator function will learn to ma-nipulate the input with imperceptible noise to preserve the model performance on the clean data and maximize the at-tack success rate on the poisoned data. Then, we solve this challenging optimization problem with an efficient, two-stage stochastic optimization procedure. Finally, the pro-posed attack framework achieves 100% success rates in sev-eral benchmark datasets, including MNIST, CIFAR10, GT-SRB, and T-ImageNet, while simultaneously bypassing ex-isting backdoor defense methods and human inspection. 