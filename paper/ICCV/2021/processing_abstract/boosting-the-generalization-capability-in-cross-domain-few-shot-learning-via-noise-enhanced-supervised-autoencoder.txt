State of the art (SOTA) few-shot learning (FSL) methods suffer signiﬁcant performance drop in the presence of do-main differences between source and target datasets. The strong discrimination ability on the source dataset does not necessarily translate to high classiﬁcation accuracy on the target dataset. In this work, we address this cross-domain few-shot learning (CDFSL) problem by boosting the gen-eralization capability of the model. Speciﬁcally, we teach the model to capture broader variations of the feature dis-tributions with a novel noise-enhanced supervised autoen-coder (NSAE). NSAE trains the model by jointly recon-structing inputs and predicting the labels of inputs as well as their reconstructed pairs. Theoretical analysis based on intra-class correlation (ICC) shows that the feature em-beddings learned from NSAE have stronger discrimination and generalization abilities in the target domain. We also take advantage of NSAE structure and propose a two-stepﬁne-tuning procedure that achieves better adaption and im-proves classiﬁcation performance in the target domain. Ex-tensive experiments and ablation studies are conducted to demonstrate the effectiveness of the proposed method. Ex-perimental results show that our proposed method consis-tently outperforms SOTA methods under various conditions. 