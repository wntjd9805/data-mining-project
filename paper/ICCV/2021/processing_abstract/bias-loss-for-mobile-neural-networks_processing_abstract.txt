Compact convolutional neural networks (CNNs) have witnessed exceptional improvements in performance in re-cent years. However, they still fail to provide the same pre-dictive power as CNNs with a large number of parameters.The diverse and even abundant features captured by the lay-ers is an important characteristic of these successful CNNs.However, differences in this characteristic between largeCNNs and their compact counterparts have rarely been in-vestigated. In compact CNNs, due to the limited number of parameters, abundant features are unlikely to be obtained, and feature diversity becomes an essential characteristic.Diverse features present in the activation maps derived from a data point during model inference may indicate the pres-ence of a set of unique descriptors necessary to distin-guish between objects of different classes. In contrast, data points with low feature diversity may not provide a sufficient amount of unique descriptors to make a valid prediction; we refer to them as random predictions. Random predictions can negatively impact the optimization process and harm the final performance. This paper proposes addressing the problem raised by random predictions by reshaping the standard cross-entropy to make it biased toward data points with a limited number of unique descriptive features. Our novel Bias Loss focuses the training on a set of valuable data points and prevents the vast number of samples with poor learning features from misleading the optimization process. Furthermore, to show the importance of diversity, we present a family of SkipblockNet models whose archi-tectures are brought to boost the number of unique descrip-tors in the last layers. Experiments conducted on bench-mark datasets demonstrate the superiority of the proposed loss function over the cross-entropy loss. Moreover, ourSkipblockNet-M can achieve 1% higher classification accu-racy than MobileNetV3 Large with similar computationalFigure 1. Accuracy v.s. FLOPs on ImageNet. Our SkipblockNet model trained with the proposed bias loss outperforms previous well-performing compact neural networks trained with the cross-entropy loss. cost on the ImageNet ILSVRC-2012 classification dataset.The code is available on the link - https://github. com/lusinlu/biasloss_skipblocknet. 