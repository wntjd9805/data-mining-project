Adversarial training (AT) has become the de-facto stan-dard to obtain models robust against adversarial exam-ples. However, AT exhibits severe robust overﬁtting: cross-entropy loss on adversarial examples, so-called robust loss, decreases continuously on training examples, while even-tually increasing on test examples. In practice, this leads to poor robust generalization, i.e., adversarial robustness does not generalize well to new examples.In this paper, we study the relationship between robust generalization andﬂatness of the robust loss landscape in weight space, i.e., whether robust loss changes signiﬁcantly when perturbing weights. To this end, we propose average- and worst-case metrics to measure ﬂatness in the robust loss landscape and show a correlation between good robust generaliza-tion and ﬂatness. For example, throughout training, ﬂat-ness reduces signiﬁcantly during overﬁtting such that early stopping effectively ﬁnds ﬂatter minima in the robust loss landscape. Similarly, AT variants achieving higher adver-sarial robustness also correspond to ﬂatter minima. This holds for many popular choices, e.g., AT-AWP, TRADES,MART, AT with self-supervision or additional unlabeled ex-amples, as well as simple regularization techniques, e.g.,AutoAugment, weight decay or label noise. For fair com-parison across these approaches, our ﬂatness measures are speciﬁcally designed to be scale-invariant and we conduct extensive experiments to validate our ﬁndings. 