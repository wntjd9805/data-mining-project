This paper introduces Click to Move (C2M), a novel framework for video generation where the user can con-trol the motion of the synthesized video through mouse clicks specifying simple object trajectories of the key ob-jects in the scene. Our model receives as input an ini-tial frame, its corresponding segmentation map and the sparse motion vectors encoding the input provided by theIt outputs a plausible video sequence starting from user. the given frame and with a motion that is consistent with user input. Notably, our proposed deep architecture incor-porates a Graph Convolution Network (GCN) modelling the movements of all the objects in the scene in a holis-tic manner and effectively combining the sparse user mo-tion information and image features. Experimental results show that C2M outperforms existing methods on two pub-licly available datasets, thus demonstrating the effective-ness of our GCN framework at modelling object interac-tions. The source code is publicly available at https://github.com/PierfrancescoArdino/C2M . 