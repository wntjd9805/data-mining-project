Recently, several frameworks for self-supervised learn-ing of 3D scene flow on point clouds have emerged. Scene flow inherently separates every scene into multiple moving agents and a large class of points following a single rigid sensor motion. However, existing methods do not lever-age this property of the data in their self-supervised train-ing routines which could improve and stabilize flow predic-tions. Based on the discrepancy between a robust rigid ego-motion estimate and a raw flow prediction, we generate a self-supervised motion segmentation signal. The predicted motion segmentation, in turn, is used by our algorithm to attend to stationary points for aggregation of motion infor-mation in static parts of the scene. We learn our model end-to-end by backpropagating gradients through Kabschâ€™s algorithm and demonstrate that this leads to accurate ego-motion which in turn improves the scene flow estimate. Us-ing our method, we show state-of-the-art results across mul-tiple scene flow metrics for different real-world datasets, showcasing the robustness and generalizability of this ap-proach. We further analyze the performance gain when per-forming joint motion segmentation and scene flow in an ab-lation study. We also present a novel network architecture for 3D LiDAR scene flow which is capable of handling an order of magnitude more points during training than previ-ously possible. 