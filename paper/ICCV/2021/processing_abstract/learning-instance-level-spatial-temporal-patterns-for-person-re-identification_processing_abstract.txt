Person re-identiﬁcation (Re-ID) aims to match pedes-trians under dis-joint cameras. Most Re-ID methods for-mulate it as visual representation learning and image search, and its accuracy is consequently affected greatly by the search space.Spatial-temporal information has been proven to be efﬁcient to ﬁlter irrelevant negative sam-ples and signiﬁcantly improve Re-ID accuracy. However, existing spatial-temporal person Re-ID methods are still rough and do not exploit spatial-temporal information sufﬁ-ciently. In this paper, we propose a novel Instance-level andSpatial-Temporal Disentangled Re-ID method (InSTD), to improve Re-ID accuracy. In our proposed framework, per-sonalized information such as moving direction is explic-itly considered to further narrow down the search space.Besides, the spatial-temporal transferring probability is disentangled from joint distribution to marginal distribu-tion, so that outliers can also be well modeled. Abun-dant experimental analyses are presented, which demon-strates the superiority and provides more insights into our method. The proposed method achieves mAP of 90.8% on Market-1501 and 89.1% on DukeMTMC-reID, improv-ing from the baseline 82.2% and 72.7%, respectively. Be-in order to provide a better benchmark for per-sides, son re-identiﬁcation, we release a cleaned data list ofDukeMTMC-reID with this paper: https://github. com/RenMin1991/cleaned-DukeMTMC-reID/ 