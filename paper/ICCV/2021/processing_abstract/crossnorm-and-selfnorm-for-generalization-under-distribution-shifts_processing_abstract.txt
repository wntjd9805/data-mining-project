Traditional normalization techniques (e.g., Batch Nor-malization and Instance Normalization) generally and sim-plistically assume that training and test data follow the same distribution. As distribution shifts are inevitable in real-world applications, well-trained models with previous normalization methods can perform badly in new environ-ments. Can we develop new normalization methods to im-prove generalization robustness under distribution shifts?In this paper, we answer the question by proposing Cross-Norm and SelfNorm. CrossNorm exchanges channel-wise mean and variance between feature maps to enlarge train-ing distribution, while SelfNorm uses attention to recal-ibrate the statistics to bridge gaps between training and test distributions. CrossNorm and SelfNorm can comple-ment each other, though exploring different directions in statistics usage. Extensive experiments on different ﬁelds (vision and language), tasks (classiﬁcation and segmenta-tion), settings (supervised and semi-supervised), and dis-tribution shift types (synthetic and natural) show the effec-tiveness. Code is available at https://github.com/ amazon-research/crossnorm-selfnorm 