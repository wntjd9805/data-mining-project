Beneﬁting from large-scale pre-training, we have wit-nessed signiﬁcant performance boost on the popular VisualQuestion Answering (VQA) task. Despite rapid progress, it remains unclear whether these state-of-the-art (SOTA) models are robust when encountering examples in the wild.To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adver-sarial human-and-model-in-the-loop procedure. Through this new benchmark, we discover several interesting ﬁnd-ings. (i) Surprisingly, we ﬁnd that during dataset collection, non-expert annotators can easily attack SOTA VQA mod-els successfully. (ii) Both large-scale pre-trained models and adversarial training methods achieve far worse perfor-mance on the new benchmark than over standard VQA v2 dataset, revealing the fragility of these models while demon-strating the effectiveness of our adversarial dataset. (iii)When used for data augmentation, our dataset can effec-tively boost model performance on other robust VQA bench-marks. We hope our Adversarial VQA dataset can shed new light on robustness study in the community and serve as a valuable benchmark for future work. 