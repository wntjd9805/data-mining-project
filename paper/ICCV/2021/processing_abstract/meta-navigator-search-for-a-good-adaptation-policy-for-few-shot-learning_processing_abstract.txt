Few-shot learning aims to adapt knowledge learned from previous tasks to novel tasks with only a limited amount of labeled data. Research literature on few-shot learning ex-hibits great diversity, while different algorithms often ex-cel at different few-shot learning scenarios. It is therefore tricky to decide which learning strategies to use under dif-ferent task conditions. Inspired by the recent success in Au-tomated Machine Learning literature (AutoML), in this pa-per, we present Meta Navigator, a framework that attempts to solve the aforementioned limitation in few-shot learning by seeking a higher-level strategy and proffer to automate the selection from various few-shot learning designs. The goal of our work is to search for good parameter adapta-tion policies that are applied to different stages in the net-work for few-shot classification. We present a search space that covers many popular few-shot learning algorithms in the literature, and develop a differentiable searching and decoding algorithm based on meta-learning that supports gradient-based optimization. We demonstrate the effective-ness of our searching-based method on multiple benchmark datasets. Extensive experiments show that our approach significantly outperforms baselines and demonstrates per-formance advantages over many state-of-the-art methods. 