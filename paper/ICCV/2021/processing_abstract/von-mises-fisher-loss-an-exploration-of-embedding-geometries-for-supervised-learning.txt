Recent work has argued that classiﬁcation losses utiliz-ing softmax cross-entropy are superior not only for ﬁxed-set classiﬁcation tasks, but also by outperforming losses de-veloped speciﬁcally for open-set tasks including few-shot learning and retrieval. Softmax classiﬁers have been stud-ied using different embedding geometries—Euclidean, hy-perbolic, and spherical—and claims have been made about the superiority of one or another, but they have not been systematically compared with careful controls. We conduct an empirical investigation of embedding geometry on soft-max losses for a variety of ﬁxed-set classiﬁcation and im-age retrieval tasks. An interesting property observed for the spherical losses lead us to propose a probabilistic classiﬁer based on the von Mises–Fisher distribution, and we show that it is competitive with state-of-the-art methods while producing improved out-of-the-box calibration. We provide guidance regarding the trade-offs between losses and how to choose among them. 