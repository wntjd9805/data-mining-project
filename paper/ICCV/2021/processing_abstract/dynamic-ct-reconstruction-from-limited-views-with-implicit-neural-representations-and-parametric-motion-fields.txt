Reconstructing dynamic, time-varying scenes with com-puted tomography (4D-CT) is a challenging and ill-posed problem common to industrial and medical settings. Ex-isting 4D-CT reconstructions are designed for sparse sam-pling schemes that require fast CT scanners to capture multiple, rapid revolutions around the scene in order to generate high quality results. However, if the scene is moving too fast, then the sampling occurs along a lim-ited view and is difﬁcult to reconstruct due to spatiotem-In this work, we design a reconstruc-poral ambiguities. tion pipeline using implicit neural representations coupled with a novel parametric motion ﬁeld warping to perform limited view 4D-CT reconstruction of rapidly deforming scenes. Importantly, we utilize a differentiable analysis-by-synthesis approach to compare with captured x-ray sino-gram data in a self-supervised fashion. Thus, our result-ing optimization method requires no training data to re-construct the scene. We demonstrate that our proposed system robustly reconstructs scenes containing deformable and periodic motion and validate against state-of-the-art baselines. Further, we demonstrate an ability to recon-struct continuous spatiotemporal representations of our scenes and upsample them to arbitrary volumes and frame rates post-optimization. This research opens a new av-enue for implicit neural representations in computed to-mography reconstruction in general. Code is available at https://github.com/awreed/DynamicCTReconstruction. 