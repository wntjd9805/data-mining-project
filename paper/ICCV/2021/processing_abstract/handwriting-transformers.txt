We propose a novel transformer-based styled handwrit-ten text image generation approach, HWT, that strives to learn both style-content entanglement as well as global and local style patterns. The proposed HWT captures the long and short range relationships within the style exam-ples through a self-attention mechanism, thereby encoding both global and local style patterns. Further, the proposed transformer-based HWT comprises an encoder-decoder at-tention that enables style-content entanglement by gather-ing the style features of each query character. To the best of our knowledge, we are the first to introduce a transformer-based network for styled handwritten text generation.Our proposed HWT generates realistic styled hand-written text images and outperforms the state-of-the-art demonstrated through extensive qualitative, quantitative and human-based evaluations. The proposed HWT can handle arbitrary length of text and any desired writing style in a few-shot setting. Further, our HWT general-izes well to the challenging scenario where both words and writing style are unseen during training, generating realistic styled handwritten text images. Code is avail-able at: https://github.com/ankanbhunia/Handwriting-Transformers 