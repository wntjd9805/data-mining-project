Recent methods for visual question answering rely on large-scale annotated datasets. Manual annotation of ques-tions and answers for videos, however, is tedious, expen-sive and prevents scalability. In this work, we propose to avoid manual annotation and generate a large-scale train-ing dataset for video question answering making use of au-tomatic cross-modal supervision. We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video nar-rations. Given narrated videos, we then automatically gen-erate the HowToVQA69M dataset with 69M video-question-answer triplets. To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer. We intro-duce the zero-shot VideoQA task and show excellent results, in particular for rare answers. Furthermore, we demon-strate our method to significantly outperform the state of the art on MSRVTT-QA, MSVD-QA, ActivityNet-QA andHow2QA. Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language bi-ases and high-quality redundant manual annotations. 