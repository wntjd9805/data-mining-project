In this paper, we present a neat yet effective transformer-based framework for visual grounding, namely TransVG, to address the task of grounding a language query to the corresponding region onto an image. The state-of-the-art methods, including two-stage or one-stage ones, rely on a complex module with manually-designed mechanisms to perform the query reasoning and multi-modal fusion. How-ever, the involvement of certain mechanisms in fusion mod-ule design, such as query decomposition and image scene graph, makes the models easily overﬁt to datasets with spe-ciﬁc scenarios, and limits the plenitudinous interaction be-tween the visual-linguistic context. To avoid this caveat, we propose to establish the multi-modal correspondence by leveraging transformers, and empirically show that the complex fusion modules (e.g., modular attention network, dynamic graph, and multi-modal tree) can be replaced by a simple stack of transformer encoder layers with higher per-formance. Moreover, we re-formulate the visual grounding as a direct coordinates regression problem and avoid mak-ing predictions out of a set of candidates (i.e., region pro-posals or anchor boxes). Extensive experiments are con-ducted on ﬁve widely used datasets, and a series of state-of-the-art records are set by our TransVG. We build the benchmark of transformer-based visual grounding frame-work and make the code available at https://github. com/djiajunustc/TransVG. 