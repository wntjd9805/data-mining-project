In this paper, we seek reasons for the two major failure cases in Semantic Segmentation (SS): 1) missing small ob-jects or minor object parts, and 2) mislabeling minor parts of large objects as wrong classes. We have an interesting finding that Failure-1 is due to the underuse of detailed fea-tures and Failure-2 is due to the underuse of visual contexts.To help the model learn a better trade-off, we introduce sev-eral Self-Regulation (SR) losses for training SS neural net-works. By “self”, we mean that the losses are from the model per se without using any additional data or super-vision. By applying the SR losses, the deep layer features are regulated by the shallow ones to preserve more details; meanwhile, shallow layer classification logits are regulated by the deep ones to capture more semantics. We conduct ex-tensive experiments on both weakly and fully supervised SS tasks, and the results show that our approach consistently surpasses the baselines. We also validate that SR losses are easy to implement in various state-of-the-art SS models, e.g., SPGNet [7] and OCRNet [62], incurring little com-putational overhead during training and none for testing1. 