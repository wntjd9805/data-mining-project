to the target domain.Domain generalization (DG) aims to generalize a model trained on multiple source (i.e., training) domains to a dis-tributionally different target (i.e., test) domain.In con-trast to the conventional DG that strictly requires the avail-ability of multiple source domains, this paper considers a more realistic yet challenging scenario, namely Sin-gle Domain Generalization (Single-DG), where only one source domain is available for training. In this scenario, the limited diversity may jeopardize the model general-ization on unseen target domains. To tackle this prob-lem, we propose a style-complement module to enhance the generalization power of the model by synthesizing im-ages from diverse distributions that are complementary to the source ones. More speciﬁcally, we adopt a tractable upper bound of mutual information (MI) between the gen-erated and source samples and perform a two-step op-(1) by minimizing the MI upper timization iteratively: bound approximation for each sample pair, the generated images are forced to be diversiﬁed from the source sam-ples; (2) subsequently, we maximize the MI between the samples from the same semantic category, which assists the network to learn discriminative features from diverse-styled images. Extensive experiments on three bench-mark datasets demonstrate the superiority of our approach, which surpasses the state-of-the-art single-DG methods by up to 25.14%. The code will be publicly available at https://github.com/BUserName/Learning to diversify 