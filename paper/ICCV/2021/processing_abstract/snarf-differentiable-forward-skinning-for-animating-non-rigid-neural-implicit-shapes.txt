Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continu-ous and resolution-independent manner. However, adapt-ing them to articulated shapes is non-trivial. Existing ap-proaches learn a backward warp ﬁeld that maps deformed to canonical points. However, this is problematic since the backward warp ﬁeld is pose dependent and thus requires large amounts of data to learn. To address this, we in-troduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deforma-tion ﬁeld without direct supervision. This deformation ﬁeld is deﬁned in canonical, pose-independent, space, enabling generalization to unseen poses. Learning the deformationﬁeld from posed meshes alone is challenging since the cor-respondences of deformed points are deﬁned implicitly and may not be unique under changes of topology. We propose a forward skinning model that ﬁnds all canonical correspon-dences of any deformed point using iterative root ﬁnding.We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural im-plicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D hu-mans in diverse and unseen poses. 