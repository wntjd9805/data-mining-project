This paper presents a novel task together with a new benchmark for detecting generic, taxonomy-free event boundaries that segment a whole video into chunks. Con-ventional work in temporal video segmentation and ac-tion detection focuses on localizing pre-deﬁned action cat-egories and thus does not scale to generic videos. Cogni-tive Science has known since last century that humans con-sistently segment videos into meaningful temporal chunks.This segmentation happens naturally, without pre-deﬁned event categories and without being explicitly asked to do so.Here, we repeat these cognitive experiments on mainstreamCV datasets; with our novel annotation guideline which ad-dresses the complexities of taxonomy-free event boundary annotation, we introduce the task of Generic Event Bound-ary Detection (GEBD) and the new benchmark Kinetics-GEBD. We view GEBD as an important stepping stone to-wards understanding the video as a whole, and believe it has been previously neglected due to a lack of proper task deﬁnition and annotations. Through experiment and human study we demonstrate the value of the annotations. Fur-ther, we benchmark supervised and un-supervised GEBD approaches on the TAPOS dataset and our Kinetics-GEBD.We release our annotations and baseline codes at CVPR’21LOVEU Challenge: https://sites.google.com/ view/loveucvpr21. 