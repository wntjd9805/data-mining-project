Text-to-image synthesis refers to generating an image from a given text description, the key goal of which lies in photo realism and semantic consistency. Previous methods usually generate an initial image with sentence embedding and then refine it with fine-grained word embedding. De-spite the significant progress, the ‘aspect’ information (e.g., red eyes) contained in the text, referring to several words rather than a word that depicts ‘a particular part or fea-ture of something’, is often ignored, which is highly helpful for synthesizing image details. How to make better utiliza-tion of aspect information in text-to-image synthesis still remains an unresolved challenge. To address this prob-lem, in this paper, we propose a Dynamic Aspect-awarEGAN (DAE-GAN) that represents text information compre-hensively from multiple granularities, including sentence-level, word-level, and aspect-level. Moreover, inspired by human learning behaviors, we develop a novel Aspect-aware Dynamic Re-drawer (ADR) for image refinement, in which an Attended Global Refinement (AGR) module and an Aspect-aware Local Refinement (ALR) module are al-ternately employed. AGR utilizes word-level embedding to globally enhance the previously generated image, whileALR dynamically employs aspect-level embedding to refine image details from a local perspective. Finally, a corre-sponding matching loss function is designed to ensure the text-image semantic consistency at different levels. Exten-sive experiments on two well-studied and publicly available datasets (i.e., CUB-200 and COCO) demonstrate the supe-riority and rationality of our method. 