Image translation methods typically aim to manipulate a set of labeled attributes (given as supervision at train-ing time e.g. domain label) while leaving the unlabeled at-tributes intact. Current methods achieve either: (i) disen-tanglement, which exhibits low visual ﬁdelity and can only be satisﬁed where the attributes are perfectly uncorrelated. (ii) visually-plausible translations, which are clearly not disentangled. In this work, we propose OverLORD, a sin-gle framework for disentangling labeled and unlabeled at-tributes as well as synthesizing high-ﬁdelity images, which is composed of two stages; (i) Disentanglement: Learning disentangled representations with latent optimization. Dif-ferently from previous approaches, we do not rely on adver-sarial training or any architectural biases. (ii) Synthesis:Training feed-forward encoders for inferring the learned attributes and tuning the generator in an adversarial man-ner to increase the perceptual quality. When the labeled and unlabeled attributes are correlated, we model an ad-ditional representation that accounts for the correlated at-tributes and improves disentanglement. We highlight that our ﬂexible framework covers multiple settings as disen-tangling labeled attributes, pose and appearance, localized concepts, and shape and texture. We present signiﬁcantly better disentanglement with higher translation quality and greater output diversity than state-of-the-art methods. 