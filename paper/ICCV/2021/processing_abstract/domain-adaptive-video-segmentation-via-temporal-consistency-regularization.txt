Video semantic segmentation is an essential task for the analysis and understanding of videos. Recent efforts largely focus on supervised video segmentation by learning from fully annotated data, but the learnt models often experi-ence clear performance drop while applied to videos of a different domain. This paper presents DA-VSN, a do-main adaptive video segmentation network that addresses domain gaps in videos by temporal consistency regulariza-tion (TCR) for consecutive frames of target-domain videos.DA-VSN consists of two novel and complementary designs.The ﬁrst is cross-domain TCR that guides the prediction of target frames to have similar temporal consistency as that of source frames (learnt from annotated source data) via adversarial learning. The second is intra-domain TCR that guides unconﬁdent predictions of target frames to have sim-ilar temporal consistency as conﬁdent predictions of target frames. Extensive experiments demonstrate the superiority of our proposed domain adaptive video segmentation net-work which outperforms multiple baselines consistently by large margins. e c r u oS t e g r aT) s r uO ( t e g r aTPrevious FrameCurrent FrameFigure 1. Temporal consistency helps in domain adaptive video segmentation: A video segmentation model trained in a Source domain often experiences clear performance drop while applied to videos of a Target domain. We employ temporal consistency, the inherent and universal nature of videos, as a constraint to regu-larize inter-domain and intra-domain adaptation for optimal video segmentation in target domain as in Target (ours). 