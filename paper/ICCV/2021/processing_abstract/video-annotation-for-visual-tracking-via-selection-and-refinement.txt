Deep learning based visual trackers entail ofﬂine pre-training on large volumes of video datasets with accu-rate bounding box annotations that are labor-expensive to achieve. We present a new framework to facilitate bounding box annotations for video sequences, which investigates a selection-and-reﬁnement strategy to automatically improve the preliminary annotations generated by tracking algo-rithms. A temporal assessment network (T-Assess Net) is proposed which is able to capture the temporal coherence of target locations and select reliable tracking results by measuring their quality. Meanwhile, a visual-geometry re-ﬁnement network (VG-Reﬁne Net) is also designed to fur-ther enhance the selected tracking results by considering both target appearance and temporal geometry constraints, allowing inaccurate tracking results to be corrected. The combination of the above two networks provides a princi-pled approach to ensure the quality of automatic video an-notation. Experiments on large scale tracking benchmarks demonstrate that our method can deliver highly accurate bounding box annotations and signiﬁcantly reduce human labor by 94.0%, yielding an effective means to further boost tracking performance with augmented training data. 