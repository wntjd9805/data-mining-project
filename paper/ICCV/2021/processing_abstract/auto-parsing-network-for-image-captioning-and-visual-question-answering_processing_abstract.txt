We propose an Auto-Parsing Network (APN) to discover and exploit the input data’s hidden tree structures for im-proving the effectiveness of the Transformer-based vision-language systems. Specifically, we impose a Probabilis-tic Graphical Model (PGM) parameterized by the atten-tion operations on each self-attention layer to incorporate sparse assumption. We use this PGM to softly segment an input sequence into a few clusters where each cluster can be treated as the parent of the inside entities. By stack-ing these PGM constrained self-attention layers, the clus-ters in a lower layer compose into a new sequence, and the PGM in a higher layer will further segment this se-quence. Iteratively, a sparse tree can be implicitly parsed, and this tree’s hierarchical knowledge is incorporated into the transformed embeddings, which can be used for solving the target vision-language tasks. Specifically, we showcase that our APN can strengthen Transformer based networks in two major vision-language tasks: Captioning and VisualQuestion Answering. Also, a PGM probability-based pars-ing algorithm is developed by which we can discover what the hidden structure of input is during the inference. 