Joint forecasting of human trajectory and pose dynam-ics is a fundamental building block of various applications ranging from robotics and autonomous driving to surveil-lance systems. Predicting body dynamics requires captur-ing subtle information embedded in the humans’ interac-tions with each other and with the objects present in the scene. In this paper, we propose a novel TRajectory andPOse Dynamics (nicknamed TRiPOD) method based on graph attentional networks to model the human-human and human-object interactions both in the input space and the output space (decoded future output). The model is sup-plemented by a message passing interface over the graphs to fuse these different levels of interactions efﬁciently. Fur-thermore, to incorporate a real-world challenge, we pro-pound to learn an indicator representing whether an es-timated body joint is visible/invisible at each frame, e.g. due to occlusion or being outside the sensor ﬁeld of view.Finally, we introduce a new benchmark for this joint task based on two challenging datasets (PoseTrack and 3DPW) and propose evaluation metrics to measure the effectiveness of predictions in the global space, even when there are invis-ible cases of joints. Our evaluation shows that TRiPOD out-performs all prior work and state-of-the-art speciﬁcally de-signed for each of the trajectory and pose forecasting tasks. 