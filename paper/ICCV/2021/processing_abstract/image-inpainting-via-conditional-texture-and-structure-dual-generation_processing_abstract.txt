Deep generative approaches have recently made consid-erable progress in image inpainting by introducing struc-ture priors. Due to the lack of proper interaction with im-age texture during structure reconstruction, however, cur-rent solutions are incompetent in handling the cases with large corruptions, and they generally suffer from distorted results. In this paper, we propose a novel two-stream net-work for image inpainting, which models the structure-constrained texture synthesis and texture-guided struc-ture reconstruction in a coupled manner so that they better leverage each other for more plausible generation. Further-more, to enhance the global consistency, a Bi-directionalGated Feature Fusion (Bi-GFF) module is designed to ex-change and combine the structure and texture information and a Contextual Feature Aggregation (CFA) module is de-veloped to reﬁne the generated contents by region afﬁn-ity learning and multi-scale feature aggregation. Quali-tative and quantitative experiments on the CelebA, ParisStreetView and Places2 datasets demonstrate the superi-ority of the proposed method. Our code is available at https://github.com/Xiefan-Guo/CTSDG. 