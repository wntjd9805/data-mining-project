Pedestrian behavior prediction is one of the major chal-lenges for intelligent driving systems. Pedestrians often ex-hibit complex behaviors inﬂuenced by various contextual elements. To address this problem, we propose BiPed, a multitask learning framework that simultaneously predicts trajectories and actions of pedestrians by relying on multi-modal data. Our method beneﬁts from 1) a bifold encoding approach where different data modalities are processed in-dependently allowing them to develop their own representa-tions, and jointly to produce a representation for all modal-ities using shared parameters; 2) a novel interaction mod-eling technique that relies on categorical semantic parsing of the scenes to capture interactions between target pedes-trians and their surroundings; and 3) a bifold prediction mechanism that uses both independent and shared decoding of multimodal representations. Using public pedestrian be-havior benchmark datasets for driving, PIE and JAAD, we highlight the beneﬁts of the proposed method for behavior prediction and show that our model achieves state-of-the-art performance and improves trajectory and action predic-tion by up to 22% and 9% respectively. We further investi-gate the contributions of the proposed reasoning techniques via extensive ablation studies. 