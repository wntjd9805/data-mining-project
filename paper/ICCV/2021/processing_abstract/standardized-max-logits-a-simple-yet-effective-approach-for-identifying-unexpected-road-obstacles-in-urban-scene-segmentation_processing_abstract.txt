Identifying unexpected objects on roads in semantic seg-mentation (e.g., identifying dogs on roads) is crucial in safety-critical applications. Existing approaches use im-ages of unexpected objects from external datasets or re-quire additional training (e.g., retraining segmentation net-works or training an extra network), which necessitate a non-trivial amount of labor intensity or lengthy inference time. One possible alternative is to use prediction scores of a pre-trained network such as the max logits (i.e., maximum values among classes before the ﬁnal softmax layer) for de-tecting such objects. However, the distribution of max logits of each predicted class is signiﬁcantly different from each other, which degrades the performance of identifying un-expected objects in urban-scene segmentation. To address this issue, we propose a simple yet effective approach that standardizes the max logits in order to align the different distributions and reﬂect the relative meanings of max log-its within each predicted class. Moreover, we consider the local regions from two different perspectives based on the intuition that neighboring pixels share similar semantic in-formation. In contrast to previous approaches, our method does not utilize any external datasets or require additional training, which makes our method widely applicable to ex-isting pre-trained segmentation models. Such a straightfor-ward approach achieves a new state-of-the-art performance on the publicly available Fishyscapes Lost & Found leader-board with a large margin. Our code is publicly available at this link1. 