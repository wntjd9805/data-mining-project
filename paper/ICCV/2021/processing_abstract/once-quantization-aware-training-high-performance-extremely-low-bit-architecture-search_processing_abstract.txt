Quantization Neural Networks (QNN) have attracted a lot of attention due to their high efﬁciency. To enhance the quantization accuracy, prior works mainly focus on designing advanced quantization algorithms but still fail to achieve satisfactory results under the extremely low-bit case. In this work, we take an architecture perspective to investigate the potential of high-performance QNN. There-fore, we propose to combine Network Architecture Search methods with quantization to enjoy the merits of the two sides. However, a naive combination inevitably faces un-acceptable time consumption or unstable training problem.To alleviate these problems, we ﬁrst propose the joint train-ing of architecture and quantization with a shared step size to acquire a large number of quantized models. Then a bit-inheritance scheme is introduced to transfer the quan-tized models to the lower bit, which further reduces the time cost and meanwhile improves the quantization accu-racy. Equipped with this overall framework, dubbed asOnce Quantization-Aware Training (OQAT), our searched model family, OQATNets, achieves a new state-of-the-art compared with various architectures under different bit-widths. In particular, OQAT-2bit-M achieves 61.6% Ima-geNet Top-1 accuracy, outperforming 2-bit counterpart Mo-bileNetV3 by a large margin of 9% with 10% less com-putation cost. A series of quantization-friendly architec-tures are identiﬁed easily and extensive analysis can be made to summarize the interaction between quantization and neural architectures. Codes and models are released at https://github.com/LaVieEnRoseSMZ/OQA 