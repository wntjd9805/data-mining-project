Modern computer vision applications suffer from catas-trophic forgetting when incrementally learning new con-cepts over time. The most successful approaches to al-leviate this forgetting require extensive replay of previ-ously seen data, which is problematic when memory con-straints or data legality concerns exist.In this work, we consider the high-impact problem of Data-Free Class-Incremental Learning (DFCIL), where an incremental learning agent must learn new concepts over time with-out storing generators or training data from past tasks.One approach for DFCIL is to replay synthetic images produced by inverting a frozen copy of the learner’s clas-siﬁcation model, but we show this approach fails for common class-incremental benchmarks when using stan-dard distillation strategies. We diagnose the cause of incremental distilla-this failure and propose a novel tion strategy for DFCIL, contributing a modiﬁed cross-entropy training and importance-weighted feature distilla-tion, and show that our method results in up to a 25.1% in-crease in ﬁnal task accuracy (absolute difference) compared to SOTA DFCIL methods for common class-incremental benchmarks. Our method even outperforms several stan-dard replay based methods which store a coreset of im-ages. Our code is available at https://github.com/GT-RIPL/AlwaysBeDreaming-DFCIL 