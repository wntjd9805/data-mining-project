eral different scene datasets.We tackle the challenge of learning a distribution over complex, realistic, indoor scenes.In this paper, we in-troduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radi-ance fields that can be rendered from a free moving cam-era. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D ob-servations. Recent work has shown that generative mod-els of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of sin-gle objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing mod-els lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from view-points that are significantly different from observed view-points. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across sev-