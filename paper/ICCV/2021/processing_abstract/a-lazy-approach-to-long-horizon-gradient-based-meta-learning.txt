Gradient-based meta-learning ﬁrst trains task-speciﬁc models by an inner loop and then backpropagates meta-gradients through the loop to update the meta-model. To avoid high-order gradients, existing methods either take a small number of inner steps or approximate the meta-updates for the situations that the meta-model and task models lie in the same space. To enable long inner horizons for more gen-eral meta-learning problems, we instead propose an intuitive teacher-student strategy. The key idea is to employ a stu-dent network to adequately explore the search space of task-speciﬁc models, followed by a teacher’s “leap” toward the regions probed by the student. The teacher not only arrives at a high-quality model but also deﬁnes a lightweight com-putational graph for the meta-gradients. Our approach is generic; it performs well when applied to four meta-learning algorithms over three tasks: few-shot learning, long-tailed object recognition, and adversarial blackbox attack. 