SSDASemi-supervised domain adaptation (SSDA) aims to adapt models trained from a labeled source domain to a different but related target domain, from which unlabeled data and a small set of labeled data are provided. Current methods that treat source and target supervision without distinction overlook their inherent discrepancy, resulting in a source-dominated model that has not effectively use the target su-pervision. In this paper, we argue that the labeled target data needs to be distinguished for effective SSDA, and pro-pose to explicitly decompose the SSDA task into two sub-tasks: a semi-supervised learning (SSL) task in the target domain and an unsupervised domain adaptation (UDA) task across domains. By doing so, the two sub-tasks can bet-ter leverage the corresponding supervision and thus yield very different classiﬁers. To integrate the strengths of the two classiﬁers, we apply the well established co-training framework, in which the two classiﬁers exchange their high conﬁdent predictions to iteratively “teach each other” so that both classiﬁers can excel in the target domain. We call our approach Deep Co-training with Task decomposition (DECOTA). DECOTA requires no adversarial training and is easy to implement. Moreover, DECOTA is well founded on the theoretical condition of when co-training would suc-ceed. As a result, DECOTA achieves state-of-the-art results on several SSDA datasets, outperforming the prior art by a notable 4% margin on DomainNet. Code is available at https://github.com/LoyoYang/DeCoTa. 