In this paper, we present a novel Dynamic DETR (De-tection with Transformers) approach by introducing dy-namic attentions into both the encoder and decoder stages of DETR to break its two limitations on small feature res-olution and slow training convergence. To address theﬁrst limitation, which is due to the quadratic computa-tional complexity of the self-attention module in Trans-former encoders, we propose a dynamic encoder to ap-proximate the Transformer encoder’s attention mechanism using a convolution-based dynamic encoder with various attention types. Such an encoder can dynamically adjust attentions based on multiple factors such as scale impor-tance, spatial importance, and representation (i.e., feature dimension) importance. To mitigate the second limitation of learning difﬁculty, we introduce a dynamic decoder by replacing the cross-attention module with a ROI-based dy-namic attention in the Transformer decoder. Such a decoder effectively assists Transformers to focus on region of inter-ests from a coarse-to-ﬁne manner and dramatically lowers the learning difﬁculty, leading to a much faster convergence with fewer training epochs. We conduct a series of experi-ments to demonstrate our advantages. Our Dynamic DETR signiﬁcantly reduces the training epochs (by 14×), yet re-sults in a much better performance (by 3.6 on mAP). Mean-while, in the standard 1× setup with ResNet-50 backbone, we archive a new state-of-the-art performance that further proves the learning effectiveness of the proposed approach. 