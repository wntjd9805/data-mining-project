We study the machineâ€™s understanding of embodied ref-erence: One agent uses both language and gesture to refer to an object to another agent in a shared physical environ-ment. Of note, this new visual task requires understanding multimodal cues with perspective-taking to identify which object is being referred to. To tackle this problem, we intro-duce YouRefIt, a new crowd-sourced dataset of embodied reference collected in various physical scenes; the dataset contains 4,195 unique reference clips in 432 indoor scenes.To the best of our knowledge, this is the first embodied ref-erence dataset that allows us to study referring expressions in daily physical scenes to understand referential behavior, human communication, and human-robot interaction. We further devise two benchmarks for image-based and video-based embodied reference understanding. Comprehensive baselines and extensive experiments provide the very first result of machine perception on how the referring expres-sions and gestures affect the embodied reference under-standing. Our results provide essential evidence that ges-tural cues are as critical as language cues in understanding the embodied reference. 