The state-of-the-art object detection and image classiﬁ-cation methods can perform impressively on more than 9k classes. In contrast, the number of classes in semantic seg-mentation datasets is relatively limited. This is not surpris-ing when the restrictions caused by the lack of labeled data and high computation demand for segmentation are consid-ered. In this paper, we propose a novel training methodol-ogy to train and scale the existing semantic segmentation models for a large number of semantic classes without in-In our embedding-based creasing the memory overhead. scalable segmentation approach, we reduce the space com-plexity of the segmentation model’s output from O(C) toO(1), propose an approximation method for ground-truth class probability, and use it to compute cross-entropy loss.The proposed approach is general and can be adopted by any state-of-the-art segmentation model to gracefully scale it for any number of semantic classes with only one GPU.Our approach achieves similar, and in some cases, even better mIoU for Cityscapes, Pascal VOC, ADE20k, COCO-Stuff10k datasets when adopted to DeeplabV3+ model with different backbones. We demonstrate a clear beneﬁt of our approach on a dataset with 1284 classes, bootstrapped fromLVIS and COCO annotations, with almost three times better mIoU than the DeeplabV3+. Our source code is available at: https://github.com/shipra25jain/ESSNet. 1 .