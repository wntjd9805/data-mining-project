3D human shape and pose estimation is the essential task for human motion analysis, which is widely used in many 3D applications. However, existing methods can-not simultaneously capture the relations at multiple lev-els, including spatial-temporal level and human joint level.Therefore they fail to make accurate predictions in some hard scenarios when there is cluttered background, occlu-sion, or extreme pose. To this end, we propose Multi-levelAttention Encoder-Decoder Network (MAED), including aSpatial-Temporal Encoder (STE) and a Kinematic Topol-ogy Decoder (KTD) to model multi-level attentions in a unified framework. STE consists of a series of cascaded blocks based on Multi-Head Self-Attention, and each block uses two parallel branches to learn spatial and temporal attention respectively. Meanwhile, KTD aims at model-ing the joint level attention.It regards pose estimation as a top-down hierarchical process similar to SMPL kine-matic tree. With the training set of 3DPW, MAED outper-forms previous state-of-the-art methods by 6.2, 7.2, and 2.4 mm of PA-MPJPE on the three widely used bench-marks 3DPW, MPI-INF-3DHP, and Human3.6M respec-tively. Our code is available at https://github.com/ ziniuwan/maed. 