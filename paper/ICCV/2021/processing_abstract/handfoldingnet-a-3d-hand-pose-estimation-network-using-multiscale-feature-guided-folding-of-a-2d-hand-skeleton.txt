With increasing applications of 3D hand pose estimation in various human-computer interaction applications, con-volution neural networks (CNNs) based estimation models have been actively explored. However, the existing models require complex architectures or redundant computational resources to trade with the acceptable accuracy. To tackle this limitation, this paper proposes HandFoldingNet, an ac-curate and efficient hand pose estimator that regresses the hand joint locations from the normalized 3D hand point cloud input. The proposed model utilizes a folding-based decoder that folds a given 2D hand skeleton into the cor-responding joint coordinates. For higher estimation accu-racy, folding is guided by multi-scale features, which in-clude both global and joint-wise local features. Experimen-tal results show that the proposed model outperforms the ex-isting methods on three hand pose benchmark datasets with the lowest model parameter requirement. Code is available at https://github.com/cwc1260/HandFold. 