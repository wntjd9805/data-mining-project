The choice of activation functions is crucial for modern deep neural networks. Popular hand-designed activation functions like Rectiﬁed Linear Unit(ReLU) and its variants show promising performance in various tasks and mod-els. Swish, the automatically discovered activation func-tion, has been proposed and outperforms ReLU on many challenging datasets. However, it has two main draw-backs. First, the tree-based search space is highly dis-crete and restricted, which is difﬁcult for searching. Sec-ond, the sample-based searching method is inefﬁcient, mak-ing it infeasible to ﬁnd specialized activation functions for each dataset or neural architecture. To tackle these draw-backs, we propose a new activation function called Piece-wise Linear Unit(PWLU), which incorporates a carefully designed formulation and learning method.It can learn specialized activation functions and achieves SOTA perfor-mance on large-scale datasets like ImageNet and COCO.For example, on ImageNet classiﬁcation dataset, PWLU im-proves 0.9%/0.53%/1.0%/1.7%/1.0% top-1 accuracy overSwish for ResNet-18/ResNet-50/MobileNet-V2/MobileNet-V3/EfﬁcientNet-B0. PWLU is also easy to implement and efﬁcient at inference, which can be widely applied in real-world applications. 