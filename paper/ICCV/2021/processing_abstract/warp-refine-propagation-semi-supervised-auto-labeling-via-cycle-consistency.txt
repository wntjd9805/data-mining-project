Deep learning models for semantic segmentation rely on expensive, large-scale, manually annotated datasets. La-belling is a tedious process that can take hours per image.Automatically annotating video sequences by propagating sparsely labeled frames through time is a more scalable alter-native. In this work, we propose a novel label propagation method, termed Warp-Reﬁne Propagation, that combines semantic cues with geometric cues to efﬁciently auto-label videos. Our method learns to reﬁne geometrically-warped la-bels and infuse them with learned semantic priors in a semi-supervised setting by leveraging cycle-consistency across time. We quantitatively show that our method improves label-propagation by a noteworthy margin of 13.1 mIoU on the ApolloScape dataset. Furthermore, by training with the auto-labelled frames, we achieve competitive results on three semantic-segmentation benchmarks, improving the state-of-the-art by a large margin of 1.8 and 3.61 mIoU onNYU-V2 and KITTI, while matching the current best results on Cityscapes. 