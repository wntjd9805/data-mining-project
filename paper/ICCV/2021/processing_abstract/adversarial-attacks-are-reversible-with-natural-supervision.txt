We find that images contain intrinsic structure that en-ables the reversal of many adversarial attacks. Attack vec-tors cause not only image classifiers to fail, but also collat-erally disrupt incidental structure in the image. We demon-strate that modifying the attacked image to restore the nat-ural structure will reverse many types of attacks, provid-ing a defense. Experiments demonstrate significantly im-proved robustness for several state-of-the-art models across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.Our results show that our defense is still effective even if the attacker is aware of the defense mechanism. Since our de-fense is deployed during inference instead of training, it is compatible with pre-trained networks as well as most other defenses. Our results suggest deep networks are vulnera-ble to adversarial examples partly because their represen-tations do not enforce the natural structure of images. 