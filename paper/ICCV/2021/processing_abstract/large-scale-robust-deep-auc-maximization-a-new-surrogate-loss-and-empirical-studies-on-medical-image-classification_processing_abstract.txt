Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural network by maximizing the AUC score of the model on a dataset. Most previous works ofAUC maximization focus on the perspective of optimiza-tion by designing efﬁcient stochastic algorithms, and stud-ies on generalization performance of large-scale DAM on difﬁcult tasks are missing.In this work, we aim to makeDAM more practical for interesting real-world applications (e.g., medical image classiﬁcation). First, we propose a new margin-based min-max surrogate loss function for the AUC score (named as the AUC min-max-margin loss or simply AUC margin loss for short). It is more robust than the commonly used AUC square loss, while enjoy-ing the same advantage in terms of large-scale stochas-tic optimization. Second, we conduct extensive empirical studies of our DAM method on four difﬁcult medical im-age classiﬁcation tasks, namely (i) classiﬁcation of chest x-ray images for identifying many threatening diseases, (ii) classiﬁcation of images of skin lesions for identifying melanoma, (iii) classiﬁcation of mammogram for breast cancer screening, and (iv) classiﬁcation of microscopic im-ages for identifying tumor tissue. Our studies demonstrate that the proposed DAM method improves the performance of optimizing cross-entropy loss by a large margin, and also achieves better performance than optimizing the ex-isting AUC square loss on these medical image classiﬁca-tion tasks. Speciﬁcally, our DAM method has achieved the 1st place on Stanford CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is the ﬁrst work that makes DAM succeed on large-scale medical image datasets.We also conduct extensive ablation studies to demonstrate the advantages of the new AUC margin loss over theAUC square loss on benchmark datasets. The proposed method is implemented in our open-sourced library LibAUC (www.libauc.org) whose github address is https://github.com/Optimization-AI/LibAUC.Figure 1. An illustrative example for optimizing different AUC losses on a toy data for learning a two-layer neural network withELU activation. The top row is optimizing the AUC square loss and the bottom row is optimizing the new AUC margin loss. Theﬁrst column depicts the initial decision boundary (dashed line) pre-trained on a set of examples. In the middle column, we add some easy examples to the training set and retrain the model by optimiz-ing the AUC loss. In the last column, we add some noisily labeled data (blue circled data) to the training set and retrain the model by optimizing the AUC loss. The results demonstrate the new AUC margin loss is more robust than the AUC square loss. 