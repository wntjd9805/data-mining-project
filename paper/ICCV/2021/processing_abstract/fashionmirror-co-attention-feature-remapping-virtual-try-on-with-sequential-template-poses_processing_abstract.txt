Virtual try-on tasks have drawn increased attention.Prior arts focus on tackling this task via warping clothes and fusing the information at the pixel level with the help of semantic segmentation. However, conducting semantic segmentation is time-consuming and easily causes error ac-cumulation over time. Besides, warping the information at the pixel level instead of the feature level limits the perfor-mance (e.g., unable to generate different views) and is un-stable since it directly demonstrates the results even with a misalignment. In contrast, fusing information at the fea-ture level can be further refined by the convolution to ob-tain the final results. Based on these assumptions, we pro-pose a co-attention feature-remapping framework, namelyFashionMirror, that generates the try-on results according to the driven-pose sequence in two stages. In the first stage, we consider the source human image and the target try-on clothes to predict the removed mask and the try-on clothing mask, which replaces the pre-processed semantic segmen-tation and reduces the inference time. In the second stage, we first remove the clothes on the source human via the re-moved mask and warp the clothing features conditioning on the try-on clothing mask to fit the next frame human. Mean-while, we predict the optical flows from the consecutive 2D poses and warp the source human to the next frame at the feature level. Then, we enhance the clothing features and source human features in every frame to generate realistic try-on results with spatio-temporal smoothness. Both quali-tative and quantitative results show that FashionMirror out-performs the state-of-the-art virtual try-on approaches. 