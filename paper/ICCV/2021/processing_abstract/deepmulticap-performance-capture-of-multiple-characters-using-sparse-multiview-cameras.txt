We propose DeepMultiCap, a novel method for multi-person performance capture using sparse multi-view cam-eras. Our method can capture time varying surface de-tails without the need of using pre-scanned template mod-els. To tackle with the serious occlusion challenge for close interacting scenes, we combine a recently proposed pixel-aligned implicit function with parametric model for robust reconstruction of the invisible surface areas. An effec-tive attention-aware module is designed to obtain the ﬁne-grained geometry details from multi-view images, where high-ﬁdelity results can be generated.In addition to the spatial attention method, for video inputs, we further pro-pose a novel temporal fusion method to alleviate the noise and temporal inconsistencies for moving character recon-struction. For quantitative evaluation, we contribute a high quality multi-person dataset, MultiHuman, which con-sists of 150 static scenes with different levels of occlusions and ground truth 3D human models. Experimental results demonstrate the state-of-the-art performance of our method and the well generalization to real multiview video data, which outperforms the prior works by a large margin. 