Quantization is a widely used technique to compress and accelerate deep neural networks. However, conven-tional quantization methods use the same bit-width for all (or most of) the layers, which often suffer signiﬁcant ac-curacy degradation in the ultra-low precision regime and ignore the fact that emergent hardware accelerators be-gin to support mixed-precision computation. Consequently, we present a novel and principled framework to solve the mixed-precision quantization problem in this paper. Brieﬂy speaking, we ﬁrst formulate the mixed-precision quantiza-tion as a discrete constrained optimization problem. Then, to make the optimization tractable, we approximate the ob-jective function with second-order Taylor expansion and propose an efﬁcient approach to compute its Hessian ma-trix. Finally, based on the above simpliﬁcation, we show that the original problem can be reformulated as a Multiple-Choice Knapsack Problem (MCKP) and propose a greedy search algorithm to solve it efﬁciently. Compared with exist-ing mixed-precision quantization works, our method is de-rived in a principled way and much more computationally efﬁcient. Moreover, extensive experiments conducted on theImageNet dataset and various kinds of network architec-tures also demonstrate its superiority over existing uniform and mixed-precision quantization approaches. 