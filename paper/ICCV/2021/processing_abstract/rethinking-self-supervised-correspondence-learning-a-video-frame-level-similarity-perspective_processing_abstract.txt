Learning a good representation for space-time corre-spondence is the key for various computer vision tasks, including tracking object bounding boxes and performing video object pixel segmentation. To learn generalizable representation for correspondence in large-scale, a vari-ety of self-supervised pretext tasks are proposed to explic-itly perform object-level or patch-level similarity learning.Instead of following the previous literature, we propose to learn correspondence using Video Frame-level Similar-ity (VFS) learning, i.e, simply learning from comparing video frames. Our work is inspired by the recent success in image-level contrastive learning and similarity learning for visual recognition. Our hypothesis is that if the repre-sentation is good for recognition, it requires the convolu-tional features to Ô¨Ånd correspondence between similar ob-jects or parts. Our experiments show surprising results thatVFS surpasses state-of-the-art self-supervised approaches for both OTB visual object tracking and DAVIS video ob-ject segmentation. We perform detailed analysis on what matters in VFS and reveals new properties on image and frame level similarity learning. Project page with code: https://jerryxu.net/VFS. 