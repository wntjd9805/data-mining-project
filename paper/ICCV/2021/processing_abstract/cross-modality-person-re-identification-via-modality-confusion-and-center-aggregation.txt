Cross-modality person re-identiﬁcation is a challeng-ing task due to large cross-modality discrepancy and intra-modality variations. Currently, most existing methods fo-cus on learning modality-speciﬁc or modality-shareable features by using the identity supervision or modality la-bel. Different from existing methods, this paper presents a novel Modality Confusion Learning Network (MCLNet).Its basic idea is to confuse two modalities, ensuring that the optimization is explicitly concentrated on the modality-irrelevant perspective. Speciﬁcally, MCLNet is designed to learn modality-invariant features by simultaneously mini-mizing inter-modality discrepancy while maximizing cross-modality similarity among instances in a single framework.Furthermore, an identity-aware marginal center aggrega-tion strategy is introduced to extract the centralization fea-tures, while keeping diversity with a marginal constraint.Finally, we design a camera-aware learning scheme to en-rich the discriminability. Extensive experiments on SYSU-MM01 and RegDB datasets show that MCLNet outperforms the state-of-the-art by a large margin. On the large-scaleSYSU-MM01 dataset, our model can achieve 65.40 % and 61.98 % in terms of Rank-1 accuracy and mAP value. 