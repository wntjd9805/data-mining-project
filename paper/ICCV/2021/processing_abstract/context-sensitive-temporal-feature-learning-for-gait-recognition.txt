Although gait recognition has drawn increasing research attention recently, it remains challenging to learn discrim-inative temporal representation since the silhouette differ-ences are quite subtle in spatial domain. Inspired by the observation that humans can distinguish gaits of differ-ent subjects by adaptively focusing on temporal sequences with different time scales, we propose a context-sensitive temporal feature learning (CSTL) network in this paper, which aggregates temporal features in three scales to ob-tain motion representation according to the temporal con-textual information. Specifically, CSTL introduces rela-tion modeling among multi-scale features to evaluate fea-ture importances, based on which network adaptively en-hances more important scale and suppresses less important scale. Besides that, we propose a salient spatial feature learning (SSFL) module to tackle the misalignment prob-lem caused by temporal operation, e.g., temporal convo-lution. SSFL recombines a frame of salient spatial fea-tures by extracting the most discriminative parts across the whole sequence. In this way, we achieve adaptive tempo-ral learning and salient spatial mining simultaneously. Ex-tensive experiments conducted on two datasets demonstrate the state-of-the-art performance. On CASIA-B dataset, we achieve rank-1 accuracies of 98.0%, 95.4% and 87.0% un-der normal walking, bag-carrying and coat-wearing con-ditions. On OU-MVLP dataset, we achieve rank-1 ac-curacy of 90.2%. The source code will be published at https://github.com/OliverHxh/CSTL. 