Remarkable results have been achieved by DCNN based self-supervised depth estimation approaches. However, most of these approaches can only handle either day-time or night-time images, while their performance degrades for all-day images due to large domain shift and the varia-tion of illumination between day and night images. To relieve these limitations, we propose a domain-separated network for self-supervised depth estimation of all-day im-ages. Speciﬁcally, to relieve the negative inﬂuence of dis-turbing terms (illumination, etc.), we partition the infor-mation of day and night image pairs into two complemen-tary sub-spaces: private and invariant domains, where the former contains the unique information (illumination, etc.) of day and night images and the latter contains essential shared information (texture, etc.). Meanwhile, to guaran-tee that the day and night images contain the same infor-mation, the domain-separated network takes the day-time images and corresponding night-time images (generated byGAN) as input, and the private and invariant feature ex-tractors are learned by orthogonality and similarity loss, where the domain gap can be alleviated, thus better depth maps can be expected. Meanwhile, the reconstruction and photometric losses are utilized to estimate complementary information and depth maps effectively. Experimental re-sults demonstrate that our approach achieves state-of-the-art depth estimation results for all-day images on the chal-lenging Oxford RobotCar dataset, proving the superiority of our proposed approach. Code and data split are available at https://github.com/LINA-lln/ADDS-DepthNet. 