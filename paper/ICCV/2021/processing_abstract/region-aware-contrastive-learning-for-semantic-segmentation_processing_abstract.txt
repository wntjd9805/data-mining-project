Recent works have made great success in semantic seg-mentation by exploiting contextual information in a local or global manner within individual image and supervis-ing the model with pixel-wise cross entropy loss. However, from the holistic view of the whole dataset, semantic rela-tions not only exist inside one single image, but also prevail in the whole training data, which makes solely consider-ing intra-image correlations insufﬁcient. Inspired by recent progress in unsupervised contrastive learning, we propose the region-aware contrastive learning (RegionContrast) for semantic segmentation in the supervised manner.In or-der to enhance the similarity of semantically similar pix-els while keeping the discrimination from others, we em-ploy contrastive learning to realize this objective. With the help of memory bank, we explore to store all the represen-tative features into the memory. Without loss of generality, to efﬁciently incorporate all training data into the memory bank while avoiding taking too much computation resource, we propose to construct region centers to represent features from different categories for every image. Hence, the pro-posed region-aware contrastive learning is performed in a region level for all the training data, which saves much more memory than methods exploring the pixel-level rela-tions. The proposed RegionContrast brings little computa-tion cost during training and requires no extra overhead for testing. Extensive experiments demonstrate that our method achieves state-of-the-art performance on three benchmark datasets including Cityscapes, ADE20K and COCO Stuff. 