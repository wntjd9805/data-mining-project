The application of light ﬁeld data in salient object de-tection is becoming increasingly popular recently. The difﬁ-culty lies in how to effectively fuse the features within the fo-cal stack and how to cooperate them with the feature of the all-focus image. Previous methods usually fuse focal stack features via convolution or ConvLSTM, which are both less effective and ill-posed. In this paper, we model the infor-mation fusion within focal stack via graph networks. They introduce powerful context propagation from neighbouring nodes and also avoid ill-posed implementations. On the one hand, we construct local graph connections thus avoiding prohibitive computational costs of traditional graph net-works. On the other hand, instead of processing the two kinds of data separately, we build a novel dual graph model to guide the focal stack fusion process using all-focus pat-terns. To handle the second difﬁculty, previous methods usu-ally implement one-shot fusion for focal stack and all-focus features, hence lacking a thorough exploration of their sup-plements. We introduce a reciprocative guidance scheme and enable mutual guidance between these two kinds of in-formation at multiple steps. As such, both kinds of features can be enhanced iteratively, ﬁnally beneﬁting the saliency prediction. Extensive experimental results show that the proposed models are all beneﬁcial and we achieve signif-icantly better results than state-of-the-art methods. 