Data augmentation has been an indispensable tool to improve the performance of deep neural networks, how-ever the augmentation can hardly transfer among differ-ent tasks and datasets. Consequently, a recent trend is to adopt AutoML technique to learn proper augmentation policy without extensive hand-crafted tuning.In this pa-per, we propose an efﬁcient differentiable search algorithm called Direct Differentiable Augmentation Search (DDAS).It exploits meta-learning with one-step gradient update and continuous relaxation to the expected training loss for ef-ﬁcient search. Our DDAS can achieve efﬁcient augmen-tation search without relying on approximations such asGumbel-Softmax or second order gradient approximation.To further reduce the adverse effect of improper augmen-tations, we organize the search space into a two level hi-erarchy, in which we ﬁrst decide whether to apply aug-mentation, and then determine the speciﬁc augmentation policy. On standard image classiﬁcation benchmarks, ourDDAS achieves state-of-the-art performance and efﬁciency tradeoff while reducing the search cost dramatically, e.g.In addition, we also use 0.15 GPU hours for CIFAR-10.DDAS to search augmentation for object detection task and achieve comparable performance with AutoAugment [8], while being 1000× faster. Code will be released in https://github.com/zxcvfd13502/DDAS_code 