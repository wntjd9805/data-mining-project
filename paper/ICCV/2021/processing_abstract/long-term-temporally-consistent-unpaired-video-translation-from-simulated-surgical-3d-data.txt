Research in unpaired video translation has mainly fo-cused on short-term temporal consistency by conditioning on neighboring frames. However for transfer from sim-ulated to photorealistic sequences, available information on the underlying geometry offers potential for achieving global consistency across views. We propose a novel ap-proach which combines unpaired image translation with neural rendering to transfer simulated to photorealistic sur-gical abdominal scenes. By introducing global learnable textures and a lighting-invariant view-consistency loss, our method produces consistent translations of arbitrary views and thus enables long-term consistent video synthesis. We design and test our model to generate video sequences from minimally-invasive surgical abdominal scenes. Because la-beled data is often limited in this domain, photorealistic data where ground truth information from the simulated do-main is preserved is especially relevant. By extending exist-ing image-based methods to view-consistent videos, we aim to impact the applicability of simulated training and evalua-tion environments for surgical applications. Code and data: http://opencas.dkfz.de/video-sim2real. 