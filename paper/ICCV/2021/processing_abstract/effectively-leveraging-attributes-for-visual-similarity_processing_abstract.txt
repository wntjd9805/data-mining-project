Measuring similarity between two images often requires performing complex reasoning along different axes (e.g., color, texture, or shape). Insights into what might be im-portant for measuring similarity can can be provided by annotated attributes. Prior work tends to view these an-notations as complete, resulting in them using a simplistic approach of predicting attributes on single images, which are, in turn, used to measure similarity. However, it is im-practical for a dataset to fully annotate every attribute that may be important. Thus, only representing images based on these incomplete annotations may miss out on key in-formation. To address this issue, we propose the PairwiseAttribute-informed similarity Network (PAN), which breaks similarity learning into capturing similarity conditions and relevance scores from a joint representation of two images.This enables our model to identify that two images contain the same attribute, but can have it deemed irrelevant (e.g., due to ﬁne-grained differences between them) and ignored for measuring similarity between the two images. Notably, while prior methods of using attribute annotations are often unable to outperform prior art, PAN obtains a 4-9% improve-ment on compatibility prediction between clothing items onPolyvore Outﬁts, a 5% gain on few shot classiﬁcation of im-ages using Caltech-UCSD Birds (CUB), and over 1% boost to Recall@1 on In-Shop Clothes Retrieval. Implementation available at https://github.com/samarth4149/PAN 