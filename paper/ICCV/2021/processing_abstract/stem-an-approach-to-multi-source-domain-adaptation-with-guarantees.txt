Multi-source Domain Adaptation (MSDA) is more prac-tical but challenging than the conventional unsupervised domain adaptation due to the involvement of diverse mul-tiple data sources. Two fundamental challenges of MSDA are: (i) how to deal with the diversity in the multiple source domains and (ii) how to cope with the data shift between the target domain and the source domains. In this paper, to address the first challenge, we propose a theoretical-guaranteed approach to combine domain experts locally trained on its own source domain to achieve a combined multi-source teacher that globally predicts well on the mix-ture of source domains. To address the second challenge, we propose to bridge the gap between the target domain and the mixture of source domains in the latent space via a generator or feature extractor. Together with bridging the gap in the latent space, we train a student to mimic the pre-dictions of the teacher expert on both source and target ex-amples. In addition, our approach is guaranteed with rigor-ous theory offered insightful justifications of how each com-ponent influences the transferring performance. Extensive experiments conducted on three benchmark datasets show that our proposed method achieves state-of-the-art perfor-mances to the best of our knowledge. 