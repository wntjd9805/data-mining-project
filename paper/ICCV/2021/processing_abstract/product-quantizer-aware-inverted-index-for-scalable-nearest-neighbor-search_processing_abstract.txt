The inverted index is one of the most commonly used structures for non-exhaustive nearest neighbor search on large-scale datasets. It allows a signiﬁcant factor of accel-eration by a reduced number of distance computations with only a small fraction of the database. In particular, the in-verted index enables the product quantization (PQ) to learn their codewords in the residual vector space. The quantiza-tion error of the PQ can be substantially improved in such combination since the residual vector space is much more quantization-friendly thanks to their compact distribution compared to the original data. In this paper, we ﬁrst raise an unremarked but crucial question; why the inverted in-dex and the product quantizer are optimized separately even though they are closely related? For instance, changes on the inverted index distort the whole residual vector space.To address the raised question, we suggest a joint opti-mization of the coarse and ﬁne quantizers by substituting the original objective of the coarse quantizer to end-to-end quantization distortion. Moreover, our method is generic and applicable to different combinations of coarse and ﬁne quantizers such as inverted multi-index and optimized PQ.Figure 1: This ﬁgure describes difference between coarse center updates of conventional inverted index and those of our proposed method. While the conventional inverted in-dex considers the distortion of the coarse centers only, the proposed method also considers the quantization error of the ﬁne quantizer in the coarse center updates. 