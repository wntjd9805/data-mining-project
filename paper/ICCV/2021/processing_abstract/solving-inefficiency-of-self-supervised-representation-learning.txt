Self-supervised learning (especially contrastive learn-ing) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing con-trastive learning methods suffer from very low learning ef-ficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accu-racy. In this paper, we reveal two contradictory phenom-ena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilar-ity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differenti-ate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from exces-sive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters.To simultaneously overcome these two problems, we pro-pose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the posi-tive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the BernoulliDistribution model. We extensively evaluate our framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate our modelâ€™s su-periority (e.g., the learning efficiency) over the latest state-of-the-art methods by a clear margin. See Codes1. 