Deep neural networks have significantly improved appearance-based gaze estimation accuracy. However, it still suffers from unsatisfactory performance when general-izing the trained model to new domains, e.g., unseen envi-ronments or persons. In this paper, we propose a plug-and-play gaze adaptation framework (PnP-GA), which is an en-semble of networks that learn collaboratively with the guid-ance of outliers. Since our proposed framework does not re-quire ground-truth labels in the target domain, the existing gaze estimation networks can be directly plugged into PnP-GA and generalize the algorithms to new domains. We testPnP-GA on four gaze domain adaptation tasks, ETH-to-MPII, ETH-to-EyeDiap, Gaze360-to-MPII, and Gaze360-to-EyeDiap. The experimental results demonstrate that thePnP-GA framework achieves considerable performance improvements of 36.9%, 31.6%, 19.4%, and 11.8% over the baseline system. The proposed framework also outper-forms the state-of-the-art domain adaptation approaches on gaze domain adaptation tasks. Code has been re-leased at https://github.com/DreamtaleCore/PnP-GA. 