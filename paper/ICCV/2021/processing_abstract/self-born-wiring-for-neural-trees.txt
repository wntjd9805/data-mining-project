Neural trees aim at integrating deep neural networks and decision trees so as to bring the best of the two worlds, in-cluding representation learning from the former and faster inference from the latter.In this paper, we introduce a novel approach, termed as Self-born Wiring (SeBoW), to learn neural trees from a mother deep neural network. In contrast to prior neural-tree approaches that either adopt a pre-deﬁned structure or grow hierarchical layers in a progressive manner, task-adaptive neural trees in SeBoW evolve from a deep neural network through a construction-by-destruction process, enabling a global-level parameter optimization that further yields favorable results. Speciﬁ-cally, given a designated network conﬁguration like VGG,SeBoW disconnects all the layers and derives isolated ﬁl-ter groups, based on which a global-level wiring process is conducted to attach a subset of ﬁlter groups, eventually bearing a lightweight neural tree. Extensive experiments demonstrate that, with a lower computational cost, SeBoW outperforms all prior neural trees by a signiﬁcant margin and even achieves results on par with predominant non-tree networks like ResNets. Moreover, SeBoW proves its scala-bility to large-scale datasets like ImageNet, which has been barely explored by prior tree networks. 