Unlike conventional zero-shot classiﬁcation, zero-shot semantic segmentation predicts a class label at the pixel level instead of the image level. When solving zero-shot semantic segmentation problems, the need for pixel-level prediction with surrounding context motivates us to in-corporate spatial information using positional encoding.We improve standard positional encoding by introducing the concept of Relative Positional Encoding, which inte-grates spatial information at the feature level and can handle arbitrary image sizes. Furthermore, while self-training is widely used in zero-shot semantic segmentation to generate pseudo-labels, we propose a new knowledge-distillation-inspired self-training strategy, namely AnnealedSelf-Training, which can automatically assign different im-portance to pseudo-labels to improve performance. We sys-tematically study the proposed Relative Positional Encod-ing and Annealed Self-Training in a comprehensive exper-imental evaluation, and our empirical results conﬁrm the effectiveness of our method on three benchmark datasets. 