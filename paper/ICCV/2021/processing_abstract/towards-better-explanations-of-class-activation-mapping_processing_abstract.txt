Increasing demands for understanding the internal be-havior of convolutional neural networks (CNNs) have led to remarkable improvements in explanation methods. Particu-larly, several class activation mapping (CAM) based meth-ods, which generate visual explanation maps by a linear combination of activation maps from CNNs, have been pro-posed. However, the majority of the methods lack a clear theoretical basis on how they assign the coefficients of the linear combination. In this paper, we revisit the intrinsic linearity of CAM with respect to the activation maps; we construct an explanation model of CNN as a linear func-tion of binary variables that denote the existence of the cor-responding activation maps. With this approach, the ex-planation model can be determined by additive feature at-tribution methods in an analytic manner. We then demon-strate the adequacy of SHAP values, which is a unique solu-tion for the explanation model with a set of desirable prop-erties, as the coefficients of CAM. Since the exact SHAP values are unattainable, we introduce an efficient approxi-mation method, LIFT-CAM, based on DeepLIFT. Our pro-posed LIFT-CAM can estimate the SHAP values of the acti-vation maps with high speed and accuracy. Furthermore, it greatly outperforms other previous CAM-based methods in both qualitative and quantitative aspects. 