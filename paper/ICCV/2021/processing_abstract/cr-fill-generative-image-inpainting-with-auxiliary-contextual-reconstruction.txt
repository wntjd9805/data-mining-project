Recent deep generative inpainting methods use attention layers to allow the generator to explicitly borrow feature patches from the known region to complete a missing re-gion. Due to the lack of supervision signals for the corre-spondence between missing regions and known regions, it may fail to ﬁnd proper reference features, which often leads to artifacts in the results. Also, it computes pair-wise simi-larity across the entire feature map during inference bring-ing a signiﬁcant computational overhead. To address this issue, we propose to teach such patch-borrowing behavior to an attention-free generator by joint training of an aux-iliary contextual reconstruction task, which encourages the generated output to be plausible even when reconstructed by surrounding regions. The auxiliary branch can be seen as a learnable loss function, i.e. named as contextual recon-struction (CR) loss, where query-reference feature similar-ity and reference-based reconstructor are jointly optimized with the inpainting generator. The auxiliary branch ( i.e. CR loss) is required only during training, and only the inpaint-ing generator is required during the inference. Experimen-tal results demonstrate that the proposed inpainting model compares favourably against the state-of-the-art in terms of quantitative and visual performance. Code is available at https://github.com/zengxianyu/crfill. 