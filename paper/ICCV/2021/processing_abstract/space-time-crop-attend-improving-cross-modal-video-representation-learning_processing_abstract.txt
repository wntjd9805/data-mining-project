The quality of the image representations obtained from self-supervised learning depends strongly on the type of data augmentations used in the learning formulation. Re-cent papers have ported these methods from still images to videos and found that leveraging both audio and video signals yields strong gains; however, they did not ﬁnd that spatial augmentations such as cropping, which are very im-portant for still images, work as well for videos.In this paper, we improve these formulations in two ways unique to the spatio-temporal aspect of videos. First, for space, we show that spatial augmentations such as cropping do work well for videos too, but that previous implementations, due to the high processing and memory cost, could not do this at a scale sufﬁcient for it to work well. To address this is-sue, we ﬁrst introduce Feature Crop, a method to simulate such augmentations much more efﬁciently directly in fea-ture space. Second, we show that as opposed to na¨ıve av-erage pooling, the use of transformer-based attention im-proves performance signiﬁcantly, and is well suited for pro-cessing feature crops. Combining both of our discoveries into a new method, Space-Time Crop & Attend (STiCA) we achieve state-of-the-art performance across multipleIn particular, video-representation learning benchmarks. we achieve new state-of-the-art accuracies of 67.0% onHMDB-51 and 93.1% on UCF-101 when pre-training onKinetics-400. Code and pretrained models are available1.Figure 1: HMDB-51 accuracy vs epoch. Our method,STiCA, combines space-time crops in feature space with self-attention of time in latent space. This yields signiﬁcant beneﬁts not only in performance but also in speed compared to cropping in input space using two RGB crops, or simply using the default cross-modal only loss. Compared to recent state-of-the-art cross-modal self-supervised learning meth-ods (XDC [6], GDT [106], AVID-CMA [97], SeLaVi [9]) pre-trained on Kinetics-400 [69] STiCA is able to achieve signiﬁcantly better results in fewer epochs. 