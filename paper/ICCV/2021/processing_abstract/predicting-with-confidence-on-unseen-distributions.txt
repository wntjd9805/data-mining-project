Recent work has shown that the accuracy of machine learning models can vary substantially when evaluated on a distribution that even slightly differs from that of the train-ing data. As a result, predicting model performance on pre-viously unseen distributions without access to labeled data is an important challenge with implications for increasing the reliability of machine learning models. In the context of distribution shift, distance measures are often used to adapt models and improve their performance on new domains, however accuracy estimation is seldom explored in these investigations. Our investigation determines that common distributional distances such as Frechet distance or Maxi-mum Mean Discrepancy, fail to induce reliable estimates of performance under distribution shift. On the other hand, we ﬁnd that our proposed difference of conﬁdences (DoC) approach yields successful estimates of a classiﬁer’s perfor-mance over a variety of shifts and model architectures. De-spite its simplicity, we observe that DoC outperforms other methods across synthetic, natural, and adversarial distribu-tion shifts, reducing error by (> 46%) on several realistic and challenging datasets such as ImageNet-Vid-Robust andImageNet-Rendition. 