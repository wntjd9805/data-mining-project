Knowledge distillation (KD) transfers the dark knowl-edge from cumbersome networks (teacher) to lightweight (student) networks and expects the student to achieve more promising performance than training without the teacherâ€™s knowledge. However, a counter-intuitive argument is that better teachers do not make better students due to the ca-pacity mismatch. To this end, we present a novel adaptive knowledge distillation method to complement traditional approaches. The proposed method, named as Student Cus-tomized Knowledge Distillation (SCKD), examines the ca-pacity mismatch between teacher and student from the per-spective of gradient similarity. We formulate the knowl-edge distillation as a multi-task learning problem so that the teacher transfers knowledge to the student only if the student can benefit from learning such knowledge. We vali-date our methods on multiple datasets with various teacher-student configurations on image classification, object detec-tion, and semantic segmentation. 