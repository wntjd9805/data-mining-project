In this paper, we abandon the dominant complex lan-guage model and rethink the linguistic learning process in the scene text recognition. Different from previous meth-ods considering the visual and linguistic information in two separate structures, we propose a Visual Language Mod-eling Network (VisionLAN), which views the visual and lin-guistic information as a union by directly enduing the vision model with language capability. Specially, we introduce the text recognition of character-wise occluded feature maps in the training stage. Such operation guides the vision model to use not only the visual texture of characters, but also the linguistic information in visual context for recognition when the visual cues are confused (e.g. occlusion, noise, etc.).As the linguistic information is acquired along with visual features without the need of extra language model, Vision-LAN significantly improves the speed by 39% and adap-tively considers the linguistic information to enhance the visual features for accurate recognition. Furthermore, anOcclusion Scene Text (OST) dataset is proposed to eval-uate the performance on the case of missing character-wise visual cues. The state of-the-art results on several benchmarks prove our effectiveness. Code and dataset are available at https://github.com/wangyuxin87/VisionLAN . 