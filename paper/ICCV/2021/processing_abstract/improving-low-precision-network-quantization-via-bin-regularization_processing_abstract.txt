Model quantization is an important mechanism for energy-efficient deployment of deep neural networks on resource-constrained devices by reducing the bit precision of weights and activations. However, it remains challenging to maintain high accuracy as bit precision decreases, espe-cially for low-precision networks (e.g., 2-bit MobileNetV2).Existing methods have been explored to address this prob-lem by minimizing the quantization error or mimicking the data distribution of full-precision networks. In this work, we propose a novel weight regularization algorithm for improv-Instead of con-ing low-precision network quantization. straining the overall data distribution, we separably opti-mize all elements in each quantization bin to be as close to the target quantized value as possible. Such bin regu-larization (BR) mechanism encourages the weight distribu-tion of each quantization bin to be sharp and approximate to a Dirac delta distribution ideally. Experiments demon-strate that our method achieves consistent improvements over the state-of-the-art quantization-aware training meth-ods for different low-precision networks. Particularly, our bin regularization improves LSQ for 2-bit MobileNetV2 andMobileNetV3-Small by 3.9% and 4.9% top-1 accuracy onImageNet, respectively. 