Deep convolutional networks have recently achieved great success in video recognition, yet their practical re-alization remains a challenge due to the large amount of computational resources required to achieve robust recog-nition. Motivated by the effectiveness of quantization for boosting efficiency, in this paper, we propose a dynamic network quantization framework, that selects optimal pre-cision for each frame conditioned on the input for efficient video recognition. Specifically, given a video clip, we train a very lightweight network in parallel with the recognition network, to produce a dynamic policy indicating which numerical precision to be used per frame in recognizing videos. We train both networks effectively using standard backpropagation with a loss to achieve both competitive per-formance and resource efficiency required for video recog-nition. Extensive experiments on four challenging diverse benchmark datasets demonstrate that our proposed approach provides significant savings in computation and memory us-age while outperforming the existing state-of-the-art meth-ods. Project page: https://cs-people.bu.edu/ sunxm/VideoIQ/project.html. 