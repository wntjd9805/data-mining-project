This paper targets at fast video moment retrieval (fastVMR), aiming to localize the target moment efﬁciently and accurately as queried by a given natural language sentence.We argue that most existing VMR approaches can be divid-ed into three modules namely video encoder, text encoder, and cross-modal interaction module, where the last module is the test-time computational bottleneck. To tackle this is-sue, we replace the cross-modal interaction module with a cross-modal common space, in which moment-query align-ment is learned and efﬁcient moment search can be per-formed. For the sake of robustness in the learned space, we propose a ﬁne-grained semantic distillation framework to transfer knowledge from additional semantic structures.Speciﬁcally, we build a semantic role tree that decomposes a query sentence into different phrases (subtrees). A hierar-chical semantic-guided attention module is designed to per-form message propagation across the whole tree and yield discriminative features. Finally, the important and discrim-inative semantics are transferred to the common space by a matching-score distillation process. Extensive experimental results on three popular VMR benchmarks demonstrate that our proposed method enjoys the merits of high speed and signiﬁcant performance. 