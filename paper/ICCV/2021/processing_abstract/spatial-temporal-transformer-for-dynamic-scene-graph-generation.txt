Dynamic scene graph generation aims at generating a scene graph of the given video. Compared to the task of scene graph generation from images, it is more chal-lenging because of the dynamic relationships between ob-jects and the temporal dependencies between frames allow-ing for a richer semantic interpretation. In this paper, we propose Spatial-temporal Transformer (STTran), a neural network that consists of two core modules: (1) a spatial en-coder that takes an input frame to extract spatial context and reason about the visual relationships within a frame, and (2) a temporal decoder which takes the output of the spatial encoder as input in order to capture the temporal dependencies between frames and infer the dynamic rela-tionships. Furthermore, STTran is flexible to take varying lengths of videos as input without clipping, which is espe-cially important for long videos. Our method is validated on the benchmark dataset Action Genome (AG). The ex-perimental results demonstrate the superior performance of our method in terms of dynamic scene graphs. More-over, a set of ablative studies is conducted and the effect of each proposed module is justified. Code available at: https://github.com/yrcong/STTran. 