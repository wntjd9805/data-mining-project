Deep neural networks (DNNs) are known to perform well when deployed to test distributions that shares high similarity with the training distribution. Feeding DNNs with new data sequentially that were unseen in the training dis-tribution has two major challenges â€” fast adaptation to new tasks and catastrophic forgetting of old tasks. Such difficul-ties paved way for the on-going research on few-shot learn-ing and continual learning. To tackle these problems, we introduce Attentive Independent Mechanisms (AIM). We in-corporate the idea of learning using fast and slow weights in conjunction with the decoupling of the feature extraction and higher-order conceptual learning of a DNN. AIM is de-signed for higher-order conceptual learning, modeled by a mixture of experts that compete to learn independent con-cepts to solve a new task. AIM is a modular component that can be inserted into existing deep learning frameworks. We demonstrate its capability for few-shot learning by adding it to SIB and trained on MiniImageNet and CIFAR-FS, show-ing significant improvement. AIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and MiniIma-geNet to demonstrate its capability in continual learning.Code made publicly available at https://github. com/huang50213/AIM-Fewshot-Continual. 