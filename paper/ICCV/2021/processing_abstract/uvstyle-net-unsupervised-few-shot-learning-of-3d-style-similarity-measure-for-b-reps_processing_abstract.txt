Boundary Representations (B-Reps) are the industry standard in 3D Computer Aided Design/Manufacturing (CAD/CAM) and industrial design due to their ﬁdelity in representing stylistic details. However, they have been ig-nored in the 3D style research. Existing 3D style metrics typically operate on meshes or point clouds, and fail to ac-count for end-user subjectivity by adopting ﬁxed deﬁnitions of style, either through crowd-sourcing for style labels or hand-crafted features. We propose UVStyle-Net, a style sim-ilarity measure for B-Reps that leverages the style signals in the second order statistics of the activations in a pre-trained (unsupervised) 3D encoder, and learns their relative impor-tance to a subjective end-user through few-shot learning.Our approach differs from all existing data-driven 3D style methods since it may be used in completely unsupervised settings, which is desirable given the lack of publicly avail-able labeled B-Rep datasets. More importantly, the few-shot learning accounts for the inherent subjectivity asso-ciated with style. We show quantitatively that our proposed method with B-Reps is able to capture stronger style signals than alternative methods on meshes and point clouds de-spite its signiﬁcantly greater computational efﬁciency. We also show it is able to generate meaningful style gradients with respect to the input shape, and that few-shot learning with as few as two positive examples selected by an end-user is sufﬁcient to signiﬁcantly improve the style measure.Finally, we demonstrate its efﬁcacy on a large unlabeled public dataset of CAD models. Source code and data are available at github.com/AutodeskAILab/UVStyle-Net. 