Traditional methods for RGB hand mesh recovery usu-ally need to train a separate model for each dataset with the corresponding ground truth and are hardly adapted to new scenarios without the ground truth for supervision. To ad-dress the problem, we propose a self-supervised framework for hand mesh estimation, where we pre-learn hand priors from existing hand datasets and transfer the priors to new scenarios without any landmark annotations. The proposed approach takes binocular images as input and mainly re-lies on left-right consistency constraints including appear-ance consensus and shape consistency to train the model to estimate the hand mesh in new scenarios. We conduct ex-periments on the widely used stereo hand dataset, and the experimental results verify that our model can get compa-rable performance compared with state-of-the-art methods even without the corresponding landmark annotations. To further evaluate our model, we collect a large real binocu-lar dataset. The experimental results on the collected real dataset also verify the effectiveness of our model qualita-tively. 