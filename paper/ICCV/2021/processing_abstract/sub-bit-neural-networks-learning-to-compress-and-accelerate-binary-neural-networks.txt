In the low-bit quantization ﬁeld, training Binarized Neu-ral Networks (BNNs) is the extreme solution to ease the de-ployment of deep models on resource-constrained devices, having the lowest storage cost and signiﬁcantly cheaper bit-wise operations compared to 32-bit ﬂoating-point coun-terparts. In this paper, we introduce Sub-bit Neural Net-works (SNNs), a new type of binary quantization design tai-lored to compress and accelerate BNNs. SNNs are inspired by an empirical observation, showing that binary kernels learnt at convolutional layers of a BNN model are likely to be distributed over kernel subsets. As a result, unlike existing methods that binarize weights one by one, SNNs are trained with a kernel-aware optimization framework, which exploits binary quantization in the ﬁne-grained con-volutional kernel space. Speciﬁcally, our method includes a random sampling step generating layer-speciﬁc subsets of the kernel space, and a reﬁnement step learning to adjust these subsets of binary kernels via optimization. Experi-ments on visual recognition benchmarks and the hardware deployment on FPGA validate the great potentials of SNNs.For instance, on ImageNet, SNNs of ResNet-18/ResNet-34 with 0.56-bit weights achieve 3.13/3.33× runtime speed-up and 1.8× compression over conventional BNNs with moderate drops in recognition accuracy. Promising re-sults are also obtained when applying SNNs to binarize both weights and activations. Our code is available at https://github.com/yikaiw/SNN . 