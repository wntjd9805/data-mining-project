Video highlight detection plays an increasingly impor-tant role in social media content filtering, however, it re-mains highly challenging to develop automated video high-light detection methods because of the lack of temporal an-notations (i.e., where the highlight moments are in long videos) for supervised learning. In this paper, we propose a novel weakly supervised method that can learn to de-tect highlights by mining video characteristics with video level annotations (topic tags) only. Particularly, we ex-ploit audio-visual features to enhance video representation and take temporal cues into account for improving detec-tion performance. Our contributions are threefold: 1) we propose an audio-visual tensor fusion mechanism that effi-ciently models the complex association between two modal-ities while reducing the gap of the heterogeneity between the two modalities; 2) we introduce a novel hierarchical temporal context encoder to embed local temporal clues in between neighboring segments; 3) finally, we alleviate the gradient vanishing problem theoretically during model op-timization with attention-gated instance aggregation. Ex-tensive experiments on two benchmark datasets (YouTubeHighlights and TVSum) have demonstrated our method out-performs other state-of-the-art methods with remarkable improvements. 