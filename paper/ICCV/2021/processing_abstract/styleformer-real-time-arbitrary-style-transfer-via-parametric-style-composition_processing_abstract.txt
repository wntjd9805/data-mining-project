In this work, we propose a new feed-forward arbitrary style transfer method, referred to as StyleFormer, which can simultaneously fulfill fine-grained style diversity and semantic content coherency. Specifically, our transformer-inspired feature-level stylization method consists of three modules: (a) the style bank generation module for sparse but compact parametric style pattern extraction, (b) the transformer-driven style composition module for content-guided global style composition, and (c) the parametric content modulation module for flexible but faithful styliza-tion. The output stylized images are impressively coher-ent with the content structure, sensitive to the detailed style variations, but still holistically adhere to the style distribu-tions from the style images. Qualitative and quantitative comparisons as well as comprehensive user studies demon-strate that our StyleFormer outperforms the existing SOTA methods in generating visually plausible stylization results with real-time efficiency. 