Temporal action proposal generation is an important and challenging task in video understanding, which aims at detecting all temporal segments containing action in-stances of interest. The existing proposal generation ap-proaches are generally based on pre-defined anchor win-dows or heuristic bottom-up boundary matching strategies.This paper presents a simple and efficient framework (RTD-Net) for direct action proposal generation, by re-purposing a Transformer-alike architecture. To tackle the essential visual difference between time and space, we make three important improvements over the original transformer de-tection framework (DETR). First, to deal with slowness prior in videos, we replace the original Transformer en-coder with a boundary attentive module to better capture long-range temporal information. Second, due to the am-biguous temporal boundary and relatively sparse annota-tions, we present a relaxed matching scheme to relieve the strict criteria of single assignment to each groundtruth. Fi-nally, we devise a three-branch head to further improve the proposal confidence estimation by explicitly predicting its completeness. Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effective-ness of RTD-Net, on both tasks of temporal action pro-posal generation and temporal action detection. Moreover, due to its simplicity in design, our framework is more ef-ficient than previous proposal generation methods, without non-maximum suppression post-processing. The code and models are made available at https://github.com/MCG-NJU/RTD-Action. 