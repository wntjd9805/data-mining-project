RGB-D semantic segmentation has attracted increasing attention over the past few years. Existing methods mostly employ homogeneous convolution operators to consume theRGB and depth features, ignoring their intrinsic differences.In fact, the RGB values capture the photometric appearance properties in the projected image space, while the depth fea-ture encodes both the shape of a local geometry as well as the base (whereabout) of it in a larger context. Compared with the base, the shape probably is more inherent and has a stronger connection to the semantics, and thus is more critical for segmentation accuracy. Inspired by this obser-vation, we introduce a Shape-aware Convolutional layer (ShapeConv) for processing the depth feature, where the depth feature is ﬁrstly decomposed into a shape-component and a base-component, next two learnable weights are in-troduced to cooperate with them independently, and ﬁnally a convolution is applied on the re-weighted combination of these two components. ShapeConv is model-agnostic and can be easily integrated into most CNNs to replace vanilla convolutional layers for semantic segmentation. Ex-tensive experiments on three challenging indoor RGB-D se-mantic segmentation benchmarks, i.e., NYU-Dv2(-13,-40),SUN RGB-D, and SID, demonstrate the effectiveness of ourShapeConv when employing it over ﬁve popular architec-tures. Moreover, the performance of CNNs with ShapeConv is boosted without introducing any computation and mem-ory increase in the inference phase. The reason is that the learnt weights for balancing the importance between the shape and base components in ShapeConv become con-stants in the inference phase, and thus can be fused into the following convolution, resulting in a network that is identi-cal to one with vanilla convolutional layers. 