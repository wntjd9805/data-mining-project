In video highlight detection, the goal is to identify the interesting moments within an unedited video. Although the audio component of the video provides important cues for highlight detection, the majority of existing efforts fo-cus almost exclusively on the visual component. In this pa-per, we argue that both audio and visual components of a video should be modeled jointly to retrieve its best moments.To this end, we propose an audio-visual network for video highlight detection. At the core of our approach lies a bi-modal attention mechanism, which captures the interaction between the audio and visual components of a video, and produces fused representations to facilitate highlight detec-tion. Furthermore, we introduce a noise sentinel technique to adaptively discount a noisy visual or audio modality. Em-pirical evaluations on two benchmark datasets demonstrate the superior performance of our approach over the state-of-the-art methods. 