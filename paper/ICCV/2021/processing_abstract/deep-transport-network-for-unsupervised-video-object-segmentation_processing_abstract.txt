The popular unsupervised video object segmentation methods fuse the RGB frame and optical ﬂow via a two-stream network. However, they cannot handle the distract-ing noises in each input modality, which may vastly de-teriorate the model performance. We propose to estab-lish the correspondence between the input modalities while suppressing the distracting signals via optimal structural matching. Given a video frame, we extract the dense lo-cal features from the RGB image and optical ﬂow, and treat them as two complex structured representations. TheWasserstein distance is then employed to compute the glob-al optimal ﬂows to transport the features in one modality to the other, where the magnitude of each ﬂow measures the extent of the alignment between two local features. To plug the structural matching into a two-stream network for end-to-end training, we factorize the input cost matrix into small spatial blocks and design a differentiable long-shortSinkhorn module consisting of a long-distant Sinkhorn lay-er and a short-distant Sinkhorn layer. We integrate the mod-ule into a dedicated two-stream network and dub our modelTransportNet. Our experiments show that aligning motion-appearance yields the state-of-the-art results on the popular video object segmentation datasets. 