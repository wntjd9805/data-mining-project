This work proposes a novel Deep Neural Network (DNN) quantization framework, namely RMSMP, with a Row-wiseMixed-Scheme and Multi-Precision approach. Specifically, this is the first effort to assign mixed quantization schemes and multiple precisions within layers – among rows of theDNN weight matrix, for simplified operations in hardware inference, while preserving accuracy. Furthermore, this pa-per makes a different observation from the prior work that the quantization error does not necessarily exhibit the layer-wise sensitivity, and actually can be mitigated as long as a certain portion of the weights in every layer are in higher precisions. This observation enables layer-wise uniformal-ity in the hardware implementation towards guaranteed in-ference acceleration, while still enjoying row-wise flexibil-ity of mixed schemes and multiple precisions to boost ac-curacy. The candidates of schemes and precisions are de-rived practically and effectively with a highly hardware-informative strategy to reduce the problem search space.With the offline determined ratio of different quantization schemes and precisions for all the layers, the RMSMP quan-tization algorithm uses Hessian and variance based method to effectively assign schemes and precisions for each row.The proposed RMSMP is tested for the image classification and natural language processing (BERT) applications, and achieves the best accuracy performance among state-of-the-arts under the same equivalent precisions. The RMSMP is implemented on FPGA devices, achieving 3.65× speedup in the end-to-end inference time for ResNet-18 on ImageNet, comparing with the 4-bit Fixed-point baseline. 