Event cameras are bio-inspired sensors that respond to brightness changes asynchronously and output in the form of event streams instead of frame-based images. They own outstanding advantages compared with traditional cam-eras: higher temporal resolution, higher dynamic range, and lower power consumption. However, the spatial resolu-tion of existing event cameras is insufficient and challeng-ing to be enhanced at the hardware level while maintain-ing the asynchronous philosophy of circuit design. There-fore, it is imperative to explore the algorithm of event stream super-resolution, which is a non-trivial task due to the sparsity and strong spatio-temporal correlation of the events from an event camera.In this paper, we propose an end-to-end framework based on spiking neural network for event stream super-resolution, which can generate high-resolution (HR) event stream from the input low-resolution (LR) event stream. A spatiotemporal constraint learning mechanism is proposed to learn the spatial and temporal distributions of the event stream simultaneously. We val-idate our method on four large-scale datasets and the re-sults show that our method achieves state-of-the-art perfor-mance. The satisfying results on two downstream applica-tions, i.e. object classification and image reconstruction, further demonstrate the usability of our method. To prove the application potential of our method, we deploy it on a mobile platform. The high-quality HR event stream gener-ated by our real-time system demonstrates the effectiveness and efficiency of our method. 