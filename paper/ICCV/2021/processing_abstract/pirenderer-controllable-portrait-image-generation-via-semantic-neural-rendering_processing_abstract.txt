Generating portrait images by controlling the motions of existing faces is an important task of great consequence to social media industries. For easy use and intuitive con-trol, semantically meaningful and fully disentangled pa-rameters should be used as modifications. However, many existing techniques do not provide such fine-grained con-trols or use indirect editing methods i.e. mimic motions of other individuals.In this paper, a Portrait Image NeuralRenderer (PIRenderer) is proposed to control the face mo-tions with the parameters of three-dimensional morphable face models (3DMMs). The proposed model can generate photo-realistic portrait images with accurate movements according to intuitive modifications. Experiments on both direct and indirect editing tasks demonstrate the superior-ity of this model. Meanwhile, we further extend this model to tackle the audio-driven facial reenactment task by ex-tracting sequential motions from audio inputs. We show that our model can generate coherent videos with convinc-ing movements from only a single reference image and a driving audio stream. Our source code is available at https://github.com/RenYurui/PIRender.Figure 1. Example results produced by our PIRenderer. This model can generate photo-realistic portrait images according to the user-specified motions (top), motions of another individual (mid-dle), and motions generated from audios (bottom). 