Image view synthesis has seen great success in recon-structing photorealistic visuals, thanks to deep learning and various novel representations. The next key step in immer-sive virtual experiences is view synthesis of dynamic scenes.However, several challenges exist due to the lack of high-quality training datasets, and the additional time dimen-sion for videos of dynamic scenes. To address this issue, we introduce a multi-view video dataset, captured with a custom 10-camera rig in 120FPS. The dataset contains 96 high-quality scenes showing various visual effects and hu-man interactions in outdoor scenes. We develop a new al-gorithm, Deep 3D Mask Volume, which enables temporally-stable view extrapolation from binocular videos of dynamic scenes, captured by static cameras. Our algorithm ad-dresses the temporal inconsistency of disocclusions by iden-tifying the error-prone areas with a 3D mask volume, and replaces them with static background observed throughout the video. Our method enables manipulation in 3D space as opposed to simple 2D masks, We demonstrate better tempo-ral stability than frame-by-frame static view synthesis meth-ods, or those that use 2D masks. The resulting view synthe-sis videos show minimal ï¬‚ickering artifacts and allow for larger translational movements. 