Recent research efforts in lifelong learning propose to grow a mixture of models to adapt to an increasing num-ber of tasks. The proposed methodology shows promising results in overcoming catastrophic forgetting. However, the theory behind these successful models is still not well un-derstood. In this paper, we perform the theoretical analy-sis for lifelong learning models by deriving the risk bounds based on the discrepancy distance between the probabilis-tic representation of data generated by the model and that corresponding to the target dataset.Inspired by the the-oretical analysis, we introduce a new lifelong learning ap-proach, namely the Lifelong Infinite Mixture (LIMix) model, which can automatically expand its network architectures or choose an appropriate component to adapt its param-eters for learning a new task, while preserving its previ-ously learnt information. We propose to incorporate the knowledge by means of Dirichlet processes by using a gat-ing mechanism which computes the dependence between the knowledge learnt previously and stored in each com-ponent, and a new set of data. Besides, we train a compactStudent model which can accumulate cross-domain repre-sentations over time and make quick inferences. The code is available at https://github.com/dtuzi123/Lifelong-infinite-mixture-model. 