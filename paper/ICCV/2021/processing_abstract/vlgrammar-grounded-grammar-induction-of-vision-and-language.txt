Cognitive grammar suggests that the acquisition of lan-guage grammar is grounded within visual structures. While grammar is an essential representation of natural language, it also exists ubiquitously in vision to represent the hi-erarchical part-whole structure. In this work, we study grounded grammar induction of vision and language in a joint learning framework. Speciﬁcally, we present VLGram-mar, a method that uses compound probabilistic context-free grammars (compound PCFGs) to induce the language grammar and the image grammar simultaneously. We pro-pose a novel contrastive learning framework to guide the joint learning of both modules. To provide a benchmark for the grounded grammar induction task, we collect a large-scale dataset, PARTIT, which contains human-written sen-tences that describe part-level semantics for 3D objects.Experiments on the PARTIT dataset show that VLGram-mar outperforms all baselines in image grammar induction and language grammar induction. The learned VLGram-mar naturally beneﬁts related downstream tasks. Specif-ically, it improves the image unsupervised clustering ac-curacy by 30%, and performs well in image retrieval and text retrieval. Notably, the induced grammar shows superior generalizability by easily generalizing to unseen categories.Code and pre-trained models are released at https:// github.com/evelinehong/VLGrammar. 