We present DepthInSpace, a self-supervised deep-learning method for depth estimation using a structured-light camera. The design of this method is motivated by the commercial use case of embedded depth sensors in nowadays smartphones. We ﬁrst propose to use estimated optical ﬂow from ambient information of multiple video frames as a complementary guide for training a single-frame depth estimation network, helping to preserve edges and reduce over-smoothing issues. Utilizing optical ﬂow, we also propose to fuse the data of multiple video framesIn particular, fused to get a more accurate depth map. depth maps are more robust in occluded areas and incur less in ﬂying pixels artifacts. We ﬁnally demonstrate that these more precise fused depth maps can be used as self-supervision for ﬁne-tuning a single-frame depth estimation network to improve its performance. Our models’ effec-tiveness is evaluated and compared with state-of-the-art models on both synthetic and our newly introduced real datasets. The implementation code, training procedure, and both synthetic and captured real datasets are available at https://www.idiap.ch/paper/depthinspace. 