We present a novel method for synthesizing both tem-porally and geometrically consistent street-view panoramic video from a single satellite image and camera trajectory.Existing cross-view synthesis approaches focus on images, while video synthesis in such a case has not yet received enough attention. For geometrical and temporal consis-tency, our approach explicitly creates a 3D point cloud rep-resentation of the scene and maintains dense 3D-2D corre-spondences across frames that reflect the geometric scene configuration inferred from the satellite view. As for syn-thesis in the 3D space, we implement a cascaded network architecture with two hourglass modules to generate point-wise coarse and fine features from semantics and per-class latent vectors, followed by projection to frames and an up-sampling module to obtain the final realistic video. By leveraging computed correspondences, the produced street-view video frames adhere to the 3D geometric scene struc-ture and maintain temporal consistency. Qualitative and quantitative experiments demonstrate superior results com-pared to other state-of-the-art synthesis approaches that ei-ther lack temporal consistency or realistic appearance. To the best of our knowledge, our work is the first one to syn-thesize cross-view images to videos.. 