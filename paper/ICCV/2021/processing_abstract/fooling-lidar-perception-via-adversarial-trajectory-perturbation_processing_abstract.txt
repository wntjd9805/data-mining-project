LiDAR point clouds collected from a moving vehicle are functions of its trajectories, because the sensor motion needs to be compensated to avoid distortions. When au-tonomous vehicles are sending LiDAR point clouds to deep networks for perception and planning, could the motion compensation consequently become a wide-open backdoor in those networks, due to both the adversarial vulnerability of deep learning and GPS-based vehicle trajectory estima-tion that is susceptible to wireless spooﬁng? We demon-strate such possibilities for the ﬁrst time: instead of directly attacking point cloud coordinates which requires tampering with the raw LiDAR readings, only adversarial spooﬁng of a self-driving car’s trajectory with small perturbations is enough to make safety-critical objects undetectable or de-tected with incorrect positions. Moreover, polynomial tra-jectory perturbation is developed to achieve a temporally-smooth and highly-imperceptible attack. Extensive experi-ments on 3D object detection have shown that such attacks not only lower the performance of the state-of-the-art de-tectors effectively, but also transfer to other detectors, rais-ing a red ﬂag for the community. The code is available on https://ai4ce.github.io/FLAT/. 