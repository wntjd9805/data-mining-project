Video Question Answering (Video QA) aims to give an answer to the question through semantic reasoning be-tween visual and linguistic information. Recently, han-dling large amounts of multi-modal video and language information of a video is considered important in the in-dustry. However, the current video QA models use deep features, suffered from signiﬁcant computational complex-ity and insufﬁcient representation capability both in train-ing and testing. Existing features are extracted using pre-trained networks after all the frames are decoded, which is not always suitable for video QA tasks.In this paper, we develop a novel deep neural network to provide videoQA features obtained from coded video bit-stream to re-duce the complexity. The proposed network includes sev-eral dedicated deep modules to both the video QA and the video compression system, which is the ﬁrst attempt at the video QA task. The proposed network is predominantly model-agnostic. It is integrated into the state-of-the-art net-works for improved performance without any computation-ally expensive motion-related deep models. The experimen-tal results demonstrate that the proposed network outper-forms the previous studies at lower complexity. https://github.com/Nayoung-Kim-ICP/VQAC 