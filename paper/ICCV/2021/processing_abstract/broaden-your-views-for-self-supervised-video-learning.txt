Most successful self-supervised learning methods are trained to align the representations of two independent views from the data. State-of-the-art methods in video are inspired by image techniques, where these two views are similarly extracted by cropping and augmenting the resulting crop.However, these methods miss a crucial element in the video domain: time. We introduce BraVe, a self-supervised learn-ing framework for video. In BraVe, one of the views has access to a narrow temporal window of the video while the other view has a broad access to the video content. Our models learn to generalise from the narrow view to the gen-eral content of the video. Furthermore, BraVe processes the views with different backbones, enabling the use of alterna-tive augmentations or modalities into the broad view such as optical ﬂow, randomly convolved RGB frames, audio or their combinations. We demonstrate that BraVe achieves state-of-the-art results in self-supervised representation learning on standard video and audio classiﬁcation benchmarks in-cluding UCF101, HMDB51, Kinetics, ESC-50 and AudioSet. 