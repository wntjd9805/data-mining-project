Event cameras, inspired by biological vision systems, provide a natural and data efficient representation of vi-sual information. Visual information is acquired in the form of events that are triggered by local brightness changes.However, because most brightness changes are triggered by relative motion of the camera and the scene, the events recorded at a single sensor location seldom correspond to the same world point. To extract meaningful informa-tion from event cameras, it is helpful to register events that were triggered by the same underlying world point. In this work we propose a new model of event data that captures its natural spatio-temporal structure. We start by develop-ing a model for aligned event data. That is, we develop a model for the data as though it has been perfectly reg-istered already. In particular, we model the aligned data as a spatio-temporal Poisson point process. Based on this model, we develop a maximum likelihood approach to reg-istering events that are not yet aligned. That is, we find transformations of the observed events that make them as likely as possible under our model. In particular we extract the camera rotation that leads to the best event alignment.We show new state of the art accuracy for rotational veloc-ity estimation on the DAVIS 240C dataset [20]. In addition, our method is also faster and has lower computational com-plexity than several competing methods. Code: https://github.com/pbideau/Event-ST-PPP 