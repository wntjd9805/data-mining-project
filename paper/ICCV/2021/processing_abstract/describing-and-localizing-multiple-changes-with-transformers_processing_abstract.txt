Change captioning tasks aim to detect changes in im-age pairs observed before and after a scene change and generate a natural language description of the changes.Existing change captioning studies have mainly focused on a single change. However, detecting and describing multiple changed parts in image pairs is essential for en-hancing adaptability to complex scenarios. We solve the above issues from three aspects: (i) We propose a simula-tion-based multi-change captioning dataset; (ii) We bench-mark existing state-of-the-art methods of single change cap-tioning on multi-change captioning; (iii) We further pro-pose Multi-Change Captioning transformers (MCCForm-ers) that identify change regions by densely correlating dif-ferent regions in image pairs and dynamically determines the related change regions with words in sentences. The proposed method obtained the highest scores on four con-ventional change captioning evaluation metrics for multi-change captioning. Additionally, our proposed method can separate attention maps for each change and performs well with respect to change localization. Moreover, the proposed framework outperformed the previous state-of-the-art meth-ods on an existing change captioning benchmark, CLEVR-Change, by a large margin (+6.1 on BLEU-4 and +9.7 onCIDEr scores), indicating its general ability in change cap-tioning tasks. The code and dataset are available at the project page 1. 