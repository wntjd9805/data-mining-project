In this paper, we present Co-scale conv-attentional imageTransformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms.First, the co-scale mechanism maintains the integrity ofTransformers’ encoder branches at individual scales, while allowing representations learned at different scales to ef-fectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mecha-nism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities.On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolu-tional neural networks and image/vision Transformers. The effectiveness of CoaT’s backbone is also illustrated on ob-ject detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks. 