We propose Anticipative Video Transformer (AVT), an end-to-end attention-based video modeling architecture that attends to the previously observed video in order to anticipate future actions. We train the model jointly to pre-dict the next action in a video sequence, while also learn-ing frame feature encoders that are predictive of succes-sive future frames’ features. Compared to existing tempo-ral aggregation strategies, AVT has the advantage of both maintaining the sequential progression of observed actions while still capturing long-range dependencies—both crit-ical for the anticipation task. Through extensive experi-ments, we show that AVT obtains the best reported per-formance on four popular action anticipation benchmarks:EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads; and it wins ﬁrst place in the EpicKitchens-100CVPR’21 challenge. 