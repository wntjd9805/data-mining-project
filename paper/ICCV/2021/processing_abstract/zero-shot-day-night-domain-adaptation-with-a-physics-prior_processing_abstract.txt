We explore the zero-shot setting for day-night domain adaptation. The traditional domain adaptation setting is to train on one domain and adapt to the target domain by ex-ploiting unlabeled data samples from the test set. As gath-ering relevant test data is expensive and sometimes even impossible, we remove any reliance on test data imagery and instead exploit a visual inductive prior derived from physics-based reﬂection models for domain adaptation. We cast a number of color invariant edge detectors as train-able layers in a convolutional neural network and evalu-ate their robustness to illumination changes. We show that the color invariant layer reduces the day-night distribu-tion shift in feature map activations throughout the network.We demonstrate improved performance for zero-shot day to night domain adaptation on both synthetic as well as nat-ural datasets in various tasks, including classiﬁcation, seg-mentation and place recognition. 