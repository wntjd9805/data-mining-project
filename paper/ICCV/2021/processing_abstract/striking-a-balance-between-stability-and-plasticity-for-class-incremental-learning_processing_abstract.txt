Class-incremental learning (CIL) aims at continuously updating a trained model with new classes (plasticity) with-out forgetting previously learned old ones (stability). Con-temporary studies resort to storing representative exem-plars for rehearsal or preventing consolidated model pa-rameters from drifting, but the former requires an addi-tional space for storing exemplars at every incremental phase while the latter usually shows poor model general-ization. In this paper, we focus on resolving the stability-plasticity dilemma in class-incremental learning where no exemplars from old classes are stored. To make a trade-off between learning new information and maintaining old knowledge, we reformulate a simple yet effective baseline method based on a cosine classiÔ¨Åer framework and recip-rocal adaptive weights. With the reformulated baseline, we present two new approaches to CIL by learning class-independent knowledge and multi-perspective knowledge, respectively. The former exploits class-independent knowl-edge to bridge learning new and old classes, while the lat-ter learns knowledge from different perspectives to facili-tate CIL. Extensive experiments on several widely used CIL benchmark datasets show the superiority of our approaches over the state-of-the-art methods. 