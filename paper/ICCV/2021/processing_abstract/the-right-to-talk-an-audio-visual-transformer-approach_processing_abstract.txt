Turn-taking has played an essential role in structuring the regulation of a conversation. The task of identifying the main speaker (who is properly taking his/her turn of speak-ing) and the interrupters (who are interrupting or react-ing to the main speakerâ€™s utterances) remains a challeng-ing task. Although some prior methods have partially ad-dressed this task, there still remain some limitations. Firstly, a direct association of Audio and Visual features may limit the correlations to be extracted due to different modalities.Secondly, the relationship across temporal segments help-ing to maintain the consistency of localization, separation and conversation contexts is not effectively exploited. Fi-nally, the interactions between speakers that usually con-tain the tracking and anticipatory decisions about transition to a new speaker is usually ignored. Therefore, this work in-troduces a new Audio-Visual Transformer approach to the problem of localization and highlighting the main speaker in both audio and visual channels of a multi-speaker con-versation video in the wild. The proposed method ex-ploits different types of correlations presented in both vi-sual and audio signals. The temporal audio-visual rela-tionships across spatial-temporal space are anticipated and optimized via the self-attention mechanism in a Transformer structure. Moreover, a newly collected dataset is introduced for the main speaker detection. To the best of our knowl-edge, it is one of the first studies that is able to automati-cally localize and highlight the main speaker in both visual and audio channels in multi-speaker conversation videos. 