Generalized zero-shot learning (GZSL) has achieved signiﬁcant progress, with many efforts dedicated to over-coming the problems of visual-semantic domain gap and seen-unseen bias. However, most existing methods di-rectly use feature extraction models trained on ImageNet alone, ignoring the cross-dataset bias between ImageNet and GZSL benchmarks. Such a bias inevitably results in poor-quality visual features for GZSL tasks, which poten-tially limits the recognition performance on both seen and unseen classes. In this paper, we propose a simple yet effec-tive GZSL method, termed feature reﬁnement for generalized zero-shot learning (FREE), to tackle the above problem.FREE employs a feature reﬁnement (FR) module that in-corporates semantic→visual mapping into a uniﬁed gener-ative model to reﬁne the visual features of seen and unseen class samples. Furthermore, we propose a self-adaptive margin center loss (SAMC-loss) that cooperates with a se-mantic cycle-consistency loss to guide FR to learn class- and semantically-relevant representations, and concatenate the features in FR to extract the fully reﬁned features. Exten-sive experiments on ﬁve benchmark datasets demonstrate the signiﬁcant performance gain of FREE over its baseline and current state-of-the-art methods. The code is available at https://github.com/shiming-chen/FREE . 