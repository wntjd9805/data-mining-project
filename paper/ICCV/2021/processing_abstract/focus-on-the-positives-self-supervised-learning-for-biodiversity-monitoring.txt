We address the problem of learning self-supervised rep-resentations from unlabeled image collections. Unlike exist-ing approaches that attempt to learn useful features by max-imizing similarity between augmented versions of each in-put image or by speculatively picking negative samples, we instead also make use of the natural variation that occurs in image collections that are captured using static monitor-ing cameras. To achieve this, we exploit readily available context data that encodes information such as the spatial and temporal relationships between the input images. We are able to learn representations that are surprisingly effec-tive for downstream supervised classification, by first iden-tifying high probability positive pairs at training time, i.e. those images that are likely to depict the same visual con-cept. For the critical task of global biodiversity monitoring, this results in image features that can be adapted to chal-lenging visual species classification tasks with limited hu-man supervision. We present results on four different cam-era trap image collections, across three different families of self-supervised learning methods, and show that careful image selection at training time results in superior perfor-mance compared to existing baselines such as conventional self-supervised training and transfer learning. 