Humans perform co-saliency detection by first summa-rizing the consensus knowledge in the whole group and then searching corresponding objects in each image. Previous methods usually lack robustness, scalability, or stability for the first process and simply fuse consensus features with im-age features for the second process. In this paper, we pro-pose a novel consensus-aware dynamic convolution model to explicitly and effectively perform the “summarize and search” process. To summarize consensus image features, we first summarize robust features for every single image using an effective pooling method and then aggregate cross-image consensus cues via the self-attention mechanism. By doing this, our model meets the scalability and stability re-quirements. Next, we generate dynamic kernels from con-sensus features to encode the summarized consensus knowl-edge. Two kinds of kernels are generated in a supplemen-tary way to summarize fine-grained image-specific consen-sus object cues and the coarse group-wise common knowl-edge, respectively. Then, we can effectively perform object searching by employing dynamic convolution at multiple scales. Besides, a novel and effective data synthesis method is also proposed to train our network. Experimental results on four benchmark datasets verify the effectiveness of our proposed method. Our code and saliency maps are avail-able at https://github.com/nnizhang/CADC. 