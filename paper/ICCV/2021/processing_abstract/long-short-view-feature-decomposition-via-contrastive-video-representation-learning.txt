Self-supervised video representation methods typically focus on the representation of temporal attributes in videos.However, the role of stationary versus non-stationary at-tributes is less explored: Stationary features, which remain similar throughout the video, enable the prediction of video-level action classes. Non-stationary features, which repre-sent temporally varying attributes, are more beneﬁcial for downstream tasks involving more ﬁne-grained temporal un-derstanding, such as action segmentation. We argue that a single representation to capture both types of features is sub-optimal, and propose to decompose the representation space into stationary and non-stationary features via con-trastive learning from long and short views, i.e. long video sequences and their shorter sub-sequences. Stationary fea-tures are shared between the short and long views, while non-stationary features aggregate the short views to match the corresponding long view. To empirically verify our ap-proach, we demonstrate that our stationary features work particularly well on an action recognition downstream task, while our non-stationary features perform better on action segmentation. Furthermore, we analyse the learned rep-resentations and ﬁnd that stationary features capture more temporally stable, static attributes, while non-stationary features encompass more temporally varying ones. 