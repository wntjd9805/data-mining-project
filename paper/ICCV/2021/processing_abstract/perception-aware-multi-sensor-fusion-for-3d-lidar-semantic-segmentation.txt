3D LiDAR (light detection and ranging) semantic seg-mentation is important in scene understanding for many applications, such as auto-driving and robotics. For ex-ample, for autonomous cars equipped with RGB cameras and LiDAR, it is crucial to fuse complementary information from different sensors for robust and accurate segmentation.Existing fusion-based methods, however, may not achieve promising performance due to the vast difference between the two modalities. In this work, we investigate a collabo-rative fusion scheme called perception-aware multi-sensor fusion (PMF) to exploit perceptual information from two modalities, namely, appearance information from RGB im-ages and spatio-depth information from point clouds. To this end, we first project point clouds to the camera coordi-nates to provide spatio-depth information for RGB images.Then, we propose a two-stream network to extract features from the two modalities, separately, and fuse the features by effective residual-based fusion modules. Moreover, we propose additional perception-aware losses to measure the perceptual difference between the two modalities. Extensive experiments on two benchmark data sets show the superi-ority of our method. For example, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8% in mIoU. 