We study self-supervised video representation learning, which is a challenging task due to 1) lack of labels for ex-plicit supervision; 2) unstructured and noisy visual infor-mation. Existing methods mainly use contrastive loss with video clips as the instances and learn visual representation by discriminating instances from each other, but they need a careful treatment of negative pairs by either relying on large batch sizes, memory banks, extra modalities or cus-tomized mining strategies, which inevitably includes noisy data. In this paper, we observe that the consistency between positive samples is the key to learn robust video represen-tation. Specifically, we propose two tasks to learn appear-ance and speed consistency, respectively. The appearance consistency task aims to maximize the similarity between two clips of the same video with different playback speeds.The speed consistency task aims to maximize the similarity between two clips with the same playback speed but dif-ferent appearance information. We show that optimizing the two tasks jointly consistently improves the performance on downstream tasks, e.g., action recognition and video re-trieval. Remarkably, for action recognition on the UCF-101 dataset, we achieve 90.8% accuracy without using any ex-tra modalities or negative pairs for unsupervised pretrain-ing, which outperforms the ImageNet supervised pretrained model. Codes and models will be available. 