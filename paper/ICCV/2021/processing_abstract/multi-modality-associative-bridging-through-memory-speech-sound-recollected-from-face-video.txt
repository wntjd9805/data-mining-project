In this paper, we introduce a novel audio-visual multi-modal bridging framework that can utilize both audio and visual information, even with uni-modal inputs. We exploit a memory network that stores source (i.e., visual) and tar-get (i.e., audio) modal representations, where source modal representation is what we are given, and target modal rep-resentations are what we want to obtain from the mem-ory network. We then construct an associative bridge be-tween source and target memories that considers the inter-relationship between the two memories. By learning the interrelationship through the associative bridge, the pro-posed bridging framework is able to obtain the target modal representations inside the memory network, even with the source modal input only, and it provides rich information for its downstream tasks. We apply the proposed frame-work to two tasks: lip reading and speech reconstruction from silent video. Through the proposed associative bridge and modality-specific memories, each task knowledge is en-riched with the recalled audio context, achieving state-of-the-art performance. We also verify that the associative bridge properly relates the source and target memories. 