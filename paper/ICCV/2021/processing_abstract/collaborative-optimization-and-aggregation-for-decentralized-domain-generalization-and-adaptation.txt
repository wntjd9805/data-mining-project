Contemporary domain generalization (DG) and multi-source unsupervised domain adaptation (UDA) methods mostly collect data from multiple domains together for joint optimization. However, this centralized training paradigm poses a threat to data privacy and is not applicable when data are non-shared across domains.In this work, we propose a new approach called Collaborative Optimiza-tion and Aggregation (COPA), which aims at optimizing a generalized target model for decentralized DG and UDA, where data from different domains are non-shared and pri-vate. Our base model consists of a domain-invariant fea-ture extractor and an ensemble of domain-speciﬁc classi-ﬁers. In an iterative learning process, we optimize a local model for each domain, and then centrally aggregate lo-cal feature extractors and assemble domain-speciﬁc classi-ﬁers to construct a generalized global model, without shar-ing data from different domains. To improve generaliza-tion of feature extractors, we employ hybrid batch-instance normalization and collaboration of frozen classiﬁers. For better decentralized UDA, we further introduce a predic-tion agreement mechanism to overcome local disparities to-wards central model aggregation. Extensive experiments onﬁve DG and UDA benchmark datasets show that COPA is capable of achieving comparable performance against the state-of-the-art DG and UDA methods without the need for centralized data collection in model training. 