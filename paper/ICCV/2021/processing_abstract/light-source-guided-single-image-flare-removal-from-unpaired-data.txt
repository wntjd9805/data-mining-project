Causally-taken images often suffer from ﬂare artifacts, due to the unintended reﬂections and scattering of light in-side the camera. However, as ﬂares may appear in a vari-ety of shapes, positions, and colors, detecting and remov-ing them entirely from an image is very challenging. Exist-ing methods rely on predeﬁned intensity and geometry pri-ors of ﬂares, and may fail to distinguish the difference be-tween light sources and ﬂare artifacts. We observe that the conditions of the light source in the image play an impor-tant role in the resulting ﬂares. In this paper, we present a deep framework with light source aware guidance for single-image ﬂare removal (SIFR). In particular, we ﬁrst de-tect the light source regions and the ﬂare regions separately, and then remove the ﬂare artifacts based on the light source aware guidance. By learning the underlying relationships between the two types of regions, our approach can remove different kinds of ﬂares from the image.In addition, in-stead of using paired training data which are difﬁcult to collect, we propose the ﬁrst unpaired ﬂare removal dataset and new cycle-consistency constraints to obtain more di-verse examples and avoid manual annotations. Extensive experiments demonstrate that our method outperforms the baselines qualitatively and quantitatively. We also show that our model can be applied to ﬂare effect manipulation (e.g., adding or changing image ﬂares). 