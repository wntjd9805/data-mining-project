Active visual exploration aims to assist an agent with a limited ﬁeld of view to understand its environment based on partial observations made by choosing the best viewing directions in the scene. Recent methods have tried to ad-dress this problem either by using reinforcement learning, which is difﬁcult to train, or by uncertainty maps, which are task-speciﬁc and can only be implemented for dense predic-tion tasks. In this paper, we propose the Glimpse-Attend-and-Explore model which: (a) employs self-attention to guide the visual exploration instead of task-speciﬁc uncer-tainty maps; (b) can be used for both dense and sparse prediction tasks; and (c) uses a contrastive stream to fur-ther improve the representations learned. Unlike previous works, we show the application of our model on multi-ple tasks like reconstruction, segmentation and classiﬁca-tion. Our model provides encouraging results while be-ing less dependent on dataset bias in driving the explo-ration. We further perform an ablation study to investi-gate the features and attention learned by our model. Fi-nally, we show that our self-attention module learns to at-tend different regions of the scene by minimizing the loss on the downstream task. Code: https://github.com/ soroushseifi/glimpse-attend-explore. 