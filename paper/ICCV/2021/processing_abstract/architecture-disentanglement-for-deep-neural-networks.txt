Understanding the inner workings of deep neural net-works (DNNs) is essential to provide trustworthy artiﬁcial intelligence techniques for practical applications. Existing studies typically involve linking semantic concepts to units or layers of DNNs, but fail to explain the inference process.In this paper, we introduce neural architecture disentangle-ment (NAD) to ﬁll the gap. Speciﬁcally, NAD learns to dis-entangle a pre-trained DNN into sub-architectures accord-ing to independent tasks, forming information ﬂows that describe the inference processes. We investigate whether, where, and how the disentanglement occurs through ex-periments conducted with handcrafted and automatically-searched network architectures, on both object-based and scene-based datasets. Based on the experimental results, we present three new ﬁndings that provide fresh insights into the inner logic of DNNs. First, DNNs can be di-vided into sub-architectures for independent tasks. Second, deeper layers do not always correspond to higher seman-tics. Third, the connection type in a DNN affects how the information ﬂows across layers, leading to different disen-tanglement behaviors. With NAD, we further explain whyDNNs sometimes give wrong predictions. Experimental re-sults show that misclassiﬁed images have a high proba-bility of being assigned to task sub-architectures similar to the correct ones. Our code is available at https://github.com/hujiecpp/NAD. 