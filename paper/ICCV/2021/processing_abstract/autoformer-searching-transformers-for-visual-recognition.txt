Recently, pure transformer-based models have shown great potentials for vision tasks such as image classiﬁca-tion and detection. However, the design of transformer net-works is challenging. It has been observed that the depth, embedding dimension, and number of heads can largely af-fect the performance of vision transformers. Previous mod-els conﬁgure these dimensions based upon manual craft-ing. In this work, we propose a new one-shot architecture search framework, namely AutoFormer, dedicated to vision transformer search. AutoFormer entangles the weights of different blocks in the same layers during supernet train-ing. Beneﬁting from the strategy, the trained supernet al-lows thousands of subnets to be very well-trained. Specif-ically, the performance of these subnets with weights in-herited from the supernet is comparable to those retrained from scratch. Besides, the searched models, which we re-fer to AutoFormers, surpass the recent state-of-the-arts such as ViT and DeiT. In particular, AutoFormer-tiny/small/base achieve 74.7%/81.7%/82.4% top-1 accuracy on ImageNet with 5.7M/22.9M/53.7M parameters, respectively. Lastly, we verify the transferability of AutoFormer by providing the performance on downstream benchmarks and distil-lation experiments. Code and models are available at https://github.com/microsoft/Cream. 