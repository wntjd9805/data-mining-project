We present a novel approach to robustly detect and per-ceive vehicles in different camera views as part of a coop-erative vehicle-infrastructure system (CVIS). Our formula-tion is designed for arbitrary camera views and makes no assumptions about intrinsic or extrinsic parameters. First, to deal with multi-view data scarcity, we propose a part-assisted novel view synthesis algorithm for data augmenta-tion. We train a part-based texture inpainting network in a self-supervised manner. Then we render the textured model into the background image with the target 6-DoF pose. Sec-ond, to handle various camera parameters, we present a new method that produces dense mappings between im-age pixels and 3D points to perform robust 2D/3D vehicle parsing. Third, we build the first CVIS dataset for bench-marking, which annotates more than 1540 images (14017 instances) from real-world traffic scenarios. We combine these novel algorithms and datasets to develop a robust ap-proach for 2D/3D vehicle parsing for CVIS. In practice, our approach outperforms SOTA methods on 2D detection, in-stance segmentation, and 6-DoF pose estimation by 3.8%, 4.3%, and 2.9%, respectively. 