Recovering dense human poses from images plays a crit-ical role in establishing an image-to-surface correspon-dence between RGB images and the 3D surface of the hu-man body, serving the foundation of rich real-world appli-cations, such as virtual humans, monocular-to-3d recon-struction. However, the popular DensePose-COCO dataset relies on a sophisticated manual annotation system, lead-ing to severe limitations in acquiring the denser and moreIn this work, we in-accurate annotated pose resources. troduce a new 3D human-body model with a series of de-coupled parameters that could freely control the genera-tion of the body. Furthermore, we build a data generation system based on this decoupling 3D model, and construct an ultra dense synthetic benchmark UltraPose, containing around 1.3 billion corresponding points. Compared to the existing manually annotated DensePose-COCO dataset, the synthetic UltraPose has ultra dense image-to-surface cor-respondences without annotation cost and error. Our pro-posed UltraPose provides the largest benchmark and data resources for lifting the model capability in predicting more accurate dense poses. To promote future researches in this field, we also propose a transformer-based method to model the dense correspondence between 2D and 3D worlds. The proposed model trained on synthetic UltraPose can be ap-plied to real-world scenarios, indicating the effectiveness of our benchmark and model.1 