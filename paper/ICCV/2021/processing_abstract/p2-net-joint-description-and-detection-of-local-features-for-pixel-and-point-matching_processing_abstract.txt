Accurately describing and detecting 2D and 3D key-points is crucial to establishing correspondences across im-ages and point clouds. Despite a plethora of learning-based 2D or 3D local feature descriptors and detectors having been proposed, the derivation of a shared descrip-tor and joint keypoint detector that directly matches pix-els and points remains under-explored by the community.This work takes the initiative to establish ﬁne-grained cor-respondences between 2D images and 3D point clouds.In order to directly match pixels and points, a dual fully-convolutional framework is presented that maps 2D and 3D inputs into a shared latent representation space to simul-taneously describe and detect keypoints. Furthermore, an ultra-wide reception mechanism and a novel loss function are designed to mitigate the intrinsic information variations between pixel and point local regions. Extensive experimen-tal results demonstrate that our framework shows competi-tive performance in ﬁne-grained matching between images and point clouds and achieves state-of-the-art results for the task of indoor visual localization. Our source code is avail-able at https://github.com/BingCS/P2-Net. 