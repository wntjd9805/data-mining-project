Video captioning is an important vision task and has been intensively studied in the computer vision community.Existing methods that utilize the ﬁne-grained spatial infor-mation have achieved signiﬁcant improvements, however, they either rely on costly external object detectors or do not sufﬁciently model the spatial/temporal relations. In this pa-per, we aim at designing a spatial information extraction and aggregation method for video captioning without the need of external object detectors. For this purpose, we pro-pose a Recurrent Region Attention module to better extract diverse spatial features, and by employing Motion-GuidedCross-frame Message Passing, our model is aware of the temporal structure and able to establish high-order rela-tions among the diverse regions across frames. They jointly encourage information communication and produce com-pact and powerful video representations. Furthermore, anAdjusted Temporal Graph Decoder is proposed to ﬂexibly update video features and model high-order temporal rela-tions during decoding. Experimental results on three bench-mark datasets: MSVD, MSR-VTT, and VATEX demonstrate that our proposed method can outperform state-of-the-art methods. 