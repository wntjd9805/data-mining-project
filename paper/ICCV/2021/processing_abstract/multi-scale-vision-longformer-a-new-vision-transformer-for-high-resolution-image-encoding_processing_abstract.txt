This paper presents a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer, which sig-nificantly enhances the ViT of[12] for encoding high-resolution images using two techniques. The first is the multi-scale model structure, which provides image encod-ings at multiple scales with manageable computational cost. The second is the attention mechanism of Vision Long-former, which is a variant of Longformer [3], originally developed for natural language processing, and achieves a linear complexity w.r.t. the number of input tokens. A com-prehensive empirical study shows that the new ViT signifi-cantly outperforms several strong baselines, including the existing ViT models and their ResNet counterparts, and thePyramid Vision Transformer from a concurrent work [47], on a range of vision tasks, including image classification, object detection, and segmentation. The models and source code are released at https://github.com/microsoft/vision-longformer. 