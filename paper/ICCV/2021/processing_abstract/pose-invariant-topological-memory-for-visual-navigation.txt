Planning for visual navigation using topological mem-ory, a memory graph consisting of nodes and edges, has been recently well-studied. The nodes correspond to past observations of a robot, and the edges represent the reacha-bility predicted by a neural network (NN). Most prior meth-ods, however, often fail to predict the reachability when the robot takes different poses, i.e. the direction the robot faces, at close positions. This is because the methods ob-serve ﬁrst-person view images, which signiﬁcantly changes when the robot changes its pose, and thus it is funda-mentally difﬁcult to correctly predict the reachability from them.In this paper, we propose pose invariant topologi-cal memory (POINT) to address the problem. POINT ob-serves omnidirectional images and predicts the reachability by using a spherical convolutional NN, which has a rota-tion invariance property and enables planning regardless of the robot’s pose. Additionally, we train the NN by con-trastive learning with data augmentation to enable POINT to plan with robustness to changes in environmental condi-tions, such as light conditions and the presence of unseen objects. Our experimental results show that POINT outper-forms conventional methods under both the same and differ-ent environmental conditions. In addition, the results with the KITTI-360 dataset show that POINT is more applicable to real-world environments than conventional methods. 