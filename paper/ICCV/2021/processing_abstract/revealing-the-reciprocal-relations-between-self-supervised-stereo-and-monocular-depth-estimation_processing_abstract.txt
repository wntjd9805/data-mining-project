Current self-supervised depth estimation algorithms mainly focus on either stereo or monocular only, neglect-ing the reciprocal relations between them. In this paper, we propose a simple yet effective framework to improve both stereo and monocular depth estimation by leveraging the underlying complementary knowledge of the two tasks. Our approach consists of three stages. In the ﬁrst stage, the pro-posed stereo matching network termed StereoNet is trained on image pairs in a self-supervised manner. Second, we introduce an occlusion-aware distillation (OA Distillation) module, which leverages the predicted depths from Stere-oNet in non-occluded regions to train our monocular depth estimation network named SingleNet. At last, we design an occlusion-aware fusion module (OA Fusion), which gener-ates more reliable depths by fusing estimated depths fromStereoNet and SingleNet given the occlusion map. Fur-thermore, we also take the fused depths as pseudo labels to supervise StereoNet in turn, which brings StereoNet’s performance to a new height. Extensive experiments onKITTI dataset demonstrate the effectiveness of our proposed framework. We achieve new SOTA performance on both stereo and monocular depth estimation tasks.Figure 1. Characteristics of stereo and monocular models. In the upper part, we paste a car ‘instance’ to both left image Il and right image Ir, respectively. Dl(stereo) and Dl(mono) are left dispar-ity maps generated from the stereo and monocular models, where the brighter color means bigger disparity.In the lower left, we move the car in left image a distance to the right, and the estimated disparity of the car becomes larger, as shown in D(cid:48) l(stereo). In the lower right, we shrink the car, and see the corresponding dis-parity becomes smaller, as shown in D(cid:48)(cid:48) l(mono). 