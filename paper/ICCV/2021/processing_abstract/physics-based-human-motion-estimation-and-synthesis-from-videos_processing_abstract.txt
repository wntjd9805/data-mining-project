Human motion synthesis is an important problem with applications in graphics, gaming and simulation environ-ments for robotics. Existing methods require accurate mo-tion capture data for training, which is costly to obtain.Instead, we propose a framework for training generative models of physically plausible human motion directly from monocular RGB videos, which are much more widely avail-able. At the core of our method is a novel optimization formulation that corrects imperfect image-based pose esti-mations by enforcing physics constraints and reasons about contacts in a differentiable way. This optimization yields corrected 3D poses and motions, as well as their corre-sponding contact forces. Results show that our physically-corrected motions signiﬁcantly outperform prior work on pose estimation. We can then use these to train a gener-ative model to synthesize future motion. We demonstrate both qualitatively and quantitatively signiﬁcantly improved motion estimation, synthesis quality and physical plausibil-ity achieved by our method on the large scale Human3.6m dataset [12] as compared to prior kinematic and physics-based methods. By enabling learning of motion synthesis from video, our method paves the way for large-scale, real-istic and diverse motion synthesis. 