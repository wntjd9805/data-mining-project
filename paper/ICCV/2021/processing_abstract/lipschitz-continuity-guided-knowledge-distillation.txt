Knowledge distillation has become one of the most im-portant model compression techniques by distilling knowl-edge from larger teacher networks to smaller student ones.Although great success has been achieved by prior dis-tillation methods via delicately designing various types of knowledge, they overlook the functional properties of neu-ral networks, which makes the process of applying those techniques to new tasks unreliable and non-trivial. To al-leviate such problem, in this paper, we initially leverageLipschitz continuity to better represent the functional char-acteristic of neural networks and guide the knowledge dis-tillation process. In particular, we propose a novel Lips-chitz Continuity Guided Knowledge Distillation framework to faithfully distill knowledge by minimizing the distance between two neural networksâ€™ Lipschitz constants, which enables teacher networks to better regularize student net-works and improve the corresponding performance. We de-rive an explainable approximation algorithm with an ex-plicit theoretical derivation to address the NP-hard prob-lem of calculating the Lipschitz constant. Experimental re-sults have shown that our method outperforms other bench-marks over several knowledge distillation tasks (e.g., classi-fication, segmentation and object detection) on CIFAR-100,ImageNet, and PASCAL VOC datasets. Our code is avail-able at https://github.com/42Shawn/LONDON/ tree/master. 