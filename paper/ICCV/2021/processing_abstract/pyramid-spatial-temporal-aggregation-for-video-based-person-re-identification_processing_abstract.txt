Video-based person re-identification aims to associate the video clips of the same person across multiple non-overlapping cameras. Spatial-temporal representations can provide richer and complementary information between frames, which are crucial to distinguish the target per-son when occlusion occurs. This paper proposes a novelPyramid Spatial-Temporal Aggregation (PSTA) framework to aggregate the frame-level features progressively and fuse the hierarchical temporal features into a final video-level representation. Thus, short-term and long-term temporal information could be well exploited by different hierar-chies. Furthermore, a Spatial-Temporal Aggregation Mod-ule (STAM) is proposed to enhance the aggregation capa-bility of PSTA. It mainly consists of two novel attention blocks: Spatial Reference Attention (SRA) and TemporalReference Attention (TRA). SRA explores the spatial cor-relations within a frame to determine the attention weight of each location. While TRA extends SRA with the cor-relations between adjacent frames, temporal consistency information can be fully explored to suppress the inter-ference features and strengthen the discriminative ones.Extensive experiments on several challenging benchmarks demonstrate the effectiveness of the proposed PSTA, and our full model reaches 91.5% and 98.3% Rank-1 accu-racy on MARS and DukeMTMC-VID benchmarks. The source code is available at https://github.com/WangYQ9/VideoReID-PSTA. 