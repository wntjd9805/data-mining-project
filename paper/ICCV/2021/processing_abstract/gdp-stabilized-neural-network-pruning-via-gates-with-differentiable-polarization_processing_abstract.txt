Model compression techniques are recently gaining ex-plosive attention for obtaining efficient AI models for var-ious real time applications. Channel pruning is one im-portant compression strategy, and widely used in slimming various DNNs. Previous gate-based or importance-based pruning methods aim to remove channels whose “impor-tance” are smallest. However, it remains unclear what cri-teria the channel importance should be measured on, lead-ing to various channel selection heuristics. Some other sampling-based pruning methods deploy sampling strategy to train sub-nets, which often causes the training instabil-ity and the compressed model’s degraded performance. In view of the research gaps, we present a new module namedGates with Differentiable Polarization (GDP), inspired by principled optimization ideas. GDP can be plugged before convolutional layers without bells and whistles, to control the on-and-off of each channel or whole layer block. Dur-ing the training process, the polarization effect will drive a subset of gates to smoothly decrease to exact zero, while other gates gradually stay away from zero by a large mar-gin. When training terminates, those zero-gated channels can be painlessly removed, while other non-zero gates can be absorbed into the succeeding convolution kernel, caus-ing completely no interruption to training nor damage to the trained model. Experiments conducted over CIFAR-10 and ImageNet datasets show that the proposed GDP algo-rithm achieves the state-of-the-art performance on various benchmark DNNs at a broad range of pruning ratios. We also apply GDP to DeepLabV3Plus-ResNet50 on the chal-lenging Pascal VOC segmentation task, whose test perfor-mance sees no drop (even slightly improved) with over 60%FLOPs saving. 