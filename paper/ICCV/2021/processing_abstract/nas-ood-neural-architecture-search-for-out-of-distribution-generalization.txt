Recent advances on Out-of-Distribution (OoD) gener-alization reveal the robustness of deep learning models against distribution shifts. However, existing works focus on OoD algorithms, such as invariant risk minimization, domain generalization, or stable learning, without consid-ering the influence of deep model architectures on OoD gen-eralization, which may lead to sub-optimal performance.Neural Architecture Search (NAS) methods search for ar-chitecture based on its performance on the training data, which may result in poor generalization for OoD tasks. In this work, we propose robust Neural Architecture Search forOoD generalization (NAS-OoD), which optimizes the archi-tecture with respect to its performance on generated OoD data by gradient descent. Specifically, a data generator is learned to synthesize OoD data by maximizing losses com-puted by different neural architectures, while the goal for architecture search is to find the optimal architecture pa-rameters that minimize the synthetic OoD data losses. The data generator and the neural architecture are jointly opti-mized in an end-to-end manner, and the minimax training process effectively discovers robust architectures that gen-eralize well for different distribution shifts. Extensive ex-perimental results show that NAS-OoD achieves superior performance on various OoD generalization benchmarks with deep models having a much fewer number of param-eters. In addition, on a real industry dataset, the proposedNAS-OoD method reduces the error rate by more than 70% compared with the state-of-the-art method, demonstrating the proposed methodâ€™s practicality for real applications.*Nanyang Ye is the corresponding author.Figure 1. NAS-OoD performs significantly better than existingOoD generalization baselines in terms of test accuracy and net-work parameter numbers. The upper left points are better than lower right ones because they have higher test accuracy and lower parameter numbers. 