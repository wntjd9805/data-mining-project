In recent years, the growing utilization of medical imag-ing is placing an increasing burden on radiologists. Deep learning provides a promising solution for automatic medi-cal image analysis and clinical decision support. However, large-scale manually labeled datasets required for training deep neural networks are difficult and expensive to obtain for medical images. The purpose of this work is to develop label-efficient multimodal medical imaging representations by leveraging radiology reports. We propose an attention-based framework for learning global and local represen-tations by contrasting image sub-regions and words in the paired report.In addition, we propose methods to lever-age the learned representations for various downstream medical image recognition tasks with limited labels. Our results demonstrate high-performance and label-efficiency for image-text retrieval, classification (finetuning and zeros-shot settings), and segmentation on different datasets. 