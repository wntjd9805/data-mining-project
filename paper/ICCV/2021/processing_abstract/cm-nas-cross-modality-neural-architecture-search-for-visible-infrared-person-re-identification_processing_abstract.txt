Visible-Infrared person re-identiﬁcation (VI-ReID) aims to match cross-modality pedestrian images, breaking through the limitation of single-modality person ReID in dark environment. In order to mitigate the impact of large modality discrepancy, existing works manually design var-ious two-stream architectures to separately learn modality-speciﬁc and modality-sharable representations.Such a manual design routine, however, highly depends on mas-sive experiments and empirical practice, which is time con-suming and labor intensive.In this paper, we system-atically study the manually designed architectures, and identify that appropriately separating Batch Normalization (BN) layers is the key to bring a great boost towards cross-modality matching. Based on this observation, the essen-tial objective is to ﬁnd the optimal separation scheme for each BN layer. To this end, we propose a novel method, named Cross-Modality Neural Architecture Search (CM-NAS). It consists of a BN-oriented search space in which the standard optimization can be fulﬁlled subject to the cross-modality task. Equipped with the searched archi-tecture, our method outperforms state-of-the-art counter-parts in both two benchmarks, improving the Rank-1/mAP by 6.70%/6.13% on SYSU-MM01 and by 12.17%/11.23% on RegDB. Code is released at https://github.com/JDAI-CV/CM-NAS. 