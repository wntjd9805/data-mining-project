Generalized zero-shot learning (GZSL) aims to classify samples under the assumption that some classes are not ob-servable during training. To bridge the gap between the seen and unseen classes, most GZSL methods attempt to as-sociate the visual features of seen classes with attributes or to generate unseen samples directly. Nevertheless, the vi-sual features used in the prior approaches do not necessar-ily encode semantically related information that the shared attributes refer to, which degrades the model generaliza-tion to unseen classes. To address this issue, in this paper, we propose a novel semantics disentangling framework for the generalized zero-shot learning task (SDGZSL), where the visual features of unseen classes are Ô¨Årstly estimated by a conditional VAE and then factorized into semantic-consistent and semantic-unrelated latent vectors. In partic-ular, a total correlation penalty is applied to guarantee the independence between the two factorized representations, and the semantic consistency of which is measured by the derived relation network. Extensive experiments conducted on four GZSL benchmark datasets have evidenced that the semantic-consistent features disentangled by the pro-posed SDGZSL are more generalizable in tasks of canon-ical and generalized zero-shot learning. Our source code is available at https://github.com/uqzhichen/SDGZSL. 