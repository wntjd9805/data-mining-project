The unsupervised domain adaptation (UDA) has been widely adopted to alleviate the data scalability issue, while the existing works usually focus on classifying independently discrete labels. However, in many tasks (e.g., medical diag-nosis), the labels are discrete and successively distributed.The UDA for ordinal classification requires inducing non-trivial ordinal distribution prior to the latent space. Tar-get for this, the partially ordered set (poset) is defined for constraining the latent vector. Instead of the typically i.i.d.Gaussian latent prior, in this work, a recursively conditionalGaussian (RCG) set is adapted for ordered constraint mod-eling, which admits a tractable joint distribution prior. Fur-thermore, we are able to control the density of content vector that violates the poset constraints by a simple “three-sigma rule”. We explicitly disentangle the cross-domain images into a shared ordinal prior induced ordinal content space and two separate source/target ordinal-unrelated spaces, and the self-training is worked on the shared space exclu-sively for ordinal-aware domain alignment. Extensive exper-iments on UDA medical diagnoses and facial age estimation demonstrate its effectiveness. 