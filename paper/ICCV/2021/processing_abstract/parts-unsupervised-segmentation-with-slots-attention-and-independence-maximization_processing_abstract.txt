From an early age, humans perceive the visual world as composed of coherent objects with distinctive properties such as shape, size, and color. There is great interest in build-ing models that are able to learn similar structure, ideally in an unsupervised manner. Learning such structure from com-plex 3D scenes that include clutter, occlusions, interactions, and camera motion is still an open challenge. We present a model that is able to segment visual scenes from complex 3D environments into distinct objects, learn disentangled representations of individual objects, and form consistent and coherent predictions of future frames, in a fully un-supervised manner. Our model (named PARTS) builds on recent approaches that utilize iterative amortized inference and transition dynamics for deep generative models. We achieve dramatic improvements in performance by introduc-ing several novel contributions. We introduce a recurrent slot-attention like encoder which allows for top-down inﬂu-ence during inference. We argue that when inferring scene structure from image sequences it is better to use a ﬁxed prior which is shared across the sequence rather than an auto-regressive prior as often used in prior work. We demon-strate our model’s success on three different video datasets (the popular benchmark CLEVRER; a simulated 3D Play-room environment; and a real-world Robotics Arm dataset).Finally, we analyze the contributions of the various model components and the representations learned by the model. 