Convolutional Neural Networks (CNNs) use pooling to decrease the size of activation maps. This process is cru-cial to increase the receptive ﬁelds and to reduce computa-tional requirements of subsequent convolutions. An impor-tant feature of the pooling operation is the minimization of information loss, with respect to the initial activation maps, without a signiﬁcant impact on the computation and mem-ory overhead. To meet these requirements, we propose Soft-Pool: a fast and efﬁcient method for exponentially weighted activation downsampling. Through experiments across a range of architectures and pooling methods, we demon-strate that SoftPool can retain more information in the re-duced activation maps. This reﬁned downsampling leads to improvements in a CNN’s classiﬁcation accuracy. Ex-periments with pooling layer substitutions on ImageNet1K show an increase in accuracy over both original architec-tures and other pooling methods. We also test SoftPool on video datasets for action recognition. Again, through the direct replacement of pooling layers, we observe consistent performance improvements while computational loads and memory requirements remain limited1. 