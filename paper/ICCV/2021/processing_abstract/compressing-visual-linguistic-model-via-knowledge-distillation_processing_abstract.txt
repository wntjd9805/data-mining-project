Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a smallVL model.In this paper, we study knowledge distillation (KD) to effectively compress a transformer based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention dis-tributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Stu-dent’s detector while the features are from Teacher’s own object detector. With aligned network inputs, the adaptedTeacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block, and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly im-proves the performance of small VL models on image cap-It reaches tioning and visual question answering tasks. 120.8 in CIDEr score on COCO captioning, an improve-ment of 5.1 over its non-distilled counterpart; and an accu-racy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effective-ness of VL distillation in both pre-training and fine-tuning stages. 