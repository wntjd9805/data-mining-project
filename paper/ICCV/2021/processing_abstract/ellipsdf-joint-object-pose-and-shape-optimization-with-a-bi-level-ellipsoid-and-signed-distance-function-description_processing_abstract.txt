Autonomous systems need to understand the semantics and geometry of their surroundings in order to comprehend and safely execute object-level task specifications. This pa-per proposes an expressive yet compact model for joint ob-ject pose and shape optimization, and an associated opti-mization algorithm to infer an object-level map from multi-view RGB-D camera observations. The model is expressive because it captures the identities, positions, orientations, and shapes of objects in the environment. It is compact be-cause it relies on a low-dimensional latent representation of implicit object shape, allowing onboard storage of large multi-category object maps. Different from other works that rely on a single object representation format, our approach has a bi-level object model that captures both the coarse level scale as well as the fine level shape details. Our ap-proach is evaluated on the large-scale real-world ScanNet dataset and compared against state-of-the-art methods. 