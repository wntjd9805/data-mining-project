Low-latency deep spiking neural networks (SNNs) have become a promising alternative to conventional artificial neural networks (ANNs) because of their potential for in-creased energy efficiency on event-driven neuromorphic hardware.Neural networks, including SNNs, however, are subject to various adversarial attacks and must be trained to remain resilient against such attacks for many appli-cations. Nevertheless, due to prohibitively high training costs associated with SNNs, an analysis and optimization of deep SNNs under various adversarial attacks have beenIn this paper, we first present a de-largely overlooked. tailed analysis of the inherent robustness of low-latencySNNs against popular gradient-based attacks, namely fast gradient sign method (FGSM) and projected gradient de-scent (PGD). Motivated by this analysis, to harness the model’s robustness against these attacks we present an SNN training algorithm that uses crafted input noise and in-curs no additional training time. To evaluate the mer-its of our algorithm, we conducted extensive experiments with variants of VGG and ResNet on both CIFAR-10 andCIFAR-100 dataset. Compared to standard trained direct-input SNNs, our trained models yield improved classifica-tion accuracy of up to 13.7% and 10.1% on FGSM andPGD attack generated images, respectively, with negligi-ble loss in clean image accuracy. Our models also out-perform inherently-robust SNNs trained on rate-coded in-puts with improved or similar classification performance on attack-generated images while having up to 25× and∼4.6× lower latency and computation energy, respectively.For reproducibility, we have open-sourced the code at github.com/ksouvik52/hiresnn2021. 