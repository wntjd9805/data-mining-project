Being able to learn dense semantic representations of images without supervision is an important problem in com-puter vision. However, despite its signiﬁcance, this problem remains rather unexplored, with a few exceptions that con-sidered unsupervised semantic segmentation on small-scale datasets with a narrow visual domain.In this paper, we make a ﬁrst attempt to tackle the problem on datasets that have been traditionally utilized for the supervised case. To achieve this, we introduce a two-step framework that adopts a predetermined mid-level prior in a contrastive optimiza-tion objective to learn pixel embeddings. This marks a large deviation from existing works that relied on proxy tasks or end-to-end clustering. Additionally, we argue about the im-portance of having a prior that contains information about objects, or their parts, and discuss several possibilities to obtain such a prior in an unsupervised manner.Experimental evaluation shows that our method comes with key advantages over existing works. First, the learned pixel embeddings can be directly clustered in semantic groups using K-Means on PASCAL. Under the fully unsu-pervised setting, there is no precedent in solving the se-mantic segmentation task on such a challenging benchmark.Second, our representations can improve over strong base-lines when transferred to new datasets, e.g. COCO andDAVIS. The code is available1. 