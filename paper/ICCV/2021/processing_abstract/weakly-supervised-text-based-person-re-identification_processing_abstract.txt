The conventional text-based person re-identiﬁcation methods heavily rely on identity annotations. However, this labeling process is costly and time-consuming. In this pa-per, we consider a more practical setting called weakly supervised text-based person re-identiﬁcation, where only the text-image pairs are available without the requirement of annotating identities during the training phase. To this end, we propose a Cross-Modal Mutual Training (CMMT) framework. Speciﬁcally, to alleviate the intra-class vari-ations, a clustering method is utilized to generate pseudo labels for both visual and textual instances. To further re-ﬁne the clustering results, CMMT provides a Mutual PseudoLabel Reﬁnement module, which leverages the clustering results in one modality to reﬁne that in the other modality constrained by the text-image pairwise relationship. Mean-while, CMMT introduces a Text-IoU Guided Cross-ModalProjection Matching loss to resolve the cross-modal match-ing ambiguity problem. A Text-IoU Guided Hard SampleMining method is also proposed for learning discrimina-tive textual-visual joint embeddings. We conduct exten-sive experiments to demonstrate the effectiveness of the pro-posed CMMT, and the results show that CMMT performs fa-vorably against existing text-based person re-identiﬁcation methods. Our code will be available at https:// github.com/X-BrainLab/WS_Text-ReID. 