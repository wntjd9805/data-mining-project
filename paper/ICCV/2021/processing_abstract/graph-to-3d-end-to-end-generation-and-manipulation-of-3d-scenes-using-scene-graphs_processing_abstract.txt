Controllable scene synthesis consists of generating 3D information that satisfy underlying speciﬁcations. Thereby, these speciﬁcations should be abstract, i.e. allowing easy user interaction, whilst providing enough interface for de-tailed control. Scene graphs are representations of a scene, composed of objects (nodes) and inter-object relationships (edges), proven to be particularly suited for this task, as they allow for semantic control on the generated content.Previous works tackling this task often rely on synthetic data, and retrieve object meshes, which naturally limits the generation capabilities. To circumvent this issue, we in-stead propose the ﬁrst work that directly generates shapes from a scene graph in an end-to-end manner. In addition, we show that the same model supports scene modiﬁcation, using the respective scene graph as interface. LeveragingGraph Convolutional Networks (GCN) we train a varia-tional Auto-Encoder on top of the object and edge cate-gories, as well as 3D shapes and scene layouts, allowing latter sampling of new scenes and shapes. 