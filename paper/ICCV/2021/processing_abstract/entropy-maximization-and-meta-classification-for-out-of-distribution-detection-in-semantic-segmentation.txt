Deep neural networks (DNNs) for the semantic segmen-tation of images are usually trained to operate on a pre-defined closed set of object classes. This is in contrast to the “open world” setting where DNNs are envisioned to be deployed to. From a functional safety point of view, the abil-ity to detect so-called “out-of-distribution” (OoD) samples, i.e., objects outside of a DNN’s semantic space, is crucial for many applications such as automated driving. A natu-ral baseline approach to OoD detection is to threshold on the pixel-wise softmax entropy. We present a two-step pro-cedure that significantly improves that approach. Firstly, we utilize samples from the COCO dataset as OoD proxy and introduce a second training objective to maximize the softmax entropy on these samples. Starting from pretrained semantic segmentation networks we re-train a number ofDNNs on different in-distribution datasets and consistently observe improved OoD detection performance when eval-uating on completely disjoint OoD datasets. Secondly, we perform a transparent post-processing step to discard false positive OoD samples by so-called “meta classification.”To this end, we apply linear models to a set of hand-crafted metrics derived from the DNN’s softmax probabilities. In our experiments we consistently observe a clear additional gain in OoD detection performance, cutting down the num-ber of detection errors by 52% when comparing the best baseline with our results. We achieve this improvement sac-rificing only marginally in original segmentation perfor-mance. Therefore, our method contributes to safer DNNs with more reliable overall system performance. 