Automation of neural architecture design has been a cov-eted alternative to human experts. Various search meth-ods have been proposed aiming to ﬁnd the optimal architec-ture in the search space. One would expect the search re-sults to improve when the search space grows larger since it would potentially contain more performant candidates.Surprisingly, we observe that enlarging search space is unbeneﬁcial or even detrimental to existing NAS methods such as DARTS, ProxylessNAS, and SPOS. This counterin-tuitive phenomenon suggests that enabling existing methods to large search space regimes is non-trivial. However, this problem is less discussed in the literature.We present a Neural Search-space Evolution (NSE) scheme, the ﬁrst neural architecture search scheme de-signed especially for large space neural architecture search problems. The necessity of a well-designed search space with constrained size is a tacit consent in existing methods, and our NSE aims at minimizing such necessity. Specif-ically, the NSE starts with a search space subset, then evolves the search space by repeating two steps: 1) search an optimized space from the search space subset, 2) re-ﬁll this subset from a large pool of operations that are not traversed. We further extend the ﬂexibility of obtain-able architectures by introducing a learnable multi-branch setting. With the proposed method, we achieve 77.3% top-1 retrain accuracy on ImageNet with 333M FLOPs, which yielded a state-of-the-art performance among pre-involve vious auto-generated architectures that do not knowledge distillation or weight pruning. When the la-tency constraint is adopted, our result also performs bet-ter than the previous best-performing mobile models with a 77.9% Top-1 retrain accuracy. Code is available at https://github.com/orashi/NSE NAS. 