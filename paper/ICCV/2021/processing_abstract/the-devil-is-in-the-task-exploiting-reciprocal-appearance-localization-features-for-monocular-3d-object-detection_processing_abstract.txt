Low-cost monocular 3D object detection plays a funda-mental role in autonomous driving, whereas its accuracy is still far from satisfactory. In this paper, we dig into the 3D object detection task and reformulate it as the sub-tasks of object localization and appearance perception, which benefits to a deep excavation of reciprocal information un-derlying the entire task. We introduce a Dynamic FeatureReflecting Network, named DFR-Net, which contains two novel standalone modules: (i) the Appearance-LocalizationFeature Reflecting module (ALFR) that first separates task-specific features and then self-mutually reflects the recipro-cal features; (ii) the Dynamic Intra-Trading module (DIT) that adaptively realigns the training processes of various sub-tasks via a self-learning manner. Extensive experiments on the challenging KITTI dataset demonstrate the effective-ness and generalization of DFR-Net. We rank 1st among all the monocular 3D object detectors in the KITTI test set (tillMarch 16th, 2021). The proposed method is also easy to be plug-and-play in many cutting-edge 3D detection frame-works at negligible cost to boost performance. The code will be made publicly available. 