We consider the problem of Federated Learning (FL) where numerous decentralized computational nodes collab-orate with each other to train a centralized machine learn-ing model without explicitly sharing their local data sam-ples. Such decentralized training naturally leads to issues of imbalanced or differing data distributions among the lo-cal models and challenges in fusing them into a central model. Existing FL methods deal with these issues by either sharing local parameters or fusing models via online dis-tillation. However, such a design leads to multiple rounds of inter-node communication resulting in substantial band-width consumption, while also increasing the risk of data leakage and consequent privacy issues. To address these problems, we propose a new distillation-based FL frame-work that can preserve privacy by design, while also con-suming substantially less network communication resources when compared to the current methods. Our framework engages in inter-node communication using only publicly available and approved datasets, thereby giving explicit pri-vacy control to the user. To distill knowledge among the various local models, our framework involves a novel en-semble distillation algorithm that uses both final prediction as well as model attention. This algorithm explicitly consid-ers the diversity among various local nodes while also seek-ing consensus among them. This results in a comprehensive technique to distill knowledge from various decentralized nodes. We demonstrate the various aspects and the asso-ciated benefits of our FL framework through extensive ex-periments that produce state-of-the-art results on both clas-sification and segmentation tasks on natural and medical images. 