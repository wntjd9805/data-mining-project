DETR is a recently proposed Transformer-based method which views object detection as a set prediction prob-lem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we inves-tigate the causes of the optimization difﬁculty in the train-ing of DETR. Our examinations reveal several factors con-tributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we pro-pose two solutions, namely, TSP-FCOS (Transformer-basedSet Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also signiﬁcantly out-perform DETR and other baselines in terms of detection accuracy. Code is released at https://github.com/Edward-Sun/TSP-Detection. 