For many fundamental scene understanding tasks, it is difﬁcult or impossible to obtain per-pixel ground truth la-bels from real images. We address this challenge by in-troducing Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding. To create our dataset, we leverage a large repository of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with detailed per-pixel labels and corre-sponding ground truth geometry. Our dataset: (1) relies ex-clusively on publicly available 3D assets; (2) includes com-plete scene geometry, material information, and lighting in-formation for every scene; (3) includes dense per-pixel se-mantic instance segmentations and complete camera infor-mation for every image; and (4) factors every image into diffuse reﬂectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects.We analyze our dataset at the level of scenes, objects, and pixels, and we analyze costs in terms of money, com-putation time, and annotation effort. Remarkably, weﬁnd that it is possible to generate our entire dataset from scratch, for roughly half the cost of training a popular open-source natural language processing model. We also eval-uate sim-to-real transfer performance on two real-world scene understanding tasks – semantic segmentation and 3D shape prediction – where we ﬁnd that pre-training on our dataset signiﬁcantly improves performance on both tasks, and achieves state-of-the-art performance on the most chal-lenging Pix3D test set. All of our rendered image data, as well as all the code we used to generate our dataset and perform our experiments, is available online.Figure 1. Overview of the Hypersim dataset. For each color image (a), Hypersim includes the following ground truth layers: depth (b); surface normals (c); instance-level semantic segmen-tations (d,e); diffuse reﬂectance (f); diffuse illumination (g); and a non-diffuse residual image that captures view-dependent light-ing effects like glossy surfaces and specular highlights (h). Our diffuse reﬂectance, diffuse illumination, and non-diffuse residual layers are stored as HDR images, and can be composited together to exactly reconstruct the color image. 