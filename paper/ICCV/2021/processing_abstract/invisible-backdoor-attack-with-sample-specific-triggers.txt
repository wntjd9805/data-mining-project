Recently, backdoor attacks pose a new security threat to the training process of deep neural networks (DNNs). At-tackers intend to inject hidden backdoors into DNNs, such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if hidden backdoors are activated by the attacker-deﬁned trigger. Ex-isting backdoor attacks usually adopt the setting that trig-gers are sample-agnostic, i.e., different poisoned samples contain the same trigger, resulting in that the attacks could be easily mitigated by current backdoor defenses. In this work, we explore a novel attack paradigm, where backdoor triggers are sample-speciﬁc. In our attack, we only need to modify certain training samples with invisible perturba-tion, while not need to manipulate other training compo-nents (e.g., training loss, and model structure) as required in many existing attacks. Speciﬁcally, inspired by the recent advance in DNN-based image steganography, we generate sample-speciﬁc invisible additive noises as backdoor trig-gers by encoding an attacker-speciﬁed string into benign images through an encoder-decoder network. The mapping from the string to the target label will be generated whenDNNs are trained on the poisoned dataset. Extensive ex-periments on benchmark datasets verify the effectiveness of our method in attacking models with or without defenses.The code will be available at https://github.com/ yuezunli/ISSBA. 