We introduce Task Switching Networks (TSNs), a task-conditioned architecture with a single uniﬁed en-coder/decoder for efﬁcient multi-task learning. Multiple tasks are performed by switching between them, perform-ing one task at a time. TSNs have a constant number of pa-rameters irrespective of the number of tasks. This scalable yet conceptually simple approach circumvents the overhead and intricacy of task-speciﬁc network components in ex-isting works.In fact, we demonstrate for the ﬁrst time that multi-tasking can be performed with a single task-conditioned decoder. We achieve this by learning task-speciﬁc conditioning parameters through a jointly trained task embedding network, encouraging constructive interac-tion between tasks. Experiments validate the effectiveness of our approach, achieving state-of-the-art results on two challenging multi-task benchmarks, PASCAL-Context andNYUD. Our analysis of the learned task embeddings fur-ther indicates a connection to task relationships studied in the recent literature. 