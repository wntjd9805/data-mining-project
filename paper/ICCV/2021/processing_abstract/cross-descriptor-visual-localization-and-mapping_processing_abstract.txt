Visual localization and mapping is the key technology underlying the majority of mixed reality and robotics sys-tems. Most state-of-the-art approaches rely on local fea-tures to establish correspondences between images. In this paper, we present three novel scenarios for localization and mapping which require the continuous update of fea-ture representations and the ability to match across different feature types. While localization and mapping is a funda-mental computer vision problem, the traditional setup sup-poses the same local features are used throughout the evo-lution of a map. Thus, whenever the underlying features are changed, the whole process is repeated from scratch.However, this is typically impossible in practice, because raw images are often not stored and re-building the maps could lead to loss of the attached digital content. To over-come the limitations of current approaches, we present the first principled solution to cross-descriptor localization and mapping. Our data-driven approach is agnostic to the fea-ture descriptor type, has low computational requirements, and scales linearly with the number of description algo-rithms. Extensive experiments demonstrate the effectiveness of our approach on state-of-the-art benchmarks for a vari-ety of handcrafted and learned features. 