We present a novel framework for mesh reconstruction from unstructured point clouds by taking advantage of the learned visibility of the 3D points in the virtual views and traditional graph-cut based mesh generation. Speciﬁcally, we ﬁrst propose a three-step network that explicitly employs depth completion for visibility prediction. Then the visi-bility information of multiple views is aggregated to gen-erate a 3D mesh model by solving an optimization prob-lem considering visibility in which a novel adaptive visibil-ity weighting in surface determination is also introduced to suppress line of sight with a large incident angle. Compared to other learning-based approaches, our pipeline only exer-cises the learning on a 2D binary classiﬁcation task, i.e., points visible or not in a view, which is much more gener-alizable and practically more efﬁcient and capable to deal with a large number of points. Experiments demonstrate that our method with favorable transferability and robust-ness, and achieve competing performances w.r.t. state-of-the-art learning-based approaches on small complex ob-jects and outperforms on large indoor and outdoor scenes.Code is available at https://github.com/GDAOSU/vis2mesh. 