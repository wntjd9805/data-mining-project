Egocentric video recognition is a challenging task that requires to identify both the actor’s motion and the active object that the actor interacts with. Recognizing the ac-tive object is particularly hard due to the cluttered back-ground with distracting objects, the frequent ﬁeld of view changes, severe occlusion, etc. To improve the active ob-ject classiﬁcation, most existing methods use object detec-tors or human gaze information, which are computation-ally expensive or require labor-intensive annotations. To avoid these additional costs, we propose an end-to-endInteractive Prototype Learning (IPL) framework to learn better active object representations by leveraging the mo-tion cues from the actor. First, we introduce a set of verb prototypes to disentangle active object features from dis-tracting object features. Each prototype corresponds to a primary motion pattern of an egocentric action, offer-ing a distinctive supervision signal for active object fea-ture learning. Second, we design two interactive oper-ations to enable the extraction of active object features, i.e., noun-to-verb assignment and verb-to-noun selection.These operations are parameter-efﬁcient and can learn ju-dicious location-aware features on top of 3D CNN back-bones. We demonstrate that the IPL framework can gen-eralize to different backbones and outperform the state-of-the-art on three large-scale egocentric video datasets, i.e.,EPIC-KITCHENS-55, EPIC-KITCHENS-100 and EGTEA. 