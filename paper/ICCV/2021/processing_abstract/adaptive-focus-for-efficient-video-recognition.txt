In this paper, we explore the spatial redundancy in video recognition with the aim to improve the computational ef-It is observed that the most informative regionﬁciency. in each frame of a video is usually a small image patch, which shifts smoothly across frames. Therefore, we model the patch localization problem as a sequential decision task, and propose a reinforcement learning based approach for efﬁcient spatially adaptive video recognition (AdaFocus). In speciﬁc, a light-weighted ConvNet is ﬁrst adopted to quickly process the full video sequence, whose features are used by a recurrent policy network to localize the most task-relevant regions. Then the selected patches are inferred by a high-capacity network for the ﬁnal prediction. During ofﬂine in-ference, once the informative patch sequence has been gen-erated, the bulk of computation can be done in parallel, and is efﬁcient on modern GPU devices. In addition, we demon-strate that the proposed method can be easily extended by further considering the temporal redundancy, e.g., dynami-cally skipping less valuable frames. Extensive experiments on ﬁve benchmark datasets, i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is signiﬁcantly more efﬁcient than the competi-tive baselines. Code is available at https://github. com/blackfeather-wang/AdaFocus. 