In dynamic neural networks that adapt computations to different inputs, gating-based methods have demonstrated notable generality and applicability in trading-off the model complexity and accuracy. However, existing works only explore the redundancy from a single point of the net-In this paper, we pro-work, limiting the performance. pose dual gating, a new dynamic computing method, to re-duce the model complexity at run-time. For each convo-lutional block, dual gating identiﬁes the informative fea-tures along two separate dimensions, spatial and chan-nel. Speciﬁcally, the spatial gating module estimates which areas are essential, and the channel gating module pre-dicts the salient channels that contribute more to the re-sults. Then the computation of both unimportant regions and irrelevant channels can be skipped dynamically during inference. Extensive experiments on a variety of datasets demonstrate that our method can achieve higher accuracy under similar computing budgets compared with other dy-namic execution methods. In particular, dynamic dual gat-ing can provide 59.7% saving in computing of ResNet50 with 76.41% top-1 accuracy on ImageNet, which has ad-vanced the state-of-the-art. Codes are available at https://github.com/lfr-0531/DGNet. 