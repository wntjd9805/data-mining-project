Video is complex due to large variations in motion and rich content in ﬁne-grained visual details. Abstracting use-ful information from such information-intensive media re-quires exhaustive computing resources. This paper stud-ies a two-step alternative that ﬁrst condenses the video se-quence to an informative “frame” and then exploits off-the-shelf image recognition system on the synthetic frame. A valid question is how to deﬁne “useful information” and then distill it from a video sequence down to one synthetic frame. This paper presents a novel Informative Frame Syn-thesis (IFS) architecture that incorporates three objective tasks, i.e., appearance reconstruction, video categorization, motion estimation, and two regularizers, i.e., adversarial learning, color consistency. Each task equips the synthetic frame with one ability, while each regularizer enhances its visual quality. With these, by jointly learning the frame syn-thesis in an end-to-end manner, the generated frame is ex-pected to encapsulate the required spatio-temporal infor-mation useful for video analysis. Extensive experiments are conducted on the large-scale Kinetics dataset. When com-paring to baseline methods that map video sequence to a single image, IFS shows superior performance. More re-markably, IFS consistently demonstrates evident improve-ments on image-based 2D networks and clip-based 3D networks, and achieves comparable performance with the state-of-the-art methods with less computational cost. 