Generating conversational gestures from speech audio is challenging due to the inherent one-to-many mapping be-tween audio and body motions. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of all possible target motions, resulting in plain/boring motions during inference.In order to over-come this problem, we propose a novel conditional vari-ational autoencoder (VAE) that explicitly models one-to-many audio-to-motion mapping by splitting the cross-modal latent code into shared code and motion-speciﬁc code. The shared code mainly models the strong correlation between audio and motion (such as the synchronized audio and mo-tion beats), while the motion-speciﬁc code captures diverse motion information independent of the audio. However, splitting the latent code into two parts poses training dif-ﬁculties for the VAE model. A mapping network facilitat-ing random sampling along with other techniques includ-ing relaxed motion loss, bicycle constraint, and diversity loss are designed to better train the VAE. Experiments on both 3D and 2D motion datasets verify that our method generates more realistic and diverse motions than state-of-the-art methods, quantitatively and qualitatively. Finally, we demonstrate that our method can be readily used to generate motion sequences with user-speciﬁed motion clips on the timeline. Code and more results are at https://jingli513.github.io/audio2gestures. 