360◦ videos convey holistic views for the surroundings of a scene.It provides audio-visual cues beyond pre-determined normal ﬁeld of views and displays distinctive spatial relations on a sphere. However, previous bench-mark tasks for panoramic videos are still limited to eval-uate the semantic understanding of audio-visual relation-ships or spherical spatial property in surroundings. We propose a novel benchmark named Pano-AVQA as a large-scale grounded audio-visual question answering dataset on panoramic videos. Using 5.4K 360◦ video clips har-vested online, we collect two types of novel question-answer pairs with bounding-box grounding: spherical spatial re-lation QAs and audio-visual relation QAs. We train sev-eral transformer-based models from Pano-AVQA, where the results suggest that our proposed spherical spatial embed-dings and multimodal training objectives fairly contribute to a better semantic understanding of the panoramic sur-roundings on the dataset. 