The presence of objects that are confusingly similar to the tracked target, poses a fundamental challenge in appearance-based visual tracking. Such distractor objects are easily misclassified as the target itself, leading to even-tual tracking failure. While most methods strive to suppress distractors through more powerful appearance models, we take an alternative approach.To tackle the problem ofWe propose to keep track of distractor objects in or-der to continue tracking the target. To this end, we intro-duce a learned association network, allowing us to prop-agate the identities of all target candidates from frame-to-frame. lacking ground-truth correspondences between distractor objects in vi-sual tracking, we propose a training strategy that com-bines partial annotations with self-supervision. We con-duct comprehensive experimental validation and anal-ysis of our approach on several challenging datasets.Our tracker sets a new state-of-the-art on six bench-marks, achieving an AUC score of 67.1% on LaSOT [21] and a +5.8% absolute gain on the OxUvA long-term dataset [41]. The code and trained models are available at https://github.com/visionml/pytracking 