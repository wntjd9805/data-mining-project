Explainability for machine learning models has gained considerable attention within the research community given the importance of deploying more reliable machine-learning systems.In computer vision applications, gen-erative counterfactual methods indicate how to perturb a model’s input to change its prediction, providing details about the model’s decision-making. Current methods tend to generate trivial counterfactuals about a model’s deci-sions, as they often suggest to exaggerate or remove the presence of the attribute being classiﬁed. For the ma-chine learning practitioner, these types of counterfactu-als offer little value, since they provide no new informa-tion about undesired model or data biases. In this work, we identify the problem of trivial counterfactual genera-tion and we propose DiVE to alleviate it. DiVE learns a perturbation in a disentangled latent space that is con-strained using a diversity-enforcing loss to uncover multiple valuable explanations about the model’s prediction. Fur-ther, we introduce a mechanism to prevent the model from producing trivial explanations. Experiments on CelebA and Synbols demonstrate that our model improves the suc-cess rate of producing high-quality valuable explanations when compared to previous state-of-the-art methods. Code is available at https://github.com/ElementAI/ beyond-trivial-explanations. 