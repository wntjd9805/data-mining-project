Vision Transformer (ViT) extends the application range of transformers from language processing to computer vi-sion tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effective-ness on transformer-based architecture. We particularly at-tend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel di-mension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novelPooling-based Vision Transformer (PiT) upon the origi-nal ViT model. We show that PiT achieves the improved model capability and generalization performance againstViT. Throughout the extensive experiments, we further showPiT outperforms the baseline on several tasks such as im-age classification, object detection, and robustness evalua-tion. Source codes and ImageNet models are available at https://github.com/naver-ai/pit. 