Deep neural networks achieve great success in many vi-sual recognition tasks. However, the model deployment is usually subject to some computational resources. Model pruning under computational budget has attracted growing attention.In this paper, we focus on the discrimination-aware compression of Convolutional Neural Networks (CNNs). In prior arts, directly searching the optimal sub-network is an integer programming problem, which is non-smooth, non-convex, and NP-hard. Meanwhile, the heuris-tic pruning criterion lacks clear interpretability and doesn’t generalize well in applications. To address this problem, we formulate sub-networks as samples from a multivari-ate Bernoulli distribution and resort to the approximation of continuous problem. We propose a new ﬂexible search scheme via alternating exploration and estimation. In the exploration step, we employ stochastic gradient Hamilto-nian Monte Carlo with budget-awareness to generate sub-networks, which allows large search space with efﬁcient computation.In the estimation step, we deduce the sub-network sampler to a near-optimal point, to promote the generation of high-quality sub-networks. Unifying the ex-ploration and estimation, our approach avoids early falling into local minimum via a fast gradient-based search in a larger space. Extensive experiments on CIFAR-10 and Im-ageNet show that our method achieves state-of-the-art per-formances on pruning several popular CNNs. 