Image harmonization, aiming to make composite images look more realistic, is an important and challenging task.The composite, synthesized by combining foreground from one image with background from another image, inevitably suffers from the issue of inharmonious appearance caused by distinct imaging conditions, i.e., lights. Current solu-tions mainly adopt an encoder-decoder architecture with convolutional neural network (CNN) to capture the context of composite images, trying to understand what it looks like in the surrounding background near the foreground. In this work, we seek to solve image harmonization with Trans-former, by leveraging its powerful ability of modeling long-range context dependencies, for adjusting foreground light to make it compatible with background light while keep-ing structure and semantics unchanged. We present the de-sign of our harmonization Transformer frameworks with-out and with disentanglement, as well as comprehensive ex-periments and ablation study, demonstrating the power ofTransformer and investigating the Transformer for vision.Our method achieves state-of-the-art performance on both image harmonization and image inpainting/enhancement, indicating its superiority. Our code and models are avail-able at https://github.com/zhenglab/HarmonyTransformer. 