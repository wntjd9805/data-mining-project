Visual correspondence is a fundamental building block on the way to building assistive tools for hand-drawn ani-mation. However, while a large body of work has focused on learning visual correspondences at the pixel-level, few ap-proaches have emerged to learn correspondence at the level of line enclosures (segments) that naturally occur in hand-drawn animation. Exploiting this structure in animation has numerous benefits: it avoids the memory complexity of pixel attention over high resolution images and enables the use of real-world animation datasets that contain correspondence information at the level of per-segment colors. To that end, we propose the Animation Transformer (AnT) which uses aTransformer-based architecture to learn the spatial and vi-sual relationships between segments across a sequence of images. By leveraging a forward match loss and a cycle consistency loss our approach attains excellent results com-pared to state-of-the-art pixel approaches on challenging datasets from real animation productions that lack ground-truth correspondence labels. 