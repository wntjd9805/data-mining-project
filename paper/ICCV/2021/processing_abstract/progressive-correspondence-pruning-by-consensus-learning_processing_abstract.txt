Correspondence pruning aims to correctly remove false matches (outliers) from an initial set of putative correspon-dences. The pruning process is challenging since putative matches are typically extremely unbalanced, largely domi-nated by outliers, and the random distribution of such out-liers further complicates the learning process for learning-based methods. To address this issue, we propose to pro-gressively prune the correspondences via a local-to-global consensus learning procedure. We introduce a “pruning” block that lets us identify reliable candidates among the ini-tial matches according to consensus scores estimated us-ing local-to-global dynamic graphs. We then achieve pro-gressive pruning by stacking multiple pruning blocks se-quentially. Our method outperforms state-of-the-arts on ro-bust line ﬁtting, camera pose estimation and retrieval-based image localization benchmarks by signiﬁcant margins and shows promising generalization ability to different datasets and detector/descriptor combinations. 