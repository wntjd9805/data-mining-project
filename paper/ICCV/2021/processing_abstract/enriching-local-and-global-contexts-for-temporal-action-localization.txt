Effectively tackling the problem of temporal action lo-calization (TAL) necessitates a visual representation that jointly pursues two confounding goals, i.e., fine-grained dis-crimination for temporal localization and sufficient visual invariance for action classification. We address this chal-lenge by enriching both the local and global contexts in the popular two-stage temporal localization framework, where action proposals are first generated followed by action clas-sification and temporal boundary regression. Our proposed model, dubbed ContextLoc, can be divided into three sub-networks: L-Net, G-Net and P-Net. L-Net enriches the lo-cal context via fine-grained modeling of snippet-level fea-tures, which is formulated as a query-and-retrieval process.G-Net enriches the global context via higher-level model-ing of the video-level representation. In addition, we intro-duce a novel context adaptation module to adapt the global context to different proposals. P-Net further models the context-aware inter-proposal relations. We explore two ex-isting models to be the P-Net in our experiments. The effi-cacy of our proposed method is validated by experimental results on the THUMOS14 (54.3% at tIoU@0.5) and Activ-ityNet v1.3 (56.01% at tIoU@0.5) datasets, which outper-forms recent states of the art. Code is available at https://github.com/buxiangzhiren/ContextLoc. 