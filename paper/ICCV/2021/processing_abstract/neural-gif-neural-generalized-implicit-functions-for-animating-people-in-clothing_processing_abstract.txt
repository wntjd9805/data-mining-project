We present Neural Generalized Implicit Functions (Neural-GIF), to animate people in clothing as a function of the body pose. Given a sequence of scans of a subject in various poses, we learn to animate the character for new poses. Existing methods have relied on template-based rep-resentations of the human body (or clothing). However such models usually have fixed and limited resolutions, require difficult data pre-processing steps and cannot be used with complex clothing. We draw inspiration from template-based methods, which factorize motion into articulation and non-rigid deformation, but generalize this concept for implicit shape learning to obtain a more flexible model. We learn to map every point in the space to a canonical space, where a learned deformation field is applied to model non-rigid effects, before evaluating the signed distance field. Our for-mulation allows the learning of complex and non-rigid de-formations of clothing and soft tissue, without computing a template registration as it is common with current ap-proaches. Neural-GIF can be trained on raw 3D scans and reconstructs detailed complex surface geometry and defor-mations. Moreover, the model can generalize to new poses.We evaluate our method on a variety of characters from dif-ferent public datasets in diverse clothing styles and show significant improvements over baseline methods, quantita-tively and qualitatively. We also extend our model to mul-tiple shape setting. To stimulate further research, we will make the model, code and data publicly available at [1]. 