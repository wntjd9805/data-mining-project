Most recent approaches for online action detection tend to apply Recurrent Neural Network (RNN) to capture long-range temporal structure. However, RNN suffers from non-parallelism and gradient vanishing, hence it is hard to be optimized.In this paper, we propose a new encoder-decoder framework based on Transformers, named OadTR, to tackle these problems. The encoder attached with a task token aims to capture the relationships and global inter-actions between historical observations. The decoder ex-tracts auxiliary information by aggregating anticipated fu-ture clip representations. Therefore, OadTR can recognize current actions by encoding historical information and pre-dicting future context simultaneously. We extensively eval-uate the proposed OadTR on three challenging datasets:HDD, TVSeries, and THUMOS14. The experimental re-sults show that OadTR achieves higher training and infer-ence speeds than current RNN based approaches, and sig-nificantly outperforms the state-of-the-art methods in terms of both mAP and mcAP. Code is available at https://github.com/wangxiang1230/OadTR. 