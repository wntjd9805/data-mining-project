In this paper, we propose a generalizable mixed-precision quantization (GMPQ) method for efﬁcient in-ference. Conventional methods require the consistency of datasets for bitwidth search and model deployment to guarantee the policy optimality, leading to heavy search cost on challenging largescale datasets in realistic appli-cations. On the contrary, our GMPQ searches the mixed-quantization policy that can be generalized to largescale datasets with only a small amount of data, so that the search cost is signiﬁcantly reduced without performance degradation. Speciﬁcally, we observe that locating net-work attribution correctly is general ability for accurate vi-sual analysis across different data distribution. Therefore, despite of pursuing higher model accuracy and complex-ity, we preserve attribution rank consistency between the quantized models and their full-precision counterparts vi-a efﬁcient capacity-aware attribution imitation for gener-alizable mixed-precision quantization strategy search. Ex-tensive experiments show that our method obtains com-petitive accuracy-complexity trade-off compared with the state-of-the-art mixed-precision networks in signiﬁcant-ly reduced search cost. The code is available at http-s://github.com/ZiweiWangTHU/GMPQ.git. 