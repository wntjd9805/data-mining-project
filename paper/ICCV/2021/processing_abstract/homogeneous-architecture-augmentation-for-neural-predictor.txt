they also suffer a severe limitation:Neural Architecture Search (NAS) can automatically de-sign well-performed architectures of Deep Neural Networks (DNNs) for the tasks at hand. However, one bottleneck of NAS is the prohibitively computational cost largely due to the expensive performance evaluation. The neural pre-dictors can directly estimate the performance without any training of the DNNs to be evaluated, thus have drawn increasing attention from researchers. Despite their pop-ularity, the short-age of annotated DNN architectures for effectively train-ing the neural predictors. In this paper, we proposed Ho-mogeneous Architecture Augmentation for Neural Predictor (HAAP) of DNN architectures to address the issue afore-mentioned. Specifically, a homogeneous architecture aug-mentation algorithm is proposed in HAAP to generate suf-ficient training data taking the use of homogeneous repre-sentation. Furthermore, the one-hot encoding strategy is introduced into HAAP to make the representation of DNN architectures more effective. The experiments have been conducted on both NAS-Benchmark-101 and NAS-Bench-201 dataset. The experimental results demonstrate that the proposed HAAP algorithm outperforms the state of the arts compared, yet with much less training data. In addi-tion, the ablation studies on both benchmark datasets have also shown the universality of the homogeneous architec-ture augmentation. Our code has been made available at https://github.com/lyq998/HAAP. 