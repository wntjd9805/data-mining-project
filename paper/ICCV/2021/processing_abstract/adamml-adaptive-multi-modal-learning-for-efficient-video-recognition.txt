Multi-modal learning, which focuses on utilizing vari-ous modalities to improve the performance of a model, is widely used in video recognition. While traditional multi-modal learning offers excellent recognition results, its com-putational expense limits its impact for many real-world applications. In this paper, we propose an adaptive multi-modal learning framework, called AdaMML, that selects on-the-fly the optimal modalities for each segment condi-tioned on the input for efficient video recognition. Specif-ically, given a video segment, a multi-modal policy net-work is used to decide what modalities should be used for processing by the recognition model, with the goal of im-proving both accuracy and efficiency. We efficiently train the policy network jointly with the recognition model us-ing standard back-propagation. Extensive experiments on four challenging diverse datasets demonstrate that our pro-posed adaptive approach yields 35% âˆ’ 55% reduction in computation when compared to the traditional baseline that simply uses all the modalities irrespective of the in-put, while also achieving consistent improvements in ac-curacy over the state-of-the-art methods. Project page: https://rpand002.github.io/adamml.html. 