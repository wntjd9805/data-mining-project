Vision models notoriously flicker when applied to videos: they correctly recognize objects in some frames, but fail on perceptually similar, nearby frames. In this work, we system-atically analyze the robustness of image classifiers to such temporal perturbations in videos. To do so, we construct two new datasets, ImageNet-Vid-Robust and YTBB-Robust, containing a total of 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB, respectively, and thor-oughly re-annotated by human experts for image similarity.We evaluate a diverse array of classifiers pre-trained on Im-ageNet and show a median classification accuracy drop of 16 and 10 points, respectively, on our two datasets. Addi-tionally, we evaluate three detection models and show that natural perturbations induce both classification as well as lo-calization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and real-istic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions. 