In this paper, we propose a novel normalization method called gradient normalization (GN) to tackle the train-ing instability of Generative Adversarial Networks (GANs) caused by the sharp gradient space. Unlike existing work such as gradient penalty and spectral normalization, the proposed GN only imposes a hard 1-Lipschitz constraint on the discriminator function, which increases the capacity of the discriminator. Moreover, the proposed gradient normal-ization can be applied to different GAN architectures with little modification. Extensive experiments on four datasets show that GANs trained with gradient normalization out-perform existing methods in terms of both Frechet InceptionDistance and Inception Score. 