Understanding videos to localize moments with natural language often requires large expensive annotated video re-gions paired with language queries. To eliminate the an-notation costs, we make a first attempt to train a natu-ral language video localization model in zero-shot man-ner. Inspired by unsupervised image captioning setup, we merely require random text corpora, unlabeled video collec-tions, and an off-the-shelf object detector to train a model.With the unpaired data, we propose to generate pseudo-supervision of candidate temporal regions and correspond-ing query sentences, and develop a simple NLVL model to train with the pseudo-supervision. Our empirical vali-dations show that the proposed pseudo-supervised method outperforms several baseline approaches and a number of methods using stronger supervision on Charades-STA andActivityNet-Captions. 