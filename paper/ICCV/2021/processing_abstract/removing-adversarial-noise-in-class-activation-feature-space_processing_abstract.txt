Deep neural networks (DNNs) are vulnerable to adver-sarial noise. Pre-processing based defenses could largely remove adversarial noise by processing inputs. However, they are typically affected by the error ampliﬁcation effect, especially in the front of continuously evolving attacks. To solve this problem, in this paper, we propose to remove ad-versarial noise by implementing a self-supervised adversar-ial training mechanism in a class activation feature space.To be speciﬁc, we ﬁrst maximize the disruptions to class activation features of natural examples to craft adversar-ial examples. Then, we train a denoising model to mini-mize the distances between the adversarial examples and the natural examples in the class activation feature space.Empirical evaluations demonstrate that our method could signiﬁcantly enhance adversarial robustness in comparison to previous state-of-the-art approaches, especially against unseen adversarial attacks and adaptive attacks. 