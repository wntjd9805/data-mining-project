Self-supervised detection and segmentation of fore-ground objects aims for accuracy without annotated train-ing data. However, existing approaches predominantly rely on restrictive assumptions on appearance and motion.For scenes with dynamic activities and camera motion, we propose a multi-camera framework in which geometric constraints are embedded in the form of multi-view consis-tency during training via coarse 3D localization in a voxel grid and Ô¨Åne-grained offset regression. In this manner, we learn a joint distribution of proposals over multiple views.At inference time, our method operates on single RGB im-ages. We outperform state-of-the-art techniques both on im-ages that visually depart from those of standard benchmarks and on those of the classical Human3.6M dataset. 