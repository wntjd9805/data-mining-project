Many state-of-the-art few-shot learners focus on develop-ing eﬀective training procedures for feature representations, before using simple (e.g., nearest centroid) classiﬁers. We take an approach that is agnostic to the features used, and focus exclusively on meta-learning the ﬁnal classiﬁer layer. Specif-ically, we introduce MetaQDA, a Bayesian meta-learning generalisation of the classic quadratic discriminant analysis.This approach has several beneﬁts of interest to practition-ers: meta-learning is fast and memory eﬃcient, without the need to ﬁne-tune features. It is agnostic to the oﬀ-the-shelf features chosen, and thus will continue to beneﬁt from future advances in feature representations. Empirically, it leads to excellent performance in cross-domain few-shot learn-ing, class-incremental few-shot learning, and crucially for real-world applications, the Bayesian formulation leads to state-of-the-art uncertainty calibration in predictions. 