A large body of recent work has identiﬁed transforma-tions in the latent spaces of generative adversarial networks (GANs) that consistently and interpretably transform gener-ated images. But existing techniques for identifying these transformations rely on either a ﬁxed vocabulary of pre-speciﬁed visual concepts, or on unsupervised disentangle-ment techniques whose alignment with human judgments about perceptual salience is unknown. This paper intro-duces a new method for building open-ended vocabular-ies of primitive visual concepts represented in a GAN’s la-tent space. Our approach is built from three components: (1) automatic identiﬁcation of perceptually salient direc-tions based on their layer selectivity; (2) human annota-tion of these directions with free-form, compositional natu-ral language descriptions; and (3) decomposition of these annotations into a visual concept vocabulary, consisting of distilled directions labeled with single words. Experi-ments show that concepts learned with our approach are reliable and composable—generalizing across classes, con-texts, and observers, and enabling ﬁne-grained manipula-tion of image style and content. 