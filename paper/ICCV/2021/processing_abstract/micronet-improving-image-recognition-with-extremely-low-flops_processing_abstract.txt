This paper aims at addressing the problem of substantial performance degradation at extremely low computational cost (e.g. 5M FLOPs on ImageNet classification). We found that two factors, sparse connectivity and dynamic activa-tion function, are effective to improve the accuracy. The former avoids the significant reduction of network width, while the latter mitigates the detriment of reduction in net-work depth. Technically, we propose micro-factorized con-volution, which factorizes a convolution matrix into low rank matrices, to integrate sparse connectivity into convo-lution. We also present a new dynamic activation function, named Dynamic Shift Max, to improve the non-linearity via maxing out multiple dynamic fusions between an in-put feature map and its circular channel shift. Building upon these two new operators, we arrive at a family of networks, named MicroNet, that achieves significant per-formance gains over the state of the art in the low FLOP regime. For instance, under the constraint of 12M FLOPs,MicroNet achieves 59.4% top-1 accuracy on ImageNet clas-sification, outperforming MobileNetV3 by 9.6%. Source code is at https://github.com/liyunsheng13/micronet. 