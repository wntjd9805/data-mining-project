The existence of noisy data is prevalent in both the train-ing and testing phases of machine learning systems, which inevitably leads to the degradation of model performance.There have been plenty of works concentrated on learning with in-distribution (IND) noisy labels in the last decade, i.e., some training samples are assigned incorrect labels that do not correspond to their true classes. Nonetheless, in real application scenarios, it is necessary to consider the inﬂuence of out-of-distribution (OOD) samples, i.e., sam-ples that do not belong to any known classes, which has not been sufﬁciently explored yet. To remedy this, we study a new problem setup, namely Learning with Open-worldNoisy Data (LOND). The goal of LOND is to simultane-ously learn a classiﬁer and an OOD detector from datasets with mixed IND and OOD noise. In this paper, we propose a new graph-based framework, namely Noisy Graph Clean-ing (NGC), which collects clean samples by leveraging ge-ometric structure of data and model predictive conﬁdence.Without any additional training effort, NGC can detect and reject the OOD samples based on the learned class proto-types directly in testing phase. We conduct experiments on multiple benchmarks with different types of noise and the re-sults demonstrate the superior performance of our method against state of the arts. 