Recently, Cross-Modal Hamming space Retrieval (CMHR) regains ever-increasing attention, mainly beneﬁt-ing from the excellent representation capability of deep neu-ral networks. On the other hand, the vulnerability of deep networks exposes a deep cross-modal retrieval system to various safety risks (e.g., adversarial attack). However, at-tacking deep cross-modal Hamming retrieval remains un-derexplored.In this paper, we propose an effective Ad-versarial Attack on Deep Cross-Modal Hamming Retrieval, dubbed AACH, which fools a target deep CMHR model in a black-box setting. Speciﬁcally, given a target model, weﬁrst construct its substitute model to exploit cross-modal correlations within hamming space, with which we create adversarial examples by limitedly querying from a target model. Furthermore, to enhance the efﬁciency of adver-sarial attacks, we design a triplet construction module to exploit cross-modal positive and negative instances. In this way, perturbations can be learned to fool the target model through pulling perturbed examples far away from the pos-itive instances whereas pushing them close to the negative ones. Extensive experiments on three widely used cross-modal (image and text) retrieval benchmarks demonstrate the superiority of the proposed AACH. We ﬁnd that AACH can successfully attack a given target deep CMHR model with fewer interactions, and that its performance is on par with previous state-of-the-art attacks. 