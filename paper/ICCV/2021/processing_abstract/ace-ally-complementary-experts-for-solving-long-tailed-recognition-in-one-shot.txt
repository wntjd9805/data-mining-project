One-stage long-tailed recognition methods improve the overall performance in a “seesaw” manner, i.e., either sac-riﬁce the head’s accuracy for better tail classiﬁcation or elevate the head’s accuracy even higher but ignore the tail.Existing algorithms bypass such trade-off by a multi-stage training process: pre-training on imbalanced set and ﬁne-tuning on balanced set. Though achieving promising per-formance, not only are they sensitive to the generalizability of the pre-trained model, but also not easily integrated into other computer vision tasks like detection and segmenta-tion, where pre-training of classiﬁers solely is not applica-ble. In this paper, we propose a one-stage long-tailed recog-nition scheme, ally complementary experts (ACE), where the expert is the most knowledgeable specialist in a sub-set that dominates its training, and is complementary to other experts in the less-seen categories without being dis-turbed by what it has never seen. We design a distribution-adaptive optimizer to adjust the learning pace of each ex-pert to avoid over-ﬁtting. Without special bells and whis-tles, the vanilla ACE outperforms the current one-stageSOTA method by 3∼ 10% on CIFAR10-LT, CIFAR100-LT,ImageNet-LT and iNaturalist datasets. It is also shown to be the ﬁrst one to break the “seesaw” trade-off by improv-ing the accuracy of the majority and minority categories simultaneously in only one stage. Code and trained models are at https://github.com/jrcai/ACE. 