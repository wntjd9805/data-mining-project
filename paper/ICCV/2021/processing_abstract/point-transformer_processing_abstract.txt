Self-attention networks have revolutionized natural lan-guage processing and are making impressive strides in im-age analysis tasks such as image classification and object detection. Inspired by this success, we investigate the ap-plication of self-attention networks to 3D point cloud pro-cessing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design im-proves upon prior work across domains and tasks. For ex-ample, on the challenging S3DIS dataset for large-scale se-mantic scene segmentation, the Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70% mIoU threshold for the first time. 