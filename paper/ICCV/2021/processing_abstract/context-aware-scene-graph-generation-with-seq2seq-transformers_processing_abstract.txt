Scene graph generation is an important task in com-puter vision aimed at improving the semantic understand-ing of the visual world. In this task, the model needs to de-tect objects and predict visual relationships between them.Most of the existing models predict relationships in paral-lel assuming their independence. While there are differ-ent ways to capture these dependencies, we explore a con-ditional approach motivated by the sequence-to-sequence (Seq2Seq) formalism. Different from the previous research, our proposed model predicts visual relationships one at a time in an autoregressive manner by explicitly condi-tioning on the already predicted relationships. Drawing from translation models in NLP, we propose an encoder-decoder model built using Transformers where the en-coder captures global context and long range interactions.The decoder then makes sequential predictions by condi-tioning on the scene graph constructed so far.In addi-tion, we introduce a novel reinforcement learning-based training strategy tailored to Seq2Seq scene graph genera-tion. By using a self-critical policy gradient training ap-proach with Monte Carlo search we directly optimize for the (mean) recall metrics and bridge the gap between train-ing and evaluation. Experimental results on two public benchmark datasets demonstrate that our Seq2Seq learn-ing approach achieves strong empirical performance, out-performing previous state-of-the-art, while remaining efÔ¨Å-cient in terms of training and inference time. Full code for this work is available here: https://github.com/ layer6ai-labs/SGG-Seq2Seq. 