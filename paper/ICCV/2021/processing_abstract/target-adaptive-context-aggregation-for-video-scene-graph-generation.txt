This paper deals with a challenging task of video scene graph generation (VidSGG), which could serve as a struc-tured video representation for high-level understanding tasks. We present a new detect-to-track paradigm for this task by decoupling the context modeling for relation predic-tion from the complicated low-level entity tracking. Speciﬁ-cally, we design an efﬁcient method for frame-level VidSGG, termed as Target Adaptive Context Aggregation Network (TRACE), with a focus on capturing spatio-temporal con-text information for relation recognition. Our TRACE framework streamlines the VidSGG pipeline with a modu-lar design, and presents two unique blocks of HierarchicalRelation Tree (HRTree) construction and Target-adaptiveContext Aggregation. More speciﬁc, our HRTree ﬁrst pro-vides an adpative structure for organizing possible relation candidates efﬁciently, and guides context aggregation mod-ule to effectively capture spatio-temporal structure informa-tion. Then, we obtain a contextualized feature representa-tion for each relation candidate and build a classiﬁcation head to recognize its relation category. Finally, we pro-vide a simple temporal association strategy to track TRACE detected results to yield the video-level VidSGG. We per-form experiments on two VidSGG benchmarks: ImageNet-VidVRD and Action Genome, and the results demonstrate that our TRACE achieves the state-of-the-art performance.The code and models are made available at https:// github.com/MCG-NJU/TRACE. 