Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments us-ing natural language instructions. Given the scarcity of domain-specific training data and the high diversity of im-age and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent meth-ods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing small-scale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB1, a large-scale and diverse in-domain VLN dataset. We first col-lect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal or-der inside PI pairs. We use BnB to pretrain our Airbert2 model that can be adapted to discriminative and genera-tive settings and show that it outperforms state of the art forRoom-to-Room (R2R) navigation and Remote Referring Ex-pression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a chal-lenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses. 