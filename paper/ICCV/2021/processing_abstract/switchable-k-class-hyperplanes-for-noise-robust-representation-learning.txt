Optimizing the K-class hyperplanes in the latent space has become the standard paradigm for efﬁcient represen-tation learning. However, it’s almost impossible to ﬁnd an optimal K-class hyperplane to accurately describe the la-tent space of massive noisy data. For this potential problem, we constructively propose a new method, named Switch-able K-class Hyperplanes (SKH), to sufﬁciently describe the latent space by the mixture of K-class hyperplanes. It can directly replace the conventional single K-class hyper-plane optimization as the new paradigm for noise-robust representation learning. When collaborated with the pop-ular ArcFace on million-level data representation learn-ing, we found that the switchable manner in SKH can ef-fectively eliminate the gradient conﬂict generated by real-world label noise on a single K-class hyperplane. More-over, combined with the margin-based loss functions (e.g.ArcFace), we propose a simple Posterior Data Clean strat-egy to reduce the model optimization deviation on clean dataset caused by the reduction of valid categories in eachK-class hyperplane. Extensive experiments demonstrate that the proposed SKH easily achieves new state-of-the-art on IJB-B and IJB-C by encouraging noise-robust represen-tation learning. Our code will be available at https://github.com/liubx07/SKH.git. 