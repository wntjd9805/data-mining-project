iCV-MEFED (Emotion Target) Happy Sad Angry The successful deployment of artiﬁcial intelligence (AI) in many domains from healthcare to hiring requires their responsible use, particularly in model explanations and privacy. Explainable artiﬁcial intelligence (XAI) provides more information to help users to understand model de-cisions, yet this additional knowledge exposes additional risks for privacy attacks. Hence, providing explanation harms privacy. We study this risk for image-based model inversion attacks and identiﬁed several attack architectures with increasing performance to reconstruct private image data from model explanations. We have developed several multi-modal transposed CNN architectures that achieve sig-niﬁcantly higher inversion performance than using the tar-get model prediction only. These XAI-aware inversion mod-els were designed to exploit the spatial knowledge in im-age explanations. To understand which explanations have higher privacy risk, we analyzed how various explana-tion types and factors inﬂuence inversion performance. In spite of some models not providing explanations, we further demonstrate increased inversion performance even for non-explainable target models by exploiting explanations of sur-rogate models through attention transfer. This method ﬁrst inverts an explanation from the target prediction, then re-constructs the target image. These threats highlight the ur-gent and signiﬁcant privacy risks of explanations and calls attention for new privacy preservation techniques that bal-ance the dual-requirement for AI explainability and privacy. 