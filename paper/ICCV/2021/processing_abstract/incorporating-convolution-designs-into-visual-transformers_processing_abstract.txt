Motivated by the success of Transformers in natural lan-guage processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervi-sion to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrow-ing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies.Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw in-put images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations.Experimental results on ImageNet and seven down-stream tasks show the effectiveness and generalization abil-ity of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of train-ing data and extra CNN teachers. Besides, CeiT models demonstrate better convergence with 3Ã— fewer training it-erations, which can reduce the training cost significantly 1. 