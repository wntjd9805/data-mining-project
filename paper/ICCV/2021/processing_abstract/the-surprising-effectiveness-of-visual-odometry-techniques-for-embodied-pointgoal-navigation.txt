It is fundamental for personal robots to reliably navigate to a speciﬁed goal. To study this task, PointGoal navigation has been introduced in simulated Embodied AI environments.Recent advances solve this PointGoal navigation task with near-perfect accuracy (99.6% success) in photo-realistically simulated environments, assuming noiseless egocentric vi-sion, noiseless actuation and most importantly, perfect lo-calization. However, under realistic noise models for visual sensors and actuation, and without access to a “GPS andCompass sensor,” the 99.6%-success agents for PointGoal navigation only succeed with 0.3%.1 In this work, we demon-strate the surprising effectiveness of visual odometry for the task of PointGoal navigation in this realistic setting, i.e., with realistic noise models for perception and actuation and without access to GPS and Compass sensors. We show that integrating visual odometry techniques into navigation poli-cies improves the state-of-the-art on the popular HabitatPointNav benchmark by a large margin, improving success from 64.5% to 71.7% while executing 6.4 times faster. 