The data poisoning attack has raised serious security concerns on the safety of deep neural networks, since it can lead to neural backdoor that misclassiﬁes certain inputs crafted by an attacker. In particular, the sample-targetedIt targets at one or backdoor attack is a new challenge. a few speciﬁc samples, called target samples, to misclas-sify them to a target class. Without a trigger planted in the backdoor model, the existing backdoor detection schemes fail to detect the sample-targeted backdoor as they depend on reverse-engineering the trigger or strong features of the trigger.In this paper, we propose a novel scheme to de-tect and mitigate sample-targeted backdoor attacks. We discover and demonstrate a unique property of the sample-targeted backdoor, which forces a boundary change such that small “pockets” are formed around the target sam-ple. Based on this observation, we propose a novel defense mechanism to pinpoint a malicious pocket by “wrapping” them into a tight convex hull in the feature space. We de-sign an effective algorithm to search for such a convex hull and remove the backdoor by ﬁne-tuning the model using the identiﬁed malicious samples with the corrected label ac-cording to the convex hull. The experiments show that the proposed approach is highly efﬁcient for detecting and mit-igating a wide range of sample-targeted backdoor attacks. 