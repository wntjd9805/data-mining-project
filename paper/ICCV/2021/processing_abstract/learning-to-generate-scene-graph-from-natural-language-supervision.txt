Learning from image-text data has demonstrated recent success for many recognition tasks, yet is currently limited to visual features or individual visual concepts such as ob-jects.In this paper, we propose one of the ﬁrst methods that learn from image-sentence pairs to extract a graphi-cal representation of localized objects and their relation-ships within an image, known as scene graph. To bridge the gap between images and texts, we leverage an off-the-shelf object detector to identify and localize object in-stances, match labels of detected regions to concepts parsed from captions, and thus create “pseudo” labels for learn-ing scene graph. Further, we design a Transformer-based model to predict these “pseudo” labels via a masked token prediction task. Learning from only image-sentence pairs, our model achieves 30% relative gain over a latest method trained with human-annotated unlocalized scene graphs.Our model also shows strong results for weakly and fully supervised scene graph generation. In addition, we explore an open-vocabulary setting for detecting scene graphs, and present the ﬁrst result for open-set scene graph generation. 