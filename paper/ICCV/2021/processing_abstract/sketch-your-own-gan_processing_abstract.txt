Can a user create a deep generative model by sketch-ing a single example? Traditionally, creating a GAN model has required the collection of a large-scale dataset of ex-emplars and specialized knowledge in deep learning. In contrast, sketching is possibly the most universally accessi-ble way to convey a visual concept. In this work, we present a method, GAN Sketching, for rewriting GANs with one or more sketches, to make GANs training easier for novice users. In particular, we change the weights of an originalGAN model according to user sketches. We encourage the model’s output to match the user sketches through a cross-domain adversarial loss. Furthermore, we explore different regularization methods to preserve the original model’s di-versity and image quality. Experiments have shown that our method can mold GANs to match shapes and poses speciﬁed by sketches while maintaining realism and diversity. Finally, we demonstrate a few applications of the resulting GAN, including latent space interpolation and image editing. 