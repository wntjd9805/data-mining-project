Autonomous navigation requires structured representa-tion of the road network and instance-wise identification of the other traffic agents. Since the traffic scene is defined on the ground plane, this corresponds to scene understanding in the birdâ€™s-eye-view (BEV). However, the onboard cam-eras of autonomous cars are customarily mounted horizon-tally for a better view of the surrounding, making this task very challenging. In this work, we study the problem of ex-tracting a directed graph representing the local road net-work in BEV coordinates, from a single onboard camera image. Moreover, we show that the method can be ex-tended to detect dynamic objects on the BEV plane. The semantics, locations, and orientations of the detected ob-jects together with the road graph facilitates a comprehen-sive understanding of the scene. Such understanding be-comes fundamental for the downstream tasks, such as path planning and navigation. We validate our approach against powerful baselines and show that our network achieves su-perior performance. We also demonstrate the effects of various design choices through ablation studies. Code: https://github.com/ybarancan/STSU 