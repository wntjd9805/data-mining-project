Visible infrared person re-identiﬁcation (VI-REID) aims to match pedestrian images between the daytime visible and nighttime infrared camera views. The large cross-modality discrepancies have become the bottleneck which limits the performance of VI-REID. Existing methods mainly focus on capturing cross-modality sharable representations by learning an identity classiﬁer. However, the heterogeneous pedestrian images taken by different spectrum cameras dif-fer signiﬁcantly in image styles, resulting in inferior dis-criminability of feature representations. To alleviate the above problem, this paper explores the correlation between two modalities and proposes a novel syncretic modality collaborative learning (SMCL) model to bridge the cross-modality gap. A new modality that incorporates features of heterogeneous images is constructed automatically to steer the generation of modality-invariant representations. Chal-lenge enhanced homogeneity learning (CEHL) and aux-iliary distributional similarity learning (ADSL) are inte-grated to project heterogeneous features on a uniﬁed space and enlarge the inter-class disparity, thus strengthening the discriminative power. Extensive experiments on two cross-modality benchmarks demonstrate the effectiveness and su-periority of the proposed method. Especially, on SYSU-MM01 dataset, our SMCL model achieves 67.39% rank-1 accuracy and 61.78% mAP, surpassing the cutting-edge works by a large margin. 