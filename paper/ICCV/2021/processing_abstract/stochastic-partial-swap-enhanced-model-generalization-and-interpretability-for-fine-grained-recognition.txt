Learning mid-level representation for fine-grained recognition is easily dominated by a limited number of highly discriminative patterns, degrading its robustness and generalization capability. To this end, we propose a novelStochastic Partial Swap (SPS)1 scheme to address this issue.Our method performs element-wise swapping for partial features between samples to inject noise during training.It equips a regularization effect similar to Dropout, which promotes more neurons to represent the concepts. Fur-thermore, it also exhibits other advantages: 1) suppress-ing over-activation to some part patterns to improve feature representativeness, and 2) enriching pattern combination and simulating noisy cases to enhance classifier general-ization. We verify the effectiveness of our approach through comprehensive experiments across four network backbones and three fine-grained datasets. Moreover, we demonstrate its ability to complement high-level representations, allow-ing a simple model to achieve performance comparable to the top-performing technologies in fine-grained recog-nition, indoor scene recognition, and material recognition while improving model interpretability. 