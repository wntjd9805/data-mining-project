Efﬁcient action recognition has become crucial to extend the success of action recognition to many real-world appli-cations. Contrary to most existing methods, which mainly focus on selecting salient frames to reduce the computa-tion cost, we focus more on making the most of the se-lected frames. To this end, we employ two networks of different capabilities that operate in tandem to efﬁciently recognize actions. Given a video, the lighter network pro-cesses more frames while the heavier one only processes a few. In order to enable the effective interaction between the two, we propose dynamic knowledge propagation based on a cross-attention mechanism. This is the main compo-nent of our framework that is essentially a student-teacher architecture, but as the teacher model continues to inter-act with the student model during inference, we call it a dynamic student-teacher framework. Through extensive ex-periments, we demonstrate the effectiveness of each compo-nent of our framework. Our method outperforms competing state-of-the-art methods on two video datasets: ActivityNet-v1.3 and Mini-Kinetics.Figure 1: mAP vs. GFLOPs curves on ActivityNet-v1.3 [5]:The proposed dynamic student-teacher framework performs similar to or better than the recent state-of-the-art meth-ods [11, 22, 26, 44, 45] at a much lower computational cost.More experimental results are available in Section 4. 