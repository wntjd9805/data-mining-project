We address the problem of generalized zero-shot se-mantic segmentation (GZS3) predicting pixel-wise seman-tic labels for seen and unseen classes. Most GZS3 meth-ods adopt a generative approach that synthesizes visual features of unseen classes from corresponding semantic ones (e.g., word2vec) to train novel classiﬁers for both seen and unseen classes. Although generative methods show de-cent performance, they have two limitations: (1) the visual features are biased towards seen classes; (2) the classiﬁer should be retrained whenever novel unseen classes appear.We propose a discriminative approach to address these lim-itations in a uniﬁed framework. To this end, we leverage visual and semantic encoders to learn a joint embedding space, where the semantic encoder transforms semantic fea-tures to semantic prototypes that act as centers for visual features of corresponding classes. Speciﬁcally, we intro-duce boundary-aware regression (BAR) and semantic con-sistency (SC) losses to learn discriminative features. Our approach to exploiting the joint embedding space, together with BAR and SC terms, alleviates the seen bias problem.At test time, we avoid the retraining process by exploiting semantic prototypes as a nearest-neighbor (NN) classiﬁer.To further alleviate the bias problem, we also propose an in-ference technique, dubbed Apollonius calibration (AC), that modulates the decision boundary of the NN classiﬁer to theApollonius circle adaptively. Experimental results demon-strate the effectiveness of our framework, achieving a new state of the art on standard benchmarks. 