Visual text recognition is undoubtedly one of the most extensively researched topics in computer vision. Great progress have been made to date, with the latest models starting to focus on the more practical “in-the-wild” set-ting. However, a salient problem still hinders practical de-ployment – prior state-of-arts mostly struggle with recog-nising unseen (or rarely seen) character sequences. In this paper, we put forward a novel framework to speciﬁcally tackle this “unseen” problem. Our framework is iterative in nature, in that it utilises predicted knowledge of char-acter sequences from a previous iteration, to augment the main network in improving the next prediction. Key to our success is a unique cross-modal variational autoencoder to act as a feedback module, which is trained with the presence of textual error distribution data. This module importantly translates a discrete predicted character space, to a contin-uous afﬁne transformation parameter space used to condi-tion the visual feature map at next iteration. Experiments on common datasets have shown competitive performance over state-of-the-arts under the conventional setting. Most importantly, under the new disjoint setup where train-test labels are mutually exclusive, ours offers the best perfor-mance thus showcasing the capability of generalising onto unseen words (Figure 1 offers a summary). 