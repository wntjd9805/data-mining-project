Batch Whitening is a technique that accelerates and sta-bilizes training by transforming input features to have a zero mean (Centering) and a unit variance (Scaling), and by removing linear correlation between channels (Decor-relation). In commonly used structures, which are empir-ically optimized with Batch Normalization, the normaliza-tion layer appears between convolution and activation func-tion. Following Batch Whitening studies have employed the same structure without further analysis; even Batch Whiten-ing was analyzed on the premise that the input of a lin-ear layer is whitened. To bridge the gap, we propose a new Convolutional Unit that in line with the theory, and our method generally improves the performance of BatchWhitening. Moreover, we show the inefficacy of the originalConvolutional Unit by investigating rank and correlation of features. As our method is employable off-the-shelf whiten-ing modules, we use Iterative Normalization (IterNorm), the state-of-the-art whitening module, and obtain significantly improved performance on five image classification datasets:CIFAR-10, CIFAR-100, CUB-200-2011, Stanford Dogs, and ImageNet. Notably, we verify that our method improves stability and performance of whitening when using large learning rate, group size, and iteration number. Code is available at https://github.com/YooshinCho/ pytorch_ConvUnitOptimization. 