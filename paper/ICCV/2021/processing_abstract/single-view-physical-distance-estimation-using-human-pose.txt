We propose a fully automated system that si-multaneously estimates the camera intrinsics, the ground plane, and physical distances between peo-ple from a single RGB image or video captured by a camera viewing a 3-D scene from a ﬁxed van-tage point. To automate camera calibration and distance estimation, we leverage priors about hu-man pose and develop a novel direct formulation for pose-based auto-calibration and distance estimation, which shows state-of-the-art performance on publicly available datasets. The proposed approach enables ex-isting camera systems to measure physical distances without needing a dedicated calibration process or range sensors, and is applicable to a broad range of use cases such as social distancing and workplace safety. Furthermore, to enable evaluation and drive research in this area, we contribute to the publicly available MEVA dataset with additional distance an-notations, resulting in “MEVADA” – an evaluation benchmark for the pose-based auto-calibration and distance estimation problem. 