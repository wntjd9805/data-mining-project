Recent breakthroughs in self-supervised learning show that such algorithms learn visual representations that can be transferred better to unseen tasks than cross-entropy based methods which rely on task-specific supervision. In this pa-per, we found that the similar holds in the continual learning context: contrastively learned representations are more ro-bust against the catastrophic forgetting than ones trained with the cross-entropy objective. Based on this novel ob-servation, we propose a rehearsal-based continual learning algorithm that focuses on continually learning and main-taining transferable representations. More specifically, the proposed scheme (1) learns representations using the con-trastive learning objective, and (2) preserves learned rep-resentations using a self-supervised distillation step. We conduct extensive experimental validations under popular benchmark image classification datasets, where our method sets the new state-of-the-art performance. Source code is available at https://github.com/chaht01/Co2L. 