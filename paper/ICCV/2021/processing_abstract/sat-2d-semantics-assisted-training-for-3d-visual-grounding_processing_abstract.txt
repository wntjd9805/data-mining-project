3D visual grounding aims at grounding a natural lan-guage description about a 3D scene, usually represented in the form of 3D point clouds, to the targeted object re-gion. Point clouds are sparse, noisy, and contain limited semantic information compared with 2D images. These in-herent limitations make the 3D visual grounding problem more challenging. In this study, we propose 2D SemanticsAssisted Training (SAT) that utilizes 2D image semantics in the training stage to ease point-cloud-language joint rep-resentation learning and assist 3D visual grounding. The main idea is to learn auxiliary alignments between rich, clean 2D object representations and the corresponding ob-jects or mentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object label, image feature, and 2D geometric feature, as the extra input in training but does not require such inputs during inference. By effectively uti-lizing 2D semantics in training, our approach boosts the accuracy on the Nr3D dataset from 37.7% to 49.2%, which signiÔ¨Åcantly surpasses the non-SAT baseline with the identi-cal network architecture and inference input. Our approach outperforms the state of the art by large margins on multiple 3D visual grounding datasets, i.e., +10.4% absolute accu-racy on Nr3D, +9.9% on Sr3D, and +5.6% on ScanRef. 