The performance of a computer vision model depends on the size and quality of its training data. Recent stud-ies have unveiled previously-unknown composition biases in common image datasets which then lead to skewed model outputs, and have proposed methods to mitigate these bi-ases. However, most existing works assume that human-generated annotations can be considered gold-standard and unbiased. In this paper, we reveal that this assumption can be problematic, and that special care should be taken to pre-vent models from learning such annotation biases. We focus on facial expression recognition and compare the label bi-ases between lab-controlled and in-the-wild datasets. We demonstrate that many expression datasets contain signifi-cant annotation biases between genders, especially when it comes to the happy and angry expressions, and that tradi-tional methods cannot fully mitigate such biases in trained models. To remove expression annotation bias, we propose an AU-Calibrated Facial Expression Recognition (AUC-FER) framework that utilizes facial action units (AUs) and incorporates the triplet loss into the objective function. Ex-perimental results suggest that the proposed method is more effective in removing expression annotation bias than exist-ing techniques. 