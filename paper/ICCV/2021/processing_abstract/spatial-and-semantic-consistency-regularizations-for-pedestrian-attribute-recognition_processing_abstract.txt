While recent studies on pedestrian attribute recognition have shown remarkable progress in leveraging complicated networks and attention mechanisms, most of them neglect the inter-image relations and an important prior: spatial consistency and semantic consistency of attributes under surveillance scenarios. The spatial locations of the same attribute should be consistent between different pedestrian images, e.g., the “hat” attribute and the “boots” attribute are always located at the top and bottom of the picture re-spectively. In addition, the inherent semantic feature of the“hat” attribute should be consistent, whether it is a base-ball cap, beret, or helmet. To fully exploit inter-image re-lations and aggregate human prior in the model learning process, we construct a Spatial and Semantic Consistency (SSC) framework that consists of two complementary reg-ularizations to achieve spatial and semantic consistency for each attribute. Specifically, we first propose a spatial consistency regularization to focus on reliable and stable attribute-related regions. Based on the precise attribute lo-cations, we further propose a semantic consistency regular-ization to extract intrinsic and discriminative semantic fea-tures. We conduct extensive experiments on popular bench-marks including PA100K, RAP, and PETA. Results show that the proposed method performs favorably against state-of-the-art methods without increasing parameters. 