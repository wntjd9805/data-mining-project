We propose a novel framework for fine-grained object recognition that learns to recover object variation in 3D space from a single image, trained on an image collection without using any ground-truth 3D annotation. We accom-plish this by representing an object as a composition of 3D shape and its appearance, while eliminating the effect of camera viewpoint, in a canonical configuration. Unlike conventional methods modeling spatial variation in 2D im-ages only, our method is capable of reconfiguring the ap-pearance feature in a canonical 3D space, thus enabling the subsequent object classifier to be invariant under 3D geo-metric variation. Our representation also allows us to go beyond existing methods, by incorporating 3D shape vari-ation as an additional cue for object recognition. To learn the model without ground-truth 3D annotation, we deploy a differentiable renderer in an analysis-by-synthesis frame-work. By incorporating 3D shape and appearance jointly in a deep representation, our method learns the discrimi-native representation of the object and achieves competitive performance on fine-grained image recognition and vehi-cle re-identification. We also demonstrate that the perfor-mance of 3D shape reconstruction is improved by learning fine-grained shape deformation in a boosting manner. 