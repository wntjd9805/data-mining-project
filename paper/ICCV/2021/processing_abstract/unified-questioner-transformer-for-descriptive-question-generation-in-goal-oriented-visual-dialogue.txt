Building an interactive artificial intelligence that can ask questions about the real world is one of the biggestIn partic-challenges for vision and language problems. ular, goal-oriented visual dialogue, where the aim of the agent is to seek information by asking questions during a turn-taking dialogue, has been gaining scholarly atten-tion recently. While several existing models based on theGuessWhat?! dataset [10] have been proposed, the Ques-tioner typically asks simple category-based questions or ab-solute spatial questions. This might be problematic for com-plex scenes where the objects share attributes, or in cases where descriptive questions are required to distinguish ob-jects. In this paper, we propose a novel Questioner architec-ture, called Unified Questioner Transformer (UniQer), for descriptive question generation with referring expressions.In addition, we build a goal-oriented visual dialogue task called CLEVR Ask. It synthesizes complex scenes that re-quire the Questioner to generate descriptive questions. We train our model with two variants of CLEVR Ask datasets.The results of the quantitative and qualitative evaluations show that UniQer outperforms the baseline. 