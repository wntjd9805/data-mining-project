Automatic report generation on medical radiographs have recently gained interest. However, identifying diseases as well as correctly predicting their corresponding sizes, locations and other medical description patterns, which is essential for generating high-quality reports, is challeng-ing. Although previous methods focused on producing read-able reports, how to accurately detect and describe ﬁnd-ings that match with the query X-Ray has not been success-fully addressed. In this paper, we propose a multi-modality semantic attention model to integrate visual features, pre-dicted key ﬁnding embeddings, as well as clinical features, and progressively decode reports with visual-textual seman-tic consistency. First, multi-modality features are extracted and attended with the hidden states from the sentence de-coder, to encode enriched context vectors for better decod-ing a report. These modalities include regional visual fea-tures of scans, semantic word embeddings of the top-K ﬁnd-ings predicted with high probabilities, and clinical features of indications. Second, the progressive report decoder con-sists of a sentence decoder and a word decoder, where we propose image-sentence matching and description accuracy losses to constrain the visual-textual semantic consistency.Extensive experiments on the public MIMIC-CXR and IUX-Ray datasets show that our model achieves consistent im-provements over the state-of-the-art methods. 