We study how stochastic differential equation (SDE) based ideas can inspire new modiﬁcations to existing al-gorithms for a set of problems in computer vision. Loosely speaking, our formulation is related to both explicit and im-plicit strategies for data augmentation and group equivari-ance, but is derived from new results in the SDE literature on estimating inﬁnitesimal generators of a class of stochas-tic processes. If and when there is nominal agreement be-tween the needs of an application/task and the inherent properties and behavior of the types of processes that we can efﬁciently handle, we obtain a very simple and efﬁcient plug-in layer that can be incorporated within any existing network architecture, with minimal modiﬁcation and only a few additional parameters. We show promising experiments on a number of vision tasks including few shot learning, point cloud transformers and deep variational segmentation obtaining efﬁciency or performance improvements. 