Learning from non-stationary data streams and over-coming catastrophic forgetting still poses a serious chal-lenge for machine learning research. Rather than aiming to improve state-of-the-art, in this work we provide insight into the limits and merits of rehearsal, one of continual learn-ingâ€™s most established methods. We hypothesize that models trained sequentially with rehearsal tend to stay in the same low-loss region after a task has finished, but are at risk of overfitting on its sample memory, hence harming general-ization. We provide both conceptual and strong empirical evidence on three benchmarks for both behaviors, bringing novel insights into the dynamics of rehearsal and continual learning in general. Finally, we interpret important contin-ual learning works in the light of our findings, allowing for a deeper understanding of their successes.1 