The popularity of multimodal sensors and the accessi-bility of the Internet have brought us a massive amount of unlabeled multimodal data. Since existing datasets and well-trained models are primarily unimodal, the modal-ity gap between a unimodal network and unlabeled multi-modal data poses an interesting problem: how to transfer a pre-trained unimodal network to perform the same task with extra unlabeled multimodal data? In this work, we propose multimodal knowledge expansion (MKE), a knowl-edge distillation-based framework to effectively utilize mul-timodal data without requiring labels. Opposite to tradi-tional knowledge distillation, where the student is designed to be lightweight and inferior to the teacher, we observe that a multimodal student model consistently rectifies pseudo la-bels and generalizes better than its teacher. Extensive ex-periments on four tasks and different modalities verify this finding. Furthermore, we connect the mechanism of MKE to semi-supervised learning and offer both empirical and theoretical explanations to understand the expansion capa-bility of a multimodal student. 1 