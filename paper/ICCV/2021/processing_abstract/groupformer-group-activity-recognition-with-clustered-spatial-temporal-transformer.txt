Group activity recognition is a crucial yet challenging problem, whose core lies in fully exploring spatial-temporal interactions among individuals and generating reasonable group representations. However, previous methods either model spatial and temporal information separately, or di-rectly aggregate individual features to form group features.To address these issues, we propose a novel group activ-ity recognition network termed GroupFormer. It captures spatial-temporal contextual information jointly to augment the individual and group representations effectively with a clustered spatial-temporal transformer. Specifically, ourGroupFormer has three appealing advantages: (1) A tailor-modified Transformer, Clustered Spatial-Temporal Trans-former, is proposed to enhance the individual representa-tion and group representation. (2) It models the spatial and temporal dependencies integrally and utilizes decoders to build the bridge between the spatial and temporal informa-tion. (3) A clustered attention mechanism is utilized to dy-namically divide individuals into multiple clusters for bet-ter learning activity-aware semantic representations. More-over, experimental results show that the proposed frame-work outperforms state-of-the-art methods on the Volleyball dataset and Collective Activity dataset. Code is available at https://github.com/xueyee/GroupFormer 