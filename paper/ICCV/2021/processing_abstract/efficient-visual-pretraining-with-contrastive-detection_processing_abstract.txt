Self-supervised pretraining has been shown to yield pow-erful representations for transfer learning. These perfor-mance gains come at a large computational cost however, with state-of-the-art methods requiring an order of mag-nitude more computation than supervised pretraining. We tackle this computational bottleneck by introducing a new self-supervised objective, contrastive detection, which tasks representations with identifying object-level features across augmentations. This objective extracts a rich learning sig-nal per image, leading to state-of-the-art transfer accuracy on a variety of downstream tasks, while requiring up to less pretraining. In particular, our strongest ImageNet-10 pretrained model performs on par with SEER, one of the largest self-supervised systems to date, which uses 1000⇥ more pretraining data. Finally, our objective seamlessly handles pretraining on more complex images such as those in COCO, closing the gap with supervised transfer learning from COCO to PASCAL.⇥ 