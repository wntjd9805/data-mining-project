In this paper, given a small bag of images, each contain-ing a common but latent predicate, we are interested in lo-calizing visual subject-object pairs connected via the com-mon predicate in each of the images. We refer to this novel problem as visual relationship co-localization or VRC as an abbreviation. VRC is a challenging task, even more so than the well-studied object co-localization task. This becomes further challenging when using just a few images, the model has to learn to co-localize visual subject-object pairs con-nected via unseen predicates. To solve VRC, we propose an optimization framework to select a common visual relation-ship in each image of the bag. The goal of the optimization framework is to find the optimal solution by learning visual relationship similarity across images in a few-shot setting.To obtain robust visual relationship representation, we uti-lize a simple yet effective technique that learns relationship embedding as a translation vector from visual subject to vi-sual object in a shared space. Further, to learn visual rela-tionship similarity, we utilize a proven meta-learning tech-nique commonly used for few-shot classification tasks. Fi-nally, to tackle the combinatorial complexity challenge aris-ing from an exponential number of feasible solutions, we use a greedy approximation inference algorithm that selects approximately the best solution.We extensively evaluate our proposed framework on variations of bag sizes obtained from two challenging pub-lic datasets, namely VrR-VG and VG-150, and achieve im-pressive visual co-localization performance. 