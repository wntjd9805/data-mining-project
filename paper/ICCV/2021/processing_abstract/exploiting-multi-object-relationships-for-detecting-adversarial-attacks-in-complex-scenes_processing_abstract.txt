Vision systems that deploy Deep Neural Networks (DNNs) are known to be vulnerable to adversarial exam-ples. Recent research has shown that checking the intrin-sic consistencies in the input data is a promising way to detect adversarial attacks (e.g., by checking the object co-occurrence relationships in complex scenes). However, ex-isting approaches are tied to specific models and do not of-fer generalizability. Motivated by the observation that lan-guage descriptions of natural scene images have already captured the object co-occurrence relationships that can be learned by a language model, we develop a novel approach to perform context consistency checks using such language models. The distinguishing aspect of our approach is that it is independent of the deployed object detector and yet of-fers very high accuracy in terms of detecting adversarial examples in practical scenes with multiple objects. Exper-iments on the PASCAL VOC and MS COCO datasets show that our method can outperform state-of-the-art methods in detecting adversarial attacks. 