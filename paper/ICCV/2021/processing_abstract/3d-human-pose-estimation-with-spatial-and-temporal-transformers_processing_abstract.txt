Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classi-ﬁcation, object detection, and semantic segmentation. How-ever, in the ﬁeld of human pose estimation, convolutional ar-chitectures still remain dominant. In this work, we presentPoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional ar-chitectures involved.Inspired by recent developments in vision transformers, we design a spatial-temporal trans-former structure to comprehensively model the human joint relations within each frame as well as the temporal corre-lations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard bench-mark datasets: Human3.6M and MPI-INF-3DHP. Exten-sive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at https://github.com/zczcwh/PoseFormer 