Existing cross-domain semantic segmentation methods usually focus on the overall segmentation results of whole objects but neglect the importance of object boundaries.In this work, we find that the segmentation performance can be considerably boosted if we treat object boundaries properly. For that, we propose a novel method calledBAPA-Net, which is based on a convolutional neural net-work via Boundary Adaptation and Prototype Alignment, under the unsupervised domain adaptation setting. Specif-ically, we first construct additional images by pasting ob-jects from source images to target images, and we de-velop a so-called boundary adaptation module to weigh each pixel based on its distance to the nearest boundary pixel of those pasted source objects. Moreover, we pro-pose another prototype alignment module to reduce the domain mismatch by minimizing distances between the class prototypes of the source and target domains, where boundaries are removed to avoid domain confusion dur-ing prototype calculation. By integrating the boundary adaptation and prototype alignment, we are able to train a discriminative and domain-invariant model for cross-domain semantic segmentation. We conduct extensive ex-periments on the benchmark datasets of urban scenes (i.e.,GTA5→Cityscapes and SYNTHIA→Cityscapes). And the promising results clearly show the effectiveness of ourBAPA-Net method over existing state-of-the-art for cross-domain semantic segmentation. Our implementation is available at https://github.com/manmanjun/BAPA-Net. 