Deep metric learning has been effectively used to learn distance metrics for different visual tasks like image re-In order to aid the training pro-trieval, clustering, etc. cess, existing methods either use a hard mining strategy to extract the most informative samples or seek to gener-ate hard synthetics using an additional network. Such ap-proaches face different challenges and can lead to biased embeddings in the former case, and (i) harder optimization (ii) slower training speed (iii) higher model complexity in the latter case. In order to overcome these challenges, we propose a novel approach that looks for optimal hard nega-tives (LoOp) in the embedding space, taking full advantage of each tuple by calculating the minimum distance between a pair of positives and a pair of negatives. Unlike mining-based methods, our approach considers the entire space be-tween pairs of embeddings to calculate the optimal hard negatives. Extensive experiments combining our approach and representative metric learning losses reveal a signiÔ¨Å-cant boost in performance on three benchmark datasets1. 