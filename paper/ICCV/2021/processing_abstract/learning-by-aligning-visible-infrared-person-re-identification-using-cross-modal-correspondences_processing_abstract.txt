We address the problem of visible-infrared person re-identification (VI-reID), that is, retrieving a set of person images, captured by visible or infrared cameras, in a cross-modal setting. Two main challenges in VI-reID are intra-class variations across person images, and cross-modal dis-crepancies between visible and infrared images. Assuming that the person images are roughly aligned, previous ap-proaches attempt to learn coarse image- or rigid part-level person representations that are discriminative and general-izable across different modalities. However, the person im-ages, typically cropped by off-the-shelf object detectors, are not necessarily well-aligned, which distract discriminative person representation learning. In this paper, we introduce a novel feature learning framework that addresses these problems in a unified way. To this end, we propose to exploit dense correspondences between cross-modal person im-ages. This allows to address the cross-modal discrepancies in a pixel-level, suppressing modality-related features from person representations more effectively. This also encour-ages pixel-wise associations between cross-modal local fea-tures, further facilitating discriminative feature learning forVI-reID. Extensive experiments and analyses on standardVI-reID benchmarks demonstrate the effectiveness of our approach, which significantly outperforms the state of the art. 