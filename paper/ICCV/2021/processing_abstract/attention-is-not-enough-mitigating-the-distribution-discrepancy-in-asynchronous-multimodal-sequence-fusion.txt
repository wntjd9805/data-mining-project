Videos flow as the mixture of language, acoustic, and vision modalities. A thorough video understanding needs to fuse time-series data of different modalities for pre-diction. Due to the variable receiving frequency for se-quences from each modality, there usually exists inherent asynchrony across the collected multimodal streams. To-wards an efficient multimodal fusion from asynchronous multimodal streams, we need to model the correlations between elements from different modalities. The recentMultimodal Transformer (MulT) approach extends the self-attention mechanism of the original Transformer network to learn the crossmodal dependencies between elements.However, the direct replication of self-attention will suf-fer from the distribution mismatch across different modal-ity features. As a result, the learnt crossmodal dependen-cies can be unreliable. Motivated by this observation, this work proposes the Modality-Invariant Crossmodal Atten-tion (MICA) approach towards learning crossmodal inter-actions over modality-invariant space in which the distribu-tion mismatch between different modalities is well bridged.To this end, both the marginal distribution and the elements with high-confidence correlations are aligned over the com-mon space of the query and key vectors which are computed from different modalities. Experiments on three standard benchmarks of multimodal video understanding clearly val-idate the superiority of our approach. 