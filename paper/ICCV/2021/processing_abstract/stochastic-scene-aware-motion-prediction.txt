A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specif-ically, by learning from data, our goal is to enable vir-tual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as train-ing data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with vary-ing styles. We must model this diversity to synthesize vir-tual humans that realistically perform human-scene inter-actions. We present a novel data-driven, stochastic motion synthesis method that models different styles of perform-ing a given action with a target object. Our Scene-AwareMotion Prediction method (SAMP) generalizes to target ob-jects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collectedMoCap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex in-door scenes and achieve superior performance than exist-ing solutions. Code and data are available for research at https://samp.is.tue.mpg.de. 