View 2: !!"!#We focus on contrastive methods for self-supervised video representation learning. A common paradigm in con-trastive learning is to construct positive pairs by sampling different data views for the same instance, with different data instances as negatives. These methods implicitly as-sume a set of representational invariances to the view se-lection mechanism (e.g., sampling frames with temporal shifts), which may lead to poor performance on downstream tasks which violate these invariances (ﬁne-grained video action recognition that would beneﬁt from temporal infor-mation). To overcome this limitation, we propose an ‘aug-mentation aware’ contrastive learning framework, where we explicitly provide a sequence of augmentation param-eterisations (such as the values of the time shifts used to create data views) as composable augmentation encodings (CATE) to our model when projecting the video representa-tions for contrastive learning. We show that representations learned by our method encode valuable information about speciﬁed spatial or temporal augmentation, and in doing so also achieve state-of-the-art performance on a number of video benchmarks. 