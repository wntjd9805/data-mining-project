Compared with the visual grounding on 2D images, the natural-language-guided 3D object localization on point clouds is more challenging.In this paper, we propose a new model, named InstanceRefer1, to achieve a superior 3D visual grounding through the grounding-by-matching strategy.In practice, our model ﬁrst predicts the target category from the language descriptions using a simple language classiﬁcation model. Then, based on the cate-gory, our model sifts out a small number of instance candi-dates (usually less than 20) from the panoptic segmentation on point clouds. Thus, the non-trivial 3D visual ground-ing task has been effectively re-formulated as a simpli-ﬁed instance-matching problem, considering that instance-level candidates are more rational than the redundant 3D object proposals. Subsequently, for each candidate, we perform the multi-level contextual inference, i.e., referring from instance attribute perception, instance-to-instance re-lation perception, and instance-to-background global local-ization perception, respectively. Eventually, the most rele-vant candidate is selected and localized by ranking conﬁ-dence scores, which are obtained by the cooperative holis-tic visual-language feature matching. Experiments conﬁrm that our method outperforms previous state-of-the-arts onScanRefer online benchmark and Nr3D/Sr3D datasets. 