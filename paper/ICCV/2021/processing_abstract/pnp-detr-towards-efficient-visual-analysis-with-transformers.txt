Recently, DETR [3] pioneered the solution of vision tasks with transformers, it directly translates the image fea-ture map into the object detection result. Though effective, translating the full feature map can be costly due to redun-In dant computation on some area like the background. this work, we encapsulate the idea of reducing spatial re-dundancy into a novel poll and pool (PnP) sampling mod-ule, with which we build an end-to-end PnP-DETR archi-tecture that adaptively allocates its computation spatially to be more efﬁcient. Concretely, the PnP module abstracts the image feature map into ﬁne foreground object feature vectors and a small number of coarse background contex-tual feature vectors. The transformer models information interaction within the ﬁne-coarse feature space and trans-lates the features into the detection result. Moreover, thePnP-augmented model can instantly achieve various de-sired trade-offs between performance and computation with a single model by varying the sampled feature length, with-out requiring to train multiple models as existing methods.Thus it offers greater ﬂexibility for deployment in diverse scenarios with varying computation constraint. We further validate the generalizability of the PnP module on panop-tic segmentation and the recent transformer-based image recognition model ViT [7] and show consistent efﬁciency gain. We believe our method makes a step for efﬁcient visual analysis with transformers, wherein spatial redundancy is commonly observed. Code and models will be available. 