Recent Visual Question Answering (VQA) models have shown impressive performance on the VQA benchmark but remain sensitive to small linguistic variations in input ques-tions. Existing approaches address this by augmenting the dataset with question paraphrases from visual question gen-eration models or adversarial perturbations. These ap-proaches use the combined data to learn an answer classi-ﬁer by minimizing the standard cross-entropy loss. To more effectively leverage augmented data, we build on the recent success in contrastive learning. We propose a novel train-ing paradigm (ConClaT) that optimizes both cross-entropy and contrastive losses. The contrastive loss encourages rep-resentations to be robust to linguistic variations in ques-tions while the cross-entropy loss preserves the discrimina-tive power of representations for answer prediction.We ﬁnd that optimizing both losses – either alternately or jointly – is key to effective training. On the VQA-Rephrasings [44] benchmark, which measures the VQA model’s answer consistency across human paraphrases of a question, ConClaT improves Consensus Score by 1.63% over an improved baseline.In addition, on the standardVQA 2.0 benchmark, we improve the VQA accuracy by 0.78% overall. We also show that ConClaT is agnostic to the type of data-augmentation strategy used. 