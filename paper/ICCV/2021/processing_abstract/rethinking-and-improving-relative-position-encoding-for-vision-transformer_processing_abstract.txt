Relative position encoding (RPE) is important for trans-former to capture sequence ordering of input tokens. Gen-eral efﬁcacy has been proven in natural language process-ing. However, in computer vision, its efﬁcacy is not well studied and even remains controversial, e.g., whether rela-tive position encoding can work equally well as absolute position? In order to clarify this, we ﬁrst review exist-ing relative position encoding methods and analyze their pros and cons when applied in vision transformers. We then propose new relative position encoding methods dedi-cated to 2D images, called image RPE (iRPE). Our methods consider directional relative distance modeling as well as the interactions between queries and relative position em-beddings in self-attention mechanism. The proposed iRPE methods are simple and lightweight. They can be eas-ily plugged into transformer blocks. Experiments demon-strate that solely due to the proposed encoding methods,DeiT [21] and DETR [1] obtain up to 1.5% (top-1 Acc) and 1.3% (mAP) stable improvements over their original versions on ImageNet and COCO respectively, without tun-ing any extra hyperparameters such as learning rate and weight decay. Our ablation and analysis also yield in-teresting ﬁndings, some of which run counter to previ-ous understanding. Code and models are open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE. 