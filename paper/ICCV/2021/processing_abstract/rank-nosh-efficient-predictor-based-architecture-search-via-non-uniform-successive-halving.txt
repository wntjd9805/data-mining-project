Predictor-based algorithms have achieved remarkable performance in the Neural Architecture Search (NAS) tasks.However, these methods suffer from high computation costs, as training the performance predictor usually requires training and evaluating hundreds of architectures from scratch. Previous works along this line mainly focus on re-ducing the number of architectures required to ﬁt the pre-dictor. In this work, we tackle this challenge from a differ-ent perspective - improve search efﬁciency by cutting down the computation budget of architecture training. We pro-pose NOn-uniform Successive Halving (NOSH), a hierar-chical scheduling algorithm that terminates the training of underperforming architectures early to avoid wasting bud-get. To effectively leverage the non-uniform supervision sig-nals produced by NOSH, we formulate predictor-based ar-chitecture search as learning to rank with pairwise com-parisons. The resulting method - RANK-NOSH, reduces the search budget by while achieving competitive or even better performance than previous state-of-the-art predictor-based methods on various spaces and datasets.⇥⇠ 5 