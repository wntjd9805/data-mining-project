Co-occurrent visual pattern makes aggregating contex-tual information a common paradigm to enhance the pixel representation for semantic image segmentation. The ex-isting approaches focus on modeling the context from the perspective of the whole image, i.e., aggregating the image-level contextual information. Despite impressive, these methods weaken the significance of the pixel representa-tions of the same category, i.e., the semantic-level con-textual information. To address this, this paper proposes to augment the pixel representations by aggregating the image-level and semantic-level contextual information, re-spectively. First, an image-level context module is designed to capture the contextual information for each pixel in the whole image. Second, we aggregate the representations of the same category for each pixel where the category re-gions are learned under the supervision of the ground-truth segmentation. Third, we compute the similarities between each pixel representation and the image-level contextual information, the semantic-level contextual information, re-spectively. At last, a pixel representation is augmented by weighted aggregating both the image-level contextual infor-mation and the semantic-level contextual information with the similarities as the weights. Integrating the image-level and semantic-level context allows this paper to report state-of-the-art accuracy on four benchmarks, i.e., ADE20K, LIP,COCOStuff and Cityscapes 1. 