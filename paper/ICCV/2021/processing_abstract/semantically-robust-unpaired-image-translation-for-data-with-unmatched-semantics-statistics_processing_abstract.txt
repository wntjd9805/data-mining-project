Many applications of unpaired image-to-image transla-tion require the input contents to be preserved semantically during translations. Unaware of the inherently unmatched semantics distributions between source and target domains, existing distribution matching methods (i.e., GAN-based) can give undesired solutions. In particular, although pro-ducing visually reasonable outputs, the learned models usu-ally ﬂip the semantics of the inputs. To tackle this without using extra supervisions, we propose to enforce the trans-lated outputs to be semantically invariant w.r.t. small per-ceptual variations of the inputs, a property we call “seman-tic robustness”. By optimizing a robustness loss w.r.t. multi-scale feature space perturbations of the inputs, our method effectively reduces semantics ﬂipping and produces transla-tions that outperform existing methods both quantitatively and qualitatively. 