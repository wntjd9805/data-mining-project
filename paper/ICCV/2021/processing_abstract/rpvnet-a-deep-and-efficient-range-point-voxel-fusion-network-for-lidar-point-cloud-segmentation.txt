Point clouds can be represented in many forms (views), typically, point-based sets, voxel-based cells or range-based images(i.e., panoramic view). The point-based view is geometrically accurate, but it is disordered, which makes it difficult to find local neighbors efficiently. The voxel-based view is regular, but sparse, and computation grows cubicly when voxel resolution increases. The range-based view is regular and generally dense, however spherical pro-jection makes physical dimensions distorted. Both voxel-and range-based views suffer from quantization loss, espe-cially for voxels when facing large-scale scenes. In order to utilize different viewâ€™s advantages and alleviate their own shortcomings in fine-grained segmentation task, we propose a novel range-point-voxel fusion network, namely RPVNet.In this network, we devise a deep fusion framework with multiple and mutual information interactions among these three views, and propose a gated fusion module (termed asGFM), which can adaptively merge the three features based on concurrent inputs. Moreover, the proposed RPV interac-tion mechanism is highly efficient, and we summarize it to a more general formulation. By leveraging this efficient in-teraction and relatively lower voxel resolution, our method is also proved to be more efficient. Finally, we evaluated the proposed model on two large-scale datasets, i.e., Se-manticKITTI and nuScenes, and it shows state-of-the-art performance on both of them. Note that, our method cur-rently ranks 1st on SemanticKITTI leaderboard without any extra tricks. 