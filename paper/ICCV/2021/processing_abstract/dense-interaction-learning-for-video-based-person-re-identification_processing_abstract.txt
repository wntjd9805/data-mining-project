Video-based person re-identification (re-ID) aims at matching the same person across video clips. Efficiently exploiting multi-scale fine-grained features while building the structural interaction among them is pivotal for its suc-cess. In this paper, we propose a hybrid framework, DenseInteraction Learning (DenseIL), that takes the principal ad-vantages of both CNN-based and Attention-based architec-tures to tackle video-based person re-ID difficulties. Den-seIL contains a CNN encoder and a Dense Interaction (DI) decoder. The CNN encoder is responsible for efficiently ex-tracting discriminative spatial features while the DI decoder is designed to densely model spatial-temporal inherent in-teraction across frames. Different from previous works, we additionally let the DI decoder densely attends to interme-diate fine-grained CNN features and that naturally yields multi-grained spatial-temporal representation for each video clip. Moreover, we introduce Spatio-TEmporal PositionalEmbedding (STEP-Emb) into the DI decoder to investigate the positional relation among the spatial-temporal inputs.Our experiments consistently and significantly outperform all the state-of-the-art methods on multiple standard video-based person re-ID datasets. 