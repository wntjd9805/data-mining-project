People navigating in unfamiliar buildings take advan-tage of myriad visual, spatial and semantic cues to efﬁ-ciently achieve their navigation goals. Towards equipping computational agents with similar capabilities, we intro-duce Pathdreamer, a visual world model for agents navigat-ing in novel indoor environments. Given one or more previ-ous visual observations, Pathdreamer generates plausible high-resolution 360◦ visual observations (RGB, semantic segmentation and depth) for viewpoints that have not been visited, in buildings not seen during training. In regions of high uncertainty (e.g. predicting around corners, imagining the contents of an unseen room), Pathdreamer can predict diverse scenes, allowing an agent to sample multiple real-istic outcomes for a given trajectory. We demonstrate thatPathdreamer encodes useful and accessible visual, spatial and semantic knowledge about human environments by us-ing it in the downstream task of Vision-and-Language Nav-igation (VLN). Speciﬁcally, we show that planning ahead with Pathdreamer brings about half the beneﬁt of looking ahead at actual observations from unobserved parts of the environment. We hope that Pathdreamer will help unlock model-based approaches to challenging embodied naviga-tion tasks such as navigating to speciﬁed objects and VLN. 