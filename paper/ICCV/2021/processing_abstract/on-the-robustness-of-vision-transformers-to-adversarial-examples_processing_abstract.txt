Recent advances in attention-based networks have shown that Vision Transformers can achieve state-of-the-art or near state-of-the-art results on many image classiﬁcation tasks. This puts transformers in the unique position of being a promising alternative to traditional convolutional neural networks (CNNs). While CNNs have been carefully stud-ied with respect to adversarial attacks, the same cannot beIn this paper, we study the said of Vision Transformers. robustness of Vision Transformers to adversarial examples.Our analyses of transformer security is divided into three parts. First, we test the transformer under standard white-box and black-box attacks. Second, we study the transfer-ability of adversarial examples between CNNs and trans-formers. We show that adversarial examples do not readily transfer between CNNs and transformers. Based on thisﬁnding, we analyze the security of a simple ensemble de-fense of CNNs and transformers. By creating a new attack, the self-attention blended gradient attack, we show that such an ensemble is not secure under a white-box adver-sary. However, under a black-box adversary, we show that an ensemble can achieve unprecedented robustness without sacriﬁcing clean accuracy. Our analysis for this work is done using six types of white-box attacks and two types of black-box attacks. Our study encompasses multiple VisionTransformers, Big Transfer Models and CNN architectures trained on CIFAR-10, CIFAR-100 and ImageNet. 