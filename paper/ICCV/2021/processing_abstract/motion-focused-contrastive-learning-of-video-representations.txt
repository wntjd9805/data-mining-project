Motion, as the most distinct phenomenon in a video to involve the changes over time, has been unique and crit-ical to the development of video representation learning.In this paper, we ask the question: how important is the motion particularly for self-supervised video representation learning. To this end, we compose a duet of exploiting the motion for data augmentation and feature learning in the regime of contrastive learning. Speciﬁcally, we present aMotion-focused Contrastive Learning (MCL) method that regards such duet as the foundation. On one hand, MCL capitalizes on optical ﬂow of each frame in a video to tem-porally and spatially sample the tubelets (i.e., sequences of associated frame patches across time) as data augmenta-tions. On the other hand, MCL further aligns gradient maps of the convolutional layers to optical ﬂow maps from spa-tial, temporal and spatio-temporal perspectives, in order to ground motion information in feature learning. Exten-sive experiments conducted on R(2+1)D backbone demon-strate the effectiveness of our MCL. On UCF101, the lin-ear classiﬁer trained on the representations learnt by MCL achieves 81.91% top-1 accuracy, outperforming ImageNet supervised pre-training by 6.78%. On Kinetics-400, MCL achieves 66.62% top-1 accuracy under the linear protocol. 