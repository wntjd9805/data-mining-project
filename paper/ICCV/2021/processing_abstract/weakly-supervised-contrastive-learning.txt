Unsupervised visual representation learning has gained much attention from the computer vision community be-cause of the recent achievement of contrastive learning.Most of the existing contrastive learning frameworks adopt the instance discrimination as the pretext task, which treat-ing every single instance as a different class. However, such method will inevitably cause class collision problems, which hurts the quality of the learned representation. Mo-tivated by this observation, we introduced a weakly super-vised contrastive learning framework (WCL) to tackle this issue. Specifically, our proposed framework is based on two projection heads, one of which will perform the regular in-stance discrimination task. The other head will use a graph-based method to explore similar samples and generate a weak label, then perform a supervised contrastive learn-ing task based on the weak label to pull the similar images closer. We further introduced a K-Nearest Neighbor based multi-crop strategy to expand the number of positive sam-ples. Extensive experimental results demonstrate WCL im-proves the quality of self-supervised representations across different datasets. Notably, we get a new state-of-the-art re-sult for semi-supervised learning. With only 1% and 10% labeled examples, WCL achieves 65% and 72% ImageNetTop-1 Accuracy using ResNet50, which is even higher thanSimCLRv2 with ResNet101. 