Event cameras are novel sensors that perceive the per-pixel intensity changes and output asynchronous event streams with high dynamic range and less motion blur. It has been shown that events alone can be used for end-task learning, e.g., semantic segmentation, based on encoder-decoder-like networks. However, as events are sparse and mostly reﬂect edge information, it is difﬁcult to recover orig-inal details merely relying on the decoder. Moreover, most methods resort to the pixel-wise loss alone for supervision, which might be insufﬁcient to fully exploit the visual de-tails from sparse events, thus leading to less optimal perfor-mance. In this paper, we propose a simple yet ﬂexible two-stream framework named Dual Transfer Learning (DTL) to effectively enhance the performance on the end-tasks with-out adding extra inference cost. The proposed approach consists of three parts: event to end-task learning (EEL) branch, event to image translation (EIT) branch, and trans-fer learning (TL) module that simultaneously explores the feature-level afﬁnity information and pixel-level knowledge from the EIT branch to improve the EEL branch. This sim-ple yet novel method leads to strong representation learning from events and is evidenced by the signiﬁcant performance boost on the end-tasks such as semantic segmentation and depth estimation. 