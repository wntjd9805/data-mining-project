Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classiﬁcation. TheViT model splits each image into a sequence of tokens withﬁxed length and then applies multiple Transformer layers to model their global relation for classiﬁcation. However,ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We ﬁnd it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low train-ing sample efﬁciency; 2) the redundant attention backbone design of ViT leads to limited feature richness for ﬁxed com-putation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vi-sion Transformer (T2T-ViT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an ef-ﬁcient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch onImageNet. It also outperforms ResNets and achieves com-parable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384×384 on ImageNet. 1 