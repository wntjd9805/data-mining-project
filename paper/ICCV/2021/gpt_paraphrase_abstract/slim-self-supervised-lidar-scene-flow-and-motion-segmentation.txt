Recently, there have been several frameworks developed for self-supervised learning of 3D scene flow on point clouds. However, these methods do not take advantage of the fact that scene flow separates scenes into moving agents and points following sensor motion. This paper proposes a self-supervised motion segmentation signal by comparing a robust rigid ego-motion estimate with a raw flow prediction. The predicted motion segmentation is then used to focus on stationary points and aggregate motion information in static parts of the scene. The model is trained end-to-end using backpropagation through Kabsch's algorithm, resulting in accurate ego-motion and improved scene flow estimation. The proposed method achieves state-of-the-art results on multiple scene flow metrics across different real-world datasets, demonstrating its robustness and generalizability. Additionally, the paper explores the performance gain of joint motion segmentation and scene flow and introduces a novel network architecture capable of handling a larger number of points during training compared to previous methods.