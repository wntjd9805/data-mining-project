This study focuses on the challenge of 3D visual grounding, which involves linking a natural language description of a 3D scene to the corresponding object region in the scene's point cloud representation. Point clouds are difficult to work with due to their sparsity, noise, and limited semantic information compared to 2D images. To address this challenge, the authors propose a method called 2D Semantics-Assisted Training (SAT). SAT utilizes the semantic information from 2D images during the training stage to facilitate joint representation learning between point clouds and language. The idea is to learn alignments between clean and rich 2D object representations and the corresponding objects or entities mentioned in the 3D scenes. SAT incorporates object labels, image features, and 2D geometric features as additional inputs during training, but these inputs are not required during inference. By effectively leveraging 2D semantics in training, the proposed approach achieves a significant improvement in accuracy compared to a baseline method without SAT. Specifically, on the Nr3D dataset, the accuracy increases from 37.7% to 49.2% using the identical network architecture and inference input. Furthermore, the proposed approach outperforms the state of the art on multiple 3D visual grounding datasets, with absolute accuracy improvements of 10.4% on Nr3D, 9.9% on Sr3D, and 5.6% on ScanRef.