This study focuses on the susceptibility of deep learning-based action recognition models to adversarial attacks. Specifically, we investigate the vulnerability of these models against the one frame attack, which involves adding a subtle perturbation to a single frame of a video clip. Our analysis reveals that these models exhibit significant vulnerability to the one frame attack due to their structural properties. Through experiments, we demonstrate the high success rates and inconspicuous nature of this attack. Additionally, we discover that strong universal one frame perturbations can be generated across different scenarios. This research highlights the critical issue of the state-of-the-art action recognition models' susceptibility to adversarial attacks from various perspectives.