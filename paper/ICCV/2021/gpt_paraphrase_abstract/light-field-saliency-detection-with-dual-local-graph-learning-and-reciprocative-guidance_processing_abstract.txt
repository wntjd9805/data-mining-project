Recently, the use of light field data in detecting salient objects has gained popularity. However, effectively combining the features of the focal stack and the all-focus image has been a challenge. Previous methods have used convolution or ConvLSTM to fuse focal stack features, but these approaches have proven to be less effective and ill-posed. In this study, we propose a graph network approach to address this issue. By using graph networks, we can incorporate powerful context propagation from neighboring nodes and avoid ill-posed implementations. We construct local graph connections to reduce the computational costs typically associated with traditional graph networks. Additionally, instead of processing the two types of data separately, we develop a novel dual graph model that guides the fusion process of the focal stack using all-focus patterns. Previous methods have used one-shot fusion, which limits the exploration of their complementary nature. To overcome this limitation, we introduce a reciprocative guidance scheme that enables mutual guidance between the focal stack and all-focus features at multiple steps. This iterative enhancement of both feature types ultimately improves the saliency prediction. Extensive experimental results demonstrate the effectiveness of our proposed models, with significantly better performance than state-of-the-art methods.