To reduce overall annotation costs for machine learning models trained with limited labeled training data, validation becomes a bottleneck. To address this issue, we propose a statistical validation algorithm that accurately estimates the F-score of binary classifiers for rare categories, where finding relevant examples for evaluation is particularly challenging. Our approach combines simultaneous calibration and importance sampling, which allows for accurate estimates even when the sample size is small (< 300 samples). We also derive a precise single-trial estimator for the variance of our method, which proves to be empirically accurate at low sample counts. This enables practitioners to determine the reliability of a given low-sample estimate. When validating state-of-the-art semi-supervised models on ImageNet and iNaturalist2017, our method achieves comparable estimates of model performance with up to 10 fewer labels compared to other approaches. Specifically, we can accurately estimate model F1 scores with a variance of 0.005 using as few as 100 labels.