Few-shot image generation aims to generate more data for novel unseen categories using only a few available images. Previous approaches have attempted to fuse these images globally using adjustable weighted coefficients. However, these methods suffer from poor generation quality and lack diversity due to semantic misalignment between different images. To address this issue, we propose a novel approach called Local-Fusion Generative Adversarial Network (LoFGAN). Instead of treating the available images as a whole, we randomly divide them into a base image and several reference images. LoFGAN then matches local representations between the base and reference images based on semantic similarities and replaces the local features with the most closely related ones. This approach allows LoFGAN to generate more realistic and diverse images at a fine-grained level while maintaining semantic alignment. Additionally, we introduce a local reconstruction loss to enhance training stability and generation quality. We evaluate our method on three datasets and demonstrate its effectiveness for few-shot image generation and downstream visual applications with limited data. The code for our approach is available at https://github.com/edward3862/LoFGAN-pytorch.