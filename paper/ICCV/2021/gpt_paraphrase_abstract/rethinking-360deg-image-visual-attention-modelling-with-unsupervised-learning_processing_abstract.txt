Despite the success of self-supervised representation learning on two-dimensional data, the application of this approach to 360° images has not been explored. This study aims to bridge this gap by adapting contrastive learning techniques to train latent representations that are invariant enough to be highly effective for predicting saliency in spherical images. The unique geometry of omni-directional images makes them well-suited for this approach. To test our hypothesis, we propose an unsupervised framework that maximizes the mutual information between different views captured from both the equator and the poles. Our experiments demonstrate that the decoder, trained using the encoder embeddings, can successfully learn saliency distributions. Remarkably, our model outperforms fully-supervised learning methods on multiple datasets, including Salient360!, VR-EyeTracking, and Sitzman. Notably, our approach achieves this performance using an unsupervised encoder and a relatively lightweight supervised decoder (with 3.8 × fewer parameters than the ResNet50 encoder). We believe that this combination of supervised and unsupervised learning is a significant step towards more flexible models of human visual attention. The code to reproduce our results is available on GitHub.