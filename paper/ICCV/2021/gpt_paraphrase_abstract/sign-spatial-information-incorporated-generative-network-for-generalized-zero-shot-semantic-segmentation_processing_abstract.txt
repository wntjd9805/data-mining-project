Zero-shot semantic segmentation differs from conventional zero-shot classification as it predicts class labels at the pixel level rather than the image level. To address the need for pixel-level prediction and incorporate spatial information, we introduce Relative Positional Encoding. This improved positional encoding method integrates spatial information at the feature level and is adaptable to arbitrary image sizes. In addition, we propose a novel self-training strategy called Annealed Self-Training, inspired by knowledge distillation. This strategy automatically assigns varying importance to pseudo-labels, leading to enhanced performance. Through a comprehensive experimental evaluation, we systematically examine the effectiveness of Relative Positional Encoding and Annealed Self-Training on three benchmark datasets. The results confirm the efficacy of our approach.