This paper introduces a transformer model called Segmenter for semantic segmentation. Unlike convolution-based methods, Segmenter allows for modeling global context from the first layer throughout the network. The model is built upon the VisionTransformer (ViT) and extends it to semantic segmentation by using output embeddings of image patches to obtain class labels through a linear decoder or a mask transformer decoder. The model leverages pre-trained models for image classification and can be fine-tuned on moderate-sized datasets. The linear decoder achieves excellent results, but further improvements can be made using a mask transformer for generating class masks. The paper includes an extensive study to analyze the impact of different parameters, revealing that larger models and smaller patch sizes lead to better performance. Segmenter outperforms state-of-the-art methods on ADE20K and Pascal Context datasets and shows competitiveness on Cityscapes.