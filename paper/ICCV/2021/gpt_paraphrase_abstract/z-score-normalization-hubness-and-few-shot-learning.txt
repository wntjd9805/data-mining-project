Few-shot learning (FSL) aims to identify new classes using only a few labeled samples, leveraging a large collection of labeled samples from existing classes. Current FSL methods employ a meta-learning approach, where a deep feature embedding network is meta-learned, and during inference, novel class samples are classified based on nearest neighbor in the learned embedding space. However, these methods are susceptible to the hubness problem, where a specific class prototype becomes the closest neighbor for many test instances, irrespective of their actual classes. Despite this issue being largely overlooked in previous FSL research, we demonstrate, for the first time, that many FSL methods indeed suffer from the hubness problem. To address this, we propose the use of z-score feature normalization during meta-training, which proves to be a simple yet effective solution. We provide a theoretical analysis explaining its effectiveness and conduct extensive experiments that show how z-score normalization enhances the performance of recent FSL methods, resulting in new state-of-the-art results on three benchmark datasets.