The use of spatio-temporal convolution in video analysis often fails to capture motion dynamics effectively. Therefore, we propose a strong and reliable motion representation called spatio-temporal self-similarity (STSS) in this paper. STSS represents local regions in a video sequence by comparing them to their neighboring regions in both space and time. By converting appearance features into relational values, our approach allows the learner to better identify structural patterns in space and time. We utilize the entire volume of STSS to train our model in extracting an effective motion representation. Our neural block, named SELFY, can be easily integrated into neural architectures and trained end-to-end without extra supervision. With a sufficient volume of the neighborhood in space and time, SELFY successfully captures long-term interactions and fast motion in videos, resulting in robust action recognition. Our experimental analysis demonstrates that SELFY outperforms previous methods for motion modeling and complements spatio-temporal features obtained from direct convolution. On popular action recognition benchmarks such as Something-Something-V1 & V2, Diving-48, and FineGym, our proposed method achieves state-of-the-art results.