Recent research has discovered transformations in the hidden spaces of generative adversarial networks (GANs) that consistently and understandably alter generated images. However, existing methods for identifying these transformations either rely on a fixed set of predetermined visual concepts or unsupervised disentanglement techniques that may not align with human perception. This study presents a novel approach to create flexible vocabularies of basic visual concepts represented in a GAN's hidden space. The approach consists of three components: (1) automatically identifying visually significant directions based on their layer selectivity, (2) human annotation of these directions using descriptive natural language, and (3) breaking down these annotations into a visual concept vocabulary, comprising concise directions labeled with single words. Experimental results demonstrate that concepts learned using this approach are reliable and combinable, as they generalize across categories, contexts, and observers, while also enabling detailed manipulation of image style and content.