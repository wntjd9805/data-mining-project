Self-supervised learning has made significant progress in utilizing unlabeled data, but its effectiveness has been primarily observed with carefully selected pre-training datasets like ImageNet. In this study, we investigate the impact of using less-curated image datasets, such as YFCC, for contrastive learning and discover a noticeable difference in the quality of resulting representations. We propose that this discrepancy is due to a shift in the distribution of image classes, which becomes more diverse and heavy-tailed, leading to fewer relevant negative samples for learning. To validate our hypothesis, we introduce a novel approach called Divide and Contrast (DnC), which combines contrastive learning with clustering-based hard negative mining. When pre-trained on less curated datasets, DnC significantly enhances the performance of self-supervised learning on downstream tasks, while remaining competitive with state-of-the-art methods on curated datasets.