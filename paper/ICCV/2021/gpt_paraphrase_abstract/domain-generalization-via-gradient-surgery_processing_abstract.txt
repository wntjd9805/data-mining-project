In practical applications, machine learning models often encounter situations where the data distribution changes between the training and test domains. This presents a challenge known as domain generalization, which refers to the task of making predictions on distributions that differ from the training data. Existing methods address this issue by training models using data from multiple source domains and then applying them to unseen target domains. Our hypothesis is that training with multiple domains leads to conflicting gradients within each mini-batch, which contain domain-specific information irrelevant to others, including the test domain. If not addressed, such disagreements can negatively impact the model's generalization performance. In this study, we investigate the conflicting gradients that arise in scenarios involving domain shift and propose novel strategies based on gradient surgery to mitigate their impact. We validate our approach using image classification tasks on three multi-domain datasets, demonstrating the effectiveness of our proposed agreement strategy in improving the generalization capability of deep learning models in domain shift scenarios.