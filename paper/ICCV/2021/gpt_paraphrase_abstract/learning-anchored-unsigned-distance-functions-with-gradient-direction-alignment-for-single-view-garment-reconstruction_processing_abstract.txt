This paper addresses the challenge of reconstructing 3D garments from a single image. While significant progress has been made in 3D reconstruction using deep shape representations, garment reconstruction remains difficult due to open surfaces, diverse topologies, and complex geometric details. To overcome these challenges, the authors propose a novel approach called AnchorUDF, which represents 3D shapes using learnable Anchored Unsigned Distance Functions (UDFs). This representation enables modeling of open garment surfaces at any resolution by predicting unsigned distance fields. 

To capture the diverse topologies of garments, AnchorUDF computes pixel-aligned local image features and leverages a set of anchor points located around the surface to enrich the 3D position features. This provides stronger 3D space context for the distance function. Additionally, during training, the authors align the spatial gradient direction of AnchorUDF with the ground-truth direction to the surface to improve the accuracy of point projection direction during inference.

The proposed AnchorUDF method is evaluated on two public 3D garment datasets, MGN and Deep Fashion3D, and achieves state-of-the-art performance in single-view garment reconstruction. The code for implementing AnchorUDF is available at https://github.com/zhaofang0627/AnchorUDF.