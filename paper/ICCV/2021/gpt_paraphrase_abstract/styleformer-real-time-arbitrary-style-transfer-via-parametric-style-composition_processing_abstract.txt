This study introduces a novel feed-forward method called StyleFormer for arbitrary style transfer. StyleFormer aims to achieve both diverse styles and coherent semantic content in the stylized images. The method consists of three modules: (a) style bank generation for extracting sparse yet compact parametric style patterns, (b) transformer-driven style composition for global style composition guided by content, and (c) parametric content modulation for flexible and faithful stylization. The resulting stylized images maintain a high level of coherency with the content structure, exhibit sensitivity to detailed style variations, and adhere to the style distributions of the input style images. Through qualitative and quantitative comparisons, as well as user studies, it is demonstrated that StyleFormer outperforms state-of-the-art methods by generating visually plausible stylization results in real-time.