In previous studies, deep features have been proven to be effective in creating accurate semantic correspondences. However, the use of multi-scale and pyramidal hierarchy in convolutional neural networks to learn discriminative pixel-level features for semantic correspondence has not been thoroughly explored. This paper presents a multi-scale matching network that is capable of detecting subtle semantic differences between adjacent pixels. The proposed network follows a coarse-to-fine matching strategy and incorporates a top-down feature and matching enhancement scheme that aligns with the multi-scale hierarchy of deep convolutional neural networks. During feature enhancement, intra-scale enhancement combines feature maps of the same resolution from multiple layers using local self-attention, while cross-scale enhancement generates higher-resolution feature maps along the top-down pathway. Additionally, the network learns complementary matching details at different scales, gradually refining the overall matching score based on features of varying semantic levels. The multi-scale matching network can be easily trained end-to-end with minimal additional learnable parameters. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on three widely used benchmarks, while maintaining high computational efficiency. The code for the method has been made publicly available at https://github.com/wintersun661/MMNet.