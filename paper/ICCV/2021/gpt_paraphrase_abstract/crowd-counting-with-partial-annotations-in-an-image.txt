This paper explores a new approach to crowd counting that leverages data from different scenes with varying view angles while minimizing the need for extensive annotation. The study introduces a crowd counting setting that uses only partial annotations in each image for training. The researchers propose a network with three components to address the unannotated regions:   1) The Unannotated Regions Characterization (URC) module utilizes a memory bank to store annotated features, allowing visual features from these annotated regions to propagate to the unannotated regions. 2) The Feature Distribution Consistency (FDC) module ensures that the feature distributions of annotated head and unannotated head regions in each image are consistent. 3) The Cross-regressor Consistency Regularization (CCR) module is designed to learn the visual features of unannotated regions in a self-supervised manner.  Experimental results demonstrate the effectiveness of the proposed model on various datasets, including ShanghaiTech, UCF-CC-50, UCF-QNRF, NWPU-Crowd, and JHU-CROWD++. Even with only 10% annotated regions in each image, the proposed model outperforms recent methods and baselines in semi-supervised or active learning settings across all datasets. The code for the model is available at https://github.com/svip-lab/CrwodCountingPAL.