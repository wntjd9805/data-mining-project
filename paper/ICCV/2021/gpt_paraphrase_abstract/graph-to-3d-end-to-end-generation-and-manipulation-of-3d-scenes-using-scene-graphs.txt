Controllable scene synthesis involves creating 3D information that meets specific requirements. These requirements should be abstract, making them easy for users to interact with, while still providing enough control over the details. Scene graphs, which are representations of a scene made up of objects and their relationships, are well-suited for this task because they allow for semantic control over the generated content. However, previous approaches to scene synthesis have relied on synthetic data and object meshes, limiting their generation capabilities. To overcome this limitation, we propose a novel approach that directly generates shapes from a scene graph in an end-to-end manner. We also demonstrate that our model can be used for scene modification by using the scene graph as an interface. By leveraging Graph Convolutional Networks (GCN), we train a variational Auto-Encoder that incorporates object and edge categories, 3D shapes, and scene layouts. This allows us to sample new scenes and shapes.