Unsupervised co-localization aims to locate objects in a scene without the use of human-annotated labels, assuming that the dataset contains only one superclass. Recent methods have achieved impressive co-localization results through self-supervised representation learning, such as predicting rotation. In this paper, we propose a new contrastive objective that directly enhances co-localization performance by focusing on attention maps. Our contrastive loss function leverages location information to effectively activate the object's extent. Additionally, we introduce pixel-wise attention pooling, which selectively aggregates feature maps based on their magnitudes across channels. Through extensive qualitative and quantitative evaluation, our simple methods demonstrate their effectiveness, achieving state-of-the-art co-localization performance on four datasets: CUB-200-2011, Stanford Cars, FGVC-Aircraft, and Stanford Dogs. We will make our code publicly available online for the research community.