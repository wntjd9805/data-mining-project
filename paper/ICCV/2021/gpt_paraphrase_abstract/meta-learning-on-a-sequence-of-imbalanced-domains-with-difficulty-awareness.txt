To achieve strong generalization in real-world machine learning systems, it is crucial to be able to recognize new objects with limited labeled examples in a changing environment. Most current meta learning algorithms assume a consistent task distribution during training, but this paper explores a more practical and challenging scenario where the task distribution changes over time with domain shifts. Additionally, the task distribution is highly imbalanced and lacks natural domain labels. To address these challenges, the authors propose a kernel-based method for detecting domain changes and a memory management mechanism that considers the imbalanced domain sizes and importance for continuous learning across domains. They also introduce an adaptive task sampling method during training to reduce task gradient variance, backed by theoretical guarantees. Finally, the authors present a benchmark with imbalanced domain sequences and varying domain difficulty to evaluate their proposed method, showcasing its effectiveness through extensive evaluations.