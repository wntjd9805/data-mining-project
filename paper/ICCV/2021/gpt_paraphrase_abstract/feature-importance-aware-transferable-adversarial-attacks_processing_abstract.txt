The transferability of adversarial examples is crucial for attacking unknown models and conducting practical scenarios such as black-box attacks. However, existing transferable attacks often distort features without considering the intrinsic characteristics of objects in the images, resulting in limited transferability. To address this issue, we propose the Feature Importance-aware Attack (FIA), which disrupts important object-aware features that consistently influence model decisions. We calculate feature importance by using the aggregate gradient, which averages the gradients of feature maps from a source model computed on a batch of random transforms of the original clean image. These gradients are highly correlated with objects of interest and exhibit invariance across different models. Additionally, the random transforms preserve intrinsic object features while suppressing model-specific information. By utilizing feature importance, FIA is able to search for adversarial examples that disrupt critical features, leading to stronger transferability. Extensive experimental evaluation demonstrates the effectiveness of FIA, achieving a 9.5% improvement in success rate against normally trained models and a 12.8% improvement against defense models compared to state-of-the-art transferable attacks. The code for FIA is available at: https://github.com/hcguoO0/FIA.