Person Re-Identification (ReID) has made significant progress in the deep learning era. However, most approaches focus on holistic pedestrian regions and fail to consider occluded pedestrians, which are common in real-world scenarios and negatively impact ReID accuracy. Existing strategies attempt to locate visible body parts using auxiliary models, but these models suffer from domain gaps and data bias issues. To address these limitations, we propose the Occlusion-Aware Mask Network (OAMN). Our approach incorporates an attention-guided mask module that relies on labeled occlusion data for guidance. To generate diverse and accurately labeled occlusion data for any dataset, we introduce a novel occlusion augmentation scheme. This scheme is more suitable for real-world scenarios compared to existing schemes that only consider limited types of occlusions. Additionally, we present a new occlusion unification scheme to handle ambiguous information during the test phase. These three components allow attention mechanisms to accurately capture body parts irrespective of occlusion. Through comprehensive experiments on various person ReID benchmarks, we demonstrate the superiority of OAMN over state-of-the-art methods.