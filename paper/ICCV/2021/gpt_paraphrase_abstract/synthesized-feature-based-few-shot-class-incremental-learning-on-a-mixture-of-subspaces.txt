In the field of few-shot class incremental learning (FSCIL), the objective is to gradually add new classes to a pre-trained model while only having access to a few instances of each new class. However, existing approaches often lead to the forgetting of previously learned classes and overfitting to the limited number of new class samples. These issues have been addressed by using class prototypes derived from visual or semantic word vector domains. In this study, we propose a different approach to tackle this problem by utilizing a mixture of subspaces. These subspaces define the cluster structure of the visual domain and help describe both the visual and semantic domains by considering the overall distribution of the data. Additionally, we suggest the use of a variational autoencoder (VAE) to generate synthesized visual samples that can be used to augment pseudo-features during the incremental learning of novel classes. By combining the mixture of subspaces with synthesized features, our proposed method effectively mitigates the problems of forgetting and overfitting in FSCIL. We conducted extensive experiments on three image classification datasets and found that our approach achieves competitive results when compared to state-of-the-art methods.