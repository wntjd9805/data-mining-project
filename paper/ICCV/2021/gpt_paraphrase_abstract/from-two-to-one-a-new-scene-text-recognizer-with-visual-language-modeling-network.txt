This paper presents a new approach to scene text recognition by introducing a Visual Language Modeling Network (VisionLAN). Unlike previous methods that treat visual and linguistic information separately, VisionLAN combines these two types of information by equipping the vision model with language capabilities. The authors propose a training stage where character-wise occluded feature maps are used for text recognition. This training operation encourages the vision model to utilize both visual texture and linguistic information in the visual context for accurate recognition, particularly when visual cues are ambiguous due to occlusion or noise. Since linguistic information is acquired in conjunction with visual features without the need for an additional language model, VisionLAN improves speed by 39% and effectively incorporates linguistic information to enhance visual features for better recognition. The authors also introduce an Occlusion Scene Text (OST) dataset to evaluate performance in the absence of character-wise visual cues. The achieved results on various benchmarks demonstrate the effectiveness of VisionLAN. The code and dataset are publicly available at https://github.com/wangyuxin87/VisionLAN.