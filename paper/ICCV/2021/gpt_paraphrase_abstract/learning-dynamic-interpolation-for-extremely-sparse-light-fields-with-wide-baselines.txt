This paper addresses the issue of reconstructing dense light fields (LF) from sparsely-sampled ones with wide baselines. The proposed approach, called dynamic interpolation, replaces the commonly-used geometry warping operation with a learnable model. The model constructs a lightweight neural network that learns weights for interpolating neighboring pixels from input views to synthesize each pixel of novel views independently. Unlike the fixed and content-independent weights used in geometry warping, the learned interpolation weights incorporate correspondences between the source and novel views and adapt to different image content. Additionally, a spatial refinement module recovers the spatial correlation between the independently synthesized pixels of each novel view by referring to that of input views based on geometry. The angular correlation between the novel views is also constrained through a disparity-oriented LF structure loss. Experimental results on LF datasets with wide baselines demonstrate that the reconstructed LFs achieve higher PSNR/SSIM and better preserve the LF parallax structure compared to state-of-the-art methods. The source code for the proposed method is publicly available at https://github.com/MantangGuo/DI4SLF.