While multi-step adversarial training is widely used to defend against strong adversarial attacks, it is computationally expensive compared to standard training. Single-step adversarial training methods have been proposed to reduce this computational cost, but their performance is not always reliable depending on the optimization setting. To address these limitations, we propose a new method called single-step latent adversarial training (SLAT). SLAT deviates from the existing input-space-based adversarial training approach by using the gradients of latent representations as the adversarial perturbation. This approach implicitly regularizes the â„“1 norm of feature gradients, promoting local linearity and ensuring more reliable performance compared to other single-step methods. Additionally, because the proposed method leverages the gradients of latent representations, which are obtained during the computation of input gradients, it has a similar computational cost to the fast gradient sign method. Experimental results demonstrate that SLAT outperforms state-of-the-art accelerated adversarial training methods, despite its simplicity.