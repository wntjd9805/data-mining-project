The focus of this study is on learning from noisy data, specifically addressing three types of noise commonly found in real-world data: label noise, out-of-distribution input, and input corruption. Unlike existing methods that primarily tackle noise through label correction, this work proposes a new learning framework that emphasizes the development of robust representations. The approach involves embedding images into a lower-dimensional subspace and applying robust contrastive learning to regularize the subspace's geometric structure. This is achieved through an unsupervised consistency loss and a supervised mixup prototypical loss. Additionally, a novel noise cleaning method is introduced, which utilizes the learned representation to enforce smoothness constraints on neighboring samples. Extensive experiments on various benchmarks demonstrate the outstanding performance of the proposed method, as well as the robustness of the learned representation. The code for the method is publicly available.