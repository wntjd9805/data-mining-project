Current differentiable neural architecture search methods assume that the distribution of architectures on each edge is independent, which contradicts the inherent properties of architecture. This paper proposes a new approach called Variational Information Maximization Neural Architecture Search (VIM-NAS) that considers the architectural distribution as the latent representation of specific data points. VIM-NAS utilizes a simple convolutional neural network to model the latent representation and optimizes for a tractable variational lower bound to the mutual information between the data points and the latent representations. By doing so, VIM-NAS is able to learn a nearly one-hot distribution from a continuous distribution with fast convergence speed. Experimental results demonstrate that VIM-NAS achieves state-of-the-art performance on various search spaces, including DARTS search space, NAS-Bench-1shot1, NAS-Bench-201, and simplified search spaces S1-S4. Notably, VIM-NAS achieves a top-1 error rate of 2.45% and 15.80% within 10 minutes on CIFAR-10 and CIFAR-100 respectively, and a top-1 error rate of 24.0% when transferred to ImageNet.