This paper introduces a new domain adaptation paradigm called Generalized Source-free Domain Adaptation (G-SFDA). Unlike existing methods that focus only on adapting a pre-trained model to a target domain, G-SFDA aims to maintain the performance of the model on both the target and source domains. The proposed approach includes two key components: local structure clustering (LSC) and sparse domain attention (SDA). LSC clusters the target features with similar neighbors to adapt the model to the target domain without the use of source data. SDA produces a binary domain-specific attention to activate different feature channels for different domains, while also regularizing the gradient during adaptation to preserve source information. Experimental results show that G-SFDA achieves comparable or superior performance to existing domain adaptation methods on the target domain, with state-of-the-art performance (85.4%) on VisDA dataset. The proposed method also demonstrates effectiveness across various domains after adaptation to single or multiple target domains. The code for G-SFDA is available at https://github.com/Albert0147/G-SFDA.