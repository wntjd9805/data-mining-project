Attentive Independent Mechanisms (AIM) is a new approach to address the challenges of fast adaptation to new tasks and catastrophic forgetting in deep neural networks (DNNs) when fed with sequentially unseen data. AIM incorporates the concept of learning using fast and slow weights, while decoupling the feature extraction and higher-order conceptual learning processes in DNNs. AIM is designed for higher-order conceptual learning, utilizing a mixture of experts that compete to learn independent concepts for solving new tasks. It can be easily integrated into existing deep learning frameworks as a modular component. Experimental results on MiniImageNet and CIFAR-FS datasets demonstrate AIM's effectiveness in few-shot learning, exhibiting significant improvements when added to the SIB approach. AIM is also applied to ANML and OML trained on Omniglot, CIFAR-100, and MiniImageNet datasets to showcase its capability in continual learning. The code for AIM is publicly available at https://github.com/huang50213/AIM-Fewshot-Continual.