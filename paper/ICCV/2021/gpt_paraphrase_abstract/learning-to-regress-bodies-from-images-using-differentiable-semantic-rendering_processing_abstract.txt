In the field of learning 3D human body shape and pose from monocular images, the lack of 3D training data necessitates the use of losses on 2D keypoints, silhouettes, and part-segmentation. However, these losses have limitations as 2D keypoints cannot supervise body shape accurately and segmentations of clothed people do not match projected minimally-clothed SMPL shapes. To address this, we propose incorporating higher-level semantic information about clothing to differentiate between clothed and non-clothed regions of the human body. We achieve this by training a body regressor using a novel "Differentiable Semantic Rendering (DSR)" loss. The DSR-MC loss is defined to ensure a tight match between a rendered SMPL body and the minimally-clothed regions of the image, while the DSR-C loss is defined to encourage the rendered SMPL body to be inside the clothing mask in clothed regions. To enable differentiable training, we learn a semantic clothing prior for SMPL vertices from a large dataset of clothed human scans. We conduct extensive qualitative and quantitative experiments to evaluate the impact of clothing semantics on the accuracy of 3D human pose and shape estimation. Our approach outperforms previous state-of-the-art methods on 3DPW and Human3.6M datasets, and achieves comparable results on MPI-INF-3DHP dataset. The code and trained models are publicly available for research purposes at https://dsr.is.tue.mpg.de/.