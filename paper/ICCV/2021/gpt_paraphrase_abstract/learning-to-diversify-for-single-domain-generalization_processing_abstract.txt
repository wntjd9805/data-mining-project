This paper focuses on the problem of domain generalization, which involves training a model on multiple source domains and applying it to a target domain that has a different distribution. However, the paper introduces a more realistic and challenging scenario called Single Domain Generalization (Single-DG), where only one source domain is available for training. This limited diversity can hinder the model's ability to generalize to unseen target domains. To address this issue, the paper proposes a style-complement module that enhances the model's generalization power by synthesizing images from diverse distributions that complement the source domain. The proposed approach involves minimizing an upper bound approximation of the mutual information (MI) between the generated and source samples, forcing the generated images to be differentiated from the source samples. Additionally, the approach maximizes the MI between samples from the same semantic category to help the network learn discriminative features from diverse-styled images. The paper conducts extensive experiments on three benchmark datasets, demonstrating that the proposed approach outperforms state-of-the-art single-DG methods by up to 25.14%. The code for the approach will be made publicly available at the specified GitHub repository.