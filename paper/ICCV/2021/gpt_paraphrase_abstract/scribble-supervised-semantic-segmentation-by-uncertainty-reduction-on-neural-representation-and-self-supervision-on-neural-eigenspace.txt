Scribble-supervised semantic segmentation has become popular as it can achieve good results without high-quality annotations. However, obtaining confident and consistent predictions is challenging due to the lack of supervision. Existing approaches address this issue by either using auxiliary tasks or incorporating graphical models with additional requirements on scribble annotations. In contrast, this study aims to perform semantic segmentation directly using only scribble annotations and without any extra information or limitations. The proposed approach involves holistic operations such as minimizing entropy and implementing a network embedded random walk on the neural representation to reduce uncertainty. The network is trained with self-supervision on its neural eigenspace, using the probabilistic transition matrix of a random walk, to ensure consistency in predictions across related images. Through comprehensive experiments and ablation studies, the proposed approach is shown to outperform existing methods, comparable to some fully labeled supervised approaches, and robust to random shrinking or dropping of scribbles.