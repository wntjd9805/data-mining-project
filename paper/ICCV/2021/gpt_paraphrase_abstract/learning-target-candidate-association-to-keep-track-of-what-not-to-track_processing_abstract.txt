The problem of appearance-based visual tracking is challenging due to the presence of objects that closely resemble the target being tracked. These distractor objects can be mistakenly identified as the target, resulting in tracking failure. While most methods attempt to address this issue by improving appearance models to suppress distractors, we propose a different approach. Our solution involves tracking the distractor objects themselves in order to effectively track the target. We achieve this by introducing a learned association network that allows us to propagate the identities of all potential targets from one frame to the next. Since there are no ground-truth correspondences between distractor objects in visual tracking, we propose a training strategy that combines partial annotations with self-supervision. We conduct extensive experiments and analysis on challenging datasets to validate our approach. Our tracker outperforms existing methods and achieves a new state-of-the-art performance on six benchmarks, including an AUC score of 67.1% on the LaSOT dataset and a +5.8% absolute gain on the OxUvA long-term dataset. The code and trained models for our approach are available at https://github.com/visionml/pytracking.