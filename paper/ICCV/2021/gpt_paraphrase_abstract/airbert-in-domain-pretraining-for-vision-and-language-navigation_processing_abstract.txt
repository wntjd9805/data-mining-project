Vision-and-language navigation (VLN) is a field that focuses on enabling robots or virtual agents to navigate real-world environments using natural language instructions. However, due to limited training data and the diverse nature of image and language inputs, it is difficult for VLN agents to generalize to unseen environments. Previous methods have attempted to improve generalization through pretraining, but using generic image-caption datasets or small-scale VLN environments has yielded limited results. To address these limitations, this study introduces BnB1, a large-scale and diverse VLN dataset specifically designed for this domain. The dataset is created by collecting image-caption pairs from online rental marketplaces and using them to generate millions of VLN path-instruction pairs. The study also proposes a shuffling loss that enhances the learning of temporal order within these pairs. The authors then use the BnB dataset to pretrain a model called Airbert2, which can be adapted for both discriminative and generative tasks. The performance of this model is compared to state-of-the-art methods on the Room-to-Room (R2R) navigation and Remote Referring Expression (REVERIE) benchmarks, and it is found to outperform them. Additionally, the in-domain pretraining on BnB significantly improves performance on a challenging few-shot VLN evaluation, where the model is trained on instructions from only a few houses.