Few-shot learning is a method that aims to apply previously learned knowledge to new tasks with limited labeled data. However, the research in this field is diverse, with different algorithms excelling in different scenarios. This makes it challenging to determine which learning strategies to use in different task conditions. To address this limitation, we propose Meta Navigator, a framework inspired by Automated Machine Learning (AutoML). Meta Navigator automates the selection of various few-shot learning designs by seeking a higher-level strategy. Our goal is to find parameter adaptation policies that can be applied to different stages in the network for few-shot classification. We introduce a search space that encompasses popular few-shot learning algorithms and develop a differentiable searching and decoding algorithm based on meta-learning, which supports gradient-based optimization. Through our experiments on benchmark datasets, we demonstrate the effectiveness of our searching-based method. Our approach outperforms baselines and shows performance advantages over many state-of-the-art methods.