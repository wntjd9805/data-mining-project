This study focuses on estimating 3D shape and instances from a single view in order to quickly capture information about an environment without extensive scanning or multi-view fusion. However, accurately estimating these factors for composite scenes, such as object stacks, is challenging due to ambiguity in shape and instance segmentation of occluded areas. The researchers propose a solution called SIMstack, which uses a depth-conditioned Variational Auto-Encoder (VAE) trained on a dataset of objects stacked under physics simulation. By leveraging physics as a constraint, they hypothesize that a latent space learned from scenes built under physics simulation can serve as a prior to improve shape and instance prediction in occluded regions. Instance segmentation is formulated as a center voting task, enabling class-agnostic detection without setting a maximum number of objects in the scene. During testing, the model can generate 3D shape and instance segmentation probabilistically by sampling proposals for the occluded region from the learned latent space. The practical application of this method is to provide robots with the ability to make rapid intuitive inferences of partially observed scenes, similar to humans. One specific application demonstrated is precise and non-disruptive object grasping of unknown objects from a single depth view.