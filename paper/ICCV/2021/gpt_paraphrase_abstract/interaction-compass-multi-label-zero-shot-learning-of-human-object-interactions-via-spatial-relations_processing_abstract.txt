We investigate the issue of multi-label zero-shot recognition, where labels consist of human-object interactions. In this scenario, images can contain multiple interactions, and some interactions lack training images. To address this problem, we propose a novel compositional learning framework that separates interaction labels into action and object scores, taking into account the spatial compatibility between the two components. By combining these scores, we can efficiently recognize both seen and unseen interactions. However, learning the spatial relations between actions and objects requires bounding-box annotations, which are expensive to collect. Additionally, it is unclear how to extend these spatial relations to unseen interactions. To overcome these challenges, we develop a cross-attention mechanism that localizes objects based on action locations and vice versa, by predicting displacements between them, known as relational directions. During training, we estimate the relational directions by maximizing the scores of ground-truth interactions, which guide predictions towards compatible action-object regions. Through extensive experiments, we demonstrate the effectiveness of our framework, achieving a 2.6% improvement in mAP score and a 5.8% improvement in recall score on the HICO and Visual Genome datasets, respectively.