We introduce a new method for accurately detecting and perceiving vehicles in different camera views within a cooperative vehicle-infrastructure system (CVIS). Our approach does not rely on any assumptions about the camera's intrinsic or extrinsic parameters, making it suitable for arbitrary camera views. To address the scarcity of multi-view data, we propose a part-assisted novel view synthesis algorithm that utilizes a self-supervised part-based texture inpainting network. This algorithm enhances the training data by rendering a textured model into the background image with the desired 6-DoF pose. Moreover, to handle variations in camera parameters, we present a novel method that establishes dense mappings between image pixels and 3D points, enabling robust 2D/3D vehicle parsing. Additionally, we have created the first CVIS dataset for benchmarking purposes, which includes annotations for over 1540 images (14017 instances) from real-world traffic scenarios. By combining these innovative algorithms and datasets, we have developed a robust approach for 2D/3D vehicle parsing in CVIS. In practical experiments, our approach surpasses state-of-the-art methods, achieving improvements of 3.8% in 2D detection, 4.3% in instance segmentation, and 2.9% in 6-DoF pose estimation.