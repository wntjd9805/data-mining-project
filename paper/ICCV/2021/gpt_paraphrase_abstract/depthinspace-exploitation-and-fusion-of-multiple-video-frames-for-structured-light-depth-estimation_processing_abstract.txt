We introduce DepthInSpace, a deep-learning method for estimating depth using a structured-light camera. The approach is inspired by the use of depth sensors in smartphones. Our method incorporates estimated optical flow from multiple video frames to guide the training of a single-frame depth estimation network. This helps preserve edges and minimize over-smoothing. We also propose fusing data from multiple video frames to create more accurate depth maps, which are more robust in occluded areas and have fewer artifacts. Additionally, we demonstrate that these fused depth maps can be used to fine-tune the single-frame depth estimation network, further improving its performance. We evaluate our models against state-of-the-art approaches on both synthetic and real datasets. The implementation code, training procedure, and datasets can be found at https://www.idiap.ch/paper/depthinspace.