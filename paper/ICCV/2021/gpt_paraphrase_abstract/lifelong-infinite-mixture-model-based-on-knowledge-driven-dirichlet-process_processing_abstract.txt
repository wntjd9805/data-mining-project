Recent studies in lifelong learning aim to develop various models that can adapt to a growing number of tasks. These models have shown promising results in addressing the issue of catastrophic forgetting. However, the underlying theory behind these successful models is still not well understood. This paper focuses on the theoretical analysis of lifelong learning models, specifically by establishing risk bounds based on the difference between the probabilistic representation of data generated by the model and the target dataset. Building upon this theoretical analysis, a new approach called the Lifelong Infinite Mixture (LIMix) model is introduced. This model has the ability to automatically expand its network architectures or select an appropriate component to adjust its parameters when learning a new task, while retaining previously acquired knowledge. To incorporate this knowledge, Dirichlet processes are utilized through a gating mechanism that assesses the dependence between previously learned knowledge stored in each component and new data. Additionally, a compact Student model is trained to accumulate representations from different domains and facilitate quick inferences. The code for implementing the proposed model is available at https://github.com/dtuzi123/Lifelong-infinite-mixture-model.