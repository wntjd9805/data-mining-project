To address the challenges faced by computer vision algorithms in dark environments, we propose a unique model called Multitask Auto Encoding Transformation (MAET). This model aims to enhance object detection by uncovering the underlying patterns in illumination translation. Through self-supervision, the MAET learns the intrinsic visual structure by encoding and decoding realistic illumination-degrading transformations, taking into account the physical noise model and image signal processing. By utilizing this representation, we are able to decode the coordinates and classes of bounding boxes for object detection. To prevent the entanglement of the two tasks, our MAET disentangles the object and degrading features by enforcing orthogonal tangent regularity. This creates a parametric manifold that allows for geometric formulation of multitask predictions by maximizing orthogonality between tangents along the outputs of each task. Our framework can be easily implemented using popular object detection architectures and can be trained end-to-end using standard target detection datasets like VOC and COCO. We have achieved state-of-the-art performance using both synthetic and real-world datasets. The codes for our model will be made available at https://github.com/cuiziteng/MAET.