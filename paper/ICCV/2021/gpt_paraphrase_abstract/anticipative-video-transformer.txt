We present the Anticipative Video Transformer (AVT), a novel video modeling architecture that utilizes attention mechanisms to anticipate future actions by attending to previously observed video frames. The model is trained to predict the next action in a video sequence and also learns frame feature encoders that can predict future frames' features. Unlike existing methods, AVT maintains the sequential progression of observed actions while capturing long-range dependencies, which are crucial for effective anticipation. Extensive experiments demonstrate that AVT achieves the highest reported performance on popular action anticipation benchmarks, including EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads. Moreover, AVT secures the first place in the EpicKitchens-100CVPR'21 challenge.