We present ResRep, a new approach for lossless channel pruning in convolutional neural networks (CNNs). Our method reduces the width of CNNs by decreasing the number of output channels in convolutional layers. Drawing inspiration from neurobiology research on memory and forgetting, we propose re-parameterizing the CNN into "remembering" and "forgetting" parts. The former maintains performance, while the latter focuses on pruning. We train the remembering parts using regular stochastic gradient descent (SGD) and update the forgetting parts using a novel rule with penalty gradients to achieve structured sparsity. The remembering and forgetting parts are then merged back into the original architecture with narrower layers, resulting in ResRep as a successful application of Structural Re-parameterization. ResRep stands out from traditional learning-based pruning methods that penalize parameters for sparsity, as it avoids suppressing parameters essential for remembering. Our experiments demonstrate that ResRep can slim down a standard ResNet-50 with 76.15% accuracy on ImageNet to a narrower version with only 45% FLOPs (floating point operations) and no loss in accuracy. This is the first approach to achieve lossless pruning with such a high compression ratio. The code and models for ResRep can be found at https://github.com/DingXiaoH/ResRep.