Recent advancements in facial landmark detection have relied on increasing model parameters or augmenting annotations. However, this approach presents three challenges: increased computational overhead, the risk of overfitting, and the burden of labor-intensive annotation. To address these weaknesses, we propose a sample-adaptive adversarial training (SAAT) method. SAAT involves optimizing an attacker and a detector interactively to enhance facial landmark detection as a defense against sample-adaptive black-box attacks. By leveraging adversarial attacks, SAAT goes beyond handcrafted transformations to exploit adversarial perturbations that reflect the detector's weaknesses. The attacker generates these perturbations, and the detector must improve its robustness against them to defend against adversarial attacks. Additionally, a sample-adaptive weight is introduced to balance the risks and benefits of augmenting adversarial examples during detector training. We also introduce a new dataset called Masked-300W, which focuses on masked face alignment, to evaluate our method. Experimental results demonstrate that our SAAT approach performs comparably to existing state-of-the-art methods. The dataset and model are publicly available at https://github.com/zhuccly/SAAT.