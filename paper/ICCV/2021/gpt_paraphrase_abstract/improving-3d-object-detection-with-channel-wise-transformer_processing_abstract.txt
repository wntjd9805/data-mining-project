State-of-the-art two-stage detectors for 3D object detection from point clouds face a significant challenge in terms of flexible and high-performance proposal refinement. Previous methods rely on human-designed components, such as keypoints sampling and multi-scale feature fusion, to generate powerful 3D object representations. However, these methods have limited capability to capture contextual dependencies among points. In this study, we propose a new two-stage 3D object detection framework called CT3D, which addresses this limitation by leveraging a high-quality region proposal network and a Channel-wise Transformer architecture. CT3D performs proposal-aware embedding and channel-wise context aggregation simultaneously, enhancing the point features within each proposal. The framework utilizes the proposal's keypoints for spatial contextual modeling and attention propagation, mapping the proposal to point embeddings. Additionally, a channel-wise decoding module enriches the interaction between query-key pairs through channel-wise re-weighting, effectively merging multi-level contexts and improving object predictions. Extensive experiments demonstrate that CT3D outperforms state-of-the-art 3D detectors, achieving an AP of 81.77% in the moderate car category on the KITTI test 3D detection benchmark. The proposed framework exhibits superior performance and scalability, with minimal hand-crafted design.