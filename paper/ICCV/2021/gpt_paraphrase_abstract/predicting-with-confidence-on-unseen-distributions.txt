Recent research has demonstrated that machine learning models can exhibit significant variation in accuracy when evaluated on data that slightly differs from the training data. This poses a challenge in predicting model performance on unseen distributions without labeled data, which is crucial for enhancing the reliability of machine learning models. Although distance measures are commonly employed to adapt models and enhance their performance on new domains in the context of distribution shift, the estimation of accuracy is often overlooked. Our study reveals that commonly used distributional distances, such as Frechet distance or Maximum Mean Discrepancy, do not reliably estimate performance under distribution shift. Conversely, our proposed approach called the difference of confidences (DoC) proves successful in estimating classifier performance across various shifts and model architectures. Despite its simplicity, DoC surpasses other methods in synthetic, natural, and adversarial distribution shifts, reducing error by more than 46% on challenging datasets like ImageNet-Vid-Robust and ImageNet-Rendition.