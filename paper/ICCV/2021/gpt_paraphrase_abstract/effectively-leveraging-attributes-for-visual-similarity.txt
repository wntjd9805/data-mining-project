Measuring similarity between images involves complex reasoning across different factors such as color, texture, and shape. Annotated attributes can provide insights into what is important for measuring similarity. However, previous approaches consider these annotations as complete, leading to a simplistic method of predicting attributes on single images to measure similarity. This approach is impractical as it is not possible to fully annotate every important attribute in a dataset. By representing images based on incomplete annotations, crucial information may be overlooked. To address this problem, we propose PAN (Pairwise Attribute-informed similarity Network), which breaks down similarity learning into capturing similarity conditions and relevance scores from a joint representation of two images. This allows our model to identify when two images share the same attribute but consider it irrelevant due to subtle differences between them. Notably, PAN outperforms previous methods that use attribute annotations, showing a 4-9% improvement in compatibility prediction between clothing items on Polyvore Outfits, a 5% gain in few-shot classification of images using Caltech-UCSD Birds (CUB), and over a 1% boost in Recall@1 on In-Shop Clothes Retrieval. The implementation of PAN is available at https://github.com/samarth4149/PAN.