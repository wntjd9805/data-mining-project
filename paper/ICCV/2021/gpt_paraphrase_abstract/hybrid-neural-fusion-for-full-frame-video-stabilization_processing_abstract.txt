This study introduces a novel approach to video stabilization that aims to eliminate visible distortion and the need for aggressive cropping, which often results in a narrower field of view. The proposed method involves estimating dense warp fields from adjacent frames and then synthesizing a stabilized frame by combining the warped contents. The key innovation of this approach lies in a learning-based hybrid-space fusion technique that reduces artifacts caused by inaccuracies in optical flow and fast-moving objects. The effectiveness of the method is validated using the NUS, selfie, and DeepStab video datasets, and extensive experimental results demonstrate its superiority over previous video stabilization methods.