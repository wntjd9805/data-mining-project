This research examines the issue of discovering new categories in data that includes both single-modal and multi-modal information, with labels coming from different but relevant categories. The paper presents a comprehensive framework that simultaneously learns a reliable representation and assigns clusters to unlabeled data. To prevent the learned embedding from being too specific to the labeled data, the authors draw inspiration from self-supervised representation learning and extend it to handle both labeled and unlabeled data. This involves incorporating category discrimination for labeled data and cross-modal discrimination for multi-modal data, in addition to the instance discrimination used in traditional contrastive learning methods. The authors also utilize the Winner-Take-All hashing algorithm to generate pseudo labels for unlabeled data in order to improve cluster assignment predictions. The framework is extensively evaluated on various large-scale datasets, including Kinetics-400 and VGG-Sound for videos, and CIFAR10, CIFAR100, and ImageNet for images, achieving state-of-the-art results.