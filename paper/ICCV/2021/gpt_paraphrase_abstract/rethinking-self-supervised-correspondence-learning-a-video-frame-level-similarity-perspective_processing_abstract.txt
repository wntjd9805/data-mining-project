Learning a strong representation for space-time correspondence is crucial for various computer vision tasks such as object tracking and video object segmentation. Previous studies have proposed self-supervised pretext tasks to learn generalizable correspondence in large-scale datasets, focusing on object-level or patch-level similarity learning. In contrast, we propose a different approach called Video Frame-level Similarity (VFS) learning, where we learn correspondence by comparing video frames directly. Our inspiration comes from the success of image-level contrastive learning and similarity learning in visual recognition. We hypothesize that a representation good for recognition requires convolutional features to identify correspondence between similar objects or parts. Surprisingly, our experiments demonstrate that VFS outperforms state-of-the-art self-supervised methods in both OTB visual object tracking and DAVIS video object segmentation. We conduct a detailed analysis to uncover the factors contributing to the success of VFS and reveal new insights into image and frame level similarity learning. For more information and access to our code, please visit our project page: https://jerryxu.net/VFS.