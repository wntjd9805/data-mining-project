Deep neural networks have achieved impressive performance in various classification tasks using softmax cross-entropy (CE) loss as the standard objective function. However, CE loss only encourages higher scores for the true class, without explicitly promoting separation between different class features. To address this limitation, we propose a new loss function called Orthogonal Projection Loss (OPL). OPL enforces orthogonality in the feature space by leveraging the observation that ground-truth class representations in CE loss are orthogonal (one-hot encoded vectors). By incorporating orthogonality constraints on the mini-batch level, OPL enhances the capabilities of CE loss by directly enforcing inter-class separation and intra-class clustering in the feature space. Unlike other alternatives to CE, OPL offers several advantages, including the absence of additional learnable parameters, no requirement for careful negative mining, and insensitivity to batch size. We evaluate OPL on various tasks, such as image recognition, large-scale classification, domain generalization, and few-shot learning, and demonstrate its effectiveness across all these tasks. Additionally, OPL exhibits better robustness against practical challenges like adversarial attacks and label noise. The code for implementing OPL is available at the following GitHub repository: https://github.com/kahnchana/opl.