Current RGB-D saliency detection models do not explicitly utilize both RGB and depth information for effective multi-modal learning. In this study, we propose a new multi-stage cascaded learning framework that incorporates mutual information minimization to model the relationship between RGB images and depth data. Firstly, we map the features of each modality to lower-dimensional feature vectors and apply mutual information minimization as a regularizer to reduce redundancy between appearance features from RGB and geometric features from depth. Then, we implement multi-stage cascaded learning to enforce the mutual information minimization constraint at each stage of the network. Extensive experiments on benchmark RGB-D saliency datasets validate the effectiveness of our framework. Additionally, to advance the field, we introduce the COME15K dataset, which is seven times larger than NJU2K and contains 15,625 image pairs with high-quality annotations at polygon, scribble, object, instance, and rank levels. Utilizing these annotations, we construct four new benchmarks with strong baselines and observe intriguing phenomena that can inspire future model designs. Our source code and dataset are publicly available at https://github.com/JingZhang617/cascaded_rgbd_sod.