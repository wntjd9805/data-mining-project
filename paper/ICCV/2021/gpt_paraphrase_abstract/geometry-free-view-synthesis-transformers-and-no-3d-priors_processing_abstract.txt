Is it necessary to have a geometric model in order to create new perspectives from a single image? While convolutional neural networks (CNNs) rely on local convolutions and require explicit 3D biases to model geometric transformations, we demonstrate that a transformer-based model can generate completely new perspectives without the need for hand-engineered 3D biases. This is accomplished through a global attention mechanism that learns long-range 3D correspondences between the source and target views, as well as a probabilistic formulation that captures the inherent ambiguity when predicting new views from a single image. This overcomes the limitations of previous approaches that are limited to small viewpoint changes. We explore different methods of incorporating 3D priors into a transformer architecture, but our experiments show that such geometric priors are unnecessary as the transformer can implicitly learn 3D relationships between images. Additionally, our approach surpasses the current state of the art in terms of visual quality and covers the full range of possible outcomes.