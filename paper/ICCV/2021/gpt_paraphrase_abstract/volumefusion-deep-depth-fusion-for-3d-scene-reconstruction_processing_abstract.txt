This paper proposes a deep neural network approach for reconstructing a 3D scene from a set of calibrated views. Traditional methods for multi-view stereo rely on two stages: computing local depth maps and fusing them to create global depth maps. Recent studies have explored using deep neural architectures for depth estimation, either through conventional fusion methods or by directly regressing Truncated Signed Distance Function (TSDF) for 3D reconstruction.The authors argue that replicating the traditional two-stage framework using deep neural networks can improve both the interpretability and accuracy of the results. Their network operates in two steps: first, a deep multi-view stereo technique is used to compute local depth maps, and then the depth maps and image features are fused to create a single TSDF volume. To address the challenge of matching images from different viewpoints, a rotation-invariant 3D convolution kernel called PosedConv is introduced.The proposed architecture is evaluated on the ScanNet dataset through a series of experiments. The results show that their approach outperforms both traditional methods and other deep learning techniques. This highlights the effectiveness of the proposed network in reconstructing 3D scenes from calibrated views.