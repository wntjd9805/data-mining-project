Current successes in Embodied AI, particularly in deep reinforcement learning (RL), heavily rely on carefully designed rewards. Without these shaped rewards, the performance of Embodied AI systems declines significantly across various tasks, such as single-agent point-goal navigation, two-agent furniture moving, and three-agent football gameplay. However, training with shaped rewards is not scalable to more realistic tasks. To address this limitation, we propose GRIDTOPIX, a two-step approach: 1) training agents with terminal rewards in simplified gridworlds that resemble Embodied AI environments, and 2) transferring the learned policies to agents operating in complex visual worlds. Remarkably, even with only terminal rewards and using the same models and RL algorithms, GRIDTOPIX achieves significant improvements in performance across tasks, including point-goal navigation, furniture moving, and football gameplay. Moreover, GRIDTOPIX also enhances the results obtained from training with shaped rewards.