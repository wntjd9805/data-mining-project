This study focuses on active visual exploration, which involves guiding an agent with a limited field of view to understand its environment by strategically choosing viewing directions. Existing methods for addressing this problem have limitations, such as the difficulty of training reinforcement learning models or the task-specific nature of uncertainty maps.   To overcome these limitations, the authors propose a model called Glimpse-Attend-and-Explore. This model utilizes self-attention instead of task-specific uncertainty maps, making it applicable to both dense and sparse prediction tasks. Additionally, the model incorporates a contrastive stream to enhance the learned representations.   Unlike previous studies, the authors demonstrate the effectiveness of their model in various tasks, including reconstruction, segmentation, and classification. The results show promising performance and reduced reliance on dataset bias for guiding the exploration. To gain further insights, an ablation study is conducted to analyze the features and attention learned by the model.   The authors also highlight that their self-attention module learns to attend to different regions of the scene by minimizing the loss on the downstream task. The code for their model is available at the provided GitHub link.