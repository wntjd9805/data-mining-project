We propose a deep neural network architecture that can accurately estimate dense depth from an image and a sparse point cloud. The network is trained using a video stream and synchronized sparse point cloud data obtained from a LIDAR or similar range sensor, along with the camera's intrinsic calibration parameters. During inference, the network takes as input the calibration parameters of the camera, which may differ from those used during training, along with the sparse point cloud and a single image. A Calibrated Backprojection Layer is used to project each pixel in the image into three-dimensional space using the calibration matrix and a depth feature descriptor. The resulting 3D positional information is combined with the image descriptor and the previous layer's output to form the input for the next layer of the encoder. The decoder, which utilizes skip-connections, generates a dense depth map. Our proposed network, called KBNet, is trained without supervision by minimizing the photometric reprojection error. Instead of relying on generic regularization techniques, KBNet imputes missing depth values based on the training set. We evaluate the performance of KBNet on public depth completion benchmarks and demonstrate that it outperforms the current state-of-the-art methods by 30% indoors and 8% outdoors when the same camera is used for both training and testing. Furthermore, when the test camera is different, KBNet achieves a significant improvement of 62%.