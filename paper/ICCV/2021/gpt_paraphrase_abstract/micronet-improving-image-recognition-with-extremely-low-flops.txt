This paper aims to address the issue of significant performance degradation at extremely low computational cost. The authors propose two effective factors, namely sparse connectivity and dynamic activation function, to improve accuracy. Sparse connectivity prevents a major reduction in network width, while the dynamic activation function mitigates the negative impact of reducing network depth. The authors introduce micro-factorized convolution, which factorizes a convolution matrix into low rank matrices, to integrate sparse connectivity into convolution. Additionally, they present a new dynamic activation function called Dynamic Shift Max, which enhances non-linearity through multiple dynamic fusions between an input feature map and its circular channel shift. By incorporating these two operators, they develop a family of networks called MicroNet, which achieves significant performance gains compared to the current state of the art in the low FLOP regime. For example, with a constraint of 12M FLOPs, MicroNet achieves a top-1 accuracy of 59.4% on ImageNet classification, surpassing MobileNetV3 by 9.6%. The source code can be found at https://github.com/liyunsheng13/micronet.