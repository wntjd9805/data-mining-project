While there has been progress in developing visual-linguistic (VL) models, few have focused on creating small VL models. This study explores the use of knowledge distillation (KD) to compress a large transformer-based VL model into a smaller one. The main challenge is aligning the hidden representations and attention distributions of the teacher and student models, which have inconsistent regional visual tokens. To address this, we retrain and adapt the teacher model using region proposals from the student's detector but with features from the teacher's own object detector. This alignment allows the adapted teacher to transfer knowledge through intermediate representations. We use mean square error loss to mimic attention distribution and introduce a token-wise noise contrastive loss to align hidden states. Our proposed distillation method significantly improves the performance of small VL models in image captioning and visual question answering tasks, achieving a CIDEr score of 120.8 on COCO captioning (a 5.1 improvement) and an accuracy of 69.8 on VQA 2.0 (a 0.8 gain). Extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages.