Universal adversarial perturbation (UAP) is a practical attack method that involves generating a single perturbation to deceive a neural network for most images. This approach is advantageous because the UAP can be created in advance and directly applied during the attack. Previous studies have observed that untargeted UAP tends to misclassify images to a dominant label, but the reason behind this phenomenon remains unclear. This research aims to provide an alternative explanation for this observation. To develop a more practical universal attack, the focus is on reducing the reliance on original training samples. This involves eliminating the need for sample labels and minimizing the sample size. The proposed solution involves using artificial Jigsaw images as training samples, which demonstrates competitive performance. Additionally, the potential for a data-free black-box attack, which is a highly practical yet challenging threat model, is explored. It is shown that optimization-free repetitive patterns can effectively attack deep models. The code for this research is available at https://bit.ly/3y0ZTIC.