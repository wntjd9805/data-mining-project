We introduce a neural network that can generate 3D objects and scenes using only 2D observations. This network represents 3D geometries as a radiance field, utilizing a set of 2D images with camera information to construct an internal representation for each point in 3D space. This representation allows for rendering the appearance and geometry of the point from any viewpoint. Our approach focuses on learning local features for each pixel in the 2D images and projecting them onto 3D points, resulting in comprehensive point representations. Additionally, we incorporate an attention mechanism to combine pixel features from multiple 2D views, effectively considering visual occlusions. Extensive experiments demonstrate that our method can produce high-quality and realistic views of new objects, unseen categories, and challenging real-world scenes.