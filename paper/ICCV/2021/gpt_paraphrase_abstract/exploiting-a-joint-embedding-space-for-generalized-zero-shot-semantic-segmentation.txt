We propose a discriminative approach to address the limitations of generative methods in generalized zero-shot semantic segmentation (GZS3). Instead of synthesizing visual features for unseen classes, we use visual and semantic encoders to learn a joint embedding space. This space allows us to transform semantic features into prototypes that act as centers for visual features. We introduce boundary-aware regression (BAR) and semantic consistency (SC) losses to learn discriminative features and alleviate the bias towards seen classes. At test time, we use semantic prototypes as a nearest-neighbor classifier, eliminating the need for retraining. Additionally, we propose Apollonius calibration (AC), an inference technique that adapts the decision boundary of the classifier to the Apollonius circle, further reducing bias. Our framework achieves state-of-the-art results on standard benchmarks.