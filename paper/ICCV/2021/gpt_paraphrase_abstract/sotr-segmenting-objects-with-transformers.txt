Recent transformer-based models have achieved impressive results in vision tasks, surpassing the performance of Convolutional Neural Networks (CNNs). In this study, we introduce a new transformer-based model called Segmenting Objects with TRansformers (SOTR) for high-quality instance segmentation. SOTR simplifies the segmentation process by using an alternative CNN backbone combined with two parallel subtasks: (1) predicting the category of each instance using a transformer, and (2) dynamically generating segmentation masks through a multi-level upsampling module. SOTR effectively extracts lower-level feature representations and captures long-range contextual dependencies by utilizing a Feature Pyramid Network (FPN) and a twin transformer, respectively. The twin transformer is designed to be more time and resource-efficient compared to the original transformer, as it only requires row and column attention to encode pixels. Additionally, SOTR can be easily integrated with various CNN backbones and transformer model variants, leading to significant improvements in segmentation accuracy and training convergence. Extensive experiments conducted on the MS COCO dataset demonstrate the superior performance of SOTR compared to state-of-the-art instance segmentation approaches. We believe that our simple yet robust framework can serve as a strong baseline for instance-level recognition. The source code for SOTR is publicly available at https://github.com/easton-cau/SOTR.