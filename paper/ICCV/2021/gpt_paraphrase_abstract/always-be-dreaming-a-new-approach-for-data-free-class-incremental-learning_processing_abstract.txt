Current computer vision applications face a significant problem known as catastrophic forgetting when learning new concepts incrementally. Existing solutions to mitigate this issue often require replaying previously seen data, which is problematic in cases where memory constraints or data legality concerns exist. This study focuses on Data-Free Class-Incremental Learning (DFCIL), where an incremental learning agent must learn new concepts over time without storing generators or training data from past tasks. One approach to DFCIL involves replaying synthetic images generated by inverting a frozen copy of the learner's classification model. However, this approach fails when using standard distillation strategies for common class-incremental benchmarks. The cause of this failure is diagnosed, and a novel distillation strategy for DFCIL is proposed. This strategy includes modified cross-entropy training and importance-weighted feature distillation. The results show that our method improves final task accuracy by up to 25.1% (absolute difference) compared to state-of-the-art DFCIL methods for common class-incremental benchmarks. Our method even outperforms several replay-based methods that store a coreset of images. The code for our method is available at https://github.com/GT-RIPL/AlwaysBeDreaming-DFCIL.