Current methods for inverting Generative Adversarial Networks (GANs) face a paradox: they can either achieve high-quality reconstruction or retain the ability to edit images, but not both. This limitation prevents the realization of real image editing. To address this issue, we propose a solution that incorporates consecutive images, such as video frames or images of the same subject in different poses, into the inversion process. The rationale behind our approach is that the continuity between consecutive images provides inherent directions for editing. We leverage this inherent property for two main purposes: 1) regularizing the joint inversion process, ensuring that each inverted code can be semantically accessed from another code and is confined to an editable domain; 2) enforcing coherence between images, maximizing the fidelity of each inverted code by complementing information from other images. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art techniques in terms of both reconstruction fidelity and editability, on both real image and synthesis datasets. Additionally, our approach introduces the first support for video-based GAN inversion and enables unsupervised semantic transfer between consecutive images. The source code for our method is available at: https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs.