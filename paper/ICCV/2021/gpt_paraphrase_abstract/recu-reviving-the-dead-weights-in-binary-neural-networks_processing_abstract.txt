Binary neural networks (BNNs) have gained significant attention for their ability to reduce computation and memory requirements. Previous research has focused on minimizing quantization error or designing gradient approximations to address gradient mismatch, but the issue of "dead weights" - weights that are rarely updated during training - has been overlooked. In this study, we investigate the impact of dead weights on BNN training and introduce a rectified clamp unit (ReCU) to revive these weights and update them. We demonstrate that reviving dead weights using ReCU can result in a smaller quantization error. Additionally, we analyze the information entropy of weights and mathematically explain the benefits of weight standardization for BNNs. We identify the contradiction between minimizing quantization error and maximizing information entropy and propose an adaptive exponential scheduler to identify the range of dead weights. By considering dead weights, our method achieves faster BNN training and outperforms recent methods on CIFAR-10 and ImageNet datasets. The code for our method is available at https://github.com/z-hXu/ReCU.