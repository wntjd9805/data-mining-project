The focus of previous research in unpaired video translation has been on achieving short-term temporal consistency by considering neighboring frames. However, when transferring from simulated to photorealistic sequences, the knowledge of the underlying geometry can be utilized to achieve global consistency across different views. In this study, we propose a new approach that combines unpaired image translation with neural rendering to transfer simulated surgical abdominal scenes to photorealistic ones. Our method incorporates global learnable textures and a lighting-invariant view-consistency loss, resulting in consistent translations of arbitrary views and enabling long-term consistent video synthesis. We specifically design and test our model for generating video sequences of minimally-invasive surgical abdominal scenes. Since labeled data is often limited in this field, the availability of photorealistic data that preserves ground truth information from the simulated domain is particularly relevant. By extending existing image-based methods to view-consistent videos, our aim is to enhance the applicability of simulated training and evaluation environments for surgical applications. The code and data for our approach can be accessed at http://opencas.dkfz.de/video-sim2real.