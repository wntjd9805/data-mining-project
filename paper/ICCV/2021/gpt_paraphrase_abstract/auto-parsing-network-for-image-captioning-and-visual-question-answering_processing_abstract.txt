We propose the Auto-Parsing Network (APN) as a method to improve the effectiveness of Transformer-based vision-language systems by uncovering and utilizing the hidden tree structures within the input data. Our approach involves using a Probabilistic Graphical Model (PGM) with attention operations on each self-attention layer to incorporate a sparse assumption. This PGM allows us to softly segment the input sequence into clusters, where each cluster represents the parent of the entities within it. By stacking these PGM constrained self-attention layers, the clusters in lower layers combine to form a new sequence, and the PGM in higher layers further segments this sequence. This iterative process results in an implicitly parsed sparse tree, and the hierarchical knowledge of this tree is incorporated into the transformed embeddings, which can be used for solving vision-language tasks. We demonstrate the effectiveness of our APN in two major vision-language tasks: Captioning and Visual Question Answering. Additionally, we develop a PGM probability-based parsing algorithm that allows us to discover the hidden structure of the input during inference.