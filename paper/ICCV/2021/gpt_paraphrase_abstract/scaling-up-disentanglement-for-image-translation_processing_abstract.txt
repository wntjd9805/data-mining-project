Existing image translation methods aim to manipulate labeled attributes while preserving unlabeled attributes. However, current methods either achieve disentanglement with low visual fidelity or visually plausible translations without disentanglement. To address this, we propose OverLORD, a framework that disentangles labeled and unlabeled attributes and synthesizes high-fidelity images. Our framework consists of two stages: (i) Disentanglement, where we learn disentangled representations using latent optimization without relying on adversarial training or architectural biases. (ii) Synthesis, where we train feed-forward encoders to infer the learned attributes and tune the generator adversarially to enhance perceptual quality. Additionally, when labeled and unlabeled attributes are correlated, we model an additional representation to improve disentanglement. Our framework covers various settings, including disentangling labeled attributes, pose and appearance, localized concepts, and shape and texture. Experimental results demonstrate that our approach achieves superior disentanglement with improved translation quality and increased output diversity compared to state-of-the-art methods.