This paper focuses on improving fast video moment retrieval (fastVMR), specifically in localizing the target moment accurately and efficiently based on a given natural language sentence query. The authors argue that existing VMR approaches can be categorized into three modules: video encoder, text encoder, and cross-modal interaction module. However, the cross-modal interaction module poses a computational bottleneck during testing. To address this issue, the authors propose replacing the cross-modal interaction module with a cross-modal common space. This common space allows for the learning of moment-query alignment and enables efficient moment search. To enhance the robustness of the learned space, the authors introduce a fine-grained semantic distillation framework that transfers knowledge from additional semantic structures. This is achieved by constructing a semantic role tree that breaks down the query sentence into different phrases or subtrees. A hierarchical semantic-guided attention module is then employed to facilitate message propagation throughout the tree and extract discriminative features. Finally, important and discriminative semantics are transferred to the common space through a matching-score distillation process. The proposed method is evaluated on three popular VMR benchmarks, and the experimental results demonstrate its high speed and significant performance.