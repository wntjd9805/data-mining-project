We present a novel method for modeling human clothing using point clouds. Our approach involves training a deep model that can generate point clouds for different outfits, poses, and body shapes. This model is capable of handling various types and structures of outfits. With this learned model, we can accurately reconstruct the geometry of new outfits using just a single image. Additionally, we can adapt these outfits to different bodies and poses through outfit retargeting. To enhance the realism of our models, we incorporate appearance modeling by using the point cloud geometry as a framework and employing neural point-based graphics to capture outfit appearance from videos and re-render them. We compare our approach with existing methods and demonstrate the effectiveness of point-based clothing modeling in both geometry and appearance aspects.