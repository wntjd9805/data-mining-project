Computer vision research has made significant strides in using contextual representations to improve model performance for specific tasks. However, these advancements primarily focus on enhancing performance for individual tasks. In this study, we examine a multi-task environment for dense prediction tasks, where a common backbone is used alongside task-specific heads. Our objective is to identify the most efficient method for refining task predictions by incorporating cross-task contexts that depend on the relationships between tasks. We investigate different attention-based contexts, such as global and local, in the multi-task setting and analyze their effectiveness when applied to refine each task independently. Our empirical findings demonstrate that different task pairs benefit from different types of context. To automate the selection process, we propose an Adaptive Task-Relational Context (ATRC) module. This module utilizes neural architecture search to sample from a pool of available contexts for each task pair and outputs the optimal configuration for deployment. Our method achieves state-of-the-art performance on two important multi-task benchmarks, namely NYUD-v2 and PASCAL-Context. The proposed ATRC module is computationally efficient and can be easily integrated as a refinement module in any supervised multi-task architecture.