This study aims to elucidate adversarial attacks by examining how adversarial perturbations contribute to the attacking task. Using the Shapley value, we estimate the contributions of different regions in an image to the reduction of the attacking cost. We also analyze the interactions among pixels in adversarial perturbations, decomposing the entire perturbation map into relatively independent components. Our decomposition reveals that adversarially-trained deep neural networks (DNNs) exhibit more foreground perturbation components compared to normally-trained DNNs. Furthermore, the adversarially-trained DNNs have a greater number of components that primarily decrease the score of the true category, in contrast to normally-trained DNNs. These findings offer novel insights into the understanding of adversarial attacks.