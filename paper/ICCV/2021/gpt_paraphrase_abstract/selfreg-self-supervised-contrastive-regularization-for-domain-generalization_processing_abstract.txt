Typically, when conducting experiments in deep learning, it is assumed that the training and test datasets are drawn from the same distribution. However, in real-world scenarios, there may be a difference in distribution between the datasets, known as domain shift, which hinders the model's ability to generalize. The research field dedicated to addressing this issue is called domain generalization, which aims to extract features that are invariant across domains. Recently, contrastive learning-based approaches have emerged as effective solutions for domain generalization, but they require sampling of negative data pairs. The quality and quantity of these negative pairs significantly impact the performance of contrastive learning. To tackle this problem, we propose a new regularization method called self-supervised contrastive regularization (SelfReg), which only utilizes positive data pairs, thus eliminating issues related to negative pair sampling. Additionally, we introduce a class-specific domain perturbation layer (CDPL), enabling effective mixup augmentation even when only positive data pairs are available. Our experimental results demonstrate that the techniques employed in SelfReg contribute to improved performance. Specifically, in the DomainBed benchmark, our proposed method achieves comparable results to the conventional state-of-the-art alternatives.