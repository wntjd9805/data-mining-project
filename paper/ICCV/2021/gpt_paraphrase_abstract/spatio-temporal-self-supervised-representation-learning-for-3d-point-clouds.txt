Currently, there is a lack of practical and generalizable pre-trained models for various 3D scene understanding tasks. This is primarily due to the complexity of these tasks and the variations introduced by factors such as camera views, lighting, and occlusions. In this study, we address this challenge by introducing a framework called spatio-temporal representation learning (STRL), which can learn from unlabeled 3D point clouds in a self-supervised manner. Inspired by how infants learn from visual data in their environment, we explore the rich spatio-temporal cues derived from the 3D data.The STRL framework takes two frames from a sequence of 3D point clouds that are temporally correlated and uses spatial data augmentation to transform them. It then learns an invariant representation in a self-supervised manner. To validate the effectiveness of STRL, we conducted extensive experiments on three types of datasets: synthetic, indoor, and outdoor. The results of these experiments show that the learned self-supervised representation enables various models to achieve comparable or even better performances compared to supervised learning methods. Additionally, these pre-trained models can be generalized to downstream tasks such as 3D shape classification, 3D object detection, and 3D semantic segmentation. Furthermore, the inclusion of spatio-temporal contextual cues in the 3D point clouds significantly improves the learned representations.