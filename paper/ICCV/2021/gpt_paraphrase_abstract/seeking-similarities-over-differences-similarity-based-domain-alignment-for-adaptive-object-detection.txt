To enable object detectors to work effectively in various scenarios, they need to be adaptable to changes in input distribution without the need for constant annotation of new data. This has led to the development of Unsupervised Domain Adaptation (UDA) algorithms for detection. These UDA methods aim to align the features of detectors from labeled source domains with those from unlabeled target domains. However, there is no consensus on which features to align and how to perform the alignment. In this study, we propose a framework that encompasses the different components commonly used in UDA methods. This framework allows for a thorough analysis of the design space of UDA. Specifically, we introduce a novel UDA algorithm called ViSGA that directly implements our framework. ViSGA incorporates the best design choices and introduces a simple yet effective method to aggregate features at the instance level based on visual similarity. We then induce group alignment through adversarial training. Our experiments demonstrate that both similarity-based grouping and adversarial training enable our model to focus on aligning feature groups instead of matching all instances across loosely aligned domains. Furthermore, we evaluate the applicability of ViSGA to scenarios where labeled data are collected from different sources. The results show that our method not only outperforms previous single-source approaches on Sim2Real and Adverse Weather datasets but also generalizes well to the multi-source setting.