The dominant approach in unsupervised learning is the instance discrimination paradigm. This paradigm involves a teacher-student framework, where the teacher provides embedded knowledge to guide the student's learning of meaningful representations. However, the teacher's outputs can vary significantly for the same instance during different training stages, introducing unexpected noise and leading to catastrophic forgetting. To address this issue, we propose a novel algorithm called Temporal Knowledge Consistency (TKC) that integrates instance temporal consistency into current instance discrimination paradigms. TKC dynamically combines the knowledge from temporal teachers and selects useful information based on its importance for learning instance temporal consistency. Experimental results demonstrate that TKC can learn better visual representations on both ResNet and AlexNet models, and it also transfers well to downstream tasks. The effectiveness and generalization of our method are supported by all the experiments. The code for our method will be made available. Figure 1 illustrates how mainstream unsupervised methods adopt the teacher-student framework, where the teacher is an ensemble of previous student encoders. The figure shows the proportion of previous students in the teacher over training steps. The red curve represents the EMA teacher that combines previous encoders with a predetermined factor α, only including encoders from very close steps. Our TKC (the green curve) reuses early models and adaptively learns the importance ω for each of them, resulting in temporal consistent representations.