This paper proposes a method to improve monocular depth estimation by incorporating 3D points as depth guidance. Unlike existing methods, this approach performs well on sparse and unevenly distributed point clouds, regardless of the source of the 3D points. The method introduces a lightweight and efficient multi-scale 3D point fusion network. The versatility of the network is demonstrated on two different depth estimation problems using 3D points obtained from structure-from-motion and LiDAR. In both cases, the network achieves comparable performance to state-of-the-art depth completion methods, but with significantly higher accuracy when using a small number of points. Additionally, the proposed method is more compact in terms of the number of parameters. Experimental results show that the method outperforms contemporary deep learning-based multi-view stereo and structure-from-motion methods in terms of both accuracy and compactness.