We present a straightforward approach to pre-training point clouds, consisting of three steps: masking occluded points in a camera view, training an encoder-decoder model to reconstruct the occluded points, and utilizing the encoder weights for downstream point cloud tasks. Our method demonstrates improved accuracy across various datasets and encoders, enhancing object classification, part-based segmentation, and semantic segmentation tasks compared to previous pre-training methods. Analysis of the pre-trained features reveals wide downstream minima, strong transformation invariance, and highly correlated activations with part labels. The code and data are accessible at: https://github.com/hansen7/OcCo.