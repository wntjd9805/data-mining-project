This abstract discusses a new framework for improving the accuracy of bounding box annotations in deep learning-based visual trackers. The framework uses a selection-and-refinement strategy to enhance the preliminary annotations generated by tracking algorithms. It introduces a temporal assessment network (T-Assess Net) to assess the quality of tracking results based on their temporal coherence, and a visual-geometry refinement network (VG-Refine Net) to further enhance the selected results by considering target appearance and temporal geometry constraints. The combination of these networks ensures high-quality automatic video annotation. Experimental results on large-scale tracking benchmarks demonstrate that this method significantly reduces the need for human labor by 94% while delivering highly accurate annotations, thereby improving tracking performance with augmented training data.