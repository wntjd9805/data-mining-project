In this study, we investigate the extent to which we can infer the complete floorplan of an environment with only limited glimpses. Existing methods are limited in that they can only map what is immediately visible or discernible from the context, requiring extensive movements to fully map a space. To overcome this limitation, we explore the potential of combining audio and visual sensing to rapidly reconstruct floorplans from restricted viewpoints. By incorporating audio, we not only gain the ability to sense the geometry outside the camera's field of view but also identify distant open spaces (e.g., a dog barking in another room) and infer the presence of rooms that are not visible to the camera (e.g., a dishwasher humming in the assumed kitchen to the left). To achieve this, we propose AV-Map, a new framework that combines audio and vision in an encoder-decoder model to reconstruct floorplans using short video sequences as input. Our model is trained to predict both the interior structure of the environment and the semantic labels of the associated rooms. Through experiments conducted on 85 real-world environments, we demonstrate the effectiveness of our approach. With just a few glimpses covering 26% of the area, our model achieves a 66% accuracy in estimating the entire floorplan, which is significantly better than the current state-of-the-art method for extrapolating visual maps.