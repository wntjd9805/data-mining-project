Neural image compression has seen significant advancements in recent years, surpassing traditional methods in terms of compression performance. However, there are still limitations in preserving fine spatial details, particularly at low compression rates. To address this issue, we propose three contributions. Firstly, we introduce a novel back projection method that incorporates attentional and multi-scale feature fusion to enhance representation power. This method recalibrates the current estimation by establishing feedback connections between high-level and low-level attributes in an attentional and discriminative manner. Secondly, we suggest decomposing the input image and separately processing the distinct frequency components. These components are then recombined using a dual attention module, allowing for explicit manipulation of details within regions of interest. Lastly, we present a new training scheme to reduce the residual rounding of the latent variables. Experimental results demonstrate the effectiveness of our approach, with our model outperforming the state-of-the-art method by reducing BD-rate by 9.88% and 10.32% on the Kodak and CLIC2020 Professional Validation datasets, respectively. Additionally, our model achieves a reduction of 4.12% and 4.32% over the latest coding standard Versatile Video Coding (VVC) in terms of PSNR. Furthermore, our approach produces visually pleasing images with improved compression quality when optimized for MS-SSIM. These results highlight the significant improvement our method offers in preserving and enhancing spatial information during image compression.