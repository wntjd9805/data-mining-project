AI algorithms have been found to learn biases from data, highlighting the need to identify and address these biases. However, current methods heavily rely on human experts to identify potential biases, which may overlook other biases that humans are not aware of. In this study, we propose a new approach to help experts discover unknown biased attributes in AI algorithms. We formulate the problem as finding the biased attribute of a classifier that predicts a target attribute in an input image. To address this challenging problem, we use a hyperplane in the latent space of a generative model to represent image attributes. By optimizing the hyperplane's normal vector and offset, we transform the original problem into a solvable one. We introduce a novel total-variation loss as the objective function and an orthogonalization penalty as a constraint to prevent trivial solutions. Our extensive experiments on different datasets demonstrate that our method effectively discovers biased attributes and improves disentanglement with respect to target attributes. Additionally, our method successfully uncovers unnoticed biases in various object and scene classifiers, showcasing its generalizability in detecting biased attributes across diverse image domains.