This study focuses on the problem of compositional generalization in inferring objects and their relationships from images using scene graphs. The existing models are trained on a small portion of the data distribution, which consists of the most frequent compositions. However, in real-world scenarios, test images often contain unseen or rare compositions. To improve generalization, the authors propose a method to synthesize rare yet plausible scene graphs by perturbing real ones. They then introduce a model based on conditional generative adversarial networks (GANs) to generate visual features of the perturbed scene graphs and learn from them. The approach shows marginal but consistent improvements in zero- and few-shot metrics when evaluated on the Visual Genome dataset. The study concludes by discussing the limitations of the proposed approach and suggesting potential directions for future research.