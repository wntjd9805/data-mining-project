Recently, there has been a growing interest in cross-modal pre-training methods. However, existing approaches face challenges in terms of computational complexity when performing cross-modal retrieval. To address this issue, we propose a method called Contrastive Cross-Modal Knowledge Sharing Pre-training (COOKIE) to learn universal representations for both text and images. Our method incorporates two main components: a weight-sharing transformer that aligns text and image semantics, and three types of contrastive learning to facilitate knowledge sharing across modalities. Through cross-modal knowledge sharing, we enhance the learning of unimodal representations. Experimental results on various multi-modal matching tasks, including cross-modal retrieval, text matching, and image retrieval, demonstrate the effectiveness and efficiency of our pre-training framework. Notably, our COOKIE model, fine-tuned on MSCOCO, Flickr30K, and MSRVTT datasets, achieves state-of-the-art results while requiring only 3/1000 of the inference time compared to one-stream models. Additionally, there are notable improvements of 5.7% and 3.9% in image retrieval and text matching tasks, respectively. The source code for our method will be made available at https://github.com/kywen1119/COOKIE.