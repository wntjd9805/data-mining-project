This paper introduces InSeGAN, an unsupervised 3D generative adversarial network (GAN) designed for segmenting nearly identical instances of rigid objects in depth images. The proposed GAN architecture allows for the synthesis of multiple-instance depth images with independent control over each instance. The generator of InSeGAN consists of two modules: the instance feature generator and the depth image renderer. The instance feature generator uses encoded 3D poses to transform a learned implicit object template into a feature map representation for each object instance. The depth image renderer combines the single-instance feature maps to generate a multiple-instance depth image. A discriminator is employed to distinguish the generated depth images from true depth images. To enable instance segmentation, an instance pose encoder is proposed to reproduce the pose code vectors for all object instances using a generated depth image. To evaluate the model, a new synthetic dataset called "Insta-10" is introduced, which consists of 100,000 depth images with 5 instances of an object from one of 10 classes. Experimental results on both Insta-10 and real-world noisy depth images demonstrate that InSeGAN achieves state-of-the-art performance, often surpassing previous methods by significant margins.