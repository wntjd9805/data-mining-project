The lack of accessibility-aware development is a significant issue for vision-based intelligent systems. This is mainly due to the absence of large-scale, standardized vision benchmarks that include tasks and scenarios related to people with disabilities. This lack of representation makes it difficult to analyze factors like pose, appearance, and occlusion characteristics of diverse pedestrians. For example, it is unclear how significant occlusion from a wheelchair affects instance segmentation quality, or how to robustly recognize interaction with mobility aids like a long and narrow walking cane. To address these questions, we introduce X-World, a development environment focused on accessibility for vision-based autonomous systems. To overcome data scarcity, we use a simulation environment to generate dynamic agents with different mobility aids. This simulation allows us to create ample amounts of finely annotated, multi-modal data in a safe, cheap, and privacy-preserving manner. Our analysis reveals new challenges and opportunities for future developments brought by our benchmark and tasks. Additionally, we conduct a real-world evaluation benchmark for in-situ navigation by pedestrians with disabilities to further expand our analysis. Our contributions represent an initial step towards the widespread deployment of vision-based agents that can understand and model the interaction needs of people with disabilities.