We propose a new approach to recognize multiple actions in videos using a multi-action relation model. This model combines relational graph convolutional networks (GCNs) and video multi-modality to better understand videos with multiple actions. We utilize multi-modal GCNs that incorporate modality-specific action representation as node features, including spatiotemporal features learned by 3D convolutional neural networks (CNNs), audio, and textual embeddings. By combining these models and feature representations, we improve the accuracy of relational action predictions. Our method outperforms existing approaches on the M-MiT benchmark for multi-action recognition. We provide publicly available code for our model at https://github.com/zhenglab/multi-action-video.