We address the problem of long-tailed visual recognition by introducing a method called DiVE (Distill the Virtual Examples) that adopts a knowledge distillation approach. We demonstrate that distilling information from virtual examples, which are based on the predictions of a teacher model, is equivalent to learning label distributions under certain constraints. We prove that when the virtual example distribution is flatter than the original input distribution, it significantly improves the performance of under-represented tail classes, which is crucial for long-tailed recognition. The DiVE method allows explicit tuning of the virtual example distribution to make it flatter. Extensive experiments on three benchmark datasets, including iNaturalist, show that the proposed DiVE method outperforms state-of-the-art methods. Additional analyses and experiments confirm the interpretation of virtual examples and demonstrate the effectiveness of tailored designs in DiVE for long-tailed problems.