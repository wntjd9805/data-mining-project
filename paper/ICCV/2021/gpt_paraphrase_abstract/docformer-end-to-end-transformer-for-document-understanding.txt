We introduce DocFormer, an architecture based on the transformer model that addresses the challenging task of Visual Document Understanding (VDU). VDU involves comprehending documents in various formats and layouts, such as forms and receipts. To tackle this task, DocFormer leverages a multi-modal approach, incorporating text, vision, and spatial features. A unique aspect of DocFormer is its employment of a multi-modal self-attention layer, which combines these different modalities. Additionally, DocFormer facilitates correlation between text and visual tokens by sharing learned spatial embeddings across modalities. We pre-train DocFormer in an unsupervised manner using carefully designed tasks that promote multi-modal interaction. To evaluate its performance, we test DocFormer on four different datasets, each with robust baselines. Remarkably, DocFormer achieves state-of-the-art results on all datasets, sometimes even outperforming models four times its size in terms of parameter count.