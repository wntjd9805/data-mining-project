The task of visual grounding on 3D point clouds, which involves understanding the 3D visual world using vision and language, has various applications. Previous works have focused on using powerful detectors and comprehensive language features for this task, but there are two aspects that have not been fully explored yet. Firstly, the modeling of complex relations to generate context-aware object proposals, and secondly, leveraging proposal relations to distinguish the true target object from similar proposals. In this study, we propose a new method called 3DVG-Transformer, inspired by the transformer architecture, to address these challenges. Our method utilizes contextual clues for relation-enhanced proposal generation and cross-modal proposal disambiguation. We achieve this through the use of a coordinate-guided contextual aggregation (CCA) module in the object proposal generation stage and a multiplex attention (MA) module in the cross-modal feature fusion stage. Our experiments on two point cloud-based visual grounding datasets, ScanRefer and Nr3D/Sr3D from ReferIt3D, demonstrate that our 3DVG-Transformer significantly outperforms existing methods, especially in complex scenarios with multiple objects of the same category.