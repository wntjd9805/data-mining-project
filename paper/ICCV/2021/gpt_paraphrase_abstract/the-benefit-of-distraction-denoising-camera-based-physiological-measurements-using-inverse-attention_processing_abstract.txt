Attention networks have shown strong performance in various computer vision tasks by selectively focusing on pixels with stronger signals ("foreground"). However, in real-world scenarios, many sources of corruption, such as illumination and motion, affect both foreground and background pixels. In this paper, we propose the use of inverse attention networks that extract information about these shared sources of corruption. By doing so, we effectively suppress shared covariates and enhance signal information, leading to improved performance. We demonstrate the effectiveness of our approach in camera-based physiological measurement, where weak signals and variations in global illumination and motion are significant sources of corruption. Through experiments on three datasets, we show that our inverse attention approach achieves state-of-the-art results. It increases the signal-to-noise ratio by up to 5.8 dB, reduces errors in heart rate and breathing rate estimation by up to 30%, recovers subtle waveform dynamics, and generalizes from RGB to NIR videos without the need for retraining.