Depth estimation is an important task in computer vision, but most previous methods assume images are all-in-focus, which is not common in real-world applications. However, a few methods consider defocus blur as a cue for depth estimation. In this paper, we propose a method to estimate both depth and an all-in-focus image from a set of images with different focus positions (known as a focal stack). We use a shared architecture to exploit the relationship between depth and all-in-focus estimation. Our method can be trained with either supervised or unsupervised learning. We demonstrate through experiments that our method outperforms state-of-the-art methods in terms of both quantitative and qualitative results, while also being more efficient in inference time.