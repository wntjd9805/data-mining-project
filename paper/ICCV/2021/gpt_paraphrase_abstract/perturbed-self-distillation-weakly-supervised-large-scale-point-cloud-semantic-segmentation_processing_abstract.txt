Large-scale point cloud semantic segmentation is a valuable application, but current research mainly relies on expensive and time-consuming manual point-wise annotation. Weakly supervised learning offers an alternative by reducing annotation efforts. However, for large-scale point clouds with limited labeled points, extracting discriminative features for unlabeled points becomes challenging, and the regularization of topology between labeled and unlabeled points is often neglected, leading to inaccurate segmentation results. 

To address this issue, we introduce a perturbed self-distillation (PSD) framework. Inspired by self-supervised learning, we construct a perturbed branch that enforces predictive consistency with the original branch. This auxiliary supervision effectively establishes the graph topology of the entire point cloud, enabling information propagation between labeled and unlabeled points. Additionally, we propose a context-aware module to explicitly regulate the affinity correlation of labeled points, further refining the graph topology of the point cloud. 

Experimental results on three large-scale datasets demonstrate a significant improvement (average gain of 3.0%) compared to recent weakly supervised methods and comparable performance to some fully supervised methods.