We propose a solution to the problem of segmenting actions from weakly-annotated videos using a framework that incorporates low-dimensional subspaces and transcripts. Our approach involves modeling actions with a union of low-dimensional subspaces, learning these subspaces from transcripts, and refining video features to align with action subspaces. We introduce a Union-of-Subspaces Network, consisting of autoencoders, to capture variations of actions within and across videos. During the learning process, positive and negative soft alignment matrices are generated based on previous segmentations and used for discriminative training. We also incorporate a constraint loss to prevent imbalanced segmentations and enforce similar durations of actions across videos. To enable real-time inference, we develop a hierarchical segmentation framework that utilizes subset selection to find representative transcripts and align a test video with increasingly refined representative transcripts. Our experiments on three datasets demonstrate that our method improves action segmentation and alignment compared to the state-of-the-art, while significantly reducing the inference time.