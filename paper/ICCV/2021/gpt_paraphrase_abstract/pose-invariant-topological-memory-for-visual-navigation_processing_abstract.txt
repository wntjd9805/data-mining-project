Planning for visual navigation using topological memory has been extensively studied. However, existing methods often struggle to accurately predict reachability when the robot changes its pose. This is due to the limitations of first-person view images, which drastically change with different poses. To address this issue, we propose a solution called pose invariant topological memory (POINT). POINT utilizes omnidirectional images and employs a spherical convolutional neural network (NN) that possesses rotation invariance. This enables planning regardless of the robot's pose. Moreover, we train the NN using contrastive learning and data augmentation to enhance robustness in various environmental conditions, such as lighting changes and the presence of unfamiliar objects. Experimental results demonstrate that POINT outperforms conventional methods in both similar and different environmental conditions. Furthermore, evaluations using the KITTI-360 dataset confirm that POINT is highly applicable to real-world environments compared to traditional approaches.