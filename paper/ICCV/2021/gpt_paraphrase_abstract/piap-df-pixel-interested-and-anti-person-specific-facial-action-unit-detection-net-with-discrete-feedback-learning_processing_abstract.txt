Facial Action Units (AUs) play a crucial role in communication and automatic AU detection can enhance the understanding of psychological conditions and emotional states. Although deep learning methods have been proposed for automatic AU detection, there are still challenges to be addressed. These include inadequate extraction of detailed and robust local AU information, model overfitting on person-specific features, and the limitation of datasets with incorrect labels. In this study, we present a joint strategy called PIAP-DF to address these problems. The strategy includes three components: 1) a multi-stage Pixel-Interested learning method with pixel-level attention for each AU, 2) an Anti Person-Specific method to minimize the influence of individual-specific features, and 3) a semi-supervised learning method with Discrete Feedback to effectively utilize unlabeled data and mitigate the negative impact of incorrect labels. Experimental results on two popular AU detection datasets, BP4D and DISFA, demonstrate that PIAP-DF outperforms existing methods and achieves state-of-the-art performance. Compared to the current best method, PIAP-DF improves the average F1 score by 3.2% on BP4D and by 0.5% on DISFA. Furthermore, all modules of PIAP-DF can be easily removed after training, allowing for the development of a lightweight model suitable for practical application.