Multimodal self-supervised learning is gaining popularity for its ability to train large networks without human supervision and to search and retrieve data across different modalities. This study proposes a framework that builds upon a pre-trained backbone to learn a shared multimodal embedding space. This space not only allows for representation sharing across modalities but also enforces grouping of semantically similar instances. The approach incorporates instance-level contrastive learning and adds a multimodal clustering step to capture semantic similarities across modalities during training. The resulting embedding space enables retrieval of samples across all modalities, even from unseen datasets and different domains. The effectiveness of the proposed approach is demonstrated by training the model on the HowTo100M dataset and evaluating its zero-shot retrieval capabilities in text-to-video retrieval and temporal action localization. The results obtained on four different datasets show state-of-the-art performance.