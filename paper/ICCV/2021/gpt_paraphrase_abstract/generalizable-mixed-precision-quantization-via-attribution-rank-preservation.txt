This paper introduces a method called generalizable mixed-precision quantization (GMPQ) for efficient inference. Existing methods for bitwidth search and model deployment require consistent datasets, resulting in high search costs for large-scale datasets in realistic applications. In contrast, GMPQ searches for a mixed-quantization policy that can be applied to large-scale datasets using only a small amount of data, reducing search costs without sacrificing performance. The authors observe that accurately locating network attribution is a general ability for visual analysis across different data distributions. Therefore, they aim to maintain attribution rank consistency between quantized models and their full-precision counterparts through efficient capacity-aware attribution imitation. Extensive experiments demonstrate that GMPQ achieves a competitive trade-off between accuracy and complexity compared to state-of-the-art mixed-precision networks, while significantly reducing search costs. The code for GMPQ is available at http-s://github.com/ZiweiWangTHU/GMPQ.git.