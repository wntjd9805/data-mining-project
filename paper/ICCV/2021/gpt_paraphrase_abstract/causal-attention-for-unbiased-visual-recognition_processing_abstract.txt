The use of attention modules in deep models does not always lead to the learning of causal features that are robust in different contexts. This is because confounding factors can deceive the attention mechanism into capturing spurious correlations that are beneficial for predictions when the training and testing data have identical and independent distributions (IID). However, these correlations can harm predictions when the data is out-of-distribution (OOD). The only fundamental solution to learning causal attention is through causal intervention, which requires additional annotations of the confounding factors. However, this approach is expensive and problematic due to the elusive nature of the confounders. In this paper, we propose a causal attention module (CaaM) that can self-annotate the confounders in an unsupervised manner. The CaaM can be stacked and integrated into conventional attention CNNs and self-attention Vision Transformers. In OOD settings, deep models equipped with CaaM outperform those without it significantly. Even in IID settings, CaaM improves attention localization, demonstrating its potential for applications that require robust visual saliency. The code for CaaM is available at https://github.com/Wangt-CN/CaaM.