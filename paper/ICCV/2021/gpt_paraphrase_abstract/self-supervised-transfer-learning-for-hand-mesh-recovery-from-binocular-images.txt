We propose a self-supervised framework for estimating hand mesh from RGB images without the need for ground truth or landmark annotations. Our approach leverages pre-learned hand priors from existing datasets and applies left-right consistency constraints to train the model. By using binocular images and relying on appearance consensus and shape consistency, our model achieves comparable performance to state-of-the-art methods even without landmark annotations. We further validate our model's effectiveness by collecting a large real binocular dataset and obtaining positive experimental results.