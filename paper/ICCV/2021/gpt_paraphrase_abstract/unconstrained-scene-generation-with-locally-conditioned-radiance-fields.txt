This abstract introduces Generative Scene Networks (GSN), a model that learns to decompose complex indoor scenes into multiple local radiance fields that can be rendered from different camera viewpoints. The model can generate new scenes or complete scenes based on sparse 2D observations. Previous generative models of radiance fields were limited to single objects like cars or faces and lacked the capacity to capture realistic indoor scenes. GSN overcomes this limitation by scaling to larger and more complex scenes while preserving details and diversity. The learned prior enables high-quality rendering from different viewpoints. When compared to existing models, GSN produces quantitatively superior scene renderings.