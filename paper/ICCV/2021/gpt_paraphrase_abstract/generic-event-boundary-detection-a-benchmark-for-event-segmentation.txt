This paper introduces a new task and benchmark for detecting event boundaries in videos without relying on predefined categories. Previous work in video segmentation and action detection has focused on localizing specific action categories, which limits their applicability to generic videos. However, research in cognitive science has shown that humans naturally segment videos into meaningful chunks without predefined categories. In this study, we replicate these cognitive experiments using mainstream computer vision datasets and propose a novel annotation guideline to address the challenges of taxonomy-free event boundary annotation. We define the task as Generic Event Boundary Detection (GEBD) and introduce a new benchmark called Kinetics-GEBD. We believe that GEBD is crucial for understanding videos as a whole, but it has been overlooked due to a lack of proper task definition and annotations. Through experiments and human studies, we demonstrate the value of our annotations. Additionally, we evaluate supervised and unsupervised GEBD approaches on the TAPOS dataset and our Kinetics-GEBD benchmark. The annotations and baseline codes are made publicly available at the CVPR'21 LOVEU Challenge website.