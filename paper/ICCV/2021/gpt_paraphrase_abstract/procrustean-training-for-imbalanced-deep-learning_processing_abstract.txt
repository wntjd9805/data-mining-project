Neural networks trained on imbalanced datasets often struggle with accurately classifying the minor classes due to overfitting. This paper presents a new explanation for this issue. The authors discovered that neural networks initially underfit the minor classes by misclassifying their data as belonging to the major classes during early training epochs. To correct these errors, the neural network focuses on pushing the features of the minor class data across the decision boundaries between major and minor classes, resulting in larger gradients for the minor class features. This underfitting phase excessively emphasizes the competition between major and minor classes, preventing the neural network from learning discriminative knowledge that can be generalized to test data and leading to overfitting. 

To address this problem, the authors propose a novel learning strategy that equalizes the training progress across classes. They achieve this by combining features of the major class data with other data in a mini-batch, intentionally weakening their features to prevent the neural network from fitting them first. This approach effectively balances the training accuracy and feature gradients across classes, successfully mitigating the underfitting and overfitting issues for the minor class data. The proposed strategy achieves state-of-the-art accuracy on benchmark datasets, particularly for the challenging cases where class imbalances are significant.