This study focuses on accurately describing and detecting key-points in both 2D images and 3D point clouds. While there have been many proposed learning-based methods for 2D and 3D local feature descriptors and detectors, the development of a shared descriptor and joint keypoint detector that directly matches pixels and points has been overlooked by the research community. To address this gap, the authors propose a dual fully-convolutional framework that maps 2D and 3D inputs into a shared latent representation space, enabling the simultaneous description and detection of keypoints. Additionally, they introduce an ultra-wide reception mechanism and a novel loss function to account for the variations in information between pixel and point local regions. The experimental results demonstrate that their framework achieves competitive performance in fine-grained matching between images and point clouds and outperforms existing methods in indoor visual localization. The source code for their work is available at https://github.com/BingCS/P2-Net.