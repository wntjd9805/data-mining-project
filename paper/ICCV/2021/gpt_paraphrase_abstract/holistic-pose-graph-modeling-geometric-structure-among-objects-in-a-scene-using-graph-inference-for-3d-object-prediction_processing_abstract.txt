Detecting 3D objects from a single RGB image is challenging due to the lack of depth cues. Current methods either predict the 3D pose for each object independently or consider local relationships within limited surroundings, neglecting the global geometric relationships. To address this limitation, we propose the Holistic Pose Graph (HPG) that models the geometric structure among objects in a scene. The HPG integrates object poses as nodes and relative poses as edges. We use a GRU to encode pose features from corresponding image regions and iteratively pass messages along the graph structure to improve pose predictions. To improve the correspondence between object and relative poses, we introduce a consistency loss to measure deviations between them. Finally, we apply Holistic Pose Estimation (HPE) to evaluate both independent object poses and relative poses. Experimental results on the SUN RGB-D dataset demonstrate a significant improvement in 3D object prediction using our proposed method.