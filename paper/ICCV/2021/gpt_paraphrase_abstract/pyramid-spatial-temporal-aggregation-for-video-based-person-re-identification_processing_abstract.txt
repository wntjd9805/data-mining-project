This paper introduces the Pyramid Spatial-Temporal Aggregation (PSTA) framework for video-based person re-identification, which aims to match video clips of the same person across different cameras. The framework utilizes spatial-temporal representations to capture richer information between frames, which is important for distinguishing individuals when occlusion occurs. The PSTA framework progressively aggregates frame-level features and fuses hierarchical temporal features to generate a final video-level representation. This allows short-term and long-term temporal information to be effectively utilized at different levels. Additionally, the authors propose a Spatial-Temporal Aggregation Module (STAM) to enhance the aggregation capability of PSTA. STAM incorporates two attention blocks, namely Spatial Reference Attention (SRA) and Temporal Reference Attention (TRA). SRA determines the attention weights of each location within a frame by exploring spatial correlations, while TRA extends SRA by considering correlations between adjacent frames to fully exploit temporal consistency information and enhance discriminative features. The effectiveness of the proposed PSTA framework is demonstrated through extensive experiments on challenging benchmarks, achieving high Rank-1 accuracies of 91.5% and 98.3% on the MARS and DukeMTMC-VID benchmarks, respectively. The source code for the proposed method is available at the provided GitHub link.