Current state-of-the-art methods for human pose estimation require a significant amount of computational resources to achieve accurate predictions. One potential solution to this problem is knowledge distillation, which involves transferring pose knowledge from a powerful teacher model to a less complex student model. However, existing pose distillation methods rely on a pre-trained estimator and involve a complex two-stage learning process. In this study, we propose a new framework called Online Knowledge Distillation for Human Pose (OKDHP). This framework distills human pose structure knowledge in a single stage to ensure efficient distillation. OKDHP trains a single multi-branch network and obtains predicted heatmaps from each branch. These heatmaps are then combined using a Feature Aggregation Unit (FAU) to create target heatmaps for teaching each branch in reverse. Instead of simply averaging the heatmaps, FAU leverages multi-scale information through parallel transformations with different receptive fields to generate higher-quality target heatmaps. The pixel-wise Kullback-Leibler (KL) divergence is used to minimize the discrepancy between the target heatmaps and the predicted ones, allowing the student network to learn the implicit keypoint relationship. Additionally, an unbalanced OKDHP scheme is introduced to customize the compression rates of the student networks. We evaluate the effectiveness of our approach through extensive experiments on the MPII and COCO benchmark datasets.