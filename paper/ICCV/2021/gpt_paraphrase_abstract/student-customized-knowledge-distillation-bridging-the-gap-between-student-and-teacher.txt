Knowledge distillation (KD) is a process of transferring knowledge from complex networks (teachers) to simpler networks (students) in order to improve the performance of the students. However, it has been observed that having a better teacher does not necessarily result in better performance for the student due to a capacity mismatch. In order to address this issue, we propose an innovative method called Student Customized Knowledge Distillation (SCKD). This method focuses on the gradient similarity between the teacher and the student as a measure of capacity mismatch. We approach knowledge distillation as a multi-task learning problem, where the teacher only transfers knowledge to the student if it can benefit from it. We validate our approach on multiple datasets using different configurations of teacher-student networks in tasks such as image classification, object detection, and semantic segmentation.