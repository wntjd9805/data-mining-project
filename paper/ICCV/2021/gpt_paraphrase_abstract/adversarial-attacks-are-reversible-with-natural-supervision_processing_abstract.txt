Images have an inherent structure that can be utilized to reverse adversarial attacks. These attacks not only disrupt image classifiers but also unintentionally disrupt other structural elements within the image. By modifying the attacked image to restore its natural structure, we can effectively reverse various types of attacks and provide a defense mechanism. Our experiments demonstrate a significant improvement in the robustness of state-of-the-art models across multiple datasets. Importantly, our defense remains effective even when the attacker is aware of the defense mechanism. Unlike other defenses, our approach is compatible with pre-trained networks and can be deployed during inference. Our findings suggest that deep networks are susceptible to adversarial examples because they do not enforce the natural structure of images.