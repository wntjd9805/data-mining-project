We present a method for real-time rendering of Neural Radiance Fields (NeRFs) using PlenOctrees, a 3D representation based on octrees that supports view-dependent effects. With our approach, we achieve a rendering speed of over 150 frames per second for 800Ã—800 images, which is more than 3000 times faster than traditional NeRFs. This high-speed rendering is achieved without compromising the quality or the ability of NeRFs to render scenes with arbitrary geometry and view-dependent effects from any viewpoint. We achieve real-time performance by pre-tabulating the NeRF into a PlenOctree. To preserve view-dependent effects like specularities, we use closed-form spherical basis functions to factorize the appearance. This allows us to train NeRFs to predict a spherical harmonic representation of radiance, eliminating the need for the viewing direction as an input to the neural network. Additionally, we demonstrate that PlenOctrees can be optimized to further minimize reconstruction loss, resulting in quality on par with or better than other methods. This optimization step also reduces training time as it eliminates the need for the NeRF training to fully converge. Our real-time neural rendering approach opens up new possibilities for applications such as industrial and product visualizations in 6 degrees of freedom (6-DOF), as well as next-generation augmented reality (AR) and virtual reality (VR) systems. PlenOctrees are also suitable for in-browser rendering, and an interactive online demo, along with videos and code, can be found on our project page at https://alexyu.net/plenoctrees.