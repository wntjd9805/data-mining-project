The safety of deep neural networks has been compromised by data poisoning attacks that can result in neural backdoors, causing misclassification of certain inputs. One specific type of attack, known as the sample-targeted backdoor attack, poses a new challenge. Existing backdoor detection schemes fail to identify this type of attack as they rely on reverse-engineering triggers or strong trigger features. In this research, we propose a novel approach to detect and mitigate sample-targeted backdoor attacks. We discover a unique characteristic of these attacks, which leads to a change in the decision boundary, forming small "pockets" around the target samples. Based on this observation, we introduce a defense mechanism that identifies and encloses these malicious pockets within a tight convex hull in the feature space. We develop an effective algorithm to search for such convex hulls and remove the backdoor by fine-tuning the model using the identified malicious samples with corrected labels according to the convex hull. Experimental results demonstrate that our proposed approach is highly efficient in detecting and mitigating a wide range of sample-targeted backdoor attacks.