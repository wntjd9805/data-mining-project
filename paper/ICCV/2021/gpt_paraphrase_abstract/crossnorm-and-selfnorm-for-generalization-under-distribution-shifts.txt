Traditional normalization techniques such as Batch Normalization and Instance Normalization assume that the training and test data have the same distribution. However, in real-world applications, distribution shifts occur, causing well-trained models with these normalization methods to perform poorly in new environments. To address this issue, we propose two new normalization methods called CrossNorm and SelfNorm.  CrossNorm enhances generalization robustness by exchanging channel-wise mean and variance between feature maps, enlarging the training distribution. On the other hand, SelfNorm uses attention mechanisms to recalibrate the statistics and bridge the gaps between the training and test distributions. Although CrossNorm and SelfNorm utilize different statistical approaches, they can complement each other.  We conducted extensive experiments in various fields such as vision and language, and tasks including classification and segmentation. We also examined different settings such as supervised and semi-supervised learning, as well as different types of distribution shifts, both synthetic and natural. The results demonstrate the effectiveness of CrossNorm and SelfNorm.  The code for implementing these methods is available at https://github.com/amazon-research/crossnorm-selfnorm.