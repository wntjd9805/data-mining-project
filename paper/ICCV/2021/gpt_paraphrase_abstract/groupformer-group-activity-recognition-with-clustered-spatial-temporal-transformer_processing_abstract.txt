Group activity recognition is a challenging task that requires understanding the spatial-temporal interactions among individuals and generating accurate group representations. Existing methods either focus on spatial or temporal information separately or simply aggregate individual features to form group features. To overcome these limitations, we propose a new network called GroupFormer. This network effectively captures spatial-temporal contextual information by using a clustered spatial-temporal transformer, which enhances both individual and group representations. The GroupFormer network has three main advantages: (1) It introduces a tailor-modified transformer, Clustered Spatial-Temporal Transformer, to improve the individual and group representations. (2) It integrates the modeling of spatial and temporal dependencies and employs decoders to connect these two types of information. (3) It utilizes a clustered attention mechanism to dynamically divide individuals into multiple clusters, enabling better learning of activity-aware semantic representations. Experimental results demonstrate that our proposed framework outperforms state-of-the-art methods on the Volleyball dataset and Collective Activity dataset. The code for our framework is available at https://github.com/xueyee/GroupFormer.