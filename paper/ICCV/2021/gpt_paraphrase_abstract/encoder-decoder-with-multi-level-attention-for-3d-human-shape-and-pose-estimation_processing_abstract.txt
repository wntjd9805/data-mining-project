The task of estimating 3D human shape and pose is crucial for analyzing human motion in various 3D applications. However, current methods lack the ability to capture relations at multiple levels, such as spatial-temporal and joint levels. As a result, they struggle to accurately predict in challenging scenarios involving cluttered backgrounds, occlusion, or extreme poses. To address this issue, we propose a Multi-level Attention Encoder-Decoder Network (MAED) that includes a Spatial-Temporal Encoder (STE) and a Kinematic Topology Decoder (KTD) to model multi-level attentions within a unified framework. The STE comprises cascaded blocks based on Multi-Head Self-Attention, with each block utilizing parallel branches to learn spatial and temporal attention. On the other hand, the KTD focuses on modeling joint-level attention, treating pose estimation as a hierarchical process akin to the SMPL kinematic tree. Through training on the 3DPW dataset, MAED outperforms previous state-of-the-art methods by achieving improvements of 6.2, 7.2, and 2.4 mm in PA-MPJPE on the widely-used benchmarks 3DPW, MPI-INF-3DHP, and Human3.6M, respectively. Our code is publicly available at https://github.com/ziniuwan/maed.