We present the Trans-FER model, a novel approach for facial expression recognition (FER) in computer vision. The model incorporates three components: Multi-Attention Dropping (MAD), ViT-FER, and Multi-head Self-Attention Dropping (MSAD). MAD addresses the challenge of locating discriminative and diverse local patches by randomly dropping attention maps, encouraging the exploration of different patches. ViT-FER utilizes Vision Transformers (ViT) to establish rich relations between local patches, enhancing FER performance. MSAD tackles the issue of similar relations being extracted by multiple self-attentions by randomly dropping one self-attention module, promoting the learning of diverse relations among local patches. Our TransFER model surpasses existing methods on FER benchmarks, demonstrating its effectiveness. The first two authors contributed equally, and this work was conducted during their internship at IDL, Baidu Research.