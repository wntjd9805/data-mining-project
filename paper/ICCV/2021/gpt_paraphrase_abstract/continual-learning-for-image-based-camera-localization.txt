The use of deep neural networks to directly predict camera pose and 3D scene coordinates from input images has shown promise for technologies like augmented reality, autonomous driving, and robotics. However, these methods typically assume a stationary data distribution with all scenes available during training. This paper addresses the problem of visual localization in a continual learning setup, where the model is trained incrementally on scenes. The study finds that non-stationary data causes deep networks to suffer from catastrophic forgetting, similar to the classification domain. To tackle this issue, the authors propose a strong baseline approach that stores and replays images from a fixed buffer. Additionally, they introduce a new sampling method called Buff-CS, which adapts existing strategies in the buffering process specifically for visual localization. Experimental results on challenging datasets (7Scenes, 12Scenes, and a combination of 19Scenes) demonstrate consistent improvements over standard buffering methods.