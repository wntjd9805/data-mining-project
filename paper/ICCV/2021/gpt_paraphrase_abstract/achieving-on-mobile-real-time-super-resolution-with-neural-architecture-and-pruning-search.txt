In recent years, there has been significant progress in the field of single image super-resolution (SISR) using deep neural networks (DNNs). However, the practical implementation of these deep learning methods faces challenges related to computation and memory consumption, especially on resource-limited platforms like mobile devices. To address this issue and enable real-time deployment of SISR tasks on mobile, we propose a novel approach that combines neural architecture search with pruning search. This automatic search framework allows us to derive sparse super-resolution (SR) models that offer high-quality image results while meeting the real-time inference requirement.   To reduce the computational cost of the search process, we employ a weight sharing strategy by introducing a supernet. This helps us divide the search problem into three stages: supernet construction, compiler-aware architecture, and pruning search, as well as compiler-aware pruning ratio search. By utilizing this framework, we have achieved real-time SR inference on mobile platforms (specifically, the Samsung Galaxy S20) with competitive image quality (as measured by PSNR and SSIM). Our method enables the implementation of 720p resolution with an inference time of only tens of milliseconds per frame.