Significant advancements have been made in the field of visual representation learning, particularly with the success of self-supervised contrastive learning methods. It has been demonstrated that supervised contrastive learning surpasses cross-entropy methods by utilizing labels to determine contrasting points. However, little research has been conducted to investigate the transferability of contrastive learning to different domains.  In this study, we extensively explore the transfer capabilities of various contrastive approaches for linear evaluation, full-network transfer, and few-shot recognition on 12 different downstream datasets from various domains. Additionally, we evaluate the performance of these approaches on object detection tasks using the MSCOCO and VOC0712 datasets. Our findings indicate that the learned representations from contrastive approaches can easily transfer to different downstream tasks.  Furthermore, we observe that combining the self-supervised contrastive loss with cross-entropy or supervised contrastive loss leads to improved transferability compared to solely using supervised counterparts. Our analysis reveals that the representations acquired through contrastive approaches contain a greater amount of low and mid-level semantics compared to cross-entropy models. This characteristic enables them to quickly adapt to new tasks.  To facilitate future research on the transferability of visual representations, we will make our codes and models publicly accessible.