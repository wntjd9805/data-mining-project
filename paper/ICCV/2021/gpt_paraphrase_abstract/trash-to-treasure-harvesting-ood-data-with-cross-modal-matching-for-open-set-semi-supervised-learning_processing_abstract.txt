We introduce a new approach to open-set semi-supervised learning (open-set SSL), which addresses the challenge of handling out-of-distribution (OOD) samples in unlabeled data. Unlike existing techniques that aim to filter out OOD samples completely, our method leverages the presence of OOD data to improve feature learning without negatively impacting SSL. We achieve this by implementing a warm-up training phase that utilizes both in-distribution (ID) and OOD samples. During this phase, a pretext task is performed to train the feature extractor to gain a high-level semantic understanding of the training images, resulting in more discriminative features that benefit downstream tasks. To mitigate the negative impact of OOD samples on SSL, we propose a novel cross-modal matching strategy to detect them. Rather than employing binary classification, we train the network to predict whether a data sample is matched to an assigned one-hot class label. This cross-modal matching approach aligns with the core classification task and generates a compatible feature space. Extensive experiments demonstrate that our approach significantly improves performance in open-set SSL, surpassing the state-of-the-art by a large margin.