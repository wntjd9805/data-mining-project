Model inversion attacks, known as MI attacks, aim to reconstruct training data from model parameters. These attacks have raised concerns about privacy, particularly with the proliferation of online model repositories. However, current MI attacks on deep neural networks (DNNs) have significant room for improvement. We introduce a new type of GAN that is specifically designed for inversion attacks, enabling better extraction of knowledge from public data for attacking private models. Unlike previous approaches that seek a single data point to represent a target class, we propose modeling a private data distribution for each target class. Our approach involves training the discriminator to differentiate between real and fake samples, as well as soft-labels provided by the target model. Through experiments, we demonstrate that these techniques can considerably enhance the success rate of state-of-the-art MI attacks by 150% and exhibit better generalization across various datasets and models. The code for our approach is available at https://github.com/SCccc21/Knowledge-Enriched-DMI.