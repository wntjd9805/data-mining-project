This study addresses the issue of universal cross-domain retrieval, where the test data can come from classes or domains that were not seen during training. The increasing number of categories and the impracticality of training on every possible domain make it necessary to generalize to unseen classes and domains. To achieve this, the authors propose a model called SnMpNet (Semantic Neighbourhood and Mixture Prediction Network), which includes two new losses to handle unseen classes and domains during testing. The first loss, called Semantic Neighbourhood, aims to bridge the knowledge gap between seen and unseen classes and ensure that the latent space embedding of unseen classes is semantically meaningful in relation to neighboring classes. The second loss, called Mixture Prediction, introduces mix-up based supervision at the image-level and semantic-level of the data for training, which improves retrieval efficiency for queries belonging to unseen domains. These losses are incorporated into the SE-ResNet50 backbone to create SnMpNet. The proposed model is extensively evaluated on two large-scale datasets, Sketchy Extended and DomainNet, and compared with state-of-the-art models, demonstrating its effectiveness.