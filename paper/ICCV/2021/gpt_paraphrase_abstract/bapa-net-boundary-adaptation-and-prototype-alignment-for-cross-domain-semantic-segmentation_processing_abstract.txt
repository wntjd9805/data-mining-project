Current cross-domain semantic segmentation methods typically focus on the overall segmentation results of entire objects but do not consider the importance of object boundaries. In this study, we have discovered that the segmentation performance can be significantly improved if object boundaries are treated appropriately. To address this, we propose a new method called BAPA-Net, which utilizes a convolutional neural network with Boundary Adaptation and Prototype Alignment under the unsupervised domain adaptation setting. Firstly, we generate additional images by inserting objects from source images into target images. We then introduce a boundary adaptation module that assigns weights to each pixel based on its proximity to the nearest boundary pixel of the inserted source objects. Additionally, we propose a prototype alignment module that reduces domain mismatch by minimizing the distances between class prototypes of the source and target domains. To avoid domain confusion during prototype calculation, boundaries are removed. By combining the boundary adaptation and prototype alignment modules, we are able to train a discriminative and domain-invariant model for cross-domain semantic segmentation.We extensively evaluate our approach on benchmark datasets of urban scenes, specifically GTA5→Cityscapes and SYNTHIA→Cityscapes. The results demonstrate the effectiveness of our BAPA-Net method compared to existing state-of-the-art approaches for cross-domain semantic segmentation. Our implementation of the method can be found at https://github.com/manmanjun/BAPA-Net.