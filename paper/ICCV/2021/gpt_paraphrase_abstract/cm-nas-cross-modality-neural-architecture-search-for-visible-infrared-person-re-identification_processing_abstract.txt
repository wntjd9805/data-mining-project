Visible-Infrared person re-identification (VI-ReID) is a task that involves matching pedestrian images from different modalities, specifically visible and infrared, to address the limitations of single-modality person re-identification in low-light environments. Existing approaches manually design two-stream architectures to learn modality-specific and modality-shareable representations, but this process requires extensive experimentation and empirical practice, which is time-consuming and labor-intensive. This paper systematically investigates these manually designed architectures and discovers that appropriately separating Batch Normalization (BN) layers is crucial for achieving significant improvements in cross-modality matching. To address this, we propose a novel method called Cross-Modality Neural Architecture Search (CM-NAS). CM-NAS uses a BN-oriented search space to optimize the separation of BN layers for the cross-modality task. With the searched architecture, our method surpasses state-of-the-art approaches in two benchmarks, SYSU-MM01 and RegDB, by improving the Rank-1/mAP performance by 6.70%/6.13% and 12.17%/11.23%, respectively. The code for our method is available at https://github.com/JDAI-CV/CM-NAS.