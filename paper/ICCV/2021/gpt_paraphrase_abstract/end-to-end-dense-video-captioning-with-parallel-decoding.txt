Dense video captioning aims to generate multiple captions for different moments in a video. Existing methods use a complex approach that involves multiple manual steps. In this paper, we propose a simple yet effective framework called parallel decoding (PDVC) for end-to-end dense video captioning. PDVC treats caption generation as a set prediction task and uses a transformer decoder with an event counter to segment the video into event pieces. This approach improves the coherence and readability of the captions. PDVC has several advantages over previous methods: it directly produces a set of events without the need for heuristics or redundancy removal, it performs localization and caption generation in parallel, making them mutually beneficial, and it achieves high-quality results without additional complexities. Our experiments on ActivityNet Captions and YouCook2 datasets demonstrate that PDVC outperforms state-of-the-art methods when localization accuracy is comparable. The code for PDVC is available at https://github.com/ttengwang/PDVC.