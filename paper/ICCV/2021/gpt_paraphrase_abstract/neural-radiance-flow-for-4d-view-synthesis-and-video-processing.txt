We introduce Neural Radiance Flow (NeRFlow), a technique for learning a 4D spatial-temporal representation of a dynamic scene using RGB images. Our method utilizes a neural implicit representation that captures the 3D occupancy, radiance, and dynamics of the scene. By ensuring consistency across different modalities, our representation allows for multi-view rendering in various dynamic scenes, surpassing current state-of-the-art methods for spatial-temporal view synthesis. Notably, our approach performs well even when given only a single monocular real video. Additionally, we demonstrate that the learned representation can serve as an implicit scene prior, enabling video processing tasks like image super-resolution and de-noising without the need for additional supervision.