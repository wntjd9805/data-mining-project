State-of-the-art few-shot learners typically focus on developing effective training procedures for feature representations and using simple classifiers. However, we propose an approach that is independent of the features used and instead focuses on meta-learning the final classifier layer. This approach, called MetaQDA, is a Bayesian meta-learning extension of the classic quadratic discriminant analysis. It offers several advantages for practitioners, including fast and memory-efficient meta-learning without the need to fine-tune features. It is also agnostic to the choice of off-the-shelf features, allowing it to benefit from future advancements in feature representations. Empirically, MetaQDA demonstrates excellent performance in cross-domain few-shot learning and class-incremental few-shot learning. Additionally, its Bayesian formulation ensures state-of-the-art uncertainty calibration in predictions, making it highly suitable for real-world applications.