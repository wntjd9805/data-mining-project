This paper introduces a new model called InstanceRefer1, which aims to improve 3D object localization on point clouds using natural language guidance. Unlike 2D image grounding, this task is more challenging. The proposed model utilizes a grounding-by-matching strategy, where it first predicts the target category from language descriptions using a simple language classification model. Based on the category, the model filters out a small number of instance candidates from the panoptic segmentation on point clouds. By doing so, the complex 3D visual grounding is transformed into a simplified instance-matching problem, as instance-level candidates are more reasonable than redundant 3D object proposals. The model then performs multi-level contextual inference for each candidate, including instance attribute perception, instance-to-instance relation perception, and instance-to-background global localization perception. The most relevant candidate is selected and localized by ranking confidence scores obtained through cooperative holistic visual-language feature matching. Experimental results demonstrate that the proposed method surpasses previous state-of-the-art approaches on the ScanRefer online benchmark and Nr3D/Sr3D datasets.