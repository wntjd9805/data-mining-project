Surface reconstruction from point clouds is a crucial problem in the computer vision and graphics community. Current approaches address this problem by individually optimizing local implicit fields. However, they require accurate normals to avoid sign conflicts in overlapping regions, making them unsuitable for raw scans where surface normals may not be available. Although SAL overcomes this limitation through sign-agnostic learning, there is a need for further research on extending this technique for local shape modeling. In this study, we propose a method to learn implicit surface reconstruction through sign-agnostic optimization of convolutional occupancy networks. Our approach offers advanced scalability, generality, and applicability to raw scans within a unified framework. We achieve this by optimizing pre-trained occupancy prediction networks using an unsigned cross-entropy loss during inference. The learning of occupancy fields is conditioned on convolutional features from an hourglass network architecture. Extensive experiments on object-level and scene-level datasets demonstrate that our approach outperforms previous state-of-the-art methods in accurately reconstructing surfaces from unoriented point clouds. The code for our approach is publicly available at the provided GitHub link.