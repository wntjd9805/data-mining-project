Convolutional neural networks (CNNs) have been successful in computer vision tasks, but they struggle to model long-range dependencies due to the local nature of the convolution operation. Transformers, originally designed for natural language processing, offer a solution with their global self-attention mechanisms. In this study, we introduce TransDepth, an architecture that combines CNNs and transformers. To address the issue of losing local details when adopting transformers, we propose a novel decoder using attention mechanisms with gates. Our paper is the first to apply transformers to pixel-wise prediction problems involving continuous labels, such as monocular depth prediction and surface normal estimation. Extensive experiments demonstrate that TransDepth achieves state-of-the-art performance on three challenging datasets. The code for our model is available at: https://github.com/ygjwd12345/TransDepth.