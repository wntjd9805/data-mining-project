We introduce a novel method called Region Similarity Representation Learning (ReSim) for self-supervised representation learning in localization-based tasks like object detection and segmentation. While previous research has primarily focused on learning global representations for entire images, ReSim simultaneously learns regional representations for localization and semantic image-level representations. ReSim works by sliding a fixed-sized window across the overlapping area between two views, aligning these areas with their corresponding convolutional feature map regions, and maximizing feature similarity across views. This enables ReSim to learn consistent feature representations that are spatially and semantically coherent throughout the convolutional feature maps of a neural network. Any shift or scale in an image region, such as an object, results in a corresponding change in the feature maps, making these representations useful for localization tasks. Through experiments in object detection, instance segmentation, and dense pose estimation, we demonstrate that ReSim significantly enhances localization and classification performance compared to a competitive MoCo-v2 baseline: +2.7 APbb 75 VOC, +1.1 APbb 75 COCO, and +1.9 APmk Cityscapes. The code and pre-trained models are available at: https://github.com/Tete-Xiao/ReSim.