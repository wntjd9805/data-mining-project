Generalization beyond the training distribution is a major challenge in machine learning. The conventional method of mixing and shuffling examples during neural network training may not be ideal for achieving this. We propose a new approach where the data is divided into carefully chosen subsets that are treated as separate training environments. This enables the learning of models that have improved generalization capabilities beyond the distribution they were trained on. Our training procedure focuses on capturing patterns that are consistent across different environments while discarding irrelevant ones. This goes beyond the traditional correlation-based learning by incorporating task-specific information that cannot be obtained from the joint distribution of the training data alone.We demonstrate the effectiveness of our approach in the context of visual question answering, a task known for its dataset biases. By constructing environments based on prior knowledge, existing metadata, or unsupervised clustering, we achieve significant improvements on the VQA-CP dataset. Additionally, we achieve improvements on GQA by utilizing annotations of "equivalent questions" and on multi-dataset training (VQA v2 / Visual Genome) by treating them as distinct environments.In summary, our method enhances the generalization capabilities of machine learning models by partitioning the data into well-chosen subsets and treating them as separate training environments. This approach allows us to capture relevant patterns while disregarding irrelevant ones, leading to improved performance on various tasks.