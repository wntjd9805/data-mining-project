We introduce BN-NAS, a method for speeding up neural architecture search (NAS) called neural architecture search with Batch Normalization (BN-NAS). BN-NAS reduces the time required for model training and evaluation in NAS. To expedite evaluation, we propose a BN-based indicator that predicts subnet performance early in the training process. This indicator allows us to enhance training efficiency by only training the BN parameters during supernet training. We have observed that training the entire supernet is unnecessary, as training only BN parameters accelerates network convergence for architecture search. Extensive experiments demonstrate that our method significantly reduces the training time of the supernet by over 10 times and the evaluation time of subnets by over 600,000 times, without sacrificing accuracy. The source code can be found at https://github.com/bychen515/BNNAS.