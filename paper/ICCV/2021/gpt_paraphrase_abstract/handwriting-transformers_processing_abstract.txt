We present a new method called HWT (Handwriting Transformer) for generating styled handwritten text images. HWT aims to learn the interplay between style and content, as well as both global and local style patterns. Our approach utilizes a self-attention mechanism to capture the relationships between characters in the style examples, allowing for the encoding of both global and local style patterns. Additionally, HWT incorporates an encoder-decoder attention that combines the style features of each individual character, enabling style-content entanglement. To the best of our knowledge, this is the first time a transformer-based network has been used for styled handwritten text generation. 

Our proposed HWT produces realistic styled handwritten text images and surpasses the performance of existing methods, as evidenced by comprehensive qualitative, quantitative, and human-based evaluations. One of the strengths of HWT is its ability to handle texts of arbitrary length and accommodate any desired writing style in a few-shot learning setting. Furthermore, HWT demonstrates strong generalization capabilities in challenging scenarios where both the words and writing style are unseen during training, resulting in the generation of realistic styled handwritten text images. The code for our HWT implementation is publicly available at the following GitHub repository: https://github.com/ankanbhunia/Handwriting-Transformers.