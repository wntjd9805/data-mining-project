We propose a novel method for 3D reconstruction using Convolutional Neural Networks (ConvNets) and point clouds. Our approach involves the use of generalized convolutional kernels that can be applied to adaptive grids generated with octrees. Unlike standard kernels, our kernels have both a distinct relative location and a relative scale level, allowing them to span multiple resolutions. This enables us to effectively apply ConvNets to adaptive grids, even for large problem sizes with sparse input data that require processing the entire domain. Our ConvNet architecture can accurately predict the signed and unsigned distance fields for large datasets with millions of input points, outperforming traditional energy minimization techniques and recent learning methods in terms of speed and accuracy. We validate our approach through a zero-shot setting, where we train only on synthetic data and evaluate on the Tanks and Temples dataset consisting of real-world, large-scale 3D scenes.