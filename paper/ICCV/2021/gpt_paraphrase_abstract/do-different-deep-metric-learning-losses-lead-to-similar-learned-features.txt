Recent research has found that many deep metric learning loss functions show similar performance under the same experimental conditions. This unexpected finding suggests that these losses may allow the network to focus on similar image regions or properties. In this study, we aim to explore this phenomenon by analyzing and comparing the learned visual features of a model trained with different loss functions.To achieve this, we employ a two-step analysis approach. Firstly, we examine the learned features at the pixel level by correlating the saliency maps of the same input images. This allows us to identify and compare the regions of importance for creating the embedding. Secondly, we investigate the clustering of embeddings based on various image properties, such as object color or illumination. To control these properties independently, we generate photo-realistic 3D car renders similar to the images in the Cars196 dataset.In our analysis, we evaluate 14 pretrained models from a recent study and discover that although these models perform similarly, different loss functions can guide the model to learn distinct features. Notably, we observe variations between classification and ranking based losses. Additionally, our analysis reveals that seemingly irrelevant properties can significantly influence the resulting embedding.We encourage researchers in the deep metric learning community to utilize our methods to gain insights into the features learned by their proposed techniques. Figure 1 illustrates our proposed analysis methods, consisting of pixel-level comparison and investigation of image property influence on clustering behavior in the embedding space.