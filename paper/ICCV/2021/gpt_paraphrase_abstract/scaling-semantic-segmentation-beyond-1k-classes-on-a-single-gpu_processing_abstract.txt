The current state-of-the-art methods for object detection and image classification are highly effective for a large number of classes. However, when it comes to semantic segmentation datasets, the number of classes is limited. This is not surprising given the lack of labeled data and the computational demands of segmentation. In this study, we propose a new training methodology that allows existing semantic segmentation models to be trained and scaled for a large number of semantic classes without increasing memory usage. Our approach reduces the space complexity of the segmentation model's output and introduces an approximation method for ground-truth class probability, which is used to compute cross-entropy loss. This approach can be applied to any state-of-the-art segmentation model, allowing it to be scaled for any number of semantic classes using just one GPU. Our approach achieves similar or even better mean Intersection over Union (mIoU) results compared to existing models when applied to different datasets. We demonstrate the benefits of our approach on a dataset with 1284 classes, achieving nearly three times better mIoU than the DeeplabV3+ model. The source code for our approach is available at the provided link.