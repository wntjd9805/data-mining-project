Fine-tuning from pre-trained ImageNet models is a popular approach in computer vision tasks. However, the default hyperparameters and fixed pre-trained models are not optimized for specific tasks and time constraints. In cloud computing or GPU clusters, where tasks arrive sequentially, faster online fine-tuning is desired to save resources. To address this, we propose NASOA, a framework that combines Neural Architecture Search and Online Adaptation. NASOA uses offline NAS to identify training-efficient networks and creates a model zoo. We introduce a joint search space for efficient searching. By estimating fine-tuning performance through an adaptive model, NASOA generates a personalized training regime for each task. Our model zoo is more efficient than state-of-the-art models and achieves better results in fine-tuning accuracy. Experiments show that NASOA outperforms the best performance in RegNet series under various constraints and tasks and is significantly faster than BOHB.