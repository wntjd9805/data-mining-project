We propose a method for replacing text in videos using deep style transfer and learned photometric transformations. Our approach extends recent advancements in still image text replacement to preserve the appearance and motion characteristics of the original video. Unlike still image text replacement, video presents additional challenges such as changing lighting, motion blur, variations in camera-object pose, and the need for temporal consistency. We break down the problem into three steps. Firstly, we normalize the text in all frames to a frontal pose using a spatio-temporal transformer network. Secondly, we replace the text in a single reference frame using a state-of-the-art still-image text replacement method. Lastly, we transfer the new text from the reference frame to the remaining frames using a novel learned image transformation network that maintains temporal consistency and captures lighting and blur effects. Our method produces realistic text transfer, achieves competitive quantitative and qualitative performance, and offers superior inference speed compared to alternatives. We introduce new synthetic and real-world datasets with paired text objects. To our knowledge, this is the first attempt at deep video text replacement.