Traditional video models use a single stream to capture spatial-temporal features, but recent advancements have shown that using two separate streams can lead to better performance. However, manually designing these streams and fusion blocks is time-consuming and often results in sub-optimal architectures. To address this, we propose a neural architecture search approach that efficiently searches for two-stream video models. We create a multivariate search space with 6 variables to capture various design choices. Additionally, we employ a progressive search procedure, searching for the architecture of individual streams, fusion blocks, and attention blocks sequentially. Our approach successfully discovers two-stream models with significantly improved performance. Our model, Auto-TSNet, consistently outperforms other models on standard benchmarks. For example, on Kinetics, our Auto-TSNet-L model achieves the same accuracy as SlowFast while reducing FLOPS by nearly 11Ã—. On Something-Something-V2, our Auto-TSNet-M model improves accuracy by at least 2% compared to other methods using less than 50 GFLOPS per video.