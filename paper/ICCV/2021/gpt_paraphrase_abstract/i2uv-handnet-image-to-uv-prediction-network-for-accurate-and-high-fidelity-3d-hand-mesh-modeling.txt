The proposed method achieves exceptional performance on challenging benchmarks, surpassing current methods in accurately reconstructing a high-precision and high-fidelity 3D human hand from a color image. This is crucial for creating realistic virtual hands in human-computer interaction and virtual reality applications. Existing methods suffer from inaccuracies and lack fidelity due to different hand poses and occlusions. To address this, we introduce the I2UV-HandNet model, which estimates hand pose, shape, and reconstructs 3D hand super-resolution. This model utilizes a UV-based 3D hand shape representation, a novel approach in this field. To reconstruct a 3D hand mesh from an RGB image, we employ an AffineNet that predicts a UV position map, transforming the input in an image-to-image translation manner. Additionally, we utilize an SRNet to enhance the fidelity of the shape by converting the low-resolution UV map from AffineNet to a higher resolution. Our experiments demonstrate the effectiveness of the UV-based hand shape representation, achieving state-of-the-art results.