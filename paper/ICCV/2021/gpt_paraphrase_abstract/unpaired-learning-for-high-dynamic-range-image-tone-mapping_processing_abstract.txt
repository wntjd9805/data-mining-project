This abstract discusses the limitations of using deep neural networks (DNN) for high dynamic range (HDR) tone-mapping in photography due to the lack of a definitive ground-truth solution. The authors propose a new approach that focuses on producing low dynamic range (LDR) renditions that accurately reproduce the visual characteristics of native LDR images. They achieve this by using an unpaired adversarial training based on unrelated sets of HDR and LDR images. To effectively train the network, they introduce range-normalizing pre-processing, a loss function that preserves input content, and a concise discriminator network. The resulting network is capable of producing photo-realistic and artifact-free tone-mapped images, and it demonstrates state-of-the-art performance on various image fidelity indices and visual distances.