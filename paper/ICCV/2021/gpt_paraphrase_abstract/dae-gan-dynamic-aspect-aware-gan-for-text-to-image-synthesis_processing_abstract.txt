Text-to-image synthesis aims to generate realistic images based on textual descriptions. Previous methods have made progress by using sentence and word embeddings, but they often overlook the 'aspect' information contained in the text, which refers to multiple words describing specific parts or features of an object. This aspect information is crucial for creating detailed images. To tackle this challenge, we propose a Dynamic Aspect-aware GAN (DAE-GAN) that comprehensively represents text information at different granularities: sentence-level, word-level, and aspect-level. Additionally, we introduce a novel Aspect-aware Dynamic Redrawer (ADR) inspired by human learning behaviors. The ADR consists of an Attended Global Refinement (AGR) module and an Aspect-aware Local Refinement (ALR) module, which alternately enhance the generated image using word-level embeddings and refine image details from a local perspective using aspect-level embeddings. Furthermore, we design a matching loss function to ensure semantic consistency between the text and image at different levels. Extensive experiments on two widely-used datasets (CUB-200 and COCO) validate the effectiveness and rationality of our approach.