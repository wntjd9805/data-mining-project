Geometric feature extraction is a critical part of point cloud registration pipelines. Previous studies have shown that supervised learning can improve the quality and compactness of 3D features. However, these approaches rely on ground-truth annotation, which limits their scalability. To address this limitation, we propose BYOC, a self-supervised method that learns visual and geometric features from RGB-D video without the need for ground-truth pose or correspondence. Our approach leverages randomly-initialized CNNs to generate reliable correspondences, enabling the learning of both visual and geometric features. We combine traditional point cloud registration techniques with modern representation learning methods. We evaluate our approach on indoor scene datasets and find that it outperforms traditional and learned descriptors, while also being competitive with state-of-the-art supervised approaches.