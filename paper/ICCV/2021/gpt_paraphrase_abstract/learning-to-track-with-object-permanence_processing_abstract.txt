This study focuses on the problem of online multi-object tracking, where objects may not be fully visible in every frame. The current dominant approach, tracking by detection, relies heavily on the quality of instantaneous observations and often fails in the presence of occlusions. In contrast, humans have the ability to track objects even when they are not fully visible, thanks to the concept of object permanence.

To address this disparity, the researchers propose an end-to-end trainable approach for joint object detection and tracking that can reason about object locations and identities even under occlusions. They build upon the existing CenterTrack architecture, which takes pairs of frames as input, and extend it to handle videos of any length. The model is augmented with a spatio-temporal, recurrent memory module that allows it to utilize the entire history of object information to make predictions in the current frame.

However, training such an approach poses challenges. To tackle this, the researchers create a new synthetic dataset for multi-object tracking, which includes ground truth annotations for invisible objects. They propose several methods for supervising tracking behind occlusions. Additionally, the model is trained on both synthetic and real data to enhance its robustness.

The experimental results demonstrate that the proposed model outperforms the state-of-the-art methods on the KITTI and MOT17 datasets, primarily due to its ability to handle occlusions effectively.