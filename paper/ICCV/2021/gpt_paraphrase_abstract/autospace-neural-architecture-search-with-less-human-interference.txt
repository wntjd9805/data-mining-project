The current neural architecture search (NAS) algorithms still require expert knowledge and effort to design a search space for network construction. In this paper, we aim to automate the search space design to minimize human interference. However, this approach faces two challenges: the complexity of the exploration space and the computational cost to evaluate different search spaces. To address these challenges, we propose a novel differentiable evolutionary framework called AutoSpace. AutoSpace evolves the search space to an optimal one using two key techniques: a differentiable fitness scoring function to efficiently evaluate the performance of cells and a reference architecture to speed up the evolution process and avoid sub-optimal solutions. This framework is generic and can be adapted to different computational constraints, allowing for the learning of specialized search spaces that fit different budgets. By using the learned search space, the performance of recent NAS algorithms can be significantly improved compared to using manually designed spaces. Notably, models generated from the new search space achieve a top-1 accuracy of 77.8% on ImageNet under the mobile setting, outperforming previous state-of-the-art EfficientNet-B0 by 0.7%.Figure 1 illustrates the comparison of different search space construction schemes. Most existing NAS methods use handcrafted search spaces, which require expertise and trial-and-error. In contrast, our proposed method automatically builds and optimizes the search space by learning to form basic operators into candidate building blocks and using an efficient approach to evolve and evaluate these building blocks. Compared to existing schemes, our proposed method involves lower human effort and searching cost.