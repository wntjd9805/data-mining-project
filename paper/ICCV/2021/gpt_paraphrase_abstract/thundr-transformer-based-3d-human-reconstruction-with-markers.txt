We introduce THUNDR, a deep neural network approach called a transformer, which can accurately reconstruct the 3D pose and shape of individuals using single RGB images. Our method incorporates an intermediate 3D marker representation that combines the strengths of model-free-output architectures and a statistical human surface model called GHUM. GHUM is a comprehensive 3D human model that is trained end-to-end and preserves human anthropometric properties. By leveraging the transformer-based prediction pipeline, our approach can effectively focus on relevant image regions, supports self-supervised learning, and ensures that the solutions adhere to human anthropometry. Our experimental results demonstrate exceptional performance on Human3.6M and 3DPW datasets, both in fully-supervised and self-supervised settings, for inferring 3D human shape, joint positions, and global translation. Moreover, our method achieves impressive 3D reconstruction accuracy for challenging human poses captured in real-world scenarios.