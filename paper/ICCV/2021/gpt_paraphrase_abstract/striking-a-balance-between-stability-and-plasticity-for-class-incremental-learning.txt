This study focuses on overcoming the challenge of balancing stability and plasticity in class-incremental learning (CIL), which involves updating a trained model with new classes without forgetting previously learned ones. Existing approaches either store representative exemplars or prevent model parameters from drifting, but these methods have limitations. In this paper, the authors propose a reformulated baseline method that uses a cosine classifier framework and reciprocal adaptive weights to address the stability-plasticity dilemma in CIL without storing exemplars. They introduce two new approaches: one that learns class-independent knowledge to connect new and old classes, and another that learns knowledge from multiple perspectives to facilitate CIL. Experimental results on popular CIL benchmark datasets demonstrate the superiority of their approaches compared to existing methods.