This study introduces a dynamic network quantization framework for efficient video recognition using deep convolutional networks. While deep networks have shown success in video recognition, their practical implementation is hindered by the high computational resources required. To address this, the proposed framework utilizes quantization to enhance efficiency. A lightweight network is trained alongside the recognition network to determine the optimal numerical precision for each frame in video recognition. Both networks are trained using standard backpropagation with a loss function, aiming to achieve competitive performance and resource efficiency. The effectiveness of the proposed approach is validated through extensive experiments on diverse benchmark datasets, demonstrating significant reductions in computation and memory usage compared to state-of-the-art methods. The project page for this research can be accessed at https://cs-people.bu.edu/sunxm/VideoIQ/project.html.