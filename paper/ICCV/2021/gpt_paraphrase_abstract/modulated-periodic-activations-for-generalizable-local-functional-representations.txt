Recent advancements in Multi-Layer Perceptrons (MLPs) have improved their ability to represent high-frequency content in low-dimensional signals such as images, shapes, and light fields. However, these improvements often come at the expense of generalization, as modern methods are typically optimized for a single signal. In this study, we propose a new representation that can generalize to multiple instances and achieve state-of-the-art fidelity. Our approach involves using a dual-MLP architecture to encode the signals. A synthesis network maps a low-dimensional input (e.g., pixel-position) to the output domain (e.g., RGB color), while a modulation network maps a latent code corresponding to the target signal to parameters that modulate the periodic activations of the synthesis network. Additionally, we introduce a local-functional representation that enables generalization. The signal's domain is divided into a regular grid, with each tile represented by a latent code. During testing, the signal is encoded with high fidelity by inferring or directly optimizing the latent code-book. Our method produces generalizable functional representations of images, videos, and shapes, surpassing the reconstruction quality of prior works optimized for a single signal.