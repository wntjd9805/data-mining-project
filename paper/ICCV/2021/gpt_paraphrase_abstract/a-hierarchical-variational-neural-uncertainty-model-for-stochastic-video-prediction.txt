Predicting future frames in a video is difficult because of the unpredictable nature of real-world phenomena. Previous methods for solving this task have focused on estimating a latent prior that captures this unpredictability, but they fail to consider the uncertainty of the deep learning model itself. These methods typically rely on the mean-squared error (MSE) between the generated frames and the ground truth as a training signal, which can be suboptimal when the predictive uncertainty is high. In order to address this issue, we propose a new approach called NeuralUncertainty QuantiÔ¨Åer (NUQ) that quantifies the model's predictive uncertainty and incorporates it into the training process by weighting the MSE loss. Our approach is based on a hierarchical, variational framework that leverages a deep, Bayesian graphical model to derive NUQ in a principled manner. We conducted experiments on three benchmark stochastic video prediction datasets and found that our proposed framework trains more effectively compared to state-of-the-art models, particularly when the training sets are small. Additionally, our framework produces videos of higher quality and diversity, as evaluated by various metrics.