Human pose estimation relies on visual cues and anatomical constraints to locate keypoints. While most CNN-based methods excel in visual representation, they often lack the ability to learn constraint relationships between keypoints. In this paper, we propose a novel approach called TokenPose for human pose estimation. In TokenPose, each keypoint is explicitly embedded as a token, allowing for simultaneous learning of constraint relationships and appearance cues from images. Through extensive experiments, we demonstrate that both the small and large TokenPose models perform on par with state-of-the-art CNN-based methods while being more lightweight. Specifically, our TokenPose-S and TokenPose-L achieve AP scores of 72.5 and 75.8, respectively, on the COCO validation dataset. These results are achieved with a significant reduction in parameters (80.6% and 56.8% reduction) and GFLOPs (75.3% and 24.7% reduction). The code for TokenPose is publicly available.