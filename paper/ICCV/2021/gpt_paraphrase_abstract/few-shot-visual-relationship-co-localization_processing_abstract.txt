This paper introduces a new problem called visual relationship co-localization (VRC), which involves localizing visual subject-object pairs connected by a common predicate in a small bag of images. VRC is more challenging than object co-localization, especially when only a few images are available and the model needs to learn to co-localize pairs with unseen predicates. To address this problem, the paper proposes an optimization framework that selects a common visual relationship in each image. The goal is to find the optimal solution by learning visual relationship similarity across images in a few-shot setting. The framework utilizes a simple yet effective technique to learn relationship embeddings and a proven meta-learning technique for few-shot classification tasks to learn visual relationship similarity. To handle the complexity of finding feasible solutions, a greedy approximation inference algorithm is used. The proposed framework is extensively evaluated on two challenging datasets, VrR-VG and VG-150, with varying bag sizes, and achieves impressive visual co-localization performance.