Recent advancements in hand-crafted neural architectures for visual recognition have emphasized the need to explore hybrid architectures composed of diverse building blocks. At the same time, there has been a surge in neural architecture search (NAS) methods, which aim to reduce human efforts. However, it remains uncertain whether NAS methods can effectively handle search spaces with different candidates such as convolutional neural networks (CNNs) and transformers. To address this, we propose Block-wisely Self-supervised Neural Architecture Search (BossNAS), an unsupervised NAS method that tackles the issue of inaccurate architecture rating caused by large weight-sharing space and biased supervision in previous methods. Our approach involves dividing the search space into blocks and utilizing a novel self-supervised training scheme called ensemble bootstrapping. Each block is trained separately before being searched as a whole towards the population center. Additionally, we introduce the HyTra search space, a hybrid CNN-transformer search space with searchable down-sampling positions. On this challenging search space, our searched model, BossNet-T, achieves an accuracy of up to 82.5% on ImageNet, surpassing EfficientNet by 2.4% with comparable compute time. Furthermore, our method achieves superior architecture rating accuracy with Spearman correlation values of 0.78 and 0.76 on the canonical MBConv search space with ImageNet and the NATS-Bench size search space with CIFAR-100, respectively, surpassing state-of-the-art NAS methods.