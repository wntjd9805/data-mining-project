Our work focuses on estimating 3D human pose and shape using event cameras, which are sensors that capture the dynamics of moving objects as events. However, event signals present unique challenges as they are better at capturing local motions rather than static body postures. To address this, we propose a two-stage deep learning approach called EventHPE. In the first stage, we train a model called FlowNet using unsupervised learning to infer optical flow from events. Both events and optical flow are closely related to human body dynamics and are used as input for the second stage, where we estimate 3D human shapes using ShapeNet. To deal with the discrepancy between image-based flow and shape-based flow, we introduce a novel flow coherence loss that exploits the fact that both flows originate from the same human motion. We have created a large in-house event-based 3D human dataset with pose and shape annotations, which is currently the largest available. Our approach is evaluated on the DHP19 dataset and our in-house dataset, and the results demonstrate its effectiveness.