This study investigates day-night domain adaptation in the zero-shot setting. Traditional domain adaptation involves training on one domain and adapting to a target domain using unlabeled data from the test set. However, gathering relevant test data can be costly or impossible. To address this issue, this study proposes a method that does not rely on test data imagery. Instead, it utilizes a visual inductive prior derived from physics-based reflection models for domain adaptation. The researchers incorporate trainable color invariant edge detectors into a convolutional neural network and assess their ability to handle illumination changes. The experiments demonstrate that the color invariant layer reduces the distribution shift between day and night features in the network's activation maps. The proposed method shows improved performance in zero-shot day-night domain adaptation tasks, including classification, segmentation, and place recognition, on both synthetic and natural datasets.