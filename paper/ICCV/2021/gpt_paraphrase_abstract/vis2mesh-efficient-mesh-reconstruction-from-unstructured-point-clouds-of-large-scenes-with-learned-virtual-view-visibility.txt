We introduce a new framework for reconstructing meshes from unstructured point clouds. Our approach combines the learned visibility of 3D points in virtual views with traditional graph-cut based mesh generation. We propose a three-step network that incorporates depth completion to predict visibility. By aggregating visibility information from multiple views, we generate a 3D mesh model using an optimization problem that considers visibility. We also introduce a novel adaptive visibility weighting technique to suppress line of sight with a large incident angle during surface determination. Unlike other learning-based methods, our pipeline focuses on a 2D binary classification task, determining whether points are visible or not in a view. This approach is more generalizable, efficient, and capable of handling a large number of points. Experimental results show that our method is transferable, robust, and performs competitively compared to state-of-the-art learning-based approaches for small complex objects. It also outperforms these approaches for large indoor and outdoor scenes. The code for our method is available at https://github.com/GDAOSU/vis2mesh.