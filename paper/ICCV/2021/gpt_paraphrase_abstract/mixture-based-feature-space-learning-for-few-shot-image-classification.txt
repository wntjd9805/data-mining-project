We present MixtFSL, a method for enhancing feature representation in few-shot image classification. Unlike previous approaches that use offline clustering algorithms, we propose simultaneously training the feature extractor and learning mixture model parameters in an online manner. This results in a more discriminative feature space that improves classification of novel examples with limited samples. The training of the MixtFSL model involves two stages: learning multimodal mixtures and feature extractor parameters using two loss functions, and refining the network and mixture models through a leader-follower learning procedure. This procedure uses a "target" network to assign instances to mixture components consistently, enhancing performance and training stability. We evaluate our approach on four standard datasets and four backbones, and demonstrate that when combined with alignment-based approaches, our robust representation achieves state-of-the-art results in the inductive setting. Specifically, we achieve an absolute accuracy of 82.45% on miniImageNet, 88.20% on tieredImageNet, and 60.70% on FC100 using the ResNet-12 backbone.