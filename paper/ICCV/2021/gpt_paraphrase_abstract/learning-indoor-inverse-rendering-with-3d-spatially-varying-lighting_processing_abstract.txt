This study focuses on the problem of estimating albedo, normals, depth, and 3D spatially-varying lighting from a single image. Existing methods typically treat this task as image-to-image translation, disregarding the 3D characteristics of the scene. However, indoor scenes involve complex 3D light transport that cannot be adequately represented in 2D. To address this, the authors propose a unified, learning-based inverse rendering framework that incorporates 3D spatially-varying lighting. Drawing inspiration from volume rendering techniques, they introduce a novel Volumetric Spherical Gaussian representation for lighting, which parameterizes the exitant radiance of the scene surfaces in a voxel grid. They also develop a physics-based differentiable renderer that utilizes this 3D lighting representation and enables joint training of all intrinsic properties with the re-rendering constraint, ensuring physically accurate predictions without the need for inaccessible ground-truth HDR lighting. Experimental results demonstrate that their approach surpasses previous methods in terms of both quantitative and qualitative performance, and is capable of generating photorealistic results for augmented reality applications, even with highly specular objects.