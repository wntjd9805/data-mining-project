Synthetic data is a potential solution to the scalability problem in supervised deep learning when acquiring or annotating real data is difficult. However, manually generating synthetic data can be expensive and time-consuming. Additionally, neural networks trained on synthetic data often struggle to perform well on real data due to differences in the data distribution. To address these challenges, we propose Sim2SG, an automatic scene generation technique that does not require supervision from real-world data. Sim2SG aims to bridge the content and appearance gaps by matching the content and features of real and synthetic data. We use scene graph generation as the downstream task due to limited labeled datasets. Experimental results demonstrate significant improvements in reducing the domain gap compared to existing methods, both qualitatively and quantitatively, on multiple synthetic datasets and the real-world KITTI dataset.