Contrastive learning allows for the creation of powerful losses by contrasting positive pairs with negative samples. This principle has been applied to learn cross-modal embeddings for video and text, but its full potential has not been fully utilized. Previous losses did not consider intra-modality similarities, resulting in inefficient embeddings where the same content is mapped to multiple points in the embedding space. Our approach, CrossCLR, addresses this issue by introducing a contrastive loss that takes into account intra-modality similarities. Additionally, we define sets of highly related samples based on their input embeddings and exclude them from the negative samples to avoid false negatives. Our experiments show that these principles consistently improve the quality of the learned embeddings. We demonstrate the effectiveness of CrossCLR by achieving state-of-the-art results in video-text retrieval and video captioning tasks on Youcook2 and LSMDC datasets. Furthermore, we show that this concept can be applied to improve joint embeddings for other pairs of modalities.