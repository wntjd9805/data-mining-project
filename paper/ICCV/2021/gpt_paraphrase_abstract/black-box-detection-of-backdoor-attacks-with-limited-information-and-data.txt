Deep neural networks (DNNs) have made significant advancements in recent years, but they are susceptible to attacks in adversarial environments. One such attack involves embedding a malicious backdoor into the model by tampering with the training dataset. This backdoor is designed to cause the model to make incorrect predictions when a specific trigger is present during inference. Several methods have been proposed to detect and defend against backdoor attacks, but they typically rely on access to the poisoned training data or the white-box model, which is not readily available in real-world scenarios.To address this limitation, this paper presents a black-box backdoor detection (B3D) method that can identify backdoor attacks with only query access to the model. The proposed method utilizes a gradient-free optimization algorithm to reverse-engineer the potential trigger for each class, thereby uncovering the presence of backdoor attacks. Additionally, the paper introduces a straightforward strategy for generating reliable predictions using the identified backdoored models.Extensive experiments conducted on numerous DNN models trained on various datasets validate the efficacy of the proposed method in detecting backdoor attacks under the black-box setting. The method successfully defends against different types of backdoor attacks, showcasing its robustness and effectiveness.