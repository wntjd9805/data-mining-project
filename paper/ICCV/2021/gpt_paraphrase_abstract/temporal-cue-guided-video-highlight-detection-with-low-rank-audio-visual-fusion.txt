Automated video highlight detection is a challenging task due to the lack of temporal annotations in long videos. To address this issue, we propose a novel weakly supervised method that utilizes video level annotations (topic tags) to learn how to detect highlights. Our approach leverages audio-visual features to enhance video representation and incorporates temporal cues to improve detection performance. We make three contributions: 1) the introduction of an efficient audio-visual tensor fusion mechanism that models the association between modalities and reduces heterogeneity, 2) the implementation of a hierarchical temporal context encoder that embeds local temporal clues between neighboring segments, and 3) the use of attention-gated instance aggregation to mitigate the problem of gradient vanishing during model optimization. Extensive experiments conducted on two benchmark datasets (YouTubeHighlights and TVSum) demonstrate the superiority of our method compared to other state-of-the-art techniques, with significant improvements in performance.