Unsupervised pretraining has been successful in achieving comparable or slightly better transfer performance than supervised pretraining on downstream target datasets. However, this paper highlights that this conclusion may not hold true when the target dataset has very few labeled samples for finetuning, known as few-label transfer. From a clustering perspective, the quality of the target samples' clustering plays a crucial role in few-label transfer. Although contrastive learning is important for learning how to cluster, its clustering quality is still inferior to supervised pretraining due to the lack of label supervision. Through analysis, it is revealed that involving some unlabeled target domain in the unsupervised pretraining can improve the clustering quality and reduce the performance gap with supervised pretraining. This discovery leads to the proposal of a new progressive few-label transfer algorithm that aims to maximize transfer performance within a limited annotation budget. Extensive experiments on nine different target datasets support the analysis and demonstrate that the proposed method significantly enhances the few-label transfer performance of unsupervised pretraining.