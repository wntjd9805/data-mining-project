This paper presents a neural approach for real-time and high-quality video portrait relighting, which addresses the challenge of achieving consistent results under dynamic illuminations. Existing methods lack video consistency supervision and struggle to recover consistent relit results from monocular RGB streams. To overcome this limitation, the proposed approach utilizes a new dynamic OLAT dataset and combines semantic, temporal, and lighting consistency modeling. The encoder-decoder architecture incorporates a hybrid structure and lighting disentanglement, while employing a multi-task and adversarial training strategy for semantic-aware consistency modeling. Additionally, a flow-based supervision scheme is utilized to encode temporal consistency. To ensure illumination consistency and enable natural portrait light manipulation, a lighting sampling strategy is introduced. Extensive experiments demonstrate the effectiveness of the proposed approach in achieving consistent video portrait light-editing and relighting, even on mobile computing devices.