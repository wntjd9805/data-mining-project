We present a new method for filling in missing parts of a video using a flow-based approach. Our approach involves creating a generative model that relates images to the scene and mappings from the scene to images. By jointly inferring the scene template and the mappings, we ensure consistency in the flow between frames, reducing geometric distortions in the inpainting process. We also introduce a new interpolation scheme to map the template to the missing regions, resulting in high-quality inpaintings with reduced blur and distortion. Our method outperforms existing techniques both quantitatively and in user studies on two benchmark datasets.