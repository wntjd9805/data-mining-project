As modern autonomous systems continue to grow rapidly, the potential for cooperative multi-agent systems becomes more feasible. These systems can benefit from sharing information and distributing workloads, which enhances task performance and computational efficiency. However, the sharing of information also introduces the risk of adversarial attacks on deep learning models commonly used in these systems. Therefore, our objective is to investigate the resilience of these systems and focus on exploring adversarial attacks in a unique multi-agent scenario where communication occurs through sharing learned intermediate representations of neural networks. We find that an indistinguishable adversarial message can significantly impair performance, but its impact weakens as the number of benign agents increases. Additionally, we demonstrate that black-box transfer attacks are more challenging in this setting compared to directly manipulating the inputs, as aligning the distribution of learned representations requires domain adaptation. Our research delves into the robustness of neural networks to provide an extra layer of fault tolerance to enhance the security protocols of modern multi-agent systems.