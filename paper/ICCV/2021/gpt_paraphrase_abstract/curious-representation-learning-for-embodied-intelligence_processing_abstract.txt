Self-supervised representation learning has been highly successful in recent years, allowing for the utilization of unlabeled images found on the Internet and in photographic datasets without the need for supervised labels. However, to create truly intelligent agents, it is necessary to develop representation learning algorithms that can learn not only from datasets but also from environments. In natural environments, agents do not have access to curated data and must explore their surroundings to gather the data they need to learn. This paper proposes a framework called curious representation learning (CRL), which simultaneously learns a reinforcement learning policy and a visual representation model. The policy is trained to maximize the error of the representation learner, encouraging the agent to explore its environment. As the policy provides increasingly challenging data, the learned representation becomes stronger. The learned representations demonstrate promising transferability to downstream navigation tasks, outperforming or performing comparably to ImageNet pretraining without any supervision. Furthermore, despite being trained in simulation, the learned representations yield interpretable results on real images. The code for this framework is available at https://yilundu.github.io/crl/.