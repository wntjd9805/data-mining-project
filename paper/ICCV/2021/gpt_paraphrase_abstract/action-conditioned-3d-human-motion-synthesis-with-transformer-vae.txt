We address the challenge of generating diverse and realistic human motion sequences based on specific actions. Unlike existing methods that require an initial pose or sequence, our approach does not. We accomplish this by training a generative variational autoencoder (VAE) to learn an action-aware latent representation for human motions. By sampling from this latent space and utilizing positional encodings, we are able to synthesize motion sequences of varying lengths conditioned on a categorical action. To achieve this, we propose ACTOR, a Transformer-based architecture that encodes and decodes parametric SMPL human body models obtained from action recognition datasets. We evaluate our approach on several datasets and demonstrate its superiority compared to existing techniques. Additionally, we showcase two practical applications: enhancing action recognition by incorporating our synthesized data into training, and reducing motion noise. For those interested, our project page [53] provides access to the code and models.