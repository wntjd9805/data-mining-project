This paper presents a new approach called DualPoseNet for estimating the pose and size of objects at the category level. The proposed method utilizes a dual pose network consisting of two parallel pose decoders and a shared pose encoder. The implicit decoder predicts object poses using a different mechanism than the explicit decoder, providing complementary supervision for training the pose encoder. The encoder is constructed using spherical convolutions, and a module called Spherical Fusion is designed to better incorporate pose-sensitive features from appearance and shape observations. Notably, the introduction of the implicit decoder allows for refined pose prediction during testing even without CAD models, achieved by enforcing pose consistency between the two decoders using a self-adaptive loss term. Extensive experiments on category- and instance-level object pose datasets demonstrate the effectiveness of the proposed method, which significantly outperforms existing approaches in terms of high precision. The code for DualPoseNet is publicly available at https://github.com/Gorilla-Lab-SCUT/DualPoseNet.