The ability to identify correspondences in visual data is crucial for computer vision tasks. However, determining the correct correspondences can be challenging, especially when dealing with objects from different categories. While visual correspondence is well-defined for images of the same object instance or objects from the same category, it becomes less clear when comparing objects from completely different categories, such as a shoe and a bottle. In this paper, we propose the concept of functional correspondences, which takes into account humans' ability to generalize beyond semantic categories and infer functional affordances. We introduce the FunKPoint dataset, which contains ground truth correspondences for 10 tasks and 20 object categories. Additionally, we present a modular task-driven representation approach to address this problem and demonstrate its effectiveness. Importantly, because our supervision signal is not limited to semantics, we show that our learned representation can generalize better in few-shot classification problems. We hope that this paper will inspire the computer vision community to think beyond semantics and focus on cross-category generalization and learning representations for robotics tasks.