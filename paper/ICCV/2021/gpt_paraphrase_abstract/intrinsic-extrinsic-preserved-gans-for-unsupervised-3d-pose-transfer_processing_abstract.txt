In recent years, there has been a resurgence of research interest in 3D pose transfer, thanks to the advancements in deep generative models. Existing methods for pose transfer rely on constraints such as manual encoding for shape and pose disentanglement. In this paper, we propose an unsupervised approach for conducting pose transfer between arbitrary 3D meshes.Our approach introduces a novel Intrinsic-Extrinsic Preserved Generative Adversarial Network (IEP-GAN) that preserves both intrinsic (shape) and extrinsic (pose) information. To capture structural and pose invariance, we propose a co-occurrence discriminator that operates on distinct Laplacians of the mesh. Additionally, we introduce a local intrinsic-preserved loss that preserves geodesic priors while avoiding heavy computations.Furthermore, we demonstrate the versatility of IEP-GAN by showcasing its ability to manipulate 3D human meshes in various ways, including pose transfer, identity swapping, and pose interpolation using latent code vector arithmetic. To validate the effectiveness of our approach, we conduct extensive experiments on diverse 3D datasets of humans, animals, and hands. The results, both qualitative and quantitative, demonstrate the generality of our approach and show that our proposed model produces superior results compared to state-of-the-art methods.We also provide the code for our approach, which is available on GitHub (https://github.com/mikecheninoulu/Unsupervised_IEPGAN).