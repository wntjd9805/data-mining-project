We introduce a new discriminator architecture for unsupervised image-to-image translation that emphasizes statistical features instead of individual patches. Our approach stabilizes the network by matching distribution of key statistical features at multiple scales. In contrast to existing methods that impose constraints on the generator, our method simplifies the framework while improving shape deformation and enhancing fine details. We demonstrate that our proposed method surpasses the current state-of-the-art models in challenging applications such as transforming selfies to anime, converting male images to female, and removing glasses.