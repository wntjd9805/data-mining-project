360-degree videos provide a comprehensive view of a scene, offering audio-visual information beyond the usual field of view. These videos display spatial relations on a spherical surface. However, existing benchmark tasks for panoramic videos only evaluate the semantic understanding of audio-visual relationships or spatial properties in the surroundings. To address this limitation, we introduce a new benchmark called Pano-AVQA, which is a large-scale dataset for grounded audio-visual question answering on panoramic videos. We collect 5.4K 360-degree video clips from the internet and create two types of question-answer pairs with bounding-box grounding: spherical spatial relation QAs and audio-visual relation QAs. We train several transformer-based models using Pano-AVQA and find that our proposed spherical spatial embeddings and multimodal training objectives significantly improve the semantic understanding of the panoramic surroundings in the dataset. The answer format of Pano-AVQA only provides the abstract answer.