Recovering accurate human poses from images is crucial for establishing a connection between RGB images and the 3D surface of the human body. This has applications in various fields like virtual humans and monocular-to-3D reconstruction. However, the current popular dataset, DensePose-COCO, has limitations due to its manual annotation process, resulting in less dense and inaccurate pose resources. To address this, we introduce a new 3D human-body model with decoupled parameters that allow for flexible body generation. We also develop a data generation system based on this model, creating a synthetic benchmark called UltraPose. This benchmark contains around 1.3 billion corresponding points and provides ultra dense image-to-surface correspondences without the cost and errors of manual annotation. UltraPose offers the largest benchmark and data resources for improving the accuracy of dense pose predictions. To encourage further research in this area, we propose a transformer-based method to model the dense correspondence between the 2D and 3D worlds. The proposed model, trained on the synthetic UltraPose dataset, demonstrates its effectiveness in real-world scenarios, highlighting the value of our benchmark and model.