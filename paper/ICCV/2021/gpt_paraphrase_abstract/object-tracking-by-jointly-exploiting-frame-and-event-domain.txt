We propose a multi-modal approach that combines visual cues from frame-based and event-based cameras to improve single object tracking performance in challenging conditions. Our approach effectively combines information from both domains using cross-domain attention schemes and a weighting scheme to adaptively balance their contributions. We create a large-scale frame-event-based dataset and use it to train a fusion-based model. Extensive experiments demonstrate that our approach surpasses state-of-the-art frame-based tracking methods in terms of success rate and precision rate. A thorough ablation study confirms the effectiveness of each key component of our approach.