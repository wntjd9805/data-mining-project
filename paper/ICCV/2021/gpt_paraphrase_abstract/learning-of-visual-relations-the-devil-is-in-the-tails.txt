Recent research has focused on modeling visual relations, primarily by increasing model complexity. However, visual relation learning is a challenging problem due to the combinatorial nature of reasoning about groups of objects. Increasing model complexity often leads to overfitting in long-tailed problems. This paper proposes an alternative approach, called Devil is in the Tails, which suggests that better performance can be achieved by keeping the model simple but improving its ability to handle long-tailed distributions. To test this hypothesis, a new training approach called Decoupled Training for Devil in the Tails (DT2) is introduced, inspired by long-tailed recognition literature. DT2 incorporates Alternating Class-Balanced Sampling (ACBS), a novel sampling technique that captures the relationship between the long-tailed entity and predicate distributions of visual relations. Experimental results demonstrate that DT2-ACBS, with a simple architecture, outperforms more complex state-of-the-art methods in scene graph generation tasks. This highlights the importance of considering the long-tailed nature of the problem when developing sophisticated models.