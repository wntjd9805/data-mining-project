We focus on contrastive methods for self-supervised video representation learning. Traditional contrastive learning involves creating positive pairs by sampling different views of the same instance and using different instances as negatives. However, these methods assume certain invariances in the view selection mechanism, which can limit performance on downstream tasks that require violating these invariances, such as fine-grained video action recognition that relies on temporal information. To address this limitation, we propose an "augmentation aware" contrastive learning framework. In this framework, we provide a sequence of augmentation parameterizations (such as time shifts) as composable augmentation encodings to our model during the projection of video representations for contrastive learning. Our method learns representations that encode valuable information about specified spatial or temporal augmentations, leading to state-of-the-art performance on various video benchmarks.