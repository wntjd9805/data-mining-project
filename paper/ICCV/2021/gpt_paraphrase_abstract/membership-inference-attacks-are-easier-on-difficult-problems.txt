Membership inference attacks (MIA) aim to identify if data samples were utilized for training a neural network model, such as for detecting copyright infringements. Our research reveals that models with higher dimensional input and output are more susceptible to MIA. Specifically, we delve into models utilized for tasks like image translation and semantic segmentation, including medical image segmentation. We demonstrate that the presence of reconstruction-errors can render MIA attacks highly effective, as these errors indicate the model's tendency to memorize training data. However, we note that relying solely on reconstruction error may not effectively differentiate between non-predictable images used during training and easily predictable images that were previously unseen. To overcome this limitation, we propose a novel measure called predictability error, which can be computed for each sample without the need for a training set. By subtracting the predictability error from the reconstruction error, we obtain our membership error, which consistently achieves high accuracy in MIA across a wide range of benchmarks.