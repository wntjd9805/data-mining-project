Event cameras are sensors inspired by biology that capture brightness changes and produce event streams rather than traditional frame-based images. They offer advantages such as higher temporal resolution, dynamic range, and lower power consumption compared to conventional cameras. However, existing event cameras have limited spatial resolution, making it difficult to enhance it at the hardware level while maintaining the asynchronous design philosophy. Therefore, there is a need to develop algorithms for event stream super-resolution, which is challenging due to the sparse and strongly correlated nature of event data. In this study, we propose an end-to-end framework based on a spiking neural network for event stream super-resolution. This framework can generate high-resolution event streams from low-resolution inputs. We introduce a spatiotemporal constraint learning mechanism to simultaneously learn the spatial and temporal distributions of the event stream. We evaluate our method on four large-scale datasets and demonstrate that it achieves state-of-the-art performance. Moreover, we show the practicality of our method by applying it to object classification and image reconstruction tasks, which yield satisfactory results. Additionally, we deploy our method on a mobile platform to demonstrate its application potential. The real-time system generates high-quality high-resolution event streams, highlighting the effectiveness and efficiency of our approach.