Blind image super-resolution methods typically assume that blur kernels are constant throughout the entire image. However, this assumption is not valid for real images, as blur kernels vary spatially due to factors like object motion and out-of-focus. This limitation leads to poor performance in practical applications. To overcome this issue, this study proposes a mutual affine network (MANet) for estimating spatially variant blur kernels. MANet has two key features: a moderate receptive field to maintain degradation locality, and a new mutual affine convolution (MAConv) layer that enhances feature expressiveness without increasing model size or computational burden. The MAConv layer exploits channel interdependence by applying an affine transformation module to each channel split using the remaining channel splits as input. Extensive experiments on synthetic and real images demonstrate that MANet performs well for both spatially variant and invariant kernel estimation. Additionally, when combined with non-blind SR methods, MANet achieves state-of-the-art blind SR performance.