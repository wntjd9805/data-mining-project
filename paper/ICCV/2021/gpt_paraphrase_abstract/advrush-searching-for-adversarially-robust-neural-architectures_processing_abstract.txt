Deep neural networks continue to impress with their impressive performance, but their predictions can be easily influenced by adversarial examples that are undetectable to humans. Current efforts to enhance the robustness of neural networks against adversarial examples focus on improving the training methods by updating the weight parameters in a more robust manner. However, this work takes a step further by addressing the issue of designing a neural architecture with inherent robustness against adversarial attacks. Introducing AdvRush, a novel algorithm for adversarially robust neural architecture search, the researchers demonstrate that the smoothness of the input loss landscape can represent the intrinsic robustness of a neural network. By incorporating a regularizer that favors architectures with smoother input loss landscapes, AdvRush successfully discovers an adversarially robust neural architecture. The researchers provide both theoretical motivation for AdvRush and conduct extensive experiments on various benchmark datasets to showcase its effectiveness. Notably, AdvRush achieves a robust accuracy of 55.91% under FGSM attack after standard training on CIFAR-10, and a robust accuracy of 50.04% under AutoAttack after 7-step PGD adversarial training.