The selection of activation functions plays a critical role in deep neural networks. Existing hand-designed functions like ReLU and its variants have shown promising performance in various models and tasks. However, Swish, an automatically discovered activation function, has been proposed and has demonstrated superior results compared to ReLU on challenging datasets. Nevertheless, Swish has two main limitations. Firstly, its tree-based search space is discrete and restricted, making it difficult for searching. Secondly, the sample-based searching method is inefficient, making it impractical to find specialized activation functions for specific datasets or neural architectures. To overcome these limitations, we introduce a new activation function called Piece-wise Linear Unit (PWLU). PWLU incorporates a carefully designed formulation and learning method, allowing it to learn specialized activation functions and achieve state-of-the-art performance on large-scale datasets such as ImageNet and COCO. For instance, on the ImageNet classification dataset, PWLU improves top-1 accuracy by 0.9%, 0.53%, 1.0%, 1.7%, and 1.0% over Swish for ResNet-18, ResNet-50, MobileNet-V2, MobileNet-V3, and EfficientNet-B0, respectively. Moreover, PWLU is easy to implement and efficient during inference, making it suitable for a wide range of real-world applications.