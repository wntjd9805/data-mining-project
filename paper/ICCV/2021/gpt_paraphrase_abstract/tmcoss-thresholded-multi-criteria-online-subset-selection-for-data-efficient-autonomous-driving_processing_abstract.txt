Training autonomous driving models is a difficult task due to the need for storage and processing of large amounts of driving video data. This paper focuses on the problem of efficiently training autonomous driving systems using edge devices. We propose a multi-criteria online video frame subset selection technique, called TM-COSS, which addresses the limitations of existing convex optimization solutions. Our algorithm uses a concave function of selection variables to prioritize important frames. Through extensive experiments using the CARLA driving simulator, we demonstrate that TM-COSS can drop 80% of frames while successfully completing 100% of driving episodes. We also show that TM-COSS improves performance in the crucial affordance of "Relative Angle" during turns by including a bucket-specific relative angle loss (BL) that selects additional frames in those areas. Furthermore, TM-COSS achieves an 80% reduction in the number of training video frames for drivable area segmentation and semantic segmentation tasks on real-world videos from the BDD and Cityscapes datasets.