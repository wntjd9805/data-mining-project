We present GNeRF, a framework that combines Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction to handle complex scenarios with unknown and randomly initialized camera poses. While previous NeRF-based methods have shown impressive results in synthesizing realistic novel views, they heavily rely on accurate camera pose estimation and are limited to forward-facing scenes with short camera trajectories. In contrast, GNeRF can handle complex outside-in scenarios using randomly initialized camera poses. Our framework consists of two phases: in the first phase, GANs are used to optimize coarse camera poses and radiance fields jointly, and in the second phase, these are refined using additional photometric loss. We address the issue of local minima through a hybrid and iterative optimization scheme. Extensive experiments on synthetic and natural scenes demonstrate the effectiveness of GNeRF. Notably, our approach outperforms baseline methods in challenging scenes with repeated patterns or low textures.