Generating conversational gestures from speech audio is a complex task due to the challenge of mapping audio to body motions which have multiple possibilities. Traditional convolutional neural networks (CNNs) and recurrent neural networks (RNNs) assume a one-to-one mapping, resulting in the prediction of average motions and ultimately generating plain and uninteresting gestures during inference. To address this issue, we propose a new approach using a conditional variational autoencoder (VAE) that explicitly models the one-to-many mapping between audio and motion. This is achieved by dividing the cross-modal latent code into two parts: shared code and motion-specific code. The shared code captures the strong correlation between audio and motion, such as synchronized beats, while the motion-specific code represents independent motion information not reliant on audio. However, training the VAE with the split latent code presents difficulties. To overcome this, we introduce a mapping network for random sampling and employ techniques like relaxed motion loss, bicycle constraint, and diversity loss to enhance VAE training. Experimental evaluations on both 3D and 2D motion datasets demonstrate that our method produces more realistic and diverse gestures compared to state-of-the-art approaches, both quantitatively and qualitatively. Additionally, we showcase the ability of our method to generate motion sequences with user-specified motion clips on the timeline. Further details, including code and additional results, can be found at https://jingli513.github.io/audio2gestures.