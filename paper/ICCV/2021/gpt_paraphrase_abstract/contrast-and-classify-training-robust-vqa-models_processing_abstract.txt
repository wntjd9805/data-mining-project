Recent models in Visual Question Answering (VQA) have achieved impressive results on benchmark tests but are sensitive to minor linguistic differences in input questions. To address this issue, existing approaches augment the dataset with question paraphrases generated by visual question generation models or adversarial perturbations. These approaches then use the combined data to train an answer classifier using standard cross-entropy loss. However, to better utilize augmented data, we propose a new training paradigm called ConClaT that incorporates both cross-entropy and contrastive losses. By doing so, the contrastive loss encourages representations to be robust against linguistic variations in questions, while the cross-entropy loss maintains the discriminative power of representations for answer prediction. We find that optimizing both losses, either alternately or jointly, is crucial for effective training. On the VQA-Rephrasings benchmark, which evaluates the answer consistency of VQA models across paraphrased questions, ConClaT improves the Consensus Score by 1.63% compared to an improved baseline. Furthermore, on the standard VQA 2.0 benchmark, ConClaT achieves an overall improvement in VQA accuracy of 0.78%. We also demonstrate that ConClaT is independent of the specific data augmentation strategy used. The answer format output only includes the abstraction.