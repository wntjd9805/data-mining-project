This study investigates how spatial redundancy in video recognition can be utilized to enhance computational efficiency. The authors observe that the most informative region in each video frame is typically a small image patch that smoothly shifts across frames. To address this, they propose a reinforcement learning-based approach called AdaFocus for efficient spatially adaptive video recognition. The approach involves using a lightweight ConvNet to process the entire video sequence quickly, with the features then utilized by a recurrent policy network to localize the most relevant regions. A high-capacity network is employed to infer the selected patches for the final prediction. The authors highlight that once the informative patch sequence is generated during offline inference, the majority of computation can be performed in parallel, making it efficient on modern GPU devices. Additionally, the proposed method can be extended to consider temporal redundancy by dynamically skipping less valuable frames. The authors validate their approach through extensive experiments on five benchmark datasets, including ActivityNet, FCVID, Mini-Kinetics, and Something-Something V1&V2. The results demonstrate that their method outperforms competitive baselines in terms of efficiency. The code for AdaFocus is available at https://github.com/blackfeather-wang/AdaFocus.