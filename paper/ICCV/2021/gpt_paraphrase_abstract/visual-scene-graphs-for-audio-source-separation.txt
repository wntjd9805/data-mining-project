Current approaches to visually-guided audio source separation often overlook the visual context of sound sources and fail to model object interactions that could enhance source characterization. To tackle this issue, we present a novel deep learning model called Audio Visual Scene Graph Segmenter (AVSGS). AVSGS utilizes a graph representation of the visual scene and segments it into subgraphs, with each subgraph associated with a unique sound obtained by jointly segmenting the audio spectrogram. Our model employs a recursive neural network that generates mutually-orthogonal subgraph embeddings using multi-head attention. These embeddings are used to condition an audio encoder-decoder for source separation. We train our pipeline end-to-end through a self-supervised task involving the separation of audio sources using the visual graph. Additionally, we introduce a new video dataset named Audio Separation in the Wild (ASIW) that contains non-musical sound sources in natural and daily-life settings. This dataset, derived from AudioCaps, offers a challenging environment for source separation. Through extensive experiments on the ASIW dataset and the standard MUSIC datasets, we demonstrate that our method achieves state-of-the-art performance in sound separation compared to recent approaches.