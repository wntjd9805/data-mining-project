Progress in text-video retrieval has been made through leveraging pretraining on visual and audio datasets to create powerful video encoders. However, the utilization of large-scale language pretraining for effective algorithms in this field remains unexplored. This study aims to address this gap by introducing a novel method called TEACHTEXT, which utilizes multiple text encoders to enhance the supervisory signal for the retrieval model. The proposed method is extended to video side modalities, allowing for a reduction in the number of modalities used during testing without compromising performance. Notably, our approach significantly advances the state of the art in video retrieval benchmarks without any additional computational overhead during testing. Furthermore, our method proves effective in eliminating noise from retrieval datasets. The code and data for this study can be accessed at https://www.robots.ox.ac.uk/Ëœvgg/ research/teachtext/.