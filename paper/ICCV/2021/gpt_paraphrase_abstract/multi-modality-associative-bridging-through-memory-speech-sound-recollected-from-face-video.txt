This paper presents a new audio-visual multi-modal bridging framework that can effectively use both audio and visual information, even when only one of these modalities is available. The framework utilizes a memory network that stores representations of the source (visual) and target (audio) modalities. The source modal representation is the given input, while the target modal representation is the desired output from the memory network. An associative bridge is then constructed between the source and target memories, taking into account the relationship between the two. By learning this interrelationship, the proposed framework is able to obtain the target modal representations using only the source modal input, providing valuable information for downstream tasks. The framework is applied to lip reading and speech reconstruction from silent video, achieving state-of-the-art performance by enriching each task with the recalled audio context through the associative bridge and modality-specific memories. The effectiveness of the associative bridge in relating the source and target memories is also verified.