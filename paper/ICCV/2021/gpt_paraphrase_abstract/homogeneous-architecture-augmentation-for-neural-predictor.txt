Neural Architecture Search (NAS) is a method used to automatically design efficient Deep Neural Networks (DNNs) for specific tasks. However, NAS is limited by its high computational cost, primarily due to the need for expensive performance evaluation. To address this limitation, researchers have turned their attention to neural predictors, which can estimate the performance of DNNs without training them. Nevertheless, the scarcity of annotated DNN architectures hinders the effective training of neural predictors. This paper proposes a solution called Homogeneous Architecture Augmentation for Neural Predictor (HAAP). HAAP uses a homogeneous architecture augmentation algorithm to generate sufficient training data, leveraging a homogeneous representation. Additionally, the one-hot encoding strategy is introduced to improve the effectiveness of representing DNN architectures in HAAP. The proposed HAAP algorithm is evaluated on NAS-Benchmark-101 and NAS-Bench-201 datasets, and the results show that it outperforms existing approaches with significantly less training data. Furthermore, ablation studies conducted on benchmark datasets confirm the universality of the homogeneous architecture augmentation. The code for HAAP is available at https://github.com/lyq998/HAAP.