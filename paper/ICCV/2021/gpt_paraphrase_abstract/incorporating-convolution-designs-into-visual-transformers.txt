In light of the success of Transformers in natural language processing (NLP), there have been attempts to apply Transformer architectures to the field of computer vision, such as ViT and DeiT. However, pure Transformer models often struggle to achieve comparable performance to convolutional neural networks (CNNs) without a large amount of training data or additional supervision. In this study, we address the limitations of directly borrowing Transformer architectures from NLP and propose a new approach called Convolution-enhanced image Transformer (CeiT). CeiT combines the strengths of CNNs in extracting low-level features and preserving locality with the advantages of Transformers in capturing long-range dependencies. To achieve this, we make three modifications to the original Transformer architecture. First, instead of directly tokenizing raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from low-level features generated by the model. Second, we replace the feed-forward network in each encoder block with a Locally-enhanced Feed-Forward (LeFF) layer, which enhances the correlation between neighboring tokens in the spatial dimension. Finally, we add a Layer-wise Class token Attention (LCA) component at the top of the Transformer, which utilizes multi-level representations.We evaluate the performance of CeiT on the ImageNet dataset and seven downstream tasks. The experimental results demonstrate the effectiveness and generalization ability of CeiT compared to previous Transformer models and state-of-the-art CNNs, without the need for a large amount of training data or additional CNN teachers. Additionally, CeiT models exhibit better convergence with three times fewer training iterations, significantly reducing the training cost.