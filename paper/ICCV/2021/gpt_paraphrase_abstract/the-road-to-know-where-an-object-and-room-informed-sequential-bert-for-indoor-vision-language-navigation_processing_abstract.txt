Vision-and-Language Navigation (VLN) tasks involve guiding an agent to a specific location using natural-language instructions and photo-realistic panoramas. Existing methods encode instructions and panoramas using discrete views and individual words, which leads to difficulties in matching different nouns against the same visual features. To address this, we propose a novel approach called object-informed sequential BERT that encodes visual perceptions and linguistic instructions at a more detailed level, focusing on objects and words. Our sequential BERT also considers the temporal context, which is crucial for multi-round VLN tasks. Additionally, our model can identify the relative direction and room type of each navigable location, as these details are often mentioned in instructions indicating the desired next and final locations. By incorporating such information, our model gains an understanding of object locations within the images and their positions in the scene. Through extensive experiments on three indoor VLN tasks (REVERIE, NDH, and R2R), we demonstrate the effectiveness of our approach compared to several state-of-the-art methods. The project repository can be found at: https://github.com/YuankaiQi/ORIST.