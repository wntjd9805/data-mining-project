This study focuses on AI's ability to learn new skills by observing human behavior through instructional videos. The goal is to develop a model that can plan goal-directed actions in real-life videos. Unlike traditional action recognition, goal-directed actions require an understanding of the potential consequences of actions and the integration of environmental structure with goals. Previous approaches that learn a single world model fail to distinguish between different tasks, resulting in an ambiguous latent space. As a result, planning through this space neglects the desired outcomes over time. To overcome these limitations, this study introduces a new formulation of procedure planning and proposes innovative algorithms that utilize Bayesian Inference and model-based Imitation Learning to model human behaviors. The experiments conducted on real-world instructional videos demonstrate that this method achieves state-of-the-art performance in reaching the indicated goals. Additionally, the learned contextual information provides interesting features for planning in a latent space.