Deep metric learning (DML) focuses on learning visual similarities in a high-dimensional embedding space. However, a challenge is to generalize from training data of seen classes to test data of unseen classes. Recent research has tried to address this challenge by using past embeddings to increase the number of instances for seen classes, but this approach still heavily relies on seen classes.

To overcome this limitation, we propose a new training strategy for DML called MemVir. Unlike previous methods, MemVir memorizes both embedding features and class weights and treats them as additional virtual classes. By exploiting these virtual classes, MemVir not only uses augmented information for training but also reduces the emphasis on seen classes, leading to better generalization.

In addition, MemVir incorporates the concept of curriculum learning, gradually adding virtual classes to increase the learning difficulty. This approach improves learning stability and final performance. The advantage of MemVir is that it can be easily applied to existing loss functions without any modification.

Extensive experiments on well-known benchmarks demonstrate that MemVir outperforms state-of-the-art competitors. The code for implementing MemVir is publicly available.