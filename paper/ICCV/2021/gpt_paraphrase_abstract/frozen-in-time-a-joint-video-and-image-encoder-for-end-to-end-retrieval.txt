This study focuses on video-text retrieval, specifically the development of a joint embedding method for efficient text-to-video retrieval. The main challenges in this field involve designing the visual architecture and dealing with noisy training data from large-scale video-text datasets like HowTo100M. To address these challenges, the authors propose an end-to-end trainable model that utilizes both image and video captioning datasets. This model is based on the ViT and Timesformer architectures, incorporating attention in both space and time. It can be trained on image and video text datasets independently or together. The training process involves a curriculum learning schedule that initially treats images as static snapshots of video and gradually learns to consider more temporal context when trained on video datasets. Additionally, the authors introduce a new video-text pretraining dataset called WebVid-2M, which consists of over two million videos with weak captions collected from the internet. Despite using smaller datasets for training, the proposed approach achieves state-of-the-art performance on popular video-retrieval benchmarks such as MSR-VTT, MSVD, DiDeMo, and LSMDC.