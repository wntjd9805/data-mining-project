We propose a method for generating 3D indoor scenes using a generative model trained on semantic-segmented depth images from various unknown scenes. Unlike existing methods that represent indoor scenes by object type, location, and properties, and learn from complete 3D scenes, our approach models each scene as a 3D semantic volume and learns a volumetric generative adversarial network (GAN) from 2.5D partial observations. We use a differentiable projection layer to convert the generated 3D volumes into semantic-segmented depth images and introduce a new multiple-view discriminator for learning the complete scene from these images. Our method reduces the workload of modeling and acquiring 3D scenes for training and produces more accurate object shapes and layouts. We evaluate our approach on different indoor scene datasets and demonstrate its advantages. Additionally, we extend our method to generate 3D indoor scenes from semantic-segmented depth images inferred from RGB images of real scenes.