Recent advancements in image-to-image (I2I) translation have resulted in the generation of high-quality and realistic images. However, when applied to small domains, this technique still faces significant challenges. Existing methods rely on transfer learning for I2I translation but require the learning of a large number of parameters from scratch, which limits its application in small domains. To address this limitation, we propose a new transfer learning approach called TransferI2I.Our approach decouples the learning process into two steps: image generation and I2I translation. In the first step, we introduce two novel techniques: source-target initialization and self-initialization of the adaptor layer. Source-target initialization fine-tunes a pretrained generative model (such as StyleGAN) using both the source and target data, enhancing the initialization for the I2I translation step. On the other hand, self-initialization allows the initialization of non-pretrained network parameters without requiring any data. These techniques contribute to a better initialization for I2I translation.Additionally, we introduce an auxiliary generative adversarial network (GAN) that further improves the training of deep I2I systems, even with small datasets. Through extensive experiments on three datasets (Animal faces, Birds, and Foods), we demonstrate that our proposed method outperforms existing approaches. In particular, the metric mFID (mean Fr√©chet Inception Distance) improves by over 25 points on several datasets.Our code for TransferI2I is publicly available at: https://github.com/yaxingwang/TransferI2I.