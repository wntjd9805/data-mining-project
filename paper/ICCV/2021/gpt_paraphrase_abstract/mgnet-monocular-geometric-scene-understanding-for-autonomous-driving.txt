MGNet is a novel framework that tackles the task of monoc-ular geometric scene understanding by combining panoptic segmentation and self-supervised monocular depth estimation. Panoptic segmentation provides a comprehensive understanding of the scene by capturing both semantic and instance-based information. On the other hand, self-supervised monocular depth estimation utilizes geometric constraints to measure depth solely from monocular video sequences. Our proposed model is the first to integrate these two tasks into a single framework. It is designed to prioritize low latency, enabling real-time inference on a consumer-grade GPU. During deployment, our model generates dense 3D point clouds with semantic labels that are aware of instance-level details from single high-resolution camera images. To evaluate the performance of our model, we conducted experiments on two well-known autonomous driving benchmarks, Cityscapes and KITTI, and achieved competitive results compared to other real-time capable methods. The source code for our framework can be accessed at https://github.com/markusschoen/MGNet.