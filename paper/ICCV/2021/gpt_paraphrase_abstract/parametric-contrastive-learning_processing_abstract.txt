This paper introduces a method called Parametric Contrastive Learning (PaCo) to address the challenge of long-tailed recognition. The authors observe that the commonly used supervised contrastive loss tends to favor high-frequency classes, making it difficult to learn from imbalanced data. To overcome this bias, they propose the use of parametric class-wise learnable centers, which helps rebalance the learning process. Through theoretical analysis, they show that PaCo enhances the effectiveness of pushing samples of the same class closer together, particularly as more samples are pulled together with their corresponding centers. This leads to improved learning of hard examples. The authors conduct experiments on long-tailed datasets such as CIFAR, ImageNet, Places, and iNaturalist 2018, and demonstrate that their approach achieves state-of-the-art performance. Specifically, on the full ImageNet dataset, models trained with PaCo outperform those trained with supervised contrastive learning, achieving a top-1 accuracy of 81.8% using ResNet-200. The code for their method is publicly available at the provided GitHub link.