We introduce EgoRenderer, a system that can create realistic full-body avatars of individuals using egocentric fisheye camera footage. This system is capable of rendering lifelike images from various camera angles. However, rendering avatars from egocentric images presents unique challenges due to the top-down perspective and distortions. To overcome these challenges, we divide the rendering process into several steps, including texture synthesis, pose construction, and neural image translation.To address texture synthesis, we propose Ego-DPNet, a neural network that identifies dense correspondences between the fisheye images and a parametric body model. This network also extracts textures from the egocentric inputs. Additionally, to capture dynamic appearances, our approach incorporates an implicit texture stack that captures detailed variations in appearance across different poses and viewpoints.For accurate pose generation, we initially estimate the body pose based on the egocentric view using a parametric model. Then, we synthesize an external free-viewpoint pose image by projecting the parametric model to the desired viewpoint specified by the user. We combine the target pose image with the textures to create a combined feature image, which is further transformed into the final color image using a neural image translation network.Through experimental evaluations, we demonstrate that EgoRenderer is capable of generating realistic free-viewpoint avatars of individuals wearing an egocentric camera. Furthermore, by comparing our approach to several baseline methods, we highlight the advantages of our system.