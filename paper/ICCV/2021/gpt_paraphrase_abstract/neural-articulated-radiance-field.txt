We introduce a new method called Neural Articulated Radiance Field (NARF) for representing 3D articulated objects using deformable models learned from images. While recent advancements in 3D implicit representation have enabled the learning of complex object models, representing articulated objects with controllable poses remains challenging. Existing methods require supervision of 3D shape and cannot render appearance. Our approach formulates an implicit representation of 3D articulated objects by considering only the rigid transformation of the most relevant object part, allowing for pose-dependent changes without significantly increasing computational complexity. NARF is fully differentiable and can be trained using pose annotations from images. Additionally, it can learn appearance variations using an autoencoder across multiple instances of the same object class. Experimental results demonstrate the efficiency and generalization capabilities of our proposed method to novel poses. The code for NARF is publicly available for research purposes at https://github.com/nogu-atsu/NARF.