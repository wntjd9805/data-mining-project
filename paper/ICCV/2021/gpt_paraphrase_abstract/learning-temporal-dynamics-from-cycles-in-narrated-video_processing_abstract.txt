Learning how the world changes over time has been a challenging problem in computer vision. In this study, we propose a self-supervised approach that addresses this problem by simultaneously solving a multi-modal temporal cycle consistency objective in both vision and language. This objective requires the model to learn functions that can predict the future and past in a modality-agnostic manner, with the predictions cancelling each other out when combined. We believe that training a model on this objective will enable it to discover long-term temporal dynamics in videos. To test this hypothesis, we use the resulting visual representations and predictive models without further training on target datasets or their labels, and evaluate their performance on various downstream tasks. Our method surpasses the performance of state-of-the-art self-supervised video prediction methods on tasks such as future action anticipation, temporal image ordering, and arrow-of-time classification.