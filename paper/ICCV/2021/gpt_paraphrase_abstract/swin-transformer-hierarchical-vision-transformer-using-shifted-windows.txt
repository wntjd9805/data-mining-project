This paper introduces a novel vision Transformer called Swin Transformer, which serves as a versatile backbone for computer vision tasks. Adapting Transformer from language to vision poses challenges due to differences between the two domains, such as varying scale of visual entities and high pixel resolution in images. To overcome these differences, the authors propose a hierarchical Transformer that utilizes Shifted windows for representation computation. This approach enhances efficiency by limiting self-attention computation to non-overlapping local windows while enabling cross-window connections. The hierarchical architecture allows modeling at different scales and has a linear computational complexity with respect to image size. Swin Transformer demonstrates remarkable performance in various vision tasks, including image classification (achieving 87.3% top-1 accuracy on ImageNet-1K), object detection (achieving 58.7 box AP and 51.1 mask AP on COCO test-dev), and semantic segmentation (achieving 53.5 mIoU on ADE20K val). It outperforms previous state-of-the-art models by a significant margin. The hierarchical design and shifted window approach also prove beneficial for all-MLP architectures. The code and models of Swin Transformer are publicly available at https://github.com/microsoft/Swin-Transformer.