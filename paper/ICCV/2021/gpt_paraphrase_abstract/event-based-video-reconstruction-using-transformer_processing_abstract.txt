Event cameras, which detect spatio-temporal brightness changes, offer a new approach to image sensors with high dynamic range and low latency. Prior research has shown impressive results in event-based video reconstruction using convolutional neural networks (CNNs). However, CNNs have limitations in modeling long-range dependencies, which are crucial for certain vision tasks. To address this, this paper presents a hybrid CNN-Transformer network called ET-Net for event-based video reconstruction. ET-Net combines the fine local information from CNNs with the global contexts captured by Transformers. Additionally, a Token Pyramid Aggregation strategy is introduced to integrate multi-scale tokens, allowing for the relationship between internal and intersected semantic concepts in the token-space. Experimental results demonstrate that the proposed method outperforms state-of-the-art techniques on multiple real-world event datasets. The code for ET-Net is publicly available at https://github.com/WarranWeng/ET-Net.