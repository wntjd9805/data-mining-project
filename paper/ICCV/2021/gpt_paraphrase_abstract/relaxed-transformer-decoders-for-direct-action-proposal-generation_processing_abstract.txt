Temporal action proposal generation is a challenging task in video understanding that involves detecting temporal segments containing action instances of interest. Existing approaches rely on pre-defined anchor windows or heuristic boundary matching strategies. This paper introduces a straightforward and effective framework called RTD-Net for direct action proposal generation. The framework repurposes a Transformer-like architecture and makes three key improvements to address the visual differences between time and space. Firstly, a boundary attentive module replaces the original Transformer encoder to better capture long-range temporal information. Secondly, a relaxed matching scheme is proposed to handle ambiguous temporal boundaries and sparse annotations. This scheme alleviates the strict criteria of single assignment to each ground truth. Finally, a three-branch head is devised to enhance proposal confidence estimation by explicitly predicting completeness. Experiments conducted on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of RTD-Net in both temporal action proposal generation and temporal action detection tasks. Additionally, the framework is more efficient than previous methods as it does not require non-maximum suppression post-processing. The code and models of RTD-Net are available at https://github.com/MCG-NJU/RTD-Action.