Multi-source Domain Adaptation (MSDA) presents practical but challenging obstacles compared to traditional unsupervised domain adaptation due to the involvement of diverse data sources. There are two primary challenges in MSDA: (i) addressing the diversity present in multiple source domains, and (ii) managing the data shift between the target domain and the source domains. This study proposes a theoretical approach to tackle the first challenge by combining domain experts trained on their respective source domains to create a combined multi-source teacher that performs well on the mixture of source domains. To overcome the second challenge, the study suggests bridging the gap between the target domain and the mixture of source domains in the latent space using a generator or feature extractor. Along with bridging the gap in the latent space, a student model is trained to mimic the predictions of the teacher expert on both source and target examples. Additionally, the proposed approach is supported by rigorous theory that offers insightful justifications for how each component influences the transfer performance. The effectiveness of the proposed method is validated through extensive experiments conducted on three benchmark datasets, demonstrating state-of-the-art performance.