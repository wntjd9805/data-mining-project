Weakly supervised object localization (WSOL) is a difficult problem in which image category labels are provided but object localization models need to be learned. When optimizing a convolutional neural network (CNN) for classification, it tends to activate only local discriminative regions and ignores the complete extent of the object, resulting in partial activation. This issue arises from the CNN's intrinsic characteristics, as convolution operations generate local receptive fields and struggle to capture long-range feature dependencies among pixels. To address this, we propose the token semantic coupled attention map (TS-CAM), which leverages the self-attention mechanism in visual transformers to extract long-range dependencies. TS-CAM splits an image into patch tokens for spatial embedding, producing attention maps that capture long-range visual dependencies and prevent partial activation. It then assigns category-related semantics to the patch tokens, enabling them to be aware of object categories. Finally, TS-CAM combines the patch tokens with a semantic-agnostic attention map to achieve semantic-aware localization. Experimental results on the ILSVRC/CUB-200-2011 datasets demonstrate that TS-CAM outperforms CNN-CAM approaches by 7.1%/27.1% in WSOL, achieving state-of-the-art performance. The code for TS-CAM is available at https://github.com/vasgaowei/TS-CAM.