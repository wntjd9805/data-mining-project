We present DeepMultiCap, a new approach for capturing the movements of multiple individuals using sparse multi-view cameras. Our method can accurately capture the changing details of surfaces without relying on pre-scanned template models. To address the challenge of occlusion in close interaction scenes, we combine a pixel-aligned implicit function with a parametric model to reconstruct the hidden surface areas robustly. We also introduce an attention-aware module that extracts fine-grained geometry details from multi-view images, resulting in high-quality results. Additionally, for video inputs, we propose a novel temporal fusion method to reduce noise and temporal inconsistencies during character reconstruction. To evaluate our method, we introduce the MultiHuman dataset, which comprises 150 static scenes with varying levels of occlusion and includes ground truth 3D human models. Experimental results demonstrate that our method outperforms previous works by a significant margin in terms of performance and generalization to real multi-view video data.