Generalized zero-shot learning (GZSL) has made significant progress in addressing the challenges of visual-semantic domain gap and seen-unseen bias. However, existing methods often neglect the cross-dataset bias between ImageNet and GZSL benchmarks, resulting in poor-quality visual features for GZSL tasks. This limitation affects the recognition performance of both seen and unseen classes. To overcome this problem, we propose a straightforward yet effective method called feature refinement for generalized zero-shot learning (FREE). FREE utilizes a feature refinement module that incorporates semantic-visual mapping into a unified generative model to enhance the visual features of seen and unseen class samples. Additionally, we introduce a self-adaptive margin center loss (SAMC-loss) that works together with a semantic cycle-consistency loss to guide the feature refinement module in learning class- and semantically-relevant representations. The refined features are obtained by concatenating the features within the feature refinement module. Extensive experiments conducted on five benchmark datasets demonstrate the significant performance improvement of FREE compared to baseline and state-of-the-art methods. The code for FREE is available at https://github.com/shiming-chen/FREE.