Most existing methods for weakly supervised semantic segmentation (WSSS) involve generating pseudo-masks and training the segmentation model using these masks in a fully supervised manner. However, there are several challenges related to the quality of the pseudo-masks and the presence of noise in the training process. In order to address these challenges and improve the performance of WSSS, we propose the following strategies: 

1. Coefficient of Variation Smoothing: This strategy aims to smooth the class activation maps (CAMs) adaptively, enhancing the quality of the pseudo-masks.
2. Proportional Pseudo-mask Generation: Instead of relying on scores from binary classifiers, we introduce a new metric that indicates the importance of each class at each location. This metric is used to project the expanded CAMs onto pseudo-masks. 
3. Pretended Under-Fitting strategy: This strategy aims to mitigate the influence of noise in the pseudo-masks by intentionally under-fitting the segmentation model during training.
4. Cyclic Pseudo-mask: This strategy involves boosting the pseudo-masks during the training of fully supervised semantic segmentation (FSSS), leading to improved performance.

Experimental results using our proposed methods demonstrate state-of-the-art performance on two challenging weakly supervised semantic segmentation datasets, achieving mIoU scores of 70.0% on PAS-CAL VOC 2012 and 40.2% on MSCOCO 2014. We have released the codes, including the segmentation framework, on GitHub at https://github.com/Eli-YiLi/PMM.