Change captioning tasks involve detecting changes in image pairs and generating a description of the changes. Previous studies have focused on single changes, but it is important to be able to detect and describe multiple changed parts in complex scenarios. To address this, we propose a simulation-based multi-change captioning dataset and benchmark existing methods on this task. We also introduce Multi-Change Captioning transformers (MCCFormers) that use correlation between different regions in image pairs to identify change regions and dynamically determine the relevant change regions in sentences. Our method achieves the highest scores on conventional evaluation metrics for multi-change captioning and performs well in change localization. It also outperforms previous state-of-the-art methods on the CLEVR-Change benchmark by a significant margin, demonstrating its general ability in change captioning tasks. The code and dataset are available for access on the project page.