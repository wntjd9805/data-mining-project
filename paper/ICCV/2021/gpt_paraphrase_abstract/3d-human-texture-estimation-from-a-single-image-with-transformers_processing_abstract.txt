We introduce a novel approach using a Transformer-based framework to estimate 3D human texture from a single image. This framework effectively utilizes global information from the input image, overcoming the limitations of existing methods that rely solely on convolutional neural networks. Additionally, we propose a mask-fusion strategy to combine the strengths of both RGB-based and texture-flow-based models. To ensure high-fidelity color reconstruction without introducing artifacts, we incorporate a part-style loss. Through extensive experiments, we demonstrate the superior performance of our method compared to state-of-the-art 3D human texture estimation approaches, both quantitatively and qualitatively. Further information can be found on our project page: https://www.mmlab-ntu.com/project/texformer.