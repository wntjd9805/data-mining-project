This paper presents a new training method for addressing imbalanced data learning problems. The method involves using a new loss during the training phase, which reduces the impact of samples that lead to an overfitted decision boundary. The proposed loss improves the performance of various imbalance learning methods and outperforms existing cost-sensitive loss methods in experiments conducted on multiple benchmark datasets. Additionally, the proposed loss can be easily combined with other recent techniques, such as re-sampling, meta-learning, and cost-sensitive learning methods, for tackling class-imbalance problems. The code for implementing the proposed method is available at https://github.com/pseulki/IB-Loss.