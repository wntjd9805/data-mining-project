We address the problem of generating stylized images of a 3D scene from different viewpoints using a set of images of the scene and a reference image with the desired style. Existing methods that combine novel view synthesis and stylization often produce blurry or inconsistent results across views. To overcome this, we propose a point cloud-based approach for achieving consistent 3D scene stylization. First, we create a point cloud by projecting the image features into 3D space. Then, we use point cloud aggregation modules to collect style information from the 3D scene and adjust the features using a linear transformation matrix. Finally, we project the transformed features back into 2D space to generate the novel views. Our experimental results on two diverse real-world scene datasets demonstrate that our method produces consistent stylized novel view synthesis results compared to other approaches.