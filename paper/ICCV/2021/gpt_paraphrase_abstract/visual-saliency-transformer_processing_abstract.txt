Current state-of-the-art methods for detecting saliency heavily rely on CNN-based architectures. In contrast, we propose a new approach that considers saliency detection as a sequence-to-sequence task without the use of convolutions. Our method, called Visual Saliency Transformer (VST), utilizes a pure transformer model to capture long-range dependencies, which are not achievable through convolutions. We apply VST to both RGB and RGB-D salient object detection (SOD) by inputting image patches and utilizing the transformer to propagate global contexts among them. Unlike existing architectures like Vision Transformer (ViT), we introduce multi-level token fusion and a novel token upsampling method within the transformer framework to obtain high-resolution detection results. Additionally, we develop a token-based multi-task decoder that performs saliency and boundary detection simultaneously by incorporating task-related tokens and a patch-task-attention mechanism. Our experimental results demonstrate that our model surpasses existing methods on benchmark datasets for both RGB and RGB-D SOD. Moreover, our framework not only offers a fresh perspective for the SOD field but also presents a new paradigm for transformer-based dense prediction models. The code for our method is available at https://github.com/nnizhang/VST.