This paper examines the issue of learning self-supervised representations in videos. Unlike images, videos require understanding the relationships between multiple frames along the temporal dimension. However, existing contrastive-based self-supervised frameworks fail to explicitly capture these relations as they only compare the distance between two augmented clips from the same video without considering their temporal order. To address this, we propose a framework called contrast-and-order representation (CORP) for learning self-supervised video representations. CORP can automatically capture both the appearance information within each frame and the temporal information across different frames. Our model first predicts if two video clips come from the same input video and then predicts the temporal ordering of the clips if they do. We also introduce a decoupling attention method to learn symmetric similarity (contrast) and anti-symmetric patterns (order). This design does not require additional parameters or computation, yet it accelerates the learning process and improves accuracy compared to vanilla multi-head attention. We extensively validate the representation ability of our learned video features for the downstream action recognition task on Kinetics-400 and Something-something V2 datasets. Our method significantly outperforms previous state-of-the-art approaches.