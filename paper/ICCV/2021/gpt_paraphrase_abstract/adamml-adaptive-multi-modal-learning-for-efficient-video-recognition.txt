This study introduces AdaMML, an adaptive multi-modal learning framework for video recognition. Traditional multi-modal learning has shown promising recognition results, but its computational cost hinders its practical application. AdaMML addresses this issue by dynamically selecting the optimal modalities for each video segment, based on the input, to improve both accuracy and efficiency. The framework incorporates a multi-modal policy network to determine the modalities for processing by the recognition model. The policy network is jointly trained with the recognition model using back-propagation. Extensive experiments on diverse datasets reveal that AdaMML reduces computation by 35% to 55% compared to the traditional approach that uses all modalities regardless of the input. Additionally, AdaMML consistently outperforms state-of-the-art methods in terms of accuracy. For more information, please visit the project page: https://rpand002.github.io/adamml.html.