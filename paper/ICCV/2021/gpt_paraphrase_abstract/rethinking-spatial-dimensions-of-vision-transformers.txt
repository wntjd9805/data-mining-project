The Vision Transformer (ViT) is a new architecture that expands the use of transformers from language processing to computer vision tasks, challenging the existing convolutional neural networks (CNNs). However, there is limited research on designing effective architectures for ViT. To address this gap, we investigate the importance of spatial dimension conversion in the ViT architecture, inspired by the successful design principles of CNNs. We find that reducing spatial dimensions while increasing depth is beneficial for ViT, similar to CNNs. Based on this observation, we propose a novel architecture called Pooling-based Vision Transformer (PiT) as an improvement over ViT. Our experiments demonstrate that PiT outperforms ViT in terms of model capability, generalization performance, image classification, object detection, and robustness evaluation. The source codes and ImageNet models for PiT are available at https://github.com/naver-ai/pit.