The majority of monocular depth sensing methods currently in use rely on conventionally captured images that do not take into account the content of the scene. In contrast, animals possess the ability to rapidly move their eyes, known as saccades, in order to control how the scene is captured by their high-resolution fovea. This paper introduces the SaccadeCam framework, which aims to allocate resolution dynamically to regions of interest within the scene. Our approach utilizes a self-supervised network to adaptively distribute resolution, and we present promising results for end-to-end learning in monocular depth estimation. Additionally, we provide preliminary findings from a physical prototype of the SaccadeCam hardware.