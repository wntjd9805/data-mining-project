This paper introduces a new technique called gradient normalization (GN) to address the training instability issue in Generative Adversarial Networks (GANs) caused by sharp gradients. Unlike existing approaches such as gradient penalty and spectral normalization, GN applies a strict 1-Lipschitz constraint on the discriminator function, enhancing its capacity. Additionally, GN can be easily implemented in various GAN architectures with minimal modifications. Extensive experiments on four datasets demonstrate that GANs trained with gradient normalization outperform existing methods in terms of both Frechet Inception Distance and Inception Score.