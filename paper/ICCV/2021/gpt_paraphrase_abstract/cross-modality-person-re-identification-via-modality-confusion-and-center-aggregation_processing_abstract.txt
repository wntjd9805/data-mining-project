Cross-modality person re-identification is a difficult task due to differences in appearance across different modalities and variations within each modality. Existing methods typically focus on learning features specific to each modality or features that can be shared across modalities using identity supervision or modality labels. This paper proposes a new approach called Modality Confusion Learning Network (MCLNet) which aims to confuse the two modalities in order to prioritize the modality-irrelevant perspective during optimization. MCLNet learns modality-invariant features by simultaneously reducing the differences between modalities and maximizing the similarities across modalities within a single framework. Additionally, the paper introduces an identity-aware marginal center aggregation strategy to extract centralized features while maintaining diversity through a marginal constraint. Furthermore, a camera-aware learning scheme is designed to enhance discriminability. Experimental results on SYSU-MM01 and RegDB datasets demonstrate that MCLNet significantly outperforms existing methods. On the large-scale SYSU-MM01 dataset, our model achieves a Rank-1 accuracy of 65.40% and an mAP value of 61.98%.