Deep learning has greatly advanced video-based Continuous Sign Language Recognition (CSLR) in recent years. The current approach involves a visual module that focuses on spatial and short-temporal information, followed by a contextual module that focuses on long-temporal information. However, the visual module is difficult to optimize due to limitations in back-propagation's chain rules. This results in the contextual module prioritizing contextual information optimization over efficient visual and contextual information balance. To address this issue, we propose a method called Self-Mutual Knowledge Distillation (SMKD). SMKD enhances the discriminative power of both the visual and contextual modules by enforcing them to focus on short-term and long-term information simultaneously. In this method, the visual and contextual modules share the weights of their corresponding classifiers and train with the Connectionist Temporal Classification (CTC) loss. Additionally, we address the spike phenomenon that occurs with CTC loss, which causes saturation in the visual module and drops frames in a gloss. To alleviate this issue, we introduce a gloss segmentation technique. We evaluate our proposed method on two CSLR benchmarks, PHOENIX14 and PHOENIX14-T, and the experimental results demonstrate the effectiveness of SMKD.