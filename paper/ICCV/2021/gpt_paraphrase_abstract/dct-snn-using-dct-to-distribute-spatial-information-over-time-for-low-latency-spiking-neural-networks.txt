Spiking Neural Networks (SNNs) offer a more efficient approach to deep learning by utilizing event-driven information processing. However, current input coding schemes for SNNs, like Poisson based rate-coding, do not effectively exploit the temporal learning capabilities of SNNs. Additionally, SNNs suffer from high inference latency, limiting their practicality. To address these issues, we propose a time-based encoding scheme called DCT-SNN that utilizes Discrete Cosine Transform (DCT) to reduce the number of timesteps required for inference. DCT decomposes an image into a weighted sum of sinusoidal basis images, and at each time step, a single frequency base, modulated by its corresponding DCT coefficient, is input to an accumulator that generates spikes when a threshold is crossed. We train DCT-SNN using leaky-integrate-and-fire neurons and surrogate gradient descent based backpropagation. Our experiments on CIFAR-10, CIFAR-100, and TinyImageNet datasets using VGG architectures demonstrate top-1 accuracies of 89.94%, 68.30%, and 52.43%, respectively. Notably, DCT-SNN achieves inference with 2-14X reduced latency compared to other state-of-the-art SNNs while maintaining comparable accuracy to traditional deep learning methods. The dimension of the transform allows us to control the number of timesteps required for inference, and we can trade-off accuracy with latency by dropping the highest frequency components during inference. The code for our proposed scheme is publicly available.