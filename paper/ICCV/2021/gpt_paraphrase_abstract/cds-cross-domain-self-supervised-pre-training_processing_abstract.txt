We introduce a novel two-stage pre-training method called Cross-Domain Self-supervision (CDS) to enhance the generalization capability of traditional single-domain pre-training. Although pre-training on a large single dataset like ImageNet can offer a solid initial representation for transfer learning, it may produce biased representations that hinder learning with new multi-domain data through domain adaptation techniques. To address this, our CDS approach utilizes unlabeled multi-domain data for downstream domain transfer tasks. It incorporates self-supervision within and across domains, employing in-domain instance discrimination to learn discriminative features in a domain-adaptive manner and cross-domain matching to acquire domain-invariant features. By applying CDS as a second pre-training step, following ImageNet pre-training, we observe a significant improvement in target accuracy for various domain transfer tasks compared to standard one-stage pre-training.