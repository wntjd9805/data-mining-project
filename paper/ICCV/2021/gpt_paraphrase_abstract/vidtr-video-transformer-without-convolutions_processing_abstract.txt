We present the Video Transformer (VidTr) with separable-attention for video classification, which outperforms 3D networks by efficiently aggregating spatio-temporal information. Initially, we introduce the vanilla video transformer, which effectively models spatio-temporal patterns but consumes substantial memory. To address this, we introduce VidTr, which reduces memory usage by 3.3Ã— while maintaining performance. Additionally, we propose a standard deviation based topK pooling for attention (pooltopK std) to further optimize the model by disregarding non-informative features along the temporal dimension, thereby reducing computation. VidTr achieves state-of-the-art performance on five widely used datasets with lower computational requirements, demonstrating the efficiency and effectiveness of our design. Moreover, error analysis and visualization indicate that VidTr excels in predicting actions that involve long-term temporal reasoning.