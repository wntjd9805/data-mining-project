We introduce DietNeRF, a 3D neural scene representation that can be estimated from a small number of images. While Neural Radiance Fields (NeRF) can reconstruct scenes accurately with a large number of images, they struggle with few-shot scenarios. To address this, we propose DietNeRF, which incorporates an auxiliary semantic consistency loss to improve the quality of few-shot renderings. DietNeRF is trained to render input views correctly and match high-level semantic attributes across different poses. We achieve this by using a pre-trained visual encoder like CLIP to extract semantics. Experimental results demonstrate that DietNeRF enhances the perceptual quality of few-shot view synthesis, can generate novel views with only one observed image when pre-trained on a multi-view dataset, and can generate plausible completions for unobserved regions. More information can be found on our project website: https://www.ajayj.com/dietnerf.