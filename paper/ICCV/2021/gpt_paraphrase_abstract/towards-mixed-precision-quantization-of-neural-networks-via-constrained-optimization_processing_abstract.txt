We present a new approach to address the mixed-precision quantization problem, which aims to compress and accelerate deep neural networks. Existing quantization methods typically use the same bit-width for all or most layers, leading to accuracy loss in ultra-low precision scenarios. However, recent hardware accelerators support mixed-precision computation, and we propose a principled framework to leverage this capability. Our method formulates the mixed-precision quantization as a discrete constrained optimization problem. To make the optimization feasible, we approximate the objective function using a second-order Taylor expansion and develop an efficient method to compute its Hessian matrix. By simplifying the problem, we demonstrate that it can be reformulated as a Multiple-Choice Knapsack Problem (MCKP). We also propose a greedy search algorithm to solve the MCKP efficiently. Compared to existing mixed-precision quantization techniques, our approach is derived in a principled manner and offers higher computational efficiency. We conduct extensive experiments on the ImageNet dataset using various network architectures to demonstrate the superiority of our method over existing uniform and mixed-precision quantization methods.