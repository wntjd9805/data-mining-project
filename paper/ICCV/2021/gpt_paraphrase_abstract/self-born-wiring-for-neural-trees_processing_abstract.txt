Neural trees aim to combine the strengths of deep neural networks and decision trees, including representation learning and faster inference. This paper introduces a new approach called Self-born Wiring (SeBoW) to learn neural trees from a deep neural network. Unlike previous methods, SeBoW constructs task-adaptive neural trees through a step-by-step process, allowing for global-level parameter optimization. By disconnecting and isolating filter groups from a designated network configuration, SeBoW then conducts a wiring process to create a lightweight neural tree. Extensive experiments show that SeBoW outperforms previous neural tree approaches with lower computational cost and achieves comparable results to non-tree networks like ResNets. Additionally, SeBoW demonstrates scalability on large-scale datasets like ImageNet, which has been largely unexplored by previous tree networks.