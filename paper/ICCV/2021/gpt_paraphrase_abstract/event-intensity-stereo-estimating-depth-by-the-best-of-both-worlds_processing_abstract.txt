Event cameras capture scene movements as a continuous stream of data known as events. These cameras have advantages over traditional cameras, including low latency, high dynamic range, and low power consumption. However, event cameras only generate output when there is movement in the scene. In contrast, traditional cameras capture complete frames at fixed intervals but have lower dynamic range and are susceptible to motion blur. To leverage the strengths of both camera types, we propose a design that combines events and intensity images to estimate dense disparity. Our end-to-end approach sequentially integrates events and images to correlate them and calculate accurate depth values with fine details. We conducted experiments in real-world and simulated scenarios, demonstrating the superiority of our method. We also extended our method to handle extreme cases, such as missing event or stereo pairs and inconsistent dynamic ranges or event thresholds.