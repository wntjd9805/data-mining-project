Developing an interactive AI system capable of asking questions about the real world is a significant challenge in the field of vision and language. Specifically, goal-oriented visual dialogue, where the AI agent seeks information through question-asking during a conversation, has recently gained attention. However, existing models based on the GuessWhat?! dataset mainly ask simple category-based or absolute spatial questions, which can be problematic for complex scenes or when descriptive questions are needed to distinguish objects. This paper introduces a new architecture called Unified Questioner Transformer (UniQer) that focuses on generating descriptive questions with referring expressions. To evaluate the performance of UniQer, a goal-oriented visual dialogue task called CLEVR Ask is created, which involves synthesizing complex scenes that require the Questioner to generate descriptive questions. The model is trained on two variants of the CLEVR Ask datasets. Quantitative and qualitative evaluations demonstrate that UniQer outperforms the baseline model. The answer format of the system only provides abstract information.