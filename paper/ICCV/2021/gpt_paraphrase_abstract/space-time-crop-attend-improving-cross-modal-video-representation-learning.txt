The quality of image representations acquired through self-supervised learning is heavily influenced by the type of data augmentations used. While recent studies have shown that leveraging both audio and video signals improves results when applied to videos, it has been observed that spatial augmentations, such as cropping, which are crucial for still images, do not work as effectively for videos. This paper introduces two unique improvements for video representations. Firstly, it demonstrates that spatial augmentations, like cropping, can indeed be effective for videos, but previous implementations were limited by processing and memory costs. To address this limitation, the paper proposes Feature Crop, a more efficient method to simulate such augmentations directly in feature space. Secondly, the paper shows that transformer-based attention, as opposed to simple average pooling, significantly enhances performance and is suitable for processing feature crops. By combining these two advancements into a new method called Space-Time Crop & Attend (STiCA), the authors achieve state-of-the-art performance on video-representation learning benchmarks. Specifically, they achieve accuracies of 67.0% on HMDB-51 and 93.1% on UCF-101 when pre-training on Kinetics-400. The paper also provides access to code and pretrained models. The results indicate that STiCA outperforms recent cross-modal self-supervised learning methods (XDC, GDT, AVID-CMA, SeLaVi) pre-trained on Kinetics-400, achieving better results in fewer epochs. Figure 1 illustrates the accuracy of STiCA on HMDB-51 over epochs, highlighting the significant benefits in both performance and speed compared to other cropping methods or the default cross-modal only loss.