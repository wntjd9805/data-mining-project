Generating scene graphs is a crucial task in computer vision that aims to improve our understanding of the visual world by detecting objects and predicting the relationships between them. Existing models often predict these relationships independently, but we propose a conditional approach inspired by the sequence-to-sequence framework. Unlike previous research, our model predicts visual relationships one at a time in an autoregressive manner, conditioning on the relationships already predicted. To accomplish this, we utilize an encoder-decoder model built using Transformers, where the encoder captures global context and long-range interactions, and the decoder makes sequential predictions based on the constructed scene graph. Additionally, we introduce a novel reinforcement learning-based training strategy specifically designed for sequence-to-sequence scene graph generation. By employing a self-critical policy gradient training approach with Monte Carlo search, we directly optimize for recall metrics, effectively bridging the gap between training and evaluation. Our experimental results on two benchmark datasets demonstrate that our sequence-to-sequence learning approach achieves superior empirical performance compared to previous state-of-the-art methods, while also being efficient in terms of training and inference time. The full code for our work can be accessed at https://github.com/layer6ai-labs/SGG-Seq2Seq.