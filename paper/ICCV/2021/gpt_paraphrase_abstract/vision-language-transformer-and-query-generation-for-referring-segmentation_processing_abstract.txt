This study focuses on the challenging task of referring segmentation, where the goal is to identify a specific object in an image based on its relationship with other objects. To accomplish this, the model needs a holistic understanding of the entire image. To address this, we propose a direct attention approach using transformer and multi-head attention. This approach allows the model to query the image using the given language expression and identify the region that is most attended to. Additionally, we introduce a Query Generation Module that produces multiple sets of queries with different attention weights, representing different understandings of the language expression. To select the best output features from these queries based on visual cues, we propose a Query Balance Module. Our approach achieves state-of-the-art performance on three referring segmentation datasets (RefCOCO, RefCOCO+, and G-Ref) and is lightweight. The code for our approach is available at https://github.com/henghuiding/Vision-Language-Transformer.