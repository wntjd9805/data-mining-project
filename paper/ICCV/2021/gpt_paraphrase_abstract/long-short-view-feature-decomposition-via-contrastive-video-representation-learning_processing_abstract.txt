Self-supervised methods for video representation typically focus on capturing temporal attributes in videos. However, the distinction between stationary and non-stationary attributes has been less explored. Stationary features, which remain consistent throughout the video, are useful for predicting video-level action classes. On the other hand, non-stationary features, representing temporally changing attributes, are more beneficial for tasks requiring fine-grained temporal understanding, like action segmentation. We argue that using a single representation to capture both types of features is not optimal. Therefore, we propose a novel approach that decomposes the representation space into stationary and non-stationary features. This is achieved through contrastive learning from long and short views, where long video sequences and their shorter sub-sequences are utilized. The stationary features are shared between the short and long views, while the non-stationary features aggregate the short views to match the corresponding long view. To validate our approach, we conduct experiments and demonstrate that the stationary features perform well in action recognition tasks, whereas the non-stationary features excel in action segmentation. Additionally, we analyze the learned representations and observe that the stationary features capture more temporally stable and static attributes, while the non-stationary features encompass more temporally varying attributes.