Recent research has shown that deep neural networks are susceptible to adversarial examples, where small intentional perturbations cause the network to misclassify inputs. This vulnerability poses a risk for security-related applications, such as semantic segmentation in autonomous cars, and raises concerns about the reliability of the models. In this study, we evaluate the robustness of existing unsupervised domain adaptation (UDA) methods in semantic segmentation and propose a new robust UDA approach. We find that the robustness of UDA methods in semantic segmentation has not been explored, which is a security concern in this field. Additionally, commonly used self-supervision techniques, like rotation and jigsaw, improve model robustness in classification and recognition tasks but fail to provide the necessary supervision signals for semantic segmentation. Based on these observations, we introduce a new approach called adversarial self-supervision UDA (ASSUDA), which maximizes agreement between clean images and their adversarial examples using a contrastive loss in the output space. Extensive experiments on popular benchmarks demonstrate that ASSUDA is resilient to adversarial attacks.