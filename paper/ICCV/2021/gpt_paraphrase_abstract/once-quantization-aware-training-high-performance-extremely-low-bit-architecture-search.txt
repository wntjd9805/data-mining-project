Quantization Neural Networks (QNN) have gained attention for their high efficiency. Previous approaches focused on improving quantization accuracy through advanced algorithms, but struggled to achieve satisfactory results with extremely low bit-widths. To address this, we propose combining Network Architecture Search with quantization to leverage the benefits of both methods. However, a simple combination would result in excessive time consumption or unstable training. To overcome these issues, we introduce joint training of architecture and quantization with a shared step size, generating numerous quantized models. We then implement a bit-inheritance scheme to transfer these models to lower bit-widths, reducing time cost and improving quantization accuracy. This framework, called Once Quantization-Aware Training (OQAT), leads to our searched model family, OQATNets, achieving state-of-the-art performance across various architectures and bit-widths. For instance, OQAT-2bit-M achieves 61.6% ImageNet Top-1 accuracy, outperforming the 2-bit counterpart MobileNetV3 by a significant margin of 9% while using 10% less computation. Our approach enables easy identification of quantization-friendly architectures and allows for extensive analysis of the interaction between quantization and neural architectures. The codes and models are available at https://github.com/LaVieEnRoseSMZ/OQA.