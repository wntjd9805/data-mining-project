The effectiveness of personal robots in reliably navigating to a specific goal is crucial. Previous studies have used PointGoal navigation in simulated Embodied AI environments to investigate this task. While recent advancements have achieved near-perfect accuracy in this navigation task (99.6% success) in noiseless and perfect condition simulations, realistic noise models for visual sensors and actuation pose challenges. Additionally, the absence of a "GPS and Compass sensor" further reduces the success rate to 0.3%. In this research, we explore the surprising effectiveness of visual odometry in PointGoal navigation under realistic conditions, considering noise models for perception and actuation without GPS and Compass sensors. Our findings demonstrate that integrating visual odometry techniques into navigation policies significantly improves the state-of-the-art on the widely used HabitatPointNav benchmark. The success rate increases from 64.5% to 71.7%, and the execution speed improves by 6.4 times.