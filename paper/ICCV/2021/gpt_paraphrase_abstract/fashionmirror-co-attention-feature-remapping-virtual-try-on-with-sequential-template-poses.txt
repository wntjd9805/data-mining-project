Virtual try-on tasks have become increasingly popular, but existing methods have limitations. Previous approaches focus on warping clothes and fusing information at the pixel level through semantic segmentation, which is time-consuming and prone to errors. Additionally, this pixel-level approach limits the performance and stability of the results. In contrast, fusing information at the feature level allows for further refinement using convolution. In this study, we propose a co-attention feature-remapping framework called FashionMirror. This framework generates try-on results in two stages: first, predicting the removed mask and try-on clothing mask using the source human image and target clothes, eliminating the need for semantic segmentation and reducing inference time. Second, removing the clothes from the source human using the removed mask and warping the clothing features based on the try-on clothing mask to fit the next frame human. Optical flows are predicted from consecutive 2D poses to warp the source human at the feature level. The clothing and source human features are enhanced in each frame to generate realistic try-on results with spatio-temporal smoothness. Both qualitative and quantitative evaluations demonstrate that FashionMirror outperforms existing virtual try-on approaches.