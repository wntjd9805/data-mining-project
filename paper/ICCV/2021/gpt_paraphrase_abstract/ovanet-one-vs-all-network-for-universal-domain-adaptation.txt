Universal Domain Adaptation (UNDA) aims to address the challenges of both domain-shift and category-shift between two datasets. One of the main difficulties lies in transferring knowledge while distinguishing "unknown" classes that are absent in the labeled source data but present in the unlabeled target data. Current methods typically rely on manually setting a threshold to reject these "unknown" samples, either through validation or a predetermined ratio of "unknown" samples. However, this approach is not practical.  In this study, we propose a novel approach to learning the threshold using source samples and adapting it to the target domain. Our idea is that the minimum inter-class distance in the source domain can serve as an effective threshold for distinguishing between "known" and "unknown" in the target domain. To achieve this, we train a one-vs-all classifier for each class using labeled source data, allowing us to learn the inter- and intra-class distances. Subsequently, we adapt the open-set classifier to the target domain by minimizing class entropy.  Our proposed framework is the simplest among existing UNDA baselines, yet it outperforms them significantly. Moreover, it is insensitive to the value of a hyper-parameter, further enhancing its effectiveness. For those interested in implementation, the code can be found at https://github.com/VisionLearningGroup/OVANet.