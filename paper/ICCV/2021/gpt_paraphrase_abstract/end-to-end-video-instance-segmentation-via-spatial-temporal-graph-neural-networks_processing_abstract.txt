This abstract discusses the challenges of video instance segmentation and proposes a novel method to address these challenges. Existing methods either rely on single-frame information or handle tracking separately, limiting their ability to fully utilize spatial-temporal information. The proposed method uses a graph-neural-network (GNN) approach to overcome these limitations. Graph nodes represent instance features for detection and segmentation, while graph edges represent instance relations for tracking. The method effectively propagates and shares both inter and intra-frame information through graph updates, optimizing all subproblems (detection, segmentation, and tracking) in a unified framework. The performance of the proposed method is significantly improved compared to existing methods, achieving 36.5% average precision (AP) on the YoutubeVIS validation dataset with a ResNet-50 backbone, operating at 22 frames per second (FPS).