We investigate the machine's comprehension of embodied reference, where one agent utilizes both language and gesture to refer to an object in a shared physical environment to another agent. This novel visual task necessitates the understanding of multimodal cues and perspective-taking to identify the object being referred to. To address this challenge, we present YouRefIt, a crowd-sourced dataset comprising 4,195 distinct reference clips in 432 indoor scenes, which enables the study of referring expressions in everyday physical environments to comprehend referential behavior, human communication, and human-robot interaction. Additionally, we introduce two benchmarks for image-based and video-based understanding of embodied reference. Through comprehensive baselines and extensive experiments, we provide the initial insights into how referring expressions and gestures impact the understanding of embodied reference. Our findings offer significant evidence that gestures are equally crucial to language cues in comprehending embodied reference.