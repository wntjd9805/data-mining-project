Adversarial training is an effective defense mechanism against adversarial attacks. Previous studies have indicated that overfitting is a major issue in adversarial training, resulting in a significant gap between the accuracy of training and testing in neural networks. This research demonstrates that the observed generalization gap is closely linked to the choice of activation function. Specifically, activation functions with low curvature values, whether exact or approximate, have a regularization effect that substantially reduces both the standard and robust generalization gaps in adversarial training. This effect is observed in differentiable/smooth activations like SiLU, as well as non-differentiable/non-smooth activations like LeakyReLU, where the curvature is approximately low. Furthermore, it is revealed that activation functions with low curvature do not exhibit the double descent phenomenon observed in adversarially trained models.