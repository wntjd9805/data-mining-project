This study addresses the limitation of global logit transfer in prevalent knowledge distillation methods used in image recognition models. It is observed that global logit transfer fails to transfer informative spatial information, which includes localized knowledge and relational information within an input scene. To overcome this limitation, the researchers propose a simple and effective logit distillation approach. They introduce a local spatial pooling layer branch to the penultimate layer, allowing for the learning of both finely-localized knowledge and holistic representation. Experimental results demonstrate that their method outperforms state-of-the-art techniques in terms of accuracy on various image classification datasets. Furthermore, the distilled students trained using their method can also be effectively utilized for object detection and semantic segmentation tasks, highlighting the high transferability of the proposed approach.