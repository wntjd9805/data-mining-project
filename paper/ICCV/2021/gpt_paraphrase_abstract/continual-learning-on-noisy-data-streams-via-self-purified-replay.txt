This study introduces a novel framework for continual learning that addresses the challenges of catastrophic forgetting and noisy labels. The framework is based on two key observations: (i) self-supervised learning can mitigate forgetting even with noisy labels, and (ii) the purity of the replay buffer is crucial. To address these observations, the authors propose two components: Self-Replay, a self-supervised replay technique that circumvents training errors caused by noisy labels, and the Self-Centered filter, which maintains a purified replay buffer using centrality-based stochastic graph ensembles. Experimental results on MNIST, CIFAR-10, CIFAR-100, and WebVision datasets with real-world noise demonstrate the effectiveness of the framework in maintaining a pure replay buffer and outperforming state-of-the-art methods in both continual learning and noisy label learning.