We propose a solution to the problem of network quantization, which involves reducing the bit-widths of weights and/or activations in network architectures. Existing quantization methods rely on non-differentiable rounding functions to map full-precision values to quantized values. This presents a challenge when training quantized networks using gradient-based optimizers. Two common approaches have been used to address this challenge: the straight-through estimator (STE) and soft quantizers. However, both approaches have their limitations, including gradient mismatch and quantizer gap problems.To overcome these limitations, we introduce a new quantizer called a distance-aware quantizer (DAQ). The DAQ consists of two main components: a distance-aware soft rounding (DASR) and a temperature controller. The DASR approximates the discrete rounding operation using a kernel soft argmax, which is based on the insight that quantization can be viewed as a distance-based assignment problem between full-precision and quantized values. The temperature controller adjusts the temperature parameter in DASR adaptively based on the input, effectively addressing the quantizer gap problem.Experimental results on standard benchmarks demonstrate that our DAQ outperforms existing methods significantly across various bit-widths without any additional complexities.