We propose a novel approach to learning self-supervised representations from unlabeled image collections. Unlike existing methods that rely on maximizing similarity between augmented images or randomly selecting negative samples, we leverage the natural variation found in image collections captured by static monitoring cameras. We utilize contextual data to capture spatial and temporal relationships between images, enabling us to identify high probability positive pairs during training. These positive pairs depict the same visual concept and help us learn surprisingly effective representations for downstream supervised classification tasks. Our approach is particularly useful for global biodiversity monitoring, where limited human supervision is available. We demonstrate the effectiveness of our method on four camera trap image collections using three different families of self-supervised learning methods. Our results show that careful image selection during training outperforms conventional self-supervised training and transfer learning baselines.