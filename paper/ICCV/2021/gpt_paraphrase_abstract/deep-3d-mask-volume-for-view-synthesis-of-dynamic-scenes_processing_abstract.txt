Image view synthesis has been successful in creating realistic visuals using deep learning and novel representations. The next step in immersive virtual experiences is synthesizing dynamic scenes from different viewpoints. However, there are challenges due to the lack of high-quality training datasets and the time dimension in videos of dynamic scenes. To overcome this, we introduce a multi-view video dataset captured with a 10-camera rig at 120FPS. The dataset includes 96 high-quality scenes showcasing visual effects and human interactions in outdoor environments. We present a new algorithm called Deep 3D Mask Volume, which allows for stable view extrapolation in temporally dynamic scenes captured by static cameras. Our algorithm addresses temporal inconsistencies by identifying error-prone areas using a 3D mask volume and replacing them with a static background observed throughout the video. Unlike simple 2D masks, our method enables manipulation in 3D space. We demonstrate superior temporal stability compared to frame-by-frame static view synthesis methods or those using 2D masks. The resulting synthesized videos exhibit minimal flickering artifacts and support larger translational movements.