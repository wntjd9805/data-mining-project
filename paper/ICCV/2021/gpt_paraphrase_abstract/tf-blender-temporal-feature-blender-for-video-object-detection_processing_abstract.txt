Video objection detection is a difficult task due to the deterioration of appearance in isolated video frames, which makes detection confusing. Current methods address this issue by using temporal information to enhance per-frame representation through the aggregation of features from neighboring frames. However, these methods mainly focus on selecting higher-level video frames for aggregation and do not consider modeling lower-level temporal relations to improve feature representation. To overcome this limitation, we propose a new solution called TF-Blender. TF-Blender consists of three modules: 1) Temporal relation, which models the relations between the current frame and its neighboring frames to preserve spatial information. 2) Feature adjustment, which enriches the representation of each neighboring feature map. 3) Feature blender, which combines the outputs of the previous two modules to produce stronger features for later detection tasks. TF-Blender is simple and can easily be integrated into any detection network to improve detection performance. We evaluated TF-Blender extensively on ImageNet VID and YouTube-VIS benchmarks and found that it performs well compared to recent state-of-the-art methods. The code for TF-Blender is available at https://github.com/goodproj13/TF-Blender.