In recent years, compact convolutional neural networks (CNNs) have made significant advancements in performance. However, they still fall short in comparison to CNNs with a larger number of parameters. The key characteristic of successful CNNs is the ability to capture diverse and abundant features in their layers. However, the difference in this characteristic between large CNNs and compact CNNs has not been thoroughly explored. Compact CNNs, due to their limited parameters, struggle to obtain abundant features, making feature diversity crucial.The activation maps derived from a data point during model inference can indicate the presence of unique descriptors necessary for distinguishing between different classes of objects. On the other hand, data points with low feature diversity may not provide enough unique descriptors for accurate predictions, leading to random predictions. These random predictions can negatively impact the optimization process and harm the final performance. To address this issue, we propose a novel Bias Loss that modifies the standard cross-entropy loss to prioritize data points with a limited number of unique descriptive features. By focusing the training on valuable data points, our Bias Loss prevents misleading optimization caused by a vast number of samples with poor learning features.Additionally, we introduce a family of SkipblockNet models, designed to increase the number of unique descriptors in the last layers. Through experiments on benchmark datasets, we demonstrate the superiority of our proposed loss function over the cross-entropy loss. Moreover, our SkipblockNet-M model achieves 1% higher classification accuracy than MobileNetV3 Large with similar computational costs on the ImageNet ILSVRC-2012 classification dataset.To access the code related to our work, it is available on GitHub at the following link: https://github.com/lusinlu/biasloss_skipblocknet.