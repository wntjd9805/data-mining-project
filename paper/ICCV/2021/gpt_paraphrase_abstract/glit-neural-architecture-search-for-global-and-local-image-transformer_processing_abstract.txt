We present the first Neural Architecture Search (NAS) technique for improving transformer architectures in image recognition. Although transformers have shown impressive performance in image recognition without CNN-based backbones, they were originally designed for NLP tasks and may not be optimal for image recognition. To enhance the visual representation capabilities of transformers, we propose a new search space and algorithm. Our approach includes a locality module that explicitly models local correlations in images with reduced computational cost. This locality module allows our search algorithm to balance global and local information and optimize the design choices in each module. To address the challenge of a large search space, we introduce a hierarchical NAS method that separately searches for the optimal vision transformer at two levels using an evolutionary algorithm. Extensive experiments on the ImageNet dataset demonstrate that our method discovers more effective and efficient transformer variants compared to the ResNet family and the baseline ViT for image classification. The source code for our method is available at https://github.com/bychen515/GLiT.