The current standard approach for efficient representation learning is to optimize the K-class hyperplanes in the latent space. However, accurately describing the latent space of large noisy datasets with a single optimal K-class hyperplane is nearly impossible. To address this issue, we propose a new method called Switchable K-class Hyperplanes (SKH). SKH constructs a mixture of K-class hyperplanes to sufficiently describe the latent space, replacing the traditional single K-class hyperplane optimization. When combined with the popular ArcFace technique for million-level data representation learning, SKH effectively eliminates gradient conflicts caused by real-world label noise on a single K-class hyperplane. Additionally, we introduce a simple strategy called Posterior Data Clean, which uses margin-based loss functions (e.g. ArcFace) to reduce model optimization deviation on clean datasets resulting from a reduction in valid categories in each K-class hyperplane. Extensive experiments demonstrate that SKH achieves new state-of-the-art performance on IJB-B and IJB-C datasets, showcasing its effectiveness in promoting noise-robust representation learning. Our code for SKH is available at https://github.com/liubx07/SKH.git.