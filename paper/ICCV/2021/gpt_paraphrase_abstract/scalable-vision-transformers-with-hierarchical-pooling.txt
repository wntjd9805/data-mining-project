The Visual image Transformers (ViT) have shown promise in image recognition tasks like classification. However, the current ViT model maintains a full-length patch sequence during inference, which is redundant and lacks hierarchical representation. To address this, we propose the Hierarchical Visual Transformer (HVT), which progressively pools visual tokens to reduce the sequence length and computational cost, similar to downsampling in Convolutional Neural Networks (CNNs). This allows us to increase model capacity without introducing extra computational complexity. Additionally, we find that the average pooled visual tokens contain more discriminative information than the single class token. We demonstrate the scalability of HVT through extensive experiments on image classification, outperforming competitive baselines on ImageNet and CIFAR-100 datasets with comparable FLOPs. The code for HVT is available at https://github.com/MonashAI/HVT.