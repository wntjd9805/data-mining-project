Deepfakes, which are videos created using artificial intelligence algorithms, have the potential for both entertainment and misuse. They can be used to manipulate speeches and spread misinformation. Detecting visual deepfakes has received more attention, but audio deepfakes and the relationship between the video and audio aspects have been overlooked. In this study, we introduce a new task of jointly detecting visual and auditory deepfakes and show that exploiting the synchronization between these modalities can improve detection accuracy. Our experiments demonstrate that the proposed joint detection framework performs better than independently trained models and is also more effective in detecting new types of deepfakes.