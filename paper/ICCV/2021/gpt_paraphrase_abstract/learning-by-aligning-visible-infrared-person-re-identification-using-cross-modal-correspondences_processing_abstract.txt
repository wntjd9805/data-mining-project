We present a solution to the issue of visible-infrared person re-identification (VI-reID), which involves retrieving a set of person images captured by visible or infrared cameras in a cross-modal setting. VI-reID faces two main challenges: intra-class variations among person images and cross-modal discrepancies between visible and infrared images. Previous approaches have focused on learning coarse image- or rigid part-level person representations that are discriminative and generalizable across different modalities, assuming that the person images are roughly aligned. However, the person images, which are typically cropped by off-the-shelf object detectors, may not be well-aligned, which hinders the learning of discriminative person representations. 

In this paper, we propose a novel feature learning framework that addresses these problems in a unified manner. Our approach involves exploiting dense correspondences between cross-modal person images. This approach enables the effective suppression of modality-related features from person representations at a pixel-level, thereby addressing cross-modal discrepancies. It also encourages pixel-wise associations between cross-modal local features, which facilitates discriminative feature learning for VI-reID. 

We have conducted extensive experiments and analyses on standard VI-reID benchmarks to evaluate the effectiveness of our approach, and the results demonstrate that our method significantly outperforms the current state of the art.