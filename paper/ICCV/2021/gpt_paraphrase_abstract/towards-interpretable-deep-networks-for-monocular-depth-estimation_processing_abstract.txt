Recent advancements in deep networks for Monocular Depth Estimation (MDE) have shown promising results. However, understanding the interpretability of these networks is crucial. Existing methods use visual cues to provide explanations but fail to explore the internal representations learned by deep networks. This paper introduces a novel approach to interpret the internal representations of MDE networks by identifying hidden units that are selective to specific depth ranges. By quantifying the interpretability based on the depth selectivity of these units, we propose a method to train interpretable MDE deep networks without altering their original architectures. Experimental results demonstrate that our approach significantly improves the depth selectivity of the units without compromising or even enhancing the depth estimation accuracy. We also conduct extensive analysis to validate the reliability of the selective units, the applicability of our method across different layers, models, and datasets, and showcase its effectiveness in analyzing model errors. The source code and models are available at https://github.com/youzunzhi/InterpretableMDE.