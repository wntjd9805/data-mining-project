This study addresses the challenge of identifying the main speaker and interrupters in a conversation. Existing methods have limitations, such as the limited correlation between audio and visual features and the lack of effective exploitation of temporal relationships. The interactions between speakers, including tracking and anticipatory decisions, are often ignored. To overcome these limitations, the authors propose an Audio-Visual Transformer approach that utilizes different types of correlations in both audio and visual signals. The proposed method optimizes the temporal audio-visual relationships using a self-attention mechanism in a Transformer structure. Additionally, a new dataset is introduced for main speaker detection. This study is one of the first to automatically locate and highlight the main speaker in both visual and audio channels of multi-speaker conversation videos.