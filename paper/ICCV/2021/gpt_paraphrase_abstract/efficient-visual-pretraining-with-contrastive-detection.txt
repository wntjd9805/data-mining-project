Self-supervised pretraining has demonstrated its effectiveness in generating valuable representations for transfer learning. However, the significant computational burden associated with state-of-the-art methods has limited its practicality. To address this issue, we propose a novel self-supervised objective called contrastive detection, which requires representations to identify object-level features across different augmentations. This objective provides a substantial learning signal per image, resulting in state-of-the-art transfer accuracy across various tasks, while reducing the need for extensive pretraining. Notably, our ImageNet-10 pretrained model achieves comparable performance to SEER, a leading self-supervised system that utilizes 1000 times more pretraining data. Furthermore, our objective seamlessly handles pretraining on more complex images, such as those in COCO, narrowing the performance gap with supervised transfer learning from COCO to PASCAL.