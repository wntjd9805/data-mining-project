DETR, a Transformer-based object detection method, has achieved impressive results but suffers from slow convergence during training. This paper investigates the reasons behind this optimization difficulty. The study identifies two main factors contributing to the slow convergence: problems with the Hungarian loss and the Transformer cross-attention mechanism. To address these issues, two solutions are proposed: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results demonstrate that these proposed methods not only converge faster than the original DETR but also exhibit significantly better detection accuracy compared to DETR and other baseline models. The code for the proposed methods is available at https://github.com/Edward-Sun/TSP-Detection.