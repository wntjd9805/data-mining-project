Federated Learning (FL) is a privacy-preserving approach that aims to create a shared model across decentralized clients. However, dealing with non-independent and identical distribution (non-IID) client data poses a challenge for FL in real-world scenarios. The performance of FL is significantly reduced when data distributions differ, leading to optimization inconsistency and feature divergence. Simply minimizing an aggregate loss function can negatively impact some clients' model performance. To address these issues, we propose FedUFO, a method for non-IID FL that combines feature learning and optimization objectives alignment. We introduce an adversary module to reduce feature representation divergence among clients and propose consensus losses to address optimization inconsistency. Extensive experiments demonstrate that FedUFO outperforms existing approaches, including data-sharing methods. Additionally, FedUFO enables more balanced and reasonable model performance among clients.