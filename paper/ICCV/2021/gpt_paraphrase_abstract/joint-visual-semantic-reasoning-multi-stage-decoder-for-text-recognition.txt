Despite advancements in text recognition, current state-of-the-art models struggle to accurately recognize text in real-world scenarios due to factors such as complex backgrounds, varying fonts, uncontrolled lighting conditions, distortions, and other artifacts. This is because these models rely solely on visual information for text recognition and lack semantic reasoning capabilities. In this research, we propose the incorporation of semantic information to complement visual information in text recognition. To achieve this, we introduce a multi-stage multi-scale attentional decoder that performs joint visual-semantic reasoning. Our main contribution is the design of a stage-wise unrolling attentional decoder that refines predictions in a step-by-step manner. To enable end-to-end training, we address the issue of non-differentiability caused by discretely predicted character labels. The first stage of our decoder utilizes visual features for prediction, while subsequent stages refine predictions using both visual and semantic information. Additionally, we introduce multi-scale 2D attention and dense and residual connections between stages to handle variations in character sizes. Our experimental results demonstrate that our approach outperforms existing state-of-the-art methods by a significant margin.