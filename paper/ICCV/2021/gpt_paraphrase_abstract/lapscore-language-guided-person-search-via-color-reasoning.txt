The primary objective of language-guided person search is to establish a connection between visual and textual input. Current approaches focus on developing multimodal attention mechanisms and novel cross-modal loss functions to implicitly learn this association. In this study, we propose a representation learning technique for language-guided person search called LapsCore, which explicitly builds a detailed cross-modal association. LapsCore accomplishes this by incorporating two dual sub-tasks: image colorization and text completion. In the image colorization task, the model learns to add color to grayscale images using text information. In the text completion task, the model comprehends the image and fills in missing color words in the captions. These sub-tasks enable the models to learn accurate alignments between text phrases and image regions, facilitating the acquisition of rich multimodal representations. Extensive experiments conducted on various datasets demonstrate the effectiveness and superiority of our proposed method.