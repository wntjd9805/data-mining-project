The aim of this study is to synchronize subtitles in sign language videos. Previous work focused on finding keyword-sign correspondences, but we aim to localize complete subtitle texts. To accomplish this, we propose a Transformer architecture trained on manually annotated alignments of over 15K subtitles spanning 17.7 hours of video. By using BERT subtitle embeddings and CNN video representations, our model makes frame-level predictions to determine if a video frame belongs to a queried subtitle or not. Through extensive evaluations, we demonstrate significant improvements compared to existing alignment baselines. This automatic alignment model has the potential to advance machine translation of sign languages by providing continuously synchronized video-text data.