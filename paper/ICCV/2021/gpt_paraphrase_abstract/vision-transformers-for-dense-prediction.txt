We present dense prediction transformers, a novel architecture that replaces convolutional networks with vision transformers as the backbone for dense prediction tasks. By combining tokens from different stages of the vision transformer into image-like representations at various resolutions, and progressively merging them into full-resolution predictions using a convolutional decoder, our approach achieves finer-grained and more globally coherent predictions compared to fully-convolutional networks. The transformer backbone operates at a constant and relatively high resolution, enabling a global receptive field at every stage. Our experiments demonstrate significant improvements in dense prediction tasks, particularly with abundant training data. For monocular depth estimation, we achieve up to a 28% improvement in relative performance compared to state-of-the-art fully-convolutional networks. In semantic segmentation, our dense prediction transformers achieve a new state-of-the-art performance of 49.02% mIoU on ADE20K dataset. Furthermore, we show that our architecture can be fine-tuned on smaller datasets like NYUv2, KITTI, and Pascal Context, where it also establishes a new state-of-the-art. Access to our models can be found at https://github.com/intel-isl/DPT.