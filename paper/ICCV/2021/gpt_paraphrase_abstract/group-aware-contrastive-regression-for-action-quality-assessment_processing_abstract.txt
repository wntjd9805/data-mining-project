Assessing the quality of actions in videos is a challenging task due to the subtle differences between videos and the large variations in scores. Most existing methods address this issue by predicting a quality score for each individual video, but they struggle with the significant differences in scores between videos. In this paper, we propose a new approach that leverages the relationships among videos to improve the accuracy of action quality assessment during both training and inference. Instead of learning scores without any reference, we reframe the problem as regressing relative scores with respect to another video that shares similar attributes, such as category and difficulty. Our proposed framework, called Contrastive Regression (CoRe), learns the relative scores through pairwise comparisons, which emphasize the distinctions between videos and guide the models to capture the crucial cues for assessment. To further exploit the relative information between two videos, we introduce a group-aware regression tree that splits the conventional score regression into two simpler sub-problems: coarse-to-fine classification and regression within small intervals. We evaluate the effectiveness of CoRe through extensive experiments on three widely-used action quality assessment datasets (AQA-7, MTL-AQA, and JIGSAWS). Our approach significantly outperforms previous methods and achieves state-of-the-art performance on all three benchmarks.