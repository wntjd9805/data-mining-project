To address the issue of annotation workload in object detection, many studies have focused on transferring knowledge from a labeled source domain to an unlabeled target domain. However, these methods assume that the labeled data comes from a single source domain, overlooking the scenario where labeled data originates from multiple source domains. To tackle this more challenging task, we propose a unified framework called Divide-and-Merge SpindleNetwork (DMSN) based on Faster R-CNN. DMSN aims to simultaneously improve domain invariance and maintain discriminative power. The framework consists of multiple source subnets and a pseudo target subnet. Firstly, we introduce a hierarchical feature alignment strategy to align low- and high-level features, recognizing their distinct impact on object detection. Secondly, we develop a novel algorithm to learn optimal parameters for the pseudo target subnet by combining the parameters from different source subnets. Lastly, we propose a consistency regularization method for the region proposal network to enhance each subnet's ability to learn more abstract invariances. Through extensive experiments in various adaptation scenarios, we demonstrate the effectiveness of our proposed model.