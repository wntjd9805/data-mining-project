Marker-based optical motion capture (mocap) is widely used to obtain precise 3D human motion data in fields such as computer vision, medicine, and graphics. However, the raw output of mocap systems consists of noisy and incomplete 3D points or short tracklets of points. In order to make this data usable, it is necessary to associate these points with corresponding markers on the captured subject, a process known as "labelling". Once labelled, the 3D skeleton or body surface mesh can be determined.Existing commercial auto-labelling tools require a specific calibration procedure during capture, which is not feasible for archival data. To overcome this limitation, we have developed a novel neural network called SOMA. SOMA is capable of labelling raw mocap point clouds with varying numbers of points, without the need for calibration data or dependence on the capture technology. It also requires minimal human intervention. Our approach leverages the fact that while labelling point clouds is inherently ambiguous, the 3D body itself provides strong constraints that can be exploited by a learning-based method.To facilitate the learning process, we have generated large training sets consisting of simulated noisy mocap markers and ground truth data, animated by 3D bodies from AMASS. SOMA utilizes an architecture with stacked self-attention elements to learn the spatial structure of the 3D body, along with an optimal transport layer to address the assignment problem and reject outliers. We have conducted extensive quantitative and qualitative evaluations of SOMA, demonstrating its superior accuracy and robustness compared to existing state-of-the-art research methods. Furthermore, SOMA can be applied in scenarios where commercial systems are not viable.In our evaluations, we have successfully automatically labelled over 8 hours of archival mocap data from 4 different datasets, captured using various technologies. The output includes SMPL-X body models. To encourage further research, we have made both the trained model and the data available at https://soma.is.tue.mpg.de/.