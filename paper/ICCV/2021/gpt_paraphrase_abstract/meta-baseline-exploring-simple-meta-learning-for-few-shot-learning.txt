Meta-learning has become the dominant approach for few-shot learning in recent years. It involves training a model on a collection of few-shot classification tasks, which aligns the training and testing objectives. However, recent studies have shown that training the model for whole-classification, i.e., classification on the entire label-set, can yield comparable or even better embeddings than many meta-learning algorithms. The relationship between these two approaches has not been thoroughly explored, leaving the effectiveness of meta-learning in few-shot learning uncertain. In this study, we propose a straightforward approach: applying meta-learning to a pre-trained model trained for whole-classification on its evaluation metric. We find that this simple method achieves competitive performance compared to state-of-the-art techniques on standard benchmarks. Through further analysis, we gain insights into the trade-offs between the meta-learning objective and the whole-classification objective in few-shot learning. The code for our method is available at https://github.com/yinboc/few-shot-meta-baseline.