Recognizing active objects in egocentric videos is a difficult task due to various challenges such as cluttered backgrounds, frequent changes in field of view, and occlusion. Existing methods for improving active object classification often rely on object detectors or human gaze information, which are computationally expensive or require labor-intensive annotations. In order to overcome these limitations, we propose an end-to-end Interactive Prototype Learning (IPL) framework that leverages the actor's motion cues to learn better representations of active objects. Our approach involves the use of verb prototypes, which help in separating active object features from distracting object features. Each prototype corresponds to a primary motion pattern of an egocentric action, providing a distinct supervision signal for learning active object features. Additionally, we introduce two interactive operations - noun-to-verb assignment and verb-to-noun selection - that facilitate the extraction of active object features. These operations are parameter-efficient and enable the learning of location-aware features on top of 3D CNN backbones. We evaluate our IPL framework on three large-scale egocentric video datasets - EPIC-KITCHENS-55, EPIC-KITCHENS-100, and EGTEA - and demonstrate its ability to generalize across different backbones. Our approach outperforms the state-of-the-art methods in terms of active object classification.