The popularity of RGB-D saliency detection has increased due to the convenience of capturing depth cues. However, existing methods often neglect modality-specific characteristics. In this study, we introduce a specificity-preserving network (SP-Net) that considers both shared and modality-specific properties for RGB-D saliency detection. Our approach involves using modality-specific networks and a shared learning network to generate individual and shared saliency maps. We also propose a cross-enhanced integration module (CIM) to fuse cross-modal features and a multi-modal feature aggregation (MFA) module to integrate modality-specific features. Additionally, we employ skip connections to combine hierarchical features. Experimental results on six benchmark datasets demonstrate that our SP-Net outperforms other state-of-the-art methods. The code for our SP-Net implementation is available at: https://github.com/taozh2017/SPNet.