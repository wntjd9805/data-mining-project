This paper addresses the vulnerability of deep neural networks (DNNs) to adversarial noise and proposes a novel approach to remove this noise using a self-supervised adversarial training mechanism. Unlike pre-processing based defenses, which can be affected by the error amplification effect, the proposed method operates in a class activation feature space. The process involves maximizing disruptions to class activation features of natural examples to create adversarial examples, and then training a denoising model to minimize the distances between the adversarial examples and natural examples in the class activation feature space. Experimental evaluations demonstrate that this method significantly improves adversarial robustness compared to previous state-of-the-art approaches, particularly against unseen and adaptive adversarial attacks.