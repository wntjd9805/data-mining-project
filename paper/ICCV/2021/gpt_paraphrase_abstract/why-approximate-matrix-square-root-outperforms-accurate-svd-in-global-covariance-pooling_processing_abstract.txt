Global Covariance Pooling (GCP) is a technique that utilizes the second-order statistics of convolutional features to improve the classification performance of Convolutional Neural Networks (CNNs). The traditional approach to computing the matrix square root in GCP involves using Singular Value Decomposition (SVD). However, it has been observed that the approximate matrix square root obtained through Newton-Schulz iteration yields better results than the accurate one computed with SVD. In this study, we investigate the reasons behind this performance gap, considering data precision and gradient smoothness. We explore various methods to compute smooth SVD gradients and propose a hybrid training protocol for SVD-based GCP meta-layers that can achieve competitive performance compared to Newton-Schulz iteration. Additionally, we introduce a new GCP meta-layer that utilizes SVD in the forward pass and Pad√© approximants in the backward propagation to compute gradients. When integrated into different CNN models, this proposed meta-layer achieves state-of-the-art performance on both large-scale and fine-grained datasets.