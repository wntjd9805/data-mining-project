Learning dense semantic representations of images without supervision is an important problem in computer vision. However, this problem has not been extensively explored, except for a few cases that focused on unsupervised semantic segmentation on small-scale datasets with a limited visual domain. In this paper, we aim to address this problem on datasets that have traditionally been used for supervised learning. To do this, we propose a two-step framework that uses a predetermined mid-level prior in a contrastive optimization objective to learn pixel embeddings. This approach differs from existing works that relied on proxy tasks or end-to-end clustering. We also discuss the significance of having a prior that contains information about objects or their parts and suggest various methods to obtain such a prior in an unsupervised manner. Our experimental evaluation demonstrates the advantages of our method compared to existing works. First, the learned pixel embeddings can be directly clustered into semantic groups using K-Means on the PASCAL dataset. This is a significant achievement, as there is no previous solution for the fully unsupervised semantic segmentation task on this challenging benchmark. Second, our representations outperform strong baselines when transferred to new datasets like COCO and DAVIS. The code for our method is available.