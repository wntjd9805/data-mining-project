The task of attribute-based person search involves finding person images that best match a given set of text attributes. However, a major challenge in this task is the significant difference between attributes and images. To address this gap, we propose a new loss function for learning cross-modal embeddings in the context of attribute-based person search. We consider a set of attributes as a category of people who share similar traits. In a joint embedding space for both attributes and images, our loss function aims to align images with their corresponding person categories. Additionally, it creates a margin between pairs of person categories based on their semantic distance. This distance metric is learned end-to-end, allowing the loss function to consider the importance of each attribute when relating person categories. By using an adaptive semantic margin, our loss function leads to more discriminative and semantically well-arranged distributions of person images. Consequently, a simple embedding model guided by this loss function achieves state-of-the-art performance on public benchmarks without any additional complex techniques or features.