Assisted and autonomous driving rely on understanding the surroundings of the ego-vehicle. Currently, cameras and laser scanners are commonly used for this purpose, but they have limitations in adverse weather conditions. Automotive radars, on the other hand, are affordable and unaffected by rain, snow, or fog. However, they are rarely used for scene understanding due to the complexity of radar data and the lack of annotated datasets. Fortunately, recent open-sourced datasets have enabled research on classification, object detection, and semantic segmentation using raw radar signals and end-to-end trainable models.

In this study, we propose new architectures and associated losses that analyze multiple "views" of the range-angle-Doppler radar tensor to segment it semantically. Our experiments on the CARRADA dataset demonstrate that our best model performs better than alternative models derived from semantic segmentation of natural images or radar scene understanding, while requiring fewer parameters. Our code and trained models are available at https://github.com/valeoai/MVRSS.