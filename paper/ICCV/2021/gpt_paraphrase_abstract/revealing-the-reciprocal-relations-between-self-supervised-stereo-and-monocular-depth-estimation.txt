Current self-supervised depth estimation algorithms primarily focus on stereo or monocular approaches separately, without considering the interconnection between them. In this study, we propose a straightforward yet efficient framework to enhance both stereo and monocular depth estimation by leveraging the complementary knowledge of these two tasks. Our approach consists of three stages. Firstly, we train a self-supervised stereo matching network called StereoNet using image pairs. Secondly, we introduce an occlusion-aware distillation module (OA Distillation) that utilizes the predicted depths from StereoNet in non-occluded regions to train our monocular depth estimation network, SingleNet. Lastly, we design an occlusion-aware fusion module (OA Fusion) that generates more reliable depth estimates by combining the estimated depths from StereoNet and SingleNet with the occlusion map. Additionally, we use the fused depths as pseudo labels to supervise StereoNet, further improving its performance. We conduct extensive experiments on the KITTI dataset to demonstrate the effectiveness of our proposed framework, achieving state-of-the-art performance in both stereo and monocular depth estimation tasks. Figure 1 illustrates the characteristics of stereo and monocular models, showcasing the impact of object movements on the estimated disparities.