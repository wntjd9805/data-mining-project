Adversarial training (AT) is commonly used to create models that are robust against adversarial examples. However, AT often suffers from robust overfitting, where the model performs well on training examples but poorly on test examples. This lack of robust generalization limits the effectiveness of AT in real-world scenarios. In this study, we investigate the connection between robust generalization and the flatness of the robust loss landscape in weight space. We introduce average- and worst-case metrics to measure the flatness of the robust loss landscape and find that there is a correlation between good robust generalization and flatness. We observe that as overfitting occurs during training, the flatness decreases significantly, but early stopping can help identify flatter minima in the robust loss landscape. We also find that AT variants that achieve higher adversarial robustness are associated with flatter minima. This relationship holds for various popular AT approaches, including AT-AWP, TRADES, MART, AT with self-supervision or additional unlabeled examples, as well as simple regularization techniques like AutoAugment, weight decay, or label noise. To ensure a fair comparison, our flatness measures are designed to be scale-invariant, and we conduct extensive experiments to validate our findings.