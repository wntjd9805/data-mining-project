Current facial editing techniques lack the ability to perform continuous and detailed edits, such as transforming a slightly smiling face into a big laughing one, while maintaining natural interactions with users. In this study, we propose Talk-to-Edit, an interactive framework for facial editing that allows users to manipulate facial attributes through dialogue with the system. Our approach involves modeling a "semantic field" in the latent space of the GAN (Generative Adversarial Network). Unlike previous methods that consider editing as linear movements in the latent space, our approach formulates fine-grained editing as finding a curved trajectory that respects the fine-grained attribute landscape in the semantic field. The curvature of the trajectory is determined by the input image and the user's language requests, resulting in location-specific edits. To facilitate meaningful dialogue, our system generates language feedback based on both the user's request and the current state of the semantic field. Additionally, we contribute CelebA-Dialog, a dataset for visual-language facial editing, which includes manually annotated fine-grained attribute annotations and template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of smoothness in fine-grained editing, preservation of identity/attributes, and visual photorealism and dialogue fluency. User studies show that our system is consistently favored by around 80% of the participants. Further information about our project can be found on our project page at https://www.mmlab-ntu.com/project/talkedit/.