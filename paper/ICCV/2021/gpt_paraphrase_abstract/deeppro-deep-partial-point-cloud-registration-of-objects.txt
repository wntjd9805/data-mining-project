This study focuses on the problem of registering partial point clouds in real-time without prior knowledge of the 3D model of the object. Partial point clouds are obtained from a depth sensor that captures only the visible part of the object from a specific viewpoint. This poses two challenges: the partial point clouds do not fully overlap, and the reliability of keypoints is reduced when the visible part lacks distinct local structures. To overcome these challenges, the authors propose DeepPRO, a deep neural network that is keypoint-free and trainable end-to-end. DeepPRO is inspired by how humans align point clouds by imagining their appearance after registration based on their shape. The network takes two partial point clouds as inputs and directly predicts the location of aligned points. By preserving the ordering of points during prediction, dense correspondences between the input and predicted point clouds can be achieved when inferring rigid transform parameters. The proposed method is evaluated on the real-world Linemod and synthetic ModelNet40 datasets, as well as the PRO1k dataset, which is a large-scale version of Linemod for testing generalization to real-world scans. Experimental results demonstrate that DeepPRO outperforms thirteen strong baseline methods, achieving the best accuracy with an average distance difference of 2.2mm on the Linemod dataset, while maintaining a high frame rate of 50 fps on mobile devices.