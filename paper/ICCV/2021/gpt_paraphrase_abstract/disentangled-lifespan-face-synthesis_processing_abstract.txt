The aim of a lifespan face synthesis (LFS) model is to generate realistic face images of a person throughout their life based on one reference photo. The generated face images should accurately reflect the person's age through plausible changes in shape and texture, while still maintaining their identity. This is a difficult task because the shape and texture of a face change in complex and non-linear ways with age. Most existing LFS models use generative adversarial networks (GANs) to apply age-related transformations to a latent face representation. While these models benefit from recent GAN advancements, they are limited in their ability to accurately model the non-linear transformations of texture and shape while preserving identity because they do not explicitly separate the latent representations into texture, shape, and identity factors. In this study, a new LFS model is proposed that effectively disentangles the key characteristics of a face, namely shape, texture, and identity. This allows for effective modeling of the unique transformations of shape and texture that occur with age. The disentanglement is achieved by extracting shape, texture, and identity features separately using an encoder. Two transformation modules are designed to model the non-linear shape and texture feature transformations: one based on conditional convolution and the other based on channel attention. These modules accommodate the distinct aging processes of shape and texture and ensure that the synthesized images are both age-sensitive and identity-preserving. Extensive experiments show that the proposed LFS model outperforms existing alternatives. The codes and a demo of the model are available on the project website: https://senhe.github.io/projects/iccv_2021_lifespan_face.