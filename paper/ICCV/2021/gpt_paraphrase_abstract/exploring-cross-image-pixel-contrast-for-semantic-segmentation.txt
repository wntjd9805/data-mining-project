Current semantic segmentation methods primarily focus on capturing local context by considering dependencies between pixels within individual images. This is achieved through the use of context-aggregation modules or structure-aware optimization criteria. However, these methods overlook the global context of the training data, which refers to the rich semantic relationships between pixels across different images. Taking inspiration from recent advancements in unsupervised contrastive representation learning, we propose a novel approach for semantic segmentation in a fully supervised setting. Our approach introduces a pixel-wise contrastive algorithm, which aims to make pixel embeddings belonging to the same semantic class more similar than embeddings from different classes. This introduces a pixel-wise metric learning paradigm for semantic segmentation, which explicitly explores the structures of labeled pixels that have been previously overlooked. Importantly, our method can be seamlessly integrated into existing segmentation frameworks without any additional overhead during testing. Through extensive experiments with popular segmentation models and backbones, such as DeepLabV3, HRNet, and OCR, and diverse datasets including Cityscapes, PASCAL-Context, COCO-Stuff, and CamVid, we demonstrate the effectiveness of our method in improving segmentation performance. We believe that our work will inspire the community to reconsider the prevailing training paradigm in semantic segmentation.