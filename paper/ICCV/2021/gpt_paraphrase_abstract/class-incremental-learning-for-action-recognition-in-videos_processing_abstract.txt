We address the issue of catastrophic forgetting in class-incremental learning for video recognition. Despite the popularity of continual learning, this specific problem has not been extensively studied. To tackle this challenge, we introduce time-channel importance maps and utilize them to learn the representations of new examples through knowledge distillation. Additionally, we integrate a regularization scheme into our objective function, which encourages uncorrelated features from different time steps in a video, leading to improved accuracy by mitigating catastrophic forgetting. We assess our approach on newly created splits of class-incremental action recognition benchmarks based on UCF101, HMDB51, and Something-Something V2 datasets. Our algorithm demonstrates its effectiveness by outperforming existing continual learning methods designed for image data.