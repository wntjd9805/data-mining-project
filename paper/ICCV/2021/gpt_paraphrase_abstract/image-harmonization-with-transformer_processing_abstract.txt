Image harmonization is a challenging task that aims to make composite images appear more realistic. These composites are created by combining foreground elements from one image with background elements from another, but often suffer from the issue of inconsistent lighting conditions. Existing solutions typically use convolutional neural networks (CNNs) to capture the context of the composite images and understand the surrounding background near the foreground. In this study, we propose a novel approach to image harmonization using Transformers, which excel at modeling long-range dependencies. Our method focuses on adjusting the foreground lighting to match the background lighting while preserving the structure and semantics of the image. We introduce two designs for our harmonization Transformer frameworks, one with disentanglement and one without, and conduct extensive experiments and ablation studies to showcase the power of Transformers in the field of computer vision. Our method achieves state-of-the-art performance in image harmonization, as well as image inpainting and enhancement tasks. We provide our code and models for public use at https://github.com/zhenglab/HarmonyTransformer.