We investigate the challenge of self-supervised video representation learning, which is difficult due to the lack of explicit labels for supervision and the presence of unstructured and noisy visual information. Current methods primarily utilize contrastive loss with video clips as instances to learn visual representation by distinguishing between instances. However, these methods require careful handling of negative pairs through large batch sizes, memory banks, extra modalities, or customized mining strategies, all of which introduce noisy data. In this study, we observe that consistency between positive samples is crucial for learning robust video representation. Consequently, we propose two tasks: appearance consistency and speed consistency. The appearance consistency task aims to maximize the similarity between two clips of the same video with different playback speeds, while the speed consistency task aims to maximize the similarity between two clips with the same playback speed but different appearance information. We demonstrate that jointly optimizing these two tasks consistently enhances performance in downstream tasks such as action recognition and video retrieval. Notably, for action recognition on the UCF-101 dataset, we achieve 90.8% accuracy without utilizing extra modalities or negative pairs for unsupervised pretraining, surpassing the performance of the supervised pretrained model from ImageNet. Furthermore, we will provide access to our codes and models.