Cognitive grammar proposes that the learning of language grammar is based on visual structures. Grammar is not only a representation of natural language but also exists universally in vision to depict hierarchical part-whole relationships. This study focuses on the induction of grammar in both vision and language through a joint learning framework. The method introduced, called VLGrammar, utilizes compound probabilistic context-free grammars (compound PCFGs) to simultaneously induce language grammar and image grammar. To facilitate the joint learning process, a novel contrastive learning framework is proposed. To establish a benchmark for grounded grammar induction, a large-scale dataset named PARTIT is collected, consisting of human-written sentences that describe the semantics of parts in 3D objects. Experimental results on the PARTIT dataset demonstrate that VLGrammar outperforms all other baseline methods in both image grammar induction and language grammar induction. Additionally, the learned VLGrammar exhibits positive effects on downstream tasks, such as a 30% improvement in unsupervised clustering accuracy for images, as well as successful image and text retrieval. Importantly, the induced grammar demonstrates superior generalizability by effectively extending to unseen categories. The code and pre-trained models for VLGrammar are publicly available at https://github.com/evelinehong/VLGrammar.