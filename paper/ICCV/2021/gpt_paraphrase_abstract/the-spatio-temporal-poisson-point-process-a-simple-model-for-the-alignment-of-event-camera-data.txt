Event cameras, inspired by biological vision systems, offer a natural and efficient way to represent visual information by capturing events triggered by local changes in brightness. However, since most brightness changes are caused by relative motion between the camera and the scene, events recorded at a single sensor location usually do not correspond to the same point in the world. To extract meaningful information from event cameras, it is important to align events triggered by the same world point. In this study, we propose a new model for event data that captures its inherent spatio-temporal structure. We first develop a model for aligned event data, assuming perfect registration. Specifically, we model the aligned data as a spatio-temporal Poisson point process. Building on this model, we present a maximum likelihood approach to align events that are not yet registered. This involves finding transformations of the observed events that maximize their likelihood under our model. In particular, we extract the camera rotation that results in the best alignment of events. Our method achieves state-of-the-art accuracy for estimating rotational velocity on the DAVIS 240C dataset, outperforming competing methods in terms of both speed and computational complexity. The code for our method is available at https://github.com/pbideau/Event-ST-PPP.