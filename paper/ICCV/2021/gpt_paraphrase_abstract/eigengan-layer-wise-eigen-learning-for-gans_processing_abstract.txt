Recent research on Generative Adversarial Networks (GAN) has shown that different layers of a generative convolutional neural network (CNN) capture different semantic attributes of the generated images. However, existing GAN models lack explicit dimensions to control the specific semantic attributes represented in each layer. In this paper, we propose a novel approach called EigenGAN, which can automatically mine interpretable and controllable dimensions from different layers of the generator. EigenGAN achieves this by embedding a linear subspace with orthogonal basis into each generator layer. Through generative adversarial training, these layer-wise subspaces discover a set of "eigen-dimensions" corresponding to semantic attributes or interpretable variations. By manipulating the coefficients of a specific eigen-dimension, the generator can produce samples with continuous changes corresponding to a specific semantic attribute. For instance, in the domain of human faces, EigenGAN can discover controllable dimensions for high-level concepts like pose and gender in the deep layers, as well as low-level concepts like hue and color in the shallow layers. Additionally, we theoretically prove that our algorithm derives principal components similar to principal component analysis (PCA) in the linear case. The code for EigenGAN can be found at https://github.com/LynnHo/EigenGAN-Tensorflow.