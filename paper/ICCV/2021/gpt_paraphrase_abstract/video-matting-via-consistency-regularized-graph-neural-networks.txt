Video matting, which involves learning foreground opacity from videos, has become increasingly important due to the rise of video conferencing. Existing methods, which are based on image matting models, struggle to maintain temporal coherence when applied to videos. These methods either rely on optical flow to smooth predictions on a frame-by-frame basis, which is dependent on the quality of the optical flow model, or simply combine feature maps from multiple frames without properly modeling pixel correspondence between adjacent frames. In this study, we propose a solution to enhance temporal coherence in video matting using Consistency-Regularized Graph Neural Networks (CRGNN) along with a synthesized video matting dataset. CRGNN utilizes Graph Neural Networks (GNN) to establish relationships between adjacent frames, allowing for the correction of incorrectly predicted pixels or regions in one frame by leveraging information from neighboring frames. To ensure the generalizability of our model from synthesized videos to real-world videos, we introduce a consistency regularization technique that enforces consistency between the alpha (opacity) and foreground when blending them with different backgrounds. To evaluate the effectiveness of CRGNN, we also create a real-world dataset with annotated alpha mattes. Compared to state-of-the-art methods that require hand-crafted trimaps or backgrounds for training, CRGNN achieves superior results using an unlabeled real training dataset. The source code and datasets for our approach are available at https://github.com/TiantianWang/VideoMatting-CRGNN.git.