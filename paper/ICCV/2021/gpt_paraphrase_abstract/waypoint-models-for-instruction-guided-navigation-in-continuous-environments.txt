The role of action spaces in language-guided visual navigation has not been extensively studied. This includes understanding how action spaces impact navigation success and the efficiency of executing the resulting trajectory. To address this gap, we utilize the VLN-CE setting for instruction following in continuous environments and develop language-conditioned waypoint prediction networks. These networks vary in expressivity, ranging from low-level actions to continuous waypoint prediction. We evaluate task performance and estimated execution time using a LoCoBot robot. Our findings show that more expressive models result in simpler and faster trajectories, while lower-level actions can achieve better navigation metrics by approximating shortest paths more accurately. Additionally, our models outperform previous work in VLN-CE and achieve a new state-of-the-art performance on the public leaderboard, improving the success rate by 4% on this challenging task.