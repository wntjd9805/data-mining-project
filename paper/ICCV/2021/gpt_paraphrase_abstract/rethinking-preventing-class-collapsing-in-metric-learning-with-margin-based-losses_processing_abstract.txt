Metric learning aims to create perceptual embeddings that place visually similar instances close together and dissimilar instances far apart. However, existing learned representations may not be optimal when dealing with diverse intra-class sample distributions and distinct sub-clusters. Margin-based losses, like the triplet loss and margin loss, are commonly used in metric learning but can produce a range of solutions. We provide theoretical and empirical evidence that, under reasonable noise assumptions, margin-based losses tend to project all samples of a class, including those from different modes, onto a single point in the embedding space. This leads to class collapse, making the space unsuitable for classification or retrieval tasks.To overcome this problem, we propose a simple modification to the embedding losses. Instead of selecting a fixed positive element for each sample, we allow each sample to choose its nearest same-class counterpart in a batch as the positive element in the tuple. This modification enables the existence of multiple sub-clusters within each class. Our approach can be incorporated into various metric learning losses. When evaluated on different fine-grained image retrieval datasets using different losses, our method consistently demonstrates significant advantages. Qualitative retrieval results confirm that samples with similar visual patterns are indeed closer in the embedding space.