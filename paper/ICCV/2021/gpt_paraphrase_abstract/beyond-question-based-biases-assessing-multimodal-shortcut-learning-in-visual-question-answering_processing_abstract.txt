We present a new evaluation method for visual question answering (VQA) that aims to identify cases of shortcut learning. Shortcut learning occurs when a model relies on statistical patterns to produce correct answers without truly understanding the desired behavior. It is important to detect and assess shortcuts in a dataset before deploying a model in real-world scenarios.Previous research in VQA has primarily focused on question-based shortcuts, where a model may answer a question like "What is the color of the sky?" with "blue" by relying heavily on the question itself rather than visual evidence. In our study, we extend this concept and consider multimodal shortcuts that involve both questions and images.To identify potential shortcuts, we analyze the widely used VQA v2 training set and mine trivial predictive rules based on word co-occurrences and visual elements. Using this information, we introduce VQA-CounterExamples (VQA-CE), an evaluation protocol that consists of image-question-answer triplets where our rules lead to incorrect answers.We use VQA-CE to conduct a large-scale study of existing VQA approaches. Surprisingly, even state-of-the-art models perform poorly in handling shortcuts, and techniques designed to reduce biases in VQA are largely ineffective in this context. Our findings suggest that previous work on question-based biases in VQA has only addressed one aspect of a complex problem.To facilitate further research, we have made the code for our evaluation method available at https://github.com/cdancette/detect-shortcuts. By adopting our approach, researchers can evaluate VQA models for shortcut learning and gain insights into their performance beyond question-based biases.