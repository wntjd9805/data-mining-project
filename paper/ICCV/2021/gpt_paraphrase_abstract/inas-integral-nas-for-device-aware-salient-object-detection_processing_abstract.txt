Current salient object detection (SOD) models often overlook the relationship between backbone feature extractors and saliency heads, resulting in sub-optimal performance. Additionally, achieving a balance between model performance and inference latency is challenging, especially across different deployment scenarios. To address these issues, we propose a flexible device-aware search scheme within an integral neural architecture search (iNAS) framework. This scheme allows us to train the SOD model once and quickly identify high-performance, low-latency models for multiple devices. We introduce an evolution search with latency-group sampling (LGS) to explore the entire latency range in our expanded search space. Models discovered through iNAS exhibit comparable performance to state-of-the-art methods while significantly reducing latency by 3.8×, 3.3×, 2.6×, and 1.9× on Huawei Nova6 SE, Intel Core CPU, the Jetson Nano, and Nvidia Titan Xp, respectively. The code for our approach is publicly available at https://mmcheng.net/inas/.