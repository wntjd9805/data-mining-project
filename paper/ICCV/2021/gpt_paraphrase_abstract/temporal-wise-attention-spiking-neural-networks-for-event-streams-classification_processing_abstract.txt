Efficiently dealing with sparse and non-uniform spatio-temporal event streams with high temporal resolution is valuable and applicable in real-life scenarios. Spiking neural networks (SNNs), a brain-inspired event-triggered computing model, can effectively extract spatio-temporal features from event streams. However, existing SNN models fail to consider the varying signal-to-noise ratios of serial frames when aggregating individual events into higher temporal resolution frames. This limitation negatively affects the performance of SNNs. To address this issue, we propose a model called temporal-wise attention SNN (TA-SNN) that learns frame-based representation for event stream processing. Our model incorporates temporal-wise attention during training to assess the significance of frames for final decision-making and discards irrelevant frames during inference. We demonstrate that TA-SNN models significantly improve the accuracy of event stream classification tasks. Additionally, we investigate the impact of multiple-scale temporal resolutions on frame-based representation. Our approach is evaluated on three classification tasks: gesture recognition, image classification, and spoken digit recognition. We achieve state-of-the-art results on these tasks, with a substantial accuracy improvement (almost 19%) in gesture recognition using only a 60 ms timeframe.