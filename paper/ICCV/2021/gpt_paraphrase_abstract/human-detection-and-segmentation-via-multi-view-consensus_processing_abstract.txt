This abstract discusses the challenges of self-supervised detection and segmentation of foreground objects without annotated training data. It points out that existing approaches often rely on limited assumptions about appearance and motion. To address this, the authors propose a multi-camera framework that incorporates geometric constraints through multi-view consistency during training. This is achieved by using coarse 3D localization in a voxel grid and fine-grained offset regression. By learning a joint distribution of proposals across multiple views, the proposed method achieves superior performance compared to state-of-the-art techniques. The method is capable of handling scenes with dynamic activities and camera motion. During inference, the method operates on single RGB images. The results demonstrate improved performance on both visually distinct images and the classical Human3.6M dataset.