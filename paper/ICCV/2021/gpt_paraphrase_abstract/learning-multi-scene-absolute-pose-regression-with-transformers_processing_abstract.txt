This study introduces a new approach for learning multi-scene absolute camera pose regression using Transformers. Instead of using a convolutional backbone with a multi-layer perceptron head, the proposed method employs Transformers, with encoders aggregating activation maps through self-attention and decoders transforming latent features and scene encodings into pose predictions. This enables the model to focus on informative features for localization while embedding multiple scenes simultaneously. The performance of the proposed method is evaluated on indoor and outdoor datasets and compared to both multi-scene and state-of-the-art single-scene absolute pose regressors, demonstrating its superiority. The code for this method is publicly available at https://github.com/yolish/multi-scene-pose-transformer.