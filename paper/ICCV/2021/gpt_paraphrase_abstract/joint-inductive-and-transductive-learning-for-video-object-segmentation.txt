This study focuses on semi-supervised video object segmentation, which involves segmenting a target object in a video sequence based on a mask annotation in the first frame. Previous methods have relied on matching-based transductive reasoning or online inductive learning, but these approaches have limitations in terms of discriminative ability and utilization of spatio-temporal information. To address these issues, the authors propose a unified framework that integrates both transductive and inductive learning. The framework consists of two branches: a transduction branch that uses a lightweight transformer architecture to aggregate spatio-temporal cues, and an induction branch that performs online inductive learning to obtain discriminative target information. To connect these two branches, a two-head label encoder is introduced to learn the suitable target prior for each branch. The generated mask encodings are also disentangled to retain their complementarity. Experimental results on various benchmarks demonstrate that the proposed approach achieves state-of-the-art performance without relying on synthetic training data. The code for this approach is available at https://github.com/maoyunyao/JOINT.