In this paper, we investigate the learning of multi-scale feature representations in transformer models for image classification. We introduce a dual-branch transformer that combines image patches of different sizes to generate more powerful image features. Our approach involves processing small and large patches separately using branches with different computational complexities, and then leveraging attention to fuse these tokens multiple times in order to enhance their complementary nature. To optimize efficiency, we propose a token fusion module based on cross attention, which exchanges information between branches using a single token as a query. This cross-attention mechanism significantly reduces computational and memory complexity, requiring linear time instead of quadratic time. Extensive experiments demonstrate that our approach outperforms or performs on par with several existing works on vision transformer, as well as efficient CNN models. Notably, on the ImageNet1K dataset, our approach surpasses the recent DeiT model by a substantial margin of 2% while only experiencing a slight to moderate increase in computational operations and model parameters. The source codes and models for our approach are publicly available at https://github.com/IBM/CrossViT.