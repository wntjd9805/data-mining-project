Knowledge Distillation has proven to be effective in transferring learned representation from a larger model (teacher) to a smaller one (student). However, existing methods have overlooked the importance of retaining inter-channel correlation of features, resulting in a failure to capture the intrinsic distribution of the feature space and the diversity properties of features in the teacher network. In order to address this issue, we propose a novel approach called Inter-Channel Correlation for Knowledge Distillation (ICKD). This approach enables the student network to align its feature space diversity and homology with that of the teacher network. We interpret the correlation between channels as diversity if they are unrelated, and as homology if they are related. The student network is then trained to mimic this correlation within its own embedding space. Additionally, we introduce grid-level inter-channel correlation, making our approach suitable for dense prediction tasks. Through extensive experiments on ImageNet classification and Pascal VOC segmentation, we demonstrate the superiority of ICKD compared to existing methods. Our approach consistently outperforms others and sets a new state-of-the-art in the field of Knowledge Distillation. Notably, our ICKD method is the first to boost ResNet18 beyond 72% Top-1 accuracy on ImageNet classification. The code for our approach is publicly available at: https://github.com/ADLab-AutoDrive/ICKD.