The recently proposed Detection Transformer (DETR) model applies the Transformer architecture to object detection and performs similarly to two-stage object detection frameworks. However, DETR has slow convergence, requiring 500 epochs to achieve high accuracy. To address this issue, we propose a simple yet effective improvement called Spatially Modulated Co-Attention (SMCA) mechanism. SMCA conducts location-aware co-attention by emphasizing responses near estimated bounding box locations. By replacing the original co-attention mechanism in DETR with SMCA, we significantly accelerate DETR's convergence without modifying other operations. Additionally, our fully-fledged SMCA, which combines multi-head and scale-selection attention designs, outperforms DETR with a dilated convolution-based backbone in terms of mAP (45.6 at 108 epochs vs. 43.3 at 500 epochs). We validate SMCA through extensive experiments on the COCO dataset and provide the code for implementation.