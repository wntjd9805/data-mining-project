Autonomous driving has proven to be more challenging than expected, partly due to the difficulty of collecting labeled data for model training. Self-supervised learning (SSL), which uses unlabeled data for representation learning, shows promise in improving model performance. However, existing SSL methods often rely on single-centric-object assumptions, which may not be suitable for datasets with multiple instances like street scenes. To address this limitation, we identify two issues: defining positive samples for cross-view consistency and measuring similarity in multi-instance scenarios. We propose using an IoU threshold during random cropping to establish local-consistency from global-inconsistency. Additionally, we introduce two feature alignment methods for measuring similarity in multi-instance contexts. We also incorporate intra-image clustering with self-attention to further explore intra-image similarity and translation-invariance. Our experiments demonstrate that our method, Multi-instance Siamese Network (MultiSiam), significantly improves generalization ability and achieves state-of-the-art transfer performance on autonomous driving benchmarks such as Cityscapes and BDD100K. In comparison, existing SSL counterparts like MoCo, MoCo-v2, and BYOL exhibit a notable drop in performance. When pre-trained on the large-scale autonomous driving dataset SODA10M, MultiSiam outperforms the ImageNet pre-trained MoCo-v2, highlighting the potential of domain-specific pre-training. The code for MultiSiam is available at https://github.com/KaiChen1998/MultiSiam.