The motion compensation used in LiDAR point clouds collected from moving vehicles can potentially create a security vulnerability in deep networks. This is due to the susceptibility of deep learning and GPS-based vehicle trajectory estimation to wireless spoofing. In this study, we demonstrate for the first time that even small perturbations in a self-driving car's trajectory can make safety-critical objects undetectable or detected with incorrect positions, without directly tampering with the raw LiDAR readings. We also introduce a polynomial trajectory perturbation method that achieves a smooth and imperceptible attack. Extensive experiments on 3D object detection reveal that these attacks significantly impact the performance of state-of-the-art detectors and can be transferred to other detectors, highlighting the need for concern within the community. The code for this study is available at https://ai4ce.github.io/FLAT/.