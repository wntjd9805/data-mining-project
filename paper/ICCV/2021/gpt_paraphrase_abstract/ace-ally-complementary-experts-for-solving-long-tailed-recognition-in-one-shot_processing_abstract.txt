One-stage long-tailed recognition methods have a trade-off between accuracy for head classes and tail classes. Existing algorithms use a multi-stage training process, which is not easily adaptable to other computer vision tasks and is sensitive to the generalizability of pre-trained models. In this paper, we propose a one-stage long-tailed recognition scheme called ACE, where experts specialize in subsets of data and complement each other. We use a distribution-adaptive optimizer to prevent overfitting. The vanilla ACE method outperforms the current state-of-the-art method on various datasets and breaks the trade-off by improving accuracy for both majority and minority categories. Code and trained models are available at https://github.com/jrcai/ACE.