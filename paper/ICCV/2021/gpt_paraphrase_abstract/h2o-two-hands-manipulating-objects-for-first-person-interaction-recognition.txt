We introduce a comprehensive framework for recognizing egocentric interactions by using markerless 3D annotations of hands manipulating objects. Our approach involves creating a unified dataset for egocentric 3D interaction recognition. This dataset, named H2O (2 Hands and Objects), includes synchronized multi-view RGB-D images, interaction labels, object classes, ground-truth 3D poses for both hands, 6D object poses, camera poses, object meshes, and scene point clouds. It is the first benchmark that enables the study of first-person actions with detailed pose information for both hands and manipulated objects. We also propose a method to predict interaction classes by jointly estimating the 3D pose of hands and the 6D pose of objects from RGB images. Our method incorporates inter- and intra-dependencies between hands and objects through a graph convolutional network that predicts interactions. By utilizing this dataset, our method establishes a strong baseline for joint hand-object pose estimation and achieves state-of-the-art accuracy in first-person interaction recognition.