Deep metric learning has been successfully utilized in various visual tasks, such as image retrieval and clustering. Existing methods employ either a hard mining strategy or generate hard synthetic samples using an additional network to aid in training. However, these approaches encounter challenges that can result in biased embeddings, harder optimization, slower training speed, and increased model complexity. To address these issues, we propose a novel approach called LoOp, which identifies optimal hard negatives in the embedding space by calculating the minimum distance between pairs of positives and negatives. Unlike mining-based methods, our approach considers the entire space between pairs of embeddings when determining the optimal hard negatives. Through extensive experiments with representative metric learning losses, we demonstrate a significant improvement in performance across three benchmark datasets.