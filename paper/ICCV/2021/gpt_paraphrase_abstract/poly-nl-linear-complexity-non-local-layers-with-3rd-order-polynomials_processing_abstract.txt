Spatial self-attention layers, such as Non-Local blocks, are used in Convolutional Neural Networks to incorporate long-range dependencies by calculating pairwise similarities between all possible positions. However, the computational complexity of these pairwise functions scales quadratically with the input size in both space and time, which severely limits their practical application to moderately sized inputs. Previous approaches have focused on reducing complexity by modifying matrix operations, but in this study, we aim to maintain the full expressiveness of non-local layers while achieving linear complexity. We address the efficiency issue by treating non-local blocks as special cases of 3rd order polynomial functions. This allows us to propose a new fast method called "Poly-NL" that reduces the complexity from quadratic to linear without sacrificing performance. The Poly-NL approach performs competitively with state-of-the-art methods in tasks such as image recognition, instance segmentation, and face detection, while significantly reducing computational overhead.