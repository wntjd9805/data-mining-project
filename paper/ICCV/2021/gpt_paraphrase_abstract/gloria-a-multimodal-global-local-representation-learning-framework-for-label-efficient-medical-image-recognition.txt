The increasing use of medical imaging is placing a heavy burden on radiologists. Deep learning offers a potential solution for automatic medical image analysis and clinical decision support. However, obtaining large-scale labeled datasets for training deep neural networks in medical imaging is challenging and costly. This study aims to develop efficient labeling methods for multimodal medical imaging by utilizing radiology reports. We propose an attention-based framework that learns global and local representations by contrasting image sub-regions and words in the paired report. Additionally, we suggest leveraging these learned representations for various downstream tasks in medical image recognition with limited labels. Our results demonstrate high-performance and efficiency in tasks such as image-text retrieval, classification, and segmentation across different datasets.