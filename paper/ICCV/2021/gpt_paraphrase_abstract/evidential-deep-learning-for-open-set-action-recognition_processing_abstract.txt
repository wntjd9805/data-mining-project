This paper introduces the Deep EvidentialAction Recognition (DEAR) method, which aims to recognize actions in a real-world scenario where human actions may deviate from the training data. Unlike image data, recognizing video actions in an open-set setting is more challenging due to uncertain temporal dynamics and static bias. To address this, the authors propose a novel approach based on evidential deep learning (EDL) and model calibration to regularize the training. Additionally, a plug-and-play module is introduced to mitigate the static bias of video representation through contrastive learning. Experimental results demonstrate that the DEAR method consistently improves the performance of various action recognition models and benchmarks. The code and pre-trained models for DEAR are available at https://www.rit.edu/actionlab/dear.