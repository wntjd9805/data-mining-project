This paper introduces a new method called GeoWarp for image matching in visual geolocalization. The method focuses on dense local features, which are robust against changes in illumination and occlusions but not against viewpoint shifts. To address this limitation, GeoWarp incorporates invariance to viewpoint shifts during the extraction of dense features. This is achieved through a trainable module that learns the appropriate invariance for recognizing places. The module is trained using a new self-supervised loss and two weakly supervised losses, utilizing unlabeled data and weak labels. GeoWarp is implemented as a re-ranking method that can be easily integrated into existing visual geolocalization pipelines. Experimental validation on standard geolocalization benchmarks demonstrates that GeoWarp significantly enhances the accuracy of state-of-the-art retrieval architectures. The code and trained models will be made available upon acceptance of this paper.