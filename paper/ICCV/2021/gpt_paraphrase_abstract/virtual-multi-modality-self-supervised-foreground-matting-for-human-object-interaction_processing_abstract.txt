This research paper introduces a novel method called Virtual Multi-modality Foreground Matting (VMFM) for separating human-object interactive foreground from the background in raw RGB images. Unlike existing algorithms, VMFM does not require additional inputs such as trimap or known background. The method formulates foreground matting as a self-supervised multi-modality problem and utilizes three auto-encoders to factor each input image into estimated depth map, segmentation mask, and interaction heatmap. A dual encoder-to-decoder network is trained to estimate the alpha matte by fully utilizing the characteristics of each modality. Additionally, a self-supervised method called Complementary Learning (CL) is introduced to predict deviation probability map and exchange reliable gradients across modalities without labels. Extensive experiments were conducted to analyze the effectiveness of each modality and the significance of different components in complementary learning. The results demonstrate that the proposed VMFM model outperforms state-of-the-art methods.