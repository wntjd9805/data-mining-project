The rise of multimodal sensors and the widespread availability of the Internet have resulted in a vast amount of unlabeled multimodal data. This presents an interesting challenge in transferring a pre-trained unimodal network to effectively utilize this unlabeled multimodal data. In this study, we propose a framework called multimodal knowledge expansion (MKE) that is based on knowledge distillation. Unlike traditional knowledge distillation where the student model is designed to be inferior to the teacher model, we find that our multimodal student model consistently improves upon pseudo labels and generalizes better than its teacher. We conduct extensive experiments on four tasks and various modalities to validate this observation. Additionally, we establish a connection between MKE and semi-supervised learning, providing both empirical and theoretical explanations for the expansion capability of our multimodal student model.