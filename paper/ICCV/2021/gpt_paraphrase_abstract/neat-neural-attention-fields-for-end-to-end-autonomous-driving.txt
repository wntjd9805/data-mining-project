Efficient understanding of the structure of a scene in terms of its meaning, space, and time is essential for self-driving cars. We introduce NEAT (NEural ATtention fields), a unique representation that enables this understanding in end-to-end imitation learning models. NEAT is a continuous function that maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantic information. It achieves this by using attention maps to compress high-dimensional 2D image features into a concise representation. By doing so, our model can selectively focus on relevant regions in the input while disregarding irrelevant information, effectively connecting images with the BEV representation. In a challenging evaluation scenario involving adverse environmental conditions, NEAT outperforms several strong baseline models and achieves driving scores comparable to those of the expert CARLA driver used to generate its training data. Additionally, visualizing the attention maps for models with NEAT intermediate representations enhances interpretability.