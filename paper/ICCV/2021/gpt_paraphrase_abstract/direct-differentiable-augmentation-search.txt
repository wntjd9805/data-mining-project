Data augmentation is a valuable technique for improving the performance of deep neural networks. However, it is often difficult to transfer augmentation methods between different tasks and datasets. Recently, researchers have turned to AutoML techniques to automatically learn the best augmentation policy without manual tuning.In this paper, we propose a new algorithm called Direct Differentiable Augmentation Search (DDAS) that enables efficient augmentation search. DDAS utilizes meta-learning with one-step gradient updates and continuous relaxation to efficiently search for the best augmentation strategy. Unlike previous approaches that rely on approximations, such as Gumbel-Softmax or second-order gradient approximation, DDAS achieves efficient search without such approximations.To mitigate the negative effects of improper augmentations, we organize the search space into a two-level hierarchy. First, we decide whether to apply augmentation, and then we determine the specific augmentation policy. This hierarchical approach helps improve the effectiveness of the search.On standard image classification benchmarks, DDAS achieves state-of-the-art performance while significantly reducing the search cost. For example, on the CIFAR-10 dataset, DDAS only requires 0.15 GPU hours for searching augmentation policies. We also apply DDAS to the object detection task and achieve comparable performance to AutoAugment, but with a speed improvement of 1000 times.The code for DDAS is publicly available at https://github.com/zxcvfd13502/DDAS_code.