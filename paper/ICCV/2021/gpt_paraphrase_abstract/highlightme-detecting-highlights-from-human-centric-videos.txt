We introduce a domain- and user-agnostic method for identifying highlightable segments in videos that focus on humans. Our approach utilizes a graph-based representation of various human-centric features in the videos, such as poses and faces. By employing an autoencoder network with spatial-temporal graph convolutions, we can detect human activities and interactions based on these features. The network is trained to map the latent structural representations of the different features to highlight scores for each frame, considering their representativeness. These scores are then used to determine which frames to highlight and how to stitch them together to create excerpts. To train our network, we use the extensive AVA-Kinetics action dataset and evaluate its performance on four benchmark video highlight datasets: DSH, TVSum, PHD2, and SumMe. Remarkably, our method achieves a 4â€“12% increase in mean average precision compared to state-of-the-art techniques on these datasets, without relying on user preferences or fine-tuning specific to the dataset.