We introduce video classification models based on pure-transformer architectures, inspired by their success in image classification. Our models extract spatio-temporal tokens from the input video and encode them using transformer layers. To handle the long sequences of tokens in videos, we propose efficient variants of our model that factorize the spatial and temporal dimensions of the input. While transformer models typically require large training datasets, we demonstrate effective regularization techniques during training and leverage pretrained image models to train on smaller datasets. Through comprehensive ablation studies, we achieve state-of-the-art results on various video classification benchmarks, including Kinetics 400 and 600, Epic Kitchens, Something-Something v2, and Moments in Time. Our models outperform previous methods based on deep 3D convolutional networks.