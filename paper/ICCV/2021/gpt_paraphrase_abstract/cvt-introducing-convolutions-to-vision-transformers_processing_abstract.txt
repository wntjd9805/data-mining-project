This paper introduces a new architecture called Convolutional vision Transformer (CvT) that enhances the performance and efficiency of Vision Transformer (ViT). By incorporating convolutions into ViT, CvT combines the strengths of both designs. The improvements are achieved through two key modifications: a hierarchy of Transformers with a new convolutional token embedding, and a convolutional Transformer block with a convolutional projection. These changes bring desirable properties of convolutional neural networks (CNNs) to ViT, such as shift, scale, and distortion invariance, while retaining the advantages of Transformers, such as dynamic attention, global context, and better generalization. Extensive experiments confirm that CvT outperforms other Vision Transformers and ResNets on ImageNet-1k, while utilizing fewer parameters and lower FLOPs. Furthermore, CvT maintains its performance gains when pretrained on larger datasets like ImageNet-22k and fine-tuned for downstream tasks. CvT-W24, pretrained on ImageNet-22k, achieves a top-1 accuracy of 87.7% on the ImageNet-1k val set. Interestingly, this research also shows that the positional encoding, a crucial component in existing Vision Transformers, can be safely eliminated in CvT, simplifying the design for high-resolution vision tasks. The code for CvT is available at https://github.com/microsoft/CvT.