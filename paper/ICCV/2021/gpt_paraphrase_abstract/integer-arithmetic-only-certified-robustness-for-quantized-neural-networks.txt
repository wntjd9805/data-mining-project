The concept of adversarial data examples has garnered significant attention from both the machine learning and security communities. One approach to addressing these examples is through certified robustness using randomized smoothing, which offers a theoretical guarantee of robustness. However, this approach often relies on floating-point arithmetic for calculations during inference, leading to high memory usage and computational costs. As a result, these defensive models are not efficient on edge devices and cannot be deployed on integer-only logical units like Turing Tensor Cores or integer-only ARM processors. In order to overcome these challenges, we propose an alternative approach called integer randomized smoothing with quantization. This approach converts any classifier into a new smoothed classifier that utilizes integer-only arithmetic to achieve certified robustness against adversarial perturbations. We provide a rigorous guarantee of robustness under the ℓ2-norm for our proposed method. Additionally, we demonstrate that our approach achieves comparable accuracy and a 4× ∼ 5× speedup compared to floating-point arithmetic certified robust methods on general-purpose CPUs and mobile devices using two different datasets (CIFAR-10 and Caltech-101).