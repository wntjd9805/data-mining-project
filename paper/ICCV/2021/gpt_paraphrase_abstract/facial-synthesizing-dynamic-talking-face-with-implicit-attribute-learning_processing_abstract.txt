This paper proposes a method for generating realistic talking face videos by taking an audio signal and a reference video clip as input. The method synthesizes a photo-realistic video of the target face with natural lip movements, head poses, and eye blinks that are synchronized with the audio signal. The synthetic face attributes include both explicit attributes, such as lip motions correlated with speech, and implicit attributes, such as head poses and eye blinks weakly correlated with the audio. To model the complex relationships between these attributes and the input audio, the paper introduces a FACe Implicit Attribute Learning Generative Adversarial Network (FACIAL-GAN). This network integrates phonetics-aware, context-aware, and identity-aware information to synthesize 3D face animation with realistic motions. Additionally, a Rendering-to-Video network is used to generate photo-realistic output video frames by taking rendered face images and an attention map of eye blinks as input. Experimental results and user studies demonstrate that the proposed method generates realistic talking face videos with synchronized lip motions, natural head movements, and eye blinks, surpassing the quality of state-of-the-art methods.