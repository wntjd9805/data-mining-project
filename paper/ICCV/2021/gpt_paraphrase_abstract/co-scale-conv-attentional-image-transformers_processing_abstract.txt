This paper introduces Co-scale conv-attentional imageTransformers (CoaT), a Transformer-based image classifier that incorporates co-scale and conv-attentional mechanisms. The co-scale mechanism ensures that the Transformers' encoder branches maintain their integrity at different scales while allowing effective communication between representations learned at different scales. This is achieved through the use of serial and parallel blocks. Additionally, the conv-attentional mechanism employs a relative position embedding formulation in the factorized attention module, implemented efficiently using convolution-like techniques. CoaT enhances the multi-scale and contextual modeling capabilities of image Transformers. Experimental results on ImageNet demonstrate that even small CoaT models achieve superior classification performance compared to similarly-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT is further illustrated in object detection and instance segmentation, highlighting its applicability to various computer vision tasks.