High quality imaging typically requires expensive and bulky lenses to correct for geometric and chromatic aberrations. However, this poses limitations for low cost applications. While algorithmic reconstruction can remove artifacts caused by low-end lenses, the spatially varying nature of optical aberrations makes the computation inefficient. This often requires patch-wise optimization or training numerous local deep neural networks to achieve high reconstruction performance across the entire image. To address this issue, we propose a PSF (Point Spread Function) aware deep network that takes both the aberrant image and the PSF map as inputs. By incorporating deep priors, our method enables universal and flexible optical aberration correction. We pre-train a base model using a diverse set of lenses and then adapt it to a specific lens by quickly refining the parameters. This significantly reduces the time and memory consumption of model learning, making our approach highly efficient in both training and testing stages. Extensive results demonstrate the promising applications of our proposed approach for compact low-end cameras. An example of computationally reconstructing a high-quality image with a simple lens is provided, showcasing the effectiveness of our method. The code for our approach is available at https://github.com/leehsiu/UABC.