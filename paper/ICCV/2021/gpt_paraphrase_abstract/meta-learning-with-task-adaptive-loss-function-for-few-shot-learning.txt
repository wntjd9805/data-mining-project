Few-shot learning aims to generalize and perform well on new unseen examples with limited labeled examples per task. Model-agnostic meta-learning (MAML) is a popular method for few-shot learning due to its flexibility and applicability. However, MAML and its variants often rely on a simple loss function, lacking auxiliary loss functions or regularization terms that can enhance generalization. This limitation arises from the fact that different applications and tasks may require distinct auxiliary loss functions. Instead of manually designing auxiliary loss functions for each task, we propose a new meta-learning framework called MeTAL. MeTAL incorporates a task-adaptive loss function that adapts to each specific task. Our framework demonstrates effectiveness and flexibility across diverse domains, including few-shot classification and regression.