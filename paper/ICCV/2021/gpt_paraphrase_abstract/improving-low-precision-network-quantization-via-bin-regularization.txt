Model quantization is a crucial method for deploying deep neural networks on devices with limited resources. It involves reducing the precision of weights and activations to improve energy efficiency. However, maintaining high accuracy becomes difficult as the precision decreases, especially for low-precision networks like 2-bit MobileNetV2. Existing approaches have aimed to tackle this challenge by minimizing quantization errors or replicating the data distribution of full-precision networks. In this study, we propose a novel weight regularization algorithm to enhance the quantization of low-precision networks. Instead of constraining the overall data distribution, our approach optimizes each element within quantization bins to closely match the desired quantized value. This bin regularization mechanism promotes sharp weight distributions within each bin, ideally resembling a Dirac delta distribution. Experimental results demonstrate that our method consistently outperforms state-of-the-art quantization-aware training methods for various low-precision networks. Notably, our bin regularization improves the top-1 accuracy of LSQ for 2-bit MobileNetV2 and MobileNetV3-Small on ImageNet by 3.9% and 4.9% respectively.