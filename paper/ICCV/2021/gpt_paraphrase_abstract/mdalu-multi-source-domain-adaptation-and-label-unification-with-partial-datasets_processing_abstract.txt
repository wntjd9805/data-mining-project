This study addresses the challenge of generalizing object recognition to new domains, more classes, and different modalities. It proposes a novel approach to combine and reuse existing datasets from different domains with partial annotations and varying data modalities. The proposed method includes two stages: a partially-supervised adaptation stage and a fully-supervised adaptation stage. In the first stage, knowledge from multiple source domains is transferred to the target domain and integrated using three new modules: domain attention, uncertainty maximization, and attention-guided adversarial alignment. These modules help mitigate negative transfer between unmatching label spaces. In the second stage, knowledge is transferred in a unified label space after completing the labels with pseudo-labels. Extensive experiments on image classification, 2D semantic image segmentation, and joint 2D-3D semantic segmentation tasks demonstrate the superior performance of the proposed method compared to other existing methods.