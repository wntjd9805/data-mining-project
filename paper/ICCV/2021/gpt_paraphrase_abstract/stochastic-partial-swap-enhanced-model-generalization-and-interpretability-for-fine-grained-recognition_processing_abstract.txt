We propose a new method called Stochastic Partial Swap (SPS) to improve the learning of mid-level representation for fine-grained recognition. Currently, this learning process is hindered by a small number of highly discriminative patterns, which limits its robustness and generalization capability. Our method addresses this issue by introducing noise through element-wise swapping of partial features between samples during training. This has a regularization effect similar to Dropout, promoting the representation of more neurons and concepts. Additionally, our method suppresses over-activation of certain patterns to enhance feature representativeness and enriches pattern combinations to improve classifier generalization. We conducted extensive experiments on different network backbones and fine-grained datasets to validate the effectiveness of our approach. Furthermore, we demonstrate that our method can complement high-level representations and achieve performance comparable to top-performing technologies in fine-grained recognition, indoor scene recognition, and material recognition. Importantly, our method also improves model interpretability.