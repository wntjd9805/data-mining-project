Domain adaptation is crucial for addressing the lack of annotations in a new domain, particularly in the context of 3D semantic segmentation of point clouds. With the availability of multi-modal datasets that include both 2D images and 3D point clouds, we propose leveraging 2D data for 3D domain adaptation through intra and inter domain cross modal learning. Existing approaches for intra-domain cross modal learning often result in the abandonment of valuable 2D features by sampling dense 2D pixel-wise features to match the sparse 3D point-wise features. To overcome this limitation, we introduce Dynamic sparse-to-dense Cross Modal Learning (DsCML) to enhance the interaction of multi-modality information for domain adaptation. For inter-domain cross modal learning, we further advance Cross Modal Adversarial Learning (CMAL) on 2D and 3D data with different semantic content to promote high-level modal complementarity. Our model is evaluated in various multi-modality domain adaptation scenarios, such as day-to-night, country-to-country, and dataset-to-dataset, and demonstrates significant improvements over both uni-modal and multi-modal domain adaptation methods in all settings. The code for our model is available at https://github.com/leolyj/DsCML.