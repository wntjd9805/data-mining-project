We present BraVe, a self-supervised learning framework designed specifically for video data. While most successful methods in self-supervised learning align representations of two independent views extracted from data, they fail to consider the temporal aspect in videos. In BraVe, one view is given a narrow temporal window of the video, while the other view has broader access to the entire video content. Our models are trained to generalize from the narrow view to the overall content of the video. Additionally, BraVe employs different backbones for processing the views, allowing alternative augmentations or modalities such as optical flow, randomly convolved RGB frames, audio, or their combinations in the broader view. Through experiments on standard video and audio classification benchmarks like UCF101, HMDB51, Kinetics, ESC-50, and AudioSet, we demonstrate that BraVe achieves state-of-the-art results in self-supervised representation learning.