Representing sign languages in a phonetic level, similar to spoken languages, involves breaking down motion into its basic components. Traditional methods of Sign Language Production (SLP) using avatars have followed this approach by constructing animations from sequences of hand motions, shapes, and facial expressions. However, recent deep learning approaches have used a single network to estimate the complete skeletal structure for SLP. In this study, we propose dividing the SLP task into two sub-tasks that are jointly trained. The first sub-task translates spoken language into a latent sign language representation with gloss supervision. The second sub-task focuses on generating expressive sign language sequences that closely resemble the learned spatio-temporal representation. For the translation sub-task, we utilize a progressive transformer and introduce a novel architecture called Mixture of Motion Primitives (MOMP) for sign language animation. During training, distinct motion primitives are learned and can be combined at inference to create continuous sign language sequences. We evaluate our approach on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset, conducting extensive ablation studies and user evaluations. Our results demonstrate that MOMP outperforms baseline methods, achieving state-of-the-art performance with an 11% improvement in back translation. Notably, we showcase stronger performance for translating from spoken language to sign language compared to translating from gloss to sign language.