We focus on the task of Re-Identification (Re-ID) in multi-target multi-camera (MTMC) tracking, where we aim to track multiple pedestrians using multiple uncalibrated cameras that overlap spatially. These synchronized videos allow us to observe individuals from different viewpoints and link their movements across cameras. To accurately associate pedestrians seen from multiple views within the same time window, we utilize a visual feature extracted from a track-let, which represents its similarity and dissimilarity to other candidate track-lets. Our approach incorporates an inter-tracklet attention mechanism that learns a representation for a target track-let while considering other track-lets across multiple views. Additionally, we introduce a second intra-tracklet attention module that captures the gait and motion of an individual, using position embeddings and a transformer encoder to learn features from a sequence of track-let features. Experimental results on the WILDTRACK and 'ConstructSite' datasets demonstrate that our model outperforms state-of-the-art ReID methods by 5% and 10% respectively in the context of uncalibrated MTMC tracking. Moreover, our model achieves state-of-the-art performance on two other benchmark datasets (MARS and DukeMTMC) that involve non-overlapping cameras.