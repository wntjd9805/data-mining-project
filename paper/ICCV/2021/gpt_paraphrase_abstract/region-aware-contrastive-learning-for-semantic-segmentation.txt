Recent advancements in semantic segmentation have shown significant progress by utilizing contextual information within individual images and training models using pixel-wise cross entropy loss. However, it has been observed that semantic relations exist not only within a single image but also across the entire training dataset. This implies that considering intra-image correlations alone is insufficient. Building upon recent developments in unsupervised contrastive learning, we propose a supervised method called region-aware contrastive learning (RegionContrast) for semantic segmentation. Our objective is to enhance the similarity between semantically similar pixels while maintaining discrimination from others. To achieve this, we employ contrastive learning and utilize a memory bank to store representative features. In order to efficiently incorporate all training data into the memory bank without excessive computational resource usage, we introduce region centers that represent features from different categories for each image. As a result, the proposed region-aware contrastive learning operates at a region level for all training data, saving more memory compared to methods that explore pixel-level relations. Our approach incurs minimal computational cost during training and does not require any additional overhead during testing. Extensive experiments on three benchmark datasets (Cityscapes, ADE20K, and COCO Stuff) demonstrate that our method achieves state-of-the-art performance.