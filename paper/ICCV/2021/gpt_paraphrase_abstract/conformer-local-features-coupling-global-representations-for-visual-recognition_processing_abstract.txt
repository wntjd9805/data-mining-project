This paper introduces a novel network structure called Conformer that combines the strengths of convolutional operations and self-attention mechanisms to enhance representation learning. While convolution operations are effective in extracting local features, they struggle to capture global representations. On the other hand, self-attention modules in visual transformers excel at capturing long-distance feature dependencies but tend to lose local feature details. Conformer addresses these limitations by employing a Feature Coupling Unit (FCU) that integrates local features and global representations at different resolutions interactively. The concurrent structure of Conformer ensures that both local features and global representations are preserved to the maximum extent. Experimental results demonstrate that Conformer outperforms visual transformers (DeiT-B) by 2.3% on ImageNet, and surpasses ResNet-101 by 3.7% and 3.6% in mean average precision (mAP) for object detection and instance segmentation, respectively, on MSCOCO. These results highlight the potential of Conformer as a versatile backbone network. The code for Conformer is available at github.com/pengzhiliang/Conformer.