Real-time video inference on edge devices, such as mobile phones and drones, is difficult due to the high computational cost of Deep Neural Networks (DNNs). This paper introduces AdaptiveModel Streaming (AMS), a novel approach to enhancing the performance of lightweight models for video inference on edge devices. AMS leverages a remote server to continuously train and adapt a small model running on the edge device, improving its performance on live video by distilling knowledge from a larger, state-of-the-art model. The paper addresses the challenges of adapting models over the network for video inference and proposes various techniques to reduce communication costs, including avoiding overfitting, updating only important model parameters, and adaptively sampling training frames on edge devices. Experimental results on video semantic segmentation demonstrate a 0.4-17.8% improvement in mean Intersection-over-Union compared to a pre-trained model across multiple video datasets. The prototype implementation achieves video segmentation at 30 frames-per-second, with a camera-to-label latency of 40 milliseconds on a Samsung Galaxy S10+ mobile phone, utilizing less than 300 Kbps of uplink and downlink bandwidth on the device.