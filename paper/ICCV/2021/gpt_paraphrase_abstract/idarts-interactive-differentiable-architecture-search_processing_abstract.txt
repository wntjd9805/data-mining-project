DARTS, a method for architecture search, has improved efficiency by learning architecture and network parameters simultaneously. However, it neglects the intrinsic relationship between these parameters, resulting in sub-optimal optimization. This is because the gradient descent method used in DARTS does not consider the coupling relationship of the parameters, leading to degraded optimization. To address this, we propose IDARTS, which formulates DARTS as a bi-linear optimization problem. We introduce a backtracking backpropagation process that decouples the relationships of different parameters and trains them within the same framework. This backtracking method coordinates the training of different parameters, exploring their interaction and optimizing training. Experimental results on CIFAR10 and ImageNet datasets demonstrate the efficacy of IDARTS, achieving a top-1 accuracy of 76.52% on ImageNet without additional search cost, compared to 75.8% with the state-of-the-art method PC-DARTS.