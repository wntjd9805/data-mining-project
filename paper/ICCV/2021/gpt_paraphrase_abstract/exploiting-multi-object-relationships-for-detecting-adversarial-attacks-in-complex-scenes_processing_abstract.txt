Adversarial attacks on vision systems using Deep Neural Networks (DNNs) can be detected by checking the intrinsic consistencies in the input data. However, existing approaches are model-specific and lack generalizability. In this study, we propose a novel approach that leverages language descriptions of natural scene images to perform context consistency checks. This approach is independent of the object detector used and achieves high accuracy in detecting adversarial examples in practical scenes with multiple objects. Experimental results on the PASCAL VOC and MS COCO datasets demonstrate that our method surpasses state-of-the-art techniques in detecting adversarial attacks.