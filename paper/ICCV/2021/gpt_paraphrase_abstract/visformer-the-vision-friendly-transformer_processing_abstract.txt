In the past year, there has been significant progress in using the Transformer module for vision problems. While some researchers have shown that Transformer-based models are effective at fitting data, there is growing evidence that these models tend to overfit when the training data is limited. This study aims to address this issue by gradually transitioning a Transformer-based model to a convolution-based model. Through this process, valuable insights are gained for improving visual recognition. Building on these findings, a new architecture called Visformer (short for 'Vision-friendly Transformer') is proposed. Visformer outperforms both Transformer-based and convolution-based models in terms of ImageNet classification accuracy, especially when the model complexity is lower or the training set is smaller. The code for Visformer can be found at https://github.com/danczs/Visformer.