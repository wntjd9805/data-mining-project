We introduce a novel approach called Mesh Graphormer, which combines graph convolutions and self-attentions in a transformer framework to reconstruct 3D human pose and mesh from a single image. Previous methods have shown promise in human mesh reconstruction using either transformers or graph convolutional neural networks (GC-NNs). Transformers excel at capturing non-local interactions among 3D mesh vertices and body joints, while GC-NNs are adept at leveraging neighborhood vertex interactions based on a predefined mesh topology. Our method aims to model both local and global interactions by integrating graph convolutions and self-attentions in a transformer. We evaluate our approach on various benchmarks, including Human3.6M, 3DPW, and FreiHAND datasets, and demonstrate that Mesh Graphormer outperforms state-of-the-art methods significantly. Code and pre-trained models are available at https://github.com/microsoft/MeshGraphormer.