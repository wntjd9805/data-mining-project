In the field of generalized zero-shot learning (GZSL), the objective is to classify samples using the assumption that some classes are not observable during training. Existing GZSL methods often associate visual features of seen classes with attributes or generate unseen samples directly to bridge the gap between seen and unseen classes. However, these approaches do not necessarily encode semantically related information that the shared attributes refer to, resulting in a degradation of model generalization to unseen classes. To address this issue, we propose a novel framework called Semantics Disentangling for Generalized Zero-Shot Learning (SDGZSL). In this framework, the visual features of unseen classes are estimated using a conditional VAE (Variational Autoencoder) and then factorized into semantic-consistent and semantic-unrelated latent vectors. We apply a total correlation penalty to ensure independence between the two factorized representations, and measure the semantic consistency using a derived relation network. We conduct extensive experiments on four GZSL benchmark datasets, which demonstrate that the semantic-consistent features disentangled by SDGZSL are more generalizable in both canonical and generalized zero-shot learning tasks. The source code for our proposed method is available at https://github.com/uqzhichen/SDGZSL.