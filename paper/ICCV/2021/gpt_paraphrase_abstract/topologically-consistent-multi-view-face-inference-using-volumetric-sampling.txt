High-fidelity face digitization methods often utilize multi-view stereo techniques and non-rigid registration to reconstruct 3D models and establish correspondence across different identities and expressions. However, these methods often require manual clean-up due to noise, outliers, and hairy surface regions in the 3D scans. Additionally, mesh registration struggles with extreme facial expressions. Most existing learning-based approaches rely on a 3D morphable model (3DMM) for robustness, but this limits accuracy for extreme expressions and cannot tightly fit ground truth surfaces. To address these limitations, we propose a new framework called ToFu (Topological consistent Face from multi-view). ToFu uses a volumetric representation instead of a 3DMM to generate topologically consistent meshes across identities and expressions. Our progressive mesh generation network embeds the face's topological structure in a feature volume, which is derived from geometry-aware local features. This architecture enables dense and accurate facial mesh predictions with consistent mesh topology. ToFu also captures displacement maps for pore-level geometric details and enables high-quality rendering, including albedo and specular reflectance maps. These assets can be readily used by production studios for avatar creation, animation, and physically-based skin rendering. Our approach achieves state-of-the-art geometric and correspondence accuracy, while being significantly faster than traditional techniques, taking only 0.385 seconds to compute a mesh with 10K vertices. The code and model for ToFu are available for research purposes at https://tianyeli.github.io/tofu.