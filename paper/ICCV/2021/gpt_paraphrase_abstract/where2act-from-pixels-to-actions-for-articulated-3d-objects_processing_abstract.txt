This study aims to enhance visual perception to enable agents to interact effectively with their surroundings. The focus is on extracting precise and localized information pertaining to basic actions, such as pushing or pulling, specifically for articulated objects with movable components. For instance, when presented with a drawer, our network can predict that pulling the handle will open the drawer. We present and evaluate new network architectures that can utilize image and depth data to anticipate the potential actions at each pixel and identify the regions of articulated parts likely to be affected by external forces. To achieve this, we propose a learning-from-interaction framework combined with an online data sampling strategy, which facilitates training the network using simulation (SAPIEN) and ensures generalizability across different object categories. The code and data release can be accessed on our website.