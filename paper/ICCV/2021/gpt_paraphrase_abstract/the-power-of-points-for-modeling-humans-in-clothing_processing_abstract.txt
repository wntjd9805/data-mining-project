The current process of creating realistic 3D human avatars with movable clothing requires manual effort from artists. Despite advancements in 3D scanning and modeling of human bodies, there is still no technology that can easily transform a static scan into an animatable avatar. Automating the creation of such avatars would have numerous applications in gaming, social networking, animation, and augmented/virtual reality (AR/VR). The main challenge lies in representing the complex topology of clothing, as standard 3D meshes used for modeling minimally-clothed bodies do not capture this effectively. Implicit surface models have gained interest for this task, but they are computationally intensive and not easily compatible with existing 3D tools. What is needed is a 3D representation that can accurately capture varied topology at a high resolution, and can be learned from data. We propose that this representation already exists in the form of point clouds. Point clouds possess properties of both implicit and explicit representations, which we leverage to model 3D garment geometry on a human body. We train a neural network using a novel local clothing geometric feature to represent the shape of different outfits. The network is trained on 3D point clouds of various clothing types, on different bodies, in different poses, and it learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, allowing for realistic reposing. Our model demonstrates superior quantitative and qualitative results in both multi-outfit modeling and animation of unseen outfits. The code for our model is available for research purposes at https://qianlim.github.io/POP.