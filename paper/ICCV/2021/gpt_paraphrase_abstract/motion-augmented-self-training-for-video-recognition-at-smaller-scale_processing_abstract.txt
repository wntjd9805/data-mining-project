In this paper, we aim to develop a 3D convolutional neural network that can be trained on unlabeled video collections for use on small-scale video datasets. We focus on utilizing motion rather than appearance for smaller datasets, but we want to avoid the computational complexity of optical flow during inference. To achieve this, we propose a self-training method called MotionFit, which combines supervised training of a motion model on a labeled video collection with pseudo-label generation for an unlabeled video collection. We then use an appearance model to learn to predict these pseudo-labels. We also introduce a multi-clip loss to enhance the quality of the pseudo-labeling. Additionally, we take into account the temporal granularity of videos during self-training, which has been overlooked in previous studies. The resulting motion-augmented representation model is well-suited for tasks like action recognition and clip retrieval in video datasets. MotionFit outperforms alternative methods for knowledge transfer by 5%-8%, video-only self-supervision by 1%-7%, and semi-supervised learning by 9%-18% when using the same number of class labels in small-scale video datasets.