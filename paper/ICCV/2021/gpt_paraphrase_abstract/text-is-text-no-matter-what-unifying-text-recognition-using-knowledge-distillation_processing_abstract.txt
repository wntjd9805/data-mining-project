Text recognition is a widely studied topic in computer vision due to its numerous commercial applications. Traditionally, research efforts have been divided into two categories: Scene Text Recognition (STR) for text in everyday scenes, and Handwriting Text Recognition (HTR) for handwritten text. In this paper, we propose a unified approach that combines both STR and HTR into a single model. We initially observe that combining the two models leads to a decrease in performance due to the differences in their inherent challenges. To address this, we introduce a knowledge distillation (KD) based framework that tackles the variable-length and sequential nature of text sequences. We propose four distillation losses specifically designed for text recognition to handle these unique characteristics. Our empirical results demonstrate that our unified model performs equally well or even better than individual models in certain cases. We also conduct experiments to validate our design choices, showing that alternative approaches such as two-stage frameworks, multi-task learning, and domain adaptation/generalization do not yield satisfactory results.