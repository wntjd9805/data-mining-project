Recently, there has been a significant focus on video deblurring, with research suggesting that high time rate events can enhance the deblurring process. However, existing methods for video deblurring assume consecutive blurry frames and overlook the fact that sharp frames are often nearby. This paper introduces D2Nets, a framework for video deblurring that takes into account non-consecutive blurry frames. D2Nets utilizes a bidirectional LST-M detector to detect the nearest sharp frames (NSFs) and then uses them as guidance for the deblurring process. Additionally, this paper proposes a flexible event fusion module (EFM) to bridge the gap between event-driven and video deblurring. The EFM can be easily incorporated into D2Nets or existing deblurring networks, resulting in improved deblurring performance. Experimental results on synthetic and real-world blurry datasets demonstrate that our approach outperforms competing methods, and the EFM significantly enhances the performance of other deblurring networks as well.