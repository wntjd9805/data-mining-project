We present HAA5001, a manually annotated dataset for recognizing human-centric atomic actions. It contains 500 classes and over 591,000 labeled frames. Unlike other atomic action datasets, HAA5001 focuses on fine-grained atomic actions to minimize classification ambiguities. Each class represents consistent actions, such as "Baseball Pitching" or "Free Throw in Basketball," rather than generic actions like "Throw." The dataset has been carefully curated to emphasize precise human movement and eliminate irrelevant motions and label noises. HAA5001 offers several advantages, including a high percentage of detectable joints in human poses (69.7% on average), scalability for adding new classes, curated videos with essential elements of atomic actions, and fine-grained atomic action classes. Through extensive experiments, including cross-data validation using real-world datasets, we demonstrate the benefits of HAA5001's human-centric and atomic characteristics. Even a baseline deep learning model can improve prediction by attending to atomic human poses. We provide detailed statistics and collection methodology for the HAA5001 dataset and compare its quantitative features with existing action recognition datasets.