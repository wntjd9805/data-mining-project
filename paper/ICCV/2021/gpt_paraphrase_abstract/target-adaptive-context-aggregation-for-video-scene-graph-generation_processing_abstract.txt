This paper focuses on the challenging task of video scene graph generation (VidSGG), which aims to provide a structured video representation for high-level understanding tasks. The authors propose a new approach called TRACE (Target Adaptive Context Aggregation Network) that decouples the context modeling for relation prediction from the complex entity tracking process. The TRACE framework is designed to efficiently perform frame-level VidSGG by capturing spatio-temporal context information for relation recognition. It consists of two key components: Hierarchical Relation Tree (HRTree) construction and Target-adaptive Context Aggregation. The HRTree provides an adaptive structure for organizing relation candidates and helps guide the context aggregation module in capturing spatio-temporal structure information. This leads to the generation of a contextualized feature representation for each relation candidate, which is then classified into its corresponding category.To achieve video-level VidSGG, a simple temporal association strategy is employed to track the detected results obtained from TRACE. The authors conduct experiments on two VidSGG benchmarks, namely ImageNet-VidVRD and Action Genome, and demonstrate that TRACE outperforms existing methods, achieving state-of-the-art performance.The code and models for TRACE are made publicly available at https://github.com/MCG-NJU/TRACE.