Labeled data for semantic segmentation is costly and time-consuming to collect due to the need for detailed pixel-level annotations. While recent semantic segmentation approaches using Convolutional Neural Networks (CNNs) have achieved impressive results with large amounts of labeled data, their performance declines significantly with limited labeled data. This is because deep CNNs trained with cross-entropy loss tend to overfit small amounts of labeled data. To tackle this issue, we propose a simple and effective training strategy based on contrastive learning. First, we pretrain the network using a pixel-wise, label-based contrastive loss, and then fine-tune it with cross-entropy loss. This strategy enhances intra-class compactness and inter-class separability, resulting in improved pixel classification. We validate the effectiveness of our approach using the Cityscapes and PASCAL VOC 2012 segmentation datasets. Our results demonstrate that pretraining with the proposed contrastive loss yields significant performance improvements (over 20% absolute improvement in some cases) with limited labeled data. In many cases, our contrastive pretraining strategy, which does not require additional data, matches or surpasses the widely-used ImageNet pretraining strategy that utilizes over a million extra labeled images.