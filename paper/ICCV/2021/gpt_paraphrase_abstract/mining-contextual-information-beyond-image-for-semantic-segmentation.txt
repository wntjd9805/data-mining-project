This study focuses on the problem of context aggregation in semantic image segmentation. Previous research has mainly concentrated on improving pixel representations by aggregating contextual information within individual images. However, these methods overlook the importance of pixel representations of the corresponding class beyond the input image. To address this, the paper proposes a method to mine contextual information beyond individual images in order to enhance pixel representations.   The proposed approach includes a feature memory module that is dynamically updated during training to store dataset-level representations of different categories. Next, the class probability distribution of each pixel representation is learned using ground-truth segmentation as supervision. Finally, the representation of each pixel is augmented by aggregating dataset-level representations based on the corresponding class probability distribution.   Additionally, the paper introduces a representation consistent learning strategy that utilizes the stored dataset-level representations. This strategy aims to enhance the classification head's ability to address both intra-class compactness and inter-class dispersion. The proposed method can be easily integrated into existing segmentation frameworks such as FCN, PSPNet, OCRNet, and DeepLabV3, and consistently improves performance. By mining contextual information beyond the image, the proposed method achieves state-of-the-art performance on various benchmark datasets including ADE20K, LIP, Cityscapes, and COCO-Stuff 1.