Most video analysis tasks require detecting changes in content over time, but existing models are typically pre-trained on general video action classification tasks. This is because annotating temporal boundaries in untrimmed videos is expensive, resulting in a lack of suitable datasets for pre-training models that are sensitive to temporal boundaries. In this paper, we propose a novel approach called boundary-sensitive pretext (BSP) task to address this issue. Instead of relying on manual annotations, we synthesize temporal boundaries in existing video action classification datasets. By defining different methods for synthesizing boundaries, BSP can be conducted in a self-supervised manner by classifying the boundary types. This allows us to learn video representations that are more transferable to temporal localization tasks. Extensive experiments demonstrate that BSP outperforms existing action classification-based pre-training methods and achieves state-of-the-art performance on various temporal localization tasks. For more details, please visit our website: https://frostinassiky.github.io/bsp.