Despite recent advancements in video-based person re-identification (re-ID), the current best methods still face challenges in real-world scenarios such as appearance similarity among different individuals, occlusions, and frame misalignment. To address these issues, we propose a new approach called Spatio-Temporal Representation Factorization (STRF), which is a versatile computational unit that can be integrated with most existing 3D convolutional neural network architectures for re-ID. The main innovations of STRF compared to previous work include explicit pathways for learning discriminative temporal and spatial features. Each component of STRF is further factorized to capture person-specific appearance and motion information that complements each other. Specifically, the temporal factorization consists of two branches: one for static features (e.g., clothing color) that do not change significantly over time, and another for dynamic features (e.g., walking patterns) that do change over time. Additionally, the spatial factorization also consists of two branches to learn global (coarse segments) and local (finer segments) appearance features. The inclusion of local features proves particularly beneficial in cases of occlusion or spatial misalignment. By combining these two factorization operations, we create a modular architecture for our parameter-wise lightSTRF unit, which can be inserted between any two 3D convolutional layers, resulting in an end-to-end learning framework. Through empirical evaluation, we demonstrate that STRF enhances the performance of various existing baseline architectures, and we achieve new state-of-the-art results on three benchmarks using standard person re-ID evaluation protocols.