We present a new task called weakly supervised learning for identifying human and object interactions in videos. This task is challenging because the system lacks knowledge about the specific types of interactions or the exact location of the human and object in the video. To overcome these challenges, we propose a contrastive weakly supervised training loss that associates spatiotemporal regions in the video with an action and object vocabulary. This loss also promotes the temporal consistency of moving objects' visual appearance as a form of self-supervision. To train our model, we create a dataset of over 6.5k videos with annotations of human-object interactions, which are generated semi-automatically from sentence captions linked to the videos. Our model outperforms weakly supervised baselines adapted to our task on our video dataset.