This study focuses on the challenge of autonomous navigation, which requires a structured representation of the road network and identification of other traffic agents. Typically, autonomous cars have horizontally mounted cameras for a better view, making the task of scene understanding in the bird's-eye-view (BEV) particularly difficult. The goal of this work is to extract a directed graph representing the local road network in BEV coordinates from a single onboard camera image. Additionally, the method is extended to detect dynamic objects on the BEV plane. By incorporating the semantics, locations, and orientations of the detected objects with the road graph, a comprehensive understanding of the scene is achieved. This understanding is crucial for downstream tasks like path planning and navigation. The proposed approach is compared against strong baselines and demonstrates superior performance. Ablation studies are also conducted to explore the effects of different design choices. The code for this study can be found at: https://github.com/ybarancan/STSU.