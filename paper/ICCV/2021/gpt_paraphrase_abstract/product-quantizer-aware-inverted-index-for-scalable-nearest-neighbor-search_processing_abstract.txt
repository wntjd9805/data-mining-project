The inverted index is commonly used for non-exhaustive nearest neighbor search on large datasets. It accelerates the process by reducing the number of distance computations and database size. The inverted index enables product quantization (PQ) to learn codewords in the residual vector space, which improves quantization error. However, it is unclear why the inverted index and product quantizer are optimized separately when they are closely related. Changes to the inverted index can distort the residual vector space. To address this, we propose a joint optimization of coarse and fine quantizers by replacing the original objective of the coarse quantizer with end-to-end quantization distortion. Our method is applicable to different combinations of quantizers. The figure demonstrates the difference between conventional inverted index and our proposed method in terms of coarse center updates. The proposed method considers both the distortion of coarse centers and the quantization error of the fine quantizer.