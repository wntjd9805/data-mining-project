Contrastive learning has shown promise in unpaired image-to-image translation, but the resulting translations often suffer from poor quality and inconsistent preservation of content. This paper addresses the issue by highlighting the crucial role of negative examples in contrastive learning for image translation. Previous methods randomly sampled negative examples from different positions in the source image, which proved ineffective in pushing positive examples closer to query examples. To tackle this problem, the paper introduces NEGCUT (Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired image-to-image Translation), which trains a generator to produce negative examples dynamically. The generator is unique in two ways: firstly, it is instance-wise, meaning that the generated examples are based on the input image, and secondly, it has the ability to generate hard negative examples due to its training with an adversarial loss. By incorporating this generator, the performance of unpaired image-to-image translation is significantly enhanced. Experiments conducted on three benchmark datasets confirm that the proposed NEGCUT framework outperforms previous methods and achieves state-of-the-art results.