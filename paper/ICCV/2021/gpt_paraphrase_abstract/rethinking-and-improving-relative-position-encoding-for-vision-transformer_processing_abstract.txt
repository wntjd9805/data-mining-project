Relative position encoding (RPE) plays a crucial role in transformers by capturing the sequential order of input tokens. While its effectiveness has been extensively studied in natural language processing, its applicability in computer vision remains uncertain. Specifically, it is unclear whether relative position encoding can perform equally well as absolute position encoding. To address this uncertainty, this study reviews existing methods of relative position encoding and evaluates their advantages and disadvantages when applied to vision transformers. Additionally, this study introduces novel relative position encoding methods, termed image RPE (iRPE), specifically designed for 2D images. The proposed iRPE methods incorporate directional relative distance modeling and consider the interactions between queries and relative position embeddings in the self-attention mechanism. These methods are simple, lightweight, and easily integrated into transformer blocks. Experimental results demonstrate that solely due to the proposed encoding methods, DeiT and DETR achieve stable improvements of up to 1.5% (top-1 Accuracy) and 1.3% (mean Average Precision) respectively on ImageNet and COCO datasets, without requiring adjustments to other hyperparameters such as learning rate and weight decay. Ablation studies and analysis reveal interesting findings, some of which challenge previous understanding. The code and models used in this study are publicly available at https://github.com/microsoft/Cream/tree/main/iRPE.