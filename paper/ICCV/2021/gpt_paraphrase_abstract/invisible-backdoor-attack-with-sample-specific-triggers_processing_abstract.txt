In recent times, there has been a rise in backdoor attacks that pose a significant security threat to deep neural network (DNN) training processes. These attacks involve injecting hidden backdoors into DNNs, allowing the attacker to manipulate the model's predictions when triggered. Existing backdoor attacks typically use sample-agnostic triggers, meaning different poisoned samples contain the same trigger. This makes these attacks easily detectable and mitigated by current defenses. In this study, we introduce a new attack paradigm where backdoor triggers are sample-specific. Our approach involves modifying certain training samples with invisible perturbations, without the need to manipulate other training components like loss or model structure as required by existing attacks. We draw inspiration from recent advancements in DNN-based image steganography and generate sample-specific invisible additive noises as backdoor triggers. We achieve this by encoding an attacker-specified string into benign images using an encoder-decoder network. The mapping from the string to the target label is generated during DNN training on the poisoned dataset. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of our method in attacking models, regardless of whether they have defenses in place. The code for our method will be made available at https://github.com/yuezunli/ISSBA.