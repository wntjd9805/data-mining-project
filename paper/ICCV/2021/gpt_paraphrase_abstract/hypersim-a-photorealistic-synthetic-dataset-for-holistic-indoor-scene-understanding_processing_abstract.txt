The authors have developed a synthetic dataset called Hypersim to address the challenge of obtaining per-pixel ground truth labels for indoor scene understanding tasks. The dataset consists of 77,400 images of 461 indoor scenes, each with detailed per-pixel labels and ground truth geometry. It relies on publicly available 3D assets and includes complete scene geometry, material information, lighting information, dense per-pixel semantic instance segmentations, and camera information for every image. The dataset also factors each image into diffuse reflectance, diffuse illumination, and a non-diffuse residual term to capture view-dependent lighting effects. The authors analyze the dataset at the scene, object, and pixel levels, considering costs in terms of money, computation time, and annotation effort. They find that generating the dataset is roughly half the cost of training a popular natural language processing model. The authors evaluate the dataset's performance on two real-world scene understanding tasks and find that pre-training on their dataset significantly improves performance and achieves state-of-the-art results on the Pix3D test set. All the rendered image data and code used to generate the dataset are available online.