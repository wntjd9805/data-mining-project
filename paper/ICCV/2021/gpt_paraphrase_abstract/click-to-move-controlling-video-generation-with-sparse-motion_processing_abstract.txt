This paper presents Click to Move (C2M), a new framework for generating videos that allows users to control the movement of the synthesized video using mouse clicks to specify object trajectories. The model takes an initial frame, its corresponding segmentation map, and sparse motion vectors provided by the user as input. It then generates a plausible video sequence starting from the given frame, incorporating motion consistent with the user's input. To achieve this, our deep architecture incorporates a Graph Convolution Network (GCN) that models the movements of all objects in the scene holistically, effectively combining the sparse user motion information and image features. Experimental results demonstrate that C2M outperforms existing methods on publicly available datasets, showing the effectiveness of our GCN framework in modeling object interactions. The source code for C2M is publicly available at https://github.com/PierfrancescoArdino/C2M.