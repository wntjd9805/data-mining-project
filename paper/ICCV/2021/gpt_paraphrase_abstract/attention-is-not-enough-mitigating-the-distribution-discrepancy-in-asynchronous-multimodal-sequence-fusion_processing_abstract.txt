Videos are a combination of language, acoustic, and vision modalities. To fully understand a video, it is necessary to integrate time-series data from different modalities. However, the varying receiving frequencies of these modalities often result in inherent asynchrony. In order to efficiently fuse asynchronous multimodal streams, it is important to model the correlations between elements from different modalities. The Multimodal Transformer (MulT) approach addresses this by extending the self-attention mechanism of the Transformer network to capture crossmodal dependencies. However, directly applying self-attention can lead to unreliable crossmodal dependencies due to distribution mismatch between modalities. In light of this, this study proposes the Modality-Invariant Crossmodal Attention (MICA) approach, which learns crossmodal interactions in a modality-invariant space that bridges the distribution mismatch. This is achieved by aligning the marginal distribution and elements with high-confidence correlations in the common space of query and key vectors computed from different modalities. Experimental results on three standard benchmarks for multimodal video understanding demonstrate the superiority of the proposed approach.