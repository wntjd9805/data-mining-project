To effectively address the problem of temporal action localization (TAL), a visual representation is needed that can simultaneously achieve fine-grained discrimination for temporal localization and sufficient visual invariance for action classification. In order to tackle this challenge, we propose a model called ContextLoc that enhances both local and global contexts within the two-stage temporal localization framework. The model consists of three sub-networks: L-Net, G-Net, and P-Net. L-Net focuses on enriching the local context by finely modeling features at the snippet-level using a query-and-retrieval approach. G-Net, on the other hand, enriches the global context by modeling higher-level representations at the video-level. Additionally, we introduce a novel context adaptation module to adapt the global context to different action proposals. Finally, P-Net models context-aware inter-proposal relations. In our experiments, we investigate two existing models as the P-Net. Our proposed method is evaluated on the THUMOS14 and ActivityNet v1.3 datasets, achieving superior performance compared to the state-of-the-art approaches, with results of 54.3% at tIoU@0.5 and 56.01% at tIoU@0.5, respectively. The code for our method is publicly available at https://github.com/buxiangzhiren/ContextLoc.