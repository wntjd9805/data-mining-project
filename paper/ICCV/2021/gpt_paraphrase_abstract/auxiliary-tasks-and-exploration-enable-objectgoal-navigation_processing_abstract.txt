The task of ObjectGoal Navigation (OBJECTNAV) involves agents navigating to a specific object in an unseen environment. Previous studies have shown that end-to-end OBJECTNAV agents, which use basic visual and recurrent modules like CNN+RNN, perform poorly due to overfitting and inefficiency in sample usage. As a result, current state-of-the-art methods combine analytic and learned components to operate on explicit spatial maps of the environment. In contrast, we propose a different approach that enhances a generic learned agent by incorporating auxiliary learning tasks and an exploration reward. Our agents achieve a success rate of 24.5% and a Success-weighted Path Length (SPL) of 8.1%, which represents a relative improvement of 37% and 8% respectively compared to previous state-of-the-art methods, on the Habitat ObjectNav Challenge. Through our analysis, we suggest that agents tend to simplify their visual inputs to ensure smoother dynamics in the recurrent neural network (RNN), and that auxiliary tasks help reduce overfitting by minimizing the effective dimensionality of the RNN. In other words, an effective OBJECTNAV agent that needs to maintain coherent plans over long periods of time achieves this by learning smooth and low-dimensional recurrent dynamics.