Efficient and reliable detection of grasp poses is crucial for robotics manipulation. Traditional methods for 6 degrees of freedom (DoF) grasping treat all points in a scene equally and use uniform sampling to select potential grasp candidates. However, we have discovered that disregarding the specific location for grasping significantly hampers the speed and accuracy of current grasp pose detection techniques. To address this issue, we propose a concept called "graspness" that evaluates the quality of a grasp based on geometric cues, enabling the identification of graspable areas in cluttered scenes. We introduce a look-ahead searching method to measure graspness, and statistical analysis confirms the validity of our approach. To achieve efficient graspness detection in practical applications, we develop a neural network called the graspness model, which approximates the searching process. Extensive experiments demonstrate the stability, generality, and effectiveness of our graspness model, making it compatible as a plug-and-play module for different methods. Integrating our graspness model into various existing techniques leads to a significant improvement in accuracy. Additionally, we propose GSNet, an end-to-end network that incorporates our graspness model to filter low-quality predictions at an early stage. Evaluation on the large-scale benchmark dataset GraspNet-1Billion demonstrates that our method outperforms previous state-of-the-art approaches by a substantial margin, with an increase in average precision (AP) of over 30%, while maintaining high inference speed.