Advancements in generative adversarial networks (GANs) have significantly improved the quality of photorealistic image generation. However, the malicious utilization of these deepfakes has raised concerns regarding visual misinformation. Although current research on deepfake detection is highly accurate, it is vulnerable to advancements in generation techniques and adversarial iterations on detection countermeasures. Therefore, we propose a proactive and sustainable solution for deepfake detection that remains unaffected by the evolution of generative models. Our approach involves incorporating artificial fingerprints into the models.  Our method is straightforward yet effective. We begin by embedding artificial fingerprints into the training data and subsequently observe a remarkable finding: these fingerprints can be transferred from the training data to the generative models, resulting in their presence in the generated deepfakes. Our experiments demonstrate that our fingerprinting solution (1) applies to various state-of-the-art generative models, (2) has minimal impact on the quality of generated images, (3) remains robust against perturbations at both the image and model levels, (4) is difficult for adversaries to detect, and (5) outperforms recent state-of-the-art baselines, simplifying deepfake detection and attribution.  By closing the responsibility loop between the publication of pre-trained generative models and their potential misuse, our solution eliminates reliance on the current arms race. It ensures that our approach remains effective regardless of the ongoing advancements in the field.