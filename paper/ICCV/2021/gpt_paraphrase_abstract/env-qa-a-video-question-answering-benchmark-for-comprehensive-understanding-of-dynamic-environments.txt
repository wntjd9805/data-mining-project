The understanding of visuals extends beyond simply studying images or videos online. Humans possess the ability to deeply comprehend their surroundings, rapidly perceive ongoing events, and continuously track changes in the state of objects, tasks that remain difficult for current AI systems. In order to equip AI systems with the capability to comprehend dynamic environments, we have created a video Question Answering dataset called Env-QA. This dataset consists of 23,000 egocentric videos, each containing a series of events that involve exploration and interaction within the environment. Additionally, Env-QA provides 85,000 questions to assess the system's understanding of the composition, layout, and state changes of the environment depicted in the videos. Furthermore, we have developed a video QA model called Temporal Segmentation and Event Attention network (TSEA), which introduces event-level video representation and corresponding attention mechanisms to better extract information about the environment and answer questions. Through comprehensive experiments, we have demonstrated the effectiveness of our framework and highlighted the significant challenges posed by Env-QA, including long-term state tracking, multi-event temporal reasoning, and event counting. The answer format only produces the abstract representation.