We introduce a novel approach for estimating depth from multiple views using a combination of traditional structure-from-motion (SfM) reconstruction and neural radiance fields (NeRF). Unlike existing methods that rely on estimated correspondences, our method optimizes directly over implicit volumes, eliminating the need for pixel matching in indoor scenes. Our approach utilizes learning-based priors to guide the optimization process of NeRF. We first adapt a monocular depth network to the target scene by fine-tuning it on sparse SfM reconstruction. We address the shape-radiance ambiguity of NeRF in indoor environments by using the adapted depth priors to monitor the sampling process of volume rendering. Additionally, we compute a per-pixel confidence map based on the rendered image to further improve depth quality. Our experiments demonstrate that our framework outperforms state-of-the-art methods for indoor scenes. We also show that our guided optimization scheme does not compromise the original synthesis capability of NeRF and improves rendering quality for both seen and novel views. The code for our method is available at https://github.com/weiyithu/NerfingMVS.