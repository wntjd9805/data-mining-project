Batch normalization (BN) is widely used in modern deep neural networks (DNNs) because it improves convergence. However, it has been observed that while BN increases model accuracy, it also decreases adversarial robustness. Understanding the impact of BN on DNNs, particularly in terms of model robustness, has become an area of interest in the machine learning community. This study aims to explore the effect of BN on DNNs from a non-robust feature perspective. It is hypothesized that the improved accuracy can be attributed to better utilization of useful features, but it is unclear whether BN primarily favors learning robust features (RFs) or non-robust features (NRFs). Through empirical evidence, this work supports the notion that BN shifts a model towards being more dependent on NRFs. To analyze this shift in feature robustness, a framework is proposed to separate robustness from usefulness. Extensive analysis using this framework provides valuable insights into DNN behavior in terms of robustness, revealing that DNNs initially learn RFs and then NRFs. Additionally, it is discovered that RFs transfer better than NRFs, leading to the development of simple techniques for strengthening transfer-based black-box attacks.