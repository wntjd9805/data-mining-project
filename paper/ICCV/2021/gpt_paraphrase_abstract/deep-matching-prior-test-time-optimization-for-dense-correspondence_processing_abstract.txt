Traditional methods for establishing dense correspondences between visually or semantically similar images have relied on designing specific matching priors, which is challenging to achieve in general. Recent approaches have attempted to address this issue by using learning-based methods to train models on large datasets. While these methods have shown improved performance, they require substantial training data and intensive learning, limiting their practicality. Additionally, using a fixed model at test time does not account for the fact that each image pair may require its own specific prior, resulting in limited performance and poor generalization to unseen images. 

In this study, we propose a novel approach that captures image pair-specific priors by optimizing untrained matching networks on a given pair of images. We introduce a residual matching network and a confidence-aware contrastive loss to ensure meaningful convergence during test-time optimization for dense correspondence. Experimental results demonstrate that our framework, called Deep Matching Prior (DMP), is competitive, and in some cases, outperforms the latest learning-based methods on various benchmarks for both geometric and semantic matching. Importantly, DMP achieves these results without requiring large training data or intensive learning. When the networks are pre-trained, DMP achieves state-of-the-art performance on all benchmarks.