We propose a regularization method called progressive self-knowledge distillation (PS-KD) to improve the generalization capability of deep neural networks. PS-KD gradually distills a model's own knowledge to soften hard targets during training. This method can be seen as a student becoming a teacher, as the model adjusts its targets by combining ground-truth and past predictions. PS-KD also rescales gradients based on the difficulty of classifying examples, effectively performing hard example mining. It can be easily combined with existing regularization methods to further enhance generalization performance. Additionally, PS-KD improves accuracy and provides high-quality confidence estimates in terms of calibration and ordinal ranking. Experimental results on image classification, object detection, and machine translation tasks consistently demonstrate the effectiveness of our method compared to state-of-the-art baselines. The source code is available at https://github.com/lgcnsai/PS-KD-Pytorch.