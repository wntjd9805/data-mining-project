We introduce a straightforward yet highly versatile approach for elucidating the interactions between components within a neural network's reasoning process. Initially, we develop an algorithm that utilizes cross derivatives to calculate the statistical effects of interaction between individual features. This algorithm is applicable to both 2-way and higher-order interactions (3-way or more). Through a comparison with a weight-based attribution technique, we demonstrate that cross derivatives offer a superior metric for detecting interactions of both 2-way and higher-order. Additionally, we expand the application of cross derivatives as an explanatory tool in neural networks to the domain of computer vision by extending Grad-CAM, a popular gradient-based explanatory tool for convolutional neural networks (CNNs), to higher-order interactions. While Grad-CAM is limited to explaining the significance of individual objects in images, our method, referred to as Taylor-CAM, enables the explanation of a neural network's relational reasoning across multiple objects. We provide qualitative and quantitative evidence, including a user study, to demonstrate the effectiveness of our explanations. To facilitate explainable deep learning, we will make our code available as a tool package.