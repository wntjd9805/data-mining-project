Video Question Answering (Video QA) is a task that involves answering questions based on both visual and linguistic information. The industry has recognized the importance of handling large amounts of multi-modal video and language data. However, current video QA models face challenges such as high computational complexity and limited representation capability during training and testing. These models typically use deep features extracted from pre-trained networks after decoding all frames, which may not be optimal for video QA tasks. To address these issues, this paper proposes a novel deep neural network that generates video QA features directly from the coded video bit-stream, reducing complexity. The network consists of dedicated deep modules for both video QA and video compression, making it the first attempt at integrating these two tasks. The proposed network is designed to be model-agnostic and can be easily integrated into state-of-the-art networks for improved performance without requiring computationally expensive motion-related deep models. Experimental results show that the proposed network outperforms previous studies at a lower complexity level. The answer format outputs only the essential information. The source code for the proposed network can be found at https://github.com/Nayoung-Kim-ICP/VQAC.