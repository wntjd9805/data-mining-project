The advancement of 3D vision in the past two decades has been greatly aided by the abundance of internet photos of landmarks and cities. These photos have facilitated automated 3D reconstructions of famous landmarks. However, the potential of language as a source of information for these 3D collections, such as image captions, has not been fully explored. This study introduces WikiScenes, a comprehensive dataset of landmark photo collections that includes descriptive text in the form of captions and hierarchical category names. WikiScenes serves as a valuable testing ground for multimodal reasoning involving images, text, and 3D geometry. The dataset is used to demonstrate the effectiveness of learning semantic concepts using images and 3D models. A weakly-supervised framework is employed to connect images, 3D structure, and semantics, making use of the dataset's strong constraints provided by 3D geometry.