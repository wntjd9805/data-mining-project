Survival outcome prediction in computational pathology is a difficult task that involves analyzing interactions within tumor images. Existing methods for modeling whole slide images (WSIs) have limitations in terms of computational complexity and incorporating biological priors. To address these challenges, we propose a Multimodal Co-Attention Transformer (MCAT) framework. MCAT learns a co-attention mapping between WSIs and genomic features, allowing us to understand how histology patches relate to gene expression when predicting patient survival. This framework also reduces the complexity of WSI analysis, making it possible to use Transformer layers as a general encoder backbone. We evaluate our method on five cancer datasets and find that it consistently outperforms existing approaches.