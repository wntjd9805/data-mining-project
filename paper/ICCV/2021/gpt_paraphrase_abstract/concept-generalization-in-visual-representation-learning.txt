The evaluation of visual representations in self-supervised learning often involves measuring concept generalization, which refers to the ability of models trained on a set of known visual concepts to recognize new, unseen concepts. However, the selection of unseen concepts for evaluation is typically arbitrary and independent from the seen concepts used for training, disregarding any semantic relationships between them. In this study, we argue that these semantic relationships impact generalization performance and introduce a new benchmark called ImageNet-CoG. This benchmark utilizes expert knowledge from WordNet to define a sequence of unseen concept sets in the ImageNet-21K dataset that become progressively more semantically distant from the ImageNet-1K subset, a widely used training set. This allows us to evaluate visual representations learned on ImageNet-1K without any modifications. We conduct a comprehensive study involving 31 models based on convolution and transformers, and examine how different architectures, levels of supervision, regularization techniques, and use of web data influence concept generalization performance.