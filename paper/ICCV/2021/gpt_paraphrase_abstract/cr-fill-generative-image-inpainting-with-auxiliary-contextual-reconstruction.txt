Recent deep generative inpainting methods often fail to find proper reference features when completing a missing region, resulting in artifacts in the results. Additionally, these methods have a high computational overhead due to computing pairwise similarity across the entire feature map during inference. To address these issues, we propose a novel approach that teaches patch-borrowing behavior to an attention-free generator through joint training with an auxiliary contextual reconstruction task. This task encourages the generated output to be plausible even when reconstructed by surrounding regions. The auxiliary branch, known as the contextual reconstruction (CR) loss, optimizes query-reference feature similarity and reference-based reconstruction in conjunction with the inpainting generator. The CR loss is only required during training, while the inpainting generator alone is sufficient for inference. Experimental results demonstrate that our proposed inpainting model outperforms state-of-the-art methods in terms of both quantitative and visual performance. The code for our method is available at https://github.com/zengxianyu/crfill.