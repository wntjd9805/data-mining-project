We present a new approach to generate facial composites by utilizing human fixations. Our method employs three neural networks: an encoder, a scoring network, and a decoder. The encoder extracts image features and predicts neural activation maps for each face observed by a person. A scoring network compares human and neural attention and assigns a relevance score to each extracted image feature. These features are then combined into a single feature vector through a linear combination weighted by relevance. The decoder then decodes this feature vector to produce the final photofit. To train the scoring network, we collected gaze data from 19 participants who viewed collages of synthetic faces. Our method surpasses a baseline predictor and a human study confirms that the generated photofits are visually plausible and resemble the observer's mental image.