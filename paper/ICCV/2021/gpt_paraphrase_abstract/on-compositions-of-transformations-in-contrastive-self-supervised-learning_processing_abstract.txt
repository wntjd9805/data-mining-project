This paper presents a generalization of contrastive learning in the image domain to include a wider range of transformations. The authors propose a practical construction that satisfies formal requirements for contrastive formulations. They express all components of noise contrastive formulations as generalized transformations of the data, including data sampling. The authors then apply these ideas to videos, considering the additional modalities of audio and text as well as the dimension of time. They find that being invariant to certain transformations and distinctive to others is crucial for learning effective video representations. The proposed approach outperforms the state-of-the-art on multiple benchmarks and even surpasses supervised pretraining. The authors provide code and pretrained models for further exploration.