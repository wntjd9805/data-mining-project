Video super-resolution (VSR) methods aim to enhance the spatial resolution of low-resolution (LR) videos. However, existing VSR techniques are primarily trained and evaluated on synthetic datasets, where LR videos are uniformly downsampled from their high-resolution (HR) counterparts using simple operators like bicubic downsampling. These synthetic degradation models fail to accurately represent the complex degradation processes in real-world videos, rendering the trained VSR models ineffective in real-world applications. To address this gap, we introduce the RealVSR dataset, which consists of LR-HR video sequences captured using the multi-camera system of iPhone 11 Pro Max. Due to the use of separate cameras, there may be misalignment and lumi-nance/color differences between the LR and HR video pairs. To improve the robustness of the VSR model during training and enhance the recovery of details from LR inputs, we convert the LR-HR videos into YCbCr space and decompose the luminance channel into a Laplacian pyramid. We then employ different loss functions for each component. Experimental results demonstrate that VSR models trained on our RealVSR dataset exhibit superior visual quality compared to models trained on synthetic datasets in real-world scenarios. Furthermore, these models demonstrate good generalization capabilities in cross-camera tests. The RealVSR dataset and code can be accessed at https://github.com/IanYeung/RealVSR.