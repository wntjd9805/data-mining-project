Active speaker detection requires integrating multiple cues from different modalities. Current methods focus on modeling and combining short-term audiovisual features for individual speakers, typically at the frame level. We propose a new approach to active speaker detection that directly addresses the multi-modal nature of the problem and offers a straightforward strategy. In our approach, independent visual features representing speakers in the scene are assigned to a previously detected speech event. Our experiments demonstrate that a small graph data structure, utilizing local information, can effectively solve the instantaneous audio-visual assignment problem. Furthermore, by extending the temporal aspect of this initial graph, we achieve a new state-of-the-art performance on the AVA-ActiveSpeaker dataset, with a mean Average Precision (mAP) of 88.8%.