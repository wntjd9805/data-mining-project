Deep learning has achieved significant success in various domains, requiring a large amount of labeled training data. However, the ultimate goal is to develop models that can quickly gain a profound understanding with only a few samples, particularly in multi-modal scenarios like visual question answering and image captioning. This study focuses on few-shot visual-semantic learning and introduces the Hierarchical Graph ATtention network (HGAT). HGAT is a two-stage network that models intra- and inter-modal relationships using limited image-text samples. The main contributions of HGAT are as follows: 1) it addresses few-shot multi-modal learning problems by effectively utilizing intra-relationships within each modality and employing an attention-based co-learning framework between modalities using a hierarchical graph-based architecture, with a primary focus on visual and semantic modalities; 2) it outperforms existing methods in the few-shot setting for visual question answering and image captioning; 3) it can be easily extended to the semi-supervised setting, where some image-text samples are unlabeled. Extensive experiments demonstrate that HGAT achieves state-of-the-art performance on three widely-used benchmarks for two visual-semantic learning tasks.