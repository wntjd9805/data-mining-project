Visual correspondence is a crucial aspect of developing assistive tools for hand-drawn animation. While there has been significant research on learning visual correspondences at the pixel-level, there is a lack of approaches that focus on learning correspondences at the level of line enclosures (segments) commonly found in hand-drawn animation. Utilizing this segment structure offers several advantages, such as reducing the memory complexity associated with pixel attention on high-resolution images and utilizing animation datasets that already contain correspondence information at the per-segment color level. To address this gap, we introduce the Animation Transformer (AnT), which employs a Transformer-based architecture to learn the spatial and visual relationships between segments across a sequence of images. Our approach achieves impressive results by incorporating a forward match loss and a cycle consistency loss, outperforming state-of-the-art pixel-based approaches on challenging datasets from real animation productions that lack ground-truth correspondence labels.