The importance of explainability in machine learning models has led to increased research in this area. In computer vision applications, generative counterfactual methods have been used to understand how a model's decision can be changed by altering its input. However, current methods often generate trivial counterfactuals that provide little value in terms of understanding model or data biases. To address this problem, we propose DiVE, a method that learns to perturb a model's input in a disentangled latent space. DiVE uses a diversity-enforcing loss to uncover multiple valuable explanations for the model's prediction and includes a mechanism to prevent the generation of trivial explanations. Our experiments on CelebA and Synbols datasets demonstrate that DiVE outperforms previous state-of-the-art methods in producing high-quality explanations. The code for our model is available at https://github.com/ElementAI/beyond-trivial-explanations.