Deep learning has made significant progress in visual recognition on large-scale balanced datasets, but it still struggles with real-world long-tailed data. Previous methods have attempted to address the imbalance issue by using class re-balanced training strategies, but this approach can lead to over-fitting on tail classes. A recent method called decoupling has attempted to overcome over-fitting by utilizing a multi-stage training scheme, but it still fails to effectively capture tail class information during the feature learning stage.In this paper, we propose a solution to incorporate label correlation into a multi-stage training scheme for long-tailed recognition. We demonstrate that soft labels can be a powerful tool for transferring knowledge from head to tail classes. Soft labels capture the intrinsic relation between classes and prove to be helpful for long-tailed recognition.Our approach, called Self Supervised to Distillation (SSD), introduces a conceptually simple yet highly effective multi-stage training scheme. Firstly, we utilize a self-distillation framework to automatically mine label relations for long-tailed recognition. Secondly, we introduce a new distillation label generation module guided by self-supervision. These distilled labels effectively model the long-tailed distribution by integrating information from both the label and data domains.Through extensive experiments, we demonstrate that our method achieves state-of-the-art results on three long-tailed recognition benchmarks: ImageNet-LT, CIFAR100-LT, and iNaturalist 2018. Our SSD outperforms the strong LWS baseline by a margin of 2.7% to 4.5% on various datasets.