The responsible use of artificial intelligence (AI) is crucial for its successful deployment in various domains. Explainable AI (XAI) plays a key role in helping users understand AI model decisions, but it also poses privacy risks. This study focuses on the privacy risks associated with image-based model inversion attacks, where private image data can be reconstructed from model explanations. The researchers developed multi-modal transposed CNN architectures that outperform using the target model prediction alone, specifically designed to exploit spatial knowledge in image explanations. The study analyzes how different types of explanations and factors influence inversion performance, even for non-explainable target models by leveraging explanations from surrogate models through attention transfer. These findings emphasize the urgent need for privacy preservation techniques that strike a balance between AI explainability and privacy.