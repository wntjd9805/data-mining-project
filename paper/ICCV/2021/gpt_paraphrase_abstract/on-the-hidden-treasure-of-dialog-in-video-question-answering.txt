Understanding the storyline of videos, such as movies and TV shows, is a difficult task. Existing video question answering (VideoQA) systems rely on external sources like plot synopses, scripts, and video descriptions to comprehend the story. However, we propose a novel approach that eliminates the need for such external sources and instead focuses on utilizing dialogue as a valuable resource. We treat dialogue as a noisy source and convert it into a text description through dialog summarization, similar to recent methods used for processing video data. Each modality, including dialogue and video, is encoded independently using transformers. To combine all modalities, we employ a simple fusion method that incorporates soft temporal attention for effectively localizing information from lengthy inputs. Our model surpasses the state-of-the-art performance on the KnowIT VQA dataset by a significant margin, without relying on question-specific human annotation or human-made plot summaries. Interestingly, our model even outperforms human evaluators who have not watched the entire episodes beforehand. The code for our approach is available at https://engindeniz.github.io/dialogsummary-videoqa. The model outputs only the abstraction of the answer.