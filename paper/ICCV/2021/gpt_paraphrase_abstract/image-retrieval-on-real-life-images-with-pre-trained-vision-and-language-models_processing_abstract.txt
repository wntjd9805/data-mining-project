We expand on the task of composed image retrieval, where an input query consists of an image and a brief textual description on how to modify the image. Existing methods have only been applied to simple images in specific domains like fashion products, which limits the exploration of visual reasoning in more complex image and language contexts. To address this limitation, we introduce the Compose Image Retrieval on Real-life images (CIRR) dataset, containing over 36,000 pairs of open-domain images with human-generated modifying text. To extend the current methods to open-domain scenarios, we propose CIRPLANT, a transformer-based model that utilizes pre-trained vision-and-language (V&L) knowledge to modify visual features based on natural language instructions. Retrieval is performed by finding the nearest neighbors of the modified features. Our experiments show that despite its relatively simple architecture, CIRPLANT outperforms existing methods on open-domain images, while achieving state-of-the-art accuracy on narrow datasets like fashion. We believe that the release of CIRR, along with our dataset, code, and pre-trained models, will inspire further research in the field of composed image retrieval. All the resources related to our work can be accessed at https://cuberick-orion.github.io/CIRR/.