Image captioning is a crucial task for evaluating visual reasoning and promoting accessibility for individuals with visual impairments. However, like in many machine learning scenarios, social biases can negatively influence image captioning. This study focuses specifically on the COCO dataset and explores the pathways through which bias propagates in image captioning. Previous research has examined gender bias using automatically-derived labels, while this work examines racial and intersectional biases using manual annotations. The first contribution of this study involves annotating the perceived gender and skin color of 28,315 people depicted in the dataset, following IRB approval. By utilizing these annotations, the researchers compare racial biases present in manually and automatically generated image captions. The study reveals differences in caption performance, sentiment, and word choice between images of individuals with lighter and darker skin tones. Additionally, the researchers find that these differences are more pronounced in modern captioning systems compared to older ones. This raises concerns that without proper consideration and mitigation, these disparities will continue to grow. The code and data for this study can be accessed at https://princetonvisualai.github.io/imagecaptioning-bias/.