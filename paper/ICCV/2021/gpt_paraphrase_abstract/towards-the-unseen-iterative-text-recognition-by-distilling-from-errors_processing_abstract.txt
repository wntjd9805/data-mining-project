Visual text recognition is a highly researched topic in computer vision, with recent models focusing on practical "in-the-wild" settings. However, a significant challenge remains in recognizing unseen or rarely seen character sequences. This paper proposes a novel framework to address this problem by employing an iterative approach that utilizes predicted knowledge from previous iterations to improve future predictions. The framework includes a unique cross-modal variational autoencoder as a feedback module, trained with textual error distribution data. This module translates discrete predicted character sequences into continuous affine transformation parameters, which are used to condition the visual feature map in subsequent iterations. Experimental results on common datasets demonstrate competitive performance compared to existing methods in the conventional setting. Notably, in a new disjoint setup where train-test labels are mutually exclusive, our framework achieves the best performance, showcasing its ability to generalize to unseen words. (Figure 1 provides a summary of the results).