This paper addresses the challenges of interaction and navigation in dynamic environments for neural agents. Specifically, it focuses on handling long sequences of subtasks and understanding complex human instructions. To tackle these challenges, the authors propose a multimodal transformer called Episodic Transformer (E.T.). E.T. encodes both language inputs and the full history of visual observations and actions. The authors also leverage synthetic instructions as an intermediate representation to separate understanding the visual appearance of an environment from the variations in natural language instructions. The paper demonstrates that encoding the history with a transformer is crucial for solving compositional tasks, and pretraining and joint training with synthetic instructions further enhance performance. The proposed approach achieves state-of-the-art results on the ALFRED benchmark, with task success rates of 38.4% and 8.5% on seen and unseen test splits, respectively.