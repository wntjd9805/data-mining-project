Rolling out new models while maintaining backward compatibility can significantly reduce costs and eliminate the need for re-encoding existing gallery images in production visual retrieval systems. However, previous approaches using knowledge distillation have led to performance degradation and lack of guaranteed compatibility. To overcome these challenges, we propose a comprehensive framework called Learning Compatible Embeddings (LCE). This framework enables cross model compatibility and supports compatible training in direct, forward, and backward manners. Our approach achieves compatibility by aligning class centers between models directly or through a transformation, and by imposing more compact intra-class distributions for the new model. We conducted experiments in various scenarios, including changes in training datasets, loss functions, network architectures, and feature dimensions. The results demonstrate that LCE efficiently ensures model compatibility with minimal sacrifice in accuracy. The code for our framework is available at https://github.com/IrvingMeng/LCE.