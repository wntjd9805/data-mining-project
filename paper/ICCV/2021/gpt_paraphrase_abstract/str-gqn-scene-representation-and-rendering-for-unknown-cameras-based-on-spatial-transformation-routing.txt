This paper introduces a Spatial Transformation Routing (STR) mechanism that can effectively model spatial properties in deep learning architectures for scene representation and rendering. Unlike existing methods that rely on accurate camera information, the proposed STR mechanism does not require any geometric prior. Instead, it treats spatial transformation as a message passing process and uses an end-to-end trainable neural network to model the relationship between view poses and routing weights. Additionally, the paper presents an Occupancy Concept Mapping (OCM) framework that provides explainable rationales for scene-fusion processes. Experimental results on multiple datasets demonstrate that the STR mechanism enhances the performance of the Generative Query Network (GQN). The visualization results further illustrate how the routing process can effectively transmit observed information between different views, highlighting the model's advantage in spatial cognition.