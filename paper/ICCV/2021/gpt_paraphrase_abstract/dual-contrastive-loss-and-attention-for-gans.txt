This paper introduces several enhancements to push the limits of image generation using Generative Adversarial Networks (GANs). Despite the impressive results achieved by GANs with large-scale image datasets, the generated images can still be easily identified, particularly in datasets with high variance like bedrooms and churches. To address this issue, the paper proposes a new dual contrastive loss that encourages the discriminator to learn more generalized and distinguishable representations, thereby improving the quality of the generated images. Additionally, the paper investigates the role of attention in image generation and explores different attention blocks in the generator. Although recent state-of-the-art models did not utilize attention, the paper demonstrates that attention remains an important module for successful image generation. Furthermore, the paper explores various attention architectures in the discriminator and introduces a reference attention mechanism. By combining these improvements, the proposed methods achieve a significant improvement of at least 17.5% in the Fr√©chet Inception Distance (FID) on several benchmark datasets. The improvements are even more pronounced in compositional synthetic scenes, with FID improvements of up to 47.5%.