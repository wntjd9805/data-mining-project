We present the concept of active audio-visual source separation, which involves an agent strategically moving to improve the isolation of sounds from a specific object in its surroundings. The agent encounters multiple audio sources simultaneously and must rely on its visual and auditory senses to separate the sounds originating from the target object within a limited time frame. To achieve this, we propose a reinforcement learning approach that trains the agent's movement policies for camera and microphone placement, aiming to enhance the predicted quality of audio separation. Our approach is demonstrated in scenarios related to augmented reality and mobile robotics, where the agent is either co-located or distant from the target object. Through advanced audio-visual simulations in 3D environments, we showcase the model's capability to identify the most effective movement sequences for optimal audio source separation. More information about the project can be found at http://vision.cs.utexas.edu/projects/move2hear.