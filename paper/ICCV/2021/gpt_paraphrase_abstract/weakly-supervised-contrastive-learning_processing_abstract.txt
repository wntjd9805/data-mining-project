Unsupervised visual representation learning has become a popular area of research in computer vision, thanks to the success of contrastive learning. However, most existing contrastive learning approaches suffer from class collision problems, which negatively impact the quality of the learned representation. To address this issue, we propose a weakly supervised contrastive learning framework (WCL). Our framework utilizes two projection heads, with one head performing instance discrimination and the other using a graph-based method to generate weak labels and perform a supervised contrastive learning task. Additionally, we introduce a K-Nearest Neighbor based multi-crop strategy to increase the number of positive samples. Through extensive experiments, we demonstrate that WCL significantly improves the quality of self-supervised representations across various datasets. Notably, our approach achieves a new state-of-the-art result for semi-supervised learning, outperforming SimCLRv2 with ResNet101 on ImageNetTop-1 Accuracy using only 1% and 10% labeled examples. Specifically, WCL achieves 65% and 72% accuracy respectively using ResNet50.