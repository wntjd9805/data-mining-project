Our goal is to enhance the efficiency of graph convolutional networks (GCNs) when applied to learning on point clouds. We analyze the basic graph convolution, which consists of a K-nearest neighbor (KNN) search and a multilayer perceptron (MLP). Through mathematical analysis, we make two discoveries that can improve the efficiency of GCNs. Firstly, the local geometric structure information of 3D representations propagates smoothly across the GCN, which relies on KNN search to gather neighborhood features. This suggests that multiple KNN searches in GCNs can be simplified. Secondly, shuffling the order of graph feature gathering and an MLP results in equivalent or similar composite operations. Building on these findings, we optimize the computational procedure in GCNs. A series of experiments demonstrate that the optimized networks have reduced computational complexity, decreased memory consumption, and accelerated inference speed, while maintaining comparable accuracy for learning on point clouds.