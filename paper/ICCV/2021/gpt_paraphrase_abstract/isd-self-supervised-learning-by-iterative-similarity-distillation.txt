Contrastive learning has been successful in self-supervised learning by pulling positive pairs closer together compared to negative pairs. However, not all negative images are equally negative. To address this, we propose a self-supervised learning algorithm that uses a soft similarity measure for negative images instead of a binary distinction. We employ an iterative process where a slowly evolving teacher model distills its knowledge to a student model by considering the similarity between a query image and random images. Our method is designed to handle unbalanced and unlabeled data more effectively than existing contrastive learning approaches, as the randomly selected negative set may contain samples that are semantically similar to the query image. Unlike standard contrastive methods that label them as negatives, our method identifies them as highly similar. We achieve comparable results to state-of-the-art models and provide our code for reference. Access our code at: https://github.com/UMBCvision/ISD.