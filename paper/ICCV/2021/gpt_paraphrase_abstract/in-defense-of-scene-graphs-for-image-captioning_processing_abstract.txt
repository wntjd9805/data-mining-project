Current image captioning models typically rely on Convolutional Neural Network (CNN) image features and recurrent models to generate captions. However, recent studies have shown that incorporating image scene graphs, which capture structural semantics like object entities, relationships, and attributes, can enhance captioning performance. Yet, using scene graphs from a black-box generator can hinder performance, and scene graph-based models often require explicit use of image features to generate quality captions. To address these challenges, we propose SG2Caps, a framework that utilizes only scene graph labels for competitive image captioning. Our approach aims to bridge the semantic gap between the scene graph derived from the input image and the one derived from its caption. To achieve this, we leverage the spatial location of objects and Human-Object-Interaction (HOI) labels to create an additional HOI graph. SG2Caps significantly outperforms existing scene graph-only captioning models, demonstrating the potential of scene graphs as a representation for image captioning. By directly utilizing scene graph labels, we eliminate the need for costly graph convolutions over high-dimensional CNN features, resulting in 49% fewer trainable parameters. Our code is available at: [Github link].