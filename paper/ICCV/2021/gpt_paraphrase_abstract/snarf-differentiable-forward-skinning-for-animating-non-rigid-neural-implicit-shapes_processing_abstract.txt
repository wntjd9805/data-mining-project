Neural implicit surface representations have shown promise in capturing 3D shapes in a continuous and resolution-independent manner. However, applying them to articulated shapes is difficult. Existing methods use a backward warp field to map deformed points to canonical points, but this approach requires a large amount of data to learn and is pose dependent. To overcome this, we propose SNARF, which combines linear blend skinning (LBS) with neural implicit surfaces. SNARF learns a forward deformation field without direct supervision, allowing for generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging due to implicit correspondences that may not be unique under changes of topology. We introduce a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients through implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural implicit representations, SNARF generalizes better to unseen poses while maintaining accuracy. We validate our method on challenging scenarios involving clothed 3D humans in diverse and unseen poses.