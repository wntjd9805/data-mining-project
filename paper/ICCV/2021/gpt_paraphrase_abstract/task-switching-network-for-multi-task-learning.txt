We present Task Switching Networks (TSNs), a task-dependent architecture that enables efficient multi-task learning. TSNs utilize a single encoder/decoder that can handle multiple tasks by switching between them one at a time. Unlike existing approaches, TSNs maintain a constant number of parameters regardless of the number of tasks, making them scalable and straightforward. We demonstrate that multi-tasking can be achieved using a single task-conditioned decoder by learning task-specific conditioning parameters through a jointly trained task embedding network. This promotes positive interaction between tasks. Our experiments show that our approach outperforms existing methods on challenging multi-task benchmarks like PASCAL-Context and NYUD. Additionally, our analysis of the learned task embeddings reveals a connection to task relationships explored in recent literature.