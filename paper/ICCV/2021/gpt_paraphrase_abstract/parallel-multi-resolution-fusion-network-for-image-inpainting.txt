Current deep image inpainting methods based on auto-encoder architecture suffer from the loss of spatial details during the down-sampling process, resulting in degraded generated results. Additionally, these methods fail to effectively integrate structure information from deep layers and texture information from shallow layers of the auto-encoder architecture. To address these limitations, we propose a novel parallel multi-resolution inpainting network with multi-resolution partial convolution. Our approach utilizes low-resolution branches to capture global structure and high-resolution branches to capture local texture details. These branches are parallel and repeatedly fused using multi-resolution masked representation fusion, producing semantically robust and textually plausible reconstructed images. Experimental results demonstrate the effectiveness of our method in fusing structure and texture information, leading to more realistic results compared to state-of-the-art techniques.