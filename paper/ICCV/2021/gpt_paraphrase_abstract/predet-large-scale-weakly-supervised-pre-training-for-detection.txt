Current object detection methods often rely on pre-trained classification models to improve performance and convergence speed. However, these models prioritize translation invariance and tend to overlook the localization aspect of the problem. In this study, we propose a novel large-scale pre-training strategy for detection that incorporates noisy class labels but excludes bounding boxes. We enhance standard classification pre-training by introducing a specific pretext task for detection. Inspired by noise-contrastive learning and self-supervised approaches, we create a task that ensures bounding boxes with high overlap have similar representations in different views of an image compared to non-overlapping boxes. We modify Faster R-CNN modules to efficiently perform this task. Our experimental results demonstrate significant advancements in detection accuracy and fine-tuning speed compared to existing weakly-supervised and self-supervised pre-training methods.