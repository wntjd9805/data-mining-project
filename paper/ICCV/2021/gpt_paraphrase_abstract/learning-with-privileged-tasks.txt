In multi-objective multi-task learning, the goal is to improve the performance of all tasks by properly leveraging their correlation and conflict. However, in practice, users may have a preference for certain tasks, while others serve as privileged or auxiliary tasks to support the training of the target tasks. These privileged tasks are considered less important or even irrelevant in the final task assessment by users.   To address this issue, we propose a privileged multiple descent algorithm that determines how target tasks and privileged tasks are learned. Specifically, we introduce a privileged parameter that allows the optimization direction to focus more on the target tasks rather than solely relying on the gradient from the privileged tasks. Additionally, we introduce a priority parameter for the target tasks to control any potential distraction from the privileged tasks during optimization. This approach allows for a more aggressive determination of the optimization direction by weighting the gradients between target and privileged tasks, thereby emphasizing the performance of the target tasks within the overall multi-task learning framework.   Extensive experiments conducted on both synthetic and real-world datasets demonstrate that our method can achieve diverse Pareto solutions based on varying preferences for the target tasks.