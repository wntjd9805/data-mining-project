This paper introduces a novel algorithm called Token-AwareCascade contrastive learning (TACo) that enhances contrastive learning in transformer-based vision-language models for video-text alignment and multi-modal representation learning. TACo incorporates two innovative techniques. Firstly, it utilizes a token-aware contrastive loss that considers the syntactic classes of words. This is motivated by the observation that content words, such as nouns and verbs, in the text are more likely to align with visual contents in videos compared to function words. Secondly, TACo employs a cascade sampling method to generate a small set of challenging negative examples, enabling efficient loss estimation for multi-modal fusion layers. Experimental validation of TACo involves fine-tuning pretrained models for various downstream tasks, including text-video retrieval, video action step localization, and video action segmentation. The results demonstrate consistent improvements across different experimental settings, surpassing previous methods and achieving new state-of-the-art performance on three public text-video retrieval benchmarks: YouCook2, MSR-VTT, and ActivityNet.