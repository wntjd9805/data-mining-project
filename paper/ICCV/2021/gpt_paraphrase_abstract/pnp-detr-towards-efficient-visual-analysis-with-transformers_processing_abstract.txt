Recently, researchers introduced a solution called DETR that uses transformers for vision tasks, specifically object detection. However, translating the entire image feature map can be computationally expensive due to redundant computation on areas like the background. In this study, we propose a new sampling module called poll and pool (PnP) that reduces spatial redundancy. We incorporate the PnP module into an end-to-end architecture called PnP-DETR, which dynamically allocates computation resources to improve efficiency. The PnP module extracts fine foreground object feature vectors and a small number of coarse background contextual feature vectors from the image feature map. The transformer model then interacts with these features to produce the detection result. Additionally, our PnP-augmented model allows for flexible trade-offs between performance and computation by adjusting the sampled feature length, without the need to train multiple models. This flexibility makes our approach suitable for various scenarios with different computation constraints. We demonstrate the effectiveness of the PnP module on panoptic segmentation and the transformer-based image recognition model ViT, showing consistent efficiency improvements. Our method contributes to efficient visual analysis with transformers, addressing the common issue of spatial redundancy. The code and models will be made available.