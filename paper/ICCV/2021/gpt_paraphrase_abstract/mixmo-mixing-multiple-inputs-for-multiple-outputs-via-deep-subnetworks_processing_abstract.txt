Recent strategies have found a way to combine multiple subnetworks within a single base network for free. During training, each subnetwork focuses on classifying one of the multiple inputs provided simultaneously. However, the most effective method for mixing these inputs has not been explored. This paper introduces a new framework called MixMo for learning multi-input multi-output deep subnetworks. The main aim is to replace the suboptimal summing operation used in previous approaches with a more appropriate mixing mechanism. Inspired by successful mixed sample data augmentations, the authors demonstrate that binary mixing in features, particularly with rectangular patches from CutMix, improves results by strengthening and diversifying subnetworks. The proposed models achieve state-of-the-art performance on image classification tasks using CIFAR-100 and Tiny ImageNet datasets. These models outperform data augmented deep ensembles without the additional computational and memory requirements. By operating on features and effectively utilizing the capabilities of large networks, this research introduces a new direction of study that complements previous works.