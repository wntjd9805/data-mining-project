Deep learning models for semantic segmentation require large-scale manually annotated datasets, which can be time-consuming and expensive. An alternative approach is to automatically annotate video sequences by propagating sparsely labeled frames through time. In this study, we propose a new label propagation method called Warp-Refine Propagation that combines semantic and geometric cues to efficiently auto-label videos. Our method learns to refine geometrically-warped labels and incorporate learned semantic priors in a semi-supervised manner by leveraging cycle-consistency over time. Our quantitative analysis demonstrates that our method significantly improves label propagation by 13.1 mIoU on the ApolloScape dataset. Additionally, by training with the automatically labeled frames, we achieve highly competitive results on three semantic segmentation benchmarks, surpassing the state-of-the-art by a substantial margin of 1.8 and 3.61 mIoU on NYU-V2 and KITTI datasets respectively, while matching the current best performance on Cityscapes dataset.