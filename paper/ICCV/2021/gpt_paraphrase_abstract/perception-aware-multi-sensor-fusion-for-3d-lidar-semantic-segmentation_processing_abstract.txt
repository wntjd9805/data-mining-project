3D LiDAR semantic segmentation is crucial for applications like auto-driving and robotics. Existing fusion-based methods struggle to achieve good performance due to the differences between RGB cameras and LiDAR. This study proposes a perception-aware multi-sensor fusion (PMF) scheme to combine information from RGB images and point clouds. The point clouds are projected onto the camera coordinates to provide spatio-depth information for the images. A two-stream network is then used to extract features from both modalities and fuse them using residual-based fusion modules. Additional perception-aware losses are introduced to measure the perceptual difference between the modalities. Experimental results on benchmark datasets demonstrate the superiority of the proposed PMF method, outperforming the state-of-the-art by 0.8% in mIoU on the nuScenes dataset.