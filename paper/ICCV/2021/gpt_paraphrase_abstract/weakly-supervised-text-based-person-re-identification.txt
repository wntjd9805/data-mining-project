The traditional methods for identifying individuals through text heavily rely on identity annotations, which can be costly and time-consuming. This paper introduces a more practical approach called weakly supervised text-based person re-identification, where only text-image pairs are available without the need for annotating identities during training. The proposed Cross-Modal Mutual Training (CMMT) framework addresses this by utilizing a clustering method to generate pseudo labels for both visual and textual instances, reducing intra-class variations. To refine the clustering results, CMMT includes a Mutual PseudoLabel Refinement module that leverages clustering results from one modality to improve the other modality based on the text-image relationship. Additionally, CMMT introduces a Text-IoU Guided Cross-Modal Projection Matching loss to resolve cross-modal matching ambiguity. To enhance learning of discriminative textual-visual joint embeddings, a Text-IoU Guided Hard Sample Mining method is proposed. Extensive experiments demonstrate the effectiveness of CMMT, outperforming existing text-based person re-identification methods. The code for CMMT is available at https://github.com/X-BrainLab/WS_Text-ReID.