We propose an unsupervised learning framework called Cross-Encoder to train 3D gaze estimators with fewer annotations. The goal is to utilize unlabeled data to learn an appropriate representation for gaze estimation. The issue of gaze features being intertwined with eye appearance is addressed by Cross-Encoder through a latent-code-swapping mechanism. This mechanism disentangles the features by using pairs of images that are consistent with the eyes and similar in gaze. Each image is encoded into a gaze feature and an eye feature. Cross-Encoder is trained to reconstruct the images in the eye-consistent pair based on their gaze feature and the other image's eye feature. Similarly, it reconstructs the images in the gaze-similar pair based on their eye feature and the other image's gaze feature. Experimental results validate the effectiveness of our approach. Firstly, the gaze estimator trained with very few samples using the Cross-Encoder-learned gaze representation outperforms other unsupervised learning methods, both within-dataset and cross-dataset. Secondly, ResNet18 pretrained by Cross-Encoder achieves competitive performance compared to state-of-the-art gaze estimation methods. Thirdly, an ablation study demonstrates that Cross-Encoder successfully disentangles the gaze feature and eye feature.