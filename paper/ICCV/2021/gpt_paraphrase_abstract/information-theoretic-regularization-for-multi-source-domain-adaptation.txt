The effectiveness of adversarial learning in addressing single-source Domain Adaptation (DA) problems has been well-established. Recently, this approach has also been applied to Multi-source DA (MDA) problems. However, the impact of using multiple domain discriminators in MDA strategies on the latent space representations is not well understood. In this study, we utilize an information-theoretic approach to identify and address potential issues caused by multiple domain discriminators in MDA, including the disintegration of domain-discriminative information, limited computational scalability, and high variance in the loss gradient during training. By incorporating information regularization, we propose a novel neural architecture called Multi-source Information-regularized Adaptation Networks (MIAN) and demonstrate its superiority over existing methods through large-scale experiments. Despite its simplicity, MIAN consistently achieves significantly better performance.