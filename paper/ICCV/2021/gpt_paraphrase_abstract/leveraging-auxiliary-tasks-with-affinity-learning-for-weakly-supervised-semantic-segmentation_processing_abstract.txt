The lack of densely labelled data makes semantic segmentation a difficult task. Using only class activation maps (CAM) with image-level labels for segmentation supervision is inadequate. Previous methods have used pre-trained models to generate coarse saliency maps, but the offline generation process does not fully utilize the benefits of these maps. To address this, we propose AuxSegNet, a weakly supervised multi-task framework that leverages saliency detection and multi-label image classification as auxiliary tasks to improve semantic segmentation using only image-level ground-truth labels. We also propose learning a cross-task global pixel-level affinity map from the saliency and segmentation representations, which can refine saliency predictions and propagate CAM maps to provide improved pseudo labels for both tasks. The iterative improvements on segmentation performance are achieved through the mutual boost between pseudo label updating and cross-task affinity learning. Extensive experiments demonstrate the effectiveness of our proposed auxiliary learning network structure and cross-task affinity learning method. Our approach achieves state-of-the-art weakly supervised segmentation performance on the challenging PASCAL VOC 2012 and MS COCO benchmarks.