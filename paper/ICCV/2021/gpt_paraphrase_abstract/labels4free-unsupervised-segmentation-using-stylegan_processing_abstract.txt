We present a new approach to segmenting objects generated by StyleGAN without the need for supervision. Our method is based on two key observations: firstly, the features generated by StyleGAN contain valuable information that can be used to train segmentation networks, and secondly, the foreground and background of these objects can be treated as independent entities and swapped across images to create realistic composite images. To achieve this, we propose adding a segmentation branch to the Style-GAN2 generator architecture and dividing the generator into separate foreground and background networks. This allows us to generate soft segmentation masks for the foreground objects in an unsupervised manner. We compared our results to state-of-the-art supervised segmentation networks on various object classes and found comparable performance. Additionally, we outperformed the best unsupervised segmentation approach in both qualitative and quantitative measures. More information about our project can be found on our project page at https:/rameenabdal.github.io/Labels4Free.