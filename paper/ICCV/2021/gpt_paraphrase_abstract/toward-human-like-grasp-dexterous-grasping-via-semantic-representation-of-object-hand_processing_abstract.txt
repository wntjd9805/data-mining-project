This paper presents a framework for teaching robotic hands to perform dexterous operations similar to human hands. The authors propose a grasp synthesis framework that involves building a dataset of objects, accurately segmenting their functional areas, and annotating semantic touch code for each area. This dataset, which consists of 18 categories of 129 objects and involves the participation of 15 individuals for data annotation, serves as a guide for the dexterous hand to perform functional grasping and post-grasp manipulation. To effectively train the model, four loss functions are designed to constrain the system, resulting in successful generation of functional grasps based on the semantic touch code. The model's performance is evaluated through experiments using synthetic data, demonstrating its ability to robustly generate functional grasps even for previously unseen objects.