The task of scene graph generation (SGG) involves identifying subject-predicate-object triplets in an image to enhance scene understanding. However, current models struggle to use informative predicates like "standing on" and "looking at" instead of common ones like "on" and "at." This results in a loss of precise information and overall performance. If a model describes an image as "stone on road" instead of "blocking," it can lead to a misunderstanding of the scene. We believe this issue stems from two imbalances: a lack of informative predicates and an imbalance in training samples. To address this problem, we propose BA-SGG, a straightforward and effective SGG framework that focuses on balance adjustment rather than traditional distribution fitting. It consists of two components: Semantic Adjustment (SA) and Balanced Predicate Learning (BPL), which aim to address these imbalances. Our method can be easily applied to existing SGG models and significantly improves performance. On Visual Genome, our method achieves 14.3%, 8.0%, and 6.1% higher Mean Recall (mR) compared to the Transformer model in three scene graph generation sub-tasks. The code is publicly available.