Recent advancements in image classification have seen transformers emerge as a powerful alternative to convolutional neural networks. However, the optimization of vision transformers has received limited attention thus far. In this study, we focus on constructing and optimizing deeper transformer networks for image classification. Our main objective is to explore the relationship between architecture and optimization in dedicated transformers. Through two significant architectural modifications, we achieve considerable improvements in the accuracy of deep transformers. As a result, our models demonstrate a sustained increase in performance with increased depth, surpassing previous benchmarks. For instance, without using any external data, we achieve an impressive top-1 accuracy of 86.5% on Imagenet, setting a new state-of-the-art record with fewer floating-point operations and parameters. Furthermore, our best model sets a new benchmark on Imagenet, considering Reassessed labels and Imagenet-V2/match frequency, without the need for additional training data. To facilitate further research, we provide our code and models.