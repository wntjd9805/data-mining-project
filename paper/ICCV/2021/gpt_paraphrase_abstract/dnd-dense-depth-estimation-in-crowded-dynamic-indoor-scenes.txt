Our novel approach aims to estimate depth from a monocular camera as it moves through complex and crowded indoor environments. This includes places like department stores or metro stations. Instead of relying on dense depth maps collected from such environments, our method trains on dynamic scenes and predicts absolute scale depth maps for the entire scene, which consists of a static background and multiple moving people. We achieve this by utilizing RGB images and sparse depth maps generated from traditional 3D reconstruction methods. Our approach overcomes the challenge of handling depth for non-rigidly moving people without explicitly tracking their motion by employing two constraints. Through experiments on the NAVERLABS dataset, which consists of complex and crowded scenes, we demonstrate that our approach consistently outperforms recent depth estimation methods.