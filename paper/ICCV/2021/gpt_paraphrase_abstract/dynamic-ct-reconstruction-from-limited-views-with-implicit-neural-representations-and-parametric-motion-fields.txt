We present a new approach to reconstructing dynamic scenes using computed tomography (CT) called 4D-CT. This is a challenging problem in both industrial and medical settings. Existing methods for 4D-CT reconstruction are designed for sparse sampling schemes, which require fast CT scanners to capture multiple revolutions around the scene. However, if the scene is moving too fast, the limited view of the sampling makes reconstruction difficult due to spatiotemporal ambiguities.In our work, we propose a reconstruction pipeline that combines implicit neural representations with a novel parametric motion field warping technique to reconstruct rapidly deforming scenes with limited views. Importantly, we use a differentiable analysis-by-synthesis approach to compare the reconstructed data with captured x-ray sinogram data in a self-supervised manner. This means that our optimization method does not require any training data to reconstruct the scene.We demonstrate the effectiveness of our proposed system by reconstructing scenes with deformable and periodic motion and compare our results with state-of-the-art baselines. Additionally, we show that our method can reconstruct continuous spatiotemporal representations of the scenes and can upsample them to arbitrary volumes and frame rates after optimization.This research introduces a new approach to computed tomography reconstruction using implicit neural representations. The code for our method is available at https://github.com/awreed/DynamicCTReconstruction.