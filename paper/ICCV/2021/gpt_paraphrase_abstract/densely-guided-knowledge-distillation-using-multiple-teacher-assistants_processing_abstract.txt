Deep neural networks have achieved great success in various tasks, leading to the need for model compression and transfer learning techniques. One such technique is knowledge distillation, where a small student network is trained using a large teacher network. However, when the sizes of the student and teacher models differ significantly, the student network often struggles to learn effectively.

To address this issue, this paper proposes a densely guided knowledge distillation approach that utilizes multiple teacher assistants. These teacher assistants gradually decrease the model size, bridging the gap between the teacher and student networks more efficiently. To enhance the learning of the student network, each teacher assistant is guided iteratively by every other smaller teacher assistant. This means that when teaching a smaller teacher assistant in the next step, the existing larger teacher assistants from the previous step are also used, along with the teacher network.

Additionally, this approach incorporates stochastic teaching, where a teacher or teacher assistant is randomly dropped for each mini-batch. This serves as a regularizer, improving the efficiency of teaching the student network. As a result, the student network can consistently learn important distilled knowledge from multiple sources.

The effectiveness of the proposed method is validated through experiments on CIFAR-10, CIFAR-100, and ImageNet datasets, using various backbone architectures such as ResNet, WideResNet, and VGG. The results demonstrate significant performance improvements compared to existing approaches.

The paper also discusses the problem of a large gap between the teacher and student networks, where the difference between layers can be more than 5 times. Existing methods, such as TAKD, have limitations, including the error avalanche problem. This occurs when a higher-level teacher assistant teaches a lower-level assistant, and any error in the teaching process accumulates with each additional assistant. In contrast, the proposed densely guided knowledge distillation approach is less prone to this error avalanche problem because it does not teach each assistant at each level independently.

In conclusion, this paper introduces a novel approach to knowledge distillation that addresses the issue of poor learning in student networks with significantly different sizes compared to their teacher networks. The approach utilizes multiple teacher assistants and incorporates stochastic teaching, resulting in improved learning efficiency and performance.