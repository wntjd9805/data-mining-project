Deep convolutional neural network (CNN) methods have been highly successful in achieving state-of-the-art results in multi-view 3D object reconstruction. However, the two core components of these methods, namely view feature extraction and multi-view fusion, are typically studied independently, and the relationships among multiple input views are often overlooked. Taking inspiration from the recent advancements in Transformer models, we propose a new framework called 3D Volume Transformer that reformulates multi-view 3D reconstruction as a sequence-to-sequence prediction problem. In contrast to previous CNN-based approaches that employ separate designs, our framework integrates feature extraction and view fusion within a single Transformer network. A major advantage of our approach is its ability to explore view-to-view relationships using self-attention among multiple unordered inputs. Through experiments conducted on the ShapeNet benchmark, we demonstrate that our method achieves state-of-the-art accuracy in multi-view reconstruction while utilizing significantly fewer parameters (a 70% reduction) compared to CNN-based methods. Furthermore, our method exhibits strong scaling capabilities. We will make our code publicly available.