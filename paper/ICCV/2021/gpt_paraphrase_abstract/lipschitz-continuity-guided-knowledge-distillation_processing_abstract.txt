Knowledge distillation is a popular technique for compressing large teacher networks into smaller student networks. However, existing methods overlook the functional properties of neural networks, making it difficult to apply them to new tasks reliably. To address this issue, this paper introduces a novel framework called Lipschitz Continuity Guided Knowledge Distillation. This framework leverages Lipschitz continuity to better represent the functional characteristics of neural networks and guide the knowledge distillation process. By minimizing the distance between the Lipschitz constants of two neural networks, the proposed method enables teacher networks to effectively regularize student networks and improve their performance. An explainable approximation algorithm is derived to calculate the Lipschitz constant, addressing the NP-hard problem. Experimental results on CIFAR-100, ImageNet, and PASCAL VOC datasets demonstrate the superiority of the proposed method over other benchmarks in various knowledge distillation tasks such as classification, segmentation, and object detection. The code for the method is available at https://github.com/42Shawn/LONDON/tree/master.