The importance of indoor visual localization for applications like autonomous robots, augmented reality, and mixed reality is widely recognized. While recent advances in visual localization have shown promise in large-scale indoor spaces, there is still room for improvement in terms of accuracy. One key limitation of existing methods is the sparse distribution of image positions in the database, leading to view differences between query and retrieved images. To address this issue, we propose a new module called pose correction, which uses local feature matching to re-estimate the pose in a similar view by reorganizing the local features. This module enhances the accuracy of the initial pose estimation and provides more reliable rankings. Our proposed method achieves a new state-of-the-art performance with over 90% accuracy within 1.0 m in the challenging indoor benchmark dataset InLoc, surpassing previous results.