Recently, model compression techniques have gained significant attention in the field of artificial intelligence (AI) for creating efficient AI models that can be used in real-time applications. One popular compression strategy is channel pruning, which is widely used to reduce the size of deep neural networks (DNNs). However, the criteria for determining the importance of channels in the pruning process are still unclear, leading to different selection heuristics. Some existing pruning methods use sampling strategies to train sub-networks, but this often results in training instability and degraded performance of the compressed model.  To address these research gaps, we propose a new module called Gates with Differentiable Polarization (GDP) that is inspired by principled optimization ideas. GDP can be easily integrated before convolutional layers without any complex modifications. It allows for the control of individual channels or entire layer blocks, enabling smooth activation and deactivation of specific channels during the training process. By leveraging the polarization effect, a subset of gates is gradually driven to zero while other gates maintain a significant margin away from zero. Once training is complete, the channels with zero-gated gates can be effortlessly removed without interrupting the training or undermining the performance of the trained model.   We conducted experiments on the CIFAR-10 and ImageNet datasets to evaluate the performance of the GDP algorithm on various benchmark DNNs with different pruning ratios. The results demonstrate that GDP achieves state-of-the-art performance compared to other methods. We also applied GDP to the DeepLabV3Plus-ResNet50 model for the challenging Pascal VOC segmentation task and observed no drop in test performance, even achieving slight improvements, while saving over 60% of the floating-point operations (FLOPs).