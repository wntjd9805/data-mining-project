The increased availability and advancement of head-mounted and wearable devices have created new possibilities for remote communication and collaboration. However, the signals provided by these devices, such as head pose, hand pose, and gaze direction, do not provide a complete representation of a person. A major challenge is therefore how to utilize these signals to accurately depict the user. This study introduces a novel method that employs variational autoencoders to generate articulated poses of a human skeleton using imperfect streams of head and hand pose data. Our approach incorporates a unique and well-founded model of pose likelihood. Through experiments on publicly available datasets, we demonstrate the effectiveness of our method even with limited signals and explore ways to enhance the accuracy and realism of pose prediction.