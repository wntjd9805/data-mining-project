Spike Neural Networks (SNNs) have gained significant attention in research due to their ability to process temporal information, low power consumption, and biological plausibility. However, developing efficient and high-performance learning algorithms for SNNs remains challenging. Existing methods typically focus on learning weights and require manual tuning of parameters that determine the dynamics of individual neurons. These parameters are usually the same for all neurons, limiting the diversity and expressiveness of SNNs. To address this, we propose a training algorithm that learns both synaptic weights and membrane time constants, inspired by the observation that these parameters vary across brain regions. Incorporating learnable membrane time constants makes the network less sensitive to initial values and accelerates learning. Additionally, we reassess pooling methods in SNNs and find that max-pooling does not result in significant information loss, while offering the advantages of low computation cost and binary compatibility. We evaluate our method on various image classification tasks using both traditional datasets like MNIST, Fashion-MNIST, CIFAR-10, and neuromorphic datasets like N-MNIST, CIFAR10-DVS, DVS128 Gesture. The experimental results demonstrate that our method outperforms state-of-the-art accuracy on most datasets, while requiring fewer time-steps for computation. Our code is available at the provided GitHub link. (Figure 1 illustrates a Leaky Integrate-and-Fire (LIF) neuron and its membrane potential)