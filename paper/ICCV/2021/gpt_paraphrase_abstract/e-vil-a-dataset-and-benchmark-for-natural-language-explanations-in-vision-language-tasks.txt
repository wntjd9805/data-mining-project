In recent times, there has been a growing interest in developing models that can generate natural language explanations (NLEs) for vision-language (VL) tasks. These models are desirable because they can provide explanations that are easy for humans to understand and are comprehensive. However, the lack of comparison between existing methods is due to the absence of reusable evaluation frameworks and limited datasets. This study introduces e-ViL and e-SNLI-VE, which address these limitations. e-ViL serves as a benchmark for explainable vision-language tasks by establishing a unified evaluation framework and conducting the first comprehensive comparison of existing approaches that generate NLEs. It includes four models and three datasets, and employs both automatic metrics and human evaluation to assess the quality of model-generated explanations. On the other hand, e-SNLI-VE is currently the largest VL dataset with NLEs, consisting of over 430k instances. Additionally, a new model is proposed that combines UNITER and GPT-2. This model outperforms the previous state of the art across all datasets by a significant margin. The code and data for this work are available at https://github.com/maximek3/e-ViL.