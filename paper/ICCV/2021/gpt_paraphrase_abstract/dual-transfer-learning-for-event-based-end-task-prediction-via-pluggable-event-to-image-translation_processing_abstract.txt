Event cameras are sensors that detect changes in pixel intensity and produce event streams with high dynamic range and less motion blur. Previous research has shown that event data can be used for end-task learning, such as semantic segmentation, using encoder-decoder networks. However, it is challenging to recover fine details using only the decoder because events are sparse and mainly capture edge information. Additionally, current methods often rely solely on pixel-wise loss for supervision, which may not fully exploit the visual details from sparse events and result in suboptimal performance. To address these issues, this paper proposes a two-stream framework called Dual Transfer Learning (DTL) to enhance end-task performance without incurring additional inference cost. The framework consists of three components: an event to end-task learning (EEL) branch, an event to image translation (EIT) branch, and a transfer learning (TL) module. The TL module leverages both feature-level affinity information and pixel-level knowledge from the EIT branch to improve the EEL branch. This straightforward yet innovative approach enables robust representation learning from events, leading to significant performance improvements in tasks like semantic segmentation and depth estimation.