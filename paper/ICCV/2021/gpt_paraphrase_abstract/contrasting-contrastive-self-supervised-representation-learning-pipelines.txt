In recent years, there have been significant advancements in self-supervised representation learning. Despite the widespread adoption and success of representations learned through this approach, there is still much to be understood about how different training methods and datasets impact performance on downstream tasks. This study focuses on analyzing contrastive approaches, which are among the most successful and popular variants of self-supervised representation learning. We examine the training algorithms, pre-training datasets, and end tasks from a comprehensive perspective. More specifically, we conduct over 700 training experiments involving 30 encoders, 4 pre-training datasets, and 20 diverse downstream tasks. Our experiments aim to address several key questions related to the performance of self-supervised models compared to supervised models, the current benchmarks used for evaluation, and the influence of pre-training data on the performance of end tasks. To facilitate further research, we have created the Visual Representation Benchmark (ViRB), which is publicly available at: https://github.com/allenai/virb.