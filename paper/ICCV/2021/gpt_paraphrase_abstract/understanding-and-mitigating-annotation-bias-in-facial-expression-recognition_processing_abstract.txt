The effectiveness of a computer vision model depends on the quality and quantity of its training data. Recent research has uncovered biases in commonly used image datasets, leading to skewed model outputs. While previous studies have attempted to address these biases, they often assume that human-generated annotations are unbiased and of high quality. However, this assumption can be problematic. In this study, we focus on facial expression recognition and compare biases in labeled data between controlled laboratory datasets and real-world datasets. We find significant biases in expression annotations, particularly in happy and angry expressions, and traditional methods are unable to fully mitigate these biases. To address this, we propose a framework called AU-Calibrated Facial Expression Recognition (AUC-FER) that incorporates facial action units (AUs) and the triplet loss into the objective function. Experimental results indicate that our proposed method is more effective in removing expression annotation biases compared to existing techniques.