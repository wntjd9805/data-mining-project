Video-Text Retrieval has become a popular area of research as the amount of multimedia data on the internet continues to grow. The use of Transformer for video-text learning has gained attention due to its promising performance. However, current cross-modal transformer approaches have two main limitations. Firstly, they do not fully exploit the potential of the transformer architecture, as different layers have different feature characteristics. Secondly, the end-to-end training mechanism restricts interactions between negative samples in a mini-batch. To address these limitations, we propose a new approach called Hierarchical Transformer (HiT) for video-text retrieval. HiT utilizes Hierarchical Cross-modal Contrastive Matching at both the feature-level and semantic-level, resulting in comprehensive and multi-view retrieval outcomes. Additionally, we draw inspiration from MoCo and introduce Momentum Cross-modal Contrast for cross-modal learning, enabling on-the-fly interactions with large-scale negative samples. This contributes to the generation of more precise and discriminative representations. Our method is evaluated on three major Video-Text Retrieval benchmark datasets, and experimental results demonstrate its advantages.