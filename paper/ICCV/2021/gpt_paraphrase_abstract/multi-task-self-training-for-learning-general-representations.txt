Training a single general model that performs well across various computer vision tasks remains a challenge, despite advancements in specialized models. In this study, we propose a method called multi-task self-training (MuST) that utilizes the knowledge from independent specialized teacher models to train a single general student model. The approach involves three steps: training specialized teachers independently on labeled datasets, using these teachers to label an unlabeled dataset and create a multi-task pseudo labeled dataset, and finally training a student model using multi-task learning on this dataset. We evaluate the student model's feature representations on six vision tasks, including image recognition and 3D geometry estimation. MuST proves to be scalable with unlabeled or partially labeled datasets and outperforms both specialized supervised models and self-supervised models when trained on large-scale datasets. Additionally, we demonstrate that MuST can enhance already strong checkpoints trained with billions of examples. These findings indicate that self-training shows promise in aggregating labeled and unlabeled training data to learn general feature representations.