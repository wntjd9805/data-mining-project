This paper addresses the problem of deep neural networks making overconfident predictions on out-of-distribution (OOD) samples. To tackle this issue, the authors propose a method called Chamfer OOD examples (CODEs) that can effectively alleviate the problem by suppressing predictions on a subset of OOD samples. CODEs are generated by manipulating in-distribution samples from different categories through slicing and splicing operations, and then transforming their distribution using the Chamfer generative adversarial network. The authors demonstrate that training with the suppression of predictions on CODEs significantly reduces OOD overconfidence without negatively impacting classification accuracy. This approach outperforms existing methods and also proves beneficial for enhancing OOD detection and classification.