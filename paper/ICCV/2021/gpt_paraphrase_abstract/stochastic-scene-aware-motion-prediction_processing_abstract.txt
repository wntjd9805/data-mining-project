The abstract discusses the goal of capturing and synthesizing realistic human behavior in computer vision. The aim is to enable virtual humans to navigate and interact with objects in cluttered indoor scenes. This has applications in virtual reality, computer games, and robotics, and can also be used as training data. However, the challenge lies in the diversity and adaptability of real human motion in different scenes. To address this, the authors propose a novel data-driven method called Scene-Aware Motion Prediction (SAMP) that models various styles of performing actions with different target objects. SAMP is able to generalize to objects with different geometries and allows the virtual character to navigate in cluttered scenes. The authors collected motion capture data covering various sitting, lying down, walking, and running styles to train SAMP. The method is demonstrated on complex indoor scenes and outperforms existing solutions. The code and data for research purposes can be accessed at https://samp.is.tue.mpg.de.