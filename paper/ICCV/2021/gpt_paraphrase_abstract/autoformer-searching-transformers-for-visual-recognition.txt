Recently, transformer-based models have shown promise for vision tasks like image classification and detection. However, designing these transformer networks is challenging. The depth, embedding dimension, and number of heads greatly impact the performance of vision transformers, but previous models determined these dimensions manually. In this study, we propose AutoFormer, a one-shot architecture search framework specifically for vision transformers. AutoFormer trains a supernet by intertwining the weights of different blocks in the same layers. This strategy allows thousands of subnets to be effectively trained using the supernet. The performance of these subnets, which inherit weights from the supernet, is comparable to subnets trained from scratch. Moreover, the searched models, referred to as AutoFormers, outperform recent state-of-the-art models like ViT and DeiT. For example, AutoFormer-tiny/small/base achieve top-1 accuracy of 74.7%/81.7%/82.4% on ImageNet with 5.7M/22.9M/53.7M parameters respectively. Additionally, we validate the transferability of AutoFormer through downstream benchmarks and distillation experiments. Code and models can be accessed at https://github.com/microsoft/Cream.