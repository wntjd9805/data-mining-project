Recent advancements in 3D object detection from single images have utilized monocular depth estimation to generate 3D point clouds, effectively transforming cameras into pseudo-lidar sensors. These two-stage detectors heavily rely on the accuracy of the intermediate depth estimation network, which can be enhanced through large-scale self-supervised learning without manual labels. However, these detectors often suffer from overfitting, are more complex, and still exhibit a significant performance gap compared to lidar-based detectors. In this study, we propose an end-to-end, single-stage, monocular 3D object detector called DD3D. Our approach leverages depth pre-training similar to pseudo-lidar methods but eliminates their limitations. We have designed our architecture to enable effective information transfer between depth estimation and 3D detection, allowing us to scale with the amount of unlabeled pre-training data. Our method achieves state-of-the-art performance on two challenging benchmarks, achieving 16.34% and 9.28% average precision (AP) for cars and pedestrians (respectively) on the KITTI-3D benchmark and 41.5% mean average precision (mAP) on NuScenes.