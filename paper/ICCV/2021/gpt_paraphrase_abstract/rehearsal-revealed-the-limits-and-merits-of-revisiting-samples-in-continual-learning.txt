This study addresses the challenge of learning from non-stationary data streams and overcoming catastrophic forgetting in machine learning. Instead of focusing on improving current methods, the authors aim to provide a better understanding of the limits and benefits of rehearsal, a commonly used method in continual learning. The authors propose that models trained sequentially with rehearsal tend to remain in a low-loss region after completing a task, but are susceptible to overfitting on the sample memory, leading to poor generalization. Through both conceptual and empirical evidence on three benchmark datasets, the authors demonstrate the presence of both behaviors and offer new insights into the dynamics of rehearsal and continual learning. Furthermore, the authors provide a fresh perspective on important works in continual learning, allowing for a deeper comprehension of their accomplishments.