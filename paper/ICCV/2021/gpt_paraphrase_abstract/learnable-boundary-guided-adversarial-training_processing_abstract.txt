Previous adversarial training has improved the robustness of models but at the expense of accuracy on natural data. This paper aims to mitigate the decrease in natural accuracy by leveraging the model logits of a well-trained clean model to guide the learning of a robust model. The logits from the clean model contain the most discriminative features of natural data, such as the generalizable classifier boundary. To achieve this, the logits from the robust model, which takes adversarial examples as input, are constrained to be similar to those from the clean model when fed with corresponding natural data. This allows the robust model to inherit the classifier boundary of the clean model. The authors find that this boundary guidance not only preserves high natural accuracy but also improves model robustness, providing new insights for the adversarial community. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet confirm the effectiveness of their method, achieving state-of-the-art robustness on CIFAR-100 without the need for additional real or synthetic data. The code for their method is available at https://github.com/dvlab-research/LBGAT.