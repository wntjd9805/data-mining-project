Gating-based methods have been successful in adapting computations in dynamic neural networks. However, existing works only focus on exploring redundancy from a single point in the network, limiting performance. This paper introduces a new dynamic computing method called dual gating, which reduces model complexity during runtime. Dual gating identifies informative features in both spatial and channel dimensions. The spatial gating module determines essential areas, while the channel gating module predicts salient channels that contribute more to the results. This allows for skipping computation of unimportant regions and irrelevant channels during inference. Experimental results on various datasets show that our method achieves higher accuracy with similar computing budgets compared to other dynamic execution methods. Specifically, dynamic dual gating reduces computing by 59.7% for ResNet50 with 76.41% top-1 accuracy on ImageNet, surpassing the state-of-the-art. The code for this method is available at https://github.com/lfr-0531/DGNet.