Recently, neural architecture search (NAS) methods have been developed to train multiple architectures in a supernet and estimate their potential accuracy using detached network weights. However, these methods face a dilemma due to the incorrect correlation between predicted accuracy and actual capability. This problem is attributed to the consistency shift during supernet training, including feature shift and parameter shift. Feature shift refers to the dynamic input distributions of a hidden layer caused by random path sampling, which affects the loss descent and architecture ranking. Parameter shift occurs when contradictory parameter updates for a shared layer are present in different paths during training steps, making it difficult to preserve architecture ranking. To address these issues, we propose a novel supernet-⇧ model called ⇧-NAS. Our approach includes cross-path learning in the supernet-⇧ model to reduce feature consistency shift and a mean teacher with negative samples to overcome parameter shift and model collision. Additionally, our ⇧-NAS operates in an unsupervised manner, allowing for the search of more transferable architectures. Extensive experiments on ImageNet and various downstream tasks demonstrate the effectiveness and universality of our ⇧-NAS compared to supervised NAS.