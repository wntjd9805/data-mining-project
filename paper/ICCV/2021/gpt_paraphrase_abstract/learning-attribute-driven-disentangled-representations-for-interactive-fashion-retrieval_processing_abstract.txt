Interactive retrieval for online fashion shopping allows users to modify image search results based on their feedback. However, a common issue with this approach is that changing one aspect of an item, such as its color, can unintentionally alter other attributes, like the sleeve type. This problem arises because existing methods learn visual representations that are closely connected in the embedding space, limiting control over the retrieved results. To address this, we propose training convolutional networks to learn attribute-specific subspaces for each visual attribute, resulting in disentangled representations. This enables operations like swapping out one attribute value without affecting others. Our model demonstrates superior performance on three fashion retrieval tasks: attribute manipulation retrieval, conditional similarity retrieval, and complementary item retrieval for outfits. The code and models for our approach are publicly available.