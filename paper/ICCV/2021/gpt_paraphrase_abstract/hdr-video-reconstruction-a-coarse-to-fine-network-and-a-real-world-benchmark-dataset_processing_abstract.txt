HDR video reconstruction from sequences captured with different exposure levels is a difficult task. Current methods use optical flow to align the input images in the image space and then merge them to create HDR output. However, this approach is not accurate due to missing details in over-exposed areas and noise in under-exposed areas, resulting in undesirable ghosting artifacts. To improve alignment and fusion, we propose a deep learning framework for HDR video reconstruction. We first perform coarse alignment and blending in the image space to estimate a rough HDR video. Then, we conduct more advanced alignment and temporal fusion in the feature space of the rough HDR video to enhance the reconstruction quality. To evaluate HDR video reconstruction methods, we have created a benchmark dataset consisting of 97 sequences of static scenes and 184 pairs of dynamic scenes. Through extensive experiments, we demonstrate that our method outperforms existing state-of-the-art techniques. Our code and dataset are available at https://guanyingc.github.io/DeepHDRVideo.