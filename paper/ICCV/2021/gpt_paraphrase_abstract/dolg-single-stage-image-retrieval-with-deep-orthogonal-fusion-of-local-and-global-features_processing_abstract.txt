The aim of this study is to develop a single-stage solution for image retrieval by integrating both local and global information into compact image representations. Traditionally, image retrieval involves retrieving candidate images based on their similarity to a query image using global features, and then re-ranking the candidates using their local features. However, previous learning-based approaches have focused on either global or local image representation learning. In this paper, we propose a Deep Orthogonal Local and Global (DOLG) framework that combines local and global information for end-to-end image retrieval.The DOLG framework first extracts representative local information using multi-atrous convolutions and self-attention. From the local information, components that are orthogonal to the global image representation are extracted. These orthogonal components are then concatenated with the global representation to provide a complementary representation. Finally, aggregation is performed to generate the final image representation. The entire framework is end-to-end differentiable and can be trained using image-level labels.Experimental results demonstrate the effectiveness of our approach, showing that our model achieves state-of-the-art image retrieval performance on the Revisited Oxford and Paris datasets.