We introduce a new technique for generating street-view panoramic videos that are both temporally and geometrically consistent using a single satellite image and camera trajectory. While existing methods focus on image synthesis, video synthesis in this context has not been extensively explored. To achieve geometric and temporal consistency, our approach involves creating a 3D point cloud representation of the scene and maintaining dense 3D-2D correspondences across frames that align with the geometric structure inferred from the satellite view. In the 3D space, we employ a cascaded network architecture comprising two hourglass modules to generate coarse and fine features based on semantics and per-class latent vectors. These features are then projected onto frames and up-sampled to produce the final realistic video. By utilizing the computed correspondences, the generated street-view video frames adhere to the 3D geometric scene structure and maintain temporal consistency. Our qualitative and quantitative experiments demonstrate superior results compared to other synthesis approaches that lack either temporal consistency or realistic appearance. To the best of our knowledge, our work is the first to synthesize cross-view images into videos.