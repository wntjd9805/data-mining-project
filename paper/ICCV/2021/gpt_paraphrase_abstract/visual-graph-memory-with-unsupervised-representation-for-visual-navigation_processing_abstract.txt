We introduce a new type of memory for visual navigation called visual graph memory (VGM). This memory is created by extracting unsupervised image representations from navigation history. The VGM is constructed gradually based on the similarities between the unsupervised representations of observed images, which are learned from an image dataset without labels. To address visual navigation problems, we propose a navigation framework that utilizes the VGM. The framework combines a graph convolutional network and an attention mechanism, allowing the agent to refer to the VGM for navigation while simultaneously constructing it. The VGM enables the agent to embed its navigation history and other relevant task-related information. To evaluate our approach, we conduct experiments on visual navigation tasks using the Gibson dataset within the Habitat simulator, offering a realistic simulation environment. The extensive experimental results demonstrate that our navigation agent with VGM outperforms existing approaches in image-goal navigation tasks. For more information about our project, please visit our project page at https://sites.google.com/view/iccv2021vgm.