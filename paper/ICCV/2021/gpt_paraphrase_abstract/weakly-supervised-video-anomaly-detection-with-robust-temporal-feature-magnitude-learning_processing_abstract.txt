Anomaly detection in videos, using weakly supervised video-level labels, is commonly approached as a multiple instance learning (MIL) problem. The goal is to identify abnormal events within video snippets, with each video being represented as a collection of these snippets. Current methods for anomaly detection show effective performance, but they are biased towards recognizing negative instances, especially when the abnormal events are subtle and only exhibit small differences compared to normal events. This bias is further amplified in methods that do not consider important temporal dependencies in videos. To address this issue, we propose a new method called Robust Temporal Feature Magnitude learning (RTFM). RTFM trains a function to effectively recognize positive instances, improving the robustness of MIL to negative instances in abnormal videos. It also incorporates dilated convolutions and self-attention mechanisms to capture both long- and short-range temporal dependencies, enabling more accurate learning of feature magnitudes. Extensive experiments demonstrate that the RTFM-enabled MIL model outperforms several state-of-the-art methods on four benchmark datasets (ShanghaiTech, UCF-Crime, XD-Violence, and UCSD-Peds). Furthermore, it significantly enhances the discriminability of subtle anomalies and improves sample efficiency.