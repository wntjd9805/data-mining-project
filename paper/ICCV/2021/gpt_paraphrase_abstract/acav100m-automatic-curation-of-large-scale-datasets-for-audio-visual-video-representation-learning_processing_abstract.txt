The natural connection between visual observations and their corresponding sound provides valuable signals for learning video representations. Online videos offer a vast amount of training data, but many contain irrelevant audio-visual signals due to edited or overdubbed audio. Models trained on these uncurated videos tend to learn suboptimal representations. To address this, current self-supervised approaches rely on datasets with predetermined semantic concepts, where audio-visual correspondence is likely. However, creating such datasets requires time-consuming manual annotation or verification, limiting the usefulness of online videos for large-scale learning. In this study, we propose an automated dataset curation method that maximizes the mutual information between audio and visual channels in videos. Our approach identifies videos with high audio-visual correspondence and demonstrates that self-supervised models trained on this data achieve competitive performance compared to models trained on manually curated datasets. The key advantage of our approach is scalability, and we release ACAV100M, a dataset containing 100 million videos with high audio-visual correspondence, ideal for self-supervised video representation learning.