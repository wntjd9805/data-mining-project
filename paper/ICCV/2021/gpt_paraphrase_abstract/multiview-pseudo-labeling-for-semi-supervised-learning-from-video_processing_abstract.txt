We introduce a new approach called multiview pseudo-labeling for video learning. This framework utilizes two different views, appearance and motion, to enhance semi-supervised learning in videos. By incorporating these complementary views, we are able to generate more accurate "pseudo-labels" for unlabeled videos, resulting in stronger video representations compared to using only supervised data. Despite leveraging multiple views, our approach trains a shared model for both appearance and motion inputs, eliminating the need for additional computational resources during inference. Our method achieves significantly better results than its supervised counterpart on various video recognition datasets and performs favorably compared to previous methods for self-supervised video representation learning on standard benchmarks.