Supervised deep learning-based hash and vector quantization have significantly improved the speed and effectiveness of image retrieval systems. These methods utilize label annotations to achieve superior retrieval performance compared to traditional methods. However, accurately assigning labels to a large amount of training data is a challenging and error-prone task. In order to address these issues, we propose a novel approach called Self-supervised Product Quantization (SPQ) network, which does not rely on labels and can train itself. Our method incorporates a Cross-Quantized Contrastive learning strategy that simultaneously learns codewords and deep visual descriptors by comparing transformed images. This allows us to extract descriptive features from image contents and enhance the accuracy of retrieval. Through extensive experimentation on benchmark datasets, we demonstrate that our method achieves state-of-the-art results even without supervised pretraining.