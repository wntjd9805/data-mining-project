Recent advancements in self-supervised learning have demonstrated that these algorithms acquire visual representations that can be applied more effectively to new tasks compared to cross-entropy based methods that rely on task-specific supervision. This paper investigates the applicability of this finding in the context of continual learning, revealing that contrastively learned representations are more resilient against catastrophic forgetting than those trained with the cross-entropy objective. Based on this novel observation, we propose a continual learning algorithm that prioritizes the continual acquisition and preservation of transferable representations. Specifically, our approach involves (1) learning representations using the contrastive learning objective and (2) preserving these learned representations through a self-supervised distillation step. We extensively validate our method using popular benchmark image classification datasets, which demonstrates its superiority as it achieves state-of-the-art performance. The source code for our algorithm is available at https://github.com/chaht01/Co2L.