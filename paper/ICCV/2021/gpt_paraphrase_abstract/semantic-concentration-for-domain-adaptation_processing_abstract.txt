Domain adaptation (DA) is a technique used to address the challenges of label annotation and dataset bias by transferring knowledge from a labeled source domain to an unlabeled target domain. Most DA methods focus on aligning the feature distributions of the two domains, but they often include irrelevant semantic information, such as messy backgrounds, which can negatively affect object matching and lead to semantically incorrect transfers. To overcome this issue, we propose a method called Semantic Concentration for Domain Adaptation (SCDA). SCDA encourages the model to focus on the most important features by adversarially aligning prediction distributions. Specifically, we train the classifier to maximize the divergence of prediction distributions for each sample pair, enabling the model to identify regions with significant differences within the same class. Simultaneously, the feature extractor works to minimize these differences, suppressing dissimilar features and emphasizing principal ones. SCDA can be easily incorporated into various DA methods as a regularizer to enhance their performance. Our extensive experiments on cross-domain benchmarks demonstrate the effectiveness of SCDA.