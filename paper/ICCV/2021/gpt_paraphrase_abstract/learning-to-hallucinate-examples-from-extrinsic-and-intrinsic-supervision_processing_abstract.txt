Learning to generate additional examples has recently emerged as a promising approach for addressing few-shot learning tasks. This study explores two crucial but often overlooked sources of guidance for the generation process: (i) extrinsic supervision, which aims to ensure that classifiers trained on generated examples closely resemble strong classifiers trained on a large amount of real examples, and (ii) intrinsic supervision, which aims to bring together clusters of generated and real examples belonging to the same class while pushing apart clusters of generated and real examples from different classes. To achieve extrinsic supervision, we introduce an additional mentor model trained on abundant data from base classes to guide the generation process. For intrinsic supervision, we employ contrastive learning to encourage the formation of coherent clusters. Our proposed approach, known as the dual mentor- and self-directed (DMAS) hallucinator, is a versatile and model-agnostic framework that significantly enhances few-shot learning performance across various scenarios on well-established benchmarks.