Deep neural networks have made significant advancements in accuracy for appearance-based gaze estimation. However, these networks still struggle to perform well when applied to new domains, such as unseen environments or individuals. To address this limitation, we propose a plug-and-play gaze adaptation framework called PnP-GA. This framework is an ensemble of networks that collaboratively learn with the help of outliers. Unlike other methods, PnP-GA does not require ground-truth labels in the target domain, allowing existing gaze estimation networks to be directly incorporated and applied to new domains. We evaluate the performance of PnP-GA on four gaze domain adaptation tasks: ETH-to-MPII, ETH-to-EyeDiap, Gaze360-to-MPII, and Gaze360-to-EyeDiap. Our experimental results demonstrate that PnP-GA achieves significant performance improvements of 36.9%, 31.6%, 19.4%, and 11.8% over the baseline system. Furthermore, our proposed framework outperforms state-of-the-art domain adaptation approaches for gaze domain adaptation tasks. The code for PnP-GA is available at https://github.com/DreamtaleCore/PnP-GA.