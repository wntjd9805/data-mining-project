Many unpaired image-to-image translation applications require the preservation of the semantic content during the translation process. However, existing distribution matching methods, such as GAN-based methods, do not consider the inherent differences in semantic distributions between the source and target domains, leading to undesired solutions. Although these methods produce visually plausible outputs, they often change the semantics of the input. In order to address this issue without additional supervision, we propose a method that enforces semantic invariance in the translated outputs with respect to small perceptual variations in the inputs, which we refer to as "semantic robustness". By optimizing a robustness loss based on perturbations in the multi-scale feature space of the inputs, our method effectively reduces semantic changes and generates translations that outperform existing methods in both quantitative and qualitative evaluations.