We present a video autoencoder that learns to extract disentangled representations of 3D structure and camera pose from videos. Our approach leverages the temporal continuity in videos to assume that the 3D scene structure remains static in nearby frames. The video autoencoder takes a sequence of video frames as input and generates a disentangled representation of the scene, consisting of a temporally-consistent deep voxel feature for the 3D structure and a 3D trajectory of camera pose for each frame. These representations are then recombined to render the input video frames. Importantly, our video autoencoder can be trained without any ground truth 3D or camera pose annotations using a pixel reconstruction loss. The disentangled representation learned by our model can be utilized for various tasks such as novel view synthesis, camera pose estimation, and video generation. We evaluate our method on large-scale natural video datasets and demonstrate its generalization capabilities on out-of-domain images. The project page, along with the code, can be found at https://zlai0.github.io/VideoAutoencoder.