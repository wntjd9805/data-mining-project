Dynamic interacting systems are common in vision tasks, but observing and measuring these interactions directly can be challenging. However, understanding these latent interactions is crucial for tasks such as forecasting. Neural relational inference (NRI) techniques have been introduced to estimate interpretable relations between entities in a system for trajectory prediction. However, NRI assumes static relations, which limits its ability to handle dynamic relations and model long-term dependencies. To address this limitation, dynamic neural relational inference (DNRI) was proposed, which uses LSTM to handle dynamic relations. However, DNRI struggles with modeling long-term dependencies and forecasting long sequences because it washes away older information when updating the latent variable. To overcome this challenge, we propose a memory-augmented dynamic neural relational inference method. This approach incorporates two associative memory pools: one for interactive relations and another for individual entities. These memory pools help retain useful relation features and node features for future estimation. Our model dynamically estimates relations by learning better embeddings and utilizing long-range information stored in the memory. With the addition of memory modules and customized structures, our memory-augmented DNRI can update and access the memory as needed. These memory pools also serve as global latent variables across time, providing detailed long-term temporal relations that can be readily used by other components. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of our proposed method in modeling dynamic relations and forecasting complex trajectories.