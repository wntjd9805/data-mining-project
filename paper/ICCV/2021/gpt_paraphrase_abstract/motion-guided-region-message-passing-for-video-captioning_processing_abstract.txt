Video captioning is a widely studied task in computer vision. While previous methods have made progress by utilizing spatial information, they either rely on expensive external object detectors or fail to adequately capture spatial/temporal relations. In this paper, we propose a novel approach for video captioning that extracts and aggregates spatial information without the need for external detectors. Our method includes a Recurrent Region Attention module that improves the extraction of diverse spatial features. Additionally, our model incorporates Motion-Guided Cross-frame Message Passing to capture temporal structure and establish high-order relations between regions across frames. This allows for effective information communication and the generation of compact and powerful video representations. We also introduce an Adjusted Temporal Graph Decoder that dynamically updates video features and models high-order temporal relations during the decoding process. Experimental results on three benchmark datasets show that our approach outperforms state-of-the-art methods.