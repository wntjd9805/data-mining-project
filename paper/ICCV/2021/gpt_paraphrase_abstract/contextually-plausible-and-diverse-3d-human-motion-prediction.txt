We aim to predict diverse 3D human motion by forecasting multiple plausible future 3D poses based on observed poses. The common method used is a Conditional Variational Autoencoder (CVAE), but current approaches either fail to capture motion diversity or generate diverse but unrealistic continuations of the observed motion. In this study, we address both issues by developing a new variational framework that considers both diversity and context of the generated future motion. Unlike previous methods, we condition the sampling of the latent variable, which contributes to diversity, on the representation of the past observation, ensuring it carries relevant information. Our experiments show that our approach produces higher quality and diverse motions while preserving the contextual information from the observed motion.