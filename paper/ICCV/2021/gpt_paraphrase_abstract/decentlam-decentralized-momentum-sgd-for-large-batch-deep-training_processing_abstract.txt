With the increasing scale of deep learning, there is a need for efficient distributed training algorithms. VanillaParallel momentum SGD, which involves a global average across all computing nodes, is not as communication efficient as Decentralized momentumSGD (DmSGD), where each node only averages with its neighbors. However, it has been shown that large-batch training is crucial for achieving faster runtime. This led us to investigate the performance of DmSGD in the large-batch scenario.

In our research, we discovered that the momentum term in DmSGD can amplify the inconsistency bias. This bias becomes more apparent as the batch size increases, resulting in significant performance degradation. To address this issue, we propose a new algorithm called DecentLaM, which is a decentralized large-batch momentum SGD that mitigates the bias caused by momentum. We establish the convergence rate of DecentLaM for both strongly convex and non-convex scenarios.

Our theoretical analysis confirms that DecentLaM outperforms DmSGD, particularly in the large-batch scenario. Additionally, our experiments on various computer vision tasks and models demonstrate that DecentLaM offers efficient and high-quality training.