We demonstrate that face-related computer vision can be successfully conducted in real-world scenarios using synthetic data exclusively. Although synthesizing training data with graphics has been practiced by the research community, the disparity between real and synthetic data has remained a challenge, particularly for human faces. Previous attempts to bridge this gap involved data mixing, domain adaptation, and domain-adversarial training; however, we present evidence that synthesizing data with minimal domain gap is achievable. Consequently, models trained on synthetic data can effectively generalize to real in-the-wild datasets. We explain how a procedurally-generated parametric 3D face model can be combined with an extensive collection of hand-crafted assets to generate training images that exhibit unparalleled realism and diversity. Through training machine learning systems on these images, we successfully accomplish face-related tasks such as landmark localization and face parsing. Our results demonstrate that synthetic data can match the accuracy of real data while also enabling novel approaches that would be otherwise impossible due to the impracticality of manual labeling.