This paper presents a novel approach for accurate video understanding by incorporating spatio-temporal relations between actors, objects, and their environment. The proposed method utilizes a message passing graph neural network to explicitly model these relationships, using explicit object representations when supervision is available, and implicit representations otherwise. By generalizing previous structured models, the study explores the impact of different design choices in graph structure and representation on the model's performance. The effectiveness of the method is demonstrated through experiments on two tasks: spatio-temporal action detection on AVA and UCF101-24 datasets, and video scene graph classification on the Action Genome dataset. The results obtained are state-of-the-art on all three datasets, indicating the superiority of the proposed method. Additionally, the paper provides quantitative and qualitative evidence of the method's ability to effectively model relationships between relevant entities in the scene.