Sparse voxel-based methods have become the leading approach for 3D semantic segmentation of indoor scenes. However, these methods have limitations in handling complex geometries and distinguishing between spatially close objects. To address these issues, we propose VMNet, a new 3D deep architecture that combines voxel and mesh representations to leverage both Euclidean and geodesic information. By extracting contextual cues from voxels and separating objects with disconnected surfaces using meshes, VMNet achieves effective feature aggregation and adaptive feature fusion through intra-domain and inter-domain attentive modules. Experimental results on the ScanNet dataset demonstrate that VMNet outperforms the state-of-the-art SparseConvNet and MinkowskiNet models in terms of mean Intersection over Union (mIoU), while also having a simpler network structure. The code for VMNet is available at https://github.com/hzykent/VMNet.