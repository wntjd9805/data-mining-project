The abstract discusses the importance of learning transferable and domain adaptive feature representations from videos for tasks such as action recognition. Current methods for video domain adaptation rely on adversarial feature alignment in the RGB image space. However, video data often includes multiple modalities, such as RGB and optical flow, making it challenging to design a method that considers cross-modal inputs under the cross-domain adaptation setting. In response, the authors propose a unified framework for video domain adaptation that simultaneously regularizes cross-modal and cross-domain feature representations. They treat each modality in a domain as a view and use contrastive learning with well-designed sampling strategies to achieve this. The proposed framework improves the connection across modalities and alignment across domains in the feature spaces. The authors conduct experiments on benchmark datasets for domain adaptive action recognition and demonstrate the effectiveness of their approach compared to state-of-the-art algorithms.