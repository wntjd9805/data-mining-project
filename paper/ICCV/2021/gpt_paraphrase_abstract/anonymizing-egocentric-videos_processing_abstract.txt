Egocentric videos, which are captured from a wearer's perspective, often give the false impression that the wearer's privacy is protected because the wearer's face is not shown. However, recent research has demonstrated that it is possible to extract the wearer's gait signature from these videos, allowing for identification of the wearer. Additionally, hand gestures in egocentric videos can be used to recognize the wearer, and the wearer can even be identified in third-person videos. This poses a significant threat to the sharing of egocentric videos and the field of egocentric vision research. To address this issue, we propose a new method for anonymizing egocentric videos. This technique involves introducing imperceptible optical flow perturbations into the video frames, which do not affect object detection or activity recognition but disrupt the gait recovery process. Our experiments on the EPIC-Kitchens dataset demonstrate that this perturbation reduces wearer recognition performance while preserving activity recognition performance. We also develop a more robust wearer recognition method based on camera egomotion cues, which achieves state-of-the-art performance but is also vulnerable to the proposed anonymizing perturbations.