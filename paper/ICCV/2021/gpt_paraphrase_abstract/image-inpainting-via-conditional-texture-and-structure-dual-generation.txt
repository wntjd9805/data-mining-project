Recent advancements in deep generative approaches have significantly improved image inpainting by incorporating structure priors. However, existing solutions fail to effectively handle cases with extensive corruptions and often produce distorted results due to limited interaction between image texture and structure reconstruction. To address these limitations, this paper presents a novel two-stream network for image inpainting. This network models structure-constrained texture synthesis and texture-guided structure reconstruction in a coupled manner, enabling better utilization of each other's strengths for more realistic generation. Additionally, a Bi-directional Gated Feature Fusion (Bi-GFF) module is introduced to enhance global consistency by exchanging and combining structure and texture information. Furthermore, a Contextual Feature Aggregation (CFA) module is developed to refine generated contents through region affinity learning and multi-scale feature aggregation. Experimental evaluations on the CelebA, ParisStreetView, and Places2 datasets demonstrate the superiority of the proposed method in terms of both qualitative and quantitative metrics. The code for the proposed method is publicly available at https://github.com/Xiefan-Guo/CTSDG.