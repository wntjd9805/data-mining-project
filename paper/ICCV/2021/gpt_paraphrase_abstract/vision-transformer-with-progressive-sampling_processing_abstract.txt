Recently, powerful global relation modeling abilities have been introduced to fundamental computer vision tasks using Transformers. One example is the Vision Transformer (ViT), which applies a transformer architecture directly to image classification. However, the naive tokenization used in ViT can cause issues such as the destruction of object structures, the inclusion of uninterested regions like backgrounds, and the introduction of interference signals. To address these problems, this paper proposes an iterative and progressive sampling strategy to identify discriminative regions. In each iteration, embeddings from the current sampling step are inputted into a transformer encoder layer, and a set of sampling offsets is predicted to update the sampling locations for the next step. This progressive sampling approach is differentiable. When combined with the Vision Transformer, the resulting PS-ViT network can dynamically learn where to focus. PS-ViT is both effective and efficient, outperforming vanilla ViT by 3.8% in top-1 accuracy on ImageNet, with only 1/4 the number of parameters and 1/10 the number of FLOPs. The code for PS-ViT is available at https://github.com/yuexy/PS-ViT.