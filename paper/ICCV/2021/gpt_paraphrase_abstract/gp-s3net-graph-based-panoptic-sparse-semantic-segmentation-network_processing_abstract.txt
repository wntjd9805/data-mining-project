This paper introduces a new panoptic segmentation framework called GP-S3Net, which utilizes LiDAR data to efficiently identify objects in a scene. Unlike traditional two-stage panoptic systems that require object proposals, GP-S3Net is a proposal-free approach that incorporates a novel instance-level network. This network uses a graph convolutional network to identify foreground objects, which are then fused with background classes. Over-segmentation priors are generated from fine-grained clusters obtained from semantic segmentation, and 3D sparse convolution is used to embed each cluster. Each cluster is treated as a node in the graph, and a graph convolutional neural network predicts the existence of edges between cluster pairs. Ground truth edge labels are generated using instance labels to supervise the learning process. Experimental results demonstrate that GP-S3Net outperforms current state-of-the-art approaches on datasets such as nuScenes and SemanticPOSS, ranking first on the SemanticKITTI leaderboard. This research is significant as it addresses the growing importance of panoptic segmentation in environmental understanding and object identification, and highlights the effectiveness of GP-S3Net in achieving accurate segmentation results.