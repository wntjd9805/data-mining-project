This paper introduces a novel architecture called Multi-Scale Vision Longformer, which improves upon the existing Vision Transformer (ViT) [12] for encoding high-resolution images. The enhancement is achieved through two techniques. Firstly, the multi-scale model structure enables image encodings at various scales without incurring excessive computational costs. Secondly, the attention mechanism of Vision Longformer, a variant of Longformer [3] initially designed for natural language processing, is employed. This attention mechanism has a linear complexity with respect to the number of input tokens. The effectiveness of the proposed ViT is demonstrated through a comprehensive empirical study, where it outperforms several strong baselines including previous ViT models, their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent work [47]. The performance improvement is observed across various vision tasks such as image classification, object detection, and segmentation. The models and source code can be accessed at https://github.com/microsoft/vision-longformer.