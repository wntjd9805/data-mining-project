Batch Whitening is a technique that enhances the efficiency and stability of training by transforming input features to have a zero mean and a unit variance, while also removing linear correlation between channels. This normalization process is typically applied between convolution and activation functions in commonly used structures optimized with Batch Normalization. However, subsequent studies on Batch Whitening have neglected further analysis, assuming that the input of a linear layer is already whitened. To address this gap, we propose a new Convolutional Unit that aligns with the underlying theory, resulting in improved performance of Batch Whitening. We also demonstrate the ineffectiveness of the original Convolutional Unit by examining the rank and correlation of features. Our method utilizes Iterative Normalization, a cutting-edge whitening module, making it compatible with existing whitening modules. Experimental results on five image classification datasets (CIFAR-10, CIFAR-100, CUB-200-2011, Stanford Dogs, and ImageNet) show significant performance gains. Notably, our method enhances the stability and performance of whitening even when using large learning rates, group sizes, and iteration numbers. The implementation code is available at https://github.com/YooshinCho/pytorch_ConvUnitOptimization.