Low-latency deep spiking neural networks (SNNs) have emerged as a promising alternative to traditional artificial neural networks (ANNs) due to their potential for improved energy efficiency on event-driven neuromorphic hardware. However, like ANNs, SNNs are vulnerable to various adversarial attacks and need to be trained to withstand such attacks for many practical applications. Unfortunately, the high training costs associated with SNNs have resulted in a lack of analysis and optimization of deep SNNs against adversarial attacks.This study aims to address this gap by conducting a comprehensive analysis of the inherent robustness of low-latency SNNs against popular gradient-based attacks, specifically the fast gradient sign method (FGSM) and projected gradient descent (PGD). Building on the insights gained from this analysis, the researchers propose an SNN training algorithm that utilizes crafted input noise to enhance the model's resilience against these attacks. Importantly, this algorithm incurs no additional training time.To evaluate the effectiveness of their algorithm, extensive experiments are conducted using variants of VGG and ResNet architectures on the CIFAR-10 and CIFAR-100 datasets. The results show that the trained SNN models outperform both standard trained direct-input SNNs and inherently-robust SNNs trained on rate-coded inputs in terms of classification accuracy on attack-generated images. Specifically, the trained models achieve classification accuracy improvements of up to 13.7% and 10.1% on FGSM and PGD attack generated images, respectively, while maintaining negligible loss in clean image accuracy. Furthermore, the trained models exhibit significantly lower latency and computation energy, with up to 25× and ∼4.6× improvements compared to inherently-robust SNNs.To facilitate reproducibility, the code for the proposed algorithm has been made openly available on GitHub. Overall, this study highlights the potential of low-latency SNNs in mitigating adversarial attacks and provides a practical solution for enhancing their robustness without incurring additional training time.