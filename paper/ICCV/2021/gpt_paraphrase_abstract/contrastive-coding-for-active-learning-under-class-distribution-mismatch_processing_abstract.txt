The assumption underlying active learning (AL) is that labeled and unlabeled data come from the same class distribution. However, AL's effectiveness declines when there is a mismatch in class distribution, meaning that the unlabeled data includes many samples that are outside the class distribution of the labeled data. To address this issue, we propose a contrastive coding based AL framework called CCAL. Unlike existing AL methods that focus on selecting the most informative samples for annotation, CCAL uses contrastive learning to extract both semantic and distinctive features. These features are then combined in a query strategy to choose the most informative unlabeled samples that match the categories of the labeled data. We provide theoretical proof that CCAL has a tight upper bound on the AL error. In our experiments, we evaluate CCAL's performance on CIFAR10, CIFAR100, and an artificial cross-dataset consisting of five datasets. CCAL achieves significantly better performance with much lower annotation cost compared to existing methods. To the best of our knowledge, CCAL is the first AL approach specifically designed for class distribution mismatch.