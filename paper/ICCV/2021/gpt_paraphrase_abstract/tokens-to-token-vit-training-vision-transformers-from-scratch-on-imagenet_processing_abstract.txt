Recently, there has been interest in applying Transformers, a popular language modeling technique, to vision tasks such as image classification. One such model, called the Vision Transformer (ViT), divides each image into fixed-length tokens and utilizes multiple Transformer layers to capture global relationships for classification. However, when ViT is trained from scratch on a medium-sized dataset like ImageNet, it performs worse than Convolutional Neural Networks (CNNs). We have identified two reasons for this: firstly, the simplistic tokenization of input images fails to capture important local structures like edges and lines between neighboring pixels, leading to inefficient training. Secondly, the redundant attention backbone design of ViT limits its feature richness within fixed computation budgets and training samples. To address these limitations, we propose a new model called Tokens-To-Token Vision Transformer (T2T-ViT). T2T-ViT introduces a layer-wise Tokens-to-Token (T2T) transformation that progressively structures the image into tokens by aggregating neighboring tokens, enabling the modeling of local structures represented by surrounding tokens and reducing the token length. Additionally, T2T-ViT incorporates an efficient backbone with a deep-narrow structure inspired by CNN architecture design, following empirical studies. Notably, T2T-ViT achieves over 3.0% improvement compared to vanilla ViT when trained from scratch on ImageNet, while reducing the parameter count and computational requirements (MACs) by half. It also outperforms ResNets and achieves comparable performance to MobileNets when directly trained on ImageNet. For instance, T2T-ViT, with a similar size to ResNet50 (21.5M parameters), achieves 83.3% top1 accuracy in image resolution 384Ã—384 on ImageNet.