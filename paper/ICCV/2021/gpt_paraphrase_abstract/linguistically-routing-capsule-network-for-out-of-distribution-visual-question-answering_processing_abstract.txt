The topic of generalization on out-of-distribution (OOD) test data in visual question answering is important but not extensively studied. Current VQA models rely on biased correlations between data and labels, leading to a significant drop in performance when the training and test data have different distributions. Taking inspiration from human ability to recognize novel concepts and the capsule network's capability of representing part-whole hierarchies, we propose using capsules to represent parts and introduce "Linguistically Routing" to merge parts based on human-prior hierarchies. Our approach involves fusing visual features with a single question word as atomic parts and then reweighting the capsule connections using the "Linguistically Routing" technique. This routing process optimizes unary and binary potentials across multiple layers, resulting in a tree structure within the capsule network. We evaluate our method on various datasets and observe that it improves VQA models on OOD split while maintaining performance on in-domain test data.