Contemporary methods for domain generalization (DG) and multi-source unsupervised domain adaptation (UDA) typically involve collecting data from multiple domains for joint optimization. However, this centralized training approach raises privacy concerns and is not feasible when data is not shared across domains. This study introduces a new method called Collaborative Optimization and Aggregation (COPA) to address these challenges. COPA aims to optimize a generalized target model for decentralized DG and UDA, where data from different domains are non-shared and private. The proposed approach consists of a domain-invariant feature extractor and an ensemble of domain-specific classifiers. Through an iterative learning process, a local model is optimized for each domain, and then local feature extractors are centrally aggregated, along with domain-specific classifiers, to construct a generalized global model. Notably, this is achieved without sharing data across domains. To enhance the generalization of feature extractors, hybrid batch-instance normalization and collaboration of frozen classifiers are employed. Additionally, a prediction agreement mechanism is introduced to address local disparities in decentralized UDA and improve the aggregation of the central model. Extensive experiments on five benchmark datasets for DG and UDA demonstrate that COPA achieves comparable performance to state-of-the-art methods without the need for centralized data collection during model training.