This paper presents a novel approach to generating high-quality talking head videos that accurately match the input audio sequence. Unlike existing methods that rely on intermediate representations such as 2D landmarks or 3D face models, our method utilizes neural scene representation networks. We directly input the audio signal into a conditional implicit function, which generates a dynamic neural radiance field. By using volume rendering, we synthesize a high-fidelity talking-head video that corresponds to the audio signal. In addition to generating the head region, our framework also generates the upper body using two separate neural radiance fields. Experimental results show that our approach produces natural and realistic results, and it allows for flexible adjustments of audio signals, viewing directions, and background images. The code for our framework is available at https://github.com/YudongGuo/AD-NeRF.