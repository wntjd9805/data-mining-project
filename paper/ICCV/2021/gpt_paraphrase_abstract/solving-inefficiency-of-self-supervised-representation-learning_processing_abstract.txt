Self-supervised learning, particularly contrastive learning, has become highly popular due to its potential in unsupervised learning of discriminative representations. However, existing contrastive learning methods suffer from low learning efficiency, requiring significantly more training epochs compared to supervised learning for similar recognition accuracy. This paper identifies two contradictory issues in contrastive learning: under-clustering and over-clustering problems, which hinder learning efficiency. Under-clustering refers to the model's inability to effectively distinguish dissimilarities between samples from different classes due to insufficient negative sample pairs. Over-clustering occurs when the model struggles to learn features from excessive negative sample pairs, resulting in samples from the same class being assigned to different clusters. To address these problems simultaneously, a novel self-supervised learning framework is proposed, utilizing a truncated triplet loss. This loss function maximizes the relative distance between positive and negative pairs to overcome under-clustering. Moreover, a negative sample deputy is selected from all negative samples to prevent over-clustering, guided by the Bernoulli Distribution model. The proposed framework is extensively evaluated on large-scale benchmarks such as ImageNet, SYSU-30k, and COCO, demonstrating its superiority in terms of learning efficiency compared to the latest state-of-the-art methods.