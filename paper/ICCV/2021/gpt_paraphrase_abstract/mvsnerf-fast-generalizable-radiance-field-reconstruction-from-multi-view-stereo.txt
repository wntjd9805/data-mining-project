We introduce MVSNeRF, an innovative neural rendering method for efficiently reconstructing neural radiance fields for view synthesis. Unlike previous approaches that rely on optimizing densely captured images on a per-scene basis, our method utilizes a generic deep neural network that can reconstruct radiance fields using only three nearby input views through fast network inference. To achieve this, we combine plane-swept cost volumes, commonly used in multi-view stereo, for scene reasoning with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset and evaluate its effectiveness and generalizability on three different datasets. Our approach surpasses concurrent methods in generating realistic view synthesis results for various scenes, including indoor scenes that differ from our training data. Additionally, if dense images are available, our estimated radiance field representation can be easily fine-tuned, resulting in faster per-scene reconstruction with higher rendering quality and significantly reduced optimization time compared to NeRF. This research was conducted during Anpei Chen's remote internship with UCSD.