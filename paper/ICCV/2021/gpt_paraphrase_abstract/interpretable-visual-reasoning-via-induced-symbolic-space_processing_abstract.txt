The aim of this study is to investigate the process of concept induction in visual reasoning, specifically in identifying concepts and their hierarchical relationships through question-answer pairs associated with images. The goal is to develop an interpretable model by working with the induced symbolic concept space. To accomplish this, a new framework called object-centric compositional attention model (OCCAM) is designed to perform visual reasoning tasks using object-level visual features. Additionally, a method is proposed to induce concepts of objects and relations by analyzing the attention patterns between objects' visual features and question words. Furthermore, interpretability is enhanced by applying OCCAM to the objects represented in the induced symbolic concept space. Experimental results on the CLEVR and GQA datasets demonstrate the effectiveness of OCCAM, as it achieves state-of-the-art performance without the need for human-annotated functional programs. The induced concepts are shown to be accurate and sufficient, as OCCAM performs equally well on objects represented in either visual features or the induced symbolic concept space. The output of the answer format focuses on abstraction.