We introduce Worldsheet, a technique for generating new views using only a single RGB image as input. Our key insight is that by fitting a flat mesh sheet onto the input image, aligned with the estimated depth, we can capture enough underlying geometry to generate realistic unseen views with significant changes in viewpoint. To implement this, we propose a unique differentiable texture sampler that allows us to texture and render the wrapped mesh sheet into an image from a desired viewpoint, while being trainable end-to-end without 3D supervision and requiring only a single image at test time. We also investigate an extension by stacking multiple layers of Worldsheets to handle occlusions better. Worldsheet consistently outperforms previous state-of-the-art methods in synthesizing views from a single image across various datasets. Furthermore, this simple approach produces impressive results on a wide range of high-resolution real-world images, transforming them into interactive 3D pop-ups. Our video results and code can be found at https://worldsheet.github.io.