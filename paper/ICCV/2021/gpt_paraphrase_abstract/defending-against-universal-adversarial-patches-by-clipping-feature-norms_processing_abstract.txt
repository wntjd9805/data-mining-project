Physical-world adversarial attacks using universal adversarial patches have demonstrated the ability to deceive deep convolutional neural networks (CNNs). This exposes the vulnerability of real-world visual classification systems that rely on CNNs. This study empirically uncovers that universal adversarial patches often result in deep feature vectors with exceptionally large norms in widely used CNNs. Building on this insight, we propose a simple yet effective defense mechanism called feature norm clipping (FNC). FNC is a differentiable module that can be easily incorporated into various CNN architectures to selectively suppress the generation of deep feature vectors with large norms. Notably, FNC does not require any trainable parameters and incurs minimal computational overhead. Experimental results across multiple datasets validate the effectiveness of FNC in enhancing the robustness of different CNN models against white-box universal patch attacks, while maintaining high recognition accuracy for clean samples.