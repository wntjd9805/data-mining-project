Model quantization is an important technique for efficient inference with Deep Neural Networks (DNN). However, existing methods either have inefficient data encoding or require time-consuming quantization aware training. To address these issues, we propose Adaptive Floating-Point (AFP) as a variant of the standard IEEE-754 floating-point format. AFP allows for flexible configuration of exponent and mantissa segments, significantly enhancing model compression without sacrificing accuracy or requiring re-training. Additionally, our AFP eliminates the computationally intensive de-quantization step in dynamic quantization techniques used by popular machine learning frameworks. We have developed a framework to automatically optimize and choose the appropriate AFP configuration for each layer, maximizing compression effectiveness. Experimental results show that AFP-encoded ResNet-50/MobileNet-v2 models only experience a slight decrease in accuracy (0.04/0.6%) compared to their full-precision counterparts. Furthermore, our approach outperforms state-of-the-art methods by achieving 1.1% higher accuracy using the same bit-width, while reducing energy consumption by 11.2 times during inference. The code for our approach is available at: [Github link].