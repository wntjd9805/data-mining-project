This paper introduces TransVG, a transformer-based framework for visual grounding, which aims to link a language query to its corresponding region in an image. Existing methods for this task often rely on complex modules with manually-designed mechanisms for query reasoning and multi-modal fusion. However, these mechanisms, such as query decomposition and image scene graphs, can lead to overfitting on specific datasets and limit the interaction between visual and linguistic contexts. To overcome this limitation, the authors propose using transformers to establish multi-modal correspondence. They empirically demonstrate that a simple stack of transformer encoder layers can achieve higher performance compared to complex fusion modules. Additionally, the authors reformulate visual grounding as a direct coordinates regression problem, avoiding the need to make predictions from a set of candidates. Extensive experiments on five popular datasets show that TransVG achieves state-of-the-art results. The authors also provide a benchmark for transformer-based visual grounding and make the code publicly available.