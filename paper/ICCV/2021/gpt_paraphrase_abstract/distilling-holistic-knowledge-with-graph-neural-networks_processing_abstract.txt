Knowledge Distillation (KD) is a technique that aims to transfer knowledge from a larger, well-optimized teacher network to a smaller, learnable student network. Previous KD methods have focused on two types of knowledge: individual knowledge and relational knowledge. However, these two types of knowledge are typically treated independently, disregarding the inherent correlations between them. It is crucial for effective student network learning to integrate both individual and relational knowledge while preserving their inherent correlation. In this paper, we propose a novel approach to distill holistic knowledge based on an attributed graph constructed among instances. The holistic knowledge is represented as a unified graph-based embedding, achieved by aggregating individual knowledge from relational neighborhood samples using graph neural networks. The student network is then trained to learn from this holistic knowledge through a contrastive learning approach. We conducted extensive experiments and ablation studies on benchmark datasets to evaluate the effectiveness of our proposed method. The results demonstrate the effectiveness of our approach. The code for our method has been published at the following GitHub repository: https://github.com/wyc-ruiker/HKD.