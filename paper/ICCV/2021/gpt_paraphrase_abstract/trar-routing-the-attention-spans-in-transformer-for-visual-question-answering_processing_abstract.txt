Transformer and its variants have become popular choices for vision-and-language tasks due to their superior global dependency modeling ability. However, tasks like Visual Question Answering (VQA) and Referring Expression Comprehension (REC) require multi-modal prediction that involves visual information at different levels. As a result, dynamically scheduling global and local dependency modeling in Transformer has become an important issue. In this paper, we propose TRAnsformer Routing (TRAR), an example-dependent routing scheme, to address this problem. TRAR equips each visual Transformer layer with a routing module that has different attention spans. This allows the model to dynamically select the appropriate attentions based on the output of the previous inference step, creating an optimal routing path for each example. Importantly, TRAR is designed to minimize additional computation and memory overhead. We evaluate TRAR on five benchmark datasets for VQA and REC and demonstrate its superior performance compared to standard Transformers and other state-of-the-art methods. The answer format outputs only the abstraction.