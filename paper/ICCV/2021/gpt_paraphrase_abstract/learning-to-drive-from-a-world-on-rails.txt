We develop a model-based approach to learn an interactive vision-based driving policy using pre-recorded driving logs. The driving policy is supervised by a forward model of the world that predicts the outcome of potential driving trajectories. To facilitate learning from the logs, we assume a fixed environment where neither the agent nor its actions affect the world. This simplifies the learning process by separating the dynamics into a non-reactive world model and a compact forward model of the ego-vehicle. Our approach calculates action-values for each training trajectory through tabular dynamic-programming evaluation of the Bellman equations, which in turn supervise the final vision-based driving policy. Despite the fixed environment assumption, our driving policy performs well in dynamic and reactive worlds, surpassing imitation learning, model-based, and model-free reinforcement learning methods on the challenging CARLA NoCrash benchmark. Moreover, our approach is significantly more efficient than state-of-the-art model-free reinforcement learning techniques on navigational tasks in the ProcGen benchmark.