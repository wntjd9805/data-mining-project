Recently, the vulnerability of machine learning models to backdoor attacks has been demonstrated, particularly in black-box models like deep neural networks. These attacks involve poisoning a third-party model so that it behaves maliciously when exposed to specific trigger patterns. However, current backdoor attack methods rely on manually defining the trigger injection function, which involves adding a small patch of pixels or deforming an image. This approach has limitations in terms of attack success rate and stealthiness. 

To address these issues, we propose a new backdoor attack framework called LIRA. This framework simultaneously learns the optimal trigger injection function and poisons the model in a stealthy manner. We formulate this as a non-convex, constrained optimization problem. The trigger generator function in this optimization framework learns to manipulate the input with imperceptible noise, preserving the model's performance on clean data while maximizing the attack success rate on poisoned data. 

We solve this challenging optimization problem using an efficient, two-stage stochastic optimization procedure. Our proposed attack framework achieves a 100% success rate on several benchmark datasets, including MNIST, CIFAR10, GT-SRB, and T-ImageNet. Additionally, it is capable of bypassing existing backdoor defense methods and human inspection.