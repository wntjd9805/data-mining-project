Deep neural networks have been successful in visual recognition tasks, but their deployment is often limited by computational resources. Model pruning, which reduces the size of the network while maintaining its performance, has gained attention. Previous methods of finding the optimal sub-network were complex and lacked interpretability. In this paper, we propose a new approach that formulates sub-networks as samples from a multivariate Bernoulli distribution and uses a continuous approximation. We introduce a flexible search scheme that alternates between exploration and estimation. In the exploration step, we use stochastic gradient Hamiltonian Monte Carlo to generate sub-networks efficiently. In the estimation step, we deduce the sub-network sampler to a near-optimal point to generate high-quality sub-networks. By combining exploration and estimation, our approach avoids getting stuck in local minimums. Experimental results on CIFAR-10 and ImageNet datasets demonstrate that our method achieves state-of-the-art performance in pruning popular CNNs.