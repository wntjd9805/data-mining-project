This study addresses the challenge of estimating camera motion and 3D scene structure using a monocular vision system. Traditional methods rely on the assumption of scene rigidity, but this assumption is violated in dynamic environments, leading to ambiguity between camera motion and object motion. To tackle this problem, the researchers propose a self-supervised learning framework for estimating 3D object motion from monocular videos. The framework consists of two main contributions. First, a two-stage projection pipeline is introduced to separate camera ego-motion from object motions using a dynamics attention module (DAM). The motion of the camera and objects is estimated in the first and second warping stages, respectively, controlled by the attention module through a shared motion encoder. Second, an object motion field estimation method called contrastive sample consensus (CSAC) is proposed, which leverages weak semantic prior information (bounding box from an object detector) and geometric constraints (rigid body motion model for each object). Experimental results on various datasets demonstrate the effectiveness of the proposed approach, surpassing state-of-the-art algorithms in tasks such as self-supervised monocular depth estimation, object motion segmentation, monocular scene flow estimation, and visual odometry.