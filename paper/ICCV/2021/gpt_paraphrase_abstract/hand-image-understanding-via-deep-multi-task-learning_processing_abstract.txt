Analyzing and understanding hand information in multimedia materials is crucial for many real-world applications and is an ongoing focus of research. While there have been efforts to recover hand information from single images, these approaches typically address a single task, such as hand mask segmentation, hand pose estimation, or hand mesh reconstruction, and struggle in challenging scenarios. To enhance the performance of these tasks, we propose a novel framework called Hand Image Understanding (HIU). HIU aims to extract comprehensive hand object information from a single RGB image by considering the relationships between these tasks. Our approach utilizes a cascaded multi-task learning (MTL) backbone, which estimates 2D heat maps, learns segmentation masks, and generates intermediate 3D information encoding. It further employs a coarse-to-fine learning paradigm and a self-supervised learning strategy. Through qualitative experiments, we demonstrate that our approach can generate reasonable mesh representations even in challenging situations. Moreover, our method outperforms state-of-the-art approaches on widely-used datasets, as evidenced by diverse evaluation metrics. The implementation of our approach can be found at https://github.com/MandyMo/HIU-DMTL.