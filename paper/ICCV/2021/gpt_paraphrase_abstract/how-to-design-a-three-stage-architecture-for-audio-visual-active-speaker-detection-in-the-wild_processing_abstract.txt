To achieve successful active speaker detection, a three-stage pipeline is necessary. This pipeline includes audio-visual encoding for all speakers in the clip, modeling the inter-speaker relations between the reference speaker and background speakers in each frame, and temporal modeling for the reference speaker. Each stage of this pipeline plays a crucial role in determining the overall performance of the architecture. Through controlled experiments, this study provides practical guidelines for audio-visual active speaker detection. Additionally, a new architecture called ASDNet is introduced, which surpasses the previous state-of-the-art on the AVA-ActiveSpeaker dataset with an impressive mAP of 93.5%, outperforming the second best model by a significant margin of 4.7%. The code and pretrained models for ASDNet are publicly available.