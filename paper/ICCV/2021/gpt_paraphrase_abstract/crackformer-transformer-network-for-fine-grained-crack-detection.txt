Crack detection in computer vision applications is a complex task due to various challenges such as intensity in-homogeneity, topology complexity, low contrast, and noisy background. The accuracy of crack detection can be significantly affected by the detection of fine-grained cracks. To address this issue, we propose a network called CrackTransformer (CrackFormer) that focuses on detecting fine-grained cracks. The CrackFormer utilizes attention modules in an encoder-decoder architecture similar to SegNet. These attention modules incorporate self-attention with 1x1 convolutional kernels to extract contextual information efficiently across feature-channels. Additionally, the CrackFormer incorporates positional embedding to capture contextual information with a large receptive field for long-range interactions. It also introduces scaling-attention modules to suppress non-semantic features and enhance semantic ones by combining outputs from the encoder and decoder blocks. We train and evaluate the CrackFormer on three classical crack datasets, and the results demonstrate its superiority over state-of-the-art methods. The CrackFormer achieves optimal dataset scale (ODS) values of 0.871, 0.877, and 0.881 on the three datasets, respectively.