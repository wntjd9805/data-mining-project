Deep networks have been successful in semantic segmentation but require training with a large amount of data in a single shot. However, in continual learning settings where new classes are learned incrementally and previous training data is unavailable, there is a challenge known as catastrophic forgetting. Existing approaches struggle when multiple incremental steps are performed or when there is a distribution shift in the background class. To address these issues, we propose a method called RECALL. RECALL tackles the problem by recreating the unavailable data for the old classes and implementing a content inpainting scheme for the background class. We suggest two sources for replay data. The first involves using a generative adversarial network to sample from the class space of past learning steps. The second source relies on web-crawled data to retrieve images that contain examples of old classes from online databases. Importantly, no samples from past steps are stored, eliminating privacy concerns. During the incremental steps, the replay data is blended with new samples. Our approach, RECALL, surpasses state-of-the-art methods in performance.