Pretraining on large labeled datasets is crucial for achieving good performance in various computer vision tasks, such as image recognition and video understanding. However, this approach is not commonly used for 3D recognition tasks because of the scarcity of annotated datasets, as labeling 3D data is time-consuming. Recent studies have demonstrated the effectiveness of self-supervised learning in pretraining 3D models, but it typically requires multi-view data and point correspondences.In this paper, we propose a straightforward self-supervised pretraining method that can be applied to single-view depth scans obtained from different sensors, without the need for 3D registration and point correspondences. We pretrain standard model architectures based on point clouds and voxels and demonstrate that joint pretraining further enhances performance. We evaluate our pretrained models on nine benchmarks, including object detection, semantic segmentation, and object classification, and achieve state-of-the-art results. Notably, we achieve a new state-of-the-art for object detection on ScanNet (69.0% mAP) and SUNRGBD (63.5% mAP). Our pretrained models are efficient in terms of label usage and significantly improve performance for classes with limited examples.