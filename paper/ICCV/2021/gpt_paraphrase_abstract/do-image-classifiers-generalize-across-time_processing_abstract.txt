Vision models often struggle with flickering when applied to videos, where they correctly identify objects in some frames but fail on similar frames. This study focuses on analyzing the robustness of image classifiers to these temporal perturbations in videos. To conduct this analysis, the researchers create two new datasets: ImageNet-Vid-Robust and YTBB-Robust, which consist of 57,897 images grouped into 3,139 sets of perceptually similar images. These datasets are derived from ImageNet-Vid and Youtube-BB, respectively, and are meticulously re-annotated by human experts to ensure accurate image similarity. The study evaluates various classifiers that are pre-trained on ImageNet and finds that they experience a median drop in classification accuracy of 16 and 10 points on the two datasets, respectively. Additionally, the researchers assess three detection models and discover that natural perturbations not only lead to classification errors but also localization errors, resulting in a median drop in detection mAP of 14 points. This analysis demonstrates that naturally occurring perturbations in videos present a significant and realistic challenge for deploying convolutional neural networks in environments that require both reliable and low-latency predictions.