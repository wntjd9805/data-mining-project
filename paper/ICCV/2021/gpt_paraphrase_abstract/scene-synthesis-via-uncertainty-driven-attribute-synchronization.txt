Developing deep neural networks to generate 3D scenes is a challenging problem in the field of neural synthesis. This has important applications in architectural CAD, computer graphics, and creating virtual robot training environments. The complexity arises from the fact that 3D scenes exhibit a wide range of patterns, including both continuous and discrete features. Continuous patterns involve object sizes and relative poses between shapes, while discrete patterns involve the occurrence and co-occurrence of objects with symmetrical relationships. To address this challenge, this paper proposes a novel approach to neural scene synthesis. Our method combines the strengths of both neural network-based and conventional scene synthesis approaches. We leverage the parametric prior distributions learned from training data, which provide information about uncertainties in object attributes and relative attributes, to regulate the outputs of feed-forward neural models. By doing so, we can generate more realistic and diverse 3D scenes.Furthermore, unlike traditional methods that only predict scene layouts, our approach predicts an over-complete set of attributes. This allows us to exploit the underlying consistency constraints among these attributes to eliminate infeasible predictions. By pruning these predictions, we can generate more accurate and coherent 3D scenes.Through experimental evaluation, our approach has demonstrated superior performance compared to existing methods. The generated 3D scenes faithfully interpolate the training data, while preserving both continuous and discrete feature patterns. This advancement in neural scene synthesis has great potential for various applications in the fields of architecture, computer graphics, and virtual robot training environments.