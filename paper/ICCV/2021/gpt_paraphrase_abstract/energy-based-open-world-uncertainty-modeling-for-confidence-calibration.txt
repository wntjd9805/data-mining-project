Confidence calibration is crucial for ensuring the reliability of machine learning systems. However, deep neural networks often produce overconfident predictions that do not accurately reflect the true likelihood of classification accuracy. This is primarily due to the closed-world nature of softmax, where models are trained to classify input into pre-defined categories with high probability. To address this issue, we propose a new formulation called K+1-way softmax, which incorporates an extra dimension to model open-world uncertainty. By unifying the learning of the original classification task and the extra dimension, we introduce a novel energy-based objective function that forces the extra dimension to capture the marginal data distribution. Our experiments demonstrate that our approach, called Energy-based Open-World Softmax (EOW-Softmax), outperforms existing methods in improving confidence calibration.