Pedestrian behavior prediction is a significant challenge in intelligent driving systems. Pedestrians exhibit complex behaviors influenced by various factors. To tackle this issue, we introduce BiPed, a multitask learning framework that predicts both trajectories and actions of pedestrians using multi-modal data. Our approach leverages: 1) a bifold encoding technique that processes different data modalities independently to generate their own representations and jointly produces a representation for all modalities using shared parameters; 2) an innovative interaction modeling method that utilizes categorical semantic parsing of scenes to capture interactions between pedestrians and their surroundings; and 3) a bifold prediction mechanism that combines independent and shared decoding of multimodal representations. By employing public benchmark datasets for pedestrian behavior (PIE and JAAD), we demonstrate the advantages of our proposed method, achieving state-of-the-art performance with improvements of up to 22% in trajectory prediction and 9% in action prediction. Additionally, through extensive ablation studies, we investigate the contributions of our reasoning techniques.