Human motion synthesis is a crucial problem in areas such as graphics, gaming, and robotics simulation. Currently, existing methods rely on expensive motion capture data for training. In contrast, we propose a framework that directly trains generative models of physically realistic human motion using easily accessible monocular RGB videos. Our approach involves a unique optimization formulation that corrects imperfect pose estimations from images by incorporating physics constraints and differentiable contact reasoning. This optimization not only produces improved 3D poses and motions but also provides corresponding contact forces. Our results demonstrate that our physically-corrected motions outperform previous methods in pose estimation. Furthermore, we utilize these corrected motions to train a generative model for synthesizing future motion. Through qualitative and quantitative evaluations on the Human3.6m dataset, we show significant improvements in motion estimation, synthesis quality, and physical plausibility compared to previous kinematic and physics-based approaches. By enabling motion synthesis learning from video, our method opens up possibilities for large-scale, realistic, and diverse motion synthesis.