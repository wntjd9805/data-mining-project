Gradient-based meta-learning typically involves training task-specific models using an inner loop and then updating the meta-model by backpropagating meta-gradients through the loop. However, existing methods either limit the number of inner steps or approximate the meta-updates when the meta-model and task models occupy the same space to avoid the complexity of high-order gradients. In order to address this limitation and enable more general meta-learning, we propose a teacher-student strategy. This strategy involves using a student network to explore the search space of task-specific models and then having a teacher network "leap" towards the regions explored by the student. The teacher not only produces a high-quality model but also defines a lightweight computational graph for the meta-gradients. Our approach is applicable to various meta-learning algorithms and performs well in tasks such as few-shot learning, long-tailed object recognition, and adversarial blackbox attack.