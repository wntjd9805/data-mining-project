The goal of visible infrared person re-identification (VI-REID) is to match pedestrian images captured by visible and infrared cameras during the day and night, respectively. However, the significant differences between these two modalities hinder the performance of VI-REID. Current methods primarily focus on learning cross-modality shareable representations through identity classification. Nevertheless, due to the diverse image styles produced by different spectrum cameras, the discriminability of these feature representations is compromised. To address this issue, this study proposes a novel syncretic modality collaborative learning (SMCL) model that explores the correlation between the visible and infrared modalities. The SMCL model automatically constructs a new modality that incorporates features from heterogeneous images to generate modality-invariant representations. The model integrates challenge enhanced homogeneity learning (CEHL) and auxiliary distributional similarity learning (ADSL) to project heterogeneous features onto a unified space and increase the disparity between different classes, thereby enhancing the discriminative power. Extensive experiments conducted on two cross-modality benchmarks demonstrate the effectiveness and superiority of the proposed method. Notably, on the SYSU-MM01 dataset, the SMCL model achieves a rank-1 accuracy of 67.39% and a mean average precision (mAP) of 61.78%, surpassing existing state-of-the-art approaches by a significant margin.