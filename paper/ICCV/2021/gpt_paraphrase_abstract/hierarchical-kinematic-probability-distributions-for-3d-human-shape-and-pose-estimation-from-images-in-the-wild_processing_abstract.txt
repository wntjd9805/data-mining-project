This paper focuses on the challenge of estimating the 3D shape and pose of a human body from a 2D RGB image. This problem is often difficult to solve because there can be multiple plausible 3D body configurations that match the visual evidence in the image, especially when the subject is partially obscured. Therefore, instead of producing a single 3D reconstruction, it is more desirable to estimate a distribution of possible 3D body shapes and poses based on the input image. To achieve this, we train a deep neural network that can estimate a hierarchical matrix-Fisher distribution for the relative 3D joint rotation matrices (body pose) and a Gaussian distribution for the SMPL body shape parameters. By utilizing the kinematic tree structure of the human body, our method captures the complex relationships between body joints. Additionally, we implement a differentiable rejection sampler to ensure that the predicted shape and pose distributions align with the visual evidence in the input image. This is achieved by comparing the ground-truth 2D joint coordinates with samples drawn from the predicted distributions and projecting them onto the image plane.We demonstrate that our approach performs competitively with state-of-the-art methods in terms of 3D shape and pose metrics on the SSP-3D and 3DPW datasets. Furthermore, our method provides a structured probability distribution over 3D body shape and pose, enabling us to quantify prediction uncertainty and generate multiple plausible 3D reconstructions that explain a given input image.