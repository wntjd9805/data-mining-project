We present a new approach to visual place recognition (VPR) called Attentional Pyramid Pooling of Salient Visual Residuals (APPSVR). Our method focuses on identifying task-relevant visual cues and incorporating them into discriminative representations. To achieve this, we utilize three types of attention modules: (1) a semantic-reinforced local weighting scheme to refine local features and suppress task-irrelevant dimensions, (2) an attentional pyramid structure to adaptively encode regional features based on their spatial saliency, and (3) a parametric normalization technique to adjust the contribution of visual clusters to image descriptor generation according to their importance to the task. Experimental results demonstrate that APPSVR outperforms existing techniques and achieves state-of-the-art performance on VPR benchmark datasets. Furthermore, the visualization of the learned saliency map, obtained in a weakly supervised manner, aligns closely with human cognition.