The issue of Federated Learning (FL) is addressed in this study, where decentralized computational nodes collaborate to train a centralized machine learning model without sharing their local data samples. The decentralized training process presents challenges such as imbalanced or differing data distributions among the local models and difficulties in merging them into a central model. Existing methods tackle these challenges by sharing local parameters or fusing models through online distillation, but this approach requires multiple rounds of inter-node communication, leading to high bandwidth consumption and privacy concerns. To overcome these problems, the authors propose a new distillation-based FL framework that ensures privacy and consumes fewer network communication resources compared to current methods. Their framework only engages in inter-node communication using publicly available and approved datasets, providing explicit privacy control to the user. To distill knowledge from the local models, the proposed framework employs an ensemble distillation algorithm that incorporates both final prediction and model attention. This algorithm takes into account the diversity among local nodes while seeking consensus among them, resulting in a comprehensive technique for knowledge distillation. The authors validate the effectiveness of their FL framework through extensive experiments, achieving state-of-the-art results in classification and segmentation tasks on natural and medical images.