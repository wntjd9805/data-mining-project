In the field of low-light video enhancement, previous research has mainly focused on training models using paired static images or videos. In this study, we have created a new dataset by implementing a novel approach. Our dataset contains high-quality video pairs that are spatially-aligned and captured from dynamic scenes under both low- and normal-light conditions. To achieve this, we developed a mechatronic system that enables precise control of the dynamics during the video capture process. We also aligned the video pairs, both spatially and temporally, by identifying the system's uniform motion stage. In addition to the dataset, we propose an end-to-end framework that incorporates a self-supervised strategy to reduce noise and enhance illumination based on the Retinex theory. We conducted extensive experiments using various metrics and a large-scale user study, which demonstrated the value of our dataset and the effectiveness of our method. The dataset and code for our framework are publicly available at https://github.com/dvlab-research/SDSD.