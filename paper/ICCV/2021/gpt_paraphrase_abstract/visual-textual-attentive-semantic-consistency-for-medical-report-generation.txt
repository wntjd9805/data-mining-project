The generation of automatic reports on medical radiographs has become a topic of interest. However, accurately identifying diseases and predicting their sizes, locations, and other medical descriptions is a challenging task that is crucial for producing high-quality reports. While previous methods have focused on creating readable reports, the accurate detection and description of findings that match the query X-Ray has not been successfully addressed. In this study, we propose a multi-modality semantic attention model that integrates visual features, predicted key finding embeddings, and clinical features to progressively decode reports with visual-textual semantic consistency. Our approach involves extracting multi-modality features and attending to them with hidden states from the sentence decoder to encode enriched context vectors for improved report decoding. These modalities include regional visual features, semantic word embeddings of the top-K findings, and clinical features. The progressive report decoder consists of a sentence decoder and a word decoder, and we introduce image-sentence matching and description accuracy losses to ensure visual-textual semantic consistency. Extensive experiments conducted on the MIMIC-CXR and IUX-Ray datasets demonstrate that our model consistently outperforms state-of-the-art methods.