Traditional methods for learning about 3D object categories have primarily relied on synthetic datasets due to the lack of real-world 3D-annotated data. Our main objective is to advance this field by creating a substantial real-world dataset that is comparable in size to existing synthetic datasets. The main contribution of our work is the creation of a large-scale dataset called Common Objects in 3D, which consists of real multi-view images of object categories annotated with camera poses and accurate 3D point clouds. This dataset contains 1.5 million frames from nearly 19,000 videos, covering objects from 50 MS-COCO categories, making it significantly larger than other datasets in terms of both categories and objects. We utilize this dataset to conduct one of the first large-scale evaluations of new-view synthesis and category-centric 3D reconstruction methods in real-world scenarios. Additionally, we introduce NerFormer, a novel neural rendering technique that utilizes the powerful Transformer model to reconstruct an object using only a few of its views.