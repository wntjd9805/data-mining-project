Virtual 3D try-on has significant commercial value for online shopping, but current methods rely on 3D shapes and templates, limiting their practical use. 2D virtual try-on approaches offer faster manipulation of clothed humans but lack realistic 3D representation. This paper proposes a novel Monocular-to-3D Virtual Try-On Network (M3D-VTON) that combines the strengths of both 2D and 3D approaches. By efficiently integrating 2D information and learning a mapping to lift the representation to 3D, the network reconstructs a 3D try-on mesh using only the target clothing and a person image as inputs. The M3D-VTON consists of three modules: 1) The Monocular Prediction Module (MPM) estimates a full-body depth map and aligns the 2D clothes with the person through a two-stage warping procedure; 2) The Depth Refinement Module (DRM) enhances the initial body depth to capture more detailed pleats and facial characteristics; 3) The Texture Fusion Module (TFM) refines the results by merging the warped clothing with the non-target body part. A high-quality synthesized dataset is also created, with each person image associated with a front and back depth map. Extensive experiments demonstrate that the M3D-VTON can manipulate and reconstruct the 3D human body wearing the given clothing with convincing details and is more efficient than other 3D approaches.