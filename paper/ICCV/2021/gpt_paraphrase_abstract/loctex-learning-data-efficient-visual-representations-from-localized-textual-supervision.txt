This paper introduces LocTex, a method for reducing the annotation effort required for computer vision tasks such as object detection and semantic/instance segmentation. Instead of relying on costly annotations, LocTex utilizes low-cost localized textual annotations, such as captions and synchronized mouse-over gestures. The authors propose a contrastive pre-training framework that learns visual features by training on images and captions. They also supervise the cross-modal attention map using rendered mouse traces to provide coarse localization signals. The learned features capture both rich semantics from free-form captions and accurate localization from mouse traces, making them effective for various downstream vision tasks. Compared to ImageNet supervised pre-training, LocTex can reduce the size of the pre-training dataset by 10× or the target dataset by 2× while achieving comparable or even improved performance on COCO instance segmentation. When given the same amount of annotations, LocTex achieves approximately 4% higher accuracy than the previous state-of-the-art "vision+language" pre-training approach on PASCAL VOC image classification.