This paper addresses the challenge of effectively integrating appearance and motion information in flow-based zero-shot video object segmentation. To tackle this issue, we propose a novel network called Attentive Multi-Modality Collaboration Network (AMC-Net). AMC-Net aims to fuse information from multiple modalities and enhance their collaboration in two stages. First, we introduce a Multi-Modality Co-Attention Gate (MCG) that balances the contributions of different modalities and suppresses redundant and misleading information. Then, we propose a Motion Correction Module (MCM) that incorporates a visual-motion attention mechanism to emphasize foreground object features by considering the spatio-temporal correspondence between appearance and motion cues. Our experiments on three challenging benchmark datasets demonstrate that our network outperforms existing state-of-the-art methods, even with limited training data. The code for our proposed network is publicly available at https://github.com/isyangshu/AMC-Net.