Recent advancements in Out-of-Distribution (OoD) generalization have shown that deep learning models are robust against changes in data distribution. However, existing research primarily focuses on OoD algorithms like invariant risk minimization, domain generalization, or stable learning, without considering how the architecture of deep models can impact OoD generalization, leading to suboptimal performance. Neural Architecture Search (NAS) methods search for the best architecture based on its performance on training data, which may result in poor generalization for OoD tasks.  To address this gap, we propose a robust Neural Architecture Search for OoD generalization (NAS-OoD) approach. NAS-OoD optimizes the architecture by considering its performance on generated OoD data using gradient descent. Specifically, we train a data generator to synthesize OoD data by maximizing losses computed by different neural architectures. The goal of architecture search is to find the optimal architecture parameters that minimize losses on the synthetic OoD data. Both the data generator and the neural architecture are jointly optimized in an end-to-end manner, allowing for the discovery of robust architectures that generalize well across different distribution shifts.  Extensive experiments demonstrate that NAS-OoD outperforms existing OoD generalization methods on various benchmarks. Notably, deep models optimized with NAS-OoD achieve superior performance while having significantly fewer parameters. Moreover, on a real industry dataset, NAS-OoD reduces the error rate by over 70% compared to the state-of-the-art method, showcasing its practicality for real-world applications.  Figure 1 illustrates the superior performance of NAS-OoD compared to existing OoD generalization baselines in terms of test accuracy and network parameter numbers. Points in the upper left quadrant indicate higher test accuracy and lower parameter numbers, indicating better performance.   In conclusion, our NAS-OoD approach addresses the limitation of existing research by considering the influence of deep model architectures on OoD generalization. By optimizing both the architecture and data generation process, NAS-OoD discovers robust architectures that exhibit superior performance across various distribution shifts. The experimental results highlight the effectiveness of NAS-OoD in achieving improved OoD generalization with reduced model complexity.