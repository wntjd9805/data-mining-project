DCNN based self-supervised depth estimation methods have shown impressive outcomes. However, these methods have limitations in handling all-day images due to the significant differences in illumination between day and night images. To overcome this, we propose a domain-separated network for self-supervised depth estimation of all-day images. Our approach partitions the information of day and night image pairs into two sub-spaces: private and invariant domains. The private domain contains unique information about illumination, while the invariant domain contains shared information such as texture. By using a domain-separated network, we ensure that the day and night images contain the same information. We learn private and invariant feature extractors using orthogonality and similarity loss to alleviate the domain gap and improve the accuracy of depth maps. Additionally, we utilize reconstruction and photometric losses to estimate complementary information and depth maps effectively. Experimental results on the Oxford RobotCar dataset demonstrate that our approach achieves state-of-the-art results in depth estimation for all-day images. This confirms the superiority of our proposed approach. The code and data split for our approach are available at https://github.com/LINA-lln/ADDS-DepthNet.