We propose a Hierarchical Operation-adaptive Predictor (HOP) for neural architecture search (NAS). Current graph-based predictors lack the ability to differentiate the significance of different operations when aggregating information. To address this, HOP incorporates an operation-adaptive attention module (OAM) that learns the relative importance of operations in cell architectures during aggregation. Furthermore, HOP includes a cell-hierarchical gated module (CGM) that enhances the obtained topological knowledge by integrating cell information from each iteration of OAM. Our experimental results, compared to existing predictors, demonstrate the effectiveness of HOP in NAS.