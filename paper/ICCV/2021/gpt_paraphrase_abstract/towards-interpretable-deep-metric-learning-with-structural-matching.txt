Understanding how neural networks distinguish between two images is crucial for developing reliable intelligent systems for high-stakes visual applications like surveillance and access control. However, most existing deep metric learning methods compare feature vectors to match images, which overlooks the spatial structure of the images and lacks interpretability. In this study, we propose a deep interpretable metric learning (DIML) method to facilitate more transparent embedding learning. Unlike traditional metric learning methods that rely on feature vector comparison, our approach introduces a structural matching strategy that explicitly aligns the spatial embeddings by calculating an optimal matching flow between the feature maps of the two images. By doing so, our method enables deep models to learn metrics in a more human-friendly manner, decomposing the similarity between two images into several part-wise similarities and their contributions to the overall similarity. Our method is not restricted to any specific model and can be applied to any backbone networks and metric learning methods. We assess the performance of our approach on three significant benchmarks of deep metric learning, namely CUB200-2011, Cars196, and Stanford Online Products, and achieve substantial improvements compared to popular metric learning methods while also providing better interpretability. The code for our method is available at https://github.com/wl-zhao/DIML.