Automating the translation of sign language is a difficult task, and research progress in this field has been slow. Most existing methods require the use of gloss sequence groundtruth, which is laborious to obtain. In this paper, we propose a new approach that does not rely on glosses, but instead only requires text groundtruth. Our approach uses a Transformer network with unique layers that utilize local winner-takes-all (LWTA) layers with stochastic winner sampling, stochastic weights estimated through variational inference, and a weight compression technique that significantly reduces memory usage during inference. We show that our approach achieves the highest reported BLEU-4 score on the PHOENIX 2014T benchmark, without the need for glosses in model training, and with a memory footprint reduced by over 70%.