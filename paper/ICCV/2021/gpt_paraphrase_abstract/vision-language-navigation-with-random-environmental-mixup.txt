Vision-language navigation (VLN) tasks are challenging due to large data bias caused by the disparity between small data scale and large navigation space. Previous approaches have used data augmentation methods to address this issue, but they do not effectively reduce the bias across different house scenes. As a result, agents tend to overfit to seen scenes and perform poorly in unseen scenes. To overcome this problem, we propose the Random Environmental Mixup (REM) method, which generates augmented data by cross-connecting house scenes. We select key viewpoints based on the room connection graph and cross-connect them to construct augmented scenes. We then generate instruction-path pairs in these cross-connected scenes. Experimental results on benchmark datasets show that our REM augmentation method helps reduce the performance gap between seen and unseen environments, leading to improved overall performance. Our model outperforms existing approaches and achieves the best results on the standard VLN benchmark.