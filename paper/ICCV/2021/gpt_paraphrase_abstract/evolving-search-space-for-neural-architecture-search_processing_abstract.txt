Automating the design of neural architectures has been a sought-after approach to replace human experts. Various search methods have been proposed to find the best architecture within the search space. It is commonly believed that enlarging the search space would lead to better results as it would contain more high-performing candidates. However, we have observed that this is not the case for existing neural architecture search (NAS) methods such as DARTS, ProxylessNAS, and SPOS. In fact, expanding the search space has shown to be unhelpful or even detrimental to these methods. This counterintuitive finding suggests that enabling existing methods to work in large search space regimes is challenging, but this issue has not been thoroughly discussed in the literature.To address this problem, we introduce a novel scheme called Neural Search-space Evolution (NSE), specifically designed for large space neural architecture search problems. Existing methods implicitly assume the need for a well-designed search space with a constrained size, and our NSE aims to minimize this requirement. The NSE starts with a subset of the search space and evolves it through two steps: 1) searching for an optimized space within the subset, and 2) refilling this subset from a larger pool of untraversed operations. We also enhance the flexibility of obtainable architectures by introducing a learnable multi-branch setting.Using our proposed method, we achieve a top-1 retrain accuracy of 77.3% on ImageNet with 333M FLOPs, which outperforms previous auto-generated architectures that do not employ knowledge distillation or weight pruning. Moreover, when considering latency constraints, our results also surpass the performance of previous best-performing mobile models, achieving a top-1 retrain accuracy of 77.9%. The code for our method is available at https://github.com/orashi/NSE NAS.