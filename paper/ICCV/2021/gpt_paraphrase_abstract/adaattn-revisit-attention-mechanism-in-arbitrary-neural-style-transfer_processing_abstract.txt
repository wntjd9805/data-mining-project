Fast arbitrary neural style transfer has gained significant attention from academia, industry, and the art community due to its versatility in facilitating various applications. Current solutions either merge deep style features into deep content features without considering feature distributions or adjust deep content features to match style global statistics. Although effective, these approaches neglect shallow features and fail to consider feature statistics locally, resulting in unnatural outputs with undesirable local distortions. To address this issue, we propose a novel module called Adaptive Attention Normalization (AdaAttN) that performs attentive normalization on a per-point basis. AdaAttN learns spatial attention scores from both shallow and deep features of content and style images. It then calculates per-point weighted statistics by treating a style feature point as a distribution of attention-weighted outputs from all style feature points. Subsequently, the content feature is normalized to exhibit the same local feature statistics as the calculated per-point weighted style feature statistics. Additionally, we introduce a novel local feature loss based on AdaAttN to enhance local visual quality. We also modify AdaAttN for video style transfer. Experimental results demonstrate that our approach achieves state-of-the-art arbitrary image/video style transfer. The codes and models are available at https://github.com/wzmsltw/AdaAttN.