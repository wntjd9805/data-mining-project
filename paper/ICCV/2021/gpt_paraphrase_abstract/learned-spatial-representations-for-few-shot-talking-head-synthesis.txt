We present a new method for generating talking-head videos with limited data. While existing techniques have shown promise, they often fail to accurately capture the identity of the subject in the source images. We believe this is because these methods use a single latent code to represent various aspects such as 3D shape, identity cues, colors, lighting, and background details. In contrast, our approach separates the representation of a subject into its spatial and style components. Our method generates the target frame in two steps: first, it predicts a spatial layout for the target image, and then an image generator uses this layout to synthesize the target frame. Through experiments, we demonstrate that this disentangled representation significantly improves upon previous methods in terms of both quantitative and qualitative results.