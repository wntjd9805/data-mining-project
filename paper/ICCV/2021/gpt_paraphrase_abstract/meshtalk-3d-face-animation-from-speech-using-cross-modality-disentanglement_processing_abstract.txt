This paper introduces a novel method for generating realistic 3D facial animation based on speech input. Existing approaches in this field often produce unnatural or rigid upper face animation, lack accurate co-articulation, or rely on person-specific models that limit their scalability. To address these limitations, we propose a generic approach that can generate highly realistic motion for the entire face. Our method utilizes a categorical latent space for facial animation, separating audio-correlated and audio-uncorrelated information using a new cross-modality loss. This ensures accurate lip motion while also synthesizing plausible animation for other facial parts, such as eye blinks and eyebrow motion. Our approach surpasses several baseline methods, achieving state-of-the-art quality both qualitatively and quantitatively. A perceptual user study confirms that our approach is considered more realistic than the current state-of-the-art in over 75% of cases. We recommend watching the supplemental video provided before reading the full paper: https://github.com/ facebookresearch/meshtalk