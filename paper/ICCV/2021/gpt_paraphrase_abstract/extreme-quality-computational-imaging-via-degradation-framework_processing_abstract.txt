In order to reduce the space occupied by optical elements in mobile cameras, free-form surfaces or high-order aspherical lenses are commonly used. However, this introduces the problem of image quality degradation. Existing model-based deconvolution methods are not effective in handling the wide range of spatial variations in the degradation. Deep learning techniques in low-level and physics-based vision are also hindered by the lack of accurate data. To tackle this issue, a degradation framework is developed to estimate the spatially variant point spread functions (PSFs) of mobile cameras. By inputting high-quality digital images, the framework generates degraded images that share a common domain with real-world photographs. Using these synthetic image pairs, a Field-Of-View shared kernel prediction network (FOV-KPN) is designed to perform spatial-adaptive reconstruction on real degraded photos. Extensive experiments demonstrate that this approach achieves exceptional computational imaging and outperforms existing methods. Additionally, it is shown that this technique can be integrated into existing postprocessing systems, leading to significantly improved visual quality.