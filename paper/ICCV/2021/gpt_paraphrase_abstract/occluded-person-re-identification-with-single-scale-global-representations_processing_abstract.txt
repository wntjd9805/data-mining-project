Occluded person re-identification (ReID) involves identifying occluded pedestrians from images taken across multiple cameras. Current state-of-the-art models rely on auxiliary modules to learn multi-scale and/or part-level features, but these models struggle with diverse occlusions and non-occluded pedestrians. To address these issues, we propose a new ReID model that learns discriminative single-scale global features using a novel distance loss on augmented data. Our model outperforms existing models without using auxiliary modules, achieving new state-of-the-art performance in both occluded and non-occluded ReID. We also introduce a large-scale occluded person ReID dataset with diverse occlusions and pedestrian dressings, providing a more accurate benchmark. The dataset is available at: https://git.io/OPReID.