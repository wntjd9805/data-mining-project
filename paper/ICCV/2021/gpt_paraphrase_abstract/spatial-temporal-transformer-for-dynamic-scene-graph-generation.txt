This paper introduces the Spatial-temporal Transformer (STTran), a neural network designed for dynamic scene graph generation in videos. Unlike scene graph generation from images, this task is more challenging due to the dynamic relationships between objects and the temporal dependencies between frames, which enable a more comprehensive semantic interpretation. STTran consists of two main modules: a spatial encoder that extracts spatial context from input frames and analyzes visual relationships within a frame, and a temporal decoder that captures temporal dependencies between frames and infers dynamic relationships using the output of the spatial encoder. A key advantage of STTran is its ability to handle videos of varying lengths as input without clipping, making it suitable for long videos. The proposed method is evaluated on the Action Genome (AG) benchmark dataset and achieves superior performance in terms of dynamic scene graphs. Ablative studies are also conducted to validate the effectiveness of each proposed module. The code for STTran is publicly available at https://github.com/yrcong/STTran.