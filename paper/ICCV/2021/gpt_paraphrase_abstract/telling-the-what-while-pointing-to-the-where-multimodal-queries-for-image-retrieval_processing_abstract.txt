Current image retrieval systems typically rely on text queries to determine what the user is searching for. However, for more detailed image retrieval, it is necessary to specify the location of the desired content within the image. While text can express these localization preferences, it is not as intuitive as using pointing gestures. This study introduces a new approach to image retrieval that combines spoken natural language (to describe the content) with mouse traces on a blank canvas (to indicate the location). The existing image retrieval model is modified to accommodate this multimodal query setup. Through qualitative and quantitative experiments, it is demonstrated that this model effectively incorporates spatial guidance and outperforms text-only systems in terms of retrieval accuracy.