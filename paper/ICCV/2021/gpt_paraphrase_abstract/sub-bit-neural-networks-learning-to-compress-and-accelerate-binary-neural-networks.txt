This paper introduces Sub-bit Neural Networks (SNNs), a novel type of binary quantization design that aims to compress and accelerate Binarized Neural Networks (BNNs). BNNs are extreme solutions for deploying deep models on resource-constrained devices, as they have the lowest storage cost and perform significantly cheaper bit-wise operations compared to 32-bit floating-point counterparts. The motivation for SNNs comes from the observation that binary kernels learned at convolutional layers of a BNN model tend to be distributed over subsets. Unlike existing methods that binarize weights individually, SNNs are trained using a kernel-aware optimization framework that exploits binary quantization in the fine-grained convolutional kernel space. This involves a random sampling step to generate layer-specific subsets of the kernel space and a refinement step to adjust these subsets of binary kernels through optimization. Experimental results on visual recognition benchmarks and FPGA hardware deployment validate the potential of SNNs. For example, on ImageNet, SNNs of ResNet-18/ResNet-34 with 0.56-bit weights achieve 3.13/3.33× runtime speed-up and 1.8× compression compared to conventional BNNs, with moderate drops in recognition accuracy. Promising results are also obtained when applying SNNs to binarize both weights and activations. The code for this research is available at https://github.com/yikaiw/SNN.