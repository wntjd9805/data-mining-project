Many state-of-the-art approaches in self-supervised representation learning aim to ensure the robustness of representations by using predefined augmentations. However, a potential problem with this approach is the occurrence of completely collapsed solutions, where features become constant. To address this issue, researchers have focused on implementation details to avoid collapsed solutions. In this study, we examine a concise framework that incorporates common components from recent approaches. We confirm the existence of complete collapse and also identify another type of collapse called dimensional collapse, which has been overlooked in previous research. We find a strong correlation between axes in dimensional collapse and propose feature decorrelation as a solution, involving standardizing the covariance matrix. Through empirical verification, we demonstrate the benefits of feature decorrelation and emphasize the importance of this insight.