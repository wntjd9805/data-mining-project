The long-tail problem in object classification tasks has been addressed in recent literature. This paper presents the first large-scale study on Long-Tail Visual Relationship Recognition (LTVRR), which focuses on learning structured visual relationships from the long-tail. Two benchmarks, VG8K-LT and GQA-LT, are introduced to evaluate the performance of state-of-the-art long-tail models on LTVRR. Additionally, a visiolinguistic hubless (VilHub) loss and a Mixup augmentation technique called RelMix are proposed to improve performance, particularly on tail classes. The benchmarks, code, and models are available at: https://github.com/Vision-CAIR/LTVRR.