Visual localization and mapping is a crucial technology for mixed reality and robotics systems. Many current methods rely on local features to establish connections between images. This paper introduces three new scenarios for localization and mapping that require continuous updates of feature representations and the ability to match different types of features. Traditionally, localization and mapping assumes that the same local features are used throughout the map's evolution. However, in practice, changing the features would require starting the process from scratch, which is often not feasible due to the loss of stored raw images and attached digital content. To address this limitation, we propose a principled solution for cross-descriptor localization and mapping. Our approach is data-driven, independent of the feature descriptor type, computationally efficient, and scalable with the number of description algorithms used. Extensive experiments demonstrate the effectiveness of our approach on various benchmark datasets, including both handcrafted and learned features.