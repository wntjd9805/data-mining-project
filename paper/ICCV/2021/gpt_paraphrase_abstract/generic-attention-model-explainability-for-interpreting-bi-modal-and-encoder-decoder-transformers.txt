Transformers are increasingly dominating tasks that require multi-modal reasoning, such as visual question answering. They achieve state-of-the-art results by contextualizing information using self-attention and co-attention mechanisms. These attention modules also play a role in computer vision tasks like object detection and image segmentation. While Transformers with self-attention only focus on a single attention map, Transformers with co-attention consider multiple attention maps in parallel to highlight relevant information. In this study, we propose a novel method to explain predictions made by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attention. We offer generic solutions and apply them to the three most commonly used architectures: pure self-attention, self-attention combined with co-attention, and encoder-decoder attention. Our method outperforms existing approaches adapted from single modality explainability. The code for our method is available at: https://github.com/hila-chefer/Transformer-MM-Explainability.