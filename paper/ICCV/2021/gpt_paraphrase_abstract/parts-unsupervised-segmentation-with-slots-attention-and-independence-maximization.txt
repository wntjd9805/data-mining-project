Humans have the ability to perceive the visual world as consisting of distinct objects with different properties. There is a desire to develop models that can learn this structure without the need for supervision. However, learning this structure from complex 3D scenes that include clutter, occlusions, interactions, and camera movement is still a challenge. In this study, we propose a model called PARTS that can segment visual scenes from complex 3D environments into separate objects, learn representations of each object, and make consistent predictions about future frames in an unsupervised manner. Our model builds on existing approaches that use iterative inference and transition dynamics for deep generative models. We have made several innovative contributions to improve performance. We introduce a recurrent slot-attention like encoder that allows for top-down influence during inference. We argue that using a fixed prior shared across the sequence is more effective for inferring scene structure from image sequences compared to an auto-regressive prior. We evaluate our model on three different video datasets, including a popular benchmark, a simulated 3D Playroom environment, and a real-world Robotics Arm dataset. Finally, we analyze the contributions of the different components of our model and the representations it learns.