An object's context is crucial for both human and machine vision, allowing us to distinguish between different objects. However, studying context in photographs has been challenging due to the lack of control over contextual violations. To address this issue, we present a synthetic Out-of-Context (OCD) dataset that allows for precise control over scene context. Using a 3D simulation engine, we manipulated gravity, object co-occurrences, and relative sizes across various object categories in a virtual household environment. We conducted experiments using the OCD dataset to understand the impact of contextual cues on human and machine vision. Psychophysics experiments helped establish a human benchmark for out-of-context recognition, which we then compared to state-of-the-art computer vision models. We developed a context-aware recognition transformer model that combines object and contextual information using multi-head attention. This model performs at a human-level, demonstrating superior robustness in out-of-context conditions compared to baseline models on the OCD dataset and other out-of-context datasets. The source code and data for our research are publicly available at https://github.com/kreimanlab/WhenPigsFlyContextAll.