Object recognition has made significant progress in recent years, but it still heavily relies on numerous well-trained examples per object category. Conversely, the ability to learn new objects from just a few examples could have a profound impact across various fields, including robotics and user personalization. However, most research on few-shot learning has been based on benchmark datasets that do not capture the high variation encountered in real-world applications. To bridge this gap, we introduce the ORBIT dataset and benchmark, which focuses on the practical application of teachable object recognizers for individuals with visual impairments. The dataset consists of 3,822 videos showcasing 486 objects, recorded by visually impaired individuals using their mobile phones. The benchmark presents a realistic and highly challenging recognition problem, providing a valuable platform for advancing research in handling few-shot learning under high-variation conditions. We establish the benchmark's initial state-of-the-art performance and demonstrate significant potential for further innovation, with the ability to impact a wide range of real-world vision applications, particularly in assisting the blind and visually impaired community. The dataset is publicly available at https://doi.org/10.25383/city.14294597, while the benchmark code can be accessed at https://github.com/microsoft/ORBIT-Dataset.