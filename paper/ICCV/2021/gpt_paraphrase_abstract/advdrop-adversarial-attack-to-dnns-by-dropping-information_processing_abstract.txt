Recognizing visual objects with limited information, such as contours in cartoons, is easy for humans. However, Deep Neural Networks (DNNs) struggle with abstract objects that have lost information. In this study, we investigate this challenge from an adversarial perspective. We introduce a new adversarial attack called AdvDrop, which creates adversarial examples by removing existing information from images. Unlike previous attacks that add noticeable disturbances to clean images, our approach explores the adversarial robustness of DNN models by dropping imperceptible details to craft adversarial examples. Extensive experiments demonstrate the effectiveness of AdvDrop and its ability to evade current defense systems.