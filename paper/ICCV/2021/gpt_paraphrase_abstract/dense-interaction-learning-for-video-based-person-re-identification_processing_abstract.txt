This paper introduces a hybrid framework called DenseInteraction Learning (DenseIL) for video-based person re-identification (re-ID). The goal of re-ID is to match the same person across different video clips. The proposed DenseIL framework combines the advantages of CNN-based and Attention-based architectures to address the challenges of video-based person re-ID.

DenseIL consists of a CNN encoder and a Dense Interaction (DI) decoder. The CNN encoder efficiently extracts discriminative spatial features, while the DI decoder models the spatial-temporal interaction across frames. In contrast to previous approaches, the DI decoder attends to intermediate fine-grained CNN features, resulting in a multi-grained spatial-temporal representation for each video clip. Additionally, the DI decoder incorporates Spatio-TEmporal Positional Embedding (STEP-Emb) to examine the positional relation among the spatial-temporal inputs.

Experimental results demonstrate that DenseIL consistently outperforms state-of-the-art methods on multiple standard video-based person re-ID datasets.