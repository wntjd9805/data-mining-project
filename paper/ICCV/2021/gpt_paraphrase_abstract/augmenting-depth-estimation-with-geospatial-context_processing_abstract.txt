We investigate the problem of depth estimation in modern cameras that have sensors for recording the geospatial context of an image. We focus on the scenario where the camera is geocalibrated, which we call geo-enabled depth estimation. Our main idea is that if we know the capture location, we can utilize the corresponding overhead viewpoint to better understand the scale of the scene. To achieve this, we propose an end-to-end architecture that leverages geospatial context to infer a synthetic ground-level depth map from an overhead image taken at the same location. We then integrate this information into an encoder/decoder style segmentation network. To evaluate our approach, we enhance an existing dataset with overhead imagery and corresponding height maps. Our results show that incorporating geospatial context significantly reduces errors compared to baseline methods, both at close ranges and when evaluating at larger distances than previous benchmarks.