A new benchmark for visual localization in outdoor scenes is proposed, aiming to address the limitations of existing benchmarks. Visual localization is the process of determining the position and orientation of an image within a known scene. It is crucial for various applications in computer vision and robotics, such as self-driving cars and augmented/virtual reality systems. To ensure reliable and robust performance, visual localization techniques need to work well in diverse conditions, including different seasons, weather conditions, lighting, and changes in the environment.

Existing benchmarks have made significant progress in evaluating visual localization algorithms by providing images under various conditions. However, these benchmarks are limited to a few specific geographical regions and are often recorded using a single camera device. Therefore, a new benchmark is proposed that incorporates crowd-sourced data to cover a wider range of geographical regions and camera devices. The focus of this benchmark is to highlight the failure cases of current algorithms, making it more challenging for state-of-the-art localization approaches.

Experiments conducted using the proposed dataset demonstrate its difficulty, as all evaluated methods fail to achieve accurate results in the most challenging parts of the dataset. Along with the release of the dataset, the tools used to generate it are provided to facilitate efficient and effective annotation of 2D correspondences for obtaining reference poses.