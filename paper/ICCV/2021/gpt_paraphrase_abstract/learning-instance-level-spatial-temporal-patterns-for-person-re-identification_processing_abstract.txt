Person re-identification (Re-ID) is a task that involves matching individuals across different cameras. Traditionally, Re-ID methods focus on visual representation learning and image search, but the accuracy of these methods is heavily influenced by the size of the search space. Spatial-temporal information has been shown to be effective in filtering out irrelevant samples and improving Re-ID accuracy. However, existing spatial-temporal Re-ID methods are not fully utilizing this information. 

To address this limitation, we propose a novel method called Instance-level and Spatial-Temporal Disentangled Re-ID (InSTD). InSTD takes into account personalized information such as moving direction to narrow down the search space. Additionally, we disentangle the spatial-temporal transferring probability from the joint distribution to the marginal distribution, allowing for better modeling of outliers. 

We conducted extensive experiments to evaluate the performance of our proposed method. The results show that our method outperforms the baseline on two benchmark datasets, Market-1501 and DukeMTMC-reID, achieving mAP scores of 90.8% and 89.1% respectively, compared to baseline scores of 82.2% and 72.7%. 

In order to provide a better benchmark for person re-identification, we also release a cleaned data list of the DukeMTMC-reID dataset, which can be accessed at https://github.com/RenMin1991/cleaned-DukeMTMC-reID/.