Temporal action localization in videos is a difficult task, especially due to the variation in action temporal scales. Short actions, which make up a significant portion of the datasets, typically have the lowest performance. In this study, we address the challenge of short actions by proposing a multi-level cross-scale solution called video self-stitching graph network (VSGN). VSGN consists of two main components: video self-stitching (VSS) and cross-scale graph pyramid network (xGPN). In VSS, we focus on a short period of a video and magnify it along the temporal dimension to obtain a larger scale. By stitching the original clip with its magnified counterpart in one input sequence, we can leverage the complementary properties of both scales. The xGPN component further exploits cross-scale correlations through a pyramid of cross-scale graph networks, each containing a hybrid module that aggregates features from different scales. Our VSGN improves feature representations and generates more positive anchors for short actions, as well as increasing the number of short training samples. Experimental results demonstrate that VSGN significantly enhances the localization performance of short actions and achieves state-of-the-art performance on THUMOS-14 and ActivityNet-v1.3 datasets. The code for VSGN is available at https://github.com/coolbay/VSGN.