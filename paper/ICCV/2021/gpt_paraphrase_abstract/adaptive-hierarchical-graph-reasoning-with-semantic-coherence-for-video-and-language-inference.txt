This paper focuses on the task of Video-and-Language Inference, which involves determining whether a natural language statement aligns or contradicts a given video clip. The authors address three main challenges in this task: evaluating the overall correctness of statements with multiple semantic meanings, reasoning jointly over video and subtitles, and modeling complex social interactions and long-range relationships. To tackle these challenges, the authors propose an adaptive hierarchical graph network that enables in-depth understanding of the video by considering complex interactions. This network performs joint reasoning over video and subtitles in three hierarchical levels, with the graph structure adjusting based on the semantic structures of the statement. Additionally, the authors introduce semantic coherence learning to enhance the alignment between vision and linguistics, as well as the coherence across a sequence of video segments. Experimental results demonstrate that their method outperforms the baseline by a significant margin.