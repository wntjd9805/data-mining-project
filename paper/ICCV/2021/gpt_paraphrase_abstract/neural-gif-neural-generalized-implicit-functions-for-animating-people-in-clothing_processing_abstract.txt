We introduce Neural Generalized Implicit Functions (Neural-GIF), a method for animating clothed individuals based on body pose. Instead of relying on fixed and limited template-based representations, our approach learns to animate characters in new poses using a more flexible model. Inspired by template-based methods, we decompose motion into articulation and non-rigid deformation and generalize this concept for implicit shape learning. By mapping each point in space to a canonical space and applying a learned deformation field, we can model complex non-rigid effects without the need for template registration. Our approach can be trained on raw 3D scans and reconstructs detailed surface geometry and deformations of clothing and soft tissue. Additionally, it can generalize to new poses. We conducted evaluations on various characters with diverse clothing styles, and our method outperformed baseline approaches both quantitatively and qualitatively. We also extended our model to multiple shape settings. To encourage further research, we will make our model, code, and data publicly available.