We present a new method for learning representations from unlabelled videos called self-supervised contrastive learning. Unlike existing approaches that focus on invariance to temporal transformations, we argue that video representations should preserve video dynamics and reflect temporal manipulations. To achieve this, we introduce constraints that ensure equivariance to temporal transformations and enhance the capture of video dynamics. Our method encodes relative temporal transformations between augmented clips of a video in a vector and contrasts it with other transformation vectors. Additionally, we propose self-supervised classification of two clips of a video into overlapping, ordered, or unordered categories to support temporal equivariance learning. Experimental results demonstrate that our time-equivariant representations achieve state-of-the-art performance in video retrieval and action recognition benchmarks (UCF101, HMDB51, and Diving48).