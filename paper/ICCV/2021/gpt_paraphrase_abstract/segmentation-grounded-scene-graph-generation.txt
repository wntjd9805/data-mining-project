This study focuses on the problem of generating scene graphs in computer vision. Scene graphs provide a detailed representation of objects, their locations, and their relationships in an image. However, existing methods for scene graph generation only operate at the level of proposal bounding boxes, which limits their accuracy. To address this limitation, the authors propose a novel framework for pixel-level segmentation-grounded scene graph generation. This framework is not dependent on any specific scene graph generation method and tackles the lack of segmentation annotations in target scene graph datasets. This is achieved through transfer and multi-task learning using an auxiliary dataset.In the proposed framework, each detected object in the target scene is assigned a segmentation mask. This mask is created by combining annotations from categories present in the auxiliary dataset. Additionally, a Gaussian masking mechanism is used to ground the relations between objects at a pixel-level within the image. These improvements contribute to more accurate relation prediction.The entire framework is designed to be end-to-end trainable and is learned in a multi-task manner. The authors have also made the code for their framework available on GitHub.