Current 3D human pose estimation methods suffer from a lack of view-point equivariance, meaning they struggle to perform well when faced with viewpoints that were not seen during training. This limitation is often due to the reliance on scale-invariant, translation-invariant, or rotation-invariant operations, such as max-pooling, in deep learning approaches. These operations do not necessarily improve viewpoint generalization and can lead to methods that are highly dependent on the training data.

To address this issue, we propose a new approach called DECA, which utilizes a capsule autoencoder network with fast Variational Bayes capsule routing. In our method, each human joint is treated as a capsule entity, and the routing algorithm is used to preserve the hierarchical and geometrical structure of the joints in the feature space, regardless of the viewpoint. This results in viewpoint equivariance, significantly reducing the network's dependence on training data and improving its ability to generalize to unseen viewpoints.

In our experimental validation, DECA outperforms other methods when applied to depth images from both seen and unseen viewpoints, including both top-view and front-view perspectives. In the RGB domain, DECA achieves state-of-the-art results in the challenging viewpoint transfer task and establishes a new framework for top-view human pose estimation. The code for our approach can be found at https://github.com/mmlab-cv/DECA.