Our approach focuses on generating realistic co-speech gestures that are synchronized with speech audio. We aim to generate movements for the entire upper body, including the arms, hands, and head. While previous data-driven methods have been successful, they still face challenges such as limited variety, poor fidelity, and a lack of objective metrics. To address these challenges, we propose a method that learns a set of gesture template vectors to represent latent conditions and reduce ambiguity. These template vectors determine the general appearance of the gesture sequence, while the speech audio drives subtle body movements necessary for creating a realistic gesture sequence. Due to the absence of a feasible objective metric for gesture-speech synchronization, we use lip-sync error as a proxy metric to fine-tune and evaluate our model's synchronization ability. Our extensive experiments demonstrate the superiority of our method in terms of both objective and subjective evaluations, particularly in fidelity and synchronization.