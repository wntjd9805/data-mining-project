Ternary Neural Networks (TNNs) have attracted attention for their potential to be much faster and more power efficient than full-precision networks. However, conventional TNNs require 2 bits to encode the ternary representation, resulting in similar memory consumption and speed as standard 2-bit models but with worse representational capability. Additionally, there is still a significant accuracy gap between TNNs and full-precision networks, limiting their use in real applications. To address these challenges, this study presents two solutions. Firstly, under certain constraints, the computational complexity of the ternary inner product can be reduced by 2Ã—. Secondly, an implementation-dependent ternary quantization algorithm called Fast and Accurate Ternary Neural Networks (FATNN) is designed to mitigate the performance gap. Experimental results on image classification show that FATNN outperforms state-of-the-art methods in terms of accuracy. Furthermore, the speedup of FATNN compared to various precision levels is evaluated on multiple platforms, providing a strong benchmark for future research. The source code and models of FATNN are available at https://github.com/MonashAI/QTool.