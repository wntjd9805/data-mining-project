Recent approaches to visual question answering (VQA) heavily rely on large annotated datasets. However, manually annotating questions and answers for videos is a laborious and costly process that hampers scalability. In this study, we propose a method to bypass manual annotation and create a large-scale training dataset for VQA using automatic cross-modal supervision. We utilize a question generation transformer trained on textual data to generate question-answer pairs from transcribed video narrations. By applying this approach to narrated videos, we automatically generate the HowToVQA69M dataset, which consists of 69 million video-question-answer triplets. To handle the diverse range of answers in this dataset, we introduce a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer. We also introduce the novel concept of zero-shot VideoQA and demonstrate its effectiveness, particularly for rare answers. Additionally, we showcase the superiority of our method compared to the state-of-the-art on various benchmark datasets such as MSRVTT-QA, MSVD-QA, ActivityNet-QA, and How2QA. Finally, we present iVQA, a newly created VideoQA dataset with reduced language biases and high-quality redundant manual annotations, for a comprehensive evaluation. The answer format of our method is designed to output only the abstraction of the answer.