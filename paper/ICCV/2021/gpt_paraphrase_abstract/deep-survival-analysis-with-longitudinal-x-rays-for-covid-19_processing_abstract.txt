Time-to-event analysis is a crucial statistical tool in healthcare resource allocation, but traditional methods like the Cox model cannot effectively incorporate high-dimensional imaging data. To address this limitation, we propose a deep learning approach that seamlessly integrates multiple time-dependent imaging studies and non-imaging data into time-to-event analysis. We evaluate our techniques on a clinical dataset of 1,894 COVID-19 patients and demonstrate that incorporating image sequences significantly enhances prediction accuracy. For instance, conventional time-to-event methods yield a concordance error of approximately 30-40% for predicting hospital admission, whereas our approach achieves an error rate of 25% without images and 20% when multiple X-rays are included. Ablation studies confirm that our models successfully avoid learning irrelevant features like scanner artifacts and highlight the superior performance of models utilizing multiple images compared to those using just one. Although our primary focus and evaluation are on COVID-19, the methods we develop have broad applicability.