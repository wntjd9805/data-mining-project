In image retrieval, deep convolutional networks have been proven effective in enhancing performance by learning local features. Attention learning has been explored to discriminate deep local features. However, existing methods generate only a single attention map per image, which limits the exploration of diverse visual patterns. To address this limitation, we propose a novel architecture for deep local feature learning that focuses on multiple discriminative local patterns in an image simultaneously. Our framework first reorganizes the activation map channels for multiple heads, and then employs a new dynamic attention module for each head to learn potential attentions. The entire architecture is trained as metric learning of weighted-sum-pooled global image features using image-level relevance labels. Following architecture training, we select local features for each database image based on their multi-head dynamic attentions, which are then indexed for efficient retrieval. Extensive experiments on the Revisited Oxford and Paris datasets demonstrate that our proposed method outperforms state-of-the-art approaches. Additionally, even with lower-dimensional local features, our method achieves competitive results. The code for our method will be released at https://github.com/CHANWH/MDA.