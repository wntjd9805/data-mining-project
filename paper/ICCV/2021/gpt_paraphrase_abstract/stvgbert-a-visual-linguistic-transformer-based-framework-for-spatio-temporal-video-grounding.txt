This study introduces a framework called STVGBert for the task of spatio-temporal video grounding (STVG). STVG involves localizing a target object in an untrimmed video using a query sentence. STVGBert is a one-stage visual-linguistic transformer based framework that can simultaneously localize the target object in both spatial and temporal domains. Unlike previous methods that rely on pre-generated object proposals, STVGBert takes a video and a query sentence as input and utilizes a cross-modal feature learning module called ST-ViLBert to generate cross-modal features. These features are then used to generate bounding boxes and predict the starting and ending frames, resulting in a predicted object tube. Notably, STVGBert is the first one-stage method that can handle the STVG task without relying on pre-trained object detectors. Extensive experiments conducted on benchmark datasets Vid-STG and HC-STVG demonstrate that the proposed framework outperforms state-of-the-art multi-stage methods.