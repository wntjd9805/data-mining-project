We introduce MetaUVFS, an innovative Unsupervised Meta-learning algorithm for Video Few-Shot action recognition. Our approach utilizes a vast collection of over 550K unlabeled videos to train a two-stream 2D and 3D CNN architecture using contrastive learning. This allows us to capture appearance-specific spatial and action-specific spatio-temporal video features. To enhance the training process, we incorporate a novel Action-Appearance Aligned Meta-adaptation (A3M) module. This module focuses on action-oriented video features in relation to appearance features through explicit few-shot episodic meta-learning over unsupervised hard-mined episodes. By conditioning the unsupervised training to mimic the downstream few-shot task, MetaUVFS surpasses all existing unsupervised methods on few-shot benchmarks. Unlike previous few-shot action recognition methods that require supervision, MetaUVFS does not rely on base-class labels or a supervised pretrained backbone. This means that MetaUVFS only needs to be trained once to achieve competitive or even superior performance compared to state-of-the-art supervised methods on popular few-shot datasets such as HMDB51, UCF101, and Kinetics100.