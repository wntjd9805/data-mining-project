The current methods for enhancing pixel representation in semantic image segmentation by aggregating contextual information focus on capturing context at the image level. However, this approach neglects the significance of pixel representations within the same category. To address this, this paper proposes a method that aggregates both image-level and semantic-level contextual information to augment pixel representations.   First, an image-level context module is developed to capture contextual information for each pixel in the entire image. Then, the representations of pixels belonging to the same category are aggregated using ground-truth segmentation as guidance.   Next, similarities between each pixel representation and the image-level and semantic-level contextual information are computed. Finally, the pixel representation is augmented by weighted aggregation of both the image-level and semantic-level contextual information, using the similarities as weights.   By integrating image-level and semantic-level context, this paper achieves state-of-the-art accuracy on four benchmark datasets: ADE20K, LIP, COCOStuff, and Cityscapes.