The neural radiance fields (NeRF) method for rendering scenes using a single ray per pixel can result in blurry or aliased images when scene content is observed at different resolutions. The conventional approach of using multiple rays per pixel for supersampling is not feasible for NeRF due to the computational cost. In this study, we propose a solution called "mip-NeRF" (based on "mipmap") that represents the scene at a continuous scale. Instead of rendering rays, mip-NeRF efficiently renders anti-aliased conical frustums, thereby reducing aliasing artifacts and enhancing the representation of fine details. Additionally, mip-NeRF is 7% faster and half the size compared to NeRF. When evaluated on a dataset, mip-NeRF achieves a 17% reduction in average error rates compared to NeRF, while on a more challenging multiscale variant of the dataset, it shows a 60% reduction. Furthermore, mip-NeRF matches the accuracy of a brute-force supersampled NeRF on the multiscale dataset but is 22 times faster.