This study focuses on learning from image-text data, which has been successful for recognition tasks but is currently limited to visual features or individual visual concepts. The researchers propose a method that learns from image-sentence pairs to extract a graphical representation of localized objects and their relationships within an image, called a scene graph. To connect images and texts, they use an object detector to identify and locate object instances and match their labels to concepts parsed from captions, creating "pseudo" labels for learning the scene graph. They also develop a Transformer-based model to predict these "pseudo" labels through a masked token prediction task. By only using image-sentence pairs for learning, their model achieves a 30% relative improvement compared to a recent method trained with human-annotated unlocalized scene graphs. The model also performs well in weakly and fully supervised scene graph generation. Additionally, the researchers explore an open-vocabulary setting for detecting scene graphs and achieve the first results for open-set scene graph generation. These findings demonstrate the effectiveness of their approach in extracting scene graphs from image-sentence pairs.