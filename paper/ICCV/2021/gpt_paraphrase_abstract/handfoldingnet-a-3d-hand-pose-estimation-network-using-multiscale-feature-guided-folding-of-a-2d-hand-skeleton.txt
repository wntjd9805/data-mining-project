This paper introduces HandFoldingNet, a novel and efficient hand pose estimator for 3D hand pose estimation in human-computer interaction applications. Current convolution neural network (CNN) based estimation models are either complex or computationally redundant to achieve acceptable accuracy. In contrast, HandFoldingNet addresses this limitation by utilizing a folding-based decoder that regresses the hand joint locations from a normalized 3D hand point cloud input. The folding process is guided by multi-scale features, incorporating both global and joint-wise local features to improve estimation accuracy. Experimental results demonstrate that HandFoldingNet outperforms existing methods on three hand pose benchmark datasets, while also requiring the fewest model parameters. The code for HandFoldingNet is publicly available at https://github.com/cwc1260/HandFold.