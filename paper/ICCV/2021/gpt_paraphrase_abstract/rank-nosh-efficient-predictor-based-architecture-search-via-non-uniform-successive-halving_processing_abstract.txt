Predictor-based algorithms have made significant progress in Neural Architecture Search (NAS) tasks. However, these methods are computationally expensive because training the performance predictor requires evaluating numerous architectures from scratch. Previous approaches focused on reducing the number of architectures needed for training the predictor. In this study, we address this issue differently by improving search efficiency through reducing the computation budget for architecture training. We propose NOn-uniform Successive Halving (NOSH), a hierarchical scheduling algorithm that terminates the training of underperforming architectures early to avoid wasting resources. To effectively utilize the non-uniform supervision signals generated by NOSH, we formulate predictor-based architecture search as a learning to rank problem using pairwise comparisons. The resulting method, called RANK-NOSH, achieves competitive or better performance than previous state-of-the-art predictor-based methods on different spaces and datasets while significantly reducing the search budget.