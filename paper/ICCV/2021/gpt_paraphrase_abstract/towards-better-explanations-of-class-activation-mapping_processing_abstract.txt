Recent advancements in understanding the internal behavior of convolutional neural networks (CNNs) have resulted in significant progress in explanation techniques. One such method is class activation mapping (CAM), which generates visual explanation maps by combining activation maps from CNNs. However, many existing CAM-based methods lack a clear theoretical foundation for assigning coefficients to the linear combination. In this study, we revisit the intrinsic linearity of CAM and propose a CNN explanation model that is a linear function of binary variables representing the presence of activation maps. This allows us to determine the explanation model using additive feature attribution methods in an analytical manner. We demonstrate the suitability of SHAP values, which possess desirable properties, as coefficients for CAM. Since exact SHAP values are unattainable, we introduce LIFT-CAM, an efficient approximation method based on DeepLIFT, which accurately and swiftly estimates the SHAP values of activation maps. LIFT-CAM surpasses previous CAM-based methods in both qualitative and quantitative aspects.