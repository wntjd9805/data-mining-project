Existing unsupervised video object segmentation methods that fuse RGB frames and optical flow using a two-stream network are unable to effectively handle distracting noises in each input modality, leading to a decline in model performance. To address this issue, we propose a method that establishes correspondence between input modalities and suppresses distracting signals through optimal structural matching. This involves extracting dense local features from the RGB image and optical flow of a video frame and treating them as complex structured representations. To measure the alignment between the local features, we utilize the Wasserstein distance to compute global optimal flows that transport the features from one modality to the other. We integrate this structural matching approach into a two-stream network for end-to-end training by factorizing the input cost matrix into smaller spatial blocks and designing a differentiable long-short Sinkhorn module, which consists of a long-distant Sinkhorn layer and a short-distant Sinkhorn layer. This module is then integrated into a dedicated two-stream network called TransportNet. Experimental results demonstrate that our approach, which aligns motion-appearance, achieves state-of-the-art results on popular video object segmentation datasets.