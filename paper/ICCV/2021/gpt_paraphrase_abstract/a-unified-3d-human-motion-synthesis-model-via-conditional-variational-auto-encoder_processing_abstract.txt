We propose a unified framework for 3D motion synthesis that addresses the tasks of motion prediction, completion, interpolation, and spatial-temporal recovery. Existing approaches typically focus on specific tasks or use different architectures for different tasks, but our framework is flexible and can handle all these tasks. We use a Conditional Variational Auto-Encoder (CVAE) as the basis of our framework, treating any input as a masked motion series. By considering the problem as a conditional generation process, we estimate a parametric distribution of the missing regions based on the input conditions and use it to sample and synthesize the full motion series. To allow for manipulation of the motion style, we introduce an Action-Adaptive Modulation (AAM) that propagates semantic guidance throughout the sequence. We also incorporate a cross-attention mechanism to capture distant relations between decoder and encoder features, enhancing realism and global consistency. We evaluate our method on Human 3.6M and CMU-Mocap datasets and demonstrate that it produces coherent and realistic results for various motion synthesis tasks.