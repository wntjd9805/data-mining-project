Recently, regression-based methods have shown promising outcomes in reconstructing human meshes from monocular images. These methods utilize neural networks to directly map raw pixels to model parameters, enabling the generation of parametric models in a feed-forward manner. However, slight deviations in these parameters can result in noticeable misalignment between the estimated meshes and image evidence. To tackle this issue, we propose a Pyramidal Mesh Alignment Feedback (PyMAF) loop. This approach utilizes a feature pyramid to rectify the predicted parameters explicitly based on the alignment status between the mesh and the image in our deep regressor. In PyMAF, aligned evidence is extracted from finer-resolution features corresponding to the currently predicted parameters and fed back for parameter rectification. To enhance the reliability of these evidences and reduce noise, we apply auxiliary pixel-wise supervision on the feature encoder. This supervision provides guidance for preserving the most relevant information in spatial features, ensuring mesh-image correspondence. We validate the effectiveness of our approach on various benchmarks such as Human3.6M, 3DPW, LSP, and COCO. Experimental results consistently demonstrate that our approach improves the mesh-image alignment of the reconstruction. For further details, including code and video results, please visit our project page at https://hongwenzhang.github.io/pymaf.