Understanding how deep neural networks (DNNs) function is crucial for developing reliable artificial intelligence techniques. Previous studies have focused on associating semantic concepts with units or layers of DNNs but have not explained the inference process. This paper aims to bridge this gap by introducing neural architecture disentanglement (NAD). NAD is a method that disentangles a pre-trained DNN into sub-architectures based on independent tasks, revealing the information flows that describe the inference processes. The authors conducted experiments using both handcrafted and automatically searched network architectures on object-based and scene-based datasets to investigate the disentanglement process. The results revealed three important findings. Firstly, DNNs can be divided into sub-architectures that handle independent tasks. Secondly, deeper layers in DNNs do not always correspond to higher semantics. Lastly, the type of connections in a DNN affects how information flows across layers, resulting in different disentanglement behaviors. The authors also used NAD to explain why DNNs sometimes make incorrect predictions. They found that misclassified images have a high probability of being assigned to task sub-architectures similar to the correct ones. The code for NAD is available at the provided GitHub link.