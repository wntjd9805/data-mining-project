This study presents a new framework called RMSMP for quantizing Deep Neural Networks (DNNs) using a Row-wise Mixed-Scheme and Multi-Precision approach. Unlike previous work, this framework assigns mixed quantization schemes and multiple precisions within layers, specifically among rows of the DNN weight matrix. The goal is to simplify hardware inference operations while maintaining accuracy. The authors also observe that the quantization error does not necessarily exhibit layer-wise sensitivity and can be mitigated by using higher precisions for a certain portion of weights in each layer. This observation allows for layer-wise uniformity in hardware implementation, resulting in guaranteed inference acceleration, while still allowing flexibility in mixed schemes and multiple precisions at the row level to improve accuracy. The researchers derived practical and effective candidates for schemes and precisions using a hardware-informative strategy to reduce the search space. The proposed RMSMP algorithm, with offline determined ratios of quantization schemes and precisions for all layers, utilizes Hessian and variance-based methods to assign schemes and precisions for each row. The RMSMP framework was tested on image classification and natural language processing applications (BERT) and achieved the best accuracy performance compared to state-of-the-art methods using equivalent precisions. The researchers implemented RMSMP on FPGA devices and achieved a 3.65Ã— speedup in the end-to-end inference time for ResNet-18 on ImageNet compared to the 4-bit Fixed-point baseline.