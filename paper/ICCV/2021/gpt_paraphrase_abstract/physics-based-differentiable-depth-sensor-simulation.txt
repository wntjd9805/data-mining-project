Gradient-based algorithms are essential for computer vision and graphics applications, allowing for learning-based optimization and solving inverse problems. While these algorithms have been successful in generating photorealistic color images, there has been limited progress in extending them to generate depth (2.5D) images. Simulating depth sensors that use structured light requires solving complex light transport and stereo-matching problems. In this paper, we propose a new end-to-end differentiable simulation pipeline that generates realistic 2.5D scans using physics-based 3D rendering and custom block-matching algorithms. Each module in the pipeline can be differentiated with respect to sensor and scene parameters, enabling automatic tuning for different devices or integration into larger computer vision applications. By training deep-learning models on these synthetic depth scans, we achieve significantly improved performance on real scans for tasks such as classification, pose estimation, and semantic segmentation. This demonstrates the fidelity and value of our synthetic depth data compared to previous static simulations and learning-based domain adaptation methods.