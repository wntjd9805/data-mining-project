This paper addresses the task of estimating the body pose of a camera wearer from their own perspective (egopose), which is important for augmented and virtual reality applications. Current approaches either use a front-facing camera with a narrow field of view that only captures a limited view of the wearer, or a top-down camera mounted on the head that provides maximum visibility but is not natural. In this paper, we propose a novel deep learning system that takes advantage of the wide angle camera on user-centric wearable devices like glasses. Our system computes the 3D head pose, 3D body pose, and figure/ground separation simultaneously, while ensuring geometric consistency across these pose attributes. We demonstrate that our system can be trained effectively using existing motion capture data, eliminating the need for large new datasets. Finally, our system is able to estimate egopose in real time with high accuracy.