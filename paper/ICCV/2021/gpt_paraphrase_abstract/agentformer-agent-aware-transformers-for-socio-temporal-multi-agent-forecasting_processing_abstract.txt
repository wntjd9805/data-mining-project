Accurately predicting the future movements of multiple agents is crucial for autonomous systems. However, this task is challenging due to the complex interactions between agents and the uncertainty in their future behaviors. In order to forecast multi-agent trajectories, two key dimensions need to be considered: the time dimension, which involves modeling the influence of past agent states on future states, and the social dimension, which involves modeling how the state of each agent affects others.

Previous methods have tackled these two dimensions separately, first using a temporal model to summarize features over time for each agent independently, and then modeling the interaction of these summarized features with a social model. However, this approach is suboptimal as encoding features independently over either the time or social dimension can lead to a loss of information. Ideally, we would prefer a method that allows an agent's state at one time to directly impact another agent's state at a future time.

To address this, we propose a new Transformer model called AgentFormer, which simultaneously models the time and social dimensions. Our model utilizes a sequence representation of multi-agent trajectories by flattening trajectory features across time and agents. Unlike standard attention operations that ignore agent identity, AgentFormer incorporates a novel agent-aware attention mechanism that preserves agent identities by attending to elements of the same agent differently than elements of other agents.

Based on AgentFormer, we further propose a stochastic multi-agent trajectory prediction model that can attend to features of any agent at any previous timestep when predicting an agent's future position. This model also jointly models the latent intent of all agents, allowing the stochasticity in one agent's behavior to influence other agents.

Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art techniques on established pedestrian and autonomous driving datasets.