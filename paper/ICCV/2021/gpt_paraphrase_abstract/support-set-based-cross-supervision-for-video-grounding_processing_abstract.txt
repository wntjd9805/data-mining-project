Current methods for video grounding have utilized complex architectures to capture the relationship between videos and texts, resulting in significant advancements. However, it has been challenging to effectively learn the intricate multi-modal relations solely through architectural design. In this study, we propose a unique module called Support-set Based Cross-Supervision (Sscs) that can enhance existing methods during the training phase without incurring additional inference costs. The Sscs module comprises two primary components: a discriminative contrastive objective and a generative caption objective. The contrastive objective aims to learn meaningful representations through contrastive learning, while the caption objective supervises the training of a robust video encoder using textual information. A limitation in video grounding arises from the presence of visual entities that exist both in the ground-truth and background intervals, resulting in mutual exclusion. Na√Øvely applying contrastive learning is inappropriate for video grounding in such cases. To address this problem, we enhance the cross-supervision by introducing the support-set concept, which gathers visual information from the entire video and eliminates the issue of mutual exclusion among entities. By combining the original objectives with the proposed Sscs module, we can augment the capabilities of existing approaches in modeling multi-modal relations. We extensively evaluate the effectiveness of Sscs on three challenging datasets and demonstrate that our method significantly outperforms current state-of-the-art approaches, particularly achieving a 6.35% improvement in terms of R1@0.5 on the Charades-STA dataset.