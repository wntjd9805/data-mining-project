This paper introduces a new framework called OadTR, which is based on Transformers, for online action detection. The framework addresses the limitations of Recurrent Neural Networks (RNN) in capturing long-range temporal structure due to non-parallelism and gradient vanishing. OadTR consists of an encoder and a decoder. The encoder incorporates a task token to capture relationships and global interactions between historical observations. The decoder extracts auxiliary information by aggregating anticipated future clip representations. By encoding historical information and predicting future context simultaneously, OadTR is able to recognize current actions. The effectiveness of OadTR is evaluated on three challenging datasets, namely HDD, TVSeries, and THUMOS14. Experimental results demonstrate that OadTR achieves higher training and inference speeds compared to RNN-based approaches and outperforms state-of-the-art methods in terms of both mean Average Precision (mAP) and mean class Average Precision (mcAP). The code for OadTR is available at https://github.com/wangxiang1230/OadTR.