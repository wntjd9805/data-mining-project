This paper presents a novel method for learning representations of multimodal data using contrastive losses. The traditional approach of contrasting different modalities fails to capture the synergies between modalities that could be beneficial for downstream tasks. Another approach of concatenating modalities into a tuple and contrasting positive and negative correspondences only considers the stronger modalities, neglecting the weaker ones. To address these limitations, we propose a new contrastive learning objective called TupleInfoNCE. This objective not only contrasts tuples based on positive and negative correspondences but also creates new negative tuples using modalities from different scenes. By training with these additional negatives, the learning model is encouraged to explore the correspondences among modalities within the same tuple, ensuring that weak modalities are not overlooked. We provide a theoretical justification based on mutual information for the effectiveness of this approach and propose an optimization algorithm for generating positive and negative samples to maximize training efficiency. Experimental results demonstrate that TupleInfoNCE outperforms previous state-of-the-art methods on three different downstream tasks.