Transformer architectures have become widely used in natural language processing and are now being applied to computer vision tasks. However, in human pose estimation, convolutional architectures still dominate. This study introduces PoseFormer, a transformer-based approach for 3D human pose estimation in videos that does not involve convolutional architectures. Inspired by recent advancements in vision transformers, PoseFormer utilizes a spatial-temporal transformer structure to effectively model the relationships between human joints within each frame and the temporal correlations across frames. This enables accurate 3D human pose estimation for the center frame. The performance of PoseFormer is evaluated on two standard benchmark datasets, Human3.6M and MPI-INF-3DHP, through quantitative and qualitative analysis. The results demonstrate that PoseFormer achieves state-of-the-art performance on both datasets. The code for PoseFormer is available at https://github.com/zczcwh/PoseFormer.