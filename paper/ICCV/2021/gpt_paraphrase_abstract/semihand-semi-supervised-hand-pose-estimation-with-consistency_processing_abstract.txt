We introduce SemiHand, a semi-supervised framework for estimating 3D hand poses from monocular images. Our approach involves pre-training the model on labeled synthetic data and fine-tuning it on unlabeled real-world data using pseudo-labeling and consistency training. We employ various data augmentation techniques, a consistency regularizer, label correction, and sample selection to improve the accuracy of RGB-based 3D hand pose estimation. Specifically, we approximate hand masks from hand poses to introduce cross-modal consistency and use semantic predictions to guide the predicted poses. Additionally, we incorporate pose registration as label correction to ensure the biomechanical feasibility of hand bone lengths. Experimental results demonstrate that our method achieves significant improvement on real-world datasets after fine-tuning.