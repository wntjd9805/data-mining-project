Despite the extensive research on modeling human motions, synthesizing realistic and controllable sequences remains a challenging task. This paper aims to generate infinite variations of realistic 3D human motion sequences based on semantic action labels. Previous work has focused on predicting future motions based on existing pose sequences, while this work focuses on generating motions of a given type and duration. The proposed method trains an action-conditioned generative model using 3D human motion data and a Transformer-based encoder-decoder architecture with a VAE objective. The use of SMPL as a parameterization of the human body allows for better modeling of interaction with the environment. The paper introduces a positional encoding approach to the Transformer model, which enables the generation of variable-length sequences without regressing to the mean pose. The proposed model also creates an action-conditioned sequence-level embedding, which improves performance over baselines. The paper addresses the challenge of limited motion capture data by relying on monocular motion estimation methods, presenting promising results on a 40-category action recognition dataset. The action-conditioned generative model can augment existing motion capture datasets and provides a source of diverse motion synthesis for applications such as action recognition. The paper provides a comprehensive ablation study of the architecture and loss components and demonstrates state-of-the-art performance on multiple datasets. Two use cases, action recognition, and motion capture denoising, are illustrated to showcase the capabilities of the proposed model. The code for the model is available on the project page.