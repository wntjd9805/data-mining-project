Estimating 3D shape from RGB or depth images is a challenging task in computer vision due to ambiguity. The classical approach involves taking images from different viewpoints and generating a 3D representation using techniques like truncated signed distance functions (TSDF). However, this approach is often exhaustive and lacks the ability to create watertight surface reconstructions. Researchers have explored learning-based approaches to reconstruct 3D shapes, such as completing partial reconstructions and predicting scenes or objects from a single depth image.In this paper, we propose a method for 3D shape prediction with instance segmentation for multiple objects from a single depth image. Our method aims to quickly estimate 3D occupancy and enable interactive tasks like object manipulation. We introduce a variational autoencoder (VAE) whose latent space is learned from stable scenes under physics simulation, allowing for better reasoning about shape and instance decomposition in occluded regions. Conditioned on depth, the VAE learns to predict 3D shape and instances, completing occluded regions using its latents. We further refine the reconstruction for collision-free decomposition.Our method is trained on randomly assembled piles of 3D parametric shapes and can generate 3D shape, instance segmentation, and reconstruction proposals for occluded regions in one pass. It can also integrate multiple views for improved reconstruction. We demonstrate the application of our method for non-disruptive grasping using a robot arm.Overall, our contributions include a depth-conditioned VAE for stacked object scenes, a center voting scheme for class-agnostic 3D instance segmentation, and a shape refinement procedure for generating a compact scene representation of parametric shapes.