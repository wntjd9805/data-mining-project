Object detection is an important problem in computer vision, and while 2D object detection has made significant progress with deep learning, it is still insufficient for applications that require 3D spatial information like autonomous driving. In recent years, there has been growing interest in 3D object detection, which detects objects as 3D bounding boxes. However, most methods rely on expensive LiDAR sensors for depth information, while monocular 3D object detection methods use low computation and energy cost to infer depth from monocular images. The core challenge of monocular 3D object detection is inferring the distance of objects in the absence of explicit depth information. Geometric priors, such as object physical size, scene layout, and imaging process of cameras, can be exploited to recover the distance. Some methods use factors like 2D keypoints, 2D bounding boxes, and object physical size to estimate pose or distance, while others directly learn a mapping from input images to distance. Our proposed method introduces a novel geometry-based distance decomposition, abstracting objects as vertical lines and their visual projection as the projection of these lines, to recover distance based on imaging geometry. This decomposition is interpretable, easy to estimate, and maintains self-consistency between predicted heights, leading to robust distance prediction. It also enables us to trace and interpret the causes of distance uncertainty and can generalize to images with different camera intrinsics. Our method achieves state-of-the-art performance on the monocular 3D object detection and Bird's Eye View tasks of the KITTI dataset and has a compact architecture that simplifies training and inference.