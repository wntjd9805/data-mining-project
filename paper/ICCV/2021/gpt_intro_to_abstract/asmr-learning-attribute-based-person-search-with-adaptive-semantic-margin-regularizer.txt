Person search is the task of finding people from a large set of images given a query describing their appearances. Existing person search techniques often require an image of the target person as the query, which is not always accessible in real-world scenarios. To address this issue, this paper proposes a method for person search using text attributes as the query. The query is given as a set of pre-defined attributes indicating traits of the target person, such as gender and clothing. The use of attributes as the query introduces challenges due to the limited descriptive capability of attributes and the modality gap between images and person categories. Previous work on attribute-based person search has attempted to reduce the modality gap through adversarial training or hierarchical embeddings. However, these methods have limitations in terms of stability, computational heaviness, and inference efficiency. This paper introduces a new attribute-based person search method that overcomes these limitations. The proposed method learns a joint embedding space of images and person categories through simple encoder networks. A person category, represented as a binary vector indicating the presence of attributes, is used as the query and projected onto the joint embedding space to retrieve images. The main contribution of this work is a new loss function that aligns images with their corresponding person categories in the embedding space. The loss function determines the margin between person categories adaptively based on their distance in the attribute space. It also uses a weighted Hamming metric to focus on important attributes. The proposed method achieves state-of-the-art performance on three benchmark datasets without requiring adversarial training or extra networks. The main contributions of this work are the novel cross-modal embedding loss, the efficient architecture and retrieval pipeline, and the state-of-the-art results on benchmark datasets.