Despite the accuracy of Deep Neural Networks (DNNs), deploying them on devices with limited resources and power constraints remains challenging. In this paper, we propose a method called Bit-Mixer to address this problem. Bit-Mixer trains a meta-quantized network that can switch to any quantization level for any layer during testing. We introduce transitional batch-norms to compensate for distribution shifts between different bit-widths, and a 3-stage optimization process to train the meta-network. Our method allows for a finer granularity of quantization at a layer level, offering more optimal trade-offs between efficiency and memory requirements. We conducted ablation studies and analyzed the inter-dependencies between accuracy and quantization level. We also extensively evaluated the accuracy of our proposed method across different architectures and model sizes. The results show the effectiveness of Bit-Mixer in achieving efficient and accurate DNN deployment on resource-constrained devices.