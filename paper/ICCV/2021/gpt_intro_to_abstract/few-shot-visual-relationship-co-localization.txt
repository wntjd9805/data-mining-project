Visual Relationship Co-localization (VRC) is a task in computer vision that aims to localize visual subject-object pairs connected by a common predicate in a set of images. In this paper, we introduce VRC as a novel task and propose a framework for performing few-shot VRC using the meta-learning paradigm. Unlike object co-localization, VRC is challenging because visual relationships can have different appearances, and it requires both visual and semantic interpretation. We formulate VRC as a labeling problem and define an objective function to optimize the localization of subject-object pairs connected by a latent predicate. We train our model on a variety of bags with different predicates to generalize to new bags. Our proposed framework achieves impressive performance on public datasets and contributes to holistic scene interpretation. Our work showcases the potential applications of VRC and highlights the effectiveness of the meta-learning approach for few-shot visual relationship co-localization.