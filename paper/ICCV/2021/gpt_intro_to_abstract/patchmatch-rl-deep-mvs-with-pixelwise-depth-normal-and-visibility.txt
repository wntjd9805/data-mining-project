Multi-view stereo (MVS) is a technique used to reconstruct a 3D scene from a series of RGB images with known camera poses. It has numerous applications in various fields including robotics, self-driving cars, infrastructure inspection, and mapping. Non-learning based MVS methods have evolved to support pixelwise estimates of depths, normals, and source view selection using PatchMatch based iterative optimization and cross-image consistency checks. On the other hand, recent learning-based MVS methods tend to use frontal plane sweeps and evaluate the same set of depth candidates for each pixel based on the same images. These methods utilize trainable photometric scores and cost-volume regularization, resulting in excellent performance with dense cameras and small depth ranges. However, the pixelwise non-learning based approach outperforms the learning-based methods for scenes with large depth ranges and slanted surfaces observed with sparser wide-baseline views. In this paper, we propose PatchMatch-RL, an end-to-end trainable PatchMatch-based MVS approach that combines the advantages of trainable costs and regularizations with pixelwise estimates of depth, normal, and visibility. We demonstrate that our method achieves smoother and more complete depth and normal map estimation compared to existing approaches. We also address challenges such as non-differentiable iterative sampling and hard decisions in PatchMatch optimization and pixelwise view selection, as well as the estimation of 3D normals of pixels. Additionally, we propose a recurrent cost regularization technique for global inference. Our main contributions include a reinforcement learning approach for end-to-end training within PatchMatch optimization, the incorporation of normal estimates in learning-based MVS, and the development of depth/normal regularization techniques. Experimental results show that our system outperforms other learning-based methods on benchmark datasets and our ablation study confirms the importance of pixelwise normal and view selection estimates.