Analyzing natural images that contain multiple objects and complex interactions between them is a challenging task in computer vision. This paper focuses on the task of scene graph generation (SGG), which involves detecting and predicting objects and the relationships between them in the form of a scene graph. SGG is crucial for various applications in computer vision and language, such as visual question answering, image captioning, and retrieval. This paper introduces a sequential and conditional approach to SGG inspired by the way humans understand images. The proposed method utilizes a Seq2Seq model based on Transformers, which leverages sequential conditioning to improve prediction accuracy. The model is designed to adjust its beliefs about future predictions by sequentially incorporating already predicted relationships. One limitation of previous SGG methods is the lack of alignment between the common evaluation metrics (recall and mean recall) and the training objective (cross-entropy loss). This paper addresses this limitation by introducing a reinforcement learning (RL) training strategy that enables the direct optimization of the target metrics. This RL approach allows for more accurate estimation of the action-value function for the model.Experimental results demonstrate that the proposed method achieves state-of-the-art performance on both recall and mean recall metrics while maintaining computational efficiency during training and inference. Overall, this paper contributes to the field of SGG by proposing a novel sequential and conditional approach, introducing an RL-based training strategy, and achieving improved performance on evaluation metrics.