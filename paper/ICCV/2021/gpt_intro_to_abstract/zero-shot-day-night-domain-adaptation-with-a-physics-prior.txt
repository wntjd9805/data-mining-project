Deep image recognition methods often struggle with illumination shifts caused by different recording conditions. This limitation is particularly problematic in safety-critical computer vision applications such as autonomous driving. Traditional approaches to address this issue involve unsupervised domain adaptation, where the training set is from one domain and the test set is from another. However, obtaining and labeling test data can be expensive and time-consuming. In this paper, we propose an alternative approach that leverages prior knowledge as a visual inductive bias to tackle the domain shift. Specifically, we introduce a novel zero-shot domain adaptation method that exploits learnable photometric invariant features as a physics-based visual inductive prior. By normalizing feature map activations in a data-free setting, we can reduce the detrimental effect of distribution shift caused by illumination changes. To achieve this, we implement a trainable Color Invariant Convolution (CI-Conv) layer that acts as the input layer to any Convolutional Neural Network (CNN). Our experiments demonstrate the effectiveness of CI-Conv in reducing the activation distribution shift and improving performance in day-night domain adaptation tasks related to autonomous driving.