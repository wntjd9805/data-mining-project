This paper introduces the problem of 6D pose estimation, which aims to predict the orientation and location of an object in 3D space from a canonical frame. This task is important for various applications in computer vision, such as robotics grasping and augmented reality. Traditional methods for 6D pose estimation rely on handcrafted features and RGB images to establish correspondence between input and canonical images. However, these methods are sensitive to illumination variations, occlusions, and cluttered backgrounds.Inspired by the success of deep neural networks in detection and recognition tasks, recent research has explored the use of deep learning for 6D pose estimation. Some approaches use single-stage regression methods or key-point based methods. While these methods have achieved remarkable improvements in accuracy, they still heavily rely on textures and are not robust to noise and incompleteness in depth information.To address these limitations, this paper proposes a novel deep learning approach called Graph Convolutional Network with Point Refinement (PR-GCN) for 6D pose estimation. The approach consists of two modules: the Point Refinement Network (PRN) and the Multi-Modal Fusion Graph Convolutional Network (MMF-GCN). The PRN is used to polish the noisy and incomplete point cloud generated from the depth map, while the MMF-GCN integrates RGB-D clues through local information propagation in a graph convolutional network. The proposed PR-GCN approach achieves state-of-the-art performance on three benchmark datasets and is shown to be well-generalized to other frameworks.The contributions of this paper are threefold: first, the PR-GCN approach enhances depth representation and multi-modal combination for 6D pose estimation; second, the PRN module introduces a novel point cloud refinement method using a regularized multi-resolution regression loss; third, the MMF-GCN module captures local geometry-aware inter-modality correlation for RGB-D fusion. Overall, the proposed approach addresses the limitations of current RGB-D pose estimation methods and achieves improved accuracy and robustness.