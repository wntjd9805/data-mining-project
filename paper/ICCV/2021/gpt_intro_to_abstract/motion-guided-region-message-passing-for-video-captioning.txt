Automatically generating descriptive sentences for videos, known as video captioning, has gained significant attention in both the computer vision and natural language processing communities. Recent advancements in video captioning methods have focused on extracting fine-grained spatial information within video frames. However, the use of object detectors to extract localized object features can be computationally inefficient and may not generalize well to certain types of video contents. In this paper, we propose a new method called Recurrent Region Attention to extract multiple diverse regions from each video frame and design Motion Guided Cross-Frame Message Passing to encourage interaction and information communication among regions of consecutive frames. We also introduce an Adjusted Temporal Graph Decoder to update high-order temporal relations among video features. These techniques allow us to fully utilize the semantic content in grid features and achieve state-of-the-art video captioning performance on popular datasets. Our contributions include the development of a new method of extracting and encoding fine-grained spatial information, a more flexible captioning decoder, and achieving improved performance on multiple datasets.