Modern image generators are typically designed to synthesize pictures of a fixed size and aspect ratio. However, the real world extends beyond the boundaries of captured photographs, leading to recent developments in architectures that can produce infinitely large images or images that partially extrapolate outside their boundary. Previous work on infinite image generation focused on homogeneous texture-like patterns and did not explore the generation of complex scenes like nature or city landscapes. Generating globally consistent frames in scenes that span multiple images is a critical challenge. Existing approaches include fitting a separate model per scene, conditioning the generation process on a global latent vector, or predicting spatial latent codes autoregressively. However, these approaches have limitations in terms of variations in style and semantics, extrapolation capabilities, and inference speed. In this work, we propose a model that generates multiple global latent codes independently and connects them to achieve global consistency and diversity in the generation process. Our model is based on coordinate-based decoders and uses Fourier coordinate embeddings to provide crucial positional information. We utilize a modified version of the StyleGAN2 architecture with a redesigned adaptive instance normalization layer to work with the coordinate-based latent vectors. Our model is trained in an unsupervised manner using a dataset of unrelated square image crops, allowing it to connect unrelated scenes into a single panorama. We evaluate our approach on multiple datasets and demonstrate superior performance in terms of infinite image quality and generation speed.