Convolutional neural networks (CNNs) have achieved exceptional performance in visual tasks like image recognition and detection. However, these networks and their results are still difficult to explain, limiting their application in areas like self-driving cars and cancer diagnosis. In recent years, more interpretable methods have been proposed to open the black box of neural networks. One intuitive strategy is to visualize the feature representation within a CNN, but there remains a gap between network visualization and semantic interpretations. This study focuses on constructing interpretable basis concepts within a CNN architecture, with characteristics such as informativeness, diversity, and discriminativeness. Researchers use high-level features in deep neural networks to represent inputs with basis concepts. However, existing methods often suffer from entangled basis concepts, which hampers classification performance. To address this, the authors introduce the use of a Grassmann manifold to construct disentangled basis concepts, where each category has its own orthogonal concept subset. Prototypical high-level patches of original images are then used to represent these concepts. The resulting interpretable network architecture, called TesNet, is evaluated on bird species and car model identification tasks, and achieves state-of-the-art performance. The contributions of this work include the proposal of TesNet, which offers transparency and interpretability in prediction results, category-aware and disentangled basis concepts, the ability to preserve essential information, and a generic solution that can be integrated into various CNN architectures.