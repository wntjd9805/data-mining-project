Text-based Person Re-Identification (Re-ID) is a challenging task in computer science that aims to retrieve corresponding person images based on textual descriptions. In recent years, fully-supervised textual-visual embedding methods have made significant progress in this area. These methods utilize identity loss to reduce intra-class variations and are supervised by automatically generated positive or negative labels. However, these methods heavily rely on identity annotations, which are costly and time-consuming. In this paper, we propose a weakly supervised text-based person Re-ID approach where only text-image pairs are available without any identity annotations. This approach addresses two main challenges: mitigating the effect of intra-class variations in both textual and visual modalities and resolving cross-modal matching ambiguity. To overcome these challenges, we introduce a Cross-Modal Mutual Training (CMMT) framework. This framework utilizes clustering and pseudo label refinement to reduce intra-class variations and employs a Text-IoU Guided Cross-Modal Projection Matching (Text-IoU CMPM) loss to mitigate cross-modal matching ambiguity. Additionally, a Text-IoU guided Hard Sample Mining (Text-IoU HSM) method is presented to learn discriminative visual-textual joint embeddings. Our experiments demonstrate the superiority of the proposed approach for weakly supervised text-based person Re-ID, even outperforming fully supervised methods. This work is the first to address the problem of weakly supervised text-based person Re-ID and contributes novel solutions to enhance textual-visual representation learning in this area.