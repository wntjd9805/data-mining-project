This paper introduces a new approach to change captioning in computer vision, specifically focusing on identifying relevant semantic changes between images with extreme viewpoint shifts. The authors propose a new network model that can pinpoint semantic changes without relying on any additional data or information. They also introduce a cycle consistency module to improve the captioning quality by matching the generated caption with the encoded after image. The authors evaluate their approach using a synthetic dataset, CLEVR-DC, which simulates extreme viewpoint shifts. Their experiments show that their method outperforms existing state-of-the-art methods on CLEVR-DC as well as two other datasets. Overall, this work contributes a robust and novel solution to change captioning in scenarios with extreme viewpoint shifts.