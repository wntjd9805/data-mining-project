Event cameras, also known as neuromorphic cameras, are a new type of visual sensors that detect and record spatio-temporal changes for each pixel. They possess superior properties such as high temporal resolution, high dynamic range, and low power consumption, making them suitable for challenging scenarios like HDR scenes and high-speed moving scenes. However, the sparse and unstructured nature of event streams makes them inconvenient for observation and post-processing. To overcome this limitation, researchers have proposed converting event streams to sequential intensity frames, which can be processed using existing frame-based algorithms. Deep learning techniques, particularly convolutional neural networks (CNNs), have been successful in computer vision tasks. Several studies have applied deep learning to reconstruct event-based videos, achieving impressive results. However, CNN-based models have limitations in capturing long-range dependencies and dealing with internal variation in texture, shape, and size. To address these limitations, non-local operations, attention gate modules, and the Transformer architecture have been proposed. In this paper, we introduce Event Transformer Network (ET-Net), which combines a hybrid CNN-Transformer architecture to leverage both detailed spatial information and global context for high-speed video reconstruction from event cameras. We also propose a Token Pyramid Aggregation module to perform multi-scale token integration, improving reconstruction accuracy. Extensive experiments on existing event camera datasets demonstrate the effectiveness of ET-Net compared to CNN-based methods. Our contributions include proposing the ET-Net framework, introducing the Token Pyramid Aggregation module, and demonstrating the superior performance of our method through comprehensive experiments.