In recent years, generative adversarial networks (GANs) have gained popularity due to their ability to generate high-quality and photo-realistic content. They have been successfully applied in various tasks such as image and video editing, image inpainting, image super resolution, and more. Initially, GANs were thought to simply memorize the training data, but recent research has shown that they can learn non-trivial semantic information about objects. For example, GANs can learn the concept of pose and generate objects with different orientations. In this paper, we explore the extent to which GANs can learn the distinction between foreground and background in images. We propose an unsupervised segmentation framework that leverages the capabilities of GANs to separate foreground and background without explicit mask-level supervision. Our framework is based on StyleGAN, a state-of-the-art GAN architecture trained on individual object classes. We augment the StyleGAN generator with a segmentation branch and split it into foreground and background networks, allowing us to generate soft segmentation masks for the foreground object. Additionally, we propose a training strategy that utilizes a fully trained network to generate unsupervised segmentation masks. This enables us to create synthetic datasets for segmentation, which can be used to train other segmentation networks. Our contributions include a novel architecture modification, a loss function, and a training strategy for splitting StyleGAN into foreground and background networks, as well as generating synthetic datasets for segmentation.