Deep metric learning is a technique that learns an embedding space in which the similarity between data samples is encoded based on their embeddings. This is achieved using deep neural networks that map the data samples to the embedding space, ensuring that samples from the same class are close while samples from different classes are far apart. Deep metric learning has achieved state-of-the-art results in various tasks such as face recognition, re-identification, and image retrieval. Several loss functions have been formulated to train these deep networks, including contrastive loss and triplet loss, but these methods only consider a few samples at a time.One limitation of existing approaches is that not all samples in a batch contribute to the loss term, as many of them already satisfy the constraints in the loss function. To address this issue, some methods have proposed using hard samples, which are positive samples that are far away and negative samples that are closer. However, existing hard negative mining and generation methods have certain limitations, including biased mappings and increased training complexity.To overcome these limitations, this paper proposes a novel approach called LoOp (optimal hard negatives in the embedding space). LoOp finds optimal hard negative samples by minimizing the distance between curves joining pairs of positives and negatives in the embedding space. Unlike mining-based methods, LoOp does not neglect any samples, and unlike generation-based methods, it does not increase training complexity or optimization difficulty. The proposed approach can be easily integrated with various metric learning-based losses and offers improved performance compared to existing mining and generation methods on benchmark datasets.The contributions of this paper include the introduction of the LoOp approach, which finds optimal hard negatives in the embedding space and maximizes the contribution of each tuple in computing the pair-based metric learning loss. The approach also presents a solution to the general problem of finding the minimum distance between two bounded curves. Additionally, a theoretical analysis of the loss function is provided, demonstrating the optimality of LoOp. Experimental results on benchmark datasets show that the proposed approach outperforms state-of-the-art methods in terms of accuracy and efficiency.