This paper introduces the need for explainable deep learning models in computer science. The authors argue that explainability is crucial for establishing trust and accountability in safety-critical applications such as healthcare and autonomous vehicles. They propose the use of natural language explanations (NLEs) as a way to explain the decision-making process of these models. The authors compare different models that generate NLEs for vision-language tasks and highlight the limitations of existing datasets and evaluation frameworks in the field. To address these limitations, the authors propose the e-ViL benchmark, consisting of three datasets of human-written NLEs, and introduce the e-SNLI-VE dataset, currently the largest dataset for VL-NLE. They also present a novel model called e-UG that outperforms existing models and conduct a study on the correlation between automatic NLG metrics and human evaluation of NLEs. Overall, this paper aims to advance the field of explainable deep learning models by providing a comprehensive evaluation framework and demonstrating the effectiveness of NLEs in vision-language tasks.