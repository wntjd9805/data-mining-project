In recent years, optimizing the hyperplane in the latent space to improve representation learning has been widely adopted in computer science. This approach has led to significant advancements in million-level face recognition tasks. However, as the scale of training datasets increases, more complex data distributions and real-world noise are introduced, making it challenging to find an optimal hyperplane to accurately describe the latent space. This paper focuses on exploring the potential problem of optimizing a single hyperplane in a latent space with noisy data. It is observed that the gradient conflict generated by local training samples collapses the hyperplane optimization, and optimizing local outliers often introduces bias to other samples. To address this problem, the authors propose a method called Switchable K-class Hyperplanes (SKH) that combines multiple hyperplanes and optimizes them in a switchable manner. Experimental results demonstrate that SKH effectively mitigates the effects of label noise and achieves state-of-the-art performance in noise-robust representation learning. Additionally, a posterior data cleaning strategy is proposed to further improve the performance by dropping intra high-confident noise samples and merging inter samples with high-similar centers in different K-class hyperplanes. Overall, this paper introduces a novel approach for noise-robust representation learning and demonstrates its superior performance in face recognition tasks.