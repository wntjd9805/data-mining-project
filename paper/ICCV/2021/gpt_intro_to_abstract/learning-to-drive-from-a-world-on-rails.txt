Vision-based autonomous driving presents challenges in perceiving, understanding, and interacting with the environment based on incomplete experiences. Existing approaches often rely on imitating expert actions, but such trajectories are biased and safety-critical observations are rare. Model-free reinforcement learning offers a solution, but the required sample complexity is prohibitive. In this paper, we propose a method that learns a navigation policy without making mistakes. We build a world-model using pre-recorded trajectories to simulate actions and estimate action-value functions. We then train a reactive visuomotor policy using the predicted impact of actions. The core challenge lies in building an expressive world-model. We simplify the environment assumption by factorizing the model into an agent-specific component and a passive world. We evaluate our method on the CARLA simulator and achieve better driving scores with significantly less training data. Our method also outperforms prior methods on benchmark tests and demonstrates generalization to different environments.