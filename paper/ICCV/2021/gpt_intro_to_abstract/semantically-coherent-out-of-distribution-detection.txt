Deep learning models are widely used for visual recognition tasks, but they have two major drawbacks: 1) their performance suffers when test data has a large covariate shift from the training data, and 2) they tend to incorrectly classify test images that have a semantic shift from the training data. These weaknesses reduce the trustworthiness of the models and hinder their deployment in high-risk applications. To address this problem, out-of-distribution (OOD) detection methods aim to differentiate and reject test samples that have covariate shifts or semantic shifts, in order to prevent unreliable predictions. Existing OOD detection methods focus on calibrating the distribution of the softmax layer, using generative models, ensemble methods, or temperature scaling. Other solutions involve collecting large amounts of OOD samples to learn the discrepancy between in-distribution (ID) and OOD data during training. However, we argue that the current OOD detection benchmarks have some irrationality in the way they split ID and OOD data. Our proposed solution is to re-design the benchmarks to be semantically coherent by organizing the ID/OOD sets based on semantics and focusing on real images where the covariate shift can be ignored. We also leverage an external unlabeled set contaminated by a portion of ID samples to improve the performance of OOD detection. Our proposed unsupervised dual grouping framework enhances the semantic expression ability of the model and dynamically separates ID and OOD samples in the unlabeled set. Our experiments demonstrate that our approach achieves state-of-the-art performance on semantically coherent OOD benchmarks.