Image inpainting is a fundamental computer vision task that aims to synthesize meaningful and plausible contents in missing regions. Conventional image inpainting methods utilize background information to fill the missing regions but perform poorly for high-resolution images or complex scenes. In recent years, deep neural networks have made significant breakthroughs in image inpainting. However, most deep inpainting networks based on auto-encoder architecture suffer from two major drawbacks: information loss due to downsampling and the inability to integrate texture and structure information effectively. To address these issues, we propose a parallel multi-resolution fusion network for image inpainting. Our network architecture consists of four parallel branches with different resolutions, allowing the integration of both local and global information. We also introduce mask-aware representation fusion and attention-guided representation fusion to enhance the coherence between structure and texture. Experimental results demonstrate that our method outperforms state-of-the-art approaches in terms of image restoration quality and coherence between structure and texture. The contributions of our work include introducing the parallel multi-resolution network architecture, proposing novel representation fusion techniques, and demonstrating superior performance compared to existing methods.