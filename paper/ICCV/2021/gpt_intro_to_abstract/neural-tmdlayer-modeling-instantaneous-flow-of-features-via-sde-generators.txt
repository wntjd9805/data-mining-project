In this paper, we examine a deep neural network model trained using a specific update rule. We then introduce a slightly general form of the update formula that includes a data transformation matrix. We explore the concept of data transformation and its implications for training deep neural networks. We discuss how certain approaches eliminate the need for explicit data augmentation by considering the entire orbit of each sample during training. We expand on this idea by considering the repeated application of a transformation on data points, generating a discrete sequence. We argue that recent results in the stochastic differential equations literature show that the dynamics of this sequence can be characterized by the inﬁnitesimal generator of the process, which can be estimated using ﬁnite data. We propose a TMDlayer that can be easily incorporated into deep learning pipelines, allowing for richer domain information, regularization, or augmentation. We introduce the concept of a stochastic process inspired layer that can be utilized in the feature space and is fully adaptive to the input. We show that this layer can effectively model the time varying/stochastic property of the data/features and can be exploited in various applications.