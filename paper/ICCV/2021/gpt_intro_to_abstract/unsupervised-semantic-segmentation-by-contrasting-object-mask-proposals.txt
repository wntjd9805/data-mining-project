The problem of semantic segmentation, which involves assigning dense semantic labels to images, is crucial in computer vision for various applications. Existing methods for semantic segmentation typically rely on fully-annotated datasets, which require significant time and effort to create. Weakly-supervised and semi-supervised methods have been proposed as alternatives, but they still require some form of human supervision. In this paper, we propose a novel approach to semantic segmentation using self-supervised representation learning. Instead of relying on ground-truth annotations, we aim to learn pixel-level representations that are discriminative with respect to semantic classes. This allows us to cluster pixels into semantic groups using unsupervised techniques, or further fine-tune the representations with a limited number of annotated examples under a semi-supervised or transfer learning setup. We primarily focus on the fully unsupervised setup in this paper, but also include fine-tuning experiments. Our approach is based on self-supervised techniques, which have shown promise in learning effective visual representations without external supervision. We employ an unsupervised saliency estimator to mine object mask proposals as a mid-level visual prior, and then use a contrastive framework to learn pixel embeddings. Our proposed method deviates from recent works that rely on proxy tasks or end-to-end clustering. We argue for the importance of mid-level visual priors that incorporate object-level information, which is different from earlier works that focus on low-level vision tasks. Our method is the first to successfully tackle the semantic segmentation task on challenging datasets like PASCAL in a fully unsupervised setting. We also report promising results when transferring our representations to other datasets, highlighting the usefulness of mid-level visual priors in self-supervised representation learning.