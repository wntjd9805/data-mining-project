Recent advancements in computer graphics and virtual reality have led to an increasing demand for generating realistic human motions for virtual avatars. Co-speech gestures play a vital role in expressing oneself and creating more vivid interactions in both the real and virtual worlds. However, the problem of generating human motions from audio clips is challenging due to the complex relationship between speech and gestures. Different individuals may perform different gestures while speaking the same words based on their mental and physical states. Existing algorithms for audio to body dynamics synthesis have limitations, such as lack of diversity in generated motions. This is attributed to the one-to-one mapping assumption, which fails to capture the one-to-many relationship between speech and gestures. To address this, we propose a co-speech gesture generation model that explicitly models the multimodality of motion generation. Our model incorporates a motion-specific latent code along with a shared code extracted from audio input to enable the regression of diverse motion targets for the same audio. To ensure the decoder utilizes deterministic information from the shared code, we introduce random noise to the motion-specific code during training. We also propose a relaxed motion loss that penalizes deviations from target motion to encourage the motion-specific code to adjust the final motion while respecting the control of the shared code. Experimental results demonstrate the effectiveness of our method in generating multimodal gestures and its superiority over state-of-the-art approaches in both 2D and 3D gesture generation tasks. Furthermore, our method can be easily applied to motion synthesis from annotations by utilizing the corresponding motion-specific code for pre-defined actions in the timeline. Overall, our contributions improve the quality and diversity of co-speech gesture generation, providing more realistic and expressive interactions in virtual environments.