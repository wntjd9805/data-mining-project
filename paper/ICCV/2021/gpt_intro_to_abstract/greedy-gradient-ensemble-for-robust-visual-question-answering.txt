Visual Question Answering (VQA) is a challenging task in computer science that requires both language-aware reasoning and image understanding. While neural networks have shown remarkable results on benchmark datasets, recent studies have revealed that many VQA methods rely on existing biases in the datasets and do not consider exact vision information. These biases lead to poor generalization ability and a lack of proper visual evidence. To address this issue, current solutions include ensemble-based, grounding-based, and counterfactual-based methods. However, these methods do not fully leverage both vision and language information. In this paper, we propose the Greedy Gradient Ensemble (GGE), a new model-agnostic de-bias framework that ensembles biased models and the base model using a gradient descent approach. Our method makes use of the over-fitting phenomenon in deep learning, allowing the base model to learn from more ideal data distribution and focus on difficult examples. We provide empirical evidence and an ablation study to demonstrate the generalization ability and effectiveness of GGE. Additionally, we analyze the two-fold nature of language bias in VQA and propose GGE as a solution to address both distribution bias and shortcut bias. Our method achieves state-of-the-art performance on the VQA-CP dataset and provides better visual evidence for answer decisions. The code for GGE is available for further exploration.