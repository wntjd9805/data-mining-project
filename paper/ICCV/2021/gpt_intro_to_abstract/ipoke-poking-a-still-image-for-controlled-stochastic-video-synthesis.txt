In this paper, we present a model for exercising local control over the kinematics of objects observed in an image. Our goal is to synthesize multiple videos that show the different plausible future dynamics resulting from a local manipulation of the scene. We propose a generative, stochastic approach that learns a representation of object kinematics from unannotated video data. Our method allows for specific but efficient control of kinematics, bridging the gap between uncontrolled future frame prediction and densely controlled video synthesis. We capture the ambiguity in global object articulation by learning a dedicated latent kinematic representation. Our model formulates the synthesis problem as an invertible mapping between object kinematics and video sequences, conditioned on the observed object manipulation. Our stochastic latent representation allows for sampling and transferring diverse kinematic realizations, fitting the sparse local user input to synthesize plausible video sequences. We evaluate our model on four different datasets, including complex and highly articulated objects, such as humans and plants. Quantitative and qualitative experiments demonstrate the capability of our model to predict and synthesize plausible and diverse object articulations inferred from local user control.