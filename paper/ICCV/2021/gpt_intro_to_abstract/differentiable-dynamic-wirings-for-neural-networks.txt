Deep neural networks have revolutionized feature learning in computer science by replacing labor-intensive feature engineering with automated feature learning. To achieve superior performance, previous research focused on increasing the capacity of models through the addition of layers or expanding convolutional sizes. However, this approach comes with a significant increase in computational cost, which limits the deployment of these models in practical scenarios.In this paper, we propose a different approach that involves adding input-dependent modules to neural networks instead of adding more computational burden. These modules increase the model capacity by accommodating data variance. We draw inspiration from the mammalian brain mechanism, where neurons are linked by synapses and activate to varying degrees when perceiving external information.Our method, called Differentiable Dynamic Wirings (DDW), optimizes the connectivity of neural networks based on inputs. We reformulate the network into a directed acyclic graph, where nodes represent convolution blocks and edges indicate connections. Unlike randomly wired neural networks, where graphs are generated using predefined generators, we rewire the graph as a complete graph to allow for more possible connections. The task of finding the most suitable connectivity for each sample becomes equivalent to finding the optimal sub-graph in the complete graph.To adjust the contribution of different nodes to the feature representation, we assign weights to the edges in the graph. These weights are generated dynamically for each input using an additional module called a router. During inference, only crucial connections are maintained, creating different paths for different instances. By generating connectivity through non-linear functions determined by routers, our method enhances the representation power of the networks.Importantly, DDW does not increase the depth or width of the network, leading to only a slight increase in computational cost. To facilitate training, we represent the network connection of each sample as an adjacency matrix and use a buffer mechanism to cache the matrices of a sample batch during training. This allows for convenient aggregation of feature maps in the forward pass and computation of gradients in the backward pass.Experimental results demonstrate the effectiveness of DDW in improving the performance of both human-designed networks, such as MobileNetV2, ResNet, and ResNeXt, and automatically searched architectures like RegNet. DDW achieves solid improvements with only a slight increase in parameters and computational cost. It exhibits good generalization ability in ImageNet classification and COCO object detection tasks. Overall, DDW offers an easy and memory-efficient approach to exploit the model capacity of neural networks by dynamically adjusting their connectivity based on inputs.