Multi-task learning (MTL) has gained significant research interest as it has been shown to outperform single-task learning (STL) on various computer vision problems. Current MTL approaches typically follow a soft-parameter or hard-parameter sharing principle to exploit task correlations. While soft-parameter sharing encourages shared knowledge through cross-talks among tasks, it is complex to design and does not scale well to multiple tasks. On the other hand, hard-parameter sharing reduces parameter size and promotes inference speed but requires task balancing. Existing heuristics for task balancing are limited in their effectiveness. To address this, we propose privileged task learning (PTL) which introduces parameters in the gradient space to achieve versatile Pareto critical points based on user preference. We optimize PTL using an efficient hybrid-block coordinate descent algorithm. Experimental results on synthetic and real datasets demonstrate the effectiveness of PTL in finding solutions that have satisfactory overall performance across all tasks, with a particular emphasis on the target task.