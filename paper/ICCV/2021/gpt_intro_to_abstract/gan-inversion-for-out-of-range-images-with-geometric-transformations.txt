Generative adversarial networks (GANs) have made significant progress in synthesizing high-quality and diverse images. Recent GAN models, such as StyleGAN, StyleGAN2, and BigGAN, are capable of producing realistic-looking images. GAN inversion, which maps a real image into the latent space of a pre-trained GAN model, has attracted attention for semantically editing real images. However, finding an in-domain latent code that aligns with the pre-trained GAN model is critical for successful semantic editing. Unfortunately, current GAN inversion methods struggle when dealing with out-of-domain images that have geometric transformations not present in the training data. This limitation severely restricts the applicability of semantic editing using GAN inversion. This paper proposes a novel GAN inversion approach called Base-Detail Invert (BDInvert) to address this issue. BDInvert inverts geometrically unaligned images and covers various types of editing for out-of-range images that are not supported by previous methods. The approach introduces a new latent space called F/W +, consisting of subspaces for geometric transformations (F) and semantic manipulations (W +). An optimization-based approach combined with a regularization method based on an encoder network is used to accurately reconstruct and semantically edit out-of-range real images. The proposed BDInvert provides a solution for semantic editing of real images with geometric transformations that are not aligned with the pre-trained GAN model, supporting a wider range of realistic image synthesis and manipulation.