Deep learning-based methods, such as convolutional neural networks (CNNs), have achieved impressive results in terms of accuracy. These methods have been applied to various applications such as classification, detection, and segmentation. One approach, known as Knowledge Distillation (KD), transfers the knowledge of a teacher model to a less-ability student model during the learning process. Many studies on KD have focused on effectively guiding the teacher's soft probabilities or outputs to the student. However, there has been a lack of consideration for the diversity of ensemble classifiers teaching the student, especially when there is a large gap between the teacher and the student. This paper proposes a novel densely guided knowledge distillation (DGKD) method using multiple intermediate-sized assistant models to bridge the gap between a teacher and a student. Unlike previous methods, DGKD guides knowledge from all previously learned higher-level assistant models, including the teacher. This method successfully alleviates the error avalanche problem that occurs when multiple assistant models are sequentially trained. The proposed DGKD algorithm also incorporates stochastic learning, where a fraction of the guided knowledge is randomly removed from the trainers during student training. Experimental results demonstrate the significant accuracy improvement achieved by the proposed method compared to other KD methods.