Our paper focuses on the importance of understanding interactions in deep learning systems. In the past, explainable deep learning has often looked at individual features or concepts in isolation, but in the real world, decisions are influenced by complex relations. For example, when approaching a yield sign, a driver needs to consider the presence of passing cars before making a decision. We propose two contributions: T-NID, an algorithm for statistical interaction effects that outperforms existing methods, and Taylor-CAM, an explanatory tool that extends Grad-CAM to capture higher-order interactions. We also provide visualizations of Taylor-CAM's explanations, which enable a better understanding of relational reasoning in visual question-answering tasks. Our work emphasizes the need for an interactional approach in explaining deep learning systems.