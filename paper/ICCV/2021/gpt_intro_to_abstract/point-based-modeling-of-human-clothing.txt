Modeling realistic clothing in 3D is a challenging task with various practical applications such as virtual clothing try-on and enhancing the realism of human avatars. This task is difficult due to the wide variations in clothing geometry and appearance. In this paper, we propose a new approach to modeling clothing using point clouds. By utilizing a synthetic dataset of simulated clothing, we learn a joint geometric model of diverse outfits represented by a latent code vector. A deep neural network, called the draping network, is then used to predict the point cloud that represents the draped clothing on a given human body geometry. Our model is advantageous as it can reproduce diverse outfits with varying topology using a single latent space and draping network. We extend our approach to include appearance modeling using differentiable rendering and neural point-based graphics. By capturing the photometric properties of the outfit from a video sequence, we can fit neural descriptors and a rendering network to the outfit code, allowing for realistic re-rendering on new bodies and poses. Experimental evaluations demonstrate the effectiveness of our geometric model in capturing deformable geometry and our full approach in capturing both geometry and appearance from videos. We will make our code and model available at https://saic-violet.github.io/point-based-clothing/.