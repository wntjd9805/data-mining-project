Machine learning techniques, particularly those based on deep neural networks, have shown great potential in various applications due to their ability to learn from high-quality training data. However, collecting a large number of accurately labeled data samples is challenging in certain problem domains, such as extreme weather conditions or natural disasters. One possible solution is to use data from similar domains to train the model and fine-tune it with limited target domain data through domain adaptation. However, obtaining a large amount of high-quality labeled data from these source domains is still difficult and expensive. To address this data insufficiency, we propose leveraging low-cost weak annotators that can generate large quantities of labeled data based on certain rules, heuristics, or other methods, even if they may be inaccurate to some extent. Our objective is to learn an accurate classifier for the target domain using the labeled target data, unlabeled source data, and weak annotators. Therefore, we develop a theoretical analysis on the error bound of the trained classifier and propose a Weak Adaptation Learning (WAL) method to lower this error bound. WAL involves obtaining cross-domain representations of the source and target domain data, estimating the classification error between the weak annotator and the optimal classifier in the target domain, re-labeling the data based on this estimation, and using the newly-relabeled data to train a better classifier. Our contributions include addressing the challenge of data insufficiency in domain adaptation, providing a theoretical analysis and a WAL method to improve classifier accuracy, and conducting experiments to validate our approach compared to various baselines.