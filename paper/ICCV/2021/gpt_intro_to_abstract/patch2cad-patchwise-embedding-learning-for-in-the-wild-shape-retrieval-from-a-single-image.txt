Many visual perception tasks require understanding the decomposition of a scene into objects and their semantic meaning. While 2D object recognition has made significant progress, there is still a limited understanding of 3D attributes such as shape and structure. In this paper, we propose Patch2CAD, a method that constructs a joint embedding space between images and CAD models to improve 3D perception from a single RGB image. Patch2CAD leverages mid-level geometric relations by establishing similarity of patches of images to patches of object geometry. This enables CAD retrieval based on part similarities, improving generalizability. We demonstrate the effectiveness of Patch2CAD on the ScanNet and Pix3D datasets, outperforming state-of-the-art methods in 3D shape perception. Our main contribution is the patch-based learning of a joint embedding space between 2D images and 3D CAD models, which enables more robust correspondences and enhances CAD retrievals with no exact matches in the database.