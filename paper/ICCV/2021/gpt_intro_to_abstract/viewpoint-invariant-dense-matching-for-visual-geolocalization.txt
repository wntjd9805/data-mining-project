Visual geolocalization is an important problem in various fields, such as robotics and augmented reality. Current methods use global image representations for image retrieval, but these representations lack robustness to occlusions and small overlaps. Sparse local invariant features have been effective in general visual matching tasks, but they struggle with visual differences in images of the same place. To address this, we propose a new dense matching method called GeoWarp, which learns an invariance to geometric transformations. We integrate this method into retrieval pipelines and use a lightweight warping regression module that can be trained in a self-supervised manner. We also introduce weakly supervised losses to improve robustness. Experimental results on standard datasets demonstrate that our method significantly improves accuracy across a variety of retrieval networks.