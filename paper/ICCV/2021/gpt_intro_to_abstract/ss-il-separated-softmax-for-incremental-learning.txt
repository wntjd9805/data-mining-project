Incremental learning, where an agent continues to learn as new training data arrives, is a significant challenge in artificial intelligence and machine learning. Storing all training data in one batch becomes unrealistic for memory- and computation-constrained applications. Deep neural network (DNN) models have been considered a viable option for incremental learning, but they suffer from catastrophic forgetting, where the performance on old data degrades after fine-tuning with new data. This paper focuses on DNN-based class incremental learning (CIL) and specifically addresses the issue of severe data imbalance between training data points for new and old classes. Exemplar-memory based approaches have been effective in mitigating the forgetting problem, but they require additional post-processing steps to correct the bias. However, previous methods lack systematic analyses and proper justifications. In this paper, an analysis of the bias is provided, and a method called Separated-Softmax for Incremental Learning (SS-IL) is proposed. SS-IL includes a separated softmax output layer to mitigate the imbalanced penalization of output probabilities for old classes and uses Task-wise Knowledge Distillation (TKD) to preserve task-wise knowledge without preserving the prediction bias. Extensive experimental validations on large-scale CIL benchmarks demonstrate that SS-IL achieves state-of-the-art accuracy by balancing prediction scores across old and new classes without additional post-processing steps. The contributions of this paper are the proposal of the separated softmax layer, the investigation of the role of GKD and TKD in CIL, and extensive experimental validation.