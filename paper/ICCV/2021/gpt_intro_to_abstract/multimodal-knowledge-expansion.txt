Deep neural networks and supervised learning have achieved remarkable success in computer vision and computer audition. The increasing popularity of multimodal data collection devices and the accessibility of the internet have resulted in a large amount of unlabeled multimodal data becoming available. This paper aims to address the challenge of transferring a unimodal network to unlabeled multimodal data. The naive solution of using the corresponding modality of the unlabeled data overlooks the valuable information provided by other modalities. While using multimodal data can improve model robustness, the process of developing a multimodal network with supervised learning is labor-intensive due to the need for human labeling. To tackle this challenge, the proposed approach, referred to as multimodal knowledge expansion (MKE), utilizes knowledge distillation to enable a multimodal network to learn from the unlabeled data with minimal human effort. This paper presents the MKE framework and demonstrates its effectiveness in leveraging unlabeled multimodal data for improved performance.