Self-supervised learning from geometric constraints has been successful in learning tasks like depth and ego-motion directly from unlabeled videos. However, tasks such as semantic segmentation and object detection rely on human-defined labels. Using synthetic datasets as an alternative to manual labeling has shown promise. Simulators can generate large quantities of diverse data with accurate labels, including for optical and scene flow, object detection, tracking, action recognition, and semantic segmentation. However, there is a significant domain gap between synthetic and real data due to differences in content, scene geometry, physics, appearance, or rendering artifacts.Unsupervised Domain Adaptation (UDA) aims to improve generalization across this domain gap without real-world labels. Most UDA methods use adversarial learning or self-training with pseudo-labels. While these methods have shown improvements, they have limitations such as the need to learn multiple networks or being hard to train. Few works have explored image-level self-supervised proxy tasks for UDA in semantic segmentation, with limited success.In this paper, we propose self-supervised monocular depth as a proxy task for UDA in semantic segmentation. We introduce a multi-task mixed-batch training method that combines synthetic supervision with a real-world self-supervised depth estimation objective to learn a domain-invariant encoder. Our method, called GUDA (Geometric Unsupervised Domain Adaptation), outperforms other UDA methods for semantic segmentation. We also demonstrate that combining GUDA with self-trained pseudo-labels achieves state-of-the-art performance on the SYNTHIA-to-Cityscapes benchmark. Furthermore, GUDA scales well with both the quantity and quality of synthetic data and achieves state-of-the-art monocular depth estimation in the real-world domain.