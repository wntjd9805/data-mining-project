Visually decoding mental images, also known as mental image reconstruction (MIR), has gained attention in various fields including computational neuroscience, computational biology, and computer vision. The dominant approach for MIR has been to reconstruct mental images directly from brain activity recorded using functional magnetic resonance imaging (fMRI) or electroencephalography (EEG). Recently, there has been interest in using human eye fixations as an alternative sensing modality for MIR, despite the challenges it presents. While previous works have focused on predicting the target of visual search from fixations and image features, only a few have attempted to visually decode the search target. This paper presents a new approach that reconstructs specific instances of mental images from eye fixations and auxiliary images, with a focus on facial images. The proposed method encodes multiple facial images viewed by an observer using a Siamese CNN encoder and then fuses them into a single feature vector using a novel scoring network. The mental image is decoded from this representation by comparing neural activation maps with human fixations. The scarcity of gaze data is addressed by training the encoder and decoder on large image datasets and only requiring joint image and gaze data for training the scoring network. The contributions of this work include an annotated dataset of human fixations on synthesized face images during face recognition, a novel problem formulation and method for synthesizing photofits (visual reconstructions) of mental face images from eye fixations, and successful experiments demonstrating gaze-based reconstruction of mental face images.