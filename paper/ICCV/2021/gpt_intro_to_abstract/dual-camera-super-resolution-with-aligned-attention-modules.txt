Most smartphone manufacturers use an asymmetric-cameras system with fixed-focal lenses instead of variable-focal lenses for optical zoom. This is due to limited assembly space. However, dual cameras with wide-angle and telephoto lenses often suffer from spatial misalignment and color discrepancy. In this paper, we explore reference-based super-resolution (RefSR) with a focus on dual-camera super-resolution (DCSR). The key challenges in RefSR are establishing correspondences between low-resolution and reference images (feature warping) and integrating reference information to improve image quality (feature fusion). Previous approaches have used either patch-wise matching or pixel-wise alignment to search and utilize correlated patterns from reference images. However, these approaches have limitations in handling spatial misalignment and often produce blurry images. Additionally, previous RefSR approaches are not directly applicable to high-resolution images captured by smartphones. This paper proposes a deep RefSR method for DCSR that addresses these limitations. We introduce an aligned attention module for explicit patch matching and learning inter-patch transformations to alleviate spatial misalignment. We also impose a fidelity loss on reference images and perform explicit high-frequency fusion to improve super-resolution quality. Our method outperforms state-of-the-art approaches in terms of both qualitative and quantitative performance. We also propose a self-supervised domain adaptation scheme to adapt our method to real-world images. The contributions of this paper include the exploration of real-world dual-camera super-resolution, the introduction of the aligned attention and adaptive fusion modules, and the demonstration of the importance of fidelity loss and explicit high-frequency fusion in super-resolution quality.