With the advancement of deep learning technologies, computer vision models have achieved human-level performance in tasks such as face detection and recognition. However, these technologies also raise privacy concerns, as they can be exploited to collect private information. Adversarial examples, which involve adding imperceptible perturbations to deceive deep learning systems, have been proposed as a defense mechanism against these threats. This paper focuses on physical adversarial attacks, where attacks are conducted by retaking input images using a camera. In particular, the authors propose a method to craft physical adversarial patches for object detectors by leveraging generative adversarial networks (GANs) pretrained on real-world images. By minimizing the detection score of a target object and applying a clipping strategy, the proposed method generates natural-looking adversarial patches with acceptable attack performance. Experimental results indicate that the proposed method outperforms state-of-the-art baselines in terms of realism and inconspicuousness. The paper also addresses the security implications of natural-looking adversarial patches, highlighting the need for further analysis. The main contributions of this work include the use of pretrained deep generative models to craft natural adversarial patches and a comprehensive evaluation under different settings.