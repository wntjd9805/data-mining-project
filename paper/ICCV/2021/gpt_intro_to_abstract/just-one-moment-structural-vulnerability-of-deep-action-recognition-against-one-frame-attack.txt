Human action recognition has become a widely studied research topic in recent years, particularly with the development of deep network-based algorithms. In this paper, we focus on the modeling of temporal information in video clips for action recognition. While various approaches have been explored, 3D CNN-based models are commonly used for their improved performance and efficiency. However, it has been observed that deep learning-based algorithms are vulnerable to adversarial attacks, where imperceptible perturbations are added to input data to mislead the model. While the vulnerability of deep image classification systems against adversarial attacks has been extensively studied, there are not many studies on the vulnerability of video-based deep action recognition systems. In this paper, we explore the structural vulnerability of recent CNN-based deep action recognition models and demonstrate that perturbing a single vulnerable frame of a video clip can significantly degrade the accuracy of these models. Our analysis shows that the efforts to efficiently model temporal information in these models induce the vulnerability issue. We also propose a one frame attack method that can fool state-of-the-art video-based action recognition models with high fooling rates, while remaining inconspicuous. Furthermore, we explore the use of video-agnostic universal perturbation based on the one frame attack, showing its effectiveness in fooling other input video clips and its applicability to time-invariant scenarios. Our findings highlight the importance of considering the structural vulnerability of deep models in action recognition and raise security concerns in this domain.