Visual media can be categorized into various types, including live photos, binocular images or videos, and dual-view images or videos. Specialized devices or platforms are often required to view these types of visual media. However, in order to make the content compatible with common devices, it is necessary to generate corresponding monocular content for binocular media. Instead of simply discarding parts of the original content, a more preferable approach is to build a reversible transformation that allows the embedding of compatible content while still enabling the restoration of the original content when needed. This reversible transformation can also help reduce storage costs and transmission bandwidth. As a result, there has been increasing interest in studying reversible image conversion (RIC) tasks to establish a reversible transformation between visual content and an embedding image.RIC tasks present challenges because they require the embedding of rich information within a single image, which can lead to information loss. Previous works have employed encoder-decoder frameworks, which have limited ability to capture the lost information. To address this, the proposed framework, called Invertible Image Conversion Net (IICNet), utilizes invertible neural networks (INNs) as a strictly invertible embedding module. To further enhance the representation capability of INNs, a channel squeeze layer and a relation module are introduced. The channel squeeze layer allows for flexible dimension reduction, while the relation module enhances the capture of cross-image relations.With its strong embedding capacity and generic module design, IICNet is capable of handling different content types without relying on task-specific techniques. The framework also allows for lower-resolution embedding, enabling higher compression rates. Concrete examples of applications are provided, such as embedding video frames into a lower-resolution image and mononizing binocular images.This paper presents the first generic framework, IICNet, for different RIC tasks. The framework is supported by extensive experiments on five tasks, including two newly-explored tasks. Quantitative and qualitative results demonstrate that IICNet outperforms existing methods on the studied tasks. Ablation studies are conducted to analyze the network modules and loss functions. Additional information and demo results can be found in the supplementary materials.