Understanding video story involves analyzing and simulating human vision, language, thinking, and behavior, which is a significant challenge to current machine learning technology. Recently, joint video-and-language understanding has received increased attention, with several video-and-language tasks proposed, such as video captioning, text-to-video temporal grounding, and video question answering. Video-and-Language Inference (VLI) is a recently proposed task where a model needs to infer whether a given statement based on video content entails or contradicts a video clip with aligned subtitles. To support the study of VLI, a large-scale dataset called VIOLIN (VIdeO-and-Language INference) is introduced. Compared to other tasks, VLI is more challenging and requires sophisticated reasoning skills, such as interpreting human emotions and relations, understanding events, and inferring causal relations. This paper proposes a novel adaptive hierarchical graph reasoning with semantic coherence approach to overcome the challenges of VLI. The approach introduces an adaptive graph construction mechanism to identify the multiple semantic meanings of a statement and an adaptive hierarchical graph network (AHGN) to jointly reason over video and subtitles and model complex social interactions. Additionally, a semantic coherence learning (SCL) method is introduced to encourage cross-modal semantic coherence. Experimental results demonstrate the effectiveness of the proposed approach, which significantly outperforms the baselines. The contributions of this paper include the proposed AHGN for joint reasoning, the SCL method to improve alignment and coherence, and the significant performance improvement over the baselines.