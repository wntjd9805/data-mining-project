Deep convolutional neural networks (CNNs) have achieved remarkable success in computer vision tasks, but they are vulnerable to adversarial attacks where malicious perturbations are added to fool the network. Physical world attacks, which involve manipulating pixels in an image, are particularly challenging. The universal adversarial patch attack, where a localized patch is generated to be effective regardless of the image or its placement, is the most widely adopted form of attack. While research on defending against this type of attack is limited, some approaches based on patch detection lack theoretical foundations and perform poorly against transparent attacks. Adversarial training is another defense strategy, but it requires high computational overhead and has not been shown to be feasible for large-scale datasets. In this paper, we propose a defending method based on a mathematical analysis of how universal adversarial patches impact deep feature representations. We observe that the norm of the feature vector at the patch position is significantly larger than other feature vectors, suggesting that a large norm feature vector can dominate the pooling result and mislead the CNN. We propose the use of Feature Norm Clipping (FNC) layers to restrict the norm of deep features and weaken the impact of adversarial patches. Experimental results on CIFAR10 and ImageNet datasets show significant improvements in robustness against white-box adversarial patches. Our proposed method is implemented in an end-to-end manner with low computational overhead, making it highly applicable to real-world visual classification systems based on CNNs.