Deep neural networks (DNN) have achieved remarkable success in various machine learning problems. However, their performance heavily relies on access to large amounts of accurately labeled training data, which is often time-consuming and cost-prohibitive to obtain. Additionally, conventional deep learning methods suffer from poor generalization on new, unlabeled datasets due to the domain shift issue. To address these challenges, domain adaptation (DA) methods aim to utilize knowledge from a label-rich domain (source) to assist in learning in a related but unlabeled domain (target). Existing DA methods mainly focus on reducing the cross-domain distribution discrepancy by aligning feature representations or using adversarial-based approaches. However, these methods often fail to emphasize relevant semantic information, leading to negative transfer. In this paper, we propose a new method called Semantic Concentration for Domain Adaptation (SCDA) which leverages dark knowledge and class-wise alignment of prediction distributions to suppress irrelevant semantic information and accentuate class objects. SCDA can easily be integrated as a regularizer into existing DA methods and achieves state-of-the-art results on multiple cross-domain benchmarks by effectively reducing irrelevant semantics during the adaptation process.