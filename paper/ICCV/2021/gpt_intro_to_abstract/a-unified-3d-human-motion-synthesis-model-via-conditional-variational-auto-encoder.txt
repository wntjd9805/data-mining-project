Generating realistic and plausible human body animation with specified actions is a challenging task in computer vision and graphics. Traditional methods rely on complex pose specifications, which are time-consuming and expensive to obtain. Recently, deep learning approaches have emerged to generate human motions, but many are limited to specific tasks or use different methods for different tasks. For example, while motion prediction approaches perform well in predicting future frames, they are not suited for tasks such as motion completion, interpolation, and spatial-temporal recovery. Furthermore, many methods focus on minimizing reconstruction error rather than considering motion diversity and human-likeness. In this paper, we propose a unified framework based on conditional variational autoencoders (CVAE) to handle various motion synthesis tasks while meeting different constraints and requirements. We introduce an Action-Adaptive Modulation (AAM) to control semantic motion styles and a cross-attention mechanism to exploit long-term context information. Our approach outperforms existing methods in terms of realism and plausibility on widely-used datasets.