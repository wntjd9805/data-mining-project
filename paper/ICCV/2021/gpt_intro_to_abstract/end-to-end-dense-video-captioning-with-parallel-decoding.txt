Video captioning, as a branch of video understanding, has gained attention in recent years. However, existing single-sentence captioning methods often generate bland and less informative sentences due to the long and untrimmed nature of realistic videos. To address this, dense video captioning (DVC) has been developed to localize and caption multiple events in a video, providing detailed and coherent descriptions. Previous DVC methods typically employ a two-stage "localize-then-describe" pipeline, which relies heavily on the quality of event proposals and includes hand-crafted components that hinder end-to-end caption generation. To overcome these limitations, this paper proposes a pure end-to-end DVC framework called Parallel Decoding Video Captioning (PDVC). Instead of the two-stage approach, PDVC directly feeds the intermediate representation for proposal generation into a parallel captioning head. This allows for the direct exploitation of inter-task association at the feature level, making feature representations more discriminative for event identification. PDVC treats DVC as a set prediction problem and uses two parallel prediction heads, namely the localization head and the captioning head, to decode frame features into event sets with their locations and corresponding captions. Additionally, PDVC introduces an event counter to accurately predict the number of events in the video, improving caption generation quality.The proposed PDVC framework is evaluated on two large-scale video benchmarks, ActivityNet Captions and YouCook2. Even with a lightweight caption head, PDVC achieves comparable performance against state-of-the-art methods that use attention-based LSTM or Transformer models. Furthermore, quantitative and qualitative analysis demonstrates that the paralleling decoding design of PDVC benefits the generated proposals. Even in a weakly supervised setting without location annotations, PDVC can implicitly learn location-aware features from captions.In summary, this paper makes three main contributions. Firstly, it presents a novel end-to-end DVC framework, PDVC, which simplifies the traditional pipeline and reduces reliance on hand-crafted components. Secondly, PDVC incorporates a novel event counter to estimate the number of events, enhancing the readability of generated captions. Lastly, extensive experiments demonstrate that PDVC achieves state-of-the-art performance on ActivityNet Captions and YouCook2 datasets.