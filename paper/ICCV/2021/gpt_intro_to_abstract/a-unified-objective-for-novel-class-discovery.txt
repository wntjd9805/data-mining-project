Deep learning has greatly advanced computer vision tasks, but the reliance on large annotated datasets poses challenges. Novel Class Discovery (NCD) has emerged as a solution to address these challenges by training a network to simultaneously classify known classes and discover unknown ones in unlabeled image sets. Most NCD methods involve supervised pretraining on labeled data followed by clustering on unlabeled data. However, these methods often require strong semantic similarity between labeled and unlabeled classes, and the optimization process is complex due to the combination of multiple objectives. In this paper, we propose a unified objective for NCD that eliminates the need for self-supervised pretraining and combines all objectives into a single loss function. We use a multi-view self-labeling strategy to generate pseudo-labels that can be treated as ground truth labels. Our unified framework enables the learning of a single model that can recognize both labeled and unlabeled classes. Experimental results demonstrate that our method outperforms existing approaches on public benchmarks, achieving significant improvements in accuracy. Furthermore, our approach is robust even when the proportion of labeled and unlabeled samples varies. Overall, our contributions simplify NCD and achieve superior performance in discovering novel classes.