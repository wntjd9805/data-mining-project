This paper introduces the concept of Generalized Source-free Domain Adaptation (G-SFDA) as a new paradigm for domain adaptation in deep neural networks. Traditional domain adaptation methods require access to labeled source data during the adaptation process, which is often not feasible in real-world applications. The SFDA setting, where only a source pretrained model is available, has gained attention recently, but existing methods focus on the target domain and may suffer from forgetting on old domains. To address this limitation, the authors propose a novel approach that aims to achieve good performance on both the source and target domains after adaptation. They introduce Local Structure Clustering (LSC) to group target features based on their nearest neighbors, ensuring similar predictions for semantically related features. They also propose Sparse Domain Attention (SDA) to activate different feature channels depending on the specific domain, thereby preserving source domain information during target adaptation. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on the VisDA benchmark while maintaining good performance on the source domain. Additionally, the authors extend their approach to Continual Source-free Domain Adaptation, showing its efficiency in handling multiple target domains. Overall, this work makes several contributions, including the introduction of G-SFDA, the use of LSC and SDA techniques for source-free domain adaptation, and the achievement of superior performance compared to existing methods on both source and target domains.