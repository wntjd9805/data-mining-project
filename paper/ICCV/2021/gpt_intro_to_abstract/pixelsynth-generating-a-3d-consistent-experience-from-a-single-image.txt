This paper introduces a novel approach for single-image scene synthesis, addressing the challenges associated with handling large view changes, generating consistent outputs, and modeling perspective changes. The authors propose a fusion of 3D reasoning and autoregressive modeling to achieve these goals. Their approach utilizes an autoregressive model for extrapolation and identifies a support set of extreme views, which are then lifted to 3D and added to a scene representation. Intermediate views can be re-rendered from this representation, ensuring consistency. The system incorporates adaptations of VQVAE2 and Locally Masked Convolutions for image completion and orderings. The authors evaluate their approach on standard datasets and demonstrate its superiority in terms of human judgments and standard metrics. The results suggest that their proposed approach outperforms existing methods for larger viewpoints and exhibits greater consistency in image generation. Overall, this paper presents a powerful and effective solution for single-image scene synthesis.