Modeling in computer vision has mainly relied on convolutional neural networks (CNNs), which have shown impressive performance on image classification tasks. However, the architecture evolution in natural language processing (NLP) has taken a different path, with the Transformer architecture becoming prevalent. The Transformer utilizes attention mechanisms to model long-range dependencies in data and has achieved great success in NLP. In this paper, the authors aim to expand the applicability of the Transformer architecture to computer vision, making it a general-purpose backbone similar to CNNs in vision tasks. The authors highlight the challenges in adapting the Transformer to the visual domain, such as the varying scale of visual elements and the higher resolution of images compared to text. To overcome these challenges, the authors propose the Swin Transformer, which constructs hierarchical feature maps and has linear computational complexity to image size. The Swin Transformer uses a shifted window approach to compute self-attention, enhancing modeling power and reducing latency. Experimental results show that the Swin Transformer outperforms previous models on image classification, object detection, and semantic segmentation tasks, demonstrating its potential as a unified architecture for both computer vision and NLP. The authors believe that this unified approach can lead to improved joint modeling of visual and textual signals and encourage further exploration in the community.