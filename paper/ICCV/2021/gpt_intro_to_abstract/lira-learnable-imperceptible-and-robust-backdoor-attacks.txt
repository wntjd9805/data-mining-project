Machine learning models, particularly deep neural networks (DNNs), have shown remarkable performance in a wide range of applications. However, recent research has revealed that these models are susceptible to adversarial attacks, which can compromise their integrity and reliability. Among these attacks, backdoor attacks have become a primary concern due to their potential to manipulate the model during the training phase. Backdoor triggers can be injected into a small portion of the training data, resulting in misclassifications when the model is exposed to inputs containing the trigger. These attacks are challenging to detect because the model behaves normally on clean inputs. In this paper, we propose a novel framework called Learnable Imperceptible and Robust Backdoor Attack (LIRA) that simultaneously generates invisible triggers and poisons the classifier. We formulate the problem as a constrained optimization task and present an efficient stochastic optimization algorithm to solve it. Our approach enables us to generate optimal triggers that are visually imperceptible and difficult to detect. We demonstrate the effectiveness of LIRA through extensive experiments, where it outperforms existing backdoor attacks in terms of attack success rate and stealthiness against defense mechanisms. Our contributions include the formulation of a unified optimization problem, the development of a stealthy trigger generation function, and achieving state-of-the-art attack performance and stealthiness.