3D surface reconstruction is a crucial task in computer vision and virtual reality applications. With the availability of 3D sensors, accurate 3D point clouds can be directly captured. However, reconstructing surfaces from unstructured point clouds of objects with complex shapes and varying properties is still challenging. Existing methods either focus on local surface recovery or global surface determination, but they are limited by the density of the point clouds. Deep learning-based approaches have shown promise in reconstructing mesh surfaces from moderately dense or sparse point clouds, but they face generalization issues and computational challenges for large scenes. In this paper, we propose a novel solution that utilizes point visibility in views to guide mesh reconstruction. By generating a large number of virtual views and using graph-cut based reconstruction, we can exploit unlimited visibility information for high-quality mesh generation. We address the challenges of visibility prediction in virtual views with varying sparsity and incident angles, using a network that incorporates partial convolution and depth completion. We also introduce an adaptive visibility weighting term to handle unfavored virtual rays. Our method combines traditional and learning-based approaches, resulting in better generalization, scalability, and robustness compared to existing methods. Our contributions include a surface reconstruction framework, a three-step network for visibility prediction, an adaptive visibility weighting term, and experimental results that demonstrate the effectiveness and superiority of our proposed method.