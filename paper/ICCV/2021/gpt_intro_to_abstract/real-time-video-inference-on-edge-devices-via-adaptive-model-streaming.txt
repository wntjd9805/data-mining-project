Real-time video inference plays a crucial role in various applications such as augmented reality, drone-based sensing, robotic vision, and autonomous driving. However, running state-of-the-art Deep Neural Network (DNN) models on low-powered edge devices or even on accelerators like Coral Edge TPU and NVIDIA Jetson is prohibitively expensive and cannot be done in real-time. To address this challenge, we propose Adaptive Model Streaming (AMS), a new approach that offloads knowledge distillation to a remote server communicating with the edge device over the network. AMS continually adapts a small student model running on the edge device to boost its accuracy for the specific video in real-time. The edge device periodically sends sample video frames to the remote server, which fine-tunes the student model based on the frames and sends the updated model back to the edge device. Communication overhead is a major challenge in performing knowledge distillation over the network. To mitigate this, we present techniques to reduce both downlink and uplink bandwidth usage for AMS. For downlink bandwidth, we develop a coordinate-descent algorithm to train and send only a small fraction of the model parameters in each update. For uplink bandwidth, we propose algorithms that dynamically adjust the frame sampling rate at the edge device based on scene changes in the video. Through evaluations using real-time semantic segmentation, we demonstrate that AMS significantly improves model accuracy compared to pretraining or customizing without video-specific techniques. It also achieves higher accuracy than a remote inference baseline with reduced bandwidth requirements. Our results highlight the effectiveness and efficiency of AMS for real-time video inference on edge devices.