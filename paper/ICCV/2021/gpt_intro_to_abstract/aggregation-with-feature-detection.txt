Representation learning is essential for computer vision applications, and deep networks have been widely used for tasks such as image classification, object detection, and semantic segmentation. Features from deeper layers in these networks typically contain more high-level semantics and are better suited for prediction. However, it has been observed that information loss can occur during the forward propagation of data in deep networks. To address this issue, skip connections or dense connections have been proposed to fuse features from previous layers, ensuring that information from all depths is utilized. Current methods mostly focus on determining which layers to combine but pay little attention to how to combine them. Existing approaches use element-wise sums or concatenation/convolution to fuse features, but these methods cannot capture the inconsistency or incompatibility between features. In this study, we propose a pipeline called DEtect-rePLAce (DEPLA) which can learn to detect the most appropriate part of a feature for aggregation. Our approach reformulates the feature aggregation process as a two-stage process, first detecting where to update and then aggregating the selected locations using the detected features. By doing so, we retain only the relevant information in the summarized features, avoiding irrelevant and inconsistent patterns. Experimental results on ImageNet and COCO datasets demonstrate that our method, when applied to popular backbone networks like ResNet, SE-ResNet, FishNet, and Feature Pyramid Network, consistently improves accuracy for various computer vision tasks compared to existing methods. Our approach outperforms other ResNet-based counterparts in terms of accuracy while maintaining similar computational complexity.