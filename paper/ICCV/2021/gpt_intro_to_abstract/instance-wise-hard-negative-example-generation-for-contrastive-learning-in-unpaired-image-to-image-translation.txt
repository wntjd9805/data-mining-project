Abstract:Image-to-image translation has become an important task in various applications such as style transfer, domain adaptation, and image colorization. Recent methods have focused on the unpaired setting, where paired training data is not easily available. Cycle-consistency loss has been commonly used to ensure consistency between the source images and the generated images. However, these methods often suffer from poor quality and inconsistent content preservation. In this paper, we propose a novel framework called NEGCUT to improve the performance of contrastive learning for unpaired image-to-image translation. We introduce a negative generator that produces hard negative examples, which are challenging enough to push positive examples closer to the query examples. We also address the issue of mode collapse by adding diversity loss to encourage the generator to produce diverse hard negative samples. The framework involves an adversarial training between the encoder network and the negative generator, resulting in fine-grained correspondence of structures and textures. Experimental results on benchmark datasets demonstrate the superiority of our method, achieving new state-of-the-art performance with visually better generated images and consistent detailed correspondence.