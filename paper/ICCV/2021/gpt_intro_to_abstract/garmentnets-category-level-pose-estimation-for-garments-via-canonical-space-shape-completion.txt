This paper introduces GarmentNets, an end-to-end neural network that aims to estimate the full configuration of garments from partial observations. Garments possess unique properties that make them difficult for machines to perceive and interact with, such as their infinite degree of freedom (DoF) and thin structures. Traditional approaches in garment perception rely on simplifying assumptions, limiting their ability to generalize to new instances. To address these challenges, GarmentNets leverages a normalized coordinate space for each garment category using a canonical human pose to enable category-level generalization. The algorithm performs shape completion under the canonical pose to handle self-occlusions and proposes a novel 3D shape representation using winding number fields (WNF) to accurately represent thin cloth structures. The study focuses on garment perception in the context of robot manipulation, which presents greater challenges due to the increased number of possible configurations and self-occlusions. By leveraging simple robot interactions, the algorithm can reduce the configuration space and be applied to realistic robot manipulation tasks. The experiments demonstrate that the trained model can generalize to novel garment instances and real-world images, making it the first solution to enable category-level full configuration estimation of garments from partial observations.