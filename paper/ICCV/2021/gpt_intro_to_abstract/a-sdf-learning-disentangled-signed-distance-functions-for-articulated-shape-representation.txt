Abstract:Modeling articulated objects is crucial in various fields such as virtual and augmented reality, object functional understanding, and robotic manipulation. While recent works have focused on training deep networks to estimate per-part poses and joint angle parameters of known articulated object categories, these approaches are limited in their ability to predict object shape after interactions. In this paper, we propose Articulated Signed Distance Functions (A-SDF), a differentiable category-level representation for reconstructing and predicting the 3D shape of articulated objects under different articulations. Our approach is based on the deep implicit Signed Distance Functions and addresses the limitations of existing models that struggle to encode articulation variation and generalize to unseen instances with unknown joint angles. To enhance generalization, we separate the encoding of shape and articulation, and design two separate networks: a shape embedding network and an articulation network. We enforce shared shape codes for instances with different joint angles, and during inference, we infer the shape code and adjust the articulation code to generate instances at different articulations. We also propose a Test-Time Adaptation (TTA) approach to adapt our model to out-of-distribution and unseen data. Our experiments demonstrate the effectiveness of our approach in generating articulated object shapes, interpolating and extrapolating between shapes, and synthesizing shapes from real-world depth images. Our work represents the first attempt to address the problem of generic articulated object synthesis in the implicit representation context and shows significant improvements over existing baselines.