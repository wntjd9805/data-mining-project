Cross-modal retrieval, which aims to search for semantically similar samples from different modalities, has gained increasing attention. With the rapid growth of video content on the internet, accurate video-text retrieval has become a significant challenge. In this paper, we focus on learning video-text retrieval and hope to inspire other cross-modal tasks. Previous works have shown the effectiveness of transformer models in learning high-level video representations. Existing approaches for cross-modal learning can be categorized into two-stream, single-stream, and dual-stream architectures. However, these architectures are not suitable for large-scale cross-modal retrieval tasks. To address this, we propose a new approach called Momentum Cross-modal Contrast (MCC) that utilizes memory banks to store negative representations and improve training efficiency. Additionally, we introduce Hierarchical Transformer (HiT) with Momentum Contrast, which performs Hierarchical Cross-modal Contrastive Matching and Momentum Cross-modal Contrast. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed methods.