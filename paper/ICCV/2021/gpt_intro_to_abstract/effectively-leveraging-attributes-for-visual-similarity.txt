Learning similarity metrics between images is a central problem in computer vision with various applications such as face recognition, image retrieval, image classification, and fashion recommendation. Prior work often relies on predicting attributes for each image and using these predictions to compute image similarity. However, this approach can result in a loss of important information about how attributes are expressed, limiting fine-grained reasoning about different attribute manifestations. To address this issue, we propose the Pairwise Attribute-informed similarity Network (PAN) that uses a joint representation of two images to compute multiple disentangled similarity scores, each corresponding to an attribute. PAN also incorporates relevance weights for each similarity score in the final similarity prediction, enabling more accurate and nuanced similarity reasoning. We demonstrate the effectiveness of PAN on tasks such as fashion compatibility prediction, few-shot image classification, and image retrieval. PAN outperforms prior methods that incorporate attribute information and shows robust performance across diverse tasks. Additionally, we explore different methods of using attributes for supervising similarity predictions and analyze the contributions of the training procedures in PAN's performance.