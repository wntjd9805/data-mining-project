Synthesizing natural and realistic facial video sequences driven by audio inputs is a challenging problem with various applications such as digital humans, chatting robots, and virtual video conferences. Previous methods relied on professional artist modeling or complex motion capture systems, limiting their application to high-end areas such as movies and games. Recently, deep-learning-based techniques using generative adversarial networks (GANs) have been proposed, but they still face challenges in accurately relating audio signals and facial expressions. Additionally, existing methods often only focus on rendering the mouth part or are fixed by static head pose, making them unsuitable for advanced talking head editing tasks. In this paper, we propose AD-NeRF, an audio-driven neural radiance fields model that tackles these limitations. Instead of relying on intermediate representations, we directly map audio features to dynamic neural radiance fields to represent the scenes of talking heads. Our model leverages the power of neural rendering techniques to capture fine-scale facial components and generate higher quality images compared to existing GAN-based methods. The volumetric representation also allows for global deformation adjustments and takes into account head pose and upper body movement. Our method takes a short video sequence with audio and video tracks as input and constructs an audio-conditional implicit function to store the neural radiance fields. Furthermore, we decompose the neural radiance fields into two branches, one for the head and one for the torso, to generate more natural talking head results. Our approach also enables pose-manipulation and background-replacement, making it suitable for virtual reality applications. The contributions of our method include the direct mapping from audio features to dynamic neural radiance fields, the decomposition of the fields to model head and torso deformation separately, and the capability for advanced talking head video editing.