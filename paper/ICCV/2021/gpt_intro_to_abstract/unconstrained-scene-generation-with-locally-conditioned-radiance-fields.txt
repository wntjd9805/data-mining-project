Spatial understanding involves inferring the geometry and appearance of a scene from different viewpoints or orientations based on limited observations. Existing approaches for 3D view synthesis can interpolate between observed views, but they struggle to extrapolate and infer unobserved parts of the scene due to their inability to learn a prior over scenes. This limitation hinders their performance in tasks such as inpainting disocclusions or synthesizing views beyond the observed boundaries. Conditional auto-encoder models can extrapolate views of simple objects, but they overfit to training viewpoints and produce blurry renderings when extrapolating far from observed views.To overcome these limitations, we propose Generative Scene Networks (GSN), a generative model for scenes that allows view synthesis for a freely moving camera in an open environment. GSN is the first generative model to represent unconstrained scene-level radiance fields. We demonstrate that decomposing a latent code into a grid of locally conditioned radiance fields leads to an expressive and robust scene representation, outperforming strong baselines. GSN can also infer observations from arbitrary cameras given a sparse set of observations by inverting the generative model. Moreover, GSN can be trained on multiple scenes to learn a rich scene prior, ensuring smooth and consistent rendered trajectories while maintaining scene coherence.The proposed GSN model has various potential applications in machine learning and computer vision, including model-based reinforcement learning, SLAM, content creation, and adaptation for augmented reality/virtual reality or immersive 3D photography. We showcase the capabilities of GSN through experiments and comparisons with related work, highlighting its effectiveness in generating realistic scenes and addressing challenges in spatial understanding.