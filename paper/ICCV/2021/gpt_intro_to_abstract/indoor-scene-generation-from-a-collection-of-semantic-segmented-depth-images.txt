Real-world indoor scenes are diverse in terms of object placement, type, and layout, making manually modeling them a time-consuming task. Automatic scene generation techniques have been developed to model object properties and distributions in real scenes and generate new 3D scenes. Early methods use manual rules or simple statistical models, which are difficult to generalize. Recent deep learning-based methods learn object properties and layouts from a large collection of 3D scenes, but struggle to model detailed object shapes and layouts. This paper introduces a generative adversarial network (GAN) for 3D indoor scene generation. Unlike previous methods, the proposed GAN represents scenes with semantic scene volumes, where each voxel is labeled as empty or belonging to a specific object type. From this representation, a volumetric GAN model is designed to generate semantic scene volumes, and CAD models are substituted for volumetric object instances to create the final 3D indoor scene. The proposed method learns from a collection of semantic-segmented depth images, rather than complete 3D scenes, and employs a multi-view discriminator to improve the generation of natural object layouts. Experimental evaluations are conducted on synthetic and real datasets to demonstrate the capabilities and advantages of the proposed method. This approach is the first to generate 3D indoor scenes from semantic-segmented depth images, reducing the workload for data acquisition and modeling, and improving the modeling of object shapes and layouts compared to existing methods.