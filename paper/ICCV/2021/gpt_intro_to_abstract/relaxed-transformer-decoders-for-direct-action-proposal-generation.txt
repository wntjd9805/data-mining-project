With the increasing availability of online videos, video understanding has emerged as an important problem in computer vision. While action recognition methods have been widely studied for classifying trimmed video clips into action labels, they cannot be directly applied to untrimmed web videos. Therefore, there is a need for temporal action detection techniques that can localize each action instance in long untrimmed videos. This task involves generating temporal action proposals and classifying them. Existing approaches for proposal generation either rely on dense anchor placement or boundary-based methods, both of which have limitations and require manual tuning. In this paper, we propose a direct action proposal generation framework using Transformers. Our approach models long-range temporal context to improve proposal generation and achieves more complete and precise localization results. We also streamline the pipeline by removing hand-crafted designs and achieve faster inference speeds. However, adapting the image detection Transformer architecture for videos presents challenges, including feature slowness and ambiguous temporal boundaries. To address these issues, we propose a Relaxed Transformer Decoder (RTD), which incorporates a customized boundary-attentive architecture, a relaxed matcher, and a three-branch detection head. Our experiments demonstrate that our method outperforms existing state-of-the-art methods on THUMOS14 and achieves comparable performance on ActivityNet-1.3 for both temporal action proposal generation and detection tasks.