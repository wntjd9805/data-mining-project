This paper addresses the task of synthesizing street-view panoramic video sequences from a single satellite image and given viewing locations. Existing methods for generating street-view images from satellite images lack temporal and geometric consistency, limiting their practical applications. In contrast, the proposed approach generates the entire scene in a 3D representation and establishes correspondence between the visible points and 2D frame pixels, ensuring natural consistency. A two-stage 3D generator is designed in a coarse-to-fine manner, utilizing different 3D convolutional neural networks. The synthesized results demonstrate temporal consistency. The contributions of this work include the first satellite-to-ground video synthesis method, a novel cross-view video synthesis approach that ensures spatial and temporal consistency by incorporating projective geometry constraints, and superior performance compared to baseline methods. The paper provides a newly-constructed dataset for evaluation and will make the source code and pre-trained models publicly available.