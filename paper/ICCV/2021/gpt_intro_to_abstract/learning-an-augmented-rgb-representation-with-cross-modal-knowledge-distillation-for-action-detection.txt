Action detection in untrimmed videos is a challenging task in computer vision, as it requires categorizing all frames corresponding to different actions. Two main challenges in action detection are handling composite action patterns and fine-grained details. This becomes even more difficult in real-world scenarios where actions are densely distributed and overlapping. To address these challenges, a commonly used approach is the two-stream network, which combines RGB with additional modalities like optical flow and 3D poses. However, this approach relies on the availability of multiple modalities and expensive processing resources, limiting its applicability in real-world applications. Previous studies have shown that cross-modal Knowledge Distillation (KD) can effectively avoid the need for computing additional modalities during test time while preserving their complementary information. However, most previous works have focused on classification tasks and short trimmed videos. In contrast, untrimmed videos contain complex temporal relations and rich sequential knowledge. Thus, distillation mechanisms tailored for classification tasks are insufficient for capturing the fine-grained details along the temporal dimension in untrimmed videos.In this paper, we propose a distillation framework for action detection in untrimmed videos that combines cross-modal information while using only the RGB stream at inference time. Our framework consists of a teacher-student network architecture and three new distillation losses dedicated to the action detection task. The first loss term, the Atomic KD loss, enables the RGB student network to mimic the feature representation of individual snippets from the teacher network in a contrastive manner. This loss promotes the transfer of sub-representations of actions within the video. However, distilling only atomic representations is not sufficient for learning discriminative action representation in untrimmed videos. Therefore, we introduce two loss terms for sequence-level KD to transfer cross-snippet relations between modalities. The Global Contextual Relation loss transfers contextual information, while the Boundary Saliency loss distills the boundary saliency from the teacher to the RGB student network.Our proposed distillation framework significantly improves the performance of the vanilla RGB baseline and achieves the performance of the two-stream network while using only the RGB stream at inference time. We evaluate our framework on five benchmark datasets and demonstrate its effectiveness and robustness. This work contributes to the field of cross-modal KD for action detection and introduces a novel formulation that incorporates sequential KD loss specifically for the action detection task.