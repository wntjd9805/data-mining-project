Object detection is the task of identifying objects in an image and predicting their category labels and bounding boxes. State-of-the-art neural detectors often use a detect-and-merge approach, where predictions are made on a set of region proposals or sliding windows, followed by a post-processing step to merge the detection results. However, this method is sub-optimal as the model optimization is not end-to-end. Recently, DETection TRansformer (DETR) was proposed as the first fully end-to-end object detector, using Transformer to directly output predictions without post-processing. However, DETR has a long training time, which is impractical for large applications. This paper focuses on accelerating the training process for DETR-like Transformer-based detectors. The authors analyze DETR's optimization difficulty and find that the slow convergence is mainly due to the cross-attention module. They also investigate an encoder-only version of DETR and observe improved performance for small objects but sub-optimal performance for large objects. Additionally, the instability of the bipartite matching in DETR's Hungarian loss contributes to slow convergence. To address these issues, the authors propose two models, TSP-FCOS and TSP-RCNN, which accelerate the training process by utilizing feature pyramids and introducing a new bipartite matching scheme. Evaluation on the COCO 2017 detection benchmark shows that the proposed methods not only converge faster than DETR but also achieve higher detection accuracy compared to DETR and other baselines.