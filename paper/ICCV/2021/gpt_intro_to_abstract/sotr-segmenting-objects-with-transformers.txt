Instance segmentation is a crucial task in computer vision that aims to correctly predict each object instance and its per-pixel segmentation mask in an image. This task becomes increasingly challenging in complex scenes with dense objects, as there is a growing demand for precise separation of instances and accurate prediction of their masks at the pixel level. Traditional instance segmentation approaches rely on convolutional neural networks (CNNs) and follow a detect-then-segment paradigm, which consists of a detector to identify objects and a mask branch to generate segmentation masks. However, these approaches have limitations such as sub-optimal results on large objects and inferior performance in complex scenarios. To address these drawbacks, recent studies have explored bottom-up strategies that learn per-pixel embedding and instance-aware features and use post-processing techniques to group them into instances based on their embedding characteristics. Despite their advantages in retaining position and local-coherence information, bottom-up models suffer from unstable clustering and poor generalization abilities. In this paper, we propose a novel bottom-up model called SOTR that combines the strengths of CNNs and transformers. Our model effectively learns position-sensitive features and dynamically generates instance masks without the need for post-processing grouping or bounding box constraints. Inspired by the success of transformers in natural language processing, we introduce a transformer model that captures global-range characteristics and models long-distance semantic dependencies. Unlike traditional transformers, our model excels at extracting low-level features and reduces memory and time requirements. Our SOTR model outperforms CNN-based approaches on the MS COCO dataset and achieves state-of-the-art performance in instance segmentation. We introduce the twin attention mechanism, a position-sensitive self-attention mechanism specifically designed for our transformer, which significantly reduces computation and memory compared to the original transformer. Unlike other transformer-based models, SOTR does not require pre-training on large datasets, making it more accessible for applications with limited data. The experiments demonstrate that SOTR achieves high accuracy, particularly on medium and large objects, thanks to its ability to extract global information using the twin transformer.