This paper introduces the problem of societal biases in computer vision applications, specifically in image captioning. The authors highlight the prevalence of gender and racial biases in image tagging and search, as well as the disparities in error rates across demographics. The focus of their work is to examine the pathways of bias propagation in image captioning, from the images to the manual and automatically generated captions. They collect data on skin color and perceived gender annotations in the widely used COCO dataset, revealing significant biases towards lighter-skinned and male individuals. They also observe racial terms and slurs in the captions, with transformer-based models learning racial descriptors. The authors find that image captioning systems perform better on images of lighter-skinned people, and there are visual differences in depictions of lighter and darker-skinned individuals. They conclude that their work lays the foundation for further studies on bias propagation in image captioning. Data and code for research purposes are made freely available.