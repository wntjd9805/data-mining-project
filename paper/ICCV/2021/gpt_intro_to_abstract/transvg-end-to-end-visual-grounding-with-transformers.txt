Visual grounding, also known as referring expression comprehension, phrase localization, and natural language object retrieval, aims to predict the location of a region referenced by a language expression in an image. This technique has great potential to provide an intelligent interface between human language expression and visual components of the physical world. Existing methods for visual grounding can be categorized into two-stage and one-stage pipelines. However, these methods often result in sub-optimal performance, particularly for long and complex language expressions. In this paper, we propose a novel transformer-based framework called TransVG for visual grounding. Instead of using manually-designed fusion modules, we use a stack of transformer encoder layers to establish intra-modality and inter-modality correspondence between visual and linguistic inputs. We also find that directly regressing the box coordinates performs better than previous methods for grounding the queried object. Our TransVG framework achieves state-of-the-art results on five visual grounding datasets, surpassing the performance of the strongest competitors. Overall, our contributions include introducing a transformer-based framework for visual grounding, capturing context using transformers, and formulating visual grounding as a coordinates regression problem. We validate the effectiveness of our method through extensive experiments.