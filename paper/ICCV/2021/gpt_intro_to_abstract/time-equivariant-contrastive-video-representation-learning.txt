The paper introduces the concept of self-supervised learning (SSL) as a solution to video understanding tasks. While large annotated video datasets have improved video understanding, the cost of annotations remains a challenge. The paper proposes SSL, which allows training on unlabelled videos and biases learning towards video dynamics. State-of-the-art SSL approaches based on contrastive learning in images have shown promising results. The paper focuses on how to utilize the additional temporal domain in videos for SSL. Instead of learning invariance to temporal input transformations, the paper proposes learning distinctiveness and how different temporal crops from the same video differ. The paper introduces a novel contrastive equivariance model that recognizes identical relative transformations between clips. This approach improves the learning of dynamics and temporal reasoning capabilities. The proposed method achieves state-of-the-art transfer learning performance in action recognition and video retrieval benchmarks.