Person re-identification (ReID) is a crucial task in computer vision that aims to search for the same person from a set of pedestrian images taken from different cameras. Existing ReID approaches typically assume that the whole body of the person is visible, which limits their effectiveness in handling occluded images. To address this challenge, several occluded ReID methods have been proposed, including key-points based methods and feature pyramid matching methods. However, these methods often rely on auxiliary models and can be biased by occlusion obstacles. Furthermore, they may struggle to handle both occluded and non-occluded pedestrians. In this paper, we propose a novel ReID model that learns discriminative single-scale global-level pedestrian representations. Our model does not require auxiliary models and can effectively handle diverse occlusions. We leverage occlusion-based data augmentation and a bounded distance loss to learn fine-grained features from non-occluded body parts. Our model is optimized using a single backbone network and is enhanced with disentangled non-local operations and a proposed reconstructive pooling layer. During inference, our model uses only the single-scale global feature representations for person matching. We also introduce a large-scale occluded ReID dataset that is more diverse and faithful than existing benchmarks. Experimental results demonstrate that our model outperforms state-of-the-art methods in both occluded and non-occluded ReID tasks.