This paper addresses the problem of inferring the 3D trajectory of human body structure from videos of human motion in order to generate new, realistic movements that obey physics constraints. The ability to accurately synthesize human motion has numerous applications in character animation, motion synthesis, pedestrian simulation, model-based control, reinforcement learning, and physics-based visual tracking. However, existing approaches predominantly rely on large-scale motion capture observations, which are expensive, time-consuming, and limited to indoor environments. Additionally, these approaches often produce unrealistic and jerky motion due to a lack of consideration for physics constraints. To overcome these limitations, this paper proposes a framework that directly trains physically plausible human motion synthesis from monocular RGB videos, without relying on motion capture data. The framework involves refining image-based pose estimates by enforcing physics constraints through contact invariant optimization and contact force computation. These refined pose estimates are then used to train a time-series generative model that synthesizes both future motion and contact forces. The contributions of this paper include the introduction of a smooth contact loss function for physics-based pose refinement, which eliminates the need for separately trained contact detectors or nonlinear programming solvers. The paper also demonstrates that combining visual pose estimation with the proposed physics-based optimization allows for training motion synthesis models that achieve comparable quality to motion capture prediction models, even without access to motion capture datasets. The proposed method is validated on the Human3.6m dataset, and the results illustrate the improved motion synthesis quality and physical plausibility compared to prior learning-based motion prediction models such as PhysCap, HMR, HMMR, and VIBE.