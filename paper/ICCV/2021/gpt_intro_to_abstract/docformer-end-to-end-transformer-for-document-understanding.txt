This paper introduces the task of Visual Document Understanding (VDU), which involves understanding digital documents such as PDFs or images. While OCR engines are good at predicting text from documents, VDU requires understanding the structure and layout of documents. This paper proposes using transformer models to exploit text, spatial features, and images for VDU. Previous approaches have used text-only or text plus spatial features only, but the goal is to fuse all three modalities. Multi-modal training is challenging because it requires mapping text to visual content. The paper presents DocFormer, an approach that incorporates a novel multi-modal self-attention with shared spatial embeddings in an encoder-only transformer architecture. DocFormer uses ResNet50 features and shared spatial embeddings instead of pre-trained object detection networks, making it more memory-efficient and easier to correlate text and visual features. The paper also introduces three unsupervised pre-training tasks, including learning-to-reconstruct and multi-modal masked language modeling. DocFormer achieves state-of-the-art results on various downstream VDU tasks without relying on custom OCR.