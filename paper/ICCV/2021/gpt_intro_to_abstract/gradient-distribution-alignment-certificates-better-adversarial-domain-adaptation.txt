Deep Neural Networks (DNNs) have achieved impressive performance for various applications such as image classification, object detection, and semantic segmentation. However, they often struggle to generalize well on new data due to the data distribution shift problem. Unsupervised Domain Adaptation (UDA) aims to address this problem by learning domain-invariant representations. Among current deep architectures, adversarial domain adaptation (ADA) approaches have been widely investigated and have achieved state-of-the-art performance. One seminal work in this area is Domain-Adversarial Neural Networks (DANN), which integrates adversarial learning and domain adaptation. Recent successful methods have shown that discriminative distribution alignment improves domain adaptation. However, the equilibrium challenge of adversarial learning still presents a major limitation. To address this, we propose a novel method called feature gradient distribution alignment (FGDA) that learns to reduce the distribution discrepancy of feature gradients between domains using adversarial learning. We show that aligning feature gradients not only reduces the domain shift but also achieves a tighter upper bound on the expected error on target samples compared to conventional domain adaptation methods. We evaluate our method through extensive experiments and demonstrate that it consistently outperforms current feature-based adversarial domain adaptation methods, achieving state-of-the-art performance in UDA tasks.