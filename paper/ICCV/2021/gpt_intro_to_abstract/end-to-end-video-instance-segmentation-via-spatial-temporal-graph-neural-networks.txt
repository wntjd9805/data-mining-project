Video instance segmentation (VIS) is a challenging task in computer vision that requires detecting and segmenting object instances in videos. Existing methods for VIS typically use two different strategies for instance tracking. The first strategy is the tracking-by-detection framework, where candidates for detection and segmentation are predicted frame by frame and then associated based on classification or re-identification. The second strategy involves propagating instance masks from a central frame to the entire video clip and merging these sequences to generate video-level results. However, both strategies have limitations in terms of tracking accuracy and the lack of inter-frame and intra-frame instance relation information.In this paper, we propose a novel framework that addresses these limitations and achieves better results in VIS. Our framework utilizes a graph neural network (GNN) to model the instance-level relations and fuse spatial-temporal features. Unlike existing methods, our framework predicts detection, segmentation, and tracking results jointly based on the graph features, allowing for the sharing of useful information between different tasks. Additionally, we incorporate inter-frame and intra-frame instance relation information, which contains rich spatial-temporal information that is beneficial for all VIS tasks.To improve segmentation accuracy, we introduce a mask-information propagation module that combines low-level instance shape features from the reference frame with the shape features of the target frame. This allows for better prediction of instance masks. We evaluate our method on the YoutubeVIS dataset and achieve competitive results compared to existing methods, with our model operating at a high speed of 22 FPS.In summary, our contributions include a GNN-based framework for VIS that simultaneously detects, segments, and associates instances, the incorporation of inter-frame and intra-frame features for improved performance in all VIS subtasks, a mask-information propagation module for more accurate mask prediction, and competitive results on benchmark datasets. The rest of the paper is organized as follows: Section 2 provides a brief introduction to related tasks and highlights the differences between our method and existing VIS methods. Section 3 describes our approach in detail, and Section 4 presents experimental results. We conclude the paper in Section 5.