Identifying nearly identical instances of objects is a common problem in various daily tasks. While humans can solve this problem effortlessly due to their understanding of individual instances, robots require a different approach. In scenarios such as industrial, manufacturing, and agricultural contexts, robots need to identify instances to plan their grasp and approach. However, annotating individual instances for training can be costly and inconvenient. Instead, our goal is to develop an unsupervised instance segmentation algorithm using unlabeled depth images that contain multiple instances of a 3D object. Our problem setting is unique, as our backgrounds are usually simple, but foreground instances can be heavily occluded or have significant appearance variations. Previous methods have relied on 3D CAD models, primitive shapes, or classical image-matching techniques. Some recent deep learning approaches focus on RGB scene decomposition, which may not generalize to segmenting foreground instances from each other. In this paper, we introduce InSeGAN, a general unsupervised framework for instance segmentation in depth images. InSeGAN combines a 3D GAN and an image encoder within an analysis-by-synthesis framework. The training data consist of unlabeled depth images, and InSeGAN learns an implicit 3D representation of the object shape and a pose decoder. At inference time, a given depth image is encoded into a latent space, and each instance is synthesized individually using the GAN. We evaluate our approach on a new dataset called Insta-10, which consists of 10 object classes and 10,000 depth images per class. Our experiments show that InSeGAN outperforms prior methods on most object classes, demonstrating its state-of-the-art performance.