Hand mesh recovery from RGB images is a crucial task in computer vision and has various applications in virtual reality, human-computer interaction, and robotics. While deep learning and large hand datasets have advanced hand mesh recovery, existing methods typically require a large labeled hand dataset for training. However, in scenarios with insufficient annotations or no landmark annotations, accurate hand mesh prediction becomes challenging. This paper focuses on self-supervised hand mesh recovery in unseen real scenarios without any landmark annotations. Previous methods either generate synthetic datasets, which lack adaptation to real data, or rely on weak supervision with 2D hand joints or depth maps, limiting their generalizability. In contrast, our approach is inspired by cognitive science and exploits the ability of humans to learn new concepts by relating them to old concepts without explicit supervision. We propose a novel self-supervised learning approach that transfers hand priors learned from past hand estimation tasks to new scenarios and regresses network parameters for hand mesh prediction with no ground truth landmark. Our approach introduces a self-supervised objective that learns hand mesh from binocular images using left-right consistency, preserving past experience and adapting to new unlabeled environments. We eliminate the need for landmark supervision to improve generalizability. By leveraging existing labeled hand datasets, our approach obtains an initial mesh estimate and refines it in unseen environments without landmark annotations. We design appearance consensus and shape consistency objectives that allow the model to explore spatial relations in binocular images and generalize to new scenarios. Our model estimates absolute hand mesh vertex coordinates from binocular RGB images, making it useful in virtual reality and robot grasping applications. Experimental results on a stereo dataset and a large real binocular dataset demonstrate the effectiveness of our proposed model, achieving comparable performance to weakly supervised and supervised methods.