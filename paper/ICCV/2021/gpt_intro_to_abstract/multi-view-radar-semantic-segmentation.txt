In this paper, we introduce a multi-view approach to semantic segmentation of radar signals in the automotive industry. Radar sensors have been widely used for applications like automatic cruise control but have been hindered by poor angular resolution. With the shift towards automated driving, there is a need for higher performance and safety. Redundancy mechanisms at all levels of the system are required for safety, and we propose the use of three sensors (camera, lidar, and radar) to achieve redundancy at the sensor level. While deep learning has advanced the use of cameras and lidars in automotive applications, radar signals have only recently been embraced. Radar scene understanding is still in its infancy but provides essential information to compensate for weaknesses of other sensors. We propose a multi-view approach that exploits the entire radar data, addressing challenges such as the large volume of data and high noise levels. Our approach performs semantic segmentation on both the range-angle and range-Doppler views of the radar signal, allowing for the localization and relative speed determination of objects. We present lightweight neural network architectures designed for multi-view semantic segmentation and propose loss terms to train models while ensuring coherence between multi-view predictions. Experimental results show that our approach outperforms other methods considered in both quantitative and qualitative evaluations.