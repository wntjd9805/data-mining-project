Facial recognition has seen significant progress in recent years, largely due to the effectiveness of deep neural networks like AlexNet, VGG, FaceNet, ResNet, and others. However, RGB-based face recognition methods are sensitive to variations in illumination, occlusions, and poses. To address this, the use of depth information from RGB-D sensors like Microsoft Kinect or Intel Realsense can enhance the robustness of face representations by providing geometric information about the face's intrinsic shape.While depth-based methods are less sensitive to pose and illumination variations, the availability of RGB sensors is more prevalent than depth sensors. This has led to an over-reliance on RGB alone in facial recognition. To tackle this issue, we propose a method that utilizes available paired RGB-D training data to learn to generate synthetic depth images, even in the absence of corresponding ground-truth depth information.Generative Adversarial Networks (GANs) and its variants have proven effective for data synthesis in various domains. In facial image synthesis, GANs have been used to generate high-quality RGB images. However, few works have attempted to synthesize depth from RGB images. Existing methods like cGAN and CycleGAN have limitations in generalizing to new test examples or translating geometric shapes and features.In this paper, we introduce a novel Teacher-Student GAN (TS-GAN) architecture to generate depth images from RGB images without corresponding depth information. Our approach consists of a teacher component, which includes a generator and discriminator, and a student component with two generators and a discriminator. The teacher aims to learn the initial mapping between RGB and depth images, while the student refines this mapping to improve generalization.We evaluate the quality of our synthesized depth images compared to ground truth and other state-of-the-art methods. Furthermore, we validate the performance of our approach in facial recognition using two RGB-D datasets. We demonstrate that our method achieves comparable or even better results than using ground-truth depth, providing a significant boost to recognition accuracy compared to pure RGB facial recognition.Additionally, we evaluate our approach on an RGB-only dataset with no depth information, showing that the addition of hallucinated depth can considerably enhance recognition results. Our contributions include the proposed teacher-student adversarial architecture for depth generation, the creation of realistic synthetic depth images, and the enhancement of facial recognition performance through the use of multimodal solutions combining generated depth and RGB images.