In this computer science paper, the authors address the issue of catastrophic forgetting in class-incremental learning. They propose a Data-Free Class-Incremental Learning (DFCIL) approach that does not require storing data. The authors explore the concept of model-inversion image synthesis, where the existing inference network is inverted to generate images with similar activations as the training data. They compare their approach with existing methods and demonstrate its superior performance in class-incremental benchmarks. The authors contribute by diagnosing the failure of standard distillation approaches, proposing a modified cross-entropy minimization, importance-weighted feature distillation, and linear head fine-tuning, and achieving a new state-of-the-art performance in the DFCIL setting.