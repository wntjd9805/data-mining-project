Recent works in computer science have made significant progress in semantic segmentation using convolutional networks (CNNs). These methods have been successful in various applications such as image editing and autonomous driving. However, they still have limitations in handling new domains/classes and require a large number of expensive pixel-level annotations for training. Weakly-supervised methods that rely on weak supervisory signals, such as image-level labels and bounding boxes, have been proposed as alternatives. Few-shot and zero-shot learning methods have also been introduced to recognize objects of previously unseen classes with a few annotated examples or even without them, respectively. This paper focuses on the problem of generalized zero-shot semantic segmentation (GZS3) and proposes a discriminative approach called JoEm. JoEm utilizes visual and semantic encoders to learn a joint embedding space. It minimizes the distances between visual features and semantic prototypes in this space, while also addressing the issue of mixing semantic information at object boundaries. The proposed approach eliminates the need for retraining the classifier during inference and demonstrates improved performance on standard GZS3 benchmarks. The contributions of this work include the introduction of the BAR and SC losses for discriminative representation learning and the use of the Apollonius circle to modulate the decision boundary of the nearest neighbor classifier. Extensive experiments and analysis with ablation studies are presented to validate the effectiveness of the proposed approach.