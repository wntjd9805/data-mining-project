Neural architecture search (NAS) algorithms have become popular for designing state-of-the-art deep neural network architectures. These algorithms typically involve three steps: designing a search space, developing a searching algorithm, and implementing an evaluation strategy. Recent NAS algorithms have used human prior knowledge to design smaller search spaces, limiting the discovery of novel architectures. This paper introduces a differentiable AutoSpace framework that automatically evolves the search space to an optimal subspace, reducing human interference and improving exploration capability and searching efficiency. The framework uses a differentiable evolutionary algorithm to evolve the search space and removes redundant cell structures to reduce search and evaluation costs. Experimental results on the ImageNet dataset show that the evolved search space produces more accurate models compared to manually designed spaces. The proposed framework can be seamlessly integrated with existing NAS algorithms, and improves the classification accuracy of previous state-of-the-art models by more than 1.8% on ImageNet.