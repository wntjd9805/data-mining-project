Deep neural networks have shown promising generalization to in-domain samples but are vulnerable to slight alterations of input images and have limited generalization to new domains. Existing defenses improve models' robustness against input variations but only provide robustness against a narrow range of threat models used in training, with poor generalization to unseen attacks. This is because they only consider a small subset of realistic examples on or near the manifold of natural images, leaving high-level semantic aspects intact. To address this, we propose adversarial training against a range of low-level to high-level variations of inputs, leveraging generative models with disentangled latent representations to systematically build diverse and realistic examples without leaving the manifold of natural images. Our approach improves generalization and robustness of the model to unseen variations of input images. We create fine and coarse-grained adversarial changes by manipulating different latent variables and use the pre-trained generative model to guide the generation process within the space of realistic images. Evaluations show that our approach achieves state-of-the-art robustness against a diverse set of adversarial attacks without training against any of them. We also extend our approach to semantic segmentation and object detection tasks, proposing the first method for generating unrestricted adversarial examples for these tasks, which improves both robustness and generalization of the model.