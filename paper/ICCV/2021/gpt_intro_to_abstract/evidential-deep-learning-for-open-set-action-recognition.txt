Video action recognition is a task of classifying human actions in videos into pre-defined categories. However, in real-world scenarios, it becomes an open set problem, where the classifier needs to recognize both known and unknown actions. Open set recognition (OSR) is more challenging than closed set recognition and has important applications in various domains. While OSR has been extensively explored in image datasets, limited progress has been made in open set action recognition (OSAR) specifically. OSAR presents novel challenges due to the temporal nature of videos and the presence of static biased cues. Existing approaches mainly rely on image-based OSR techniques and do not effectively address these challenges. To address this, we propose a Deep Evidential Action Recognition (DEAR) method that formulates OSAR as an uncertainty estimation problem using evidential deep learning. Our approach leverages deep neural networks to predict a Dirichlet distribution of class probabilities, allowing for uncertainty quantification. We also propose a model calibration method to mitigate overfitting risks and a debiasing module to address static bias. Experimental results demonstrate that our DEAR method significantly improves action recognition performance on both small and large-scale unknown videos, while maintaining high performance in closed set recognition. Importantly, DEAR is the first evidential learning model for large-scale video action recognition and offers advantages over existing Bayesian uncertainty-based methods by directly inferring model uncertainty without requiring approximate posterior inference or costly Monte Carlo sampling. Our proposed method is flexible in training without access to unknown actions and effectively mitigates static bias in the open set setting. Overall, the contributions of this paper include the novel DEAR method for open set action recognition, the Evidential Uncertainty Calibration and Contrastive Evidential Debiasing modules to address overconfidence and static bias, respectively, and extensive validation on challenging benchmarks.