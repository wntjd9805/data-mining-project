Person re-identification (ReID) is an important task in computer vision that involves matching images of a person across different cameras. This task is widely used in areas such as video surveillance, security, and smart cities. However, there are still practical challenges that need to be addressed, with occlusion being a common problem that negatively affects ReID performance in real-world scenarios. People can easily be occluded by various obstacles, such as baggage, counters, cars, or trees, which hampers accurate person matching.Existing state-of-the-art methods for occluded person ReID heavily rely on fine-grained pose cues, but these methods often suffer from noisy pose detections, making them ineffective in handling occlusion. To overcome this limitation, previous approaches have attempted to incorporate additional cues, such as pose or human parsing, to aid in determining occlusion scenarios. However, these methods assume the availability of error-free, fine-grained extra cues, which is difficult to achieve in practical scenarios. For example, some methods utilize multiple pose keypoints, but these keypoints may come with high estimation errors in practice.This paper aims to address the occlusion problem in ReID without relying heavily on fine-grained pose information. The goal is to develop a solution that can achieve comparable or superior performance using coarse pose information. By learning a robust mapping from imperfect pose cues to the visibility of body parts, the proposed model aims to alleviate the negative impact of occlusion on ReID performance. The effectiveness of the proposed approach will be evaluated and compared with existing methods in experiments carried out on various datasets.