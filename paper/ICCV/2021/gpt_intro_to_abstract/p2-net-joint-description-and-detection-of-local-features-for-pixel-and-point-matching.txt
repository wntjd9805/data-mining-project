Establishing accurate matches between pixels in 2D images and points in 3D point clouds is crucial for various computer vision applications. Existing methods typically involve a two-step procedure of 3D reconstruction followed by matching based on 2D to 3D reprojected features. However, this approach is not always feasible and does not take into account the equivalence of sensors capable of directly capturing 3D point clouds. In this paper, we propose a unified approach to pixel and point matching without the need for reconstruction. We introduce the task of direct 2D pixel and 3D point matching and propose a dual fully-convolutional framework, named P2-Net, that simultaneously achieves feature description and detection between 2D and 3D views. To address the challenges arising from the distinct properties of 2D images and 3D point clouds, we design a novel loss function that robustly learns distinctive descriptors and guides accurate detections. Our experiments demonstrate the effectiveness of our framework and the generalization ability of the new loss function. This is the first joint learning framework to handle 2D and 3D local features description and detection for direct pixel and point matching.