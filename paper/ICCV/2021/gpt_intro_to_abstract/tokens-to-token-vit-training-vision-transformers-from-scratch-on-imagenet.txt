Self-attention models, such as Transformers, have shown promise for language modeling and have recently been applied to vision tasks. The Vision Transformer (ViT) is the first full-transformer model designed for image classification. However, ViT's performance lags behind similar-sized CNN models when trained from scratch on midsize datasets like ImageNet. We hypothesize that this performance gap arises from two main limitations of ViT: 1) the straightforward tokenization of input images hinders its ability to capture local structure, requiring more training samples than CNNs, and 2) the attention backbone of ViT is not well-designed for vision tasks, resulting in limited feature richness and difficulties in model training. To verify our hypotheses, we conduct a pilot study comparing the learned features of ViT and ResNet50. We find that ResNet captures local structure features progressively from lower to middle layers, while ViT focuses more on capturing global relations. These observations indicate that ViT's direct tokenization approach ignores local structure and leads to inefficient feature representation. Motivated by this, we propose a new full-transformer vision model called T2T-ViT. T2T-ViT overcomes ViT's limitations by introducing a progressive tokenization module that models local structure through aggregation of neighboring tokens. Additionally, we incorporate architecture designs from CNNs, such as wide-residual networks and dense connections, to improve feature richness in transformer layers. Through experiments, we demonstrate that T2T-ViT outperforms ViT and comparable CNN models in terms of performance and model efficiency. Our contributions include showcasing the superiority of visual transformers over CNNs, introducing progressive tokenization, and highlighting the benefits of CNN architecture engineering for vision transformers.