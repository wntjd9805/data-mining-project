Deep learning has achieved remarkable success in computer vision tasks, largely due to the availability of large annotated datasets like ImageNet. However, obtaining high-quality annotations is costly and time-consuming, especially for pixel-wise labeling tasks. To address this issue, pre-training plus fine-tuning has been widely adopted in the computer vision community. This involves pre-training a convolutional neural network (CNN) on ImageNet for image classification and then transferring the learned features to other tasks through fine-tuning. While effective, ImageNet pre-training has limitations, such as expensive annotations and limited effectiveness for tasks reliant on localization. To overcome these limitations, researchers have explored pre-training backbone networks with coarse labels or self-supervised pre-training. However, these approaches lack semantic understanding and often require extensive training schedules. In this paper, we propose LocTex, a method for data-efficient visual representation learning using localized textual supervision. This approach involves annotating images with free-form captions synchronized with mouse traces, which can be acquired easily and at a lower cost. We bridge the vision and language modalities using contrastive learning and supervise the cross-modal attention map with rendered mouse traces to improve localization-sensitive downstream tasks. Our proposed method achieves similar training times as ImageNet pre-training and can be trained in less than a day with 8 GPUs. Transfer learning experiments demonstrate that LocTex outperforms ImageNet supervised pre-training, achieving higher accuracy on COCO instance segmentation and PASCAL VOC image classification tasks.