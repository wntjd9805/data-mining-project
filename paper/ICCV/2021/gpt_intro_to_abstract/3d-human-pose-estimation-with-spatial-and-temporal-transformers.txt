This paper introduces the concept of using transformer models for human pose estimation (HPE) in 2D-to-3D lifting approaches. HPE is important for applications such as human-computer interaction and motion analysis. Current approaches can be categorized into direct estimation methods and 2D-to-3D lifting methods. While 2D-to-3D lifting methods generally outperform direct estimation methods, mapping 2D poses to 3D poses is challenging due to depth ambiguity and occlusion. To address these issues, recent works have incorporated temporal information from videos. However, existing approaches have limited temporal connectivity. This paper suggests utilizing transformer models, which have been successful in natural language processing, for 3D HPE. Transformers have the capability to capture global correlations across long input sequences, making them suitable for sequence data problems. However, transformers require specific designs to achieve comparable performance with CNN counterparts for vision tasks. This paper aims to explore how to leverage the power of transformers for 3D HPE. A baseline approach is presented where the transformer is directly applied to 2D-to-3D lifting HPE, treating each frame's 2D pose as a token. Preliminary results show that this approach achieves state-of-the-art performance on Human3.6M and MPI-INF-3DHP datasets. The paper concludes by highlighting future directions for further improving the transformer-based approach for 3D HPE.