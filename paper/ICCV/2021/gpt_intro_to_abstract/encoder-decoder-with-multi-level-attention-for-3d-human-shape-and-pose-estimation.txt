3D human shape and pose estimation from single images or videos is a challenging task in computer vision. Existing methods based on Deep Neural Networks (DNN) have improved the accuracy and robustness of this task, but they often fail in challenging scenarios such as cluttered backgrounds, occlusion, and extreme poses. To overcome these challenges, this paper proposes a novel approach called Multi-level Attention Encoder-Decoder Network (MAED) for video-based 3D human shape and pose estimation. MAED explores three intrinsic relations: spatial relation, temporal relation, and human joint relation, by utilizing multi-level attentions in a unified framework. The proposed approach consists of a Spatial-Temporal Encoder (STE) and a Kinematic Topology Decoder (KTD). The STE uses parallel branches to learn spatial and temporal attention, while the KTD simulates the kinematic tree of human joints to capture the hierarchical dependence and implicitly allocate attention to parent joints. Experimental results demonstrate that the proposed MAED outperforms existing methods in terms of accuracy and robustness. Overall, this paper introduces a new approach that effectively utilizes multi-level attention for 3D human shape and pose estimation from videos.