This paper addresses the problem of depth estimation in computer vision tasks such as autonomous driving and augmented reality. While supervised methods require ground-truth depth data, self-supervised methods have gained attention by eliminating the need for such data. However, most self-supervised methods have only been evaluated on outdoor datasets, leaving their performance in indoor environments uncertain. This paper proposes MonoIndoor, a self-supervised depth estimation method specifically designed for indoor scenes. The method introduces a depth factorization module that allows the depth network to adapt to scale changes, as well as a residual pose estimation module to improve rotation prediction. The proposed method achieves state-of-the-art performance on three indoor datasets.