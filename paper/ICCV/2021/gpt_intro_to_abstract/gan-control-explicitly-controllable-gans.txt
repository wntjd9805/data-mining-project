Generating controllable photorealistic images is crucial in various fields, including cinematography, graphic design, video games, medical imaging, virtual communication, and machine learning research. While previous methods focused on 3D modeling, they often lacked photorealism. On the other hand, recent works utilizing generative adversarial networks (GANs) have achieved photorealistic results but lacked control and interpretability. To address these limitations, researchers have explored combining GANs with 3D modeling techniques to achieve both high photorealism and control over attributes such as pose, illumination, and expression. However, relying on 3D models introduces challenges such as the synthetic-to-real domain gap. Additionally, the expressive power of these methods is limited to the capabilities of the underlying model. This paper presents a unified approach for training a GAN to generate high-quality, controllable images in the domains of facial portrait photos, painted portraits, and dogs. Departing from the use of detailed 3D models, the approach uses supervision signals from pre-trained models to control different features. By creating a latent space composed of sub-spaces corresponding to specific properties and enforcing similarity or difference between images generated by identical or different sub-vectors, the approach achieves disentanglement. Additionally, the paper introduces an encoder for human-interpretable control and a novel image projection method for disentangled latent spaces. The contributions of this work include a state-of-the-art approach for training explicitly controllable GANs, its extensibility to domains beyond 3D modeling frameworks, and a disentangled projection method for real image editing.