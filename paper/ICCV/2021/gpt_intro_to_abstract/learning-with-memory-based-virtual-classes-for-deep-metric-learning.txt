Deep metric learning (DML) plays a crucial role in learning visual similarities in various vision tasks. These tasks include image clustering, unsupervised learning, and image retrieval. The goal of DML is to create an embedding space that accurately reflects the visual similarities between images using a defined distance metric. However, DML faces a unique challenge as the training and test data typically belong to different classes. Therefore, the main objective is to maximize generalization performance from the training distribution to the shifted test distribution. This differs from traditional classification tasks that deal with independent and identically distributed data.Existing DML approaches focus on learning visual similarities using objective functions that consider pair-wise similarity or similarity between samples and class representatives. Some recent studies propose utilizing past embeddings from training steps to increase the number of samples in a mini-batch and hard negative pairs. However, these methods are still limited to the seen classes in the training data, which can lead to overfitting on seen classes and underperformance on unseen classes in the test data. To address this issue, we propose a novel training strategy called Memory-based Virtual classes (MemVir) for DML.In MemVir, we maintain memory queues for both class weights and embedding features. Instead of using them to increase the number of instances of seen classes, they are treated as virtual classes to compute the loss function along with the actual classes. We also incorporate the idea of curriculum learning (CL) by gradually increasing the learning difficulty through the addition of virtual classes. MemVir offers several advantages, including training the model with augmented information, improved optimization stability and performance through CL, and a more generalized embedding space by alleviating the focus on seen classes.Our contributions include proposing MemVir, a novel training strategy that exploits past embeddings and class weights as virtual classes to improve generalization. We also enhance the training process and performance by incorporating CL. Through theoretical and empirical analysis, we demonstrate that employing virtual classes enhances generalization by reducing the strong focus on seen classes. MemVir achieves state-of-the-art performance on popular DML benchmarks under both conventional and Metric Learning Reality Check evaluation protocols. Furthermore, MemVir can be easily applied to existing loss functions without modification, resulting in a significant performance boost.