Deep learning has achieved great success due to the availability of data and the increasing computing capability of deep learning systems. However, deploying deep neural networks (DNN) on edge devices such as IoT devices or mobile phones is challenging due to limited resources and energy budgets. This results in large latency, which is a major concern during inference. To address this, DNN quantization techniques have been proposed to compress the network and reduce storage requirements and arithmetic operations, thus improving hardware efficiency.Quantization methods map continuous weights to discrete values, significantly reducing model size. Non-uniform methods, such as deep compression, cluster weights using k-means and encode them as indexes. On the other hand, uniform methods, like INT8, map weight values to uniformly distributed integers. Power-of-two based quantization simplifies expensive multiplication operations into shift operations. However, low-bit quantization often requires retraining DNN models to mitigate quantization error, which is not feasible for many users. Most non-uniform quantization methods use 32-bit floating-point format (FP32), which is hardware-intensive compared to integer-based (INT) quantization methods. In terms of inference, reducing latency and energy consumption are crucial. Dynamic quantization allows for lower-energy math operations and faster inference, but it often requires de-quantization and re-quantization processes, leading to high energy consumption and longer latency.To address the trade-off between data format precision (quantization accuracy) and hardware efficiency, this paper proposes a novel Adaptive Floating-Point (AFP) representation. AFP retains the 32-bit floating-point format but allows configurable bit-widths for each segment. This approach minimizes quantization-caused accuracy degradation and reduces computation cost. The proposed AFP format is optimized using Bayesian optimization, providing the optimal setting for accuracy and computation cost.The paper presents a comprehensive framework that automatically tunes the AFP configuration for DNN quantization. Experiments conducted on the ImageNet dataset with popular models show that the proposed method outperforms existing methods, with minimal decrease in accuracy. The method achieves state-of-the-art accuracy with a 5-bit quantization on average.In summary, this paper introduces a novel AFP representation for DNN quantization, along with a comprehensive framework for automatic configuration tuning. The experimental results demonstrate the efficacy of the proposed method in terms of accuracy and computation cost reduction.