Person re-identification is an important task in computer vision that aims to identify individuals across different camera views. Many existing methods focus on visual features such as appearance, clothes, and textures, but their performances are still not satisfactory in real-world situations. Recent methods have proposed using spatial-temporal patterns to improve re-identification accuracy, but they neglect instance-level information and are not robust to outliers. In this paper, we propose a novel method called Instance-level and Spatial-Temporal Disentangled Re-ID (InSTD) that models instance-level and spatial-temporal patterns separately. We incorporate instance-level state information, such as the walking direction of the pedestrian, and disentangle the spatial-temporal pattern by constructing their marginal distributions. By adaptively combining the spatial and temporal patterns, our method achieves superior results compared to baseline models and the state-of-the-art in terms of mean Average Precision (mAP) on benchmark datasets. This paper contributes a personalized approach to modeling spatial-temporal patterns for person re-identification and demonstrates the effectiveness of our proposed method.