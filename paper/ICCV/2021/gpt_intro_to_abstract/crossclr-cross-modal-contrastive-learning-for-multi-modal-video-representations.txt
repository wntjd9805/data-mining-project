This paper introduces cross-modal contrastive learning, which connects video and text data, expanding the applications of computer vision. The authors address two issues in existing cross-modal contrastive learning techniques. First, they propose a contrastive loss that ensures similar features from the same modality stay close in the joint embedding, improving semantic meaning and generalization. Second, they identify and remove inï¬‚uential samples from the set of negatives, preventing the separation of strongly related samples. The authors demonstrate the effectiveness of their approach on video-text retrieval and video captioning tasks. The proposed cross-modal loss is shown to generalize to other pairs of modalities beyond video and text.