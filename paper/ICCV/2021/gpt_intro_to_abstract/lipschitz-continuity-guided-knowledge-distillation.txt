Recently, there have been significant advancements in deep learning models for various computer vision and natural language processing tasks. However, these models are often too large and computationally expensive to be deployed on resource-constrained edge devices. To address this issue, researchers have proposed network compression methods, including network pruning, network quantization, and knowledge distillation (KD). KD involves transferring knowledge from a larger network (teacher) to a smaller one (student) to improve its training process.Previous KD methods primarily focus on aligning shallow information between networks, neglecting the high-level information and functional properties of the teacher networks. In this paper, we propose incorporating Lipschitz continuity, which represents the relationship between input perturbation and output variation, into KD to leverage the functional properties of neural networks. We calculate the Lipschitz constant for each intermediate block of the teacher networks and use it as knowledge to guide the training of student networks.Calculating the Lipschitz constant is a computationally complex problem, so we develop an approximation algorithm using Transmitting Matrices (TMs) to avoid high complexity. The TMs enable us to calculate the spectral norm, which represents the Lipschitz constant, through an iterative method. We aggregate all the Lipschitz constants calculated from the TMs as the knowledge of the Lipschitz continuity transferred to student networks.Our contributions in this paper are four-fold: 1. We are the first to utilize Lipschitz continuity as a high-level functional property in knowledge distillation, providing theoretical explanations and empirical evidence for its effectiveness.2. We propose a novel knowledge distillation framework called Lipschitz cONtinuity Guided Knowledge DistillatiON (LONDON) that utilizes the Lipschitz constant for distilling knowledge.3. To address the computational complexity of calculating the Lipschitz constant, we develop a numerical approximation method using Transmitting Matrices.4. We conduct experiments on various knowledge distillation tasks, including classification, object detection, and segmentation, and achieve state-of-the-art results on CIFAR-100, ImageNet, and VOC datasets.In conclusion, our proposed LONDON framework incorporating Lipschitz continuity improves the performance of knowledge distillation by leveraging the functional properties of neural networks.