This paper introduces a novel approach to salient object detection (SOD) using a unified model based on the pure transformer architecture. Previous SOD methods have focused on building powerful decoders, but they have been limited in their ability to learn global long-range dependencies. To address this limitation, the authors propose incorporating the self-attention mechanism from the Transformer model to capture global cues throughout the network. The proposed approach, named Visual Saliency Transformer (VST), is able to effectively fuse cross-modal information and achieve impressive results on both RGB and RGB-D SOD benchmark datasets. The main contributions of this work include the design of a multi-task transformer decoder for joint saliency and boundary detection, a token upsampling method for high-resolution prediction, and the demonstration of the effectiveness and potential of transformer-based models for SOD.