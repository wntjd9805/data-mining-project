Recent advances in deep learning have shown that deep neural networks can easily make incorrect predictions with high confidence when a small adversarial perturbation is added to the input. To defend against these attacks, numerous techniques have been proposed to improve the robustness of neural networks. One promising approach is to develop certifiably robust classifiers that can guarantee the absence of adversarially perturbed examples within a certified region. However, existing certified robustness algorithms primarily focus on floating-point neural networks, while compressed neural network models, such as quantized networks, are widely used in resource-limited platforms like edge devices and mobile systems. Quantization is a technique that compresses neural networks by replacing high-precision floating-point values with low-bit precision, reducing both storage and computational complexity. In this paper, we address the under-studied problem of certified robustness for quantized neural networks and propose an integer-arithmetic-only certified robustness mechanism called IntRS. We demonstrate through experiments that IntRS achieves comparable robustness and accuracy performance to existing methods for full precision networks, while significantly improving inference time efficiency by 4× to 5×. Our results highlight the importance of considering the characteristics of quantized neural networks in developing certified robustness defenses.