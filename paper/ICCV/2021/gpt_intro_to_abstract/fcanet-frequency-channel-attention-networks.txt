Attention mechanisms for convolutional neural networks (CNNs) have garnered significant attention in the field of feature modeling. These mechanisms, which selectively focus on important information, have been widely used in computer vision and natural language processing. Channel attention, in particular, has become a popular tool for the deep learning community due to its simplicity and effectiveness in feature modeling. However, the use of scalar values for each channel in channel attention approaches poses computational constraints and often fails to capture complex information. In this paper, we propose a novel approach that utilizes the discrete cosine transform (DCT) to compress channels in the channel attention mechanism. The DCT has properties that make it suitable for channel attention, including its energy compaction property and differentiability. Additionally, DCT can be seen as a generalization of global average pooling (GAP), which is commonly used in the deep learning community. We introduce a multi-spectral channel attention (MSCA) framework that leverages the limited frequency components of DCT to better compress channels and explore components that are left out by GAP. We also propose three frequency component selection criteria for the MSCA framework. Our experiments demonstrate that our proposed method, called FcaNet, achieves state-of-the-art results on ImageNet and COCO datasets, with comparable computational cost to the baseline SENet.