Deep neural networks (DNNs) are widely used in computer vision systems, but their complex and non-linear nature makes it difficult to understand how they interpret pixel data. This lack of interpretability creates a demand for techniques that can explain why a particular input yields a specific output. Explanations are crucial for verification, planning, diagnosis, and other areas, as they increase user confidence and help identify faults in automated procedures. However, defining what constitutes a good explanation is still a challenge. The need for explainable AI has grown with the increasing use of machine learning, particularly deep learning, leading to initiatives by organizations like DARPA to promote understanding and trust in learning algorithms. Explanations for image classifier results often rely on ranking the importance of pixels, where higher scores indicate greater significance for the DNN's classification outcome. User-friendly explanations can be subsets of highly ranked pixels sufficient for the original classification. However, determining these rankings is an NP-complete problem with an exponential number of possibilities, making brute-force approaches impractical. Existing explanation-generating algorithms approximate the solution using image classification-tailored heuristics, which work well on benchmark datasets like ImageNet where objects are clearly visible. But for images with partial occlusion, such as when a person enters a scene or there is dirt on the camera lens, current methods perform poorly. To address this issue, we propose a new algorithm grounded in causal theory that achieves better accuracy on existing datasets and introduces a new benchmark set called Photo Bombing, where masked objects serve as the ground truth for the "photobomber." The algorithm is iterative, highly parallelizable, and improves explanation quality. The tool, benchmark set, and results can be accessed at https://www.cprover.org/deepcover/.