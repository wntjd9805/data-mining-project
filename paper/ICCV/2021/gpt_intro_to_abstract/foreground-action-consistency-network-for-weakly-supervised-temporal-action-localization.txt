Temporal action localization in videos is an important task that aims to localize action instances along the temporal dimension. Most existing methods for this task are trained in a fully supervised manner, requiring frame-level annotations which are expensive and time-consuming to obtain. To address this challenge, weakly supervised methods with only video-level labels have been developed. However, existing weakly supervised methods lack the ability to maintain consistency between foreground and actions. In this paper, we propose a framework that explicitly models and regularizes the foreground-action consistency. Our method introduces three branches, including a class-wise foreground classification branch, a class-agnostic attention branch, and a multiple instance learning branch. Each branch incorporates a hybrid attention mechanism to improve attention learning and ensure accurate action boundary detection. We evaluate our method on two benchmark datasets and demonstrate its superior performance compared to state-of-the-art approaches. The contributions of this work include the introduction of a class-wise foreground classification pipeline, a hybrid attention mechanism, and improved action localization performance.