We present VidTr, a transformer-based video action classification architecture that performs global spatio-temporal feature aggregation. Current convolution-based approaches in video classification have limited receptive fields and inefficient information aggregation. To overcome these limitations, we propose using attention for spatio-temporal modeling. Inspired by the success of transformers in NLP and computer vision, we introduce a transformer-based video network that directly applies attention to raw video pixels, aiming for higher efficiency and improved performance. We first introduce a vanilla video transformer that learns spatio-temporal features from raw-pixel inputs. However, the vanilla video transformer is memory intensive. To address this, we propose separable-attention, which performs spatial and temporal attention separately, significantly reducing memory consumption. We further reduce memory and computational requirements by utilizing redundant temporal information in videos, proposing a topK std pooling operation. Our VidTr achieves state-of-the-art or comparable performance on six commonly used datasets with lower computational requirements and latency compared to previous approaches. Our error analysis and ablation experiments show that VidTr outperforms I3D on activities requiring longer temporal reasoning. We also demonstrate that ensemble techniques combining VidTr with I3D can lead to performance improvements. We visualize the separable-attention using attention rollout, showing that spatial-attention focuses on informative patches and temporal attention reduces duplicates and non-informative instances. Our contributions include the video transformer, VidTr, with efficient spatio-temporal information aggregation, various permutations of VidTr, and detailed results and analysis on six datasets.