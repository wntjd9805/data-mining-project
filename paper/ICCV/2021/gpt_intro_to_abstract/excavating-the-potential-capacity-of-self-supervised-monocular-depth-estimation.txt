This paper addresses the problem of depth estimation in computer vision and its importance in various applications such as virtual reality, autonomous driving, and robotics. While monocular depth estimation has made significant progress with the use of Convolutional Neural Networks, most existing approaches rely on supervised training, which requires a cumbersome and formidable collection of training datasets. As an alternative, self-supervised methods eliminate the need for ground-truth depth by framing depth estimation as a reconstruction problem. Recent works have shown a reduction in the gap between self-supervision and full-supervision, but this reduction is heavily influenced by sophisticated model architectures and constraints from external modalities. In this paper, the authors demonstrate the potential of self-supervised monocular depth estimation without these additional constraints, focusing on data augmentation, self-distillation, and model architecture. They propose a novel data augmentation method called Data Grafting to weaken the relationship between depth and vertical image position and introduce Selective Post-Processing to generate better pseudo-labels for self-distillation. Additionally, they extend the multi-scale network to a full-scale network by incorporating prediction modules in both the encoder and decoder. The authors present superior results compared to existing models, achieving state-of-the-art performance within self-supervised methods even without the use of extra constraints. The main contributions of this paper include the introduction of an efficient data augmentation approach, the application of self-distillation to depth estimation, the proposal of a more efficient full-scale network, and the achievement of state-of-the-art performance within self-supervised methods.