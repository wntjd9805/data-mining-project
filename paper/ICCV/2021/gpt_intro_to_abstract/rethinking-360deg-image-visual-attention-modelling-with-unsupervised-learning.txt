This paper discusses the limitations of existing approaches for omni-directional image (ODI) saliency prediction and proposes a framework for improving the prediction accuracy. The authors highlight two main limitations of current methods: the lack of leveraging unlabelled data and the use of computationally complex CNNs on distorted projections. To address these issues, the authors introduce a representation learning function that maximizes the mutual information (MI) between the input and local regions, specifically focusing on the problematic poles of the ODI. They leverage contrastive learning and propose a loss formulation to induce invariance to different projections. The authors evaluate their approach and demonstrate its effectiveness for saliency prediction, achieving comparable performance to fully supervised methods. Additionally, they show that their approach eliminates the need for using cube map projections and significantly reduces computational complexity. The contributions of this work include extending contrastive/self-supervised learning to the 360° image domain, addressing challenges in 360° saliency prediction, and reducing computational cost.