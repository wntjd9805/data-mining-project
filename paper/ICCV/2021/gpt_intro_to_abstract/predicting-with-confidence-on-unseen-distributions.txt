Machine learning models are highly sensitive to variations in performance when the test data differs from the training data. Dataset replication studies have shown that even when efforts are made to closely replicate the training data, model accuracy still changes significantly on new test sets. Given that real-world environments involve different data distributions, accurate estimates of a model's performance on new test sets are crucial. Acquiring new labeled data for every distribution shift may be costly, so it is important to find alternative methods to predict accuracy changes. In this paper, we explore the problem of automatic model evaluation across different model architectures and natural and synthetic distribution shifts. We evaluate various distributional distances and predictive uncertainty measures as features in a regression model for predicting accuracy on unseen distributions. We find that common distributional distances fail to reliably predict accuracy changes on natural distribution shifts. However, we discover that the difference of confidences (DoC) between the base distribution and the target distribution encodes valuable information about both synthetic and natural distribution shifts. Using DoC as a feature, our regression models outperform other methods and significantly reduce predictive error across challenging natural distribution shifts. This work demonstrates the possibility of obtaining high-quality estimates of model accuracy across different model architectures and types of distribution shifts.