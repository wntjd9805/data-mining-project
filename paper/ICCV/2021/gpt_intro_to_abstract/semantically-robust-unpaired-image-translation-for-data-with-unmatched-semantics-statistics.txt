Recently, unpaired image-to-image translation has gained popularity in the computer vision community. This approach, which assumes unlabelled images from different domains as inputs and does not require paired images for training, has been widely used in image manipulations, style transfer, domain adaptation, and data augmentation. However, unpaired image translation is a challenging task due to its unsupervised learning nature. Existing methods rely on visual cues and use adversarial loss to align image statistics between translation images and target images. However, these methods fail to address the unmatched semantics statistics problem, where the underlying distributions of semantics in the two domains are typically different. Preserving the semantics during translations is crucial, and forcibly matching distributions can lead to spurious solutions. There have been attempts to address this issue, but they either require extra supervision or are too restrictive and prone to artifacts. In this paper, we propose SRUNIT (Semantically Robust Unpaired Image Translation), which encourages perceptually similar contents to be mapped to contents with high semantic similarity. We introduce a semantic robustness loss based on the CUT framework and demonstrate the effectiveness of SRUNIT in reducing semantics flipping. Our method outperforms existing GAN-based methods qualitatively and quantitatively on multiple datasets.