The goal of this work is to develop a system, called EgoRenderer, that can generate full-body avatars of a person wearing an ego-centric fisheye camera from arbitrary external camera viewpoints. This system has various applications in sports performance analysis, healthcare, virtual reality (VR), and augmented reality (AR), particularly in telepresence. Unlike existing methods that rely on external camera views, our approach utilizes a lightweight and compact sensor that can be easily mounted on glasses, headsets, or caps, providing full mobility to the actors. However, this presents several challenges, including egocentric pose estimation, appearance transfer, and free-viewpoint neural rendering. We address these challenges by combining novel solutions for each component. Our method involves synthesizing explicit and implicit textures from egocentric images, constructing accurate poses, and performing neural image translation to render the avatars in external views. We evaluate our system qualitatively and quantitatively and demonstrate its superior performance compared to baseline methods. Our contributions include a large synthetic training dataset and a specialized neural network for dense correspondence prediction. Additionally, we propose an end-to-end system that can generate full-body avatar renderings from single egocentric images and user-defined external viewpoints.