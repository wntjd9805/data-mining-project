This paper introduces a model called Predict, Attend, Refine with Transformers and Slots (PARTS) that aims to extract object-level segmentations and representations of complex 3D environments and make predictions about future observations. The model extends and improves upon previously proposed models by incorporating a recurrent slot-attention mechanism that incorporates top-down information. The use of transformers and other architectural changes are also shown to improve performance. The paper analyzes each part of the model and studies the resulting learned representations. The goal of this research is to develop models that represent scenes in terms of their constituent objects, which can allow for better generalization, planning, and future predictions for agents. While progress has been made in this field, extracting meaningful objects from complex 3D environments remains a challenge.