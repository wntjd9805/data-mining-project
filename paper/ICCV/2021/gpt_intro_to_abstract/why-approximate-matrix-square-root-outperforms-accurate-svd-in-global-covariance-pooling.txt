Global Covariance Pooling (GCP) is a method that normalizes the covariance matrix of convolutional features before feeding them to a fully-connected layer. It has been shown to outperform first-order pooling methods such as max-pooling and average-pooling. The normalization can be done using either matrix logarithm or matrix square root. In this paper, we investigate the performance gap between accurate matrix square root computation using singular value decomposition (SVD) and approximate matrix square root computation using the Newton-Schulz iteration method. We observe that the approximate square root consistently outperforms the exact one. However, SVD suffers from gradient instability, especially for large matrices. To address this issue, we propose several SVD backward remedies to smooth the gradients. Although these remedies bring marginal performance gain, they are not comparable to the approximate square root method. We also observe that the performance gap between the remedies and the approximate square root method decreases when the learning rate decreases. This suggests that well-conditioned covariance matrices benefit SVD. Based on these observations, we propose a hybrid training protocol that combines the approximate square root method with SVD. This protocol achieves competitive and sometimes better performance than the approximate square root method alone. We also introduce a SVD backward algorithm that uses Padé approximants for gradient approximation, which consistently achieves state-of-the-art performance. Our contributions include analyzing the reasons behind the performance difference between the approximate and accurate square root, proposing a hybrid training protocol for GCP, and introducing a robust gradient approximation method using Padé approximants. The source codes of all the methods will be released upon acceptance.