The paper introduces the challenge of domain shift in 3D semantic segmentation, particularly in real-world applications such as robotics, autonomous driving, and virtual reality. Existing methods for addressing domain shift in 2D semantic segmentation have been proposed, but there is little research on 3D semantic segmentation. The authors propose a cross modal learning method that leverages the complementary advantages of 2D and 3D data to improve domain adaptation in 3D semantic segmentation. However, the previous method only considers matched features between 2D and 3D data, ignoring the potential useful information in mismatched features. To address this limitation, the authors propose a dynamic sparse-to-dense cross modal learning strategy that allows the interaction between sparse point cloud features and dense pixel features. Furthermore, they introduce a novel sparse-to-dense learning loss to support the learning of multi-modality features. The proposed method is evaluated on various real-to-real adaptation settings and achieves state-of-the-art segmentation performance. The contributions of the paper include exploring both intra and inter-domain cross modal learning, proposing the DsCML module for establishing relationships between multi-modality features, and introducing the CMAL method for high-level cross modal interaction and feature alignment between different domains.