In recent years, deep convolutional neural networks have achieved significant advancements in vision tasks, while transformer-based architectures, specifically the transformer, have become prominent in natural language processing (NLP). Transformers excel at modeling long-range dependencies, allowing attention over all elements in an input sequence. This is in contrast to convolutions, which have limited receptive fields. Inspired by the success of attention-based models in NLP and the recent emergence of pure-transformer architectures in image classification, we explore the application of transformers in video classification. Currently, 3D convolutional architectures augmented with self-attention are the most performant for video classification. We propose a pure-transformer approach for video classification, where self-attention is computed on spatio-temporal tokens extracted from the input video. To handle the large number of tokens in videos, we present efficient methods of factorizing the model along spatial and temporal dimensions. Additionally, we demonstrate techniques for regularization and leveraging pretrained image models to train the video classification model effectively on smaller datasets. We conduct a thorough ablation analysis to determine the best design choices for pure-transformer architectures and achieve state-of-the-art results on multiple video classification benchmarks. Our findings contribute to the growing body of research on attention-based models in computer vision.