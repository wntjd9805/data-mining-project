Humans possess the ability to leverage their prior knowledge and experiences to quickly learn new skills. Meta-learning, a field of study in computer science, aims to imbue machine learners with a similar level of intelligence. Meta-learning explores the idea of using past learning experiences to more efficiently learn for new tasks. This is done through a hallmark experiment design where a meta-learner is trained on a variety of few-shot learning tasks and then tested on previously unseen, yet related, few-shot learning tasks. This approach enforces the concept of "learning to learn." Recent advancements in meta-learning have focused on deep neural networks, with some methods using recurrent neural networks and others utilizing transfer attention schemes. Gradient-based meta-learning, specifically model-agnostic meta-learning (MAML), has gained momentum following seminal work in the field. MAML learns a global model initialization from which a meta-learner can derive task-specific models using only a few training examples. However, the MAML approach is a challenging problem due to its bilevel optimization nature. To address this challenge, a "greedy" algorithm has been proposed, which includes an inner loop for task-specific model updates and an outer loop for common initialization updates. This greedy approach has practical limitations due to high-order derivatives, memory footprints, and the risk of vanishing or exploding gradients. In this paper, we explore the possibility of making the gradient-based meta-learner less greedy and evaluate its performance on various methods and tasks. We introduce a lookahead optimizer into the inner loop, creating a teacher-student scheme that allows for long-horizon exploration. The teacher network takes a "leap" based on the regions explored by the student network, resulting in a high-performing model and a lightweight computational graph. Our approach shows promising results across different meta-learning scenarios, including few-shot learning, long-tailed classification, and meta-attack. We believe that our approach and experimental results can contribute to future work in addressing the challenge of making gradient-based meta-learners less greedy.