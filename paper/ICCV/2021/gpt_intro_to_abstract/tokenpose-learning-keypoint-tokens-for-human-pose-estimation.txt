2D human pose estimation is a crucial task in computer vision that involves localizing anatomical keypoints of humans. Deep convolutional neural networks (CNNs) have achieved impressive results in pose estimation due to their visual representation and recognition capabilities. However, existing models using heatmap representation often struggle to represent keypoints explicitly and capture constraint relationships between parts. In this paper, we introduce TokenPose, a novel token-based representation for human pose estimation that borrows concepts from natural language processing (NLP). We tokenize keypoints and visual patches, enabling the model to learn both visual clues and constraint relations between keypoints. TokenPose incorporates both hybrid and pure Transformer-based architectures, with TokenPose-T being the first pure Transformer-based model for 2D human pose estimation. We evaluate TokenPose on benchmark datasets, achieving state-of-the-art performance with significantly fewer parameters and computation cost compared to CNN-based counterparts. Our contributions include the explicit incorporation of visual and constraint cues, exploration of hybrid and pure Transformer architectures, and competitive performance on benchmark datasets.