Traditional optical motion capture systems that use external, outside-in facing cameras have limitations for pose estimation applications that require mobility beyond a fixed recording volume. To address this, researchers have explored methods for egocentric 3D human pose estimation using head- or body-mounted cameras. While some methods focus on estimating face and hand motions, the estimation of global full body pose has been less explored. Existing methods like Mo2Cap2 and xR-egopose use a single head-mounted fisheye camera for marker-less 3D skeletal body pose estimation, but they can only estimate local pose in egocentric camera space and not global pose in the world coordinate system. This limitation hinders applications that require global poses, such as animating virtual avatars in xR environments. To overcome this limitation, we propose a novel approach for accurate and temporally stable egocentric global 3D pose estimation using a single head-mounted fisheye camera. Our approach leverages spatio-temporal optimization, CNN detection, VAE-based motion priors, and a new convolutional VAE-based motion prior to obtain realistic and smooth pose sequences. To address occlusion issues, we introduce an uncertainty-aware reprojection energy term. Additionally, we introduce a global pose optimizer with a separate VAE to ensure consistency between local body poses and camera poses estimated by SLAM. We evaluate our method on existing datasets and a new benchmark, and our results demonstrate superior accuracy and stability compared to state-of-the-art methods. Our technical contributions include a novel framework for global 3D human pose estimation, an optimization algorithm with efficient motion priors, an uncertainty-aware reprojection loss, and improved accuracy and stability in estimating global and local pose.