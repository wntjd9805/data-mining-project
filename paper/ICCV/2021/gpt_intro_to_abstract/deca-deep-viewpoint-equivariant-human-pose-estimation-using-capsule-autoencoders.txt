Human pose estimation is a crucial task in computer vision with various applications such as action recognition, animation, and gaming. State of the art methods using RGB images have been successful in localizing human joints even in the presence of occlusions. However, they struggle in challenging scenarios, particularly in the top-view perspective, which has the highest amount of joint occlusions and limited training data. Humans have a natural ability to estimate poses from different viewpoints, but computer vision methods lack this generalization capability due to limited training data and the inability to model the hierarchical and geometrical structure of poses. Convolutional Neural Networks (CNNs) rely on max-pooling and data augmentation to achieve viewpoint invariance, but a more desirable property would be for the network to retain the viewpoint transformation applied to the input. Capsule networks (CapsNets) can explicitly model viewpoint transformations and capture the part-whole relationships of objects across different viewpoints. Unlike traditional CNNs, CapsNets achieve viewpoint equivariance and can learn from limited training data. In this work, we propose a Deep viewpoint-Equivariant Capsule Autoencoder (DECA) architecture for 3D and 2D human pose estimation from single depth or RGB images. We demonstrate the effectiveness of DECA in limited training data scenarios without data augmentation and across different input domains. We also show that jointly addressing multiple tasks and organizing the feature space with capsule entities improves performance. Our method is evaluated on the ITOP and PanopTOP31K datasets, establishing a new baseline for viewpoint transfer and RGB domain tasks.