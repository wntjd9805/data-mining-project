Video grounding is the task of localizing specific time intervals in an untrimmed video based on a text query. Previous methods have been proposed to tackle this task, but they often rely on complex video-text interaction modules, which may have limited capabilities to learn video-caption relations. In this paper, we propose a Support-Set Based Cross-Supervision (Sscs) module to improve multi-modal relation learning for video grounding. The Sscs module consists of a contrastive objective and a caption objective, which together enhance the correlation modeling between videos and texts. We also introduce the support-set concept to address the mutual exclusion of video entities, making it more suitable to apply contrastive learning for video grounding. Our experiments on three public datasets demonstrate the effectiveness of Sscs and its ability to significantly improve the performance of state-of-the-art approaches. Overall, our contributions include the introduction of a novel cross-supervision module, the application of the support-set concept, and the empirical validation of our method's efficacy.