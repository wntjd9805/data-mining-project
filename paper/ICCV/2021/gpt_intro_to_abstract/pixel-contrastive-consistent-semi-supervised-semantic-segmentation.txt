Modern deep learning based solutions to semantic segmentation often rely on large-scale pixel-wise annotated datasets for training. However, when limited labeled data is available, the performance of these models declines due to overfitting. To address this issue, researchers have explored semi-supervised segmentation methods, where models are trained using additional unlabeled data. In this paper, we propose a novel approach called Pixel Contrastive-Consistent semi-supervised segmentation (PC2Seg) that simultaneously enforces two desired properties in the model. First, we aim for label consistency, ensuring that the predicted segmentation mask remains unchanged under different augmentations. Second, we introduce contrastiveness in the feature space, enabling the model to accurately classify visually similar pixels. We highlight the importance of both properties and explore their joint regularization. Our approach utilizes a pixel-wise â„“2 consistency loss for label consistency and extends the InfoNCE-based contrastive loss for feature contrastiveness. We address the unique challenges of high computational cost and false negative examples in the pixel contrastive loss by proposing effective negative sampling strategies. Our contributions include introducing the first framework that leverages both pixel-consistency and pixel-contrastive properties for semi-supervised semantic segmentation, generalizing image-level contrastive learning to the pixel-level, and achieving state-of-the-art performance on multiple benchmarks.