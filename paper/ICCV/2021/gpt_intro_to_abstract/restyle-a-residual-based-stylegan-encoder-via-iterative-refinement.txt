Recently, Generative Adversarial Networks (GANs) have gained popularity for their ability to generate high-quality and diverse images. These networks, such as Style-GAN, have shown to effectively encode semantic information in their latent spaces. However, manipulating real images using GANs requires inverting the images into the latent space. Previous works have explored learning-based inversion approaches using encoders, which map real images to their latent codes. While encoders are faster and better suited for editing, there is a significant gap in reconstruction accuracy compared to optimization-based methods. In this paper, we propose a novel encoder-based inversion scheme called ReStyle that incorporates an iterative feedback mechanism. Unlike traditional encoder-based methods, our scheme performs multiple forward passes to gradually converge to an accurate inversion of the input image. We train a residual encoder to predict the offset between the current and target latent codes, allowing for progressive convergence. Our experiments demonstrate that ReStyle achieves significant improvement in reconstruction quality compared to standard feed-forward encoders. We also analyze the iterative nature of our approach, showing the regions refined at each step and the decreasing magnitude of change. Furthermore, we demonstrate the generalization of ReStyle to other tasks and explore its robustness and usefulness in downstream applications. We also introduce an encoder bootstrapping technique to enhance the translation of real images using two well-trained encoders.