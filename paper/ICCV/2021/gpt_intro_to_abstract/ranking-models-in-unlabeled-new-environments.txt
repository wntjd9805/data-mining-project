This paper introduces a problem of ranking the performance of source models on unlabeled target domains in the field of computer science. The authors propose using a labeled proxy as a substitute to estimate model rankings in new environments. They focus on the person re-identification task and discuss the requirements for a suitable proxy, including the availability of labels for evaluation and the ability to reflect true model rankings. The paper explores different proxy sets that meet these requirements, such as existing datasets. The authors also investigate the use of distribution difference measurements, namely Fr√©chet Inception Distance and feature variance gap, to determine the similarity between the proxy set and the target domain. The paper presents a dataset search procedure to construct the proxy set, and experimental results validate the effectiveness of the proposed method. The paper concludes by highlighting the significance of the findings in terms of dataset similarities and model evaluation.