The rise of short-form video sharing applications like TikTok and Reels has led to an increasing need for automated methods to identify highlight clips from untrimmed videos. In the field of computer vision, various methods have been proposed for video highlight detection, including supervised learning and weakly supervised learning based methods. However, weakly supervised methods have limitations in capturing complex interactions between audio and visual streams and in enhancing semantic continuity modeling. Additionally, existing fusion schemes for video representation are not able to fully capture the association between modalities, and temporal evolution across consecutive segments is not adequately exploited. To address these limitations, we introduce a low-rank audio-visual tensor fusion mechanism and a hierarchical temporal context encoding scheme. The fusion mechanism efficiently models the interaction between audio and visual modalities, while the context encoding scheme exploits temporal cues to capture the relationships among adjacent video segments. We also propose an attention-gated instance aggregation module to derive representative video scores from individual segment scores, addressing the gradient vanishing problem during optimization. Experimental results demonstrate the effectiveness of our proposed methods in video highlight detection, outperforming competing approaches. Overall, our study contributes advancements in weakly supervised video highlight detection by addressing key limitations and introducing novel techniques.