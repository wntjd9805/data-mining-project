Interactive segmentation is a challenging vision task that involves allowing users to provide inputs to guide the segmentation algorithm. In this study, we focus on click-based interactive segmentation and aim to improve the understanding of user intentions. Users place positive/negative clicks to indicate foreground/background regions, and these clicks contain spatial and visual information. The current methods encode clicks into distance maps or other representations and use a segmentation network to make predictions. However, these methods often fail to generalize the label of a click to target regions further away, and they can also result in labels spilling over to wrong regions. To address these issues, we propose the Conditional Diffusion Network (CDNet), which models the affinity between different locations and diffuses representations from clicks to unlabeled regions. CDNet consists of a Feature Diffusion Module (FDM) and a Pixel Diffusion Module (PDM) that work together to extract representations with high-level and low-level consistency. We also develop a training regime called Diversified Training (DT) to optimize CDNet. Experimental results on various datasets demonstrate the effectiveness of CDNet and its state-of-the-art performance in click-based interactive segmentation. Our contributions include the formulation of click-based interactive segmentation as a process of conditional diffusion, the design of FDM and PDM to enhance unlabeled regions, and the development of DT to improve the training of CDNet.