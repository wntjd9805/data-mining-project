This paper introduces a new method for remote photoplethysmography (rPPG) or imaging PPG, which involves measuring changes in transmitted or reflected light from the body to estimate physiological information. Compared to traditional PPG methods, rPPG offers non-contact and low-cost health monitoring. Previous approaches to rPPG have used supervised deep learning techniques, but these require annotated data and are expensive to scale. In contrast, this paper proposes a self-supervised learning approach based on certain assumptions about the underlying signal, allowing for the estimation of the PPG signal from facial video data without the need for ground truth training data. The paper also introduces a saliency-based sampling module to enhance the relevant parts of the input data and provide interpretable saliency maps. The contributions of this work include novel loss functions, robustness to desynchronization, and a unified pipeline for training and evaluation. The goal is to encourage further research in computer vision for human health monitoring.