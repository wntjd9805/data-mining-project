Robustness to distribution shift is a major challenge in deep learning, particularly in the case of convolutional neural networks (CNNs). While CNNs perform well on independent and identically distributed (i.i.d) training and test sets, their performance drops significantly when faced with out-of-distribution (OOD) test sets. This decrease in performance can be attributed to the reliance of models on unstable correlations present in the i.i.d dataset. When these correlations are missing due to distribution shift, the performance of CNNs is severely affected. Although some prior work has been done to address this problem, it is still not fully understood or resolved.To mitigate the performance drop, commonly used approaches include data augmentation schemes and adversarial training. The texture hypothesis states that classification models learn feature representations biased towards textures, many of which are unstable and can be destroyed due to weather effects or digital corruptions. This hypothesis has also been studied from a Fourier perspective, which demonstrated that models can achieve reasonable accuracy on the i.i.d test set of ImageNet even with low or high pass filtering applied to the input images during training and testing.However, data augmentation often comes with trade-offs in terms of performance on clean images. To address this, the authors propose a new approach called RoHL (Robust mixture of a HF and a LF expert model). The RoHL approach consists of a high-frequency (HF) expert model, obtained by applying TV minimization and generic augmentations that affect high-frequency components, and a low-frequency (LF) expert model, based on plain contrast augmentation. By combining these complementary models, the authors demonstrate improved performance on both corrupted and clean images. Compared to a standard two-member ensemble, RoHL adds robustness without any additional cost.In summary, this paper makes two main contributions. Firstly, it proposes a regularization scheme that enforces low total variation in convolutional feature maps, boosting high-frequency robustness. Secondly, it introduces the concept of mixing two experts specializing in high-frequency and low-frequency robustness, which is shown to be complementary to existing data augmentation approaches.