Model pretraining is an important aspect of deep transfer learning, where a model is pretrained on a large auxiliary dataset before being fine-tuned on a smaller target dataset. Previous studies have shown that supervised pretraining on large labeled datasets can lead to strong transfer performance. However, obtaining a large amount of labeled data can be challenging. Unsupervised pretraining, on the other hand, involves pretraining on larger-scale unlabeled data and has also shown promising results in terms of transfer performance. In this paper, we investigate whether unsupervised pretraining truly achieves comparable transfer performance to supervised pretraining, particularly in scenarios where the target dataset has a limited number of labeled samples for finetuning. We hypothesize that the clustering of target samples in the feature space plays a crucial role in few-label transfer, and that unsupervised pretraining may have inferior clustering quality compared to supervised pretraining. Empirical analysis supports this hypothesis, demonstrating that supervised pretrained models exhibit better clustering of target samples compared to unsupervised pretrained models. We further analyze the contrastive loss used in unsupervised pretraining and identify its impact on clustering quality. To address the inferior clustering quality of unsupervised pretraining, we propose a "target-aware" unsupervised pretraining approach that involves including some unlabeled target data in the pretraining process. This approach significantly improves the clustering quality and reduces the performance gap between unsupervised and supervised pretraining. Additionally, we propose a progressive few-label transfer algorithm that leverages the clustering property of the pretrained representation to maximize target performance with limited annotation budget. Experimental results demonstrate the effectiveness of our approach, showing significant improvements in transfer performance compared to unsupervised and supervised pretraining alone. Overall, our contributions include identifying the few-label transfer gap between unsupervised and supervised pretraining, proposing a strategy to improve unsupervised pretraining in real-world scenarios with limited unlabeled data, and introducing a progressive few-label transfer strategy to enhance performance with limited annotation budget.