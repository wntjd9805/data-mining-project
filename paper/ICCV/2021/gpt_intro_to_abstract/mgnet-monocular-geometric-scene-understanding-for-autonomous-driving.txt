The introduction of this computer science paper discusses the importance of scene understanding in autonomous driving perception systems and the recent advancements in semantic segmentation and instance segmentation. It introduces the task of panoptic segmentation, which combines semantic and instance segmentation, and explains its relevance in providing both object masks and information about amorph regions in autonomous vehicles. The majority of existing panoptic segmentation methods prioritize quality over inference speed, making them unsuitable for on-board integration into autonomous vehicles. Monocular depth estimation is introduced as a solution to the limitations of 2D pixel location in panoptic segmentation models, allowing for reasoning about the current environment in 3D space. The challenges and limitations in camera-based depth estimation and the advantages of lidar sensors are discussed. The use of self-supervised training strategies in monocular depth estimation is explained, as well as the benefits and challenges of multi-task learning. The authors propose a multi-task framework called MGNet, which combines panoptic segmentation and self-supervised monocular depth estimation, and describe its lightweight network architecture. They also introduce an improved version of the Dense Geometrical Constrains Module for scale-aware depth estimation and discuss their approach to generating pseudo labels for video sequence frames. The use of homoscedastic uncertainty weighting and a novel weighting scheme in multi-task performance is described. The authors evaluate their method on Cityscapes and KITTI datasets and demonstrate improved latency and competitive accuracy compared to previous approaches.