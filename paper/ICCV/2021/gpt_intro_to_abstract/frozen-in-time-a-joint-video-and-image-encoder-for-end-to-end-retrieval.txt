Joint visual-text models have gained popularity in computer science as they enable various tasks such as text-to-visual retrieval, visual captioning, and visual question-answering. These models have seen rapid development due to improvements in neural network architectures, large-scale datasets, and new loss functions. However, the development of these models has mostly been separated into two tracks: one for images and one for videos. This separation is suboptimal considering the overlap in information conveyed by images and videos across multiple tasks. In this paper, the authors propose a dual encoder architecture that aims to unify these two tracks. The architecture utilizes the flexibility of a transformer visual encoder to train from images-with-captions, video clips-with-captions, or both. The authors treat images as a special case of videos that are "frozen in time," allowing them to train with variable-length sequences. This is in contrast to standard 3D CNNs where training on images jointly with videos requires generating a static video. The proposed model also differs from previous methods by not relying on pre-trained expert networks but instead training the model end-to-end. To facilitate the end-to-end training, the authors scrape the web to create a large-scale video-text captioning dataset called WebVid-2M. They also utilize existing large-scale image captioning datasets. The authors make several contributions in this work. Firstly, they propose an end-to-end model for video retrieval without relying on expert features, utilizing a transformer architecture with modified divided space-time attention. Secondly, their versatile architecture can be trained on both video and image datasets by treating images as single-frame videos. They introduce a curriculum learning schedule that gradually learns to attend to increasing temporal context when trained on video datasets. This schedule improves efficiency and reduces GPU time. Lastly, they achieve state-of-the-art performance on various datasets using only the video modality, outperforming works that use pre-extracted experts from multiple modalities.Overall, this paper presents a novel approach to unify the development of joint visual-text models for images and videos. The proposed dual encoder architecture, along with the WebVid-2M dataset, contributes to advancements in video retrieval and demonstrates promising results in various tasks.