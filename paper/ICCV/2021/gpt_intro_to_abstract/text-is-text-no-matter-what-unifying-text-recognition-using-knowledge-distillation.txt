In this paper, we address the problem of text recognition in both Scene Text Recognition (STR) and Handwriting Text Recognition (HTR) scenarios. We propose a knowledge distillation (KD) based framework that unifies individual STR and HTR models into a single multi-scenario model. Our framework consists of three additional distillation losses that specifically target the variable-length and sequential nature of text data. We introduce a character aligned hint loss, an attention distillation loss, and an affinity distillation loss to capture the unique characteristics of text recognition. We also utilize a logit distillation loss to match output probabilities between the student network and the pre-trained teachers. Our extensive experiments on public datasets demonstrate the effectiveness of our framework, showing superior performance compared to existing approaches. Our contributions include the design of a unified text recognition setting, a novel KD paradigm, and the introduction of three additional distillation losses.