Image quality assessment (IQA) aims to quantify the perceptual quality of images. In recent years, deep learning approaches utilizing convolutional neural networks (CNNs) have achieved significant success in IQA. However, CNN-based models often require fixed-size input images, which poses challenges for IQA as images in the wild have varying aspect ratios and resolutions. Resizing and cropping images can impact their composition and introduce distortions, thus affecting their quality.To address these limitations, we propose a patch-based multi-scale image quality Transformer (MUSIQ) model that can process full-size images with varying aspect ratios and resolutions. The Transformer model, originally used for natural language processing, splits the image into a sequence of fixed-size patches and applies self-attention to capture information at different granularities. We also introduce a novel hash-based 2D spatial embedding and a scale embedding to support positional encoding and distinguish patches from different scales.Our experiments on four large-scale IQA datasets demonstrate that MUSIQ achieves state-of-the-art performance on three technical quality datasets and is on-par with the state-of-the-art on an aesthetic quality dataset. MUSIQ overcomes the constraints of CNN-based models and enables effective quality prediction on native resolution images, aligning with the multi-scale nature of the human visual system.