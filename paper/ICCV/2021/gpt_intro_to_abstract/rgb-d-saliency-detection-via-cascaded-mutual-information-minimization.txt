Saliency detection models aim to identify regions in an image that attract human attention. While conventional saliency detection is mostly performed on RGB images, the availability of depth data has led to a growing interest in RGB-D saliency detection. Depth data provides real-world geometric information, which is useful in scenarios where the foreground and background have similar appearances. Existing RGB-D saliency detection models focus on modeling the complementary information of the RGB image and depth data through different fusion strategies. However, these models do not explicitly evaluate the contribution of depth data and lack constraints on network design to learn complementary information. In this paper, we propose a multi-stage cascaded learning framework that explicitly models the redundancy between appearance features and geometric features using mutual information minimization. Our solution effectively fuses appearance and geometric features, as demonstrated by the saliency maps produced. Additionally, we address the lack of large-scale RGB-D saliency detection datasets by contributing the largest dataset with 8,025 image pairs for training and 7,600 image pairs for testing. The dataset includes various annotations and is suitable for fully-/weakly-/un-supervised RGB-D saliency detection. Our contributions include the design of a mutual information minimization framework, the creation of a large-scale dataset, the introduction of new benchmarks, and the development of baseline models for different types of RGB-D saliency detection.