Accurately estimating depth is crucial for applications such as augmented reality and autonomous driving. While the traditional geometric approach requires multiple views, the single-image variant has gained attention but is known to be ill-posed. Convolutional neural networks show promise in capturing prior information about object appearance and shape. There are two classes of methods: supervised approaches that rely on ground-truth depth labels, and self-supervised approaches that use consistency across multiple inputs. However, both methods have limitations in terms of scale ambiguity and depth estimation at long ranges. In this paper, we propose a geo-enabled depth estimation method that leverages geospatial context to address these weaknesses. We introduce an end-to-end architecture that uses a height map and transforms it into a synthetic ground-level depth map, which is then fused into an encoder/decoder segmentation architecture. Our approach does not assume maximum observed depth and requires no post-processing step. We extend the HoliCity dataset to include overhead imagery and height data for evaluation. Experimental results show that our method significantly reduces error compared to baselines, even at longer depth ranges.