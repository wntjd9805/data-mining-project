Scene understanding is essential in computer vision, particularly in the autonomous driving domain, where detecting pedestrians, vehicles, obstacles, and hazards is crucial. While traditional approaches focus on still 2D images, 3D sensing is becoming commonplace, with vehicles equipped with 3D LiDAR sensors and multiple cameras producing 3D Point Clouds (PC) and RGB frames. Additionally, these sensors provide data in real-time, making the data 4-dimensional when combined. However, very few methods have utilized this 4D sensor data. Most existing approaches for 3D object detection use a single 3D point cloud, and only recently have point cloud data in time been considered. Furthermore, the point cloud and RGB data have complementary characteristics, but combining them effectively is challenging, leading to limited research on this topic. To address these challenges, we propose 4D-Net, a novel approach that efficiently combines point cloud information and RGB camera data in time. Our method learns to connect and fuse feature representations from different modalities and levels of abstraction. We show that 4D-Net effectively processes 4D information from multiple sensors, outperforming state-of-the-art methods in 3D object detection and achieving competitive runtime. Moreover, our approach improves detection at far ranges and for small and hard-to-see objects. Our contributions include the first 4D-Net for object detection, a novel learning method for fusing multiple modalities in 4D, a sampling technique for 3D Point Clouds in time, and a new state-of-the-art for 3D detection on the Waymo Open Dataset.