Visual navigation is a crucial component in the development of intelligent autonomous agents. In order to navigate effectively in large-scale environments, agents need to create an internal representation of the environment based on sensory inputs and their own actions. In this paper, we propose a method called visual graph memory (VGM) and a navigation framework that utilizes the VGM for visual navigation tasks. The VGM is constructed using an RGBD image encoder that is trained in an unsupervised manner, eliminating the need for elaborate annotation rules. The navigation framework consists of a memory update module and a navigation module, which selectively store image representations and use the VGM to select actions for navigation tasks. Unlike previous methods, our framework does not rely on robot pose information and can incrementally build the VGM during navigation, allowing it to perform tasks in unfamiliar environments without pre-exploration. Experimental results demonstrate that our method outperforms state-of-the-art approaches in image-goal navigation tasks. The contributions of this paper include the introduction of the VGM for unsupervised topological SLAM and a navigation framework that is robust against pose information errors.