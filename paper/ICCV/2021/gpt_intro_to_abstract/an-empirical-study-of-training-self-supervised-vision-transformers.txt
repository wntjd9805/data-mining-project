Unsupervised pre-training has had a significant impact on natural language processing (NLP), but the pre-training paradigms differ in computer vision. This paper investigates the differences between self-supervised pre-training in vision and language, focusing on training Transformers with leading self-supervised frameworks in vision. The authors examine the components of training deep neural networks and find that instability is a major issue in self-supervised ViT training. They propose a trick to improve stability and demonstrate its effectiveness through experiments. The authors benchmark and analyze self-supervised ViT in various scenarios, exploring architecture designs and scaling up models. They report that self-supervised Transformers achieve strong results using a contrastive learning framework and showcase the potential of ViT models compared to convolutional ResNets. The study also highlights the need for self-supervised pre-training and suggests the underutilization of positional information in ViT models. Overall, this paper provides evidence, challenges, and open questions in self-supervised Transformers, aiming to bridge the gap between vision and language pre-training.