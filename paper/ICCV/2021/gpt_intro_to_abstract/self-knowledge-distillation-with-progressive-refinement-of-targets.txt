The recent progress in deep neural networks (DNNs) has led to significant improvements in various computer vision and natural language processing tasks. However, deeper networks require a large number of model parameters, making them more prone to overfitting and producing overconfident but incorrect predictions. To address these issues, regularization methods such as L1/L2-weight decay, dropout, and batch normalization have been proposed. Additionally, task-specific methods, like advanced data augmentation techniques and target adjustment methods, have shown promising results. In this paper, we propose a regularization technique called progressive self-knowledge distillation (PS-KD) that utilizes a model's own knowledge to soften hard targets and make them more informative during training. PS-KD is easy to implement, can be applied to any supervised learning task, and can be combined with existing regularization techniques. Experimental results on diverse tasks and benchmark datasets demonstrate that PS-KD improves the generalization performance of state-of-the-art baselines, providing better predictive performance and high-quality confidence estimates. This method also enhances the performance of advanced image augmentation techniques and improves machine translation. Overall, PS-KD greatly improves the generalization ability of DNNs in terms of accuracy, the quality of confidence estimates, and misclassification detection.