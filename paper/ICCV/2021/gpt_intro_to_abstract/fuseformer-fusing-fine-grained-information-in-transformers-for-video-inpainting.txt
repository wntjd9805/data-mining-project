This paper introduces the use of Transformer models in video inpainting tasks. Previous work, such as the Spatial Temporal Transformer Net (STTN), has shown promise but has limitations in exploiting rich experiences from other Transformer models. The authors propose a Video inpainting Baseline with vanilla Transformer (ViB-T), which differs from previous models in terms of embedded tokens and the use of a convolutional encoder and decoder. However, ViB-T, like other patch-based Transformer models, is unable to effectively encode sub-token level representations, resulting in inconsistent content between neighboring patches. To address this issue, the authors propose a Soft Split (SS) module to split images into overlapping patches and a Soft Composition (SC) module to composite these patches. They also introduce a Fusion Feed Forward Network (F3N) to improve sub-token fusion ability. The proposed model, FuseFormer, outperforms state-of-the-art video inpainting approaches, as demonstrated through qualitative and quantitative comparisons. The contributions of this paper include the proposal of a strong baseline for video inpainting, the introduction of soft operations to enhance performance, and the development of FuseFormer, a sub-token fusion enabled Transformer model.