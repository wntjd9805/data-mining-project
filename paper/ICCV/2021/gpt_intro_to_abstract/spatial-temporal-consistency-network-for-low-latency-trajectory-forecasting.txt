Intelligent agents, such as autonomous vehicles and mobile robots, operate in shared spaces with humans, necessitating a focus on human safety. To ensure safety, agents must be able to understand and predict the trajectories of pedestrians and vehicles in their vicinity, enabling them to make informed decisions and navigate through crowded areas. However, trajectory forecasting is a complex task due to the intricate behavior and interactions of humans in crowds. Additionally, limited resources, such as energy-efficient hardware, restrict the capabilities of these agents. Therefore, there is a need for compact forecasting models with low latency.In practical applications, low latency is crucial as it allows for real-time adjustments in motion, potentially saving lives. Existing graph-based methods fail to accurately model the temporal motion of trajectories, leading to shaky and unrealistic predictions. In contrast, our approach addresses this issue by generating spatially and temporally consistent trajectories. Numerous trajectory forecasting methods have been proposed, including RNN-based, GAN-based, and graph-based approaches. These methods incorporate recurrent neural networks to generate future trajectories but suffer from high latency. To overcome this latency issue, the Social-STGCNN method was introduced, employing a CNN for time-extrapolation. However, this approach sacrifices temporal consistency, resulting in noisy forecasts.To resolve these limitations, we present the Spatial-Temporal Consistency Network (STC-Net), which generates consistent trajectories with a latency of less than 2ms. STC-Net combines graph convolutions for spatial modeling and dilated temporal convolutions for temporal modeling. It enforces consistency by reconstructing the past, jointly refining the reconstructed past and forecast future, and computing the loss across the entire trajectory. Our approach avoids recurrent layers and proposes feature-wise convolutions, enabling the generation of variable-length future trajectories in a single pass. Notably, our network design is compact, with only 0.7k parameters and 1.3ms latency.To evaluate the effectiveness of our approach, we conducted extensive experiments on three datasets with six different scenes. We also assessed the generalization ability of STC-Net through cross-dataset experiments and evaluated its performance under different temporal sampling rates. Our approach outperforms other graph-based methods in terms of accuracy, model size, and latency. Furthermore, most existing methods fail to achieve real-time performance, even with slow frame rates. In contrast, our approach demonstrates both higher accuracy and faster processing speed, making it suitable for real-world applications.