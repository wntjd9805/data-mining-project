Video recognition is a challenging task that involves learning both spatial and temporal features. Single-stream models for video recognition often underperform two-stream models, which consist of separate streams for spatial and temporal information. However, the design choices for two-stream models are complex and limited in prior hand-crafted models. In this paper, we propose a neural architecture search (NAS) approach to automatically discover high-performance two-stream models. Our approach explores a large design space that includes inter-stream fusion blocks, attention blocks, kernel sizes, output channels, and expansion rates. We adopt a multi-step progressive procedure to efficiently search the large space. Our experiments demonstrate that the discovered Auto-TSNet models outperform other models on action recognition benchmarks. Our contributions include a prescribed search space, a decomposed search procedure, and the discovery of high-performance models.