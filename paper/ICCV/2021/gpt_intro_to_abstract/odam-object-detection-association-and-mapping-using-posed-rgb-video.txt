Endowing machine perception with the capability of inferring 3D object-based maps brings AI systems one step closer to semantic understanding of the world. This task requires building a consistent 3D object-based map of a scene. In this paper, we propose ODAM, a novel framework that incorporates a deep learning front-end and multi-view optimization back-end to address 3D object mapping from posed RGB videos. We focus on representing objects by their 3D bounding volumes, which serve as a valuable abstraction of location and space. However, localizing objects and estimating their extents in 3D using RGB-only videos presents challenges such as depth-scale ambiguity and the lack of consensus on leveraging multi-view constraints. Additionally, the associations of detections of individual 3D object instances from different viewpoints need to be solved prior to multi-view optimization. To address these challenges, we introduce ODAM, which utilizes a graph neural network (GNN) for data association and a super-quadric-based optimization approach to improve the representation of 3D bounding volumes. Our contributions include presenting ODAM as the best performing 3D detection and mapping RGB-only system for complex indoor scenes, proposing a novel method for associating single-view detections to the object-level using an attention-based GNN, and introducing a super-quadric-based optimization approach that shows improvements over previous methods.