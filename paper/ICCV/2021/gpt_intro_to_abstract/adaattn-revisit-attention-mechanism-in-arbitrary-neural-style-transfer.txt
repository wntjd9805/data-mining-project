This paper introduces a novel approach called Adaptive Attention Normalization (AdaAttN) for arbitrary style transfer in computer-aided art generation. The goal of style transfer is to apply the style patterns of a given style image onto a content image while preserving the content structure. The existing methods for arbitrary style transfer have limitations in terms of local style pattern modeling capability and local stylization performance. This paper addresses these limitations by proposing the AdaAttN module, which performs attentive normalization on a per-point basis for feature distribution alignment. The module calculates spatial attention scores from shallow and deep features of both the content and style images. It then calculates per-point weighted statistics to align the content feature with the style feature. A new optimization objective called local feature loss is introduced to improve the training process and enhance the quality of arbitrary style transfer. Extensive experiments and comparisons with other state-of-the-art methods are conducted to demonstrate the effectiveness of the proposed method. The paper also discusses the extension of the model for video style transfer.