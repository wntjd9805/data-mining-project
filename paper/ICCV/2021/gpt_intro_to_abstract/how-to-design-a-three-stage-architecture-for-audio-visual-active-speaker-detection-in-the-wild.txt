In this paper, we address the problem of Audiovisual Active Speaker Detection (AV-ASD) in the wild, where speech is to be detected and assigned to one of possibly multiple active speakers at each instant in time. We discuss the challenges of AV-ASD and the importance of fusing audio and video modalities to overcome these challenges. Previous research has shown the limitations of visual-only and audio-only approaches in accurately detecting and assigning speech to the correct sources. We introduce a three-stage pipeline, named ASDNet, for audio-visual active speaker detection, which incorporates audio-visual encoding, inter-speaker relation modeling, and temporal modeling. We propose novel architectures for the audio and video backbones of the audio-visual encoder and a simple yet effective mechanism for inter-speaker relation modeling. Our experiments on the AVA-ActiveSpeaker dataset demonstrate the superiority of ASDNet over previous methods, with a 93.5% mean Average Precision (mAP) and a 4.7% margin over the second best method. We also provide a detailed ablation study and guidelines for tuning the components of ASDNet. Overall, our contributions include a state-of-the-art solution for AV-ASD and novel architectures and mechanisms for active speaker detection.