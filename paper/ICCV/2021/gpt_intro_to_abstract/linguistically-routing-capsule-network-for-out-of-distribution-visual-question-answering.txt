Visual question answering (VQA) is a task that involves correctly answering a question about an image, and it is considered a core task in the development of complete artificial intelligence. However, the complexity of this task makes it difficult to obtain sufficient training data to cover all background knowledge and reasoning routes. Existing VQA models tend to focus on increasing capacity but struggle to handle test data with different distributions. Some models attempt to represent atomic elements and integrate them to infer final results, but they perform worse than state-of-the-art neural networks on general and unconstrained test data. Humans have the ability to recognize novel concepts by incorporating learned concepts, demonstrating compositional generalization ability. The capsule network shows potential in connecting end-to-end neural networks with part-based models, but it lacks satisfactory results on large-scale image datasets. To address these challenges, we propose injecting human-developed structure into the capsule network to improve compositional generalization ability while maintaining performance. We introduce Linguistically Routing, which generates adaptive reasoning routines inside the capsule network guided by the question parse tree. This approach fuses visual capsules with question words and uses linguistically routing to generate reweighting vectors. The linguistically routing learns to predict unary potentials and binary potentials, maximizing them with a conditional random field. Our proposed method allows the capsule network to encode question words, phrases, clauses, and sentences. Through extensive experiments, we demonstrate the effectiveness of our proposed linguistically routing capsule network in achieving good generalization capability while maintaining performance on in-domain test data. This work contributes an end-to-end trainable routing method that incorporates external structure information into the capsule network and utilizes linguistic parse trees to guide the routing in the VQA task.