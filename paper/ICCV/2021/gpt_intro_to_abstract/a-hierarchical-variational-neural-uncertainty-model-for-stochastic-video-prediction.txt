The paper introduces the concept of visual future generation in artificial intelligence systems and its potential applications in various fields such as video surveillance, human action recognition, and autonomous driving. It highlights the challenge of predicting future sequences due to stochasticity in real-world scenarios. The paper then discusses existing stochastic methods for video generation and their limitations in ignoring predictive uncertainty. To address these challenges, the authors propose a new framework called Neural Uncertainty QuantiÔ¨Åer (NUQ) that quantifies the predictive uncertainty of a stochastic frame prediction model and uses it to calibrate the training objective. NUQ leverages a variational, deep, hierarchical, graphical model to bridge the variance of the latent space prior and the output. The paper also presents a variant of the framework that incorporates a sequence discriminator for motion regularization. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of NUQ in improving video generation quality and training convergence even with small datasets. The paper concludes by summarizing the main contributions of the research, including the introduction of NUQ, the hierarchical variational training scheme, and the improved video generation capabilities of the framework.