Recent years have witnessed the increasing popularity of self-supervised learning methods in computer vision tasks, such as masked language models and jigsaw solving. Among these methods, contrastive learning has shown great potential for image tasks and has even surpassed fully-supervised models in downstream tasks like detection and segmentation. However, labeling videos is more challenging and expensive than labeling static images due to the temporal dimension. Therefore, there is a need for powerful self-supervised learning algorithms specifically designed for video analysis.Existing contrastive learning methods for videos mainly focus on pulling representations of augmented clips from the same video closer together in the embedding space. However, these approaches do not explicitly involve temporal modeling processes, which is crucial for understanding complex video tasks. To address this limitation, we propose a Contrast and Order RePresentation (CORP) framework that incorporates temporal modeling into self-supervised learning.Our framework consists of two implementations: CORPm and CORPf. In CORPm, we randomly sample augmented video clips from two videos and form ordered pairs. The model is trained to classify whether the clips are from the same video and which clip occurs earlier in time. In CORPf, inspired by the SimCLR design, we sample two augmented clips for each video in a batch. The model aims to solve a contrastive pretest and predict the temporal order of the clips. Different fractions of mismatched pairs are used to train the two models, allowing them to learn different patterns.We evaluate our self-supervised models on two benchmark datasets, Kinetics400 and Something-something V2, and compare them with existing contrastive-based methods. The results show that our CORPf model outperforms CVRL on both datasets, while our CORPm model achieves a higher accuracy on Something-something V2, minimizing the performance gap with supervised learning. Ablation studies further validate the effectiveness of our methods in learning both appearance and temporal relations, which are vital for video tasks.In conclusion, our CORP framework addresses the limitations of existing contrastive learning methods for videos by incorporating temporal modeling. Our models achieve promising results on benchmark datasets and demonstrate the importance of considering both appearance and temporal relations in video analysis tasks.