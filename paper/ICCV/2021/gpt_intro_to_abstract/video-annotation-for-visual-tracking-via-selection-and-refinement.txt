In this paper, we address the problem of video target localization in visual tracking using target appearance models. Previous studies have achieved record-breaking results by employing offline pre-trained deep features. However, these approaches heavily rely on large-scale video datasets with accurate annotations, which are scarce and expensive to obtain. To alleviate this issue, recent works have explored machine learning techniques to facilitate video annotation by reducing the manual labor required. However, relying solely on tracking algorithms for label generation can lead to unreliable annotations. In this context, we propose a new framework called Video Annotation by Selection-and-Refinement (VASR) that aims to improve the accuracy of tracking results and reduce the need for manual annotations. We introduce two novel architecture designs, the T-Assess Net and the VG-Refine Net, to measure the quality of tracking results and integrate appearance and temporal geometry cues for refining the tracking accuracy. Our empirical results demonstrate that VASR can significantly reduce manual labeling efforts by 94.0% while achieving comparable and even more robust tracking performance compared to models trained with manual annotations. Our method serves as an effective tool to enhance the state-of-the-art tracking performance by providing high-quality annotations at a manageable cost.