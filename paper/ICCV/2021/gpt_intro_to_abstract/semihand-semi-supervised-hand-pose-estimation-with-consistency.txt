Monocular 3D hand pose estimation poses a challenge in obtaining high-quality ground-truth poses due to the difficulty in labeling real-world data. Synthesizing training data has been used as an alternative, but there is a significant domain gap between synthetic and real-world images. To address this, a mix-and-train strategy has been proposed, but its success relies on the quantity and quality of labeled samples. In this paper, we explore a scenario where we learn from labeled synthetic data and fully unlabeled real-world data. We introduce SemiHand, a framework for domain-separated semi-supervised learning for 3D hand pose estimation. We utilize pseudo-labeling and consistency training to generate pseudo-labels for the unlabeled data and increase their reliability. We also address the domain gap by proposing cross-modal consistency and leveraging semantic predictions from an auxiliary task. Additionally, we introduce label correction and sample selection based on feasibility to mitigate the impact of confirmation bias. We demonstrate the effectiveness of our approach through experiments and comparisons with existing methods. Our results show improvements in synthetic image enhancement and outperform previous works.