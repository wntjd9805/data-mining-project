Camera calibration is a fundamental step in computer vision, connecting images to the physical world. Traditional methods use calibration objects, but these may not always be available or practical. Self-calibration, which does not require external objects, has been proposed as a solution. However, existing self-calibration algorithms have limitations: they use a simple linear pinhole camera model, rely on sparse image correspondences, and do not improve the 3D geometry of objects. In this paper, we propose a self-calibration algorithm that learns camera parameters for a generic camera model, including radial distortion and non-linear camera noise. We also introduce a geometric consistency constraint and train the system using photometric consistency. Experimental results show that our model can accurately learn camera parameters, even without standard initialization. Our model outperforms baselines in analyzing distortion and can be applied to other variants of the Neural Radiance Fields model.