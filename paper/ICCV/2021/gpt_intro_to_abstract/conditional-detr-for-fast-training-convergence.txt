The DEtection TRansformer (DETR) method has shown promising results in object detection by applying the transformer encoder and decoder architecture. It eliminates the need for hand-crafted components such as non-maximum suppression and anchor generation. However, the original DETR approach suffers from slow convergence during training, requiring 500 training epochs to achieve good performance. In contrast, the recent work on deformable DETR addresses this issue by incorporating deformable attention, which attends to a small set of key sampling points and uses the high-resolution and multi-scale encoder. In this paper, we propose a novel approach called conditional DETR, which retains the global dense attention mechanism but improves the decoder cross-attention mechanism to accelerate the training process. Our approach is motivated by the high dependency on content embeddings and the minor contributions made by spatial embeddings in cross-attention. We demonstrate that conditional DETR achieves faster convergence compared to the original DETR on various backbone architectures. Our findings suggest that the spatial attention weight maps from cross-attention in the original DETR trained with fewer epochs do not accurately highlight the extremities of objects, resulting in decreased performance. We compare our approach with anchor-based and anchor-free object detection methods and show that conditional DETR outperforms them in terms of convergence speed and accuracy. Overall, our conditional DETR approach offers a more efficient and effective solution for object detection in computer vision tasks.