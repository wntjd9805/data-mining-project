Deep neural networks (DNNs) have been widely adopted in many areas due to their success in various applications. However, the lengthy and involved training process of DNNs becomes a bottleneck, and to reduce the overhead, third-party resources are utilized. But this brings new security threats, specifically the emerging threat of backdoor attacks in the training process of DNNs. Backdoor attacks maliciously manipulate the prediction of DNN models by injecting attacker-specified patterns called backdoor triggers into poisoned training samples. These triggers can be invisible and only a small fraction of samples need to be poisoned, making the attack stealthy. Several backdoor defenses have been proposed, but they rely on the assumption that backdoor triggers are sample-agnostic. In this paper, we explore a novel attack paradigm where the backdoor trigger is sample-specific, breaking the fundamental assumption of current defenses. We generate sample-specific invisible additive noises as backdoor triggers using DNN-based image steganography. This attack paradigm easily bypasses existing defenses. The contributions of this paper include a comprehensive discussion on the success conditions of current backdoor defenses, exploration of a novel attack paradigm with sample-specific and invisible backdoor triggers, and extensive experiments that verify the effectiveness of our proposed method.