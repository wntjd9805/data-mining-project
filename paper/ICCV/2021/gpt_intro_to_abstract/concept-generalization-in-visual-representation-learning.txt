The increasing demand for large-scale data in deep models has led to the development of transfer learning techniques, which transfer learned representations from resourceful datasets to problems with limited annotations. The success of transfer learning relies on the quality of the learned representations, which can be determined by their generalization capabilities to various downstream vision tasks. While generalization to different input distributions or new tasks has been extensively studied, there is a need for a more principled analysis of generalization across different semantic concepts. This paper focuses on this last facet of generalization and introduces a systematic way to study concept generalization. The authors propose a benchmark, called ImageNet Concept Generalization (ImageNet-CoG), which evaluates models pretrained on a popular dataset (IN-1K) using a large ontology (IN-21K) to measure the semantic distance between seen and unseen concepts. The benchmark consists of five concept generalization levels, each containing a set of unseen concepts with increasing semantic distance to the seen ones. The authors conduct a large-scale study comparing 31 visual representation learning approaches on ImageNet-CoG, analyzing the impact of different architectures, levels of supervision, regularization techniques, and additional web data on concept generalization performance. The results provide valuable insights into the effectiveness of different techniques in achieving concept generalization.