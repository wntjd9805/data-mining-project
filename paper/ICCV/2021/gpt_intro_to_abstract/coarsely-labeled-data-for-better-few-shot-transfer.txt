Large annotated datasets have significantly contributed to the advancements in visual recognition systems. However, acquiring expert annotations for many recognition problems can be challenging and costly. This has led to the development of few-shot learning (FSL), which aims to train domain-specific learners capable of recognizing new classes with only a few examples. FSL systems still lag behind systems trained on large labeled datasets, leading to the question of whether the base dataset provides enough information about the novel classes. This paper proposes leveraging auxiliary information about novel classes that may be more readily available, such as unlabeled data or coarser labels. While unlabeled data can inform the learner about the data distribution, they do not contain semantic information. On the other hand, coarser labels, even if easy to acquire, can provide semantic knowledge about the class distinctions. Existing FSL techniques do not effectively use this additional information. The paper introduces a new approach called Parent-Aware Self-training (PAS) that leverages coarsely-labeled data for representation learning. The approach uses a base dataset classifier to provide fine-grained pseudo-labels to the coarsely-labeled data. These pseudo-labels are filtered to ensure consistency with the coarse labels, creating a fine-grained grouping of the novel examples. The model is then trained using these pseudo-labels to improve the feature representation and capture the unknown novel class distinctions.Experiments conducted on three different datasets demonstrate that using coarsely-labeled data improves five-shot accuracy by 5 to 15 points on the all-way classification setup. The proposed approach outperforms multitask training and provides an average improvement of 2 points in five-shot accuracy compared to other methods. These results highlight the effectiveness of leveraging auxiliary information and the ability of PAS to utilize it effectively.