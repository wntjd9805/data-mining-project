The lack of expertise in video editing poses a challenge for aspiring video creators. Composing the right cuts, which involves joining untrimmed videos to maintain continuity editing, is a time-consuming and critical task. This paper aims to explore the plausibility of artificial systems ranking video cuts. The task of video cut ranking is defined, where the goal is to find the best moments in each video to trigger cuts and create a single continuous sequence. The challenge lies in generating videos that give the audience an illusion of continuous action. Previous research in computer vision has not focused on this problem, with related works in graphics and HCI communities requiring significant manual work. Lack of data, particularly raw footage and corresponding edits, has hindered progress in this area. This paper introduces a learning-based method that leverages edited video content to learn audio-visual patterns that commonly trigger cuts. A large-scale dataset of professionally edited movies is used to train an audio-visual model called Learning-to-Cut, which ranks cuts via contrastive learning. Experimental results demonstrate the feasibility and effectiveness of data-driven models in ranking the plausibility of video cuts. This work brings two contributions: the proposal of Learning-to-Cut and the introduction of a benchmark and performance metrics for video cut ranking. Expert editors are shown to have a preference for cuts generated by the proposed method compared to random ranking and other baselines.