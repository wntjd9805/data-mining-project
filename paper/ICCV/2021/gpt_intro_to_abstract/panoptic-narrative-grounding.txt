This paper introduces the concept of Panoptic Narrative Grounding, a task formulation that combines computer vision and natural language processing to achieve high-level tasks such as image captioning, referring expression comprehension, and visual question answering. The paper proposes a spatially detailed visual grounding approach using segmentations instead of bounding boxes, and includes all panoptic categories to fully exploit semantic richness in visual scenes. The authors establish an experimental framework for studying this task and provide ground-truth annotations, metrics, and a strong baseline model. The paper also discusses the challenges of collecting pixel-wise annotations and presents a method for transferring annotations. The proposed baseline model, based on the Cross-Modality Relevance method, is the first natural language visual grounding method capable of aligning multiple noun phrases with panoptic region segmentations. The contributions of this paper include the formulation of the Panoptic Narrative Grounding problem, the establishment of an experimental framework, and the introduction of a strong baseline model. The authors provide all resources for reproducibility and further research on their project webpage.