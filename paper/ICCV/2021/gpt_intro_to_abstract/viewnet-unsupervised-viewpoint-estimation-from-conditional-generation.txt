Object viewpoint estimation is crucial for enabling autonomous systems to understand the 3D world. While earlier methods have shown success in controlled environments, recent work using modern learnable representations has demonstrated improvements in various vision tasks. This paper focuses on recovering the 3D pose of an object relative to the camera from a single image. Obtaining viewpoint annotations can be laborious and error-prone, making large-scale datasets challenging to create. To address this, the authors propose ViewNet, a self-supervised object viewpoint estimation method that learns from unlabeled image pairs. The method leverages multi-view consistency and does not require manual viewpoint annotations. The authors present a conditional generation approach that accurately estimates viewpoints from single images and outperforms related approaches in evaluations on ShapeNet and PASCAL3D+. They also highlight the limitations of current evaluation procedures and biases introduced by calibration steps. Overall, this paper offers a novel self-supervised method for object viewpoint estimation and provides insights into evaluation procedures in this field.