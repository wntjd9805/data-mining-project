This paper focuses on improving the performance of semantic segmentation in driving scenarios using 3D computer vision. The data is collected in the form of point clouds from Light Detection and Ranging (LiDAR) sensors. Existing methods for processing point cloud data include voxel-based and point-based representations, each with their own limitations. Voxel-based methods retain physical dimensions but suffer from quantization loss and require high resolution. Point-based methods can directly process points but have inefficient neighbor searching. Range-based methods use a spherical projection, which distorts physical dimensions and causes object overlap. The authors propose a deep and adaptive range-point-voxel fusion framework that combines the advantages of all three representations. They design a fusion strategy that transfers features between points, range-pixels, and voxel-cells, and apply an adaptive feature selection for each point. The proposed method improves performance and efficiency by utilizing hash mapping, sparse convolution, and efficient range processing. Additionally, the authors address the issue of class imbalance in the datasets by introducing a CutMix augmentation technique. The contributions of this paper include the fusion framework, the efficient interaction mechanism, and achieving state-of-the-art results on SemanticKITTI and nuScenes datasets.