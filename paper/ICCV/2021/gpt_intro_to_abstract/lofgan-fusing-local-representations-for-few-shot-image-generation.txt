Generative adversarial networks (GANs) have gained attention in recent years for their impressive results in visual tasks. However, most GAN models struggle with limited training data. Few-shot image generation, where the goal is to generate diverse images for a novel category with only a few available images, has become a topic of interest. Current few-shot generation approaches can be categorized into transformation-based, optimization-based, and fusion-based methods. Fusion-based methods aim to create a label-consistent mapping from conditional inputs to diverse outputs, but existing methods still have limitations in terms of generation capacity and imprecise generation space. To address these limitations, this paper proposes a novel local-fusion approach for few-shot image generation. This approach randomly selects a base image and uses other images as reference images. Local representations are matched and fused at a fine-grained level, resulting in generated images with fewer artifacts. Additionally, a new local reconstruction loss is proposed to better incorporate the input images during training. Experimental results demonstrate the effectiveness of the proposed method in achieving state-of-the-art performance in few-shot image generation.