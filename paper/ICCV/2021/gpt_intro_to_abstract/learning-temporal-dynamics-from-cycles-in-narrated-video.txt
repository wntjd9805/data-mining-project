Prediction is a fundamental problem in computer vision, and previous deep learning methods in this field have focused on predicting fixed offsets into the future. However, this formulation is flawed because it limits the scope of temporal transitions that can be learned and hinders downstream performance. To address this, we identify three central challenges in training a model to predict the future: the expensive and difficult annotation of videos with temporal relationships, the need for high-level concepts in modeling long-term transitions, and the varying duration of temporal transitions. In this paper, we propose a new self-supervised training objective called Multi-Modal Temporal Cycle Consistency (MMCC) and a model that learns a representation to solve it. MMCC addresses the challenges mentioned above by learning from large unlabeled datasets, leveraging abstract latent representations, and making predictions at varied offsets into the future. Our model, trained on the HowTo100M dataset, captures long-term dynamics in its predictive model of the future and can be applied to various practical applications. We demonstrate the importance of modeling temporal dynamics and the quality of the predictions by evaluating the learned representations and predictive models on various qualitative and quantitative tasks. Our model outperforms the state-of-the-art in self-supervised video prediction. Our contributions include the MMCC objective, a novel attention-based model, and a suite of evaluation tasks for self-supervised learning in video prediction.