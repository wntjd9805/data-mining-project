Category-level 6D object pose estimation is a critical task in computer science with various applications including robotic manipulation, augmented reality, and 3D scene understanding. Unlike conventional instance-level pose estimation, which only works for instances seen during training, category-level pose estimation aims to predict the position and orientation of novel objects within the same category. Current methods for this task still have limitations, particularly in capturing the shape variations across different objects within a category. To address this, previous approaches have used a canonical object space and category-level shape priors. However, these priors are static and not adaptable to individual instances, leading to poor generalization performance. In this paper, we propose a novel Structure-Guided Prior Adaptation network (SGPA) that dynamically adapts the category-level prior to each instance based on their structural similarity. We use a transformer network to model the global structure similarity between the prior and the instance's point cloud, and inject the instance's semantic information into the prior features. We also design a structure regularized low-rank transformer to leverage the inherent structure characteristic of instances for more effective prior adaptation. Our experimental results demonstrate significant performance improvements over existing methods for category-level 6D object pose estimation.