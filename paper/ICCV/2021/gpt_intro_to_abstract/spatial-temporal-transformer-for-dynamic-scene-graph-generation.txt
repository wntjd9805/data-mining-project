Scene graphs have recently been successfully applied in various computer vision tasks, such as image retrieval, object detection, semantic segmentation, and image synthesis. They are considered a promising approach for holistic scene understanding and connecting the gap between vision and natural language domains. The task of scene graph generation, which summarizes objects and their relationships, has gained increasing attention. While significant progress has been made in static scene graph generation from a single image, the task of dynamic scene graph generation from a video is relatively new and more challenging. The popular approach of static scene graph generation relies on object detectors and does not consider temporal dependencies, making it unsuitable for dynamic scene graph generation. Action recognition is an alternative but fails to decompose activities into consistent groups and hierarchical structures. This paper introduces a novel framework called Spatial-Temporal Transformer (STTran) for generating dynamic scene graphs effectively. It encodes spatial context within frames and decodes visual relationship representations with temporal dependencies across frames. The framework applies multi-label classification for relationship prediction and introduces a strategy for generating a dynamic scene graph with confident predictions. Experimental results demonstrate the positive effect of temporal dependencies on relationship prediction, and STTran achieves state-of-the-art results on the Action Genome dataset.