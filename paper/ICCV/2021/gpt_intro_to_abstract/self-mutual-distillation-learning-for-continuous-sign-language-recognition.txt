Sign language plays a crucial role in communication for the hearing-impaired population, conveying meaning through manual elements and facial expressions. Automatic Sign Language Recognition (SLR) aims to bridge the communication gap by translating sign language videos into corresponding sign gloss sequences. However, the lack of frame-level annotations and limited dataset scale pose challenges for video-based Continuous Sign Language Recognition (CSLR). Recent studies have proposed deep network architectures with visual and contextual modules, but end-to-end training hinders the learning of effective visual features and reduces network performance on test sets. To address these issues, this study proposes a knowledge distillation method called Self-Mutual Knowledge Distillation (SMKD) that encourages the contextual module to focus on visual information and enhances the discriminative capability of the visual module. SMKD shares the weights of the classifiers in both modules and performs CTC training simultaneously. To alleviate the spike phenomenon caused by the CTC loss, a gloss segmentation technique is introduced, where pseudo gloss segment labels are produced by the Gloss Segment Boundary Assignment (GSBA) algorithm. These mechanisms enhance the discriminative power of visual features and improve generalization capability. The proposed SMKD method is evaluated on two CSLR benchmarks, showing its effectiveness in improving the performance of CSLR systems.