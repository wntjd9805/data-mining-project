Head-mounted and wearable devices have become increasingly available and mature, providing opportunities to develop tools for remote communication and collaboration. Mixed reality devices, such as Microsoft HoloLens, allow for the display and viewing of 3D content in physical space with local or remote collaborators. However, effectively communicating with remote collaborators using such devices presents a challenge in accurately representing the motion of a person wearing the headset. To address this challenge, this paper proposes a model for reconstructing the articulated pose of a human skeleton using noisy streams of head and hand pose. The model utilizes the variational autoencoder framework and formulates a pose likelihood model that is factorized into approximately Gaussian models for each joint in the skeletal model. The accuracy of pose prediction can be improved by pretraining the generative model on full body poses and incorporating a temporal history of head and hand poses. This research differs from previous work in that it focuses on predicting body poses from sparse signals (head and hands only) and considers motion cues for prediction.