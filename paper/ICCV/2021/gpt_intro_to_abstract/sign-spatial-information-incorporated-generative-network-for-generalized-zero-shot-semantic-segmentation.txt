This paper introduces the problem of zero-shot semantic segmentation and investigates the incorporation of spatial information to improve its performance. Previous work has mainly focused on generative methods for zero-shot semantic segmentation, but the use of spatial information has not been widely studied. In this work, the authors propose a novel Relative Positional Encoding scheme to incorporate spatial information without dividing the input image into patches. Additionally, they propose an Annealed Self-Training strategy inspired by knowledge distillation to generate better pseudo-annotations for self-training. The authors evaluate the performance of their method on three benchmark datasets and conduct ablation experiments to demonstrate its effectiveness. The contributions of this paper include the introduction of the Spatial Information Module and the AST self-training strategy, as well as the evaluation of their method's performance.