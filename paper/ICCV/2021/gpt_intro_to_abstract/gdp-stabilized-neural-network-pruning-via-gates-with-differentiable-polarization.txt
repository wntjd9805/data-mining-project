We introduce the concept of model compression techniques for Deep Neural Networks (DNNs) to accommodate devices with limited computing and storage capabilities. One popular method for model compression is pruning, which includes unstructured and structured pruning. In this paper, we focus on structured pruning, which eliminates entire neurons or blocks while preserving the model's structure and connections. Importance-based pruning methods are commonly used but lack clarity on the criteria for measuring importance, leading to various selection heuristics. Existing methods also require pruning from a skewed yet continuous histogram of weight values, making the selection of the pruning threshold critical yet ad-hoc. Additionally, removing many small non-zero weights can harm the network and require fine-tuning. Sampling-based methods, which sample sub-networks or calculate expectations over the entire network, also have limitations such as unstable training or converging at local minima. To address these issues, we propose Gradient-based Dynamic Pruning (GDP), which efficiently prunes DNNs based on gradient magnitude and dynamic threshold selection. GDP is easy to use, produces stable training, incurs minimal computational overhead, and does not rely on specific modules. Our experiments on CIFAR-10 and ImageNet classification tasks demonstrate that GDP outperforms previous methods significantly. Additionally, GDP achieves improved performance with reduced FLOPs in the Pascal VOC segmentation task using DeepLabV3Plus-ResNet50.