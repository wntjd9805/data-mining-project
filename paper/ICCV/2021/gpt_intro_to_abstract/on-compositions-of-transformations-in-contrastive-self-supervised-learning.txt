In recent years, several works have demonstrated the feasibility of pre-training high-quality image representations without the use of manually-labeled data. These methods incorporate noise contrastive learning and aim to learn image representations that are invariant to certain transformations while being distinctive to changes that alter the meaning of an image. However, the selection and composition of these transformations have been based on intuition rather than formal understanding. In this paper, we provide a formal analysis of composable transformations in contrastive learning and shed light on the principles that guide the construction of training batches. Our analysis reveals how invariance and distinctiveness to individual transformations can be effectively incorporated into the learning formulation. We also examine existing sampling schemes, such as the one used in SimCLR, and demonstrate their advantages and disadvantages. Moreover, we extend our analysis to contrastive learning for video data, which introduces temporal dynamics and multiple modalities. By considering a class of generalized data transformations (GDTs), we show how these effects can be uniformly integrated into contrastive learning. This allows us to design a coherent learning objective and batch sampling strategy based on a small number of design principles identified through our analysis. Our experimental results show that using a wider range of transformations significantly improves the performance of contrastive learning for video representation compared to traditional image-centric methods like SimCLR. Leveraging temporal dynamics and multiple modalities leads to substantial performance gains, almost doubling the performance. We also find that solely focusing on learning invariance to transformations is not optimal for video data; instead, combining invariance to certain factors with distinctiveness to others yields better results. This finding is novel in contrastive learning. Additionally, we achieve state-of-the-art results in audio-visual representation learning on various downstream tasks, including action recognition benchmarks. Our pre-training models on both small and large video datasets outperform previous methods, achieving high accuracy on standard benchmarks such as UCF-101 and HMDB-51. Overall, our work contributes to the understanding and improvement of contrastive learning for image and video representation, providing insights into the design of more effective training strategies.