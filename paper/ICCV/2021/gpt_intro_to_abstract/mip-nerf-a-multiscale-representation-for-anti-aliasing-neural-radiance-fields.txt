Neural volumetric representations have become popular in the field of computer graphics for learning to represent 3D objects and scenes from images. One such representation, called neural radiance fields (NeRF), has shown impressive results in rendering photorealistic novel views. However, NeRF's rendering model suffers from excessive blurring and aliasing. This paper introduces mip-NeRF, a novel extension of NeRF that addresses these issues. Mip-NeRF leverages the concept of mipmapping from computer graphics to prevent aliasing artifacts. It represents the prefiltered radiance field for a continuous space of scales and uses an integrated positional encoding (IPE) to compactly encode the region of space surrounding a 3D position. Experimental results show that mip-NeRF outperforms NeRF in accuracy, especially in scenarios where scene content is observed at different resolutions. Additionally, mip-NeRF achieves comparable performance to NeRF with slightly faster computation and fewer parameters.