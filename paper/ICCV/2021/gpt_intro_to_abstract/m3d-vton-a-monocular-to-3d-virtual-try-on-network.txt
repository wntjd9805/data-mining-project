3D virtual try-on, the process of fitting a specific clothing item onto a 3D human shape, has gained attention in both research and commercial domains. Previous approaches have focused on physics-based or scan-based methods, but recent interest has shifted towards learning-based 3D try-on methods. However, these methods often rely on predefined digital wardrobes and suffer from slow inference speeds due to the use of parametric 3D representations. Alternatively, image-based virtual try-on methods have been explored extensively, but they lack the ability to represent the human body in 3D. To address these limitations, we propose a Monocular-to-3D Virtual Try-On Network (M3D-VTON) that combines both 2D image-based virtual try-on and 3D depth estimation. M3D-VTON consists of three modules: the Monocular Prediction Module (MPM) for regression of transformation parameters, conditional person segmentation, and full-body depth estimation; the Depth Refinement Module (DRM) for enhancing geometric details in the depth map; and the Texture Fusion Module (TFM) for synthesizing the try-on texture using both 2D and 3D information. Our approach improves upon existing methods by reconstructing realistic 3D clothed humans, while also being computationally efficient. We introduce a self-adaptive pre-alignment strategy for more accurate geometric matching, utilize shadow information and a depth gradient constraint for better geometric recovery, and construct a new synthesized 3D virtual try-on dataset (MPV-3D) to foster further research in this field. Extensive experiments demonstrate the superior shape recovery and texture generation ability of our M3D-VTON approach.