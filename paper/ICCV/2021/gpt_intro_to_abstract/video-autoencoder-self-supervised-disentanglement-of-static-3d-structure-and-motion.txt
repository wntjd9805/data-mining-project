In this paper, we present a method for learning a disentangled 3D scene representation using videos as training data. Our approach separates the static 3D scene structure from the camera motion by leveraging the temporal continuity within a video. We propose a Video Autoencoder formulation, where an input video is encoded into two codes: one for the 3D scene structure and the other for the camera trajectory. The 3D structure is represented using 3D deep voxels, while the camera pose is represented by a 6-dimension rotation and translation vector. We reconstruct the original video by applying the camera transformation to the 3D structure features and decoding back to pixels. Our framework has the advantage of providing 3D representations that can be easily integrated with modern neural rendering methods, without the need for ground truth annotation or running Structure-from-Motion (SfM) as preprocessing. We demonstrate the effectiveness of our method on various downstream tasks, including novel view synthesis, pose estimation in video, and video following. Our experiments show improved view synthesis results compared to state-of-the-art approaches, even on out-of-domain data. Additionally, we show that our method can factorize structure from motion in novel videos and achieve video following by swapping the 3D structure and camera trajectory codes between videos.