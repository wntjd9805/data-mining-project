Deep neural networks are effective in learning from large labeled datasets but struggle to generalize this knowledge to new domains. Active Learning has studied the problem of identifying informative instances for labeling but often focuses on learning a model from scratch and does not operate under a domain shift. In this paper, we focus on the problem of active learning under domain shift, known as Active Domain Adaptation (Active DA). Active DA aims to select target instances for labeling in order to adapt a source model to an unlabeled target domain. Existing active learning methods based on uncertainty or diversity sampling are less effective for Active DA. We propose CLUE, an active learning method designed for Active DA that selects instances that are both uncertain and diverse in feature space. CLUE leads to more cost-effective adaptation than competing active learning and Active DA methods. Active DA has practical utility in scenarios where collecting a new dataset for every new deployment domain is impractical or when labeling in the target domain is prohibitive. Despite its practical utility, there has been limited follow-up work on Active DA since its introduction. We compare CLUE with state-of-the-art methods on challenging domain shifts and show that CLUE outperforms them in Active DA as well as in active learning. Our method improves the state-of-the-art in Active DA and active learning across multiple learning strategies on various benchmarks for image classification.