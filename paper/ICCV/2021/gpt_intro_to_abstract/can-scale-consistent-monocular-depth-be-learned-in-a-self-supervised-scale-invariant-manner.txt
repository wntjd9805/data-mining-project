Self-supervised monocular depth estimation has gained interest as a means of alleviating the need for high-quality ground truth depth data. However, these approaches often suffer from the per-frame scale ambiguity issue and lack of globally consistent trajectories. In this paper, we propose a new paradigm for self-supervised depth estimation that achieves both scale-consistency and scale-invariant training. We introduce a scale-aware geometric loss that enforces scale consistent depth estimation and a two-stream depth network that disentangles depth and scale prediction. Through iterative training, depth consistency is propagated through entire sequences. Our method improves depth accuracy and benefits long-term ego-motion estimation. Experimental results on KITTI datasets validate the effectiveness of our approach. Contributions of this work include a new self-supervised depth estimation framework, a scale-aware geometric loss, and a two-stream depth network for disentangling depth and scale prediction.