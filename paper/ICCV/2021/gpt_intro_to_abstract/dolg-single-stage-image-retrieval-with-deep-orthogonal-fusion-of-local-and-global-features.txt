Image retrieval is an important task in computer vision, aimed at finding images from a large-scale database that are similar to a query image. Previous methods have used handcrafted features, but recent advancements in deep learning have led to significant progress. In this paper, we focus on image retrieval with deep networks and propose a novel single-stage approach that combines global and local features. Previous solutions typically involve a two-stage paradigm, retrieving candidates using global features and then re-ranking them with local features. However, this approach involves ranking images twice and can accumulate errors. To overcome these limitations, we propose a unified single-stage image retrieval framework that integrates local and global features into a compact descriptor. Our model, called DOLG, consists of a local and a global branch with an orthogonal fusion module to combine them. By decomposing the orthogonal components from the local features and concatenating them with the global feature, we can extract critical local information while eliminating redundant components. We enhance local feature learning with multi-atrous convolutions and self-attention mechanisms. Experimental results on benchmark datasets demonstrate the effectiveness of our framework, which outperforms previous two-stage methods and achieves state-of-the-art performance. Our contributions include proposing a single-stage image retrieval paradigm with an orthogonal global and local feature fusion framework, designing a module for attentively extracting discriminative local features, and providing comprehensive analysis and experimental evidence of the effectiveness of our approach.