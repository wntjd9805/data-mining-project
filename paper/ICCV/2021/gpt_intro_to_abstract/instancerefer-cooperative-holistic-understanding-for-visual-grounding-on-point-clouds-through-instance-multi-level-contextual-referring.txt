In this paper, we address the problem of visual grounding on point clouds, which involves localizing desired objects or areas in 3D data based on object-related linguistic queries. Previous 2D-based approaches are not suitable for 3D scenarios due to the sparse and irregular nature of point clouds. We propose a novel framework called InstanceRefer that uses panoptic segmentation and language cues to select instance point clouds as candidates for visual grounding. Our framework incorporates three contextual learning modules to capture the attributes, relationships, and global localization of each candidate. We also introduce a matching module and contrastive strategy to efficiently and effectively select and localize the target. Our experimental results on the ScanRefer and Sr3D/Nr3D datasets demonstrate that InstanceRefer outperforms previous methods and achieves state-of-the-art performance.