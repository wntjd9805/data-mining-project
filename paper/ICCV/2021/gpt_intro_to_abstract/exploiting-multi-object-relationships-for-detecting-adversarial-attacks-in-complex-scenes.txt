Abstract:Deep neural networks (DNNs) have demonstrated exceptional performance in vision tasks, but they are susceptible to adversarial examples. Current defense strategies rely on capturing intrinsic dependencies within the input data in order to detect adversarial attacks. However, there is limited work on utilizing context for detecting adversarial attacks. In this paper, we propose a novel model-agnostic approach based on object co-occurrence to detect adversarial perturbations against object detectors. We use recent advances in natural language models to learn the dependencies between objects based on co-occurrence and detect adversarial attacks as violations of the learned context model. Our method achieves high detection performance in different types of adversarial attacks on large-scale datasets, outperforming state-of-the-art attack detection methods that do not use context and performing comparably to previous context-inconsistency-based approaches that are model-dependent. Our approach provides a promising defense strategy against adversarial attacks on DNN-based vision systems.