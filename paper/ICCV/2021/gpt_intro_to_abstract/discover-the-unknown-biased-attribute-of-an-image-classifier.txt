Deep neural networks have achieved impressive performance on various tasks by training on large-scale datasets. However, these AI algorithms often learn biases, which can lead to unfair and discriminatory outcomes. Previous methods for identifying biases rely heavily on human experts to speculate potential biases, which may result in overlooking certain biases. Additionally, these methods require expensive human efforts to collect testing images and annotate biased attributes, making them time-consuming and not scalable. To address these challenges, we propose a novel framework for discovering unknown biases in AI algorithms with less human effort. Our framework formulates the problem as the task of discovering the unknown biased attribute of a classifier that predicts a target attribute of an input image. We utilize generative models to optimize a biased attribute hyperplane in the latent space, representing the unknown biased attribute. We propose a total variation loss to optimize the hyperplane without any attribute labels and an orthogonalization penalty to ensure the discovered biased attribute is different from the known ones. To interpret the optimized hyperplane, we generate a sequence of traversal images, which provide the semantic meaning of the optimization result. Through experiments on disentanglement datasets and face datasets, we demonstrate the effectiveness of our method in discovering biased attributes. Moreover, our method shows generalizability in detecting biases in various image domains. The contributions of our work include proposing the unknown biased attribute discovery task, developing a novel method to solve this task, and providing comprehensive experiment settings and evaluation metrics. Our framework has the potential to benefit related fields, such as algorithmic de-biasing and dataset auditing.