The goal of this paper is to address the challenging problem of image-based holistic 3D indoor scene understanding in computer vision. Scene context, which captures high-order relations between objects, has been widely used to improve scene understanding. However, with the emergence of deep learning methods, the importance of top-down context versus bottom-up local appearance-based approaches for scene parsing is unclear. This paper proposes a novel deep learning framework that leverages both local image information and global context for panoramic 3D scene understanding. The proposed framework achieves high performance in terms of geometry accuracy and object arrangement. The key to this performance gain is a novel context model that predicts relations across objects and room layout, which are used to adjust the object arrangement. The optimization process is fully differentiable and can be combined with any neural network for joint training. However, existing panoramic scene datasets for holistic 3D scene parsing are lacking, so a new dataset is created to train and evaluate the proposed model. The contributions of this paper include the first deep learning pipeline for holistic 3D scene understanding, the design of a novel context model, and the creation of a new dataset for panoramic 3D scene understanding. The proposed model achieves state-of-the-art performance in terms of geometry accuracy and 3D object arrangement.