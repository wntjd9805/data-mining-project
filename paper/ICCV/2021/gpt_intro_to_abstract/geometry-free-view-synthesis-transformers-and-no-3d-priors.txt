This paper introduces the problem of novel view synthesis (NVS) in computer vision, which aims to generate new views of a scene based on a single initial image and a desired change in viewpoint. The authors specifically focus on modeling large camera transformations, such as rotating the camera by 90 degrees, to enable a more interactive, 3D exploration of the depicted scene. They propose a probabilistic generative model that learns the distribution of possible target images and synthesizes them at high fidelity. The paper also discusses the difference between interpolation (in the multi-view setting) and extrapolation (with a single image) tasks, highlighting the novelty and appeal of the latter. The authors argue that existing approaches for single-view synthesis, which typically rely on small camera transformations, are insufficient for achieving large transformations and propose their probabilistic framework to address this challenge. They also compare the use of explicit and implicit geometry in transformer architectures and find that transformers can learn the required geometric transformation implicitly without the need for hand-engineered operations. The contributions of this work include the proposed probabilistic model for single view synthesis, the analysis of the need for explicit 3D inductive biases in transformer architectures, and the investigation of recovering explicit depth representation from implicitly learned geometric transformations in transformers. The experimental results demonstrate the effectiveness and benefits of the proposed approach.