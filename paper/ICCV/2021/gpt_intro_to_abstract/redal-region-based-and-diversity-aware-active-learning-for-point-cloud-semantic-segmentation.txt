Point cloud semantic segmentation plays a crucial role in applications like indoor robotics and autonomous driving. While supervised deep learning approaches have made significant advancements with the help of large-scale datasets, the process of obtaining and annotating point-by-point labeled datasets is still expensive and challenging. This is due to the large number of points in a room-sized point cloud scene and the complex annotation process involved. In order to address the burden of manual labeling, previous works have attempted to reduce the number of labeled point cloud scans or lower the annotation density within a single scan. However, these methods overlook the fact that not all regions in a scan contribute equally to the model's performance. Our work introduces a novel Region-based and Diversity-aware Active Learning (ReDAL) framework that actively selects informative and diverse sub-scene regions for annotation, resulting in a reduction in annotation costs while maintaining model performance. We calculate the information score of each region using a combination of softmax entropy, color discontinuity, and structural complexity. Additionally, we develop a diversity-aware selection algorithm to avoid redundant annotations and further reduce manual labeling effort. Experimental results on indoor and outdoor datasets demonstrate the superiority of our proposed method compared to existing deep active learning approaches. We achieve high-performance semantic segmentation with significantly fewer annotations, paving the way for more efficient 3D deep active learning.