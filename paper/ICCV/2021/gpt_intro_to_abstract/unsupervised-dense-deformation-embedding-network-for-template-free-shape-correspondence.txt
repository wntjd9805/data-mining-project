The alignment of deformable 3D shapes is a common challenge in computer vision and graphics with various applications. Traditional methods rely on optimizing correspondences and objective minimization, but they can be limited by initialization and local minima. Deep learning techniques have been applied to predict large deformations, but they often neglect deformation disciplines and over-parameterize the deformation, resulting in high-frequency artifacts. Moreover, existing methods often neglect low-level geometric details and struggle with establishing connections between deformable shapes. To address these challenges, this paper proposes a new Unsupervised Dense Deformation Embedding Network (UD2E-Net) that combines the Embedded Deformation technique and deep learning to predict deformations between arbitrary shape pairs. The UD2E-Net leverages local features and dense embedding and fusion to reason about deformation parameters for each node. An Extrinsic-Intrinsic Autoencoder is used to encode source features into intrinsic coordinates and synthesize corresponding target features, and a Bounded Maximum Mean Discrepancy loss is designed to minimize the distribution gap between the synthesized and original target features. To overcome artifacts caused by incorrect node-to-vertex assignment, a trace and propagation algorithm is developed. Experimental results demonstrate the effectiveness of the UD2E-Net in various applications, including shape retrieval and human pose transfer. Overall, the contributions of this work are the development of UD2E-Net, the design of the Extrinsic-Intrinsic Autoencoder, the introduction of the Bounded Maximum Mean Discrepancy loss, and the development of the trace and propagation algorithm to improve the quality and efficiency of deformation.