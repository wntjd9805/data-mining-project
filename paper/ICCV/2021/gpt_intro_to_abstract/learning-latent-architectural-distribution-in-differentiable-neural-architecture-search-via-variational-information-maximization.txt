Abstract:Neural architecture search (NAS) has gained attention in the field of deep learning as it automates the process of finding the optimal network architecture for specific datasets. NAS can be seen as searching for the architectural preferences of a given dataset, where the architectural parameters represent the latent representations of specific data points. Existing differentiable NAS approaches make assumptions that conflict with the intrinsic properties of architecture and have prohibitive search costs. In this paper, we propose Variational Information Maximization Neural Architecture Search (VIM-NAS), a novel search strategy that maximizes the mutual information between data points and the latent architectural representations. VIM-NAS utilizes a simple yet effective convolutional neural network to model the dependencies among architectural distributions and optimizes for a tractable variational lower bound to the mutual information. Experimental results show that VIM-NAS achieves the fastest convergence speed within one epoch and achieves state-of-the-art performance on various search spaces, including DARTS, NAS-Bench, and simplified search spaces.