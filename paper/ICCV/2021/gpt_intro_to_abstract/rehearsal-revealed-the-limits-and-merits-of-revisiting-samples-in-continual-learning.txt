Recent advancements in neural networks have demonstrated their ability to outperform humans in various tasks. However, one major limitation in neural network optimization is the assumption of independent and identically distributed (iid) training and testing distributions. When this assumption is not met, neural networks are prone to catastrophic forgetting, where they completely forget previously acquired knowledge. Continual or lifelong learning aims to address this limitation by employing mechanisms such as rehearsal, which approximates observed input distributions over time and resamples from this approximation to prevent forgetting. While rehearsal has shown promising results, there is a lack of fundamental analysis on why it works and what its limitations are. In this paper, we investigate the internal workings of rehearsal from the perspective of loss landscapes. We define two fundamental open questions: why does rehearsal work despite overfitting to the rehearsal memory, and how does overfitting on the rehearsal memory affect generalization? We propose two hypotheses and provide empirical evidence on MNIST, CIFAR, and Mini-Imagenet datasets to test these hypotheses. Our findings support the idea that rehearsal tends to reside in the same low-loss region as the task minimum and show that overfitting on the rehearsal memory harms generalization. We further propose a heuristic to mitigate the negative effects of overfitting in rehearsal. These findings contribute to a deeper understanding of catastrophic forgetting and provide insights for improving rehearsal-based methods.