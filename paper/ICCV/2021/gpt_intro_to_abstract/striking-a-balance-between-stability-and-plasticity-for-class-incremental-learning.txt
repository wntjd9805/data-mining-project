This paper addresses the challenge of class-incremental learning (CIL), which involves continuously updating a trained model with samples from new classes without forgetting old ones. Existing approaches either suffer from overfitting to new classes or fail to incorporate knowledge of new classes to improve generalization. To resolve this stability-plasticity dilemma, the authors propose a reformulated baseline method called SPB that modulates the balance between knowledge from old and new classes. Additionally, they introduce two new approaches: SPB-I incorporates a class-independent learner to bridge the gap between new and old classes, while SPB-M exploits knowledge from multiple perspectives to enhance understanding of both old and new classes. Experimental results demonstrate that the proposed methods outperform state-of-the-art approaches in various CIL tasks.