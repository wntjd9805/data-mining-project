This paper addresses the problem of real-time super-resolution (SR) implementation for video on edge devices with limited resources. Deep learning-based SR algorithms have shown superior visual performance but are computationally expensive. Weight pruning is often used to reduce computation and power consumption, but existing approaches have limitations. In this paper, the authors propose a framework that combines architecture and pruning search to find the optimal configuration of SR blocks and pruning schemes for each layer. They train a supernet to provide a well-trained unpruned model for all possible combinations before the search, eliminating the need for separate training. The authors also employ Bayesian optimization to accelerate the search process. Their proposed method achieves real-time SR inference with competitive image quality on mobile platforms. This achievement has practical implications for applications such as live streaming and video communication.