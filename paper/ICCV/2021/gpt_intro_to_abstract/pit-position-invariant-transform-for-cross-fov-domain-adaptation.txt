Object detection and semantic segmentation are essential tasks in computer vision. Training models for these tasks requires large amounts of annotated data, which is time-consuming to prepare. Additionally, models trained on one dataset often perform poorly on another domain due to domain shift. Unsupervised Domain Adaptation (UDA) has been proposed to transfer knowledge from a labeled source domain to an unlabeled target domain. Two approaches to UDA, pixel-level alignment and feature-level alignment, have been explored in previous works. However, little attention has been paid to the influence of the camera's intrinsic parameters, specifically the Field of View (FoV), which can cause noticeable domain discrepancy. This paper focuses on addressing the adverse impact of FoV discrepancy between domains for cross-domain detection and segmentation tasks. The paper proposes the Position-Invariant Transform (PIT) to narrow the FoV gap by mapping pixels in the original imaging space to a spherical surface, aligning instance structural appearances in different positions. The paper also introduces a loss re-weighting strategy to speed up training. The effectiveness of PIT is verified on both detection and segmentation tasks, showing improved performance compared to previous UDA methods.