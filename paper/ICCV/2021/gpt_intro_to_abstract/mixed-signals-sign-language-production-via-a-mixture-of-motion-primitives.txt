This paper introduces the problem of Sign Language Production (SLP), which involves translating spoken language sentences into continuous sign language sequences and animating them expressively. Traditional approaches have treated these two sub-tasks as a single task, leading to under-expressive production. The authors propose a two-stage approach that includes an initial translation using gloss supervision and subsequent animation. They utilize a progressive transformer model for translation and propose a novel Mixture of Motion Primitives (MOMP) network for animation. The MOMP network employs a Mixture-of-Expert (MoE) architecture to combine distinct motion primitives. The authors evaluate their approach on the PHOENIX14T corpus and achieve state-of-the-art performance. They also demonstrate superior performance in translating sign sequences directly from spoken language compared to using an intermediate gloss representation. The contributions of this paper include the novel MOMP architecture, separate modeling of translation and animation sub-tasks, and improved performance on the PHOENIX14T dataset.