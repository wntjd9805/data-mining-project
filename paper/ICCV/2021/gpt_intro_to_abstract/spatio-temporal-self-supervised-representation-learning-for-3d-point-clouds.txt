This paper introduces the use of self-supervised learning for 3D point clouds in order to address the challenges of supervised training in this domain. The authors propose a spatio-temporal representation learning (STRL) framework that leverages the invariance and contextual cues present in 3D point clouds. The STRL framework uses two neural networks, online and target networks, to interact and learn from each other. By augmenting input pairs and considering temporal differences, the models can capture randomness and invariance across different viewpoints. The learned representations are then tested on various downstream tasks, including 3D shape classification, object detection, and semantic segmentation, where they outperform existing unsupervised methods and show improvements in semi-supervised learning with limited data. The authors also demonstrate that the learned representations can be generalized to different domains and achieve comparable or better performance than representations pre-trained on synthetic data. The simplicity of the learning strategy and the incorporation of spatio-temporal cues contribute to the satisfying performance of the learned 3D representations.