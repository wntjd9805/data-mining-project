Visual odometry (VO) is an essential technology for various vision applications, such as robotics and virtual/augmented/mixed reality. Over the past two decades, numerous algorithms have been proposed for VO, including geometric, deep learning-based, and hybrid approaches. However, motion blur remains a major challenge for visual odometry methods, degrading both feature-based and direct methods. In this paper, we present a novel hybrid visual odometry method that is robust to motion blur. Our method estimates the local camera motion trajectory within the exposure time for each frame, allowing us to explicitly model and compensate for motion blur during tracking. We also introduce a benchmarking dataset targeting motion blur-aware VO, providing sequences with varying levels of motion blur and accurate ground truth trajectories. Our experimental results demonstrate that our approach improves the robustness of visual odometry while maintaining comparable accuracy to images without motion blur. Furthermore, our motion blur-aware VO is capable of real-time performance on a standard laptop. By sharing our dataset with the research community, we aim to encourage further advancements in making VO more robust for real-world deployments.