Recent advances in deep learning have made it possible to develop highly accurate predictive models. However, in domains such as medical diagnosis, the adoption of black-box models raises concerns about the lack of interpretability, as potential risks may arise from the inability to understand how these models make predictions. Explainable artificial intelligence (XAI) has emerged as a promising approach to address this issue by providing insights into the inference process of deep models. A popular paradigm in XAI is attributive explanation, which identifies the contribution of pixels or regions to the final prediction. In this paper, we focus on understanding how these regions actually contribute to the decision-making process.To gain a better understanding, we introduce a fully-connected classifier that represents the inference process of the deep model. This classifier can be interpreted as a combination of discriminative patterns, each assigned a weight. These patterns can either be positive or negative, indicating their contribution towards or against a specific category. We define a set of linearly independent patterns for each category, which includes both positive and negative elements. By differentiating between positive and negative patterns, we can gather useful information about the decision-making process. In this work, we propose SCOUTER (Slot-based COnfig-Urable and Transparent classifiER), an explainable classifier that explicitly models positive and negative patterns. SCOUTER leverages the recently emerged slot attention module, known as xSlot, which offers an object-centric approach for image representation. The xSlot module identifies the spatial support of positive and negative patterns in the image and uses it as the confidence value for each category. This eliminates the need for a traditional fully-connected classifier and allows for the visualization of the support regions in the image. SCOUTER is characterized by its configurability over the patterns to be learned, allowing the incorporation of prior knowledge on the task. This controllability over the size of explanatory regions can be particularly beneficial in domains such as medicine and manufacturing.We experimentally evaluate SCOUTER and demonstrate its ability to learn both positive and negative patterns, as well as visualize their support in the given image. Our approach achieves state-of-the-art results in commonly-used evaluation metrics such as IAUC/DAUC. Additionally, we present a case study in medicine that highlights the importance of both types of explanations and the control over the size of explanatory regions. Overall, our transparent classifier provides a valuable tool for understanding deep models and their decision-making processes, especially in critical domains where interpretability is crucial.