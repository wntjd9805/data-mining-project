The success of modern deep learning relies on accurately labelled training data, but obtaining large quantities of gold-standard labelled data is often not feasible due to cost constraints. As a result, machine learning practitioners often use crowd-sourcing, web-crawled incidental annotations, or imperfect machine annotation, which can introduce label noise. This has led to the development of noise-robust learning approaches, including modifying the training algorithm and designing noise-robust loss functions. In this paper, we propose a data-driven AutoML approach to loss design and search for a simple white-box function that provides a general-purpose noise-robust drop-in loss. We perform evolutionary search on a parameterized space of loss functions and evaluate their performance on noisy data. By exploring domain randomization, we create a reusable loss function that can be applied to new datasets and architectures. We demonstrate the effectiveness and reusability of our learned loss on various benchmarks and a real-world noisy label dataset. Our approach offers a simple and fast replacement for conventional losses and does not require a clean validation set.