Learning temporal correspondence is a fundamental problem in computer vision, with applications in video object tracking, video object segmentation, and flow estimation. However, collecting dense annotations for large-scale videos is a time-consuming task. To address this, self-supervised methods have been developed to learn dynamic objects from unlabeled videos. This paper proposes a graph-based framework to learn temporal correspondence by capturing both longer and broader views. The video is represented as a joint space-time graph, with nodes representing grid patches and edges representing neighbor relations and similarity relations. The proposed method outperforms state-of-the-art self-supervised approaches on various visual tasks, such as object and pose tracking.