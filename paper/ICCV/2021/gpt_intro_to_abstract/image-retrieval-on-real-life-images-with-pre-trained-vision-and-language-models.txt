This paper introduces the task of composed image retrieval, which involves finding an image from a large database that best matches a user query presented as an image-language pair. Unlike traditional content-based or text-based image retrieval, composed image retrieval utilizes both visual and textual modalities to describe the user's intent. The challenge lies in the inherent ambiguity and underspecification of visual aspects to be preserved or modified. Existing datasets for this task are limited in their ability to adequately study this problem. To address this, the authors introduce the Compose Image Retrieval on Real-life images (CIRR) dataset, which focuses on distinguishing between visually similar images and places more emphasis on fine-grained vision-and-language reasoning. Additionally, the authors propose the Composed Image Retrieval using Pretrained LANguage Transformers (CIRPLANT) model, which leverages large-scale pretrained models to improve the performance of composed image retrieval. The proposed model adopts a metric learning pipeline and achieves state-of-the-art results on existing datasets and outperforms current methods on CIRR.