This paper introduces Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding in computer vision. It addresses the limitations of existing synthetic datasets and simulators by including publicly available 3D assets, complete scene geometry, material and lighting information, dense semantic instance segmentations, and disentangled image representations. The dataset consists of 77,400 images of 461 indoor scenes with detailed per-pixel labels and ground truth geometry. To generate the dataset, a computational pipeline is proposed, which leverages scenes downloaded from an online marketplace and utilizes a novel view sampling heuristic, cloud rendering system, and interactive mesh annotation tool. The dataset is analyzed at the scene, object, and pixel levels, and the costs of generating the dataset are assessed. Sim-to-real transfer performance is evaluated on semantic segmentation and 3D shape prediction tasks, demonstrating significant improvements and state-of-the-art performance. The entire rendered image data and code used are made available online.