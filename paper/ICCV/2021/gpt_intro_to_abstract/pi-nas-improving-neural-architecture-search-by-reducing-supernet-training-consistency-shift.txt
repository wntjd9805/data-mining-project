Automatic neural architecture search (NAS) has been a major focus in machine learning for the past four years. Early methods used reinforcement learning or evolutionary algorithms to search for high-performance architectures, but these methods are computationally expensive. Weight-sharing NAS methods have been proposed to alleviate this burden, where candidate architectures share weights and train simultaneously in a supernet. However, weight-sharing NAS methods still suffer from instability and inaccurate architecture ranking. In this paper, we identify the supernet training consistency shift, including feature shift and parameter shift, as the cause of this problem. We propose a novel supernet-⇧ model called ⇧-NAS to address these shifts. The ⇧-NAS model reduces feature shift by applying a consistency cost between predictions from different paths, and it reduces parameter shift by introducing a nontrivial mean teacher model that maintains an exponential moving average of weights. Our experiments demonstrate that ⇧-NAS improves architecture ranking and achieves state-of-the-art performance on various tasks. Furthermore, our ⇧-NAS method has the advantage of unsupervised learning, enabling the search for more transferable and universal architectures. Overall, this paper contributes to understanding and addressing the limitations of weight-sharing NAS methods and presents a novel approach, ⇧-NAS, that improves architecture ranking and achieves state-of-the-art performance.