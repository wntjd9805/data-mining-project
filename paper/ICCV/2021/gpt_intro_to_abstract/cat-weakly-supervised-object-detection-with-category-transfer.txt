Object detection plays a crucial role in computer vision, and many successful methods have been developed using deep neural networks. However, most of these methods require large amounts of high-quality annotations, which can be time-consuming and expensive to acquire. To address this issue, weakly supervised object detection methods have been proposed, which only require image-level category labels. However, these methods often suffer from issues such as instance ambiguity and low-quality proposals due to the lack of bounding-box-level supervision. As a result, there is still a significant performance gap between fully supervised and weakly supervised object detection methods. To narrow this gap, some previous methods have explored knowledge transfer from additional datasets. However, these methods either ignore the category information or are limited by the domain gap between datasets. In this paper, we present a category transfer framework for weakly supervised object detection that leverages both visually-discriminative and semantically-correlated category information from fully supervised datasets. We propose a double-supervised mean teacher network for overlapping category transfer and a semantic graph convolutional network for non-overlapping category transfer. Our framework bridges the domain gap and effectively exploits category information, leading to improved performance compared to state-of-the-art weakly supervised object detection methods. We conduct extensive experiments to validate the effectiveness of our approach. Our contributions include introducing a novel category transfer framework, proposing a double-supervised mean teacher network for overlapping category transfer, and developing a semantic graph convolutional network for non-overlapping category transfer. Our method outperforms existing weakly supervised object detection methods and achieves competitive results compared to fully supervised baselines on benchmark datasets.