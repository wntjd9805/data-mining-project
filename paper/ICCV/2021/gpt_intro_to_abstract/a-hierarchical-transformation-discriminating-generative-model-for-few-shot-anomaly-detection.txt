Anomaly detection is the task of identifying unusual samples in data. In this paper, we focus on the problem of one class classification, where the learner must classify new samples as normal or anomalous given a large collection of normal samples. We aim to solve this problem with very few training samples, including the case of a single training sample. This is motivated by the scarcity of training samples in visual domains and the human ability to solve this task with limited samples. Our model consists of two main components. The first component is a hierarchical generative model that captures patch statistics at multiple scales. This allows for the detection of anomalies in both global properties and local regions. The second component is a self-supervised learning task, where a classifier is trained on a proxy task of identifying transformations applied to input samples. This classifier can distinguish between normal and anomalous samples. Our proposed method combines these components into a single model. At test time, a sample is considered anomalous if many of its patches, at all scales, are determined to be anomalous according to learned discriminators. We demonstrate through experiments that our method outperforms recent baselines on various anomaly detection benchmarks, including the one-shot, five-shot, and ten-shot settings. We also show that our method outperforms the state-of-the-art in localizing anomalous patches in the field of defect detection. Overall, our work contributes a novel method for few-shot anomaly detection that can be extended to few-shot defect detection. We introduce unique components, such as a multiclass discriminator trained adversarially using labels from a self-supervised learning task, and a novel use of a discriminator at test time. We also apply transformations on patches at different scales and extend a hierarchical generative model to multiple images using a conditional generator. Our proposed method is extensively evaluated on five datasets and achieves better performance compared to baselines.