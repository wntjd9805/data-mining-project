This work focuses on text-video retrieval, which involves identifying the video that best matches a given natural language query. Video search has various applications, including wildlife monitoring, security, industrial process monitoring, and entertainment. With the increasing production of videos, effective and efficient video search is becoming crucial for platforms like YouTube. Recent retrieval methods have explored using multiple modalities to improve performance, such as mixtures-of-experts and multi-modal transformers. In this work, we investigate whether leveraging multiple text embeddings learned on large-scale written corpora can lead to similar gains in performance. We observe considerable variance and inconsistency in the performance of different text embeddings, motivating the use of multiple embeddings. Inspired by this finding, we propose the TEACHTEXT algorithm, which effectively utilizes the knowledge captured by collections of text embeddings. TEACHTEXT distills the information from different text embeddings into an enhanced supervisory signal for a student model. Our approach achieves a significant performance gain and is complementary to the addition of video modalities without incurring additional computational cost. Our contributions include the introduction of the TEACHTEXT algorithm, the effectiveness of learning the retrieval similarity matrix between joint query video embeddings, the application of noise elimination in training datasets, and the empirical demonstration of state-of-the-art performance on six text-video retrieval benchmarks.