Video understanding tasks, such as action recognition, temporal action localization, and spatio-temporal action detection, have gained significant research attention in recent years. While most existing methods focus on providing single labels or spatio-temporal extents of action instances, an ideal video understanding system should also provide detailed and structured interpretations of the entire scene. This structured representation, known as video scene graph, can improve action recognition accuracy and enable complex inference tasks. However, video scene graph generation (VidSGG) has received less research effort compared to image scene graph generation. Existing VidSGG benchmarks can be broadly categorized into video-level and frame-level scene graph representations. Video-level VidSGG requires accurate trimming of long videos into short clips, making it challenging to apply to untrimmed videos. On the other hand, previous works neglect frame-level scene graphs and rely heavily on object tracking. In this paper, we propose a new approach called TRACE (TaRget Adaptive Context AggrEgation Network) to simultaneously address video relation recognition and temporal tracking in VidSGG. The key idea is to generate a video scene graph at each frame using short-term video information and then track the frame-level scene graph over time to obtain the video-level result. TRACE utilizes an adaptive hierarchical relation tree and target-adaptive context aggregation to effectively capture complex spatio-temporal contextual information in videos. We evaluate TRACE on two datasets and show that it outperforms existing methods in terms of scene graph detection, classification, and predicate classification metrics. Our contributions include proposing the detect-to-track VidSGG paradigm, presenting a modular framework for spatio-temporal context information modeling, and introducing the hierarchical relation tree structure for efficient context information aggregation.