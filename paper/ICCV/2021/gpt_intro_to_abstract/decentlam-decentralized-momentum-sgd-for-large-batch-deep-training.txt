Efficient distributed training is crucial for large-scale deep learning tasks. Parallel SGD is a widely used algorithm for distributed optimization, but it incurs high bandwidth cost or latency due to global synchronization. Decentralized SGD based on partial averaging has emerged as a solution to reduce communication overhead. However, existing decentralized methods primarily utilize small-batch in their algorithm design, while recent advances in hardware enable large-batch training. Large-batch training has been extensively studied in Parallel SGD and has shown significant speedup. This motivates the study of how Decentralized momentum SGD (DmSGD) performs with large batch-size.In this paper, we compare the performance of DmSGD and Parallel momentum SGD (PmSGD) using small and large batches on Cifar-10 and ImageNet datasets. We find that while DmSGD achieves the same accuracy as PmSGD with small batch-size, it suffers from severe performance degradation in the large-batch scenario. We investigate the reasons behind this phenomenon and propose DecentLaM, a novel decentralized large-batch momentum SGD, to address the momentum-incurred bias in DmSGD. We establish the convergence rate of DecentLaM for both strongly convex and non-convex scenarios. Our theoretical results demonstrate the superiority of DecentLaM over existing decentralized momentum methods, especially as the batch size increases.Experimental results on various computer vision tasks and models show that DecentLaM outperforms DmSGD, DA/QG/D2-DmSGD, PmSGD, and PmSGD with layer-wise adaptive rate scaling (LARS) in terms of both training speed and accuracy. The rest of the paper is organized as follows: we provide a brief overview of related works and DmSGD in Sections 2 and 3, respectively. In Section 4, we identify the issue causing performance degradation in DmSGD and propose DecentLaM in Section 5 to resolve it. Convergence analysis and experimental results are presented in Sections 6 and 7, respectively.