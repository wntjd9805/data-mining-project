This introduction discusses the importance of information seeking through interaction in artificial intelligence, specifically in the context of human-agent interaction. It highlights the need for task-oriented agents to understand user beliefs, preferences, and intentions in order to interpret instructions accurately. The paper focuses on building effective information-seeking agents in goal-oriented visual dialogue, where agents engage in turn-taking dialogues to guess the Oracle's reference object by asking yes-no questions. The complexity of the presented image determines the type of questions to be asked, ranging from category-based questions to descriptive questions with referring expressions. Existing models in this field mostly generate simple category-based questions, which are not effective when faced with similar-looking candidates. To address this limitation, the paper proposes the Unified Questioner Transformer (UniQer) and introduces the CLEVR Ask task, which requires the Questioner to generate descriptive questions. UniQer unifies the question generator and the guesser into a single transformer architecture, allowing both components to use the same object features and effectively consider object relations. An object-targeting module is also introduced to determine the objects that should be addressed in the question. Experimental results demonstrate that the proposed model outperforms the baseline in terms of task success rate by approximately 20% and the ablation studies highlight the structural advantages of the model. The contributions of this paper include the introduction of a novel unified transformer architecture for the Questioner, the construction of the CLEVR Ask task, and the evaluation of UniQer's performance in asking descriptive questions in complex scenes with hard-to-distinguish objects.