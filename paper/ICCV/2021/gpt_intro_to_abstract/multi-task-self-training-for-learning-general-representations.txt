Visual representation learning is a fundamental problem in computer vision. Previous approaches, such as supervised and self-supervised pre-training, have shown promising results in transferring learned feature representations to downstream tasks. However, it is observed that features obtained from pre-training may not always be useful for specific tasks. In order to address this challenge, we propose Multi-Task Self-Training (MuST), a novel algorithm for large-scale multi-task feature learning in computer vision. MuST utilizes pseudo labeling, generated by training teacher models on datasets like COCO and Objects365, to distill knowledge from multiple tasks and datasets into a single student model. We demonstrate the effectiveness of MuST through experiments on various computer vision tasks, including image recognition, detection, segmentation, and 3D geometry estimation. Our results show that MuST outperforms supervised and self-supervised methods, and can even improve upon strong pre-trained models. We contribute MuST as a simple yet powerful algorithm for creating general visual representations that improve with more unlabeled data.