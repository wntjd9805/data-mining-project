This paper introduces a new approach to reconstructing category-centric 3D models of common objects in the wild. The authors address the challenge of a lack of real-world datasets with 3D ground truth by using a photogrammetric approach that leverages object-centric multi-view RGB images. They gather a large-scale dataset of almost 19,000 videos of 50 MS-COCO categories, each annotated with camera pose and with 20% of the videos annotated with a high-resolution 3D point cloud. The dataset is significantly larger than previous alternatives and includes point cloud annotations. The authors also propose a novel model called NerFormer that combines Transformers and neural implicit rendering to reconstruct object categories from a small number of input source views. NerFormer outperforms 14 baseline models that use common shape representations, demonstrating its effectiveness. This paper represents one of the first large-scale evaluations of learning 3D object categories in real-world settings.