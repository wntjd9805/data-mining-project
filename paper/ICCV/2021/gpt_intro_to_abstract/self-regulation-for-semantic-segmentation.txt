Semantic Segmentation (SS) is an essential component in computer vision systems for scene understanding, such as autonomous driving and medical imaging. Recent advancements in deep convolutional neural networks and the availability of pixel-level annotations have led to significant progress in SS research. However, there are still failure cases where small objects or minor object parts are missed, and minor parts of large objects are mislabeled as wrong classes. In this paper, we present our empirical inspections of these failure cases and propose a novel approach called Self-Regulation (SR) to address them. SR is generic and can be implemented in any deep backbones without the need for auxiliary data or supervision. It introduces three operations: regulating predictions with ground-truth labels, using shallow-layer features to regulate deeper-layer feature maps, and using deep-layer predictions to regulate shallower-layer predictions. Our empirical results demonstrate that SR significantly improves the performance of various baseline models in both fully-supervised and weakly-supervised SS settings. Our contributions include the introduction of SR operations and extensive validation of their effectiveness.