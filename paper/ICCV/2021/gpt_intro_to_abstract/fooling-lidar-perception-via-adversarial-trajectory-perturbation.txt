Autonomous driving systems rely on LiDAR sensors to gather geometric information about the environment. However, the movement of the vehicle can distort the LiDAR scans, as the points are sampled at different timestamps and locations. This distortion is typically compensated for by tracking the vehicle's pose using GPS, INS, or SLAM-based localization. While this motion compensation ensures accurate point cloud data, it also introduces a vulnerability in self-driving systems. By spoofing the wireless GNSS signals, hackers can deceive the perception modules of self-driving cars that rely on LiDAR point clouds processed by deep neural networks (DNNs). This presents a serious security issue that needs to be addressed. In this paper, we investigate this vulnerability by attacking the vehicle trajectory instead of directly manipulating the point cloud coordinates. We propose a method to simulate motion distortion using real-world LiDAR sweeps and demonstrate that LiDAR point clouds can be treated as a differentiable function of the vehicle trajectories. We also develop an approach called Fool LiDAR perception with Adversarial Trajectory (FLAT) to generate adversarial trajectory perturbations that are less perceptible. Our experiments on 3D object detection tasks show that advanced detectors can be effectively blinded by spoofed trajectories.