Deep neural networks (DNNs) have been widely deployed in various fields, but they are still vulnerable to adversarial perturbations, which poses security threats. Evaluating the robustness of DNNs in the black-box setting, where only the inputs and outputs are known, is a challenging task. This paper focuses on decision-based black-box attacks, where the goal is to craft an adversarial example classified as a target class. The existing decision-based attack methods often require a large number of queries and have inflexible coefficient adjustments. To address these issues, we propose an Adaptive History-driven Attack (AHA) that utilizes information from all queries with an adaptive coefficient adjustment strategy. AHA reduces the magnitude of perturbations iteratively by utilizing a source direction and a random direction guided by historical queries. We dynamically adjust the coefficient based on the actual reduction in perturbation magnitude. Extensive experiments show that AHA outperforms existing methods, generating smaller perturbations with the same number of queries. The effectiveness of AHA is further demonstrated on a real-world face verification API, where it achieves a 24.9% reduction in perturbation magnitude compared to the baseline method.