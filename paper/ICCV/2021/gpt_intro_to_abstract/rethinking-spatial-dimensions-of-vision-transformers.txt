This paper explores the use of dimension configurations in Vision Transformer (ViT) architectures and compares them to Convolutional Neural Networks (CNNs). While ViT has proven its image recognition ability, it does not incorporate design principles that have been effective in CNNs. The authors propose a Pooling-based Vision Transformer (PiT) that combines a newly designed pooling layer with the ViT structure to enable spatial size reduction. Experimental results show that the ResNet-style dimensions improve the capability and generalization performance of both ResNet and ViT. The authors also analyze the attention patterns inside ViT and PiT using entropy and average distance measures. The results demonstrate that PiT outperforms ViT on various tasks including ImageNet classification and object detection. The robustness of PiT is also evaluated in different environments. Overall, the findings suggest that incorporating dimension configurations from CNNs can enhance the performance of ViT architectures.