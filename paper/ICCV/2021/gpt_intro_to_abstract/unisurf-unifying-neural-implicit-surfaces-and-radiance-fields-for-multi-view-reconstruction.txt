Capturing the geometry and appearance of 3D scenes from images is a fundamental problem in computer vision. In recent years, neural models based on coordinates have emerged as a powerful tool for 3D reconstruction. These models use neural networks to parameterize continuous implicit functions as representations of geometry or appearance. They have shown impressive performance in reconstructing geometry and synthesizing novel views. The choice of the 3D representation and the rendering technique are important factors in multi-view reconstruction using neural implicit models. Some methods represent the implicit surface as a level set and render the appearance from surfaces, while others integrate densities by sampling along viewing rays. Surface rendering techniques have achieved impressive results, but they require per-pixel object masks and suitable network initialization. They are limited to object-level reconstruction and do not scale well to larger scenes. On the other hand, volume rendering methods have shown promising results for novel view synthesis in larger scenes, but the surfaces extracted from the underlying volume density are often non-smooth. In this paper, we propose a unified formulation, called UNISURF, which enables accurate surface reconstruction from images without the need for object masks. Our method addresses the limitations of existing surface rendering techniques and provides a more scalable solution for 3D scene reconstruction.