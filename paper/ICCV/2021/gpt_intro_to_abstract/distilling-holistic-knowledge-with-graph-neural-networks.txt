Deep Neural Networks (DNNs) have achieved remarkable success in various applications. However, their reliance on extensive computational and storage resources makes them impractical for embedded and mobile systems. Knowledge distillation has been proposed as a solution to transfer knowledge from a larger well-trained teacher network to a smaller student network. The success of knowledge distillation relies on the extraction of two types of knowledge: individual knowledge and relational knowledge. While previous methods have extracted these types of knowledge independently, ignoring their inherent correlations, our proposed Holistic Knowledge Distillation (HKD) method aims to integrate both types of knowledge using graph neural networks. We construct attributed graphs for each network, where nodes represent instances and edges are constructed based on predictions. By leveraging the power of Graph Neural Networks (GNNs), we extract holistic knowledge by aggregating node attributes from the neighborhood samples, resulting in a unified graph-based embedding. We demonstrate that existing individual and relational knowledge are special cases of holistic knowledge under certain conditions. Rather than directly aligning the graph-based embeddings, which may be too strict for the student network, HKD maximizes the mutual information between the teacher and student network's graph-based representations using the InfoNCE estimator in a contrastive manner. The holistic knowledge guides the student network's learning by encouraging the learning of similar instance features and relational neighborhoods. We also employ the memory bank technique to enhance training efficiency. Our contributions include the proposal of HKD, a method that efficiently distills holistic knowledge, the integration of individual and relational knowledge using graph neural networks, and extensive experiments demonstrating the effectiveness of the proposed HKD method.