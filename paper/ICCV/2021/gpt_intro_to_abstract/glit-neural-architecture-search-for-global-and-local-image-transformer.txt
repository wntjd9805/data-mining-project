In recent years, Convolutional Neural Networks (CNN) have greatly contributed to the success of deep learning in computer vision tasks. However, there is a growing interest in developing more efficient architectures based on the transformer, a popular architecture in Natural Language Processing (NLP) tasks. The vision transformer (ViT) is one such architecture that solely relies on self-attention modules in the transformer for establishing global correlations among input image patches. While ViT has achieved impressive performance, it still falls short in accuracy compared to well-designed CNN models. In this paper, we argue that there are significant gaps between different data modalities, such as image and text, which result in disparities between tasks. Therefore, directly applying the vanilla transformer architecture to other tasks may not be optimal. To address this, we propose a Neural Architecture Search (NAS) method to discover a better transformer architecture specifically for image recognition tasks. Our approach expands the search space by introducing a locality module to the self-attention module in the transformer, allowing for the modeling of local correlations. We also propose a Hierarchical NAS strategy to efficiently search the expanded search space. Experimental results demonstrate that our searched Global Local image Transformer (GLiT) outperforms state-of-the-art transformer backbones on the ImageNet dataset. Our contributions include being the first to explore NAS for transformer architecture in image classification and introducing locality modules to decrease computation costs and model local correlations explicitly. Additionally, we propose a Hierarchical NAS strategy to handle the large search space and improve search performance.