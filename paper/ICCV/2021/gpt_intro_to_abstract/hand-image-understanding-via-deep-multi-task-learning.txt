Hand image understanding (HIU) is an area of focus in computer vision and graphics, with the goal of recovering spatial configurations from RGB/depth images of hands. This includes tasks such as hand pose estimation, hand mask segmentation, and hand mesh reconstruction. While many existing methods for HIU require depth cameras or synchronized multi-view images, this paper addresses the challenge of recovering spatial configurations using only monocular RGB images. The main obstacles in this scenario include the lack of high-quality datasets with precise annotations, the limited generalizability of trained models to diverse wild images, and the failure to exploit unlabeled images. To tackle these difficulties, the paper proposes an innovative multi-task learning (MTL) framework for HIU. The framework consists of a backbone and several regressor heads corresponding to different tasks. The backbone learns elementary representations from hand images, while the regressor heads are used to regress parameters of a generative hand model called MANO. The paper also introduces a task attention module (TAM) to aggregate semantic features across tasks and remove redundancy. The framework is trained using existing multi-modality datasets through supervised learning, and self-supervised learning strategies are employed to leverage the implicit relationship constraints among the predictions of different tasks. Additionally, a high-quality dataset with manually labeled 2D hand pose and hand mask is collected to address the lack of large-scale labeled hand datasets. The contributions of this work include the design of the HIU-DMTL framework, the introduction of self-supervised learning for HIU, the proposal of the task attention module, and the demonstration of the framework's superiority over contemporary approaches in hand mesh recovery. The framework achieves state-of-the-art performances on various benchmarks, indicating its efficacy in hand image understanding.