Generating realistic synthetic handwritten text images is a challenging problem in computer science. This paper investigates the problem of generating unconstrained handwritten text sequences with diverse calligraphic attributes representing different writing styles. The existing GAN-based methods for styled handwritten text image generation have limitations in style-content entanglement at the character-level and capturing local style patterns. To address these issues, the paper proposes a new approach called Handwriting Transformers (HWT) that utilizes a multi-headed self-attention mechanism in an encoder-decoder network. The HWT captures both global and local writing styles and achieves style consistency through a loss term. Extensive evaluations show that HWT outperforms existing methods in terms of human plausibility and generalization capabilities. The proposed approach generates realistic styled handwritten text images.