This paper focuses on the problem of deploying deep convolution neural networks (DCNN) algorithms to mobile/embedded edge devices with limited computing resources. The authors propose a solution called FATNN, which combines a ternary implementation of computation with a quantization algorithm to reduce the model complexity. The proposed FATNN reduces the computational complexity of TNNs by 2Ã— and works efficiently on various devices. The authors also design a ternary quantization algorithm that optimizes the step size of each quantization level and improves network performance. The main contributions of this paper include the ternary quantization pipeline, a fast ternary inner product implementation, and a highly accurate ternary quantization algorithm. The authors evaluate the execution speed of FATNN and demonstrate its superior performance compared to state-of-the-art approaches in image classification tasks.