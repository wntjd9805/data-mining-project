In recent years, convolution neural networks (CNNs) have achieved great success in fully supervised image semantic segmentation, but they require a large amount of pixel-level annotations. In order to reduce the human labeling efforts, few-shot image segmentation has been introduced, which aims to predict the segmentation mask of a novel class with only a few labeled support images. However, current state-of-the-art methods treat the few-shot segmentation task as a binary foreground-background segmentation problem, without considering class information. In this paper, we propose a novel Meta-class Memory Module (MMM) that learns middle-level meta-class embeddings shareable between base and novel classes for few-shot segmentation. We introduce a set of meta-class memory embeddings into the pipeline, which are learned during the base class training. These embeddings are used to generate meta-class activation maps, aligning query and support middle-level features. We also propose a Quality Measurement Module (QMM) to measure the quality of support images, enabling effective fusion of support features. Experimental results on PASCAL-5i and COCO datasets demonstrate that our method achieves state-of-the-art performance, outperforming previous methods and achieving significant improvements in mean mIoU. Our contributions include the introduction of learnable embeddings for meta-class information, the development of the Meta-class Memory Module for query mask prediction, and the Quality Measurement Module for better fusion of support features.