In this paper, we propose a method called Preservational Contrastive Representation Learning (PCRL) for training deep neural networks in medical image analysis without the need for manual annotations. We highlight the challenges of obtaining reliable medical annotations and the limited availability of labeled medical data. To overcome these challenges, we incorporate self-supervised learning techniques, specifically contrastive learning, to learn medical image representations. However, we argue that solely relying on the contrastive loss is not sufficient for preserving all relevant information. Therefore, we introduce a novel approach that combines contrastive learning with explicit image reconstruction. Our method, shown in Figure 1, incorporates Transformation-conditioned Attention and Cross-model Mixup to enhance the information captured by the learned representations. We demonstrate the effectiveness of our approach through extensive experiments, comparing it to other self-supervised learning methodologies. Our results show that PCRL outperforms both self-supervised and supervised counterparts in various classification and segmentation tasks. Additionally, we discuss related work in the field, including methods for pretext-based approaches, contrastive learning, and mixup strategies. Overall, our contributions involve the introduction of PCRL, the proposal of Transformation-conditioned Attention and Cross-model Mixup modules, and the extensive experimental evaluation of our method.