Semantic segmentation is an important task in computer vision, with many advanced methods using deep learning achieving high accuracies in various applications. However, the need for large amounts of accurately annotated data limits their practical usage in many domains. Unsupervised domain adaptation (UDA) has been explored as a solution to reduce the annotation workload, but often results in inferior performance due to the distortion of the target-domain distribution. Active learning (AL) has shown promise in boosting performance with minimal annotation, but previous studies have not considered the potential multimodal distribution of the source domain. To address these issues, we propose a novel framework called Multi-anchor Active Domain Adaptation (MADA) that combines active learning and domain adaptation for semantic segmentation. MADA consists of two stages: a multi-anchor based active sample selection strategy to identify representative samples for manual annotation, and a semi-supervised learning stage where the network is fine-tuned using annotations from source and selected target samples. We demonstrate the effectiveness of MADA through extensive experiments and ablation studies, showing significant improvements in segmentation performance. Our work is the first to adopt active learning for domain adaptation in semantic segmentation tasks, effectively preventing target-domain distribution distortion and achieving superior performance with minimal manual annotation.