Attention mechanisms and Transformers have recently made a significant impact in various fields of artificial intelligence, including natural language processing, document analysis, speech recognition, and computer vision. While convolutional neural networks (CNNs) have been the primary approach for image classification, recent developments have shown promising results for Transformer-based image classifiers. Both convolution and attention operations address the representation problem for structured data by modeling local contents and contexts. However, attention operations differ from convolutions in terms of receptive fields and weight computations. Self-attention provides more adaptive and general modeling means, and position encodings enhance spatial configuration modeling. While self-attention is more computationally demanding, it offers enriched modeling capabilities. Transformers have been successfully applied to object detection and segmentation tasks, but image classifiers based solely on Transformers still have performance and design gaps compared to CNN models. The paper introduces Co-scale conv-attentional image Transformers (CoaT), which utilize separate encoder branches to engage attention across scales and implement a conv-attention module for relative position embeddings. CoaT achieves state-of-the-art classification results on the ImageNet benchmark, outperforming both CNNs and other Transformer-based image classifiers.