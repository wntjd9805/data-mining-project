3D hand pose and shape reconstruction is important in various applications, such as AR/VR and robotics. While previous works have focused on reconstructing single hand poses and shapes, we address the problem of reconstructing interacting hand poses and shapes from a single color image. Existing methods rely on expensive depth sensors, multi-view camera systems, or optimization over tracked motion sequences, which are not cost-effective or widely available. In this paper, we propose a novel deep learning architecture for interacting hand pose and shape estimation. Our network consists of an encoder that extracts multi-scale features and a decoder that refines predictions using context between interacting hands. We also propose pose-aware attention modules to handle feature ambiguity between hands and leverage the two-hand context to improve accuracy. Our method achieves state-of-the-art performance on main datasets and can inspire further research in multiple person and hand-object interaction reconstruction.