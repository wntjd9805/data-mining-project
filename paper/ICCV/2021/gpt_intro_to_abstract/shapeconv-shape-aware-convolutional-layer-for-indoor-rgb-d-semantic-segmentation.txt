The widespread use of depth sensors, such as Microsoft Kinect, has led to the availability of RGB-D data, which has boosted the advancement of RGB-D semantic segmentation in computer vision. Convolutional Neural Networks (CNNs) have become the go-to method for tackling this problem. However, RGB and depth information have inherent differences, with depth features capturing shape and base information while RGB values focus on appearance properties. This discrepancy poses challenges for applying vanilla convolution operators to both types of data. To address these challenges, we propose a Shape-aware Convolutional layer (ShapeConv) that can learn the adaptive balance between shape and base information. We decompose input patches into base-components and shape-components, and process them separately using base- and shape-kernels. The outputs are then combined to form a shape-aware patch, which is further convolved with a normal convolution kernel. The proposed ShapeConv can be easily integrated into existing CNN models for RGB-D semantic segmentation without increasing computation and memory requirements in the inference phase. Experiments on three benchmark datasets demonstrate that ShapeConv improves segmentation accuracy, particularly around object boundaries, by effectively leveraging depth information.