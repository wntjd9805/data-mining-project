Abstract:The rapid increase in the number of images being taken and shared every day has highlighted the need for efficient image retrieval systems. In order to expedite the search process, these systems use content analysis to build an index that represents the collection of images. A query is a description of the desired image, which can be a list of specific object classes or a natural language description of its content. However, the current query paradigms fall short in expressing the spatial location of the desired objects in the image. In this paper, we propose a new query modality where the user simultaneously describes the characteristics of the desired image using spoken natural language and mouse traces over a blank canvas. This approach utilizes the natural human tendency to point at objects and provides a more precise grounding signal for retrieval. We present an image retrieval model that takes this multimodal query as input, utilizing a repurposed image-to-text matching model. Experimental evaluation using the Localized Narratives dataset demonstrates a significant improvement in retrieval accuracy when incorporating the spatial information provided by the mouse traces. Our contributions include a novel query modality, a transformer-based implementation, an experimental setup, and empirical results demonstrating the effectiveness of our approach.