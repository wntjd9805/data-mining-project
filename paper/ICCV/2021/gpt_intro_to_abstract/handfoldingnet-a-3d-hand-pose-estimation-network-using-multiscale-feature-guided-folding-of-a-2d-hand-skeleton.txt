3D hand pose estimation is a crucial task in various human-computer interaction applications, especially in virtual reality and augmented reality. Despite recent progress using deep neural networks (DNNs) and hand depth images, accurate and real-time estimation remains challenging due to issues such as self-occlusion, noise, high dimensionality, and various orientations of the hand. 2D convolution neural networks (CNNs) have been commonly used for hand pose estimation, but they cannot fully utilize the 3D spatial information necessary for high accuracy. To address this, some researchers have proposed discretizing hand depth images into a 3D voxelized representation and using 3D CNNs, but this approach suffers from high memory consumption in higher image resolutions. Alternatively, the point cloud representation has been considered as an efficient and precise method for 3D hand pose estimation, but conventional DNNs cannot directly process point clouds due to the irregular order of points. However, the emergence of PointNet has enabled the direct processing of point clouds by utilizing a point-wise shared-weights multi-layer perceptron (MLP) and max-pooling layer, leading to the development of PointNet-based hand pose estimation models. These models can be categorized into regression-based and detection-based methods. Regression-based methods encode hand shape into a global feature and perform non-linear regression to estimate joint coordinates, while detection-based methods use hierarchical features to compute heat-map features for each point. However, both methods have their limitations, such as insufficient complexity mapping or increased computational cost. In this paper, we propose HandFoldingNet, an accurate and efficient 3D hand pose estimation network that folds a 2D hand skeleton into a 3D pose guided by multi-scale features extracted from both global and local information. The network includes a global-feature guided folding decoder and a joint-wise local-feature guided folding block to handle different scales of features and improve estimation performance. We evaluate our network on challenging datasets and demonstrate its superiority in terms of accuracy and efficiency compared to state-of-the-art methods. The proposed network achieves mean distance errors of 5.95mm, 7.34mm, and 8.58mm on different datasets while running in real-time on a single GPU. Our contributions include the introduction of HandFoldingNet, the global-feature guided folding decoder, the joint-wise local-feature guided folding block, and extensive experiments to analyze the network's efficiency and accuracy.