Deep learning methods have achieved impressive performance in various computer vision tasks. However, their performance significantly declines when facing test data that differs from the training data, which is known as the domain shift problem. Domain generalization (DG) research aims to tackle this issue by developing models with good DG properties. Data augmentation, a popular DG strategy, has been widely used to improve generalization to novel domains. Existing augmentation-based approaches mainly focus on image-space augmentations, which can be computationally expensive and complex. In contrast, feature-level data augmentation has shown effectiveness in supervised learning contexts. Inspired by this, we propose a feature augmentation method for DG. Our approach perturbs latent features using white Gaussian noise, which is simple to implement and fast to run. We demonstrate that this simple feature augmentation strategy achieves comparable performance to recent state-of-the-art approaches that rely on more complex techniques. However, adding too much noise can risk inducing non-class-preserving augmentations and introducing label-noise. To address this limitation, we estimate the natural directions of correlation in the source dataset and simulate a joint noise distribution based on this estimation. We further estimate class-conditional covariance to ensure that the learned noise follows class-preserving but inter-domain directions. Our method can be applied to any base DG method and outperforms the vanilla method on benchmarks such as Digit-DG, VLCS, and PACS. We also analyze the feature evolution from the vanilla model to our stochastic feature augmentation (SFA) trained features to gain insights into how our method improves model generalization.