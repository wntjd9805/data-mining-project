Generative Adversarial Networks (GAN) have made significant advancements in generating high-quality images that are visually indistinguishable to humans. However, the disentanglement of attributes in these generated images remains an open problem. Specifically, the separation of content and style is crucial in image generation tasks such as faces. Content refers to spatial information like face direction and expression, while style involves features like color, makeup, and gender. Existing methods, such as StyleGAN, attempt to disentangle style and content using AdaIN codes and per-pixel noises. However, the control of global content and style remains incomplete. Recent approaches, like structured noise injection (SNI), have shown good performance in separating global style and content information by using AdaIN and independent content latent codes. However, SNI has limitations with input tensor size and fails to work properly when the content control exceeds capacity. To address these issues, this paper proposes a new Diagonal spatial ATtention (DAT) module that manipulates content features in a hierarchical manner. The content code is applied as diagonal attention maps to multiple layer features at various resolutions. DAT allows for independent modulation of image content and style in a symmetric manner and enables hierarchical control of spatial content. This results in effective disentanglement of content and style components in generated images. Additionally, the proposed method can easily integrate with state-of-the-art GAN inversion, facilitating flexible post-hoc control of content and style in translated images from multi-domain image translation.