Finding correspondences across pairs of images is a fundamental task in computer vision, with various applications such as camera calibration, optical flow, Structure from Motion (SfM), visual localization, point tracking, and human pose estimation. Traditionally, there are two research directions for this problem: sparse and dense methods. Sparse methods extract sets of sparse keypoints and aim to minimize an alignment metric, while dense methods interpret correspondence as a process where every pixel in one image maps to a pixel in the other image. However, there is a divide between these methods based on the applications they were developed for. In this paper, we propose a novel network architecture, called the Correspondence Transformer (COTR), which bridges this divide. COTR leverages the inductive bias of densely connected networks to represent smooth functions and utilizes the attention mechanism of transformers to control and learn both global and local priors. We formulate the correspondence problem as a functional mapping and show that COTR can match arbitrary query points while learning smoothness and effectively handling large camera motion. Our main contributions include the proposal of the COTR architecture, recursive application at multiple scales for accurate correspondences, achieving state-of-the-art performance on multiple datasets and tasks, and substantiating the importance of the transformer component through ablation studies. This is the first work to apply transformers for obtaining accurate correspondences in computer vision.