Fake news and misinformation have become pervasive issues, especially with the rise of digital image manipulation and sharing. To address this problem, it is crucial to trace the origins of an image and attribute it to a trusted source, as it can help users make informed trust decisions. Existing standards for image attribution embed provenance information within metadata, but social platforms often strip this metadata, leading to misattribution of images. In this paper, we propose a novel object-centric approach for computing a robust hash from an image to perform visual matching for image attribution. Traditional cryptographic hashes are unsuitable as fingerprints due to "benign transformations" that images undergo during online distribution, such as changes in format, resolution, size, and padding. Therefore, we develop a representation learning technique that achieves both invariance to benign transformations and sensitivity to tampering, where image content is altered to change its meaning. Our method uses a hybrid network architecture, combining a fully-connected Graph Neural Network (GNN) and Transformer, to encode a scene graph representation of the image and generate a compact visual hash. By decomposing the scene into salient objects and considering their description, our object-centric hash exhibits improved sensitivity to minor manipulation of objects and robustness to benign transformations. We train our model using contrastive training, data augmentation, and manipulations using Adobe PhotoshopTM, surpassing existing perceptual hashing techniques. The proposed OSCAR-Net representation, combined with geometric verification, forms a prototype image attribution tool that can help users determine the provenance of images encountered online. Such a tool holds great potential for fighting fake news by assisting users in assessing the trustworthiness of content.