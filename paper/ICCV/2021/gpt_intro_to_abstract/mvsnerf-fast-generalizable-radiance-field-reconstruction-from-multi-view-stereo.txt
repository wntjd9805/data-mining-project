Abstract:Neural rendering approaches have made significant progress in novel view synthesis, but their practicality is limited by the lengthy per-scene optimization process required to obtain high-quality radiance fields. In this paper, we propose MVSNeRF, a novel approach that enables efficient radiance field estimation by generalizing across scenes. Our approach uses deep multi-view stereo (MVS) techniques to build a cost volume that incorporates both scene geometry and appearance. We then reconstruct a neural scene encoding volume using a 3D CNN, which can be used for final rendering by ray marching. Our approach combines the benefits of MVS and neural rendering, enabling differentiable training without 3D supervision and allowing for inference time optimization. Compared to existing methods, our MVS-like architecture facilitates cross-view correspondence reasoning, leading to better neural scene reconstruction and rendering. Experimental results demonstrate that our approach can synthesize photo-realistic images with only three input images, and can even perform well on datasets with different scene distributions. Additionally, our estimated radiance field can be easily optimized on novel scenes for further improvements. Overall, our approach contributes to the practicality of realistic neural rendering and is released as open-source code.