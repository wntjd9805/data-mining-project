Learning compact and general representations in computer vision is a significant goal in the field of computer science. Recent advancements in learning representations from labeled data and self-supervised representation learning methods have shown promising results. Contrastive self-supervised learning methods, in particular, have achieved impressive performance by contrasting latent representations of different image transformations or cluster assignments. However, the common protocol of pre-training models on large datasets like ImageNet without class labels may lead to biased and optimistic estimates of performance. In this paper, we aim to comprehensively study contrastive self-supervised methods by exploring different algorithms, pre-training datasets, and end tasks. Our goal is to answer open questions regarding the choice of encoder, metric for measuring progress, comparison of training algorithms, suitability of self-supervision for different tasks, impact of pre-training data distribution, and the influence of dataset balance on representation learning. Through extensive experiments, we provide insights into the characteristics and performance of contrastive self-supervised models, presenting a mixture of intuitive and unintuitive results.