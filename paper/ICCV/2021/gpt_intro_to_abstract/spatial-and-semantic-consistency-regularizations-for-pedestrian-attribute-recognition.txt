Pedestrian attribute recognition aims to predict various human attributes for pedestrian images. This has applications in surveillance, scene understanding, and human perception. Several methods have been proposed to enrich attribute representation and locate attribute-related regions. However, these methods focus on learning features from individual images rather than exploring inter-image relations between images of the same attribute. In this paper, we propose a framework that mines spatial and semantic relations between pedestrian images of the same attribute. We introduce the concepts of spatial consistency and semantic consistency, and design modules to ensure precise spatial attention and extract discriminative semantic features. Our method achieves state-of-the-art performance on multiple datasets, demonstrating the effectiveness of the proposed approach.