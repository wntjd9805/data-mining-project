Semantic segmentation is a fundamental task in computer vision that involves assigning a semantic class to each pixel in an image. While deep learning has greatly improved the field, obtaining large-scale datasets with pixel-level annotations is expensive and remains a bottleneck. Label propagation (LP) and self-training (ST) have been proposed as methods to address data scarcity. However, LP methods often struggle with estimating dense motion fields and have limitations in handling de-occlusion and accurate optical flow. ST methods face challenges in controlling noise in pseudo-labels and misalignment of category definitions. To overcome these drawbacks, this paper proposes a novel method called Warp-Refine Propagation that combines motion cues with semantic cues and leverages cycle-consistency to learn in a semi-supervised setting. The method generates dense pixel-level labels for raw video frames by first combining motion and semantic cues, then rectifying the initial estimate through a refinement network trained with a cycle-consistency loss. The method achieves superior accuracy in propagating labels over longer time intervals and accurately labels rare classes and thin structures. Quantitative analysis on multiple datasets demonstrates the effectiveness of the proposed method compared to previous state-of-the-art auto-labeling methods. The contributions of this work include the novel Warp-Refine Propagation algorithm, the cycle-consistency loss function for semi-supervised training, and the quantitative analysis of different auto-labeling methods on diverse datasets. This work is the first to combine semantic and geometric understanding for video auto-labeling.