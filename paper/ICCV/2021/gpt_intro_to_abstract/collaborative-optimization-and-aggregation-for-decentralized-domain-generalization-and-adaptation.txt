Deep neural networks have shown promising performance for visual recognition tasks. However, models trained on specific source domains often suffer from performance degradation when applied to new target domains due to data bias or domain shift. To address this problem, domain generalization (DG) and unsupervised domain adaptation (UDA) have been studied, with recent focus on multi-source DG and UDA. In this work, we investigate decentralized DG and UDA, where data from different domains cannot be shared for joint training. We propose a new approach called Collaborative Optimization and Aggregation (COPA) that optimizes a generalized target model through decentralized learning with non-shared data. For decentralized DG, local models are optimized in each source domain and centrally aggregated as a global model. For decentralized UDA, additional unlabeled data from the target domain is used for model aggregation and fine-tuning. We conduct experiments on five benchmark datasets and demonstrate that COPA achieves comparable performance to state-of-the-art methods.