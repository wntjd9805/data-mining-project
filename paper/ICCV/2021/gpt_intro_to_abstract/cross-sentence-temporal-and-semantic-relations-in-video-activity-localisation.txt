Video activity localisation by natural language is a challenging task in computer science. It aims to find the best video segment that corresponds to a query sentence in an untrimmed video. Existing methods for this task are typically fully supervised, meaning they require precise annotations of the start and end times of each video moment. However, this approach is costly and biased. As an alternative, weakly-supervised learning methods have been proposed, but they often struggle to accurately localise multiple video moments due to the lack of cross-sentence relations. In this paper, we introduce a weakly-supervised method called Cross-sentence Relations Mining (CRM) that leverages the relations between sentences in a paragraph to improve the temporal and semantic alignment of video moments. CRM differs from existing models by considering the temporal order of events in videos based on the order of sentences in the corresponding text descriptions. We also introduce a semantic consistency constraint to further enhance the alignment. Our experimental results show that CRM outperforms existing methods on two benchmark datasets for video activity localisation. Overall, this work presents a novel approach for video activity localisation that effectively utilizes cross-sentence relations to improve performance in a weakly-supervised setting.