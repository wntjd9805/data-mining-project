Scene content generation, specifically generating 3D object shapes, images, and scenes, is a significant area of research in computer vision. This research has numerous applications, including assisting designers by automatically generating intermediate results and understanding and modeling scenes in terms of object constellations and co-occurrences. Conditional synthesis allows for more controllable content generation, where users can specify which image or 3D model they want to appear in the generated scene. Common conditions include text descriptions, semantic maps, and scene graphs. Scene graphs, in particular, have shown to be a suitable interface for controllable synthesis and manipulation, enabling semantic control over complex scenes. While there are methods for scene graph inference from images and the reverse problem, few works have focused on scene graph prediction from 3D data. In this paper, we propose a method for end-to-end generation of 3D scenes from scene graphs. Unlike previous methods that rely on retrieval from a database to construct a 3D scene, we employ a fully generative model that can synthesize novel context-aware 3D shapes. To enable changes in the scene, such as adding new objects or changing object relationships, we employ a graph manipulation network. We also introduce a novel relationship discriminator on 3D bounding boxes to address the one-to-many problem of label to object inference. Additionally, to ensure unbiased learning, we learn 3D scene prediction from real data, addressing limitations such as information holes and lack of annotations. We evaluate our method on a large-scale real 3D dataset and demonstrate its effectiveness in terms of quality, diversity, and fulfillment of relational constraints. Overall, this work contributes a fully learned method for generating 3D scenes from scene graphs, as well as a model for scene manipulation and a relationship discriminator loss.