Motion deblurring is a challenging task in computer vision due to motion ambiguities and the loss of intensity textures. An event camera, which emits events asynchronously with low latency, can help address this problem as it inherently captures motions and textures. However, existing event-based motion deblurring methods that learn from synthesized datasets suffer from inconsistency between synthetic and real data, leading to degraded performance on real-world event cameras. Moreover, the physical intrinsic noise of event cameras makes it difficult to simulate labeled events that match real event data accurately. To overcome these challenges, we propose a self-supervised framework for learning an event-based motion deblurring network using real-world events and motion blurred images. Our framework includes two neural networks: a motion deblurring network and an optical flow estimation network. The motion deblurring network takes events and a single motion blurred image as input and generates a sequence of sharp clear images. The optical flow estimation network receives events and computes accurate motion flows between the reconstructed sharp images. We consider the non-linearities of real-world motion blurs using a piece-wise linear motion model, which improves the accuracy of optical flow estimation. The proposed framework is trained using a partially labeled dataset composed of synthesized data with ground-truth sharp clear images and real-world data containing real events and motion blurred images. Experimental results demonstrate that our method produces high-quality sharp frames and achieves state-of-the-art results on real blurry event datasets.