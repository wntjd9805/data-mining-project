Instance segmentation is a fundamental task in computer vision, but traditional methods require a large amount of training data with pixel-wise annotations, which limits their applicability in various high-level vision tasks. Weakly supervised instance segmentation (WSIS) has been explored as an alternative, but it is a challenging task with limited previous attempts. Existing bottom-up WSIS methods use classification networks to identify object instances, but they suffer from coarse localization cues and struggle with small instances. Top-down methods, on the other hand, rely on weakly supervised object detection (WSOD) to generate pseudo-ground-truth masks, but the quality of these masks depends heavily on the WSOD. In this paper, we propose a unified parallel detection-and-segmentation learning (PDSL) framework for WSIS using only image-level labels. The proposed framework combines the strengths of top-down and bottom-up approaches, decouples the generation of pseudo-ground-truth masks from detectors, imposes bounding-box constraints on segmentation learning, and explicitly models correlations between detection and segmentation. The PDSL framework consists of three key components: object detection, image segmentation, and correlation learning modules. Experimental results on PASCAL VOC and MS COCO datasets demonstrate that the proposed PDSL outperforms baseline models and achieves state-of-the-art results in WSIS. The contributions of this work are three-fold: 1) introducing a cooperative parallel detection-and-segmentation learning framework for WSIS with only image-level labels, 2) progressively modeling class-agnostic to class-specific object segmentation using self-supervised learning and self-training, and 3) enhancing the coherence between detection and segmentation branches through instance-activation correlation learning.