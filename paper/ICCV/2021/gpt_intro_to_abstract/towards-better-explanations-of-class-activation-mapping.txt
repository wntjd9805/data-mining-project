Recently, convolutional neural networks (CNNs) have achieved great success in various real-world vision tasks. However, the lack of understanding of their internal behavior makes it difficult to explain their predictions. To address this issue, various saliency methods have been proposed to generate visual explanation maps that show the importance of different regions in an input image for the model's decision. CAM-based methods have been widely used in this regard, but they often rely on heuristic assumptions and lack a clear theoretical basis for determining the coefficients in the linear combination of activation maps. In this work, we leverage the linearity of CAM to analytically determine the coefficients by formulating the explanation model as a linear function of binary variables representing the presence of activation maps. We propose a novel saliency method, LIFT-CAM, based on DeepLIFT, that efficiently approximates the SHAP values of the activation maps. Our contributions include a new framework for determining visual explanation maps, the formulation of SHAP values as a unified solution, and the introduction of LIFT-CAM, which outperforms previous CAMs in terms of both qualitative and quantitative evaluation.