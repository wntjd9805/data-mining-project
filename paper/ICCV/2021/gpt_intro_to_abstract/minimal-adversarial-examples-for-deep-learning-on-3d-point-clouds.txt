Deep learning has shown promise in solving computer vision tasks, but it has been found that deep neural networks are susceptible to adversarial examples that can deceive the networks into making incorrect predictions. While various adversarial attack techniques have been proposed to generate such examples, countermeasures in the form of adversarial defense methods have also been developed. Understanding how attacks and defenses operate is crucial for enhancing the reliability and robustness of deep learning techniques.While existing research has explored the vulnerability of deep networks on 3D data, particularly 3D point clouds, these studies have largely focused on common scenarios such as perturbing points to generate adversarial point clouds. However, these approaches have flaws since the perturbations are easily detectable and do not involve manipulating the entire point cloud.This paper addresses the problem of generating minimal adversarial point clouds that maintain the perceptibility of the original point cloud while perturbing the minimum number of points. The proposed solution is a new formulation for adversarial point cloud generation that can be adapted to different attack strategies. This work is novel in that it explores the unexplored problem of minimal 3D point cloud attacks and proposes a formulation that considers both perceptibility and optimality of adversarial samples. It also introduces a unified framework that generalizes both point perturbation and point addition.The contributions of this paper include the development of a technique to generate minimal 3D adversarial point clouds, a unified formulation that generalizes two adversarial point cloud generation strategies, an analysis of the vulnerability of perturbed points in relation to the concept of critical points in PointNet, and a benchmark of adversarial attacks on synthetic and real-world 3D point clouds, demonstrating consistent performance across both domains.