Image-based geo-localization has garnered significant attention in the computer vision community. This involves matching a query image with geo-tagged reference images to assign the GPS location of the best matching reference image to the query image. While existing works have focused on solving same-view and cross-view image matching problems, there is a growing need to geo-localize video clips by determining GPS trajectories. In this paper, we specifically explore the task of video geo-localization for same-view data, where both query videos and gallery images are from ground views. Previous methods have treated each frame in the video independently, resulting in predicted GPS locations that may not accurately represent the camera's trajectory due to the lack of temporal closeness between frames. To address this, we propose leveraging the geo-temporal proximity between video frames by incorporating transformer-based architectures to learn feature representations. The proposed approach captures coherent features and provides smoother predicted trajectories. Additionally, we introduce a novel GPS loss to learn smoother features for video clips in geographically similar locations. We also propose a trajectory smoothing network based on transformer encoders to identify and smooth out noisy GPS values. To evaluate our framework, we construct a new video geo-localization benchmark dataset using Berkeley Driving Dataset (BDD) videos and matching Google StreetView (GSV) images from different regions of the USA. Our contributions include the development of a geo-temporal feature learning approach, a GPS loss formulation, a trajectory smoothing network, and a new dataset for video geo-localization with comprehensive evaluations.