Image inpainting is a task in computer vision that involves predicting missing regions in images. This problem is important in various applications such as image restoration, compositing, manipulation, re-targeting, and image-based rendering. Traditional methods rely on borrowing example patches from known regions or external datasets and pasting them into the missing regions. However, these methods struggle with cases involving complex, non-repetitive structures. Recent research has focused on data-driven deep convolutional neural network (CNN) approaches for inpainting, which have shown promising results for complex scenes. One of the main challenges in inpainting is the ambiguity in filling missing regions, which often leads to blurry or distorted structures. To address this issue, recent methods have attempted to reduce uncertainty by assigning a known region as a reference for filling a missing region. This is done through a patch-borrowing operation, such as the contextual attention (CA) module. However, current methods lack direct supervision on feature similarity and patch correspondences, resulting in artifacts in the inpainting results. Additionally, the patch-borrowing operation in the CA layer is computationally expensive for high-resolution images. In this paper, we propose a new approach that avoids explicit patch borrowing, making the inpainting generator more efficient and robust to borrowing incorrect reference patches. We achieve this by introducing an auxiliary contextual reconstruction loss, which encourages the generator to produce plausible outputs even when reconstructed by surrounding regions/features. Through extensive experiments, we demonstrate the effectiveness of our approach and its favorable performance compared to state-of-the-art methods. Our method not only produces realistic inpainting results but also reduces the computational overhead during inference, making it more efficient for testing. Overall, our contributions include the introduction of a new learnable auxiliary contextual reconstruction branch/loss, an attention-free inpainting generator trained with traditional and auxiliary losses, and extensive experimentation to validate the effectiveness of our approach.