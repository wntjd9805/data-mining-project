Human pose estimation has become increasingly important in various applications such as action recognition, human-computer interaction, augmented reality/virtual reality, and robotics. While there has been growing interest in pose estimation from videos due to the faithful capture of human dynamics, analyzing thousands of hours of video using deep models can be computationally expensive. This paper proposes a Motion Adaptive Pose Net that leverages motion vectors and motion-compensation residuals from compressed video streams to improve the efficiency of video-based pose estimation models. By exploiting the readily available motion and residual information in compressed streams, the proposed method achieves state-of-the-art performance with significantly reduced computation. Previous approaches to temporal dynamics modeling have focused on frame-by-frame analysis without considering the natural coherence between neighboring frames. In contrast, video compression techniques rely on temporal coherence to reduce video size, storing only sparse motion vectors and residual errors. The proposed method utilizes the compressed streams to dynamically switch between computationally light motion-warped features and accurate features from decoded frames based on the residual errors. This approach offers substantial computation savings without sacrificing accuracy. The proposed method is validated on the Penn Action and Sub-JHMDB datasets, demonstrating superior efficiency and accuracy compared to existing methods. In summary, this paper introduces a novel approach that utilizes motion signals and residual errors in compressed videos for efficient pose estimation, providing reliable features and reducing computation costs compared to state-of-the-art models.