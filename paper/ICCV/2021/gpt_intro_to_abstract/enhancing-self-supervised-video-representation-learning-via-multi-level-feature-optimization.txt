Video representation learning is a crucial problem in computer vision, with applications in tasks such as action recognition, video retrieval, and video captioning. While there are human-annotated datasets available for these tasks, manual labeling is expensive and fully-supervised methods are unable to effectively utilize unlabeled video data. Therefore, it is important to develop unsupervised video representation learning methods. Previous works have explored pretext tasks and contrastive learning for image representation, but these approaches have limitations when applied to video representation learning. This paper proposes a novel framework that optimizes features from multiple levels of deep neural networks to achieve more general representations. High-level features focus on instance discrimination and semantic structure modeling, while low-level features are optimized for cross-task transferability. The framework also includes a multi-level temporal modeling module to enhance the temporal analysis of the pretrained representation. Experimental results demonstrate that the proposed method outperforms existing approaches in action recognition and video retrieval tasks on UCF-101 and HMDB-51 datasets.