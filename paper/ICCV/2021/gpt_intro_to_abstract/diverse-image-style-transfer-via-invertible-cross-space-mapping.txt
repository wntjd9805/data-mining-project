This paper introduces the problem of style transfer in computer science, where the goal is to repaint an existing photograph with the style of an artist. The key challenge in style transfer is representing the content and style of an image. Previous approaches have used pre-trained deep convolutional neural networks (DCNNs) to extract content and style features from images, allowing for the creation of novel artworks by combining these features. However, these approaches have focused primarily on efficiency, quality, and generalization, with less attention given to diversity in stylization results. This paper addresses the issue of diversity by proposing a Diverse Image Style Transfer (DIST) framework that enforces an invertible cross-space mapping. The framework takes random noise vectors and everyday photographs as inputs, using the noise vectors for style variations and the photographs for content. The framework consists of three branches: disentanglement, inverse, and stylization. The disentanglement branch separates artworks into content and style space, while the inverse branch maps the style information of the generated images back to the input noise vectors. The stylization branch then applies the style of an artist to the input content image. The proposed approach can generate significantly diverse stylized images without sacrificing quality. This paper contributes a new style transfer framework, a method for disentangling style and content, and demonstrates the effectiveness and superiority of the approach through extensive comparisons with state-of-the-art methods.