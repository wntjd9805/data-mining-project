Semantic image segmentation is a fundamental problem in computer vision with significant real-world applications. Recent developments in deep neural networks have led to improvements in semantic segmentation using the encoder-decoder structure of Fully Convolutional Networks (FCNs). However, there are still challenges in improving the encoder structure and modeling context to enhance segmentation performance. This paper focuses on aggregating contextual information from both image-level and semantic-level perspectives. Existing approaches have primarily focused on either multi-scale context modeling or relational context modeling. While these approaches have shown impressive results, they neglect the importance of pixel representations in the same category and can mislabel pixels due to unevenly capturing contextual information. To address this problem, this paper proposes a novel method to augment pixel representations by aggregating both image-level and semantic-level contextual information. The paper introduces an image-level context module (ILCM) and a semantic-level context module (SLCM), which capture contextual information from the whole image and category region, respectively. Through weighted aggregation based on calculated similarities, the proposed method effectively improves pixel representations. Experimental results demonstrate the effectiveness of the proposed approach, which achieves state-of-the-art accuracy on segmentation benchmarks. The paper also presents a general architecture framework called ISNet for consistently boosting semantic image segmentation performance using ILCM and SLCM.