Progress in computer vision has been driven by the use of deep neural networks trained on large labeled datasets. However, the cost and effort required to capture and annotate such datasets often hinders the deployment of these solutions in real-world applications. To address this challenge, researchers have started to rely on synthetic images rendered from 3D databases for training. While physics-based rendering methods have narrowed the gap between real and synthetic color images, the same progress has not been made in depth-based applications. Little effort has been put into generating realistic synthetic depth scans or understanding their impact on the training of depth-based recognition methods. Existing simulation pipelines and domain adaptation schemes suffer from limitations such as extensive domain knowledge requirements and inability to generalize to new sensors or scenes. In this paper, we propose a novel pipeline that virtually replicates depth sensors and can be optimized for new use-cases. Our solution, called Differentiable Depth Sensor Simulation (DDS), leverages differentiable ray-tracing techniques and soft stereo-matching to generate highly realistic depth scans. DDS can be optimized through gradient descent and outperforms other simulation tools and domain adaptation methods. Our contributions include the introduction of DDS, an end-to-end differentiable simulation pipeline for depth sensors, the ability to optimize the simulation through gradient descent, and the benefits of DDS to deep-learning recognition methods. Experimental results show that methods trained with DDS perform significantly better when tested on real data compared to previous simulation tools and domain adaptation algorithms.