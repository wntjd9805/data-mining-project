Developing animatable clothed human avatars is crucial for various applications in 3D content generation. Traditional methods for creating avatars with realistically deforming clothing involve time-consuming artist work or expensive 4D scans. This paper aims to investigate the possibility of transforming a single static 3D scan into an animatable avatar at a low cost. While automatic rigging-and-skinning methods can animate a static scan, they fail to produce pose-dependent clothing deformations. Physics-based simulations can achieve realistic deformations, but they require manual "reverse-engineering" of a simulation-ready clothing mesh. This paper proposes a data-driven approach to learn a model capable of producing reasonable pose-dependent clothing deformation across different outfit types and styles, with the ability to generalize to unseen outfits. The main technical challenge lies in the choice of 3D shape representation. Classical triangle meshes are efficient for rendering but have fixed topology limitations, while explicit point clouds offer topology flexibility and capture geometric details. This work demonstrates the power of point clouds as a representation for modeling clothed humans. It introduces a new shape representation of dense point clouds that achieves dense up-sampling during inference, eliminating the need for patches. Additionally, the paper addresses the challenge of encoding outfits of different types and styles in a unified model. By separating the intrinsic shape of a clothed person from the pose-dependent shape, the proposed model can focus on modeling pose-dependent effects and leverage common deformation properties across outfits. The model, known as POP, is evaluated on both captured and synthetic datasets and demonstrates state-of-the-art performance in clothing modeling and generalization to unseen outfits. Contributions of this paper include the novel dense point cloud shape representation, the geometric feature tensor for cross-garment modeling, and an application for animating static scans with pose-dependent deformations. The model and code are available for research purposes.