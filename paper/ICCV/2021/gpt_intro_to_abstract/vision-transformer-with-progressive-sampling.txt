Transformers have become the standard architecture for natural language processing tasks, and researchers have begun exploring their use in computer vision tasks such as image classification and object detection. However, transformers are not well-suited for processing images with large numbers of pixels due to their computational complexity. The Vision Transformer (ViT) introduced a naive tokenization scheme to address this complexity issue, but it has limitations in terms of separating semantically correlated regions and focusing on relevant image content. Inspired by the human vision system, this paper proposes a progressive sampling module for transformers, which learns where to focus attention in images. This module updates sampling locations iteratively, allowing attention to progressively converge on discriminative regions. The proposed module, called PS-ViT, outperforms existing transformer-based approaches in terms of accuracy, efficiency, and parameter usage. Experimental results on ImageNet demonstrate the effectiveness of PS-ViT compared to ViT and other state-of-the-art networks.