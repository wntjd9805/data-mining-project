Deep Neural Networks (DNNs) have revolutionized computer vision by automatically learning features and achieving impressive results in machine learning tasks. However, their success heavily relies on the availability of large annotated datasets. Data augmentation is a commonly used technique to mitigate overfitting, and traditional methods apply predefined transformations that do not change the class label. Advanced techniques, such as mixup and cutout, incorporate more loosely defined transformations in the data space. Adversarial training has also gained attention, where small imperceptible perturbations in image space can fool a well-performing classifier. However, empirical studies have shown that adversarial training reduces training accuracy, indicating a trade-off between robustness and generalization. This paper explores the use of advanced normalizing flows, specifically the Glow model, to define unsupervised augmentations with the goal of improving generalization. Normalizing flows offer advantages such as exact latent-variable inference and efficient inference and synthesis. The exact reversibility of normalizing flows is exploited to perform efficient and controllable augmentations in the learned manifold space. The contributions of this work include demonstrating the limitations of previous methods in generating on-manifold perturbations, proposing a data augmentation method based on reversible normalizing flows, generating semantically meaningful data perturbations, and achieving consistent improvements in performance compared to standard training on CIFAR-10/100 datasets.