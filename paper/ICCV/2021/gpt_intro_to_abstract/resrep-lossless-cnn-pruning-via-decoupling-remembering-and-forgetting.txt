In this paper, we address the problem of reducing the width of convolutional neural networks (CNNs) without sacrificing performance. Traditional methods such as channel pruning have limitations in achieving high compression ratios without performance drops. To overcome this, we propose a new approach called ResRep, inspired by neurobiology research on memory and forgetting. ResRep decouples the processes of "remembering" (maintaining performance) and "forgetting" (pruning), which allows for a better trade-off between resistance to training-caused damage and prunability during the pruning phase. Our approach consists of two key components: Convolutional Re-parameterization (Rep) and Gradient Resetting (Res), which are used to create structures for pruning and zero out unnecessary channels. We provide a detailed algorithm for the ResRep channel pruning pipeline. The advantages of ResRep include high resistance to training-caused damage, high prunability, automatic determination of the appropriate width for each layer, and end-to-end training. Experimental results demonstrate that ResRep outperforms state-of-the-art methods in terms of compression ratio and performance on benchmark models, including achieving a pruning ratio of 54.5% on ResNet-50 on the ImageNet dataset.