Visual content creation in 3D space has become increasingly popular, with recent methods making significant progress on various content creation tasks for 3D scenes, such as semantic view synthesis and scene extrapolation. This paper focuses on the problem of 3D scene stylization, where the goal is to render stylized images of a scene from arbitrary novel views. This has applications in virtual reality (VR) and augmented reality (AR), such as augmenting a street scene with the style of van Gogh's "Cafe Terrace at Night". The challenge lies in ensuring that the synthesized novel views not only contain the desired style but are also consistent across different viewpoints. Existing approaches combining novel view synthesis and image stylization schemes have shown problematic results, either producing blurry, short-range inconsistent, or long-range inconsistent results. A possible solution is to treat the novel view synthesis results as a video and use video stylization frameworks, but these also fail to enforce long-range consistency. To address these challenges, this paper proposes a point cloud-based method for consistent 3D scene stylization. By operating on the 3D scene representation, i.e. the point cloud, the method transfers the style of the holistic 3D scene to synthesize novel views that are both stylized and consistent across different viewpoints. The proposed method is evaluated on two real-world scene datasets and is shown to produce high-quality and consistent stylized novel view synthesis results. The contributions of this paper include the point cloud-based framework, the point cloud transformation module for style transfer, and the validation of the method's high-quality and consistent results.