In this paper, we introduce a new shopping scenario using a fashion mirror in a shopping mall. The mirror displays real-time virtual try-on results of selected clothes, allowing users to view how suitable the garments are in multiple poses. We explore different methods for achieving this goal, including single-pose virtual try-on methods and multi-pose virtual try-on methods. However, these methods have limitations such as accumulating errors in sequential generation and inconsistent results between consecutive frames. To address these issues, we propose a co-attention feature-remapping try-on framework called FashionMirror. This framework consists of two stages: parsing-free co-attention mask prediction and human and clothing feature remapping. We use a co-attention mechanism to learn the relation between consecutive human frames and the target try-on clothes, and then predict clothing masks for removal and target try-on. To achieve realistic try-on results, we warp the human and clothing information at the feature level instead of the pixel level. We evaluate the efficacy of FashionMirror against state-of-the-art methods and demonstrate its superiority in subjective and objective experiments. The contributions of this work include the proposal of FashionMirror as a novel virtual try-on framework and the introduction of parsing-free co-attention mask mechanism, which reduces inference time and improves try-on performance.