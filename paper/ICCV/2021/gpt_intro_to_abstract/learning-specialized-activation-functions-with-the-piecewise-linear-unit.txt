Activation functions play a crucial role in deep neural networks by affecting both model expressiveness and optimization. Rectified Linear Units (ReLU) have shown superior convergence compared to other non-linearities like tanh. However, human-designed activation functions, such as Leaky ReLU, PReLU, ELU, and SELU, have limitations in their shape and application across tasks. On the other hand, automatically searched activation functions, like Swish, have shown better performance. However, Swish has its own limitations, such as a highly discrete search space and computationally expensive search process. There are also methods like APL and PAU that use universal approximators as activation functions but suffer from complicated formulations and unstable learning. To address these drawbacks and fully exploit the potential of activation functions, this paper introduces a new method called Piecewise Linear Unit (PWLU). PWLU consists of a piecewise-linear-based formulation and a gradient-based learning method. The formulation covers a wide range of scalar functions, is easily learnable, and efficient for inference. The learning method addresses the input-boundary misalignment problem and combines gradient-based learning with statistic-based realignment. PWLU outperforms Swish and other activation functions on large-scale datasets like ImageNet and COCO, across various architectures. The learned activation functions show different preferences across architectures and layers, highlighting the benefits of learning specialized activation functions. Overall, this paper proposes a flexible and efficient activation function formulation and an effective and efficient learning method, demonstrating the advantages of specialized activation functions.