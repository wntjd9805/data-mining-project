Robotic grasping and manipulation of objects have been a major goal in the field of robotics. While there are some dexterous manipulators available, controlling them to operate like human hands is challenging. The existing methods for dexterous grasp synthesis can be categorized into hand-centered and object-centered methods. Hand-centered methods focus on recording grasping activity through hand joint configuration or grasp types. However, accurately annotating the coordinates or angles of each hand joint is difficult due to factors such as high degrees of freedom, self-occlusion, and self-similarity among fingers. Object-centered grasp synthesis methods pay more attention to the pose, shape, and function of the object. Existing approaches, such as GraspIt! and ContactDB, have limitations in generating anthropomorphic grasps and predicting contact patterns accurately. In this paper, we propose an approach that uses the real functional area of the object to guide the generation of functional grasp, avoiding the issues of multiple hand configurations resulting in the same contact pattern. We segment the functional parts of the object and use touch codes to describe the contact between the hand and each part of the object. We optimize the hand positions to touch the object surface and train a deep learning model to generate a human-like grasp that fits the functional parts and touch code. We also create a dataset and validate our method in synthetic data, demonstrating the effectiveness of our approach in generating human-like grasps. Our proposed grasp synthesis framework considers both the object functional parts and touch code, leading to successful task-oriented grasps.