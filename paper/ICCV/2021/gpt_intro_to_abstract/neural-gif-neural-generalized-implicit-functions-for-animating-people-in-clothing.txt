Human avatars are essential for applications in augmented and virtual reality, enabling enhanced communication and entertainment. However, realistic animation of human shape, particularly regarding articulation, soft-tissue, and non-rigid clothing dynamics, poses significant challenges. Existing body models typically rely on fixed topology templates and linear blend skinning to model articulation and non-rigid effects. However, this approach limits the types of clothing and dynamics that can be represented. Additionally, training these models requires correspondence between 3D/4D scans, which is particularly challenging for clothing that varies in topology. Recent works have used implicit function representation to reconstruct human shape but lack animatability. In this paper, we propose a new model called Neural Generalized Implicit Functions (Neural-GIF) that can animate people in clothing based on body pose. We demonstrate that Neural-GIF can accurately model complex clothing and body deformations without the need for fixed templates or registration. Our model can represent different topologies and can animate jackets, coats, skirts, and soft-tissue. Neural-GIF utilizes a neural network to approximate the signed distance field of the posed surface. We leverage the concept of factorizing motion into articulation and non-rigid deformation, but generalize it for implicit shape learning. By learning a base shape in canonical space that can be deformed, our model can more effectively represent complex character/clothing scans. We introduce a canonical mapping network to learn continuous skinning fields and a displacement field network for fine details and deformation. Extensive quantitative and qualitative comparisons demonstrate the effectiveness and robustness of our method in modeling complex clothing and generalizing to new poses.