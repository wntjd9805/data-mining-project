Deep networks have shown impressive performance in various computer vision tasks, but they are vulnerable to adversarial attacks, limiting their applicability in sensitive and safety-critical applications. Existing methods focus on improving the training algorithm to enhance robustness, but they struggle to adapt to individual characteristics of different attacks at testing time. In this paper, we propose a defense strategy that reverses the attack process and adapts to each attack during testing. We leverage the natural and intrinsic structure present in images to reverse many types of adversarial attacks. Our approach uses self-supervised objectives for defense at test time, ensuring the protection of the intrinsic signals that come with the images. The modular nature of our framework allows it to scale efficiently to any corruption that violates the natural image manifold and can be integrated into existing and future defense models. Experimental results demonstrate significant improvements in robust prediction across multiple datasets and attacks, even when the attacker is aware of our defense mechanism. Our approach outperforms state-of-the-art defense methods, providing gains in accuracy on benchmarks such as CIFAR-10, CIFAR-100, SVHN, and ImageNet. Our models, data, and code are available for further research and evaluation.