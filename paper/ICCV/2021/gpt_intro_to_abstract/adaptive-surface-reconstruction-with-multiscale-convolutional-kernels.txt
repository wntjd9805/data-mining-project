Generating accurate 3D reconstructions of objects and scenes is a challenging task in computer science. As the availability of image and scan data increases, the need for efficient methods to combine this information into a comprehensive 3D structure becomes essential. Volumetric fusion is a critical component of many 3D reconstruction pipelines, where partial observations are fused to create a global 3D representation. However, this approach faces challenges such as the exponential growth in computational costs due to the large volume of data, as well as the presence of noisy input data that necessitates the use of effective priors in the fusion process.Existing approaches have explored the use of adaptive data structures like octrees to efficiently store 3D grids and adapt algorithms accordingly. However, these approaches require specialized and highly engineered algorithms, often employing partial differential equations (PDEs) that implement basic priors favoring surfaces with minimum curvature or area. In contrast, learning-based approaches, particularly Convolutional Neural Networks (ConvNets), have shown promise in capturing complex priors from data, making them well-suited for fusion tasks. ConvNets are commonly used in 2D image processing pipelines due to their flexibility and efficiency on regular grids.This paper proposes a method to generalize standard convolutional kernels to adaptive grids, allowing ConvNets to efficiently process volumetric data. Adaptive grids not only enable efficient data storage but also capture information at different scales, which is crucial for reconstructing large datasets with varying regions of interest. The paper introduces multiscale convolutional kernels that span multiple scales and incorporate spatial and scale relationships on adaptive grids. These kernels are integrated into a U-Net-like architecture to create an end-to-end trainable volumetric fusion pipeline that computes signed and unsigned distance fields.The proposed approach demonstrates superior performance compared to traditional analytical volumetric fusion methods and recent learning-based techniques. The performance is evaluated qualitatively and quantitatively in a zero-shot generalization setting using real-world datasets. The method is capable of reconstructing large datasets with hundreds of millions of points and achieves a processing speed that is more than twice as fast as baseline methods. Overall, the proposed approach offers an innovative solution to the challenges of 3D reconstruction, leveraging the advantages of ConvNets and adaptive grids to improve accuracy and efficiency.