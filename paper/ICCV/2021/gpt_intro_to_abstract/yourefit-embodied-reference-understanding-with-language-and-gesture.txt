This paper introduces the concept of embodied reference understanding, focusing on the often neglected nonverbal form of communication - gesture. The authors argue that gestures are a crucial part of human communication and are deeply rooted in cognition and language development. The paper presents the YouRefIt dataset, consisting of videos where participants reference objects in a scene using both language and gestures. The dataset is annotated with bounding boxes and fine-grained semantic parsing. To evaluate the machine's ability in embodied reference understanding, two benchmarks are proposed: Image ERU, which predicts the bounding box of the referenced object given a frame and a sentence, and Video ERU, which locates the reference target within a video clip. The authors devise a multimodal framework that incorporates language and gestural cues and conducts experiments with various baselines and model variants. The results confirm the importance of both language and gestures in resolving ambiguities and overloaded semantics in embodied reference understanding. Overall, this paper contributes to the understanding of reference in an embodied fashion, provides benchmarks for evaluation, and presents a multimodal framework for embodied reference tasks.