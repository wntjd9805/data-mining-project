This paper introduces VL distillation, a technique that leverages knowledge distillation to facilitate the training of smaller visual linguistic (VL) models. The authors propose a set of strategies to address the challenges of aligning attention distributions and hidden embeddings between the Teacher and Student models. They use the same set of object proposals for visual token extraction, mimic the Teacher's self-attention distribution, and align token embeddings using a noise contrastive loss. The effectiveness of VL distillation is examined using a compact transformer architecture and a lightweight object detector. The proposed DistillVLM model achieves comparable performance to a large VL model and outperforms its non-distilled counterpart in VL tasks such as image captioning and visual question answering. The paper provides extensive ablations of DistillVLM and analyzes the effect of various knowledge distillation strategies, offering insights for future research on VL model distillation.