The impact of vision-based machines on individuals with disabilities has become apparent as these machines move from controlled development labs to real-world settings. Different individuals with various abilities may react differently when interacting with autonomous platforms, dynamic contexts, and infrastructure components, leading to increased risk of traffic fatalities among pedestrians with disabilities. A specific case involving Starship Technologies' food and package delivery robot highlights the consequences of failing to consider the diverse interaction needs of individuals with disabilities. Currently, the development of accessibility-aware vision-based autonomous systems lacks shared and principled development tools. One fundamental barrier is the lack of data on individuals with disabilities, who are both highly diverse and rare in large-scale data collection efforts. This rarity makes it challenging to address both joint safety-critical and accessibility-related events. In this paper, we aim to develop perception models that are robust and functional in representing pedestrians with disabilities, taking a crucial step towards comprehensive understanding of their needs. We introduce the X-World platform, which includes a simulation module integrated into CARLA, as well as a large-scale accessibility-oriented instance segmentation dataset. Additionally, we provide a real-world dataset obtained from internet videos for complementary analysis. By releasing tools and data for integrating computer vision and accessibility research, we aim to improve the quality of life for individuals with disabilities.