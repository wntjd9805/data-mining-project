Object detectors need to be adaptable to "domain shift," which can occur due to changes in weather or camera compared to the training data. Domain shifts can significantly decrease the performance of object detectors. This issue is addressed by domain adaptation methods, which aim to learn models from a source domain and adapt to a target domain. In object detection, where collecting bounding box annotations can be expensive, it is crucial to perform domain adaptation without the need for annotations in every new domain. This motivates the use of unsupervised domain adaptation (UDA), where only labeled source data and unlabeled target data are available. Additionally, training data itself can be gathered under different conditions, known as multi-source domain adaptation. UDA methods typically involve aligning source and target domains to learn invariant representations, but the choice of features to align and the mechanism to induce alignment remain open questions. This paper proposes a novel UDA method for object detection called visually similar group alignment (ViSGA). The method utilizes adversarial training and leverages the visual similarity of object proposals for aggregation. By aggregating proposals from different spatial locations, the effectiveness of adversarial training is increased. The aggregation process is dynamic, based on the distance at which proposals are aggregated, improving adaptability to varying numbers of objects in the input. The method's design choices are based on an analysis of common components of UDA methods for object detection, including the level of alignment and the alignment mechanism. The paper presents key contributions, including the proposed UDA method, an in-depth analysis of alignment techniques, evaluations in different domain shift scenarios, and consideration of multi-source domain adaptation. The effectiveness of the proposed method is demonstrated through state-of-the-art results.