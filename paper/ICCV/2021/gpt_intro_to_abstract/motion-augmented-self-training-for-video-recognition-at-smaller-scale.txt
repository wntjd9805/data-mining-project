The goal of this paper is to self-train a 3D convolutional neural network on an unlabeled video collection so that it can be effectively fine-tuned on small scale datasets. This is particularly useful for applications in small-sized companies, households, or search and rescue robotics where large amounts of labeled video data are often unavailable and deployment in compute-efficient scenarios is preferred. The common self-training approach is to transfer knowledge from pre-trained appearance models through pseudo-label prediction. However, existing methods are focused on large-scale datasets and use optical flow computation during inference, which affects efficiency. In this paper, we propose a motion-augmented self-training procedure called MotionFit, which transfers motion knowledge to the appearance model without the need for optical flow computation during inference. We generate pseudo-labels by training a motion model on a small labeled dataset and perform unsupervised clustering on the motion features to obtain cluster assignments as pseudo-labels. Our key contribution is the MotionFit procedure, which extracts motion knowledge and transfers it to the appearance model via self-training on a large-scale unlabeled video dataset. We also conduct an empirical study to determine the optimal form of video pseudo-labels at a smaller scale. Our experimental evaluation shows that our method outperforms state-of-the-art approaches for action classification and clip retrieval on two datasets.