This paper explores the relationship between the flatness of the robust loss in weight space and robust generalization in adversarial training (AT). AT is a technique used to improve the robustness of machine learning models against adversarial examples by training them with on-the-fly generated adversarial examples. However, AT often suffers from robust overfitting, where the robustness on test examples decreases over time while the robustness on training examples continues to increase. This paper investigates whether flatness in the robust loss landscape can mitigate robust overfitting and improve robust generalization. The authors propose average- and worst-case flatness measures for the robust case and evaluate their effectiveness across various AT variants and regularization schemes. The results show a clear relationship between flatness and robust generalization, with models exhibiting higher average-case flatness demonstrating improved robustness. The findings suggest that methods that improve flatness, such as weight averaging and entropy-SGD, can enhance robust generalization in AT. The paper also examines the impact of hyperparameters and other techniques on flatness and robust generalization.