Recent advancements in natural language processing (NLP) have been dominated by Transformers. These models, originally developed for language understanding, have also shown promise in the field of computer vision. The Vision Transformer (ViT) is the first computer vision model to exclusively rely on the Transformer architecture for image classification tasks. However, ViT still falls short of convolutional neural network (CNN) counterparts in terms of performance when trained on smaller datasets. This discrepancy may be due to ViT lacking certain desirable properties inherent in the CNN architecture, such as capturing local spatial structure and hierarchically learning visual patterns. In this paper, we introduce the Convolutional vision Transformer (CvT), which strategically incorporates convolutions into the ViT structure to improve performance and robustness while maintaining computational and memory efficiency. Our proposed CvT design includes convolutional token embeddings and convolutional projections in the Transformer modules. These additions allow for capturing local information, downsampling the sequence length, and reducing semantic ambiguity in the attention mechanism. Our results demonstrate that CvT achieves state-of-the-art performance on image classification tasks while utilizing fewer floating point operations and parameters compared to CNN-based models and prior Transformer-based models. Additionally, CvT can accommodate variable resolutions of input images without the need for positional embeddings. The proposed CvT architecture combines the benefits of CNNs and Transformers, enabling efficient and effective image classification.