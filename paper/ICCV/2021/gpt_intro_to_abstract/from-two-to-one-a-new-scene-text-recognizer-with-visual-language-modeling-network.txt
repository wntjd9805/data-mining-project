Scene text recognition (STR) is a crucial task in computer vision, aiming to read text content from natural images. Early methods treated text recognition as a symbol classification task, but struggled to recognize images with confused visual cues. To address this, recent STR works have shifted focus to acquiring linguistic information and adopted a two-step architecture with separate vision and language models. However, these methods suffer from the extra computation cost and difficulty of aggregating two independent information. In this paper, we propose a new architecture called Vision Language Modeling Network (VisionLAN) to tackle these problems. VisionLAN uses the vision model as the basic network and trains it to learn linguistic information in the visual context. In the testing stage, the vision model adaptively considers linguistic information for feature enhancement, improving recognition accuracy in the presence of occlusion or noise. The proposed VisionLAN achieves state-of-the-art performance on multiple benchmarks and introduces zero computation cost for capturing linguistic information. Additionally, we introduce the Occlusion Scene Text (OST) dataset to evaluate the performance of occluded images. The main contributions of this paper are the proposed VisionLAN architecture, a Weakly-supervised Complementary Learning approach for generating accurate character-wise mask maps, and the OST dataset for evaluating occlusion scenarios.