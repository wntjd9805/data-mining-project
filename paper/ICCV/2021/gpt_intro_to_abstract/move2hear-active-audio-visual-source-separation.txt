Audio-visual events have a significant impact on our daily lives. However, physical factors can either hinder or improve our ability to perceive these events. This paper introduces the concept of active audio-visual source separation, where an agent learns to move intelligently to recover the sounds emitted by a target object within a bounded time. Unlike traditional audio-visual source separation, which focuses on isolating sounds in pre-recorded videos, this task requires actively controlling the camera and microphones' positions. The paper considers two variants: one where the agent starts at the location of the desired sounding object and fine-tunes its positioning, and another where the agent starts at an arbitrary position away from the object and must make larger movements within the environment. The proposed framework, Move2Hear, utilizes reinforcement learning to train the agent to make movements that enhance the separation of the target sound from other distractor sounds. Realistic simulations are conducted using the SoundSpaces and Habitat platforms, demonstrating the success of the model in learning how to move to improve sound perception. The paper's contributions include defining the active audio-visual source separation task, presenting an RL-based framework to tackle this task, and conducting comprehensive experiments with various sounds, visual environments, and use cases. This work sets the foundation for further exploration of multi-modal agents that move to enhance audio perception.