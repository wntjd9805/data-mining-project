Referring segmentation, the task of generating segmentation masks based on natural language queries, is a challenging multi-modal task that combines natural language processing and computer vision. While deep learning methods have shown promise in this area, there are still challenges to address. One challenge is the complicated correlations between objects in images and the descriptions provided by the query expression, requiring a holistic understanding of both the image and language. Another challenge arises from the diverse objects and expressions, leading to a high degree of randomness. To tackle these challenges, we propose a Vision-Language Transformer (VLT) method that incorporates attention mechanisms to model direct interactions among all elements. This allows for capturing global semantic information and better modeling the global context of the image. Additionally, we introduce a Query Generation Module (QGM) that comprehends the language expression in multiple ways, improving the network's ability to handle randomness. We further propose a Query Balance Module to select the most suitable comprehension methods for better mask generation. Our approach enhances the fusion and utilization of multi-modal features and achieves state-of-the-art performance on multiple datasets. The proposed method demonstrates improved understanding of the holistic vision-language features and addresses the challenges presented by referring segmentation.