This paper introduces a person-centric vision-and-language grounding task and benchmark in the field of computer science. The problem of linking textual descriptions with image regions, known as visual grounding, is a fundamental capability in visual semantic tasks such as image captioning and visual question answering. The proposed task abstracts over identity information and focuses on relations and properties specified in images and text. The task utilizes captions that mask out people's names to avoid biases. The paper presents a dataset called Who's Waldo, consisting of nearly 300K images of people paired with textual descriptions and annotated with alignments between mentions of people's names and their corresponding visual regions. A Transformer-based model is proposed to link people across text and images using similarity measures in the joint embedding space. The model effectively distinguishes between different individuals in complex scenes, outperforming strong baselines.