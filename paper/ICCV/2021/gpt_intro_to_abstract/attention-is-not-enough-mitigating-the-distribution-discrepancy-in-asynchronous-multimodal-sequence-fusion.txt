This paper introduces the problem of multimodal fusion in video analysis, where the collected streams of different modalities are usually asynchronous. Previous works manually align the sequences, but this process is time-consuming. The recent MulT approach addresses this issue by extending the self-attention mechanism of Transformer. However, the direct replication of Transformer is suboptimal for asynchronous multimodal sequence fusion due to distribution mismatch between different modalities. To overcome this, the paper proposes the Modality-Invariant Crossmodal Attention (MICA) approach, which performs crossmodal attention over a modality-invariant space to bridge the distribution discrepancy. The approach uses Maximum Mean Discrepancy (MMD) and Propagated Element-level Alignment (PEA) strategies to match the distribution and achieve consistency across network layers, respectively. Experimental results demonstrate the effectiveness of the proposed approach in building reliable crossmodal relationships for multimodal fusion in asynchronous multimodal sequences. The contributions of this work include addressing the distribution discrepancy problem, performing crossmodal attention in a modality-invariant space, and achieving state-of-the-art performance on multimodal video understanding benchmarks.