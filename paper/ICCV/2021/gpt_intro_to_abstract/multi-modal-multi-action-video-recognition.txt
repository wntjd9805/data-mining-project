Video understanding is a complex task in computer vision that aims to recognize activities occurring in videos. Multi-action video recognition is a task that aims to automatically recognize all the actions co-occurring in a video. While there has been progress in single-action video recognition, multi-action recognition is still limited. To better understand videos and solve the multi-action recognition problem, it is crucial to explore relations among multiple actions, known as multi-action relations. This can be achieved by leveraging multi-modal information in videos, such as visual frames, audio, and text. Previous works have mainly focused on developing hand-crafted features or using 3D convolutional neural networks for multi-action video recognition, but they did not consider relations among multiple actions or fully exploit multi-modal information. Graph convolutional networks (GCNs) provide a generic way to model relational data, and in this work, we explore multi-action relations using GCNs by setting multiple actions as graph nodes. We also build multi-modal GCNs to better model modality-specific multi-action relations. We propose a novel approach that combines visual, audio, and textual relations to improve the recognition of multiple actions. Our contributions include the introduction of relational GCNs and multi-modal joint learning for multi-action video understanding, the development of modality-specific relational GCNs, and achieving state-of-the-art performance on a multi-action benchmark dataset.