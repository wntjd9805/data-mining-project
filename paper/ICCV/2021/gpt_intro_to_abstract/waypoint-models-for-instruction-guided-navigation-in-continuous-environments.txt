The goal of this research is to develop AI for robotic agents that can navigate new environments by following natural language instructions. Existing work in this domain has focused on simulators where agents navigate between a fixed set of nearby locations. However, transferring these agents to real robotic systems faces challenges in producing appropriate sets of nearby locations. To address this, the authors introduced a variant of the Vision-and-Language Navigation task in continuous simulated environments. In this variant, agents navigate by executing low-level actions, such as moving forward or turning. The authors explore a spectrum of action spaces between these two extremes, studying instruction-guided navigators that predict relative waypoints with varied expressivity. They develop an attention-based waypoint prediction network and train the agents using large-scale reinforcement learning. The authors find that more expressive waypoint prediction networks result in simpler paths that are faster to execute. Their models, when paired with low-level navigators, achieve better performance in navigation success and execution time compared to prior work. The contributions of this research include developing waypoint prediction networks, analyzing their effect on navigation success and execution time, and setting a new state-of-the-art on the Vision-and-Language Navigation task. Code and pre-trained models are made available.