Disentanglement learning aims to break down complex, high-dimensional data variation into interpretable factors. These factors represent distinct variables, with each dimension corresponding to one factor that can change independently of the others. Achieving disentanglement learning has several applications, including transfer learning, few-shot learning, and controllable image synthesis. While supervised approaches have made progress using fully-labeled data, it is not always feasible to have access to ground-truth labels. Unsupervised approaches have been proposed to address this limitation but often rely on the assumption that the target data is well-structured. Weakly-supervised approaches, which combine supervised and unsupervised methods, have gained popularity for their flexibility. However, existing methods in this category struggle to extract factor-aware latent representations, specifically for multi-factor disentanglement tasks. In this paper, we present a weakly-supervised multi-factor disentanglement learning framework that uses explicit and near-orthogonal latent representation to handle arbitrary numbers of factors. Our framework is designed to deal with challenging factors that are difficult to label or interpret by considering them as a single unknown factor. We validate the effectiveness and robustness of our approach through extensive evaluations on various benchmark datasets and demonstrate its applicability to real-world tasks without additional manual labeling efforts. Our contributions include a flexible framework that scales well to different datasets and tasks, a two-stage training architecture for explicit disentanglement, learning strategies to improve adversarial training, and state-of-the-art performance on multiple challenging tasks.