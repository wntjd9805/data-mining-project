Video instance segmentation (VIS) is an important task in video understanding, with applications in video editing and autonomous driving. Unlike image-level instance segmentation, VIS requires both detection and segmentation of objects in each frame, as well as tracking objects throughout the entire video. Existing approaches for VIS can be categorized into two paradigms: 'Track-by-Detect' and 'Clip-Match'. The 'Track-by-Detect' paradigm detects and segments instances for each frame, followed by frame-by-frame tracking to obtain instance sequences. In contrast, the 'Clip-Match' paradigm divides the video into short clips, obtains VIS results for each clip, and matches overlapped sub-sequences between clips. Both paradigms involve merging incomplete sequences, which can lead to error accumulation. To overcome this, we propose a new paradigm called Propose-Reduce, which generates complete instance sequences in a single step. By propagating instance segmentation results from a key frame to all others, our paradigm ensures high recall and avoids error-accumulating tracking/matching modules. We also propose a variant of Mask R-CNN for videos, named Seq Mask R-CNN, which incorporates an extra sequence propagation head to establish temporal relations across frames. Our framework achieves state-of-the-art results on the YouTube-VIS and DAVIS-UVOS validation sets.