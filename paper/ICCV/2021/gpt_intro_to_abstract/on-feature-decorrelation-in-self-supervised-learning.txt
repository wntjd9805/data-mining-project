Deep learning has become widely used in various domains, but the reliance on supervised methods for data labeling can be limiting and costly. Self-supervised learning has emerged as a promising approach to overcome this limitation by offering visual representations without the need for extensive annotations. Recent advancements in self-supervised learning have focused on improving augmentation techniques, sampling strategies, and clustering-based mechanisms. However, a common challenge in self-supervised learning is the issue of feature collapsing, where representations collapse into a constant. Existing approaches have addressed this issue but have overlooked other potential collapse problems. Feature decorrelation, a concept used in other areas of machine learning, has shown promise in improving performance. This work revisits the collapse issue in self-supervised learning and proposes a framework that incorporates feature decorrelation to mitigate collapse problems. The contributions of this work include successfully addressing complete collapse, discovering another type of collapse called dimensional collapse, establishing the connection between dimensional collapse and strong correlations, and demonstrating the benefits of feature decorrelation in various settings. The insights from this work highlight the importance and potential of feature decorrelation in self-supervised learning.