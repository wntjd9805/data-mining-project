Transformer-based models have achieved significant breakthroughs in computer vision and natural language processing tasks due to their strong ability to capture long-range dependencies. However, these models come with a high computational cost, hindering their widespread adoption on resource-constrained devices. To improve efficiency, there have been efforts to design efficient and scalable Transformers through model compression and addressing the quadratic memory and computational complexity of the self-attention mechanism. Despite these efforts, there is still a lack of specific efficient designs for Visual Transformers that take advantage of visual patterns. This paper proposes a hierarchical pooling regime that gradually reduces the sequence length as the model goes deeper, improving scalability and pyramidal feature hierarchy. The paper also highlights that average pooled visual tokens contain richer discriminative patterns than the class tokens for classification. Experimental results demonstrate that the proposed approach outperforms the baseline model on image classification benchmarks, including ImageNet and CIFAR-100, while maintaining comparable computation requirements.