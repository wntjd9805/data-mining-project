Many attempts have been made to improve the efficiency of Convolutional Neural Networks (CNNs) for deployment on mobile devices. These attempts include efficient architectural designs, pruning, network quantization, and knowledge distillation. Binary Neural Networks (BNNs) are a type of network quantization that represents networks using only 1-bit values. BNNs achieve remarkable compression and acceleration performance by replacing addition and multiplication operations with cheap bitwise operations. However, there remains a less-explored direction of further compressing and accelerating BNNs. This paper challenges the common understanding that BNNs provide the best compression and acceleration performance by training a neural network with lower than 1-bit per weight, leading to even greater compression. The authors observe that in well-trained BNN models, binary kernels tend to be clustered to a subset at each layer. This observation inspires the development of a method to identify and utilize these important subsets to achieve a more compressed model. The proposed method, called Sub-bit Neural Networks (SNNs), involves randomly sampling layer-specific subsets of binary kernels and refining them through optimization. Experimental results show that SNNs achieve significantly larger compression and acceleration ratios over BNNs while maintaining high accuracies. Hardware deployment also demonstrates that SNNs provide more than 3x speedup compared to BNNs. This work introduces a new perspective for the design and optimization of binary neural network quantization and opens up opportunities for compressed and accelerated BNNs with hardware-friendly properties.