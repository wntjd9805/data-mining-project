High-resolution videos are commonly used in computer vision tasks, but downsampling is often necessary due to storage and computational constraints. However, downscaled videos present challenges when it comes to restoring the original high-resolution quality and achieving high performance in downstream tasks. Existing methods for single image rescaling have not been adapted for video and have not addressed the issue of lost information during downsampling. This paper proposes a self-conditioned probabilistic learning approach for video rescaling, which utilizes the temporal information within the video itself. The proposed framework involves a learnable frequency analyzer to decompose the high-resolution video, a Gaussian mixture distribution to model the high-frequency component conditioned on the downscaled video, and local and global temporal aggregation modules to aggregate spatial information from adjacent frames. The original video can then be restored using a frequency synthesizer. The framework is optimized by minimizing the negative log-likelihood of the distribution. The proposed framework is also applied to lossy video compression and video action recognition tasks, achieving state-of-the-art performance and reducing storage cost and computational complexity. The main contributions of this paper include the SelfC framework for video rescaling, the exploitation of temporal information for accurate estimation of distribution parameters, and a gradient estimation method for non-differential lossy codecs, allowing for the extension of the framework to a video compression system.