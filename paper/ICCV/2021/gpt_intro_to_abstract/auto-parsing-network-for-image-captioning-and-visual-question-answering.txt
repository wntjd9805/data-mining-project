The introduction of the computer science paper discusses the prevalent use of Transformer-based frameworks in vision-language tasks, such as image captioning and visual question answering. The researchers attribute the progress to the advantages of Transformers, including efficient parallel computing, sequence-to-sequence function approximation, and the use of graph priors provided by self-attention. However, vision and language data have hierarchical and sparse structures, which differ from fully connected graphs. Without proper constraints, systems may overlook critical local context and rely too heavily on global dependencies. Previous strategies involve parsing inputs into sparse and hierarchical structures but require annotated graphs for training, which can be burdensome. To address this, the researchers propose the Auto-Parsing Network (APN), inspired by Tree-Transformer, which automatically parses inputs into trees during end-to-end training without additional graph annotations. They leverage a Probabilistic Graphical Model (PGM) to segment sequences into clusters, where only entities within the same cluster can attend to each other, embedding local context. By stacking constrained self-attention layers, a hierarchical tree can be automatically parsed. APN improves performance in image captioning and visual question answering compared to Transformer models. The researchers also develop a parsing algorithm to generate constituent trees for vision and language inputs based on calculated PGM probabilities. Overall, the contributions include the proposal of APN, its application in image captioning and visual question answering, and its consistent improvement over Transformer models in these tasks.