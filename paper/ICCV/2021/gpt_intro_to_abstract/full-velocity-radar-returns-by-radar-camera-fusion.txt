Radar is a widely used 3D sensor in the automotive industry for driving assistance and autonomous driving. Unlike LiDAR, radar has the advantage of being cost-effective and easily integrated into existing vehicles without changing their appearance. With the availability of autonomous driving datasets that include radar data, there is growing interest in exploring the use of radar data for various vision tasks such as object detection. Radar has the capability of measuring 3D positions and obtaining radial velocity based on the Doppler effect. However, using radial velocity alone to determine the movement of radar points is insufficient and can be misleading, especially for objects moving in non-radial directions. Therefore, it is essential to estimate point-wise full velocity instead of relying on radial velocity for accurate motion sensing. Additionally, radar returns are sparser compared to LiDAR, making it necessary to accumulate multiple radar frames to obtain dense point clouds for tasks like object detection. In order to address the limitations of radial velocity and enhance raw radar measurements, we propose a solution that fuses radar with an RGB camera. This fusion allows us to derive a closed-form solution for inferring point-wise full velocity from radial velocity and optical flow obtained from the camera's projected image motion. We also train a neural network to establish reliable associations between moving radar points and image pixels. Experimental results demonstrate that our method significantly improves point-wise velocity estimation and its application in object velocity estimation, radar point accumulation, and 3D object localization. The main contributions of this work are the introduction of a novel research task for radar-camera perception systems, the proposal of a closed-form solution for inferring full radar-return velocity, and the demonstration of state-of-the-art performance in various radar-related tasks.