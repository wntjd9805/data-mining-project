Self-supervised learning (SSL) has shown remarkable results in representation learning, specifically in computer vision tasks. However, the learning efficiency of state-of-the-art SSL methods is about ten times lower than supervised learning methods. This is attributed to the under-clustering and over-clustering problems in existing SSL methods. Under-clustering occurs when there are insufficient negative sample pairs, resulting in overlaps between different object categories. Over-clustering, on the other hand, happens when there are excessive negative samples, leading to false negative samples and the model clustering samples of the same category into different clusters. To address these problems, a few pioneering works have proposed methods to analyze the role of negative samples in the contrastive loss. In this paper, we propose a novel SSL framework that uses a truncated triplet loss to address both under-clustering and over-clustering. The triplet loss maximizes the distance between positive and negative pairs, and the truncated triplets avoid over-clustering by using confidence obtained from the Bernoulli Distribution model. Our method significantly improves SSL's learning efficiency and achieves state-of-the-art performance in large-scale benchmarks and downstream tasks.