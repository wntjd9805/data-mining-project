The paper introduces the task of multi-change captioning, which involves detecting and describing multiple changes in scenes without prior knowledge of the number of changes. The authors propose a framework called MCCFormers, based on encoder-decoder transformers, to perform multi-change captioning and localization. Unlike existing methods, the proposed framework dynamically adjusts spatial attention for each generated word, enabling better distinction between different changes. The authors also present a novel dataset, CLEVR-Multi-Change, for evaluating the proposed method. Experimental results demonstrate the effectiveness of MCCFormers in both change captioning and localization. The contributions of the work include addressing the novel task of multi-change captioning, proposing the MCCFormers framework, and providing a dataset for evaluating multi-change captioning.