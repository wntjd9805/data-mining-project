Deep neural networks (DNNs) have gained significant attention for their performance in various tasks such as image classification and speech recognition. The robustness of DNNs against adversarial attacks has also been a topic of recent research, with studies focusing on attacking algorithms, adversarial example detection, and adversarial training. In this research, we aim to explain the signal-processing behavior behind adversarial attacks by investigating how pixel-wise perturbations cooperate with each other to achieve the attack. We propose new methods to explain adversarial attacks from two perspectives. First, we compute regional attributions to diagnose the importance of each image region in decreasing the attacking cost. Second, we examine pixel-wise interactions and identify perturbation components in the adversarial attack. We quantify interactions among perturbation pixels using game theory and decompose the effect of the adversarial attack into these components. The Shapley value is used for explanation, providing a unique attribution that satisfies desirable properties, such as linearity, symmetry, and efficiency. Our analysis reveals that important image regions for different types of attacks are similar, suggesting that adversarial perturbations primarily interact with the foreground. Additionally, adversarially-trained DNNs exhibit different interaction patterns compared to normally-trained DNNs. We propose an efficient approximation approach to decompose the perturbation components, as the computation of Shapley values is NP-hard. Experimental results on benchmark DNNs and datasets demonstrate the effectiveness of our methods and provide valuable insights into adversarial attacks.