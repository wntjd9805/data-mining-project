Deep neural networks (DNNs) have become highly popular in various applications. However, they heavily rely on large labeled training datasets, which often introduce label noise. Cleaning up these corrupted labels can be time-consuming and expensive. Label noise can lead to overfitting in overparameterized deep networks, making it crucial to mitigate its effects. Many promising methods have been proposed to address this issue, including analyzing output predictions and reweighting samples. These methods primarily focus on in-distribution (IND) label noise and neglect out-of-distribution (OOD) noise in the testing phase. In this paper, we propose a new graph-based framework called NGC, which corrects IND noisy labels and detects OOD samples by utilizing model predictions' confidence and the geometric structure of the data. NGC does not require additional training and can reject OOD samples during testing. We evaluate NGC on benchmark datasets and real-world tasks, demonstrating its superiority over existing methods. The rest of the paper discusses related work, explains the learning problem, presents the framework, and provides experimental analysis before concluding.