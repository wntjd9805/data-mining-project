Deep neural networks have achieved impressive results in various applications, but their computationally expensive nature prevents their deployment on devices with limited computational resources. Model compression techniques, such as knowledge distillation, have emerged as a solution to obtain smaller models without significant performance loss. However, it has been observed that better teachers do not necessarily result in better student performance, indicating a capacity mismatch between the two. Existing approaches to resolve this issue require manual tuning and are not adaptable to different student models. In this paper, we propose an adaptive knowledge distillation method called Student Customized Knowledge Distillation (SCKD) that adjusts the knowledge transfer process based on the target student model's gradient similarity with the teacher. SCKD can be applied to various vision tasks and improves student performance immediately when integrated into existing knowledge distillation frameworks. Our contributions include the formulation of SCKD, its application to different visual tasks, and its superiority over conventional knowledge distillation methods.