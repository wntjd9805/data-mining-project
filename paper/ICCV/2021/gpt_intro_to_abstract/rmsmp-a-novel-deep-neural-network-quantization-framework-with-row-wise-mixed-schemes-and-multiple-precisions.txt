The deep learning or deep neural networks (DNNs) have achieved tremendous success, leading to a demand for deploying inference models onto edge-computing platforms. DNN quantization, a major type of model compression technique, has become essential to reduce computation, memory, and storage requirements in on-device inference. This is especially important for platforms with customized architecture designs like FPGA devices and ASIC chips. Various quantization schemes have been explored, including binary, ternary, fixed-point, power-of-two, and additive power-of-two. These schemes have different accuracy and hardware performance characteristics. While binary and ternary quantization reduce computation by eliminating multiplication operations, they suffer from significant accuracy loss. On the other hand, low bit-width fixed-point quantization achieves better accuracy with negligible loss compared to full-precision models. However, it still requires multiplication operations. Power-of-two quantization replaces multiplications with bit-shifting operations, leading to faster inference, but still results in moderate accuracy loss due to resolution issues. To overcome this, additive power-of-two quantization was proposed. Previous works have highlighted the importance of the first and last layers for accuracy preservation, leading to their exclusion from quantization or requiring a minimum of 8 bits. Inspired by this, layer-wise multi-precision quantization has been investigated. In this paper, we propose a novel DNN quantization framework called RMSMP, which uses a row-wise mixed-scheme and multi-precision approach. Our framework assigns a combination of quantization scheme and precision to each filter or row in the weight tensor/matrix. Candidates for schemes and precisions are practically derived to facilitate hardware implementation and preserve accuracy. This highly hardware-informative quantization strategy reduces the search space of the DNN quantization problem. We adopt power-of-two and fixed-point schemes with 4-bit and 8-bit precisions to reduce computation and preserve accuracy. Unlike existing works, we observe that using higher precisions does not necessarily depend on layer-wise sensitivity. As long as a portion of weights in each layer use higher precisions, the quantization error can be mitigated. This enables layer-wise uniformity for practical hardware implementation while maintaining row-wise flexibility. Our quantization framework contributes with a novel row-wise mixed-scheme and multi-precision approach, a highly hardware-informative solution, the best accuracy performance under the same equivalent precision as existing works, and significant inference speedup on real devices.