Deep Neural Networks (DNNs) have achieved great success in various visual tasks but require significant memory usage and computational burden. To address this challenge, the community has proposed different techniques, including network quantization. Binary Neural Networks (BNNs) offer a promising solution by restricting weights and activations to -1 and +1. However, BNNs suffer from a decrease in accuracy compared to real-valued networks due to quantization error and gradient mismatch. In this paper, we propose an approach that considers both quantization error and weight diversity to improve the performance of BNNs. We introduce a rectified clamp unit (ReCU) to revive "dead weights" and provide a mathematical proof of reduced quantization error. We analyze the impact of weight standardization on BNNs and highlight the contradiction between minimizing quantization error and maximizing information entropy. Experimental results on CIFAR-10 and ImageNet datasets demonstrate that our approach achieves state-of-the-art performance and faster training convergence. Overall, this paper contributes by analyzing "dead weights" in BNNs, proposing ReCU, and providing insights into weight standardization and the trade-off between quantization error and information entropy in BNNs.