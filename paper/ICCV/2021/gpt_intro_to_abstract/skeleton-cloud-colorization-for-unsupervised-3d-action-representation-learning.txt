Human action recognition is a rapidly growing field with applications in various domains such as human-computer interaction, video surveillance, and game control. Different types of input data, such as RGB-based, depth-based, and 3D skeleton-based, are used for human action recognition. Among these, 3D skeleton data has gained increasing attention due to its ability to encode high-level representations of human behaviors, as well as its lightweight and robust nature. Many supervised methods have been developed to learn spatio-temporal representations for skeleton-based action recognition using deep neural networks, such as Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Graph Convolution Networks (GCNs). However, these methods require a large number of labeled training samples, which can be expensive to obtain. Learning effective feature representations with minimal annotations is therefore a critical challenge. Few works have explored unsupervised representation learning from unlabelled skeleton data, where the primary approach is to reconstruct skeleton sequences from encoded features using encoder-decoder structures. In this paper, we propose an unsupervised representation learning scheme that treats skeleton sequences as 3D skeleton clouds and learns features from spatial and temporal color labels. We stack skeletal data from all frames together to create a spatial-temporal skeleton cloud and colorize each point in the cloud based on its temporal and spatial orders in the original sequence. We then apply a point-cloud based auto-encoder framework to learn spatial-temporal features from the corresponding joints' colors. Our proposed method outperforms state-of-the-art unsupervised and semi-supervised skeleton action recognition methods, and its performance is comparable to supervised methods. This work is the first to convert the problem of unsupervised skeleton representation learning into a novel skeleton cloud repainting task.