Video inpainting has become an important task in repairing missing or corrupted regions in video sequences. It has been widely used for video editing purposes, but the advances in video inpainting techniques have also raised concerns about the potential misuse of this technology for creating fake videos. Therefore, there is a need for effective video inpainting detection methods to distinguish between pristine and inpainted videos. Existing methods in this field have limitations in terms of leaving clues and artifacts or being frame-level based. In this paper, we propose a frequency-aware spatiotemporal transformer framework for video inpainting detection. Our approach focuses on capturing spatial and temporal artifacts using a multihead self-attention mechanism and incorporating frequency-aware features to improve generalizability. We optimize our framework using a hybrid loss function and evaluate its performance on different datasets. The experimental results show that our proposed framework achieves competitive performance and robustness against unseen approaches. Overall, our contributions include introducing a transformer-based framework for video inpainting detection, incorporating frequency-aware features, and achieving competitive performance in inpainting detection tasks.