Compared with low dynamic range (LDR) images, high dynamic range (HDR) images provide better visual details in both bright and dark regions. While there have been advancements in HDR image reconstruction, HDR video reconstruction is a more challenging and less explored problem. Existing HDR video reconstruction techniques often rely on costly specialized hardware, limiting their practicality for ordinary consumers. To address this, a low-cost approach is to utilize video sequences captured with alternating exposures. However, accurate alignment and fusion of LDR images with different exposures present difficulties due to over-exposed and under-exposed regions. This paper proposes a two-stage framework for HDR video reconstruction from sequences with alternating exposures. The first stage aligns and blends the images to reconstruct the coarse HDR video, while the second stage performs more sophisticated alignment and fusion in the feature space using deformable convolution and temporal attention. By avoiding the need for highly accurate optical flow estimation, the proposed approach reduces learning difficulty and eliminates ghosting artifacts. To evaluate the method, a real-world video dataset captured with alternating exposures is created as a benchmark. The proposed approach achieves state-of-the-art results on both synthetic and real-world datasets.