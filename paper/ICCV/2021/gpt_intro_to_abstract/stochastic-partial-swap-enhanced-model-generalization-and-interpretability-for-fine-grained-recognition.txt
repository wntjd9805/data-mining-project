Fine-grained recognition is a challenging task in computer vision, as it requires recognizing subtle differences in object categories. Conventional methods that focus on learning high-level features often fail in fine-grained classification. Previous works have explored part-based and sampling-based techniques to address this challenge, but they either require complex training procedures or intense computation, limiting their applicability. In recent endeavors, deep mid-level models have shown promise in fine-grained recognition, as they can capture local information and complement high-level features. However, prior approaches have merely adopted off-the-shelf mid-level models without enhancing them. In this paper, we propose a novel Stochastic Partial Swap (SPS) strategy to enhance mid-level models' generalization abilities. Our method randomly swaps feature units between samples during training, exploiting sample features as a noise source. This strategy encourages more neurons for feature representation, helps suppress dominant neurons, and enhances the classifier's robustness through augmentation. We extensively evaluate our approach on seven datasets and show significant improvements over baseline methods and even high-level models. Our approach achieves state-of-the-art or comparable performance on various datasets, demonstrating its effectiveness in fine-grained recognition.