This paper discusses the challenges and limitations of using scribble annotations for semantic segmentation in neural networks. Scribble annotations are more easily available but provide sparse and inconsistent supervision. To address these issues, the paper proposes two solutions: minimizing entropy to encourage deterministic predictions, and using a network-embedded random walk module to promote uniform intermediates. Additionally, self-supervision is adopted during training to impose consistency on the neural representation. The effectiveness of the proposed approach is demonstrated through experiments on a scribble dataset, including scenarios where scribbles are shrunk or dropped. The approach outperforms existing methods and is comparable to fully supervised ones. Ablation studies are conducted to verify the effectiveness of each operation. The code and data for this approach are available online.