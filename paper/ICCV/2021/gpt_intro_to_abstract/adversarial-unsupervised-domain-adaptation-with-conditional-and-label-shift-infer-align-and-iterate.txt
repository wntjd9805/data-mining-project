Abstract:Deep learning methods heavily rely on large volumes of labeled training data and the assumption that the training and testing data are independently and identically distributed (i.i.d.). However, real-world implementation scenarios can be diverse, making it costly to label datasets for every target environment. To address this, unsupervised domain adaptation (UDA) techniques can transfer knowledge from a labeled source domain to unlabeled target domains. One common approach in UDA is adversarial training, which uses a discriminator to enforce domain invariance. However, this approach assumes covariate shift, which is not always realistic as different classes may have their own appearance shift protocols. This paper proposes an alternative optimization scheme to align conditional and label distributions in UDA, which includes class-level balancing and aligning the posterior distribution of the target classifier. The proposed method is evaluated on UDA classification and semantic segmentation benchmarks and can be extended to partial UDA scenarios. Theoretical and empirical analyses of conventional UDA methods under this assumption are also provided.