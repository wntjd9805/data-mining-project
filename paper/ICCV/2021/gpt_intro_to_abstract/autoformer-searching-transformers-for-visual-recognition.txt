The introduction presents the growing interest in vision transformers for computer vision tasks, highlighting their superior capabilities in capturing long-range dependencies compared to traditional convolutional neural network models. However, the design of transformer neural architectures is challenging, and previous works relied on manual crafting or were focused on natural language tasks. To address these issues, the paper introduces AutoFormer, a new architecture search algorithm specifically designed for vision transformers. AutoFormer aims to strike a good combination of key factors in transformers, such as network depth, embedding dimension, and head number, and efficiently find different transformer models that suit various resource constraints and application scenarios. The approach involves constructing a large search space and proposing a supernet training strategy called weight entanglement, where different transformer blocks share weights for common parts during training. The weight entanglement strategy enables thousands of well-trained subnets, making it possible to obtain diverse architectures that meet different resource constraints while maintaining the same level of accuracy as training from scratch. The paper demonstrates the superior performance of AutoFormer compared to hand-crafted transformer models on ImageNet and showcases its promising transferability to downstream vision classification tasks. The contributions of this work include being the first automatic search algorithm for vision transformer models, a simple and effective framework for efficient training of transformer supernets, and achieving state-of-the-art results and promising transferability.