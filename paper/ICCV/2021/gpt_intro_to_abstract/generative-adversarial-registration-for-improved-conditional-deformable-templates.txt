Deformable image registration is an important technique in computer science that allows for the quantification of geometric dissimilarity through the warping of images. In population studies, the registration of images onto a deformable template is a crucial step in standardized analyses. Templates are used in various biomedical imaging tasks such as alignment, brain extraction, segmentation, and regression models. While templates can be obtained from a reference database, it is preferred to construct templates specific to the population of interest. Templates strongly influence subsequent morphometric analysis, and the construction of these templates has received significant attention. However, a single template may not capture the wide structural variability within a population, so conditional template estimation with continuous and/or categorical attributes is considered. Conditional templates constructed on diverse image sets enable sub-population modeling and eliminate the need for arbitrary thresholding of demographic information. Implicit and explicit models have been proposed for template estimation, but they often produce templates that are unrealistic and do not resemble the data they represent. This can negatively impact downstream applications. In this paper, we present a learning framework that uses generative adversarial learning to estimate sharp and realistic templates. Our approach combines template generation and registration sub-networks with a discriminator that assesses the realism and condition-specificity of the synthesized templates. By encouraging high-frequency detail through adversarial objectives, our templates gain naturalistic boundaries without the need for post-processing. We have developed stable and accurate 3D GANs for large medical volumes with limited sample and batch sizes. Our contributions include a generative adversarial approach to template generation and registration, construction of conditional templates across diverse datasets, and improvements in template construction methodologies. The code for our approach is available online.