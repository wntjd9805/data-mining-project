This paper presents a new fast sample re-weighting method to address the challenges caused by data biases in deep neural networks (DNNs). The performance of DNNs is influenced by the scale and quality of training datasets, including noisy labels and imbalanced classes. Sample re-weighting is an effective strategy to handle data biases by upgrading the weights of good samples and downgrading the weights of bad samples. However, finding optimal weights for model training is a dynamic process, and traditional methods can have negative effects on overall DNN accuracy. This paper introduces a learning-to-learn based algorithm that optimizes sample weights concurrently with model training. The proposed method incorporates meta optimization inside the supervised training process to find the optimal weights per sample. Unlike existing approaches, the proposed method does not require an additional reward dataset, which is typically biased and has clean and class-balanced labels. The method leverages a dictionary to monitor the training history and provides reward signals to optimize sample weights. Additionally, instead of maintaining separate model and sample weight updates, the proposed method enables feature sharing to improve training efficiency. The method is applicable to various data biases, including noisy labels and imbalanced classes, and can be further enhanced with momentum re-labeling and MixUp regularization techniques. Experimental results demonstrate the competitive performance of the proposed method compared to previous approaches.