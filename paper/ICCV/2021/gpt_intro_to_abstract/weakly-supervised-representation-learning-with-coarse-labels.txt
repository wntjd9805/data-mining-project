Deep learning has gained significant attention in computer vision and NLP applications for its ability to achieve superior performance. This success has made various applications, such as autonomous driving, visual search, and question-answering systems, feasible. Compared to traditional models with hand-crafted features, deep learning works in an end-to-end learning manner, allowing it to explore the most discriminative patterns directly from raw materials. Deep learning also demonstrates improved performance compared to methods using hand-crafted features. However, deep learning requires a massive amount of labeled examples closely related to the target task to achieve ideal performance. Although large-scale labeled datasets are available, the correlation between the learned representations and the target task is often not well-investigated. In scenarios where task-specific labels are expensive to obtain, learning representations for the target task becomes suboptimal. To address this issue, this paper proposes leveraging coarse-class labels to learn appropriate representations for the target task. Fine-grained patterns essential for the target task are often neglected when training deep models solely with coarse-class labels. Additionally, the popular pretext task of instance classification may introduce irrelevant noisy patterns. The paper introduces a new algorithm that incorporates coarse-class classification to improve representation learning when task-specific labels are unavailable. The proposed algorithm, along with a novel instance proxy loss, significantly enhances performance on real-world applications where only coarse-class labels are accessible.