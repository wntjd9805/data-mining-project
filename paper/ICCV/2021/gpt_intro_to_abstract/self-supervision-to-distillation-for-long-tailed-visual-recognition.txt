Deep learning has made significant progress in visual recognition by training neural networks on large-scale balanced datasets. However, real-world data follows a long-tailed distribution, which poses challenges in collecting balanced datasets. Learning directly from long-tailed data leads to performance degradation due to the imbalanced data distribution. Current approaches to address this issue include class re-balanced strategies, but they often risk over-fitting tail classes. To overcome these challenges, recent work has proposed a two-stage training scheme that separates representation learning and classifier training. However, this scheme does not effectively handle imbalance label distribution issues. In this paper, we propose a new learning paradigm for long-tailed visual recognition that combines the advantages of existing methods. We incorporate label correlation into a multi-stage training scheme through the use of soft labels generated by a teacher network. Soft labels capture the relation between classes and can transfer knowledge from head classes to tail classes. We introduce a self-distillation framework called Self Supervision to Distillation (SSD) that includes a self-supervision guided distillation label generation module. This module generates less biased but more informative soft labels for self-distillation. We demonstrate the effectiveness of our approach on various long-tailed recognition benchmarks and achieve state-of-the-art performance. Our contributions include the introduction of the SSD framework, the proposal of a self-supervision guided soft label generation module, and the achievement of state-of-the-art performance on multiple datasets.