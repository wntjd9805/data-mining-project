In this paper, we propose Saccade-Cam, an algorithmic and hardware framework for visual attention control in deep depth estimation from a single view. Previous monocular approaches assume an equal distribution of sensor pixels across the camera's field-of-view (FOV), whereas animal eyes distribute resolution unevenly using saccades to view the scene with high acuity. Saccade-Cam leverages visual attention to automatically distribute resolution onto a scene, improving monocular depth estimation. Unlike previous efforts that apply attention after image capture, our framework distributes resolution during image capture, complementing existing attention-based learning methods. By leveraging attention during image capture, Saccade-Cam offers potential bandwidth reduction and computational efficiency benefits. We demonstrate the effectiveness of Saccade-Cam in depth estimation and object detection for robot navigation. Our contributions include defining the problem of distributing image resolution around the scene, designing an end-to-end network for resolution control, and validating our method on a real hardware prototype. Compared to existing alternatives, Saccade-Cam is the only self-supervised approach that provides adaptive, monocular depth estimation by manipulating attention inside the camera during image capture.