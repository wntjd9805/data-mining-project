Camera calibration is a crucial task in computer vision, with applications in various fields such as image rotation correction, visual aesthetics assessment, and augmented reality. Traditional approaches rely on detecting line segments in an image to infer camera parameters, but they can degrade when inlier lines are falsely detected. Recent deep learning-based methods have shown promise in accurately inferring camera parameters using semantic cues learned by convolutional neural networks (CNNs). However, these methods have limitations in capturing global image characteristics and learning geometric structures without explicit supervision. In this paper, we propose a novel neural network called CTRL-C (Camera calibration TRansformer with Line-Classification) that leverages transformers, which have been successful in other vision tasks. Transformers allow for the incorporation of both geometric and semantic cues, as well as capturing long-term dependencies in an image. The proposed network architecture includes a transformer encoder that processes image features and positional encoding to generate semantic tokens, as well as a transformer decoder that aggregates semantic and geometric tokens for camera parameter prediction. Additionally, an auxiliary task is introduced to classify line segments into convergence lines, improving the overall camera parameter regression performance. Experimental results on benchmark datasets demonstrate that CTRL-C outperforms previous methods in terms of camera calibration accuracy, particularly in estimating the horizon line.