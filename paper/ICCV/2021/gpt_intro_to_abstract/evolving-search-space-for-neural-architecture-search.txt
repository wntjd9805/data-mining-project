Deep neural networks have become prevalent in various real-world applications, and Neural Architecture Search (NAS) has emerged as a way to automate the customization of deep neural network architectures for specific applications. Previous research has investigated methods such as Reinforcement Learning (RL) and Evolutionary Algorithm (EA) based approaches to automate architecture design. Additionally, weight sharing based methods have been proposed to reduce computational costs. While these methods have shown promising results, the quality of the search space plays a crucial role in NAS. Improved search space design has been shown to positively impact the performance of existing works. However, recent methods have limitations in terms of the number of candidate operations available. This paper aims to address this limitation by proposing a Neural Search-space Evolution (NSE) scheme that enables the exploration of a larger search space. The NSE scheme starts with a reasonably sized subset of the full search space and progressively explores additional operation candidates while retaining past knowledge. The search process is iterative and combines all architectures found by a supernet trained with the One-Shot method. By maintaining an optimized search space as knowledge, the search process can always proceed with new candidate operations added to the pending list. The proposed scheme is also extended to multi-branch schemes, allowing for the adaptive selection of multiple operations. Experimental results on ImageNet demonstrate that NSE effectively exploits the potential of a large search space and achieves state-of-the-art results under resource constraints. The contributions of this paper include the proposal of NSE as a scheme for large space neural architecture search and the introduction of a probabilistic modeling approach for the multi-branch scheme.