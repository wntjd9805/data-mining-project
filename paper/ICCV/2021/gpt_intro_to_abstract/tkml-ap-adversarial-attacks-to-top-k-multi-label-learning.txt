The past decade has witnessed significant advancements in deep neural networks (DNNs) for computer vision problems. However, these powerful DNN models are susceptible to adversarial attacks, where specially designed perturbations can cause the model to make incorrect predictions. This vulnerability hinders the practical application of machine learning systems and motivates research into generating adversarial examples to analyze and improve the security of DNN models. Most existing works on adversarial example generation focus on multi-class classification problems, where instances are assigned to one mutually exclusive class. In real-world applications such as image annotation or document categorization, the problem is often multi-label learning, where instances are associated with a non-empty subset of labels. Furthermore, the output of the system is typically a fixed-sized set of top-k predicted labels. This introduces more opportunities for attackers and uncertainties for defenders. Two common settings for adversarial attacks in top-k multi-label learning are untargeted attacks, where the top-k labels are replaced with arbitrary labels, and targeted attacks, where the model is coerced to predict a specific set of labels.This paper introduces the first untargeted and targeted adversarial attack algorithms for top-k multi-label learning, based on a continuous formulation of the ranking operation. The goal is to perturb the predictions of a top-k multi-label learning algorithm by either removing ground-truth labels from the top-k set (for untargeted attacks) or moving target labels to the top-k set (for targeted attacks) with minimum changes to the original label rankings. The key challenge is optimizing perturbations that change the top-k rankings of predicted labels. To address this, the paper proposes a reformulation of the ranking operation that allows for efficient numerical algorithms based on gradient descent methods. The loss functions for adversarial perturbations are convex, even for non-linear models, encouraging equally effective local optima. The effectiveness of the proposed method is demonstrated through experiments on benchmark datasets. The contributions of this work include the presentation of the first algorithms for untargeted and targeted adversarial attacks in top-k multi-label learning, a convex reformulation of the ranking operation for optimization, and empirical evidence of the method's effectiveness in attacking state-of-the-art top-k multi-label learning algorithms.