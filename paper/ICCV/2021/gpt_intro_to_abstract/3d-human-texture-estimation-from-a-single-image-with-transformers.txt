This paper addresses the problem of estimating 3D human texture from a single image, which is crucial for 3D human reconstruction and has various applications in virtual reality, augmented reality, gaming, and biometrics. While existing methods use deep convolution neural networks (CNNs) for this task, they suffer from the limitation of processing global information effectively due to the misalignment between the input and output. In contrast, 2D computer vision tasks have aligned input and output. To overcome this limitation, the paper proposes a Transformer-based framework called Texformer. The Transformer utilizes an attention mechanism that allows for the global processing of information, leading to improved 3D human texture estimation. The Transformer consists of Query, Key, and Value components, where the Query corresponds to a vertex on the 3D human mesh, the Key is a part-segmentation map obtained from the image, and the Value is the input image. Through weighted averaging, the Transformer aggregates source information to generate the output UV map while preserving fine details. The contributions of this work include the introduction of the Texformer framework, the combination of RGB values and texture flow for UV map synthesis, and the use of a part-style loss to enforce closer appearances to the input image. Experimental results using the Market-1501 dataset demonstrate the effectiveness of the proposed method.