This paper introduces a novel Audio-Visual Transformer approach for localizing and highlighting the main speaker in both audio and visual channels in multi-speaker conversation videos. The proposed approach utilizes various correlations between visual and audio signals, including virtual interactions between speakers and relationships between visual and auditory modalities. Unlike previous methods, relationships across temporal segments are also considered through a temporal self-attention mechanism in the Transformer structure, allowing for robust identification of the main speaker. Additionally, a Cycle Synchronization Loss is introduced for self-supervised learning of main speaker localization. A new dataset is collected for main speaker detection. This work is among the first to automatically localize and highlight the main speaker in multi-speaker conversation videos in both audio and visual channels.