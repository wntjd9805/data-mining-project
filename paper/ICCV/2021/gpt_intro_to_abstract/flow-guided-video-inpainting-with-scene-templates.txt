Video inpainting is a technique used to fill masked regions in videos with content that blends naturally with the rest of the video. This is useful in video editing tasks such as removing watermarks or unwanted objects, and video restoration. Many existing methods for video inpainting rely on optical flow, which involves copying unmasked data from other frames into the masked region of a given frame. However, these methods are highly dependent on the quality of the optical flow, and even small errors can result in noticeable visual distortions in the inpainted video. In this paper, we propose a new approach that aims to reduce visual distortions by deriving a generative model that closely represents the physical image formation process in video. Our model constrains the warps between frames to be consistent with the scene and the images, enabling more temporally consistent and plausible flows. We also introduce a novel optimization procedure and interpolation strategy that further improve the quality of inpainting and reduce geometric distortions and blurring artifacts. Additionally, we introduce a benchmark dataset for foreground removal in videos and demonstrate the superiority of our algorithm compared to state-of-the-art methods through user studies and quantitative evaluations.