Generative Adversarial Networks (GANs) have shown remarkable success in generating high-quality images. However, GANs lack the ability to comprehend or control the underlying generative factors. To address this limitation, researchers have focused on studying the structure of GAN's latent space and identifying interpretable directions within it. Some methods rely on supervised learning and leverage labels assigned to the generated images to discover meaningful directions. Others impose unsupervised constraints to identify linear interpretable latent space directions. However, these approaches have their limitations, either assuming known factors or requiring subjective evaluation.In this paper, we propose a novel unsupervised and model-agnostic method for learning non-linear interpretable paths in GAN's latent space. We achieve this by employing radial basis function-based warping functions, each parametrized by a set of latent space warping operations. These warping functions enable the generation of non-linear paths through their gradients. By optimizing the trainable parameters of the warping functions, we ensure that images generated along different paths are easily distinguished. We compare our method with state-of-the-art approaches qualitatively and quantitatively, demonstrating its superiority in producing more disentangled and interpretable changes in the image space.To evaluate the interpretability and disentanglement of the paths in the latent space, we propose a quantitative evaluation protocol that measures the correlation between the paths and corresponding attribute changes in the generated images. We utilize pretrained semantic classifiers to assign attributes to the generated images. Our experiments show that our non-linear paths lead to larger attribute changes in the attribute space compared to linear paths, without compromising the quality of the generated images.The main contributions of this paper include: (1) the proposal of an unsupervised and model-agnostic method for discovering non-linear interpretable paths in GAN's latent space using RBF-based warping functions, (2) the introduction of a quantitative evaluation protocol for measuring the interpretability and disentanglement of latent space paths by analyzing attribute changes in the generated images, and (3) the application of our method to four pretrained GANs, demonstrating its superiority over state-of-the-art approaches in terms of disentanglement, interpretability, and attribute changes.