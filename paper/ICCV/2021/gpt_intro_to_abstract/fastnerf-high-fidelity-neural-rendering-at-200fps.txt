Rendering scenes in real-time at photorealistic quality has been a long-standing goal in computer graphics. Traditional approaches such as rasterization and ray-tracing require significant manual effort and pre-processing to achieve both quality and speed. Neural rendering has emerged as a disruptive alternative, incorporating neural networks into the rendering pipeline to output images directly or model implicit functions to represent scenes accurately. Neural Radiance Fields (NeRF) is one prominent neural rendering technique that learns an implicit volumetric representation of a scene from a few images, enabling rendering from novel viewpoints with high-quality and realistic results. However, NeRF-based methods suffer from high computational requirements for image rendering, limiting their real-time capabilities. To address this challenge, we propose a caching-based approach called FastNeRF that balances memory and computational efficiency. By factorizing the problem into two independent functions, one conditioned on position and the other conditioned on ray direction, we can cache the outputs separately, reducing the required memory. This factorized architecture enables FastNeRF to be cached in the memory of high-end consumer GPUs, enabling fast function lookup times and significantly improving test-time performance. Our contributions include: (1) the first NeRF-based system capable of rendering photorealistic novel views at 200FPS, thousands of times faster than NeRF, (2) a graphics-inspired factorization that can be compactly cached and queried to compute pixel values in rendered images, and (3) a blueprint for efficiently implementing the proposed factorization on GPUs.