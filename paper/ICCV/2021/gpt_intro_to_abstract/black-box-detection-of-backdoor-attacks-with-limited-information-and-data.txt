This paper introduces the problem of backdoor attacks in Deep Neural Networks (DNNs) and proposes a black-box backdoor detection (B3D) method to address this issue. Backdoor attacks aim to embed a backdoor in a DNN model by injecting poisoned samples into its training data, which can cause the model to produce desired outputs when activated by a backdoor trigger. The existing backdoor defenses rely on strong assumptions of model and data accessibility, which are impractical in real-world scenarios. In the black-box setting, where only query access to the model is attainable, the proposed B3D method formulates backdoor detection as an optimization problem and solves it using clean data to reverse-engineer the potential trigger for each class. The paper also presents B3D-SS, an adaptation of B3D that uses synthetic samples when clean samples are unavailable. Extensive experiments on several datasets demonstrate the effectiveness of B3D and B3D-SS in detecting backdoor attacks on DNN models. Additionally, the paper proposes a strategy to mitigate the discovered backdoor in an infected model by rejecting any input with the trigger stamped, without modifying the black-box model.