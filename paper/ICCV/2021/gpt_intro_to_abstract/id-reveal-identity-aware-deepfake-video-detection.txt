Advancements in synthetic media generation have enabled realistic manipulation of images and videos. However, this has also led to concerns about misuse, prompting significant research in digital media forensics, particularly in DeepFake detection. While current detection methods show excellent performance, they struggle to generalize to previously unseen manipulation methods, limiting their practicality. This issue has been addressed in the literature through various strategies such as domain adaptation, active learning, augmentation during training, and ensemble procedures. However, these methods still face challenges when dealing with videos characterized by different digital histories. Additionally, the focus of existing research has predominantly been on face-swapping, overlooking other effective manipulation techniques like facial reenactment. To illustrate the limitations of current approaches, an experiment was conducted using the winning solution of the DeepFake Detection Challenge. The results demonstrated a significant drop in performance when detecting facial reenactment, as well as a loss in accuracy when dealing with low-quality compressed videos. Moreover, current approaches often function as black-box models, making it difficult to predict their behavior in realistic scenarios. Recognizing the lack of reliability in current supervised deep learning methods, a new perspective is proposed, focusing on preserving biometric traits rather than addressing a binary real or fake classification. A new example-based forgery detection approach is introduced, which detects facial manipulations based on the subject's identity and specific face motion. The approach incorporates a facial feature extractor, a temporal network for detecting biometric anomalies, and a generative adversarial network for predicting person-specific motion based on expressions. The networks are trained on real videos from various subjects, and during test time, the distance between the test video and a set of pristine videos of the target person is computed using the temporal ID network's embedding. The proposed method demonstrates generalization to different types of manipulations, even on low-quality videos, with a significant average improvement compared to state-of-the-art approaches.