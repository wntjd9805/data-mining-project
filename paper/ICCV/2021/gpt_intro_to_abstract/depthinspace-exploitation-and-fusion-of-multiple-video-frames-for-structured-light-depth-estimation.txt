This paper discusses the limitations of existing depth-sensing algorithms and proposes a novel approach using deep neural networks trained in a self-supervised manner. The authors build upon previous work that uses a photometric loss function and introduce a novel training scheme that uses optical flow predictions from ambient images to find matched pixels, enhancing training stability and accuracy. They also extend the model to fuse information from multiple video frames, resulting in more precise disparity maps with sharper edges and fewer artifacts. Finally, they propose to fine-tune a single-frame disparity estimation network using the fused disparity maps. Overall, the contributions of this paper include the novel training scheme, the fusion of information from multiple frames, and the fine-tuning of the disparity estimation network.