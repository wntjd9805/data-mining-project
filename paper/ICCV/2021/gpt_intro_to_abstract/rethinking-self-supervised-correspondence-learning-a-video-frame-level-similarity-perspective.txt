Learning visual correspondence across space and time is a fundamental problem in computer vision with applications in 3D reconstruction, scene understanding, and object dynamics modeling. The research in this field can be divided into two categories: object-level correspondence for object tracking and fine-grained correspondence for optical flow estimation and video object segmentation. However, most existing approaches rely on simulations or limited human annotations for training supervision, which limits their generalization ability across different data and tasks. To address this problem, this paper proposes Video Frame-level Similarity (VFS) learning, a self-supervised approach for space-time correspondence without explicit tracking-based tasks. VFS learns correspondence by computing the similarity between frame-level features using a siamese network. The proposed model is based on the ResNet architecture and achieves state-of-the-art performance surpassing previous self-supervised correspondence learning approaches. The paper identifies several key elements for successful VFS learning: training with large frame gaps and multiple frame pairs, training with or without color augmentation for different levels of correspondence, training without negative pairs, and using deeper networks for improved performance. By analyzing the results and performance, the paper presents VFS as a strong baseline for self-supervised correspondence learning and highlights the importance of intermediate representations in understanding image-level self-supervised similarity learning. Additionally, VFS demonstrates the effectiveness of learning without negative pairs in improving both fine-grained and object-level correspondence. Overall, this paper contributes to the field by providing a comprehensive study of self-supervised correspondence learning and its applications.