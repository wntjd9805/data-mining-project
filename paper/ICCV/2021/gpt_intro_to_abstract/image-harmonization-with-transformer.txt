Combining regions of different photographs into a realistic composite is a fundamental problem in computer vision and graphics. The composite, synthesized by combining the foreground from one image with the background from another image, often suffers from the issue of inharmonious appearance between the foreground and background due to distinct imaging conditions. This issue can be addressed by image harmonization, which aims to adjust the foreground to make it compatible with the background in terms of appearance. Traditional harmonization methods have focused on better matching techniques using hand-crafted statistics, while recent deep learning models have achieved better performance by leveraging deep models and large-scale datasets. However, current deep models still face limitations in capturing global background context and addressing the inharmony introduced during the reconstruction process. To overcome these limitations, this paper proposes the use of Transformer, a neural network architecture known for capturing long-range context dependencies. Transformer is applied to image harmonization to capture global context and improve the adjustment of foreground and background colors. Inspired by the role of light in harmonizing images, the paper further introduces the concept of disentangled harmonization, which separates material-dependent reflectance and light-dependent illumination for better image harmonization. The contributions of this work include the design and implementation of harmonization Transformer frameworks with and without disentangled representation, as well as comprehensive experiments demonstrating the efficacy of the proposed frameworks in image harmonization and other vision tasks such as image inpainting and enhancement. The results show that the proposed frameworks outperform previous methods in terms of performance.