Transformer has gained increasing popularity in computer vision due to its ability to capture long-range dependencies. The core component of Transformer, self-attention, models the relationship between tokens in a sequence. However, self-attention lacks the ability to consider the ordering of input tokens. Therefore, it is crucial to incorporate explicit representations of position information for Transformer models, especially when dealing with structured data. There are two main approaches to encoding positional representations for Transformers: absolute and relative. Absolute methods encode the absolute positions of input tokens, while relative methods encode the relative distance between input elements and learn the pairwise relations of tokens. Relative position encoding (RPE) has been proven effective in natural language processing but its efficacy in computer vision remains unclear. Recent studies have provided conflicting conclusions on the effectiveness of relative position encoding in vision transformers.Furthermore, the original relative position encoding is designed for 1D word sequences in language modeling. It is unclear whether extending this approach to 2D images, which are spatially structured, is suitable and whether directional information is important in vision tasks.In this paper, we review existing relative position encoding methods and propose new methods specifically designed for 2D images. Our contributions include comprehensive analysis of key factors in relative position encoding, an efficient implementation that reduces computational cost, four new relative position encoding methods dedicated to vision transformers, and empirical evidence demonstrating the effectiveness of relative position encoding in image classification tasks while highlighting the importance of absolute encoding for object detection.Experimental results on ImageNet and COCO datasets show that our proposed methods outperform the original models, improving top-1 accuracy by 1.5% and mean average precision by 1.3% without any hyperparameter adjustments. This research addresses previous controversies and provides valuable insights for the usage of relative position encoding in vision transformers.