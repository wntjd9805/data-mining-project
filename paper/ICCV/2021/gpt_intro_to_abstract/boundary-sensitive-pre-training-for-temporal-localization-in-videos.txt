Video analysis has expanded from trimmed video action classification to video temporal localization due to the prevalence of long, untrimmed videos in real-world applications. However, existing datasets for video temporal localization are either too small or lack temporal boundary annotations. In this paper, we propose a novel boundary-sensitive pretext (BSP) task to address this problem. We also investigate the issue of model pre-training for temporal localization in videos, as current models are typically pre-trained on action classification datasets. To overcome the lack of large-scale datasets with temporal boundary annotations, we propose a method to synthesize untrimmed videos with boundary annotations. Three types of temporal boundaries are generated using existing action classification video data. We compare the performance of different pretext tasks and show that the BSP task outperforms others. Our experiments demonstrate that model pre-training for temporal localization tasks leads to improved performance on benchmark datasets for tasks such as temporal action localization, video grounding, and step localization.