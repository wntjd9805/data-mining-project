Inverse rendering is a challenging task that aims to estimate reï¬‚ectance, shape, and lighting from a single image, with potential applications in augmented and mixed reality. Current optimization-based methods use hand-crafted priors to address the ill-posed nature of the problem but may lead to artifacts. This paper proposes a holistic inverse rendering framework that jointly estimates these intrinsic properties by formulating the rendering process in an end-to-end trainable way with a 3D lighting representation. The proposed Volumetric Spherical Gaussian representation captures view-dependent effects and enables handling of strong directional lighting. A differentiable renderer is designed to train the model by enforcing the re-rendering constraint, ensuring physically correct predictions. The experimental results demonstrate that the proposed approach outperforms existing methods in terms of lighting estimation and object insertion in indoor scenes. The proposed lighting representation enables realistic cast shadows and angular high-frequency details, allowing for more realistic object insertion in augmented reality applications, including highly specular objects. This work is the first to estimate a complete continuous light field function from a single image, including both high dynamic range (HDR) and high-frequency spatial and angular details, despite being trained with only low dynamic range (LDR) images.