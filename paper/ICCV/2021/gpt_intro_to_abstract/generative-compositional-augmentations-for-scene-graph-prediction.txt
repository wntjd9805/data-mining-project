Reasoning about objects and their relationships is crucial for both human and machine cognition. However, machines struggle with understanding compositions that they have not encountered before. This paper focuses on the task of predicting compositions of objects and relationships from images, known as scene graph generation (SGG). Accurate scene graph generation can improve various downstream tasks, such as visual question answering and image captioning. The authors highlight the challenges of SGG, including the long-tail data distribution and the appearance of zero-shot compositions at test time. Despite recent improvements, the state-of-the-art model still has a low recall for zero-shot compositions. To address this, the authors propose a compositional augmentation approach using generative adversarial networks (GANs) to increase the diversity of training examples. They introduce scene graph perturbation methods and novel metrics to evaluate the quality of perturbed scene graphs. The proposed model outperforms a strong baseline in zero-shot, few-shot, and all-shot recall. The code for the model is provided in the paper.