Normalization methods, such as Batch Normalization, Layer Normalization, and Instance Normalization, have been widely used in training deep neural networks to improve stability and convergence speed. However, these traditional normalization techniques are not effective in scenarios with distribution shifts, where the training and test data come from different distributions. In this paper, we propose two novel normalization techniques called CrossNorm and SelfNorm to address the problem of distribution shift. CrossNorm focuses on enlarging the training distribution by augmenting the style of the images. By swapping the channel-wise mean and variance of the feature maps during training, CrossNorm transfers style between images without changing the content labels, making the model more robust to appearance changes. On the other hand, SelfNorm aims to reduce the discrepancy between the training and test data distributions by adjusting the RGB means and variances using attention mechanisms. This recalibration of style reduces appearance variance and promotes style consistency between the training and test data. Although CrossNorm and SelfNorm appear to take opposite actions (style augmentation vs. style reduction), they both use channel-wise statistics and have the common goal of improving generalization robustness. Furthermore, CrossNorm enhances the capacity of SelfNorm by exposing it to more diverse styles during training. The contributions of this paper are three-fold: first, we explore the use of feature normalization for generalization under distribution shifts, which is a new direction compared to previous efforts. Second, we introduce CrossNorm and SelfNorm as simple yet effective normalization techniques that complement each other and improve generalization robustness. Third, we demonstrate that CrossNorm and SelfNorm outperform existing methods in various fields, tasks, settings, and types of distribution shift, such as vision or language tasks, classification and segmentation tasks, fully or semi-supervised settings, and synthetic or natural distribution shifts.