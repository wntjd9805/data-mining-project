Recovering 3D representations from 2D images is a fundamental task in computer vision. Recent advancements in neural radiance fields methods have shown great potential in synthesizing new views with high fidelity. However, these methods heavily rely on accurate camera pose estimation, which is a challenging task. Existing approaches to address this issue can only optimize camera poses under certain constraints. In this paper, we propose GNeRF, a novel algorithm that estimates both camera poses and neural radiance fields without requiring accurate initialization. Our algorithm consists of two phases: coarse estimation using adversarial training and joint refinement using a photometric loss. By incorporating Generative Adversarial Networks (GANs), we enhance the NeRF model to optimize 3D representation and camera poses in complex scenes. Unlike previous methods, our pipeline is fully differentiable and end-to-end trainable, overcoming challenges such as repeated patterns, low textures, and noise. Additionally, our method can predict new poses without per-scene estimation or extensive optimization. Experimental results demonstrate that GNeRF performs on par with existing methods in regular scenes and outperforms baselines in challenging scenarios with limited texture.