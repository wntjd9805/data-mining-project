This paper introduces a method for knowledge distillation in deep networks for object detection. The authors address two problems: the imbalance of background and foreground features in distillation, and the lack of consideration for instance-level relations. They propose a structured instance graph, where nodes represent region of interest (RoI) features and edges represent feature similarity between instances. By directly distilling the graph from teacher to student, the authors aim to capture global topological structures while matching local feature patches. To overcome the issues of background noise and dense background-related edges, the authors introduce techniques to control background node loss and design a loss function for edges. The proposed method aims to compress and transfer knowledge from a large teacher model to a smaller student model, making it suitable for deployment on mobile devices.