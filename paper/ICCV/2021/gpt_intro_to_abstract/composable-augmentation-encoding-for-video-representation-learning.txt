This paper focuses on contrastive learning for self-supervised video representations. The contrastive objective aims to bring positive pairs of latent representations closer together while pushing negative pairs apart. This approach eliminates the need for additional annotations as positive and negative pairs can be constructed from the data itself. Previous successful techniques for positive pair generation, such as augmentation-invariant contrastive learning, have shown impressive results for image representation learning. However, for videos, the use of temporal shift as a natural data augmentation can remove valuable fine-grained information. In this paper, the authors propose a generalized framework called Composable AugmenTation Encoding (CATE) to retain augmentation-aware information in self-supervised video representation learning. CATE incorporates encoded data transformations, including spatial and temporal relationships between views, into the contrastive learning framework. The authors conduct thorough evaluations and demonstrate that CATE learns representations that preserve useful information more effectively than a view-invariant baseline without augmentation encoding. They also observe that different downstream tasks benefit from the awareness of different augmentations, with temporal awareness being particularly helpful for fine-grained action recognition. The proposed framework achieves state-of-the-art performance on datasets designed for action recognition, as well as standard benchmarks.