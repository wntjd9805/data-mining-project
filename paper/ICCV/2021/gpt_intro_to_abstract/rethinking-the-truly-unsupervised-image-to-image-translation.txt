Image-to-image translation is the task of generating plausible images in a target domain given an image from a source domain. Existing methods for image translation have used either image-level supervision (paired data) or set-level supervision (domain labels). However, the latter approach assumes that the domain labels are given a priori, which can be a bottleneck as the number of domains and samples increases. In this paper, we propose a truly unsupervised image-to-image translation model that does not rely on any form of supervision. Our model consists of three sub-modules: clustering the images to approximate the domains, encoding the content and style of input images, and learning a mapping function among the estimated domains. To achieve this, we introduce a guiding network that utilizes a shared encoder with two branches: one provides pseudo domain labels and the other encodes images into feature vectors (style codes). We employ a differentiable clustering method and a contrastive loss to estimate domain labels and extract style codes, respectively. By jointly training the guiding network and a generative adversarial network (GAN), our model successfully separates domains and translates images in an unsupervised manner. We evaluate our model quantitatively and qualitatively, comparing it with existing set-level supervised models under unsupervised and semi-supervised settings. The results demonstrate that our model outperforms baselines across different levels of supervision and is insensitive to hyperparameters. Our contributions include clarifying the definition of unsupervised image-to-image translation, introducing the guiding network to handle the unsupervised translation task, and demonstrating the superiority of our model compared to previous approaches. Furthermore, our model serves as a strong baseline for semi-supervised image translation.