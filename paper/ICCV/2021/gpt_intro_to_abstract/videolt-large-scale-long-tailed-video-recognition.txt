Deep neural networks in computer vision have achieved remarkable success in tasks like image classification and object detection. However, training these networks with real-world data, which often have a long tail of categories with very few training samples, poses significant challenges. Existing methods for balancing class distributions in long-tailed data focus mainly on image classification tasks, with limited efforts made for video classification. This is due to the weak labeling nature of videos and the difficulty in applying existing resampling and reweighting techniques. This paper introduces FrameStack, a simple yet effective approach for long-tailed video classification. FrameStack operates on video features and can be easily integrated into state-of-the-art video recognition models. It addresses the long-tail problem by dynamically selecting the number of frames based on the recognition performance of the model for each target class. The proposed method adapts to prevent overfitting for head classes and underfitting for tail classes. Furthermore, the paper presents a large-scale long-tailed video recognition dataset called VideoLT, which contains 256,218 videos manually labeled into 1,004 classes. Extensive experiments are conducted to compare the proposed method with existing long-tailed image methods, demonstrating its superiority in long-tailed video recognition. The dataset, code, and results are publicly available.