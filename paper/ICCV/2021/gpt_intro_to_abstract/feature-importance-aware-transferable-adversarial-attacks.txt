Deep neural networks (DNNs) have achieved impressive performance in various vision tasks. However, recent studies have shown that DNNs are vulnerable to adversarial examples, which are crafted to deceive these networks. Adversarial attacks pose significant concerns in safety- and security-sensitive areas such as autonomous driving. While many methods have been proposed to generate adversarial examples, black-box attacks that rely on limited knowledge have gained attention due to their practicality. Transfer-based attacks, which exploit the cross-model transferability of adversarial examples, have shown promise. However, traditional attacking methods often exhibit weak transferability due to overfitting to the source model. Existing methods also generate adversarial examples by indiscriminately distorting features without considering the intrinsic features of objects in the images, leading to overfitting and hindering transferability. This paper presents a Feature Importance-aware Attack (FIA) that aims to improve the transferability of adversarial examples by disrupting the important object-aware features that influence different models' decisions. FIA introduces aggregate gradients to effectively suppress model-specific features while preserving object-aware importance. Compared to traditionally indiscriminate attacks, FIA significantly defocuses and misleads the models, failing to capture important features and focusing on trivial areas. By applying random transformations, FIA ensures consistency in object-aware features while fluctuating non-object features. Gradients are then averaged to statistically suppress model-specific features while preserving object-aware features.The main contributions of this paper are three-fold. Firstly, we propose FIA, which disrupts critical object-aware features and enhances the transferability of adversarial examples. Secondly, we analyze the low transferability of existing works and introduce aggregate gradients to guide the generation of more transferable adversarial examples. Finally, extensive experiments on diverse classification models demonstrate the superior transferability of adversarial examples generated by FIA compared to state-of-the-art transferable attacking methods.