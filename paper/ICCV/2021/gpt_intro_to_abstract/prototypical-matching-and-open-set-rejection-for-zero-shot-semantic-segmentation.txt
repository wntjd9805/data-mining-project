This paper introduces the concept of zero-shot segmentation (ZSS) in the field of semantic segmentation. ZSS aims to generate segmentation masks for both seen and unseen categories in a given image. The paper discusses the challenges faced in ZSS, such as the need for massive training samples with pixel-wise annotations and the demand for knowledge transfer from seen classes to unseen classes. It also compares ZSS with zero-shot learning (ZSL) and generalized zero-shot learning (GZSL) and highlights the importance of semantic representations in ZSS. The paper proposes a strict inductive setting for ZSS, where only the information of seen classes is available during training. It suggests using a prototypical way instead of the convolutional classifier way and introduces a lightweight projection network to bridge the mapping between semantic information and visual features. The paper also addresses the bias problem in ZSS by proposing an open-set rejection (OSR) module. The OSR module helps in accurately classifying unseen objects and achieving better parsing performance. Overall, the paper makes several contributions, including clarifying the different settings of ZSS, introducing prototypical matching, incorporating open-set rejection, and achieving state-of-the-art performance in ZSS.