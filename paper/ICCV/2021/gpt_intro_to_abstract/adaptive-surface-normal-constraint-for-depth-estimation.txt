Depth estimation from a single RGB image is an important task in computer vision. Previous approaches have focused on minimizing pixel-wise depth errors, but these methods do not accurately capture 3D geometric features. To address this issue, recent efforts have incorporated geometric constraints, with surface normal being a commonly used attribute. Surface normal can be estimated from depth and encodes local geometric context. However, existing methods struggle to determine reliable local geometry, leading to difficulties in capturing local features, sensitivity to geometric variations, and high computational costs. In this paper, we present a simple and effective method to correlate depth estimation with surface normal constraint. Our approach randomly samples a set of candidates for local geometry and determines the confidence score of each candidate based on the consistency of learned geometric features. The normal is then estimated as a weighted sum of all candidates. Our method represents a significant improvement in depth prediction quality compared to state-of-the-art methods. We achieve this by capturing sufficient information from the target point's neighborhood, robustly estimating normals in the presence of local variations, and leveraging learned contextual features. Our method outperforms existing approaches on public datasets. The main contributions of this paper are the introduction of the adaptive surface normal constraint for depth estimation, the development of a simple and effective method, and the superior performance compared to state-of-the-art methods.