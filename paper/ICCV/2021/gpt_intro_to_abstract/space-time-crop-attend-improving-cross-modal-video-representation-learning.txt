Visual representations have evolved significantly in the last two decades, with advancements in manually designed algorithms and representations learned from data using deep neural networks and manual supervision. The current third generation of visual representations focuses on self-supervised learning, where representations are learned from data without any manual annotations. These self-supervised representations, obtained from methods such as MoCo, SimCLR, or SwAV, outperform supervised ones on various downstream tasks. However, existing self-supervised video representation learning approaches still have limitations. Firstly, there is a lack of sufficient encoding of spatial invariances in video representations. Second, the encoding of temporal information in self-supervised learning is not optimal. To address these challenges, this paper proposes two improvements. Firstly, spatial augmentations are moved to the feature space, allowing for more efficient and effective self-supervised learning without increasing memory usage. Secondly, a contextualized pooling function based on the transformer architecture is proposed to better capture temporal dependencies in video representations. The proposed approach, Space-Time Attention and Cropping (STiCA), combines these improvements and achieves state-of-the-art performance on standard benchmark datasets.