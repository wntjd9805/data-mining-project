Large-scale point cloud semantic segmentation is gaining attention for its applications in autonomous driving, virtual reality, and robotics. While progress has been made in small-scale segmentation, fully supervised learning requiring extensive annotation is still a challenge. To address this issue, weakly supervised methods have been proposed but have limitations in large-scale scenarios. In this paper, we propose a perturbed self-distillation framework that addresses two critical issues: designing auxiliary supervision for unlabeled points and deriving context regularization for modeling the relationship among labeled points. We introduce perturbed self-distillation to establish a well-formed graph topology among all points, enabling effective information flow between labeled and unlabeled points. We also propose a context-aware module to refine the graph topology. Our contributions include the introduction of the perturbed self-distillation framework and the context-aware module, which significantly improve performance over state-of-the-art methods. Our proposed framework achieves a 3.0% improvement on average across three datasets and also enhances the performance of the fully supervised baseline.