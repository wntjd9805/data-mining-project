Learning good visual representations without supervision has gained significant attention in recent years. These representations are beneficial for training downstream tasks such as image segmentation. Various unsupervised representation learning methods, including contrastive learning, have achieved state-of-the-art performance by relying on pretext tasks. However, in real-world scenarios, decentralized image data is growing rapidly, and data privacy regulations prevent centralization of the data. Existing methods cannot leverage decentralized unlabeled data while preserving privacy. Federated learning, an emerging distributed training technique, enables collaborative learning but relies on data labels and struggles with non-IID data. In this paper, we propose FedU, a federated unsupervised representation learning framework that learns representations from decentralized data while preserving privacy. We introduce a communication protocol for aggregating and updating online encoders, considering the impact of non-IID data on Siamese networks. Additionally, we propose a divergence-aware predictor update module that dynamically determines how to update predictors based on the degree of divergence caused by non-IID data. Extensive experiments demonstrate the effectiveness and importance of FedU.