Video-based problems have been extensively studied in computer science due to their wide applications. Neural networks have made significant advancements in these problems, especially with the availability of large-scale labeled video data. However, obtaining sufficient training video data can be challenging and expensive due to the need for annotations. In this paper, we focus on the problem of Video-based Unsupervised Domain Adaptation (VUDA), which aims to transfer knowledge from a labeled source domain to an unlabeled target domain by learning domain-invariant feature representations. While existing VUDA methods assume that the source and target domains share the same label space, this may not hold in real-world applications. Therefore, we introduce the concept of Partial Domain Adaptation (PDA), where the target label space is a subspace of the source label space. We propose a novel problem called Partial Video Domain Adaptation (PVDA), which further extends PDA by considering the additional temporal features present in videos. We address the challenge of negative transfer caused by the misalignment of spatial and temporal features between source-only classes and target data in PVDA. To tackle this challenge, we present a Partial Adversarial Temporal Attentive Network (PATAN) that constructs robust temporal features and utilizes both spatial and temporal features to accurately filter out source-only outlier classes. Additionally, we propose three benchmark datasets for PVDA research, covering a wide range of scenarios with distinct domain shifts. Our contributions include the formulation of PVDA as a novel and challenging problem, the introduction of PATAN to address the challenges of PVDA, and the creation of PVDA benchmarks. Experimental results demonstrate the effectiveness of our proposed method, achieving state-of-the-art performance on the benchmark datasets.