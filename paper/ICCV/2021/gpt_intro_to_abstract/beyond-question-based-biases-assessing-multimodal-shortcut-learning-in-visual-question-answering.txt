Visual Question Answering (VQA) is a popular task in computer vision that aims to develop models capable of answering questions about the contents of images. However, recent research has shown that VQA models can rely on superficial statistical patterns, known as "shortcuts," in the training data to predict correct answers without truly understanding the underlying content. These shortcuts can introduce biases and limit the generalizability of VQA models. While existing evaluation protocols focus on text-based shortcuts, this paper introduces VQA-CounterExamples (VQA-CE), an evaluation protocol that considers multimodal shortcuts involving both textual and visual elements. VQA-CE is designed to identify and assess the reliance of VQA models on these shortcuts without necessitating retraining. The paper proposes a method to discover these shortcuts and identifies a collection of co-occurrences of textual and visual elements that strongly predict certain answers. Furthermore, the paper introduces a set of counterexamples from the validation set where these shortcuts produce incorrect answers. Evaluation of existing VQA models reveals that they are significantly affected by these shortcuts, and current bias-reduction methods are ineffective in mitigating their impact. The paper concludes by identifying several shortcuts that VQA models may exploit and highlights the need for further research in understanding and addressing the issue of dataset biases in VQA. The contributions of this paper include the proposed shortcut discovery method, the introduction of the VQA-CE evaluation protocol, and the identification of the reliance of state-of-the-art VQA models on these shortcuts.