In this paper, we focus on two important learning scenarios in computer science: generalized few-shot learning (GFSL) and incremental few-shot learning (IFSL). These scenarios involve learning a high-performing classifier for a set of base classes with ample training samples, but only a few training samples are available for novel classes, making it challenging to learn them. Additionally, preventing the forgetting of base classes and addressing classifier calibration across classes are also crucial in GFSL and IFSL due to the imbalance in training samples.While previous work has addressed some of these challenges, this paper aims to address all three. To accomplish this, we propose a three-phase framework. In the first phase, we focus on general representation learning using a large base dataset. In the second phase, we concentrate on achieving high performance on novel classes while preventing forgetting of base classes. To achieve this, we introduce base-normalized cross entropy, which amplifies the softmax output of novel classes and enforces weight constraints to preserve previous knowledge. In the third phase, we tackle the problem of calibrating the overall model across base and novel classes.To demonstrate the effectiveness of our framework, we provide T-SNE plots showing the performance of the model at each phase. We also highlight the contributions of this work, including the framework itself, the base-normalized cross entropy method, and an extensive evaluation of the proposed framework on images and videos, showcasing its state-of-the-art results in GFSL and IFSL.Overall, this paper presents a comprehensive approach to address the challenges in generalized and incremental few-shot learning, providing solutions for learning novel classes, preventing forgetting of base classes, and achieving classifier calibration across classes.