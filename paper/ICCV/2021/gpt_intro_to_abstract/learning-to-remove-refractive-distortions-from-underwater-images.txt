Underwater scenes observed in air often suffer from distortion caused by refraction at the wavy water surface. Removing these distortions can be beneficial for various applications in underwater exploration and outer-space expeditions. However, it is challenging to remove these distortions due to the non-linear light transport through the water surface and the blurriness caused by fast-evolving waves. Classical approaches rely on a sequence of images or videos of a static scene, which limits their applicability to images captured on moving platforms. Recent work has proposed model-based and learning-based methods, but they have limitations such as fixed parametric models or the requirement for a large training set. In this paper, we propose a distortion-guided network (DG-Net) for restoring distortion-free underwater images. The key idea is to use a distortion map that models the pixel displacement caused by water refraction to guide the network in making better predictions. We use a convolutional neural network (CNN) to estimate the distortion map from the refracted image, incorporating physical constraints into the training losses. The temporal consistency of the distortion map is exploited by taking three sequential images as input. Recurrent layers refine the CNN-predicted distortion maps to enforce temporal consistency. We then use a distortion-guided generative adversarial network (GAN) to recover a sharp, distortion-free image. The distortion map guides the training of both the generator and discriminator of the GAN. Our network is trained on a synthetic refracted image dataset that resembles underwater scenes.We evaluate the DG-Net on both synthetic and real underwater image datasets and compare it to state-of-the-art methods. Our results demonstrate superior performance, especially in the presence of large distortions. Unlike model-based methods, our approach does not require long video sequences of static scenes for accurate reconstruction. Furthermore, our method can handle dynamic scenes captured in a short time interval, making it suitable for scenarios with moving objects. Compared to learning-based methods, our network achieves better accuracy with fewer training data and generalizes well to real scenes.