Abstract:Enabling intelligent agents, such as indoor mobile robots, to plan context-sensitive actions in their environment requires a comprehensive understanding of the scene, including both geometric and semantic information. Machine learning methods have shown promise in predicting geometric and semantic attributes; however, their performance is hindered when the distribution of training data does not align with the scenes encountered at test-time. Gathering costly annotated data or employing semi-supervised learning can help alleviate this issue, but it may not be feasible in open-set scenarios with unknown classes. Therefore, self-supervised methods that do not rely on labeled data are advantageous. Recent advancements in scene-specific methods, such as Neural Radiance Fields (NeRF), have demonstrated the ability to represent scene geometry and appearance using neural networks trained solely on images and camera poses. Building upon this, we propose a scene-specific 3D semantic representation called Semantic-NeRF, which can be efficiently learned with in-place supervision to perform various applications. We show that the joint prediction of geometry and semantics benefits from the self-supervised learning of geometry, as they are highly correlated. While semantic labels are inherently human-defined and require some form of annotation, our method clusters self-similar structures to generate categories. We present a system that takes RGB images and associated camera poses as input and trains a network to generate implicit 3D representations of both geometry and semantics. We evaluate our system on both synthetic and real-world scenes, demonstrating its ability to generate dense semantic labels from partial or noisy input. This has practical applications for scenarios where limited labeling is possible or when only an imperfect single-view network is available, such as in robotics.