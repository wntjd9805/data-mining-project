Analyzing sports videos and automating action recognition, retrieval, and detection require robust algorithms. Current skeleton-based deep learning techniques for action recognition heavily rely on accurate 2D pose detection. However, pose detectors often fail in fast-paced sports videos due to blur and occlusions. To address this issue, end-to-end learned models operate directly on the video stream but tend to latch onto specific visual patterns rather than fine-grained motion. We propose Video Pose Distillation (VPD), a weakly-supervised technique where a student network learns to extract robust pose features from RGB video frames in a new video domain. VPD combines the advantages of both pose-based and end-to-end methods by exploiting rich visual patterns and constraining descriptors to agree with the pose estimator whenever possible. We enforce additional constraints to encourage the student network to predict both instantaneous pose and its temporal derivative. Through weak supervision, the student network learns to fill in the gaps left by the noisy pose teacher. VPD features improve performance on few-shot, fine-grained action recognition, retrieval, and detection tasks in the target sport domain without requiring additional ground-truth action or pose labels. Experimental results on diverse sports video datasets demonstrate the benefits of VPD features. The proposed method surpasses its teacher in situations where pose is crucial and remains competitive even in cases where end-to-end methods dominate. The contributions of this paper include the introduction of VPD, achieving state-of-the-art accuracy in few-shot, fine-grained action understanding tasks, and the creation of a new dataset for figure skating and extensions to three existing datasets to facilitate future research.