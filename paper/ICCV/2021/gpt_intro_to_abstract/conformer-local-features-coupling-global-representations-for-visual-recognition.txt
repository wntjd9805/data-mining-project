Convolutional neural networks (CNNs) have revolutionized computer vision tasks by effectively extracting local features from images. However, CNNs struggle to capture global representations, which are crucial for tasks such as object detection. To address this limitation, the transformer architecture has been introduced to visual tasks, showing promise in capturing complex spatial transforms and long-distance feature dependencies. However, visual transformers often neglect local feature details, reducing discriminability between foreground and background. In this paper, we propose a dual network structure called Conformer, which combines the strengths of CNNs and visual transformers. Conformer consists of a CNN branch and a transformer branch, leveraging local convolution blocks, self-attention modules, and MLP units. We introduce the Feature Coupling Unit (FCU) to align and fuse the CNN and transformer features. This fusion procedure greatly enhances the global perception capability of local features and the local details of global representations. Conformer outperforms CNNs and visual transformers in terms of representation learning, demonstrating its potential as a general backbone network.