Deep neural networks, like convolutional neural networks (CNNs), have shown success in computer vision tasks. Recently, there has been a focus on extending this research from 2D to 3D space with the development of 3D scanning techniques. 3D sensors, such as LiDAR and RGB-D cameras, capture data in the form of point clouds, making it important to effectively recognize this data in robotics and autonomous driving applications. However, labeling 3D data is more expensive than 2D data, highlighting the need for unsupervised learning techniques specifically designed for point clouds. While CNNs have been successful with 2D image data, they struggle with the irregularity of point cloud data. Thus, specialized network architectures are needed for point cloud recognition. Previous studies have used point cloud classification networks or graph layers as encoders, extracting a global feature representation for the decoder. The decoder typically "folds" a 2D plane into a 3D object surface to generate point clouds. However, current methods generate the point cloud from an arbitrary 2D plane, which may lack the capacity to model complex 3D surfaces. In this paper, we propose a novel framework, PSG-Net, that generates the seed for the decoder from the input point cloud, instead of using fixed or randomly sampled 2D points. We incorporate a seed generation module and seed feature propagation module in the PSG-Net architecture. Our network achieves comparable performance to state-of-the-art methods in various unsupervised tasks and outperforms them in supervised point cloud completion. We also demonstrate the superiority of our model through experimental analysis. Overall, our proposed framework improves reconstruction-based learning for point clouds by using input-dependent point-wise features as the seed.