Active speaker detection is a challenging task in computer science that involves identifying the current speaker in a video. This problem requires the integration of facial motion patterns and speech waveform. Despite its various applications, such as speaker diarization and human-computer interaction, active speaker detection in real-world scenarios remains an open problem.Current approaches for active speaker detection focus on modeling the audio and visual streams using recurrent neural networks or 3D convolutional models. These approaches are suitable for single speaker scenarios but are overly simplified for multi-speaker cases.The multi-speaker scenario presents two major challenges. First, there is a possibility of incorrect face-voice assignments, leading to false positives when facial gestures resemble speech motion patterns. Second, the temporal consistency of multi-modal data needs to be enforced as active speakers switch during conversations.In this paper, we propose a principled approach to address the multi-speaker problem. Our key insight is to jointly model visual representations from multiple speakers along with a single audio representation. By formulating the active speaker detection task as an assignment problem, we aim to match multiple visual representations with a singleton audio embedding.Our approach, called Multi-modal Assignation for Active Speaker detection (MAAS), utilizes multi-modal graph neural networks to solve the local assignation problem. It is also flexible enough to propagate information from a long-term analysis window. Our empirical findings show that our approach outperforms state-of-the-art methods on the AVA Active speaker benchmark, bringing significant performance improvements.The contributions of this paper include a novel formulation for the active speaker detection problem, the demonstration of its solvability using Graph Convolutional Networks, and the introduction of a new benchmark dataset called "Talkies" for active speaker detection. We also plan to make all resources of this project publicly available to ensure reproducible results and facilitate future research in this area.