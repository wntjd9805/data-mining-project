Recent years have seen the success of deep convolutional neural networks (DCNNs) in semantic segmentation. However, training these networks requires a large amount of accurately annotated data, which is expensive and time-consuming to collect. Semi-supervised learning (SSL) offers a solution by using a small amount of labeled data combined with a large amount of unlabeled data to train an accurate model. Self-training, which involves generating pseudo labels for unlabeled data using model predictions, has been a popular approach in semi-supervised learning for semantic segmentation. However, most self-training methods assume a class-balanced data distribution, which does not align with the long-tailed class distributions found in many real-world datasets. This can lead to biased predictions and harm the effectiveness of self-training. Only a few recent works have attempted to address this issue by sampling pixels based on predicted results, but they still suffer from bias due to the underlying distribution mismatch. This distribution mismatch problem is largely overlooked and hinders further improvements in semi-supervised semantic segmentation. In this work, we propose a simple yet effective method to address the distribution mismatch issue and improve semi-supervised semantic segmentation. Our method, called Distribution Alignment and Random Sampling (DARS), redistributes the biased pseudo labels to align with the true distribution. We also introduce a progressive strategy during self-training to prevent the model from being overwhelmed by noisy data and to avoid overfitting to highly confident pseudo-labeled examples. Our method is generic, simple, and efficient, and can be easily incorporated into other self-training pipelines for semi-supervised semantic segmentation. Experimental results show that our approach achieves significant performance improvements compared to state-of-the-art methods on datasets such as Cityscapes and PASCAL VOC 2012. We also analyze the performance gain in semi-supervised semantic segmentation with increasing amounts of unlabeled data, and identify potential bottlenecks and future directions for research in this area.