Object pose estimation is an important task in computer vision and robotics applications. Most existing works focus on instance-level estimation, which limits their ability to perceive the poses of a wide range of objects in our daily life. To address this limitation, previous approaches have proposed category-level pose estimation methods that can handle novel object instances without requiring CAD models. In this paper, we present a problem called CAPTRA, which stands for CAtegory-level Pose Tracking for Rigid and Articulated Objects, and propose a solution using a live point cloud stream. Our goal is to continuously track the 9DoF pose for rigid objects or each individual part of an articulated object. We compare our work to a related approach called 6-PACK, which focuses on rigid object pose tracking. To achieve accurate and fast pose tracking, we propose a pipeline that combines coordinate-based approaches and direct pose regression. We introduce RotationNet, a neural network that directly regress small rotations, and CoordinateNet, which predicts dense normalized coordinates to handle translation and object size. By combining the outputs from these networks, we can compute sizes and translations analytically. Our proposed method outperforms the previous state-of-the-art on benchmark datasets for both rigid and articulated object pose estimation. Additionally, our method demonstrates robustness to pose errors and achieves the fastest speed among all methods.