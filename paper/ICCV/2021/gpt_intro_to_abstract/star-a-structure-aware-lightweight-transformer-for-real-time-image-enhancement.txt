Recent advancements in learning-based methods have led to significant progress in image and video enhancement tasks like super-resolution, denoising, low light enhancement, color constancy, and tone mapping. However, deploying these methods on edge devices faces two main challenges. Firstly, these methods need to process high-resolution images efficiently within a limited computation budget, necessitating trade-offs between model flexibility and computation efficiency. Secondly, they must incorporate structural and global information from input images to achieve stable and high-quality results. This is especially important for tasks like color constancy, low light enhancement, and tone mapping, where incorporating global information is crucial. Even for tasks with local support, region-based processing that is aware of image structure tends to produce better results. Previous work has attempted to tackle these challenges through three approaches: using stacked, deep CNNs, estimating one set of global adjustment functions, or using a segmentation network to process images in semantically-meaningful regions. However, these approaches have their limitations. In this paper, we propose STAR (Structure-aware Transformer), a lightweight backbone for real-time image post-processing tasks. STAR captures long-range dependencies among image patches, implicitly capturing the structural relationships within an image. It is a general architecture that can be easily adapted for various learning-based image enhancement tasks. Unlike stacked convolution layers, STAR is based on the Transformer module, which consists of multi-head self-attention and fully connected layers. We design the STAR network specifically for image enhancement tasks to efficiently extract structural information. STAR tokenizes image patches into token embeddings and learns token-wise dependencies, similar to word embeddings in natural language processing. Experimental results demonstrate that STAR outperforms CNNs in terms of efficiency and semantic meaningfulness in various image enhancement tasks. Extensive validations on tasks like illumination enhancement, white balance, and photo retouching with 3D lookup tables show that STAR achieves improved performance with significantly reduced model complexity, making it suitable for real-time processing on edge devices. For instance, compared to CNN-based methods, STAR-based methods can achieve 1.8dB PSNR improvement in low light enhancement while requiring only 25% parameters and 13% FLOPS.