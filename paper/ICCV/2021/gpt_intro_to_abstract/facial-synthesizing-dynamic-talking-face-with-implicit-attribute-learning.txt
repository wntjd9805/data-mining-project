Synthesizing realistic talking faces driven by audio input is a challenging task in computer vision, computer graphics, and virtual reality. While there have been some research advances, generating talking faces that are indistinguishable from real videos remains difficult. The information contained in dynamic talking faces can be categorized into explicit attributes, such as lip motions, which need to be synchronized with the audio, and implicit attributes, such as head movements and eye blinks, which have weak correlations with the phonetic signal. Most existing works focus on explicit attributes only, while few explore the correlation between implicit attributes, like head pose, and the input audio. This paper proposes a FACe Implicit Attribute Learning (FACIAL) framework for synthesizing dynamic talking faces. The framework jointly learns explicit and implicit attributes using adversarial learning. It also utilizes a FACIAL-GAN module to encode contextual, phonetic, and personalized information, and predicts eye blinks for realistic rendering. The contributions of this paper include the joint learning of explicit and implicit attributes, the design of the FACIAL-GAN module, and the incorporation of eye blinks in the rendering process. The proposed framework generates photo-realistic talking faces that outperform state-of-the-art methods.