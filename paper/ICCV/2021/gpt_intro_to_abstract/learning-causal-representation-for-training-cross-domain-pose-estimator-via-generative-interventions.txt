3D pose estimation has gained significant attention in recent years due to its applications in areas such as human-computer interaction, action recognition, and privacy preservation. Deep learning models have shown remarkable improvements in this field, thanks to advancements in model architecture, novel loss functions, and the availability of high-quality datasets. However, existing methods still struggle to generalize beyond the training data domain, particularly in cross-domain pose estimation where precise joint detection becomes challenging for unfamiliar subjects or views.This limitation can be attributed to dataset biases and shortcut learning, where deep learning models tend to learn dataset-dependent spurious correlations based on statistical associations. These correlations may not be consistent across domains, leading to a drop in the performance of trained models when faced with test samples that differ from the training dataset in terms of skin color or clothing appearance.Prior works have shown that generalization beyond the training domain requires models to learn not only statistical associations but also underlying causal relations. Causal relations reflect the fundamental data-generating mechanism that is universal and invariant across different domains, providing transferable and confident information to unseen domains.In this paper, we propose a novel method for learning causal representations, which is subsequently utilized to train a robust model for cross-domain pose estimation. Our method is based on the observation that the causal generative process of an image, assuming it is constructed from a content variable and a domain variable, is domain-invariant. Building on prior work, we consider domain variable changes as interventions on images, steering generative models to produce counterfactual features from specified content and random noise. By enforcing similarity between representations learned with different interventions, our model can learn transferable and causal relations across different domains.The main contributions of our work are:1. We propose a novel framework for causal representation learning, exploiting the causal structure of the task to generate out-of-distribution features through interventions on images.2. We demonstrate the effectiveness of our counterfactual feature generator by utilizing the generated features to enhance cross-domain pose estimation performance and generalize well to domain generalization settings.3. We conduct experiments on both human pose and hand pose estimation tasks, examining various components of our framework and the impact of different training datasets. We also discuss how increasing the source dataset and interventions can improve performance.Overall, our approach shows promise in addressing the limitations of existing methods in cross-domain pose estimation and advances the field in learning causal representations for improved generalization.