Humans rely on multiple sensory signals and language representations, along with visual inputs, to robustly learn visual events and concepts. Recent techniques in computer vision aim to mimic this paradigm by training models on multimodal video data. However, learning a joint embedding space across multiple modalities poses challenges because of the differences in the modalities and the weak supervision available. To address these challenges, this paper proposes the Multimodal Clustering Network (MCN), which combines the contrastive loss at the instance level and the semantic consistency at the cluster level. Unlike prior works, MCN creates joint clusters using multimodal representations, allowing for retrieval across different modalities. The proposed method is evaluated in the context of zero-shot learning and outperforms baseline models in text-to-video retrieval and temporal action localization tasks. The contributions of this work are the novel method that combines contrastive loss and clustering loss, the demonstration of learning across three modalities, and the significant performance gains in zero-shot settings.