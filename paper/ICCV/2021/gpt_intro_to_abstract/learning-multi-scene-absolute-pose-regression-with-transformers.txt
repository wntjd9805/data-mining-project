Abstract:Localizing a camera using a query image is a crucial task in various computer vision applications. Existing approaches for estimating camera position and orientation offer different trade-offs between accuracy, runtime, and memory. Hierarchical localization pipelines achieve state-of-the-art pose accuracy but are slow and require high memory usage. Absolute pose regressors (APRs) are faster but less accurate and typically only support single scenes. In this paper, we propose a novel formulation of multi-scene absolute pose regression using Transformers, inspired by recent successful applications in computer vision tasks. We employ encoders to focus on pose-informative features and decoders to transform scene identifiers to latent pose representations. Our approach achieves new state-of-the-art accuracy for both multi-scene and single-scene APRs across various benchmarks. We also demonstrate that our model is scalable and achieves competitive results across datasets with different characteristics. Our contributions include the proposal of a new formulation for multi-scene absolute pose regression using Transformers, experimental validation of the effectiveness of self-attention for cue aggregation, and achieving new state-of-the-art accuracy in camera localization.