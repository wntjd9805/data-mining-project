Predicting road user behavior in complex urban environments is crucial for assistive and intelligent driving systems. However, predicting pedestrian behavior can be challenging due to their diverse behaviors that are influenced by various contextual factors such as social interactions, road structure, traffic conditions, and environmental factors. In recent studies, it has been shown that both implicit prediction of future trajectories and explicit prediction of upcoming actions are important.To address these challenges, we propose a multitask learning framework that simultaneously predicts pedestrian trajectories, actions, and final locations. Our proposed method leverages multiple data modalities, including visual context, pedestrian motion, and ego-vehicle dynamics, to learn complex pedestrian behavior. The framework independently processes different input modalities and tasks, allowing each modality or task to learn its own parameters. Additionally, joint processing acts as a regularizer, encouraging the model to learn more representative features.Since pedestrians' behaviors are often influenced by their surroundings, we introduce a novel technique to model interactions between pedestrians and their surroundings based on the semantic composition of the scenes. This technique utilizes visuospatial semantic representations of the scenes divided into categories based on object classes.We evaluate the performance of our proposed method using public pedestrian behavior benchmark datasets, PIE and JAAD, and demonstrate significant improvements over state-of-the-art algorithms in both pedestrian trajectory and action prediction tasks. This work contributes to the advancement of assistive and intelligent driving systems by improving the prediction of pedestrian behavior in complex urban environments.