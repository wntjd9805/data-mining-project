The task of view synthesis, which involves recovering a 3D scene representation from observed images to render the scene from unobserved viewpoints, has seen significant advancements with the use of neural volumetric representations such as Neural Radiance Fields (NeRF). However, NeRF's rendering process is slow, making it impractical for interactive view synthesis. To address this issue, we propose a method called Sparse Neural Radiance Grid (SNeRG) that accelerates NeRF's rendering procedure. SNeRG "bakes" the continuous neural volumetric scene representation of NeRF into a sparse 3D voxel grid, achieving real-time rendering speeds. Our approach precomputes and stores the trained NeRF into the SNeRG, allowing for faster rendering times. We introduce two key modifications to NeRF to effectively integrate it into the sparse voxel representation. First, we design a "deferred" NeRF architecture that represents view-dependent effects with an MLP that runs once per pixel instead of once per 3D sample. Second, we regularize NeRF's predicted opacity field during training to encourage sparsity, improving storage cost and rendering time. Experimental results demonstrate that our approach significantly increases the rendering speed of NeRF while preserving its ability to represent fine geometric details and convincing view-dependent effects. Additionally, our representation is compact, requiring less than 90 MB on average to represent a scene.