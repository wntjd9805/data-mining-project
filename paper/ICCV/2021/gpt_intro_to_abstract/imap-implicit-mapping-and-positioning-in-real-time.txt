Real-time Simultaneous Localisation and Mapping (SLAM) systems for intelligent embodied devices require the incremental construction of a 3D world representation for localization and scene understanding. This representation should encode geometry accurately, be efficient in memory usage, able to estimate unobserved regions, and adaptable to new scenarios. Implicit neural representations, which employ a multilayer perceptron (MLP), have shown promise in off-line reconstruction. MLPs can approximate implicit functions effectively with few parameters and without quantization artifacts. In this paper, we demonstrate for the first time that MLPs can be used as the sole scene representation in a real-time SLAM system with a hand-held RGB-D camera. Our system, iMAP, utilizes a keyframe structure and multiprocessing computation flow similar to PTAM. It involves a tracking process that aligns live RGB-D observations with predictions from the MLP scene map, and a mapping process that selects and maintains keyframes and continuously trains and improves the MLP while optimizing the keyframe poses. We dynamically sample informative RGB-D pixels to reduce geometric uncertainty. Our system achieves efficient multi-scale mapping without a fixed resolution. We evaluate iMAP on various real-world sequences and compare its performance to standard dense SLAM systems using the Replica Dataset and the TUM RGB-D dataset. Our contributions include the first dense real-time SLAM system using an implicit neural scene representation, the ability to train the scene network in real-time, and a parallel implementation of our SLAM formulation using PyTorch and multiprocessing.