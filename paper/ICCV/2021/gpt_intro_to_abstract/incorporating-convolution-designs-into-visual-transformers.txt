This paper introduces a new visual Transformer architecture called Convolution-enhanced image Transformer (CeiT) that combines the strengths of convolutional neural networks (CNNs) in extracting low-level features and strengthening locality, with the advantages of Transformers in establishing long-range dependencies. The CeiT architecture addresses the limitations of previous Transformer-based models in the vision domain, such as their reliance on large datasets and trained CNN models. Three modifications are made to the vanilla Vision Transformer (ViT) architecture to enhance the extraction of low-level features and promote spatial correlation among neighboring tokens. Experimental results on ImageNet and downstream tasks demonstrate the effectiveness and generalization ability of CeiT compared to previous Transformers and state-of-the-art CNNs. CeiT models achieve comparable performance to ResNet-50 with a smaller model size and demonstrate faster convergence with fewer training iterations, reducing training costs significantly.