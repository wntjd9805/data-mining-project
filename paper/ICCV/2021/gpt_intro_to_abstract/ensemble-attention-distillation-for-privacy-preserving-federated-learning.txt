Modern deep learning algorithms rely on massive annotated datasets for many practical applications. However, this data is often physically dispersed across multiple locations and regulated by different entities, presenting challenges in terms of centralization, privacy, and network bandwidth. Federated learning (FL) has emerged as a solution, where a single centralized model is trained in a distributed, decentralized manner. FL faces unique challenges such as privacy preservation, efficient training of centralized models, and variations in data collection and preprocessing mechanisms among local nodes. Existing FL techniques use parameter-based communication methods or distillation-based techniques, but they suffer from drawbacks such as high network bandwidth overhead, limited applicability to homogeneous architectures, and security vulnerabilities. In this paper, we propose a new distillation-based federated learning framework that overcomes these challenges. We ensure stronger privacy guarantees by only using model outputs of unlabeled public data during distillation, without exchanging local model parameters or gradients. In addition, we leverage fully-trained local models to capture their structural knowledge, enabling the use of top-down class-specific attention maps and ensemble strategies to coordinate local expertise. Our framework is more efficient and flexible compared to prior art, and we demonstrate its efficacy through extensive experiments on CIFAR10/100 and large-scale chest x-ray datasets. We also show its applicability to other tasks through preliminary experiments on segmentation, achieving state-of-the-art privacy/performance trade-offs. Our contributions include the proposal of a one-shot federated learning framework with one-way distillation, addressing communication inefficiencies, introducing a distillation algorithm that aggregates structural knowledge and balances local model diversity and consensus, and demonstrating the extension of the framework to other applications.