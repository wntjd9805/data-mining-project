This paper introduces a neural rendering system that enables scene editing in real-world scenes. Current neural rendering methods struggle to support scene modifications such as moving or adding furniture. This paper proposes a top-down approach that learns a unified neural rendering model for the entire scene, respecting the object placement as in the captured scene. The authors also design a conditional neural rendering architecture that renders each object standalone, allowing for object manipulation. The challenge of learning object-compositional neural radiance field for clustered and real-world scenes with only rough 2D instance masks is addressed by incorporating a separate scene branch to provide biased sampling distribution and dense depth estimation. The proposed system demonstrates high-quality novel view rendering and object manipulation capabilities.