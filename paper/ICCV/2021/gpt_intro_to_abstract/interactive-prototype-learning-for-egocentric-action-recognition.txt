Egocentric videos have gained popularity on social media and have become an important focus in computer vision research. Datasets such as EGTEA, Charades-Ego, and EPIC-KITCHENS have contributed to the understanding of egocentric videos by focusing on person and object interactions. Unlike third-person videos, where actions occur at a distance, egocentric videos provide a closer look at these interactions. However, recognizing actions in egocentric videos poses several challenges, including the limited usefulness of scene context information and the difficulty of accurately classifying active objects. Previous methods have used object detectors or human gaze annotations to improve noun recognition, but these approaches have limitations in terms of computational cost and availability. In this paper, we propose an Interactive Prototype Learning (IPL) framework to enhance active object recognition in egocentric videos. By leveraging information learned from the actor's motion, IPL aims to improve the accuracy of noun classification. The IPL framework involves two interactive operations, noun-to-verb assignment and verb-to-noun selection, which extract location-aware spatio-temporal features for noun classification. IPL differs from previous approaches by learning verb prototypes with direct supervision from verb annotations, sharing prototypes between verb and noun classification tasks, and utilizing a verb-to-noun selection mechanism for identifying discriminative features from active objects.We conduct extensive experiments and ablation studies to evaluate the performance of IPL on three large-scale egocentric video datasets: EPIC-KITCHENS-100, EPIC-KITCHENS-55, and EGTEA. The results show that IPL outperforms existing methods and demonstrates robustness across different video backbones. The contributions of this work include proposing the use of the actor's motion to improve active object classification, designing the IPL framework for better information flow between verb and noun classification tasks, and achieving superior results on egocentric video datasets without the need for object detection or human gaze annotations.