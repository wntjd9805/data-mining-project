Dynamic vision sensors (DVS) represent visual information using sparse and asynchronous events, providing advantages such as low resource requirement, high temporal resolution, and high dynamic range. However, the redundant temporal dimension of event streams makes them difficult to process directly with deep neural networks (DNNs). Spiking neural networks (SNNs) offer event-triggered computation characteristics but face challenges in training and optimization. To address this, we propose temporal-wise attention SNNs (TA-SNNs) that incorporate an attention mechanism to filter out irrelevant frames. We introduce the TA module inspired by squeeze-and-excitation blocks to obtain statistical features of events and generate attention scores. We also propose a data augmentation method called random consecutive slice (RCS) to utilize event data while preserving its characteristics. Our approach achieves state-of-the-art results in tasks such as gesture recognition, image classification, and spoken digit recognition, demonstrating the effectiveness of TA-SNNs in low-latency, power-saving event processing. Our contributions include the introduction of temporal-wise attention into SNNs, the development of the input attention pruning (IAP) method for power-saving, and the utilization of the RCS data augmentation method.