Sign Language is a complete language that combines manual and non-manual components to convey information. Vision-based Continuous Sign Language Recognition (CSLR) aims to automatically recognize signs from image streams, bridging the communication gap between Deaf and hearing individuals. However, data collection and annotation for sign language are costly, leading to the use of weakly supervised approaches in CSLR. This paper proposes a non-iterative CSLR approach with a visual alignment constraint (VAC) to enhance the generalization ability of the feature extractor. The VAC consists of two auxiliary losses that provide extra supervision for the feature extractor. Experimental results on PHOENIX14 and CSL datasets show that the proposed method achieves competitive performance compared to previous approaches. Additionally, the paper introduces metrics to evaluate the contributions of the feature extractor and alignment module, demonstrating the effectiveness of the proposed method. The contributions of this paper include revisiting the iterative training scheme in CSLR, proposing the VAC to make the network end-to-end trainable, and presenting evaluation metrics for the feature extractor and alignment module.