Domain adaptation (DA) is crucial in situations where labeled target data is scarce but labeled source data is abundant. This paper focuses on semi-supervised domain adaptation (SSDA), which involves semi-supervised learning (SSL) in the target domain and unsupervised DA (UDA) across domains. The authors propose to decompose SSDA into two sub-tasks and learn two distinct classifiers: one for UDA and one for SSL. These classifiers are trained jointly using a co-training approach, where they exchange high-confidence predictions to improve each other's performance. The authors introduce DECOTA, a deep co-training framework for SSDA, and evaluate it on two benchmark datasets. DECOTA outperforms state-of-the-art methods and satisfies the expandability requirement of co-training. This work contributes by explicitly decomposing different sources of supervision in SSDA, presenting DECOTA as a simple deep learning-based approach, providing insights on why DECOTA works, and supporting the findings with strong empirical results.