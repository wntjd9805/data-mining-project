Learning 3D object representation from multi-view images is a challenging problem in computer animation and virtual reality. Recent advancements in deep learning, particularly convolutional neural network (CNN)-based approaches, have improved multi-view 3D reconstruction. These CNN-based methods typically follow a divide-and-conquer approach, where separate CNNs are used for single-view feature extraction and multi-view fusion. However, the relationship between these modules and the exploration of relations across different views are often overlooked. Additionally, CNN-based methods suffer from scaling issues, where the accuracy saturates when the number of input views exceeds a certain threshold. To address these challenges, we propose a new framework called "3D Volume Transformer" that utilizes the self-attention-based Transformer model for multi-view 3D object reconstruction. We reformulate the problem as a sequence-to-sequence prediction task and combine feature extraction and view fusion into a single Transformer network. Transformers, with their self-attention mechanism, are effective at learning complex semantic abstractions from an arbitrary number of input tokens. Our Transformer-based framework consists of a 2D-view Transformer encoder and a 3D-volume Transformer decoder, which jointly explore the relationships between 2D views and 3D volumes. We also address the "attention uniformity" problem in Transformers by proposing a divergence-enhanced Transformer that maintains expressive power at deeper layers. Our method achieves state-of-the-art results on ShapeNet with fewer parameters compared to CNN-based methods, and exhibits better scalability with the number of input views.