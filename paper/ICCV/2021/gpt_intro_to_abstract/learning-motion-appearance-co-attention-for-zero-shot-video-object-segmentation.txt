Zero-shot Video Object Segmentation (ZVOS) is a valuable task in computer vision, as it aims to automatically separate primary objects from the background in video sequences without any human intervention. This has wide-ranging applications in various fields, such as video compression, visual tracking, and person re-identification. However, accurately distinguishing target objects from complex and diverse backgrounds without any prior knowledge remains a challenge in ZVOS. Previous methods have attempted to address this issue through multi-modality interaction schemes that leverage external object motion information. However, these methods have limitations, including the inclusion of redundant and invalid information that affects segmentation accuracy.In this paper, we propose an Attentive Multi-Modality Collaboration Network (AMC-Net) for zero-shot video object segmentation. Our approach incorporates a co-attention mechanism to effectively fuse robust spatio-temporal representations from multi-modality features. In the first stage, we introduce a Multi-Modality Co-Attention Gate (MCG) to unify appearance and motion information, with a gate function used to predict co-attention scores and balance the contributions of multi-modality features. In the second stage, we propose a Motion Correction Module (MCM) that incorporates visual-motion attention to emphasize foreground object features through spatio-temporal correspondence between appearance and motion cues.To evaluate the effectiveness of our proposed model, we conduct experiments on three benchmark datasets: DAVIS-16, Youtube-Objects, and FBMS. Our results demonstrate that our method outperforms state-of-the-art approaches, even with training solely on the DAVIS-16 dataset. Contributions of our work include the development of an Attentive Multi-Modality Collaboration Network, a Multi-Modality Co-Attention Gate for unifying multi-modality information, and achieving superior performance on challenging benchmark datasets.