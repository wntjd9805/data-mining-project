The introduction discusses the recent advancements in transformer architecture for sequence-to-sequence modeling in natural language processing (NLP) tasks. It highlights the interest in understanding whether transformers can compete with Convolutional Neural Network (CNN)-based architectures in vision tasks. Previous research has focused on combining CNNs with self-attention, but these hybrid approaches have limited scalability compared to attention-based transformers. The Vision Transformer (ViT) is the first convolution-free transformer that performs well in vision tasks. However, it requires large datasets for training. The paper proposes a dual-branch transformer for image recognition that combines image patches of different sizes to generate stronger visual features. The main focus of the work is on developing appropriate feature fusion methods for vision transformers, achieved through an efficient cross-attention module. The proposed approach outperforms previous methods and demonstrates comparable results with other existing models. The main contributions of the paper are the dual-branch vision transformer and the token fusion scheme based on cross-attention. Overall, the paper aims to explore multi-scale feature representations in transformer models for image classification, showcasing improvements over existing approaches.