Recovering human mesh from monocular images is an important goal in the field of computer vision with various applications in fields such as robotics and augmented reality. Existing methods for human mesh recovery can be categorized into regression-based and optimization-based methods, both of which require either 3D annotations or optimized modules. In this paper, we propose a novel approach called Skeleton2Mesh which aims to recover human mesh from a single image without the need for 3D annotations, optimized modules, or temporal information. Our method decouples the problem into unsupervised 3D pose estimation and unsupervised human mesh recovery, focusing on the latter. The challenges in human mesh recovery include pose mismatching and pose ambiguity, as well as shape ambiguity. To address these challenges, our method consists of three modules: a differentiable inverse kinematics module, a pose refinement module, and a shape refinement module. These modules allow us to infer 3D rotations from estimated 3D skeletons, refine poses, and alleviate shape ambiguity using silhouettes. Our approach outperforms state-of-the-art unsupervised methods on benchmark datasets, demonstrating its effectiveness in recovering human mesh from monocular images.