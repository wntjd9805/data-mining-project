This paper explores the problem of domain generalization in deep learning models. The goal is to develop models that can perform well on data from domains different from those seen during training. The authors discuss the challenges of domain generalization, including the lack of access to unlabeled data from the target domain. They also present different strategies that have been proposed to address this problem, including domain-specific models, domain-invariant representations, and data augmentation. However, the gains in performance obtained by current domain generalization techniques are still modest. In this work, the authors focus on understanding the implications of multi-domain gradient interference in domain generalization. They propose a hypothesis that multiple domains give rise to conflicting gradients associated with different domains. To address this issue, they introduce a novel gradient agreement strategy based on gradient surgery, which aims to distill domain-invariant information by encouraging gradient agreement among the source domains. The authors evaluate the effectiveness of their approach in image classification tasks using three multi-domain datasets, demonstrating that their agreement strategy enhances the generalization capacity of deep learning models under domain shift conditions.