Floorplans of complex 3D environments are widely used for visualization, navigation, safety planning, and architectural design. Traditional methods for creating floorplans are limited to mapping only the regions directly observed. In this paper, we propose a new approach called audio-visual floorplan reconstruction, where sounds in the environment are used to infer both the geometric properties and semantic labels of unobserved rooms. By leveraging the spatial and semantic signals provided by audio, we extend the floorplan estimation beyond the camera's field of view. We introduce the AV-Map approach, which utilizes deep learning techniques to analyze sequences of audio and visual data and extract floorplan-aligned features. We consider two settings: active, where the camera emits a known sound during movement, and passive, where only naturally occurring sounds are observed. Our experimental results demonstrate that AV-Map outperforms traditional vision-based mapping and improves the state-of-the-art approach for extrapolating occupancy maps. Despite observing only a fraction of the full environment, our model achieves accurate interior maps and enables high-level perception of the space's semantics. This research represents the first attempt to infer floorplans from audio-visual data.