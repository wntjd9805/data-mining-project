Deep neural networks (DNNs) have achieved remarkable progress in various fields such as computer vision, speech recognition, and natural language processing. However, their vulnerability to adversarial examples (AEs) has become a significant concern. AEs are data with carefully designed imperceptible perturbations added, which can pose a threat to the safety and security of DNNs in real-world applications. Adversarial training-based models have been widely used to defend against adversarial attacks and improve adversarial robustness. These models solve a min-max optimization problem, with the inner problem finding the strongest AE by maximizing the loss function, and the outer problem minimizing the classification loss of the AE. Despite the effectiveness of adversarial training, these models still suffer from relatively poor generalization on both clean and adversarial examples. In this paper, we propose an Attention Guided Knowledge Distillation (AGKD) module to improve the model's adversarial robustness. The AGKD module transfers attention knowledge from a pre-trained teacher model on clean images to an on-training student model. This transfer of attention information corrects the intermediate features corrupted by AEs. Additionally, we introduce a Bi-directional attack Metric Learning (BML) method that efficiently constrains the representations of different classes in the feature space. By integrating AGKD and BML, our proposed AGKD-BML model outperforms state-of-the-art approaches on CIFAR-10 and SVHN datasets under different attacks. Our contributions include the introduction of the AGKD module to transfer attention knowledge and correct corrupted features, the proposal of the BML method to enhance constraint efficiency in the feature space, and thorough experimental evaluations demonstrating the superiority of our AGKD-BML model in terms of both qualitative visualization and quantitative evidence.