This paper introduces the application of dialog summarization to video question answering (VideoQA) for the first time. Previous VideoQA systems have focused on multimodal embeddings/description, temporal attention and localization, multimodal fusion, and reasoning. However, these systems often rely on external knowledge and are limited by biases introduced by datasets. To address these limitations, the authors propose using dialog summaries as input to the VideoQA system, eliminating the need for external knowledge. The authors find that their dialog summaries outperform human-generated summaries in handling questions that require knowledge of the entire story. The contributions of this paper include the application of dialog summarization to VideoQA, converting all input sources into text descriptions, introducing a weakly-supervised soft temporal attention mechanism for localization, devising a simple multimodal fusion mechanism, and achieving state-of-the-art performance on the KnowIT VQA dataset.