In this paper, we address the challenge of identifying meaningful image transformations in the latent space of Generative Adversarial Networks (GANs) through the use of natural language descriptions. We propose a method for constructing a diverse and expressive vocabulary of visual concepts by decomposing freeform language descriptions of GAN transformations. Previous methods for identifying these transformations have been limited in capturing only a subset of human-salient dimensions of variation. To illustrate the need for our approach, we consider the task of finding a direction that makes an outdoor market more festive. Random search in the large latent space of GANs is not feasible, and supervised approaches cannot verify if the desired direction exists. Additionally, unsupervised approaches may not discover the desired direction as they may not capture changes that are visually salient to humans.To build a comprehensive glossary of primitive, perceptually salient directions in the GAN latent space, we propose a three-component approach. Firstly, we introduce a procedure based on layer selectivity to generate perceptually salient directions that make meaningful local changes to a scene. Secondly, we collect data using a paradigm where human annotators directly label directions with their semantics, allowing complex descriptions of visual transformations. Finally, we present a bag-of-directions model that automatically decomposes these annotations into a glossary of "primitive" visual transformations associated with single words.Our method covers the entire breadth of the GAN latent space, enabling reliable image editing with an open-ended vocabulary. We demonstrate the generalization of our vocabulary to novel compositions and transfer across classes. The code, data, and additional information are publicly available at visualvocab.csail.mit.edu.