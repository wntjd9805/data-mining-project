Text recognition has been a popular research area for decades, with numerous commercial applications such as translation apps and assistive technology for the visually impaired. Recent advancements in deep learning and sequence-to-sequence learning frameworks have improved word recognition accuracy. However, state-of-the-art text recognition systems still struggle in wild scenarios due to complex backgrounds and other challenges. Humans, on the other hand, excel at recognizing text in these scenarios by utilizing joint visual-semantic reasoning. This paper aims to develop a visual-semantic reasoning skill for text recognition by proposing a novel multi-stage prediction paradigm. The first stage predicts using visual cues, while subsequent stages refine the predictions using joint visual-semantic information. The paper also introduces a multi-scale attention decoder and utilizes Gumbel-Softmax operation to make the embedding layer differentiable. Experimental results show that the proposed framework outperforms other state-of-the-art methods on benchmark datasets, demonstrating the effectiveness of the multi-stage approach for text recognition.