Deep learning has revolutionized image understanding tasks, but progress in video understanding has lagged behind. Video understanding presents unique challenges due to the need to analyze interactions between actors, objects, and the context in a scene over long temporal intervals. The high dimensionality of video signals makes it difficult to learn these interactions directly from datasets with convolutional networks. To address this, we propose a structured graph neural network that models spatio-temporal interactions explicitly. Our model represents actors and objects as nodes in a graph and uses message passing inference to capture their relations. Existing graph-structured models for action recognition lack a unifying formulation and only focus on either spatial or temporal relations or require additional supervision. Our graph network formulation allows us to explicitly model scene interactions over time and interpret different design choices. We demonstrate the versatility of our model on two tasks: spatio-temporal action detection and video scene graph prediction, achieving state-of-the-art results. Our model shows significant improvements on action classes involving human-to-human and human-to-object interactions, highlighting its relevance for action classification.