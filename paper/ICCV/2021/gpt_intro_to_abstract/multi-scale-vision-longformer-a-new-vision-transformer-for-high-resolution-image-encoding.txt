The Vision Transformer (ViT) has shown promising results in image classification tasks, but its computational and memory complexity limits its application on tasks requiring high-resolution feature maps. This paper proposes a new vision Transformer architecture called Multi-Scale Vision Longformer that enhances ViT for encoding high-resolution images. It achieves this through a multi-scale model structure and the attention mechanism of Vision Longformer. The multi-scale structure allows for encoding images at multiple scales, improving computation and memory efficiency while boosting classification performance. The proposed architecture has the same structure as conventional CNN models and can be used as a replacement in various applications. However, scaling up ViT for high-resolution images and feature maps poses challenges due to the quadratic increase in computation and memory complexity. To address this, a 2-D version of Longformer, called Vision Longformer, is developed, achieving linear complexity. The experiments show that Vision Longformer significantly reduces computational and memory costs without performance drops. Furthermore, it outperforms previous ViT models, their ResNet counterparts, and ViTs with other efficient attention mechanisms in image classification, object detection, and segmentation tasks. The main contributions are the proposed vision Transformer architecture and the empirical study demonstrating its superiority.