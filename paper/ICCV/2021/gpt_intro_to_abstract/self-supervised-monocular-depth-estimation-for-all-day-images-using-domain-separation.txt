Self-supervised depth estimation has been widely used in various fields including augmented reality, 3D reconstruction, SLAM, and scene understanding. This approach does not rely on large and accurate ground-truth depth labels, but instead estimates depth using spatial and temporal consistency in image sequences. Deep Convolution Neural Network (DCNN) based approaches have shown impressive results compared to traditional methods that use handcrafted features and camera geometry/motion. However, most current DCNN based self-supervised depth estimation approaches only focus on day-time images and fail to generalize well to night-time images due to the domain shift and low visibility in night-time conditions. Some methods have been proposed to estimate depth in low-light conditions, but their performance is limited due to unstable visibility. Generative adversarial networks (GANs) have also been used to address the domain shift between day and night-time images, but obtaining natural day-time images/features from night-time inputs is difficult. In this paper, we propose a domain-separated network for self-supervised depth estimation of all-day RGB images. The network separates day and night image pairs into two subspaces: private and invariant domains. The private domain contains information that disturbs depth estimation (e.g. illumination), while the invariant domain contains information that is useful for depth estimation (e.g. texture). Our network utilizes DCNN and orthogonality and similarity losses to extract private and invariant features, which are then used to estimate depth maps. Reconstruction and photometric losses are employed to refine the depth maps. Experimental results show that our framework achieves state-of-the-art depth estimation performance for all-day images.