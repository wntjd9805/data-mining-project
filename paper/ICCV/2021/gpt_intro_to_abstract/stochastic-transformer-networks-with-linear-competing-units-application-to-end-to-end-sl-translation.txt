The aim of this paper is to improve the translation quality of Sign Language (SL) videos to written language by combining Sign Language Recognition (SLR) with SL translation (SLT). Traditional SLR methods focus on recognizing sequences of sign glosses, which are minimal language words that encode the meaning of SL signs. However, these glosses have limited usefulness in real-world applications. To address this limitation, recent researchers have proposed methods that combine SLR with SLT. This paper proposes a novel formulation of Transformer networks, which are state-of-the-art models for sequential data modeling, to achieve improved SL translation accuracy. The proposed method does not rely on gloss sequence groundtruth information and reduces memory requirements at inference time. Experimental evaluation on the PHOENIX 2014T dataset demonstrates comparable or better results compared to the state-of-the-art, while also reducing memory usage.