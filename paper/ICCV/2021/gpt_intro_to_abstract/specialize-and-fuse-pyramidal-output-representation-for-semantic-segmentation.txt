Semantic segmentation is a crucial task in computer vision that aims to assign semantic labels to each pixel in an RGB image. Recent advancements in deep neural networks have significantly improved the performance of semantic segmentation methods. However, most existing approaches focus on designing better feature representations in the networks, neglecting the potential of leveraging the structure in the output representation.This paper proposes a novel approach that exploits the spatial distribution of classes in the output representation to dynamically predict semantic labels at a coarser level. The authors observe that a large portion of pixels in most images share the same label at a coarse spatial level. To address this, they introduce a pyramidal output representation, consisting of two types of outputs: unity pyramid and semantic pyramid. The unity pyramid identifies whether a group of pixels shares the same label, while the semantic pyramid contains semantic labels at multiple levels. These outputs are then fused together to generate the final semantic output.To further improve the predictions, a coarse-to-fine contextual module is designed to aggregate the features representation from different levels. This module enhances the contextual information and helps refine the semantic pyramid predictions.The main contributions of this paper are: 1) Introducing a pyramidal output representation and a specialized fusion process that allows each level to specialize in different class distributions and ensures parsimony.2) Designing a coarse-to-fine contextual module to improve the feature representations from different levels.3) Demonstrating the effectiveness of the proposed method on benchmark datasets including ADE20K, COCO-Stuff, and Pascal-Context. The results obtained using HRNet and ResNet backbones show competitive performance compared to state-of-the-art methods.