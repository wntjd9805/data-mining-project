Object Detection on Demand (ODOD) is a challenging task in applications such as robotics exploration and autonomous driving, where systems need to detect objects of novel classes in new environments with minimal human interaction. Traditional few-shot detection methods require extra training on base class data, which is unfeasible in ODOD due to limited resources and the inability to perform back-propagation on embedded systems. To address this problem, we propose a novel morphable detector (MD) that can detect novel classes without extra training. The MD has two sets of parameters for feature embedding and class representation, and uses prototypes to represent each class in a common feature space. To morph the MD for novel classes, a few samples are used to compute the prototype and update the network parameters in an EM-like approach. By integrating visual and semantic information, the MD achieves better generalizability to novel classes. We compare the MD with existing methods and demonstrate its superiority through extensive experiments on different datasets. Our contributions include studying the ODOD task, proposing the MD for online morphing without extra training, introducing joint visual and semantic embedding in an EM-like approach, and providing experimental evidence of the MD's effectiveness.