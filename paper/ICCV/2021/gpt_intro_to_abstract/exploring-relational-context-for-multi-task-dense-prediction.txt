The role of context in computer vision is critical for breakthroughs in extraction, learning, and utilization of contextual representations. Convolutional Neural Networks (CNN) excel in capturing local context and building contextual hierarchies. Attention mechanisms have replaced traditional convolutional building blocks, allowing for flexible context descriptions. Multi-Task Learning (MTL) focuses on sharing representations between tasks, and recent works have extended context extraction across tasks through soft-gated message passing. However, current multi-modal distillation schemes have limitations in capturing task interactions and restricted local message passing. This paper proposes an attention-driven multi-modal distillation scheme that addresses these limitations. Three key contributions are highlighted: increasing the expressivity of cross-task gates by conditioning on source and target task interdependence, enabling global cross-task message passing by enlarging the receptive field, and customizing the distillation context type for each source-target task pair. The paper introduces the Adaptive Task-Relational Context (ATRC) module, which can be used as a drop-in module for CNNs. The proposed method is empirically shown to achieve state-of-the-art performance on important benchmarks.