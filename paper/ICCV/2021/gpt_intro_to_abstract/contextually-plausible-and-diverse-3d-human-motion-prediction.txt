Human motion prediction is a crucial task in computer science, with applications in various fields such as autonomous driving and animation generation. Prior work in this area has predominantly used recurrent encoder-decoder architectures, which generate a single future trajectory based on past observations. However, these methods fail to capture the stochastic nature of human motion, as they are based on deterministic network operations and training datasets. This paper introduces a new approach called LCP-VAE, which models the stochasticity of human motion by learning multiple modes of motion. The focus is on generating diverse and contextually plausible motion predictions, where context refers to the natural continuation of an observed sequence of poses. The authors argue that existing approaches, such as conditional variational autoencoders (CVAEs), are not suitable for capturing diversity and context in human motion prediction. This is because CVAEs struggle to generate diverse motions when the conditioning signal is highly informative, and their use of a general prior on the latent variable hinders the encoding of contextual information. To address these limitations, the proposed LCP-VAE explicitly conditions the sampling of the latent variable on the past observation, ensuring that relevant information is encoded. Experimental results demonstrate that LCP-VAE generates higher-quality diverse motions compared to state-of-the-art methods, while also preserving contextual and semantic information without explicit exploitation. Overall, this work significantly advances the field of human motion prediction by incorporating stochasticity and improving the generation of diverse and contextually plausible motions.