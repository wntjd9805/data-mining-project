In computer vision, the simultaneous estimation of camera motion and scene geometry is a fundamental research topic. Traditional methods for solving this problem include feature-based approaches and direct methods that minimize photometric inconsistency. Recent work has introduced deep neural networks for joint training of depth and motion networks using a self-supervisory signal. Several self-supervised depth and motion learning frameworks have been proposed, incorporating additional signals such as geometric consistency, optical flow, segmentation maps, and edge and normal maps. However, these methods assume static scenes or require masking out moving objects, making them unsuitable for dynamic scenes. In recent years, there has been growing interest in learning the motion of objects together with the camera's ego-motion and depth for dynamic scene understanding. Two main approaches have emerged, stereo-based techniques that leverage depth information from sensor and monocular-based techniques that rely on instance segmentation labels. The latter approach is limited by the need for expensive human-labeled data, reducing the practicality of self-supervised depth and motion prediction frameworks. To address these challenges, we propose a novel self-supervised learning framework that estimates depth, camera motion, and object motion fields using weak semantic priors in the form of 2D object bounding boxes. Our approach offers versatility and reduces data dependency by leveraging cheaper data labels. We introduce a dynamics attention module to dynamically train motion features by disentangling dynamic objects and static backgrounds. Additionally, we propose a contrastive sample consensus method to refine object motion field learning, improving the distinction between object motion boundaries. Our proposed framework achieves favorable results in motion segmentation, monocular depth and scene flow estimation, and visual odometry on benchmark datasets.