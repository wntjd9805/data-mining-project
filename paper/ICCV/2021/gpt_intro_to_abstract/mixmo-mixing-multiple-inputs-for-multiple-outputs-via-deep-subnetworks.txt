Convolutional Neural Networks (CNNs) have shown exceptional performance in computer vision tasks, but obtaining reliable predictions remains challenging. To improve robustness, CNNs are often paired with data augmentation and ensembling strategies. Data augmentation helps in reducing overfitting by diversifying training samples, while ensembling aggregating predictions from diverse neural networks improves generalization and uncertainty estimation. However, ensembling is costly in terms of time and memory. In this paper, we propose MixMo, a new generalized multi-input multi-output framework that trains a base network with multiple inputs and outputs. By leveraging the available neurons and over-parameterization, we aim to prevent homogenization and enforce diversity among subnetworks. We introduce a mixing block that ensures feature independence and diversity, implementing a new variant called Cut-MixMo that incorporates the CutMix method. Our experimental results on standard datasets demonstrate excellent accuracy and uncertainty estimation, outperforming other approaches at the same inference cost as a single network. Overall, our contributions include the proposed MixMo framework, the identification of an appropriate mixing block, and the demonstration of excellent performance on various computer vision tasks.