Despite decades of research, imaging through atmospheric turbulence remains a challenge in optics and image processing. A major obstacle is the lack of a physically justifiable approach to generate large-scale datasets for training and testing image reconstruction algorithms. In this paper, we propose a new method for generating turbulence-distorted images with verifiable statistics. Our simulator combines optics/signal processing steps with a lightweight shallow neural network to perform a Phase-to-Space (P2S) transform. By parallelizing the computation, our simulator achieves a 1000Ã— speed-up compared to existing approaches while preserving essential turbulence statistics. Furthermore, when using our simulator to synthesize training data, the resulting deep neural network image reconstruction model outperforms architectures trained with data from less sophisticated simulators. We present the details of our simulator, including approximating the spatially varying convolution by invariant convolutions, learning the basis representation via known turbulence statistics, and implementing the Phase-to-Space transform network. Our simulator offers a promising solution that combines physics-based principles with computational efficiency for turbulence-distorted image reconstruction.