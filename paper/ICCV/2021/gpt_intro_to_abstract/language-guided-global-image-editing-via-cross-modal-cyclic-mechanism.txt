This paper addresses the task of global image editing through linguistic requests. With the popularity of social media platforms like Instagram and Facebook, users prefer to edit their photos before sharing them. However, using professional software like Photoshop can be difficult and time-consuming, especially for novice users. To make image editing more user-friendly, the paper explores the possibility of automatically editing images using voice commands, similar to Siri or Cortana.The focus of the paper is on retouching images by adjusting various attributes like brightness, hue, saturation, contrast, and tint. While there are existing methods for text-guided image manipulation, they are limited to domain-specific datasets and template inputs. The proposed method aims to edit images from real-world environments with various objects and scenes, using linguistic requests that describe the editing process.To achieve language-guided global image editing, the paper proposes the Cycle Augmentation GAN (CAGAN) approach. It introduces a cross-modal cyclic mechanism and data augmentation strategy to address the issue of insufficient and imbalanced data. The mechanism involves an Editing Description Network (EDNet) that generates editing embeddings to specify the image transformations. By swapping input and target images and applying random augmentations, the EDNet learns to reconstruct the input image without relying on linguistic requests. The similarity between the editing embeddings obtained from the EDNet and the condition embedding obtained from requests helps improve the performance of the generator.Additionally, the paper introduces the Image-Request Attention (IRA) mechanism to adaptively edit different spatial locations of the input image. This mechanism calculates the attention between linguistic request embeddings and visual feature maps, allowing for different editing degrees in different areas. This ensures more reasonable and interpretable editing results.To evaluate the performance of language-guided image editing, the paper proposes a new metric called Redescription Similarity Score (RSS). This metric utilizes a pre-trained speaker model to generate requests for the input and generated images, and then calculates sentence similarity metrics to measure the quality of the generated requests.Experiments conducted on the GIER and MA5k-Req datasets demonstrate the effectiveness of the proposed method. Overall, the paper contributes a novel approach to language-guided global image editing, addressing data imbalance issues, spatial adaptivity, and introducing a new evaluation metric.