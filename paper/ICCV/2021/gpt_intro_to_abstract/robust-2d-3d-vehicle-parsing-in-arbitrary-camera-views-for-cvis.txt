Cooperative vehicle-infrastructure systems (CVIS) have garnered significant attention and research in the field of autonomous driving. In CVIS, various sensors are mounted on vehicles and street-light poles to provide simultaneous perception of vehicles and road terminals, enabling early warning for potential collisions. However, the challenge lies in effectively detecting vehicles in novel views, as commonly used autonomous driving datasets only provide labeled front-view data. To address this data scarcity issue, we propose augmenting existing datasets through novel view synthesis. Existing view synthesis methods have limitations in the context of CVIS, such as the difficulty in obtaining ground-truth annotations and reliance on multi-view or paired images for training. To overcome these limitations, we introduce a new approach for data augmentation using part-based texture inpainting and a robust 2D/3D vehicle parsing method. Our approach includes the construction of a CVIS-oriented dataset with different camera views, which contains extensive annotations for benchmarking. Our contributions include a novel data augmentation pipeline, a self-supervised part-based texture inpainting network for novel view synthesis, dense correspondences between image pixels and 3D points for robust vehicle parsing, and the construction of a real-world CVIS dataset for evaluation. Comparative experiments demonstrate the effectiveness and accuracy improvements of our approach in vehicle parsing for CVIS.