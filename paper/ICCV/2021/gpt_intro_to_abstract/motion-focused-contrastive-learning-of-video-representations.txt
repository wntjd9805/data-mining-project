In this paper, we explore the importance of motion information in self-supervised video representation learning. We investigate this question from two perspectives: leveraging motion information for data augmentation and incorporating motion into feature learning optimization. We propose a novel method called Motion-focused Contrastive Learning (MCL) that utilizes spatio-temporal motion maps and tubelets as augmentations. MCL measures clip-level motion and selects clips with significant motion for temporal augmentation. It then estimates motion from a spatial viewpoint and localizes spatial patches as tubelets. In feature learning, MCL aligns the gradient maps of convolutional layers with motion maps through back-propagation. These alignments are integrated into the contrastive learning framework as additional constraints. Our experiments demonstrate that MCL outperforms ImageNet supervised pre-training on video benchmarks and validates its effectiveness on downstream video tasks. Overall, our work contributes to a deeper understanding of how to effectively incorporate motion information in self-supervised video representation learning.