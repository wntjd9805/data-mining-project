This paper introduces the concept of studying visual abilities in the context of embodied artificial intelligence (AI). While previous works have made advancements in computer vision systems for understanding web data, this paper focuses on the challenges of visual ability in real-world environments. The paper identifies two main challenges: "broader" to "deeper" visual understanding and static to dynamic visual understanding. The paper highlights the lack of research in studying these visual abilities in an embodied AI setting and proposes a question-answering task as a proxy for studying dynamic environments. To support this task, the authors construct a large-scale dataset called Env-QA, containing videos and questions. To control sample distribution and address biases, the authors utilize a virtual simulator to generate videos. The authors also propose a method called Temporal Segmentation and Event Attention (TSEA) network to represent the video at the level of events and perform temporal reasoning for answering questions. Experimental results demonstrate the effectiveness of the proposed method and highlight the challenges of capturing long-term state changes and multi-event temporal reasoning.