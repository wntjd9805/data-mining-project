The paper proposes a new approach to point cloud processing by combining Convolutional Neural Networks (ConvNets) and Transformers. The authors introduce a new building block called the cloud transform block that takes unordered sets of vectors as input and processes them using multiple parallel heads, similar to self-attention layers in transformers. The cloud transform block performs rasterization of the computed keys onto a regular grid and uses convolution to propagate information across elements. Multiple cloud transform blocks can be stacked together and trained end-to-end. The authors also design cloud transformer architectures for semantic segmentation, classification, point cloud inpainting, and image-based reconstruction. The proposed architectures achieve state-of-the-art performance on benchmark tasks. The key contributions of the paper include a new approach to point cloud processing, the use of multi-head self-attention, and the design of versatile architectures for different point cloud tasks.