Reconstructing high-resolution (HR) images from lower-resolution (LR) ones is a challenging problem with various approaches and objectives. Single LR image reconstruction requires strong priors, while data-driven methods using convolutional neural networks (CNNs) have been effective for natural images. Generative adversarial networks (GANs) can synthesize impressive HR images, but may contain hallucinated details. In the true super-resolution setting, where multiple LR frames are available, technical challenges such as sub-pixel registration arise but offer potential applications in various domains. Super-resolution in videos has also been explored, but the complexity of the digital video pipeline makes inversion difficult. However, with the availability of raw image bursts from modern cameras, there is an opportunity to restore frames before irreversible damage from image signal processing. This paper proposes a new approach that combines classical inverse problem formulations with data-driven learning, addressing challenges such as unknown motions, demosaicking, and effective image priors. The proposed approach utilizes a model-based optimization procedure and a plug-and-play prior that integrates deep neural networks with variational approaches. By unrolling the optimization procedure, the model parameters can be learned end-to-end using training data with synthetic motions. The paper demonstrates that aliasing, typically considered a nuisance, can actually be beneficial for super-resolution, and achieves state-of-the-art results on standard benchmarks using synthetic motion and qualitative results on real data from smartphones and prosumer cameras. The contributions of the paper include the proposal of the first model-based architecture that can learn end-to-end for joint image alignment and super-resolution from raw image bursts, the introduction of a differentiable image registration module, and the demonstration of excellent results on both real and synthetic image bursts.