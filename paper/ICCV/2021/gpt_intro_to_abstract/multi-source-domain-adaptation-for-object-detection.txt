In the past decade, object detection in computer vision has gained significant attention, with the development of convolutional neural networks (CNNs) leading to the emergence of successful CNN-based detectors like Faster R-CNN. However, these detectors require large-scale training images with annotated bounding boxes, and the performance on test images may decrease due to domain discrepancies between training and test images. Unsupervised domain adaptation (UDA) has been used to mitigate this domain gap in object detection, but existing algorithms assume that the source data comes from a single domain, limiting their generalization. In this paper, we propose a framework called Divide-and-Merge Spindle Network (DMSN) for domain adaptive object detection from multiple sources. DMSN includes multiple source subnets and a pseudo subnet, and employs hierarchical feature alignment, pseudo subnet learning algorithm, and consistency regularization on region proposals to improve detection performance. Our contributions include the first work on multi-source domain adaptation for object detection, a unified framework for the problem, and the design of a new consistency regularization technique. Experimental results demonstrate the effectiveness of our proposed framework.