This paper focuses on the task of vision-and-language navigation (VLN), where an agent is asked to navigate in home environments based on natural language instructions. The manual collection and annotation of VLN training data at scale is challenging, leading to limited performance of current methods, particularly in previously unseen environments. This paper is motivated by advancements in vision and language pretraining, which have shown the effectiveness of self-supervised proxy tasks using large-scale image-text datasets. The authors propose a new approach to pretraining a generic VLN agent by collecting a large-scale in-domain dataset from online marketplaces and transforming static image-caption pairs into visual paths and navigation-like instructions. They also introduce a shuffling loss to improve temporal reasoning abilities. The pretrained model, called Airbert, achieves state-of-the-art performance in VLN tasks and demonstrates strong generalization abilities in one/few-shot evaluations. The contributions of this work include the collection of the BnB dataset, the reduction of distribution shift between pretraining and VLN, and the improved temporal reasoning abilities of the model. Overall, this paper presents a promising approach for pretraining agents in VLN tasks and opens up new possibilities for future research in this area.