Interpretability is an important aspect of machine learning, particularly in the field of deep learning models such as convolutional neural networks (CNNs). While there have been many methods proposed to interpret the decision logic of a CNN on individual images or small groups of images, the challenge of interpretation for image classification on CNNs is still unresolved. Interpretations that rely on single images or small groups are sensitive to noise and cannot be robustly applied to large groups of unseen images. To address this challenge, we propose seeking representative interpretations that capture the common decision logic of a CNN. Representative interpretations are more convincing and can be reused to interpret the predictions on a large number of unseen images. Additionally, representative interpretations provide deeper insights into the common semantics of similar images, which are believed to be captured by CNNs for achieving high prediction performance. However, interpreting the common decision logic of a CNN on a large group of similar images in an unsupervised manner is a novel problem that has not been extensively studied. Existing interpretation methods focused on single images or small groups are not suitable for producing representative interpretations. Moreover, it is difficult to obtain representative interpretations without access to a large group of similar images predicted by a common decision logic of a CNN. This paper proposes an unsupervised task of finding and visualizing the common decision logic of a CNN on an input image and a large set of similar images. The authors present a series of technical contributions, including:- Modeling the common decision logic of a CNN as a decision region formed by pieces of the decision boundaries of the CNN.- Formulating the task as a co-clustering problem to simultaneously find the largest group of similar images and the corresponding decision region.- Developing an efficient heuristic method to solve the NP-hard submodular cost submodular cover problem without requiring conceptual image annotations or modifications to CNNs.- Visualizing the boundaries of the decision region as heat maps to identify important image regions for predictions.- Ranking similar images based on their semantic distances to the input image.- Conducting extensive experiments to evaluate the quality and reusability of representative interpretations.