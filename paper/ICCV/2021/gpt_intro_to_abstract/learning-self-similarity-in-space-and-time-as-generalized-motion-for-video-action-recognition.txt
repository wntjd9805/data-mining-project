Learning spatio-temporal dynamics is crucial for video understanding. While there has been considerable research on extending standard spatial and temporal convolution for this purpose, it has been found that spatio-temporal convolution alone often learns irrelevant context bias instead of motion information. Therefore, the use of optical flow in addition to spatio-temporal convolution has been shown to improve performance. To address this issue, we propose a novel approach called spatio-temporal self-similarity (STSS) representation learning. STSS represents each position in a video by its similarities to its neighbors in space and time, allowing for a comprehensive view of motion. Unlike existing methods that extract explicit motion information, our method focuses on learning a richer and more robust form of motion representation for videos in the wild. We introduce a neural block called SELFY for STSS representation, which can be easily integrated into existing neural architectures and learned end-to-end without any additional supervision. Our experimental analysis demonstrates the superiority of SELFY over previous methods for motion modeling, as well as its complementarity to spatio-temporal features from direct convolutions. The proposed method achieves state-of-the-art results on benchmark datasets for action recognition, including Something-Something V1&V2, Diving-48, and FineGym.