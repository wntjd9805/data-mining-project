This paper introduces a new training strategy for semantic segmentation models that reduces the need for large amounts of pixel-level annotations and the use of a large-scale labeled dataset. The traditional approach of pretraining models on the ImageNet dataset and fine-tuning them with pixel-level annotations has several disadvantages. To address this issue, the proposed strategy focuses on training models with a limited number of pixel-level annotated images using a two-stage training process. The first stage involves pretraining the feature extractor using a pixel-wise, label-based contrastive loss, which encourages intra-class compactness and inter-class separability. In the second stage, the entire network, including the pixel-wise softmax classifier, is fine-tuned using the cross-entropy loss. The proposed strategy is evaluated on the PASCAL VOC 2012 and Cityscapes datasets and shows significant performance gains compared to traditional methods. The contributions of this work include the introduction of new loss functions, a simple contrastive learning-based pretraining strategy, strong results on benchmark datasets, and detailed analyses of class distributions in the feature spaces of trained models.