Convolutional neural networks (CNNs) and attention-based transformers are widely used in computer vision and natural language processing tasks, respectively. Transformers, specifically self-attention mechanisms, have shown promise in image processing as well. These types of transformers, known as Vision Transformers (ViTs), have achieved near state-of-the-art results on various image datasets. Similarly, CNNs trained on large datasets and fine-tuned on smaller datasets, referred to as Big Transfer Models (BiT-M), have also achieved state-of-the-art performance. However, CNNs are susceptible to adversarial examples, where small perturbations to input images cause misclassification by the CNN with high confidence. Adversarial attacks can be carried out by both white-box and black-box adversaries, with the former having access to the CNN's parameters and architecture, and the latter having limited knowledge but the ability to query or estimate gradient information. While CNNs have been extensively studied in the adversarial context, Vision Transformers have not received as much attention. In this paper, we investigate the vulnerability of Vision Transformers to adversarial attacks and analyze their transferability to other models. We also explore potential security improvements and propose new white-box and black-box attack strategies. Our findings show that Vision Transformers are as vulnerable as other models under white-box attacks, but their transferability to non-transformer models is unexpectedly low. However, we demonstrate that ensemble models consisting of Vision Transformers and Big Transfer Models can achieve a high level of robustness under black-box attacks without sacrificing clean accuracy.