With the increased complexity of Convolutional Neural Networks (CNNs), deploying them on resource-sensitive platforms such as mobile devices becomes challenging. This is due to the computational cost and size of model parameters. To address this issue, weight or structural pruning is commonly used to reduce the computational burden. However, existing methods still face problems such as introduced bias or difficulty in handling the intra-neuron relation. In this paper, we propose a new continuous formulation for pruning pretrained CNNs that can be solved through alternating exploration and estimation. We utilize a probabilistic model to characterize the sub-network space, which avoids the ambiguity and potential bias of non-integer values. In the exploration step, sub-networks are sampled using stochastic gradient Hamiltonian Monte Carlo, allowing for a larger search space and avoiding sub-optimal areas. In the estimation step, we compute the optimal distribution for generating high-quality sub-networks, learned from the data, and use it as a guide for subsequent exploration. Our contributions include the proposal of an interpretable probabilistic formulation for model compression, a method for solving the model through alternating exploration and estimation, the design of a correction term to adhere to computational budgets, and an effective search in the parameter space. Extensive experimental results demonstrate the state-of-the-art performance of our approach in pruning VGG and ResNet CNNs on CIFAR10 and ImageNet datasets.