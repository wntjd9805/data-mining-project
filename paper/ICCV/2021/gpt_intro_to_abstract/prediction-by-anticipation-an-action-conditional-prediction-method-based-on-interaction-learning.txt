This paper addresses the challenge of predicting the future state of a scene with moving objects using machine learning techniques. Specifically, the focus is on predicting high-dimensional observations in pixel space, where multiple interacting agents are present and the observations are ego-centric. The authors propose a novel approach that splits the problem into two subproblems, namely ego-features and environment-features. By decomposing the observation and learning the effect of actions on each feature set separately, the authors aim to improve the generalization and adaptability of the models. They propose a conditional deep generative model that combines a deterministic mapping of historical observations and agent movement with a stochastic mapping that captures future interactions. This modular model allows for multi-step action-conditional prediction in autonomous driving scenarios, without the need for labeling. The authors also extend their model to difference learning, which performs well in dense urban traffic. Experimental results on two real-world autonomous driving datasets demonstrate the effectiveness of the proposed model in predicting a wide variety of actions and driving situations.