Spatio-temporal human action detection in untrimmed videos is a crucial task for applications such as surveillance and sports analysis. While considerable progress has been made in recognizing actions from short trimmed videos, these models cannot be directly applied to video analysis in a multi-person scene. Additionally, existing temporal action detection methods for untrimmed videos are unable to spatially detect multiple concurrent human actions. The current benchmarks for spatio-temporal action detection fall into two categories: densely annotated high-level actions and sparsely annotated atomic actions. However, these benchmarks do not cover the realistic challenges of this task, including multiple persons performing different actions concurrently, semantically and temporally well-defined actions, and fine-grained actions that require accurate human pose and motion information. To address these shortcomings, we propose the Multi-Sports dataset, which is a large-scale, high-quality dataset containing precise and dense annotations of fine-grained action categories in both spatial and temporal domains. The dataset focuses on four sports (basketball, volleyball, football, and aerobic gymnastics) due to the presence of multiple concurrent action instances, well-defined action categories and boundaries, and the complexity of sports actions. The dataset is annotated by a team of professional athletes and crowd-sourced annotators, ensuring consistent and clean annotations. We benchmark spatio-temporal action detection on the Multi-Sports dataset and compare the results with existing benchmarks, highlighting the challenges and providing insights for future research in this area. Our main contributions are the development of the Multi-Sports benchmark and the empirical studies and error analysis conducted on the dataset.