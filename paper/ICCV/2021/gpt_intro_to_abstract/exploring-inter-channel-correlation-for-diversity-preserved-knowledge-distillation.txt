The introduction of the paper discusses the challenge of deploying large networks on mobile systems due to high energy consumption and computation costs. Knowledge distillation is identified as a promising method to reduce model size while preserving performance. However, prior works focused on instance-level classification and neglected structural information. This paper proposes a new method called inter-channel correlation knowledge distillation (ICKD) that focuses on exploring inter-channel correlation to improve representation learning. The paper introduces the concept of inter-channel correlation and proposes a grid-level inter-channel correlation distillation method. Experimental results show that the proposed method outperforms existing state-of-the-art methods in various tasks, including classification and semantic segmentation. The contributions of the paper include introducing inter-channel correlation as a measure of feature diversity and homology, introducing grid-level inter-channel correlation for dense prediction tasks, and conducting extensive experiments to validate the effectiveness of the proposed framework.