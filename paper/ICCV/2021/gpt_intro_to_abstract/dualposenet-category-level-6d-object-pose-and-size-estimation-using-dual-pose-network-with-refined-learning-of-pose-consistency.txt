Object detection in a 3D Euclidean space is important in various practical applications, such as augmented reality, robotic manipulation, and self-driving cars. While current object detection methods focus on detecting objects in a 7DoF setting, which assumes objects stand upright in the 3D space, this approach fails to accurately locate objects that lean in 3D space. Precise pose predictions are crucial in safety-critical scenarios like autonomous driving, as they enable better perception and decision making. In this paper, we introduce the task of category-level 6D object pose and size estimation, which involves predicting the full pose configuration of objects in a 3D space. This task is more challenging compared to existing tasks of 7DoF object detection and instance-level 6D pose estimation, as it requires learning rotation-equivariant shape features and cannot leverage privileged 3D shapes for refining pose predictions. To address these challenges, we propose a novel method called DualPoseNet, which consists of two parallel pose decoders and a shared pose encoder. The decoders predict poses using different mechanisms, and the encoder learns pose-sensitive shape features. During testing, a refined learning process is activated to improve the prediction by enforcing pose consistency between the two decoders. Our method achieves superior performance compared to existing methods in terms of pose precision. The proposed DualPoseNet not only improves pose estimation but also enables refined predictions without relying on testing CAD models. We conduct extensive experiments on benchmark datasets and demonstrate the effectiveness of our approach. Our technical contributions include the proposal of Dual Pose Network, the introduction of an implicit decoder for refined pose prediction, and the design of an encoder based on spherical convolutions for learning pose-sensitive shape features. Additionally, we introduce a module called Spherical Fusion for better embedding of appearance and shape features from RGB-D regions.