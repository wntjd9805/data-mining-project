Navigating large dynamic scenes for autonomous driving requires a representation of both the spatial and temporal aspects of the scene. Imitation Learning (IL) by behavior cloning has emerged as a promising approach for this task. This involves training a behavior cloning agent using a dataset of expert trajectories, where the agent predicts the actions of the expert given sensory input of the scene. To handle the complex scene structure, IL-based driving agents have incorporated auxiliary tasks in their training objectives. Examples of these tasks include predicting ego-vehicle velocity and performing semantic segmentation of the scene.In this paper, we propose NEural ATtention fields (NEAT), a feature representation designed to address the challenge of semantic prediction in the Bird's Eye View (BEV) coordinate frame using camera inputs. The key challenge is associating the image pixels with a given BEV query location, which requires understanding 3D geometry, scene motion, ego-motion, and interactions between scene elements. NEAT uses a multi-layer perceptron (MLP) query function to learn a function from any query location to an attention map for features obtained from encoding the input images. The output of this MLP is a compact low-dimensional representation that can be used for dense prediction in space and time.We train several autonomous driving models for the CARLA driving simulator using NEAT intermediate representations. We evaluate our models using a challenging new setting that includes multiple evaluation towns, new environmental conditions, and challenging pre-crash traffic scenarios. Our results show that NEAT outperforms several strong baselines and achieves performance comparable to the expert's performance on our evaluation routes. Additionally, on the secret routes of the CARLA Leaderboard, NEAT achieves competitive driving scores while incurring fewer infractions compared to existing methods.The contributions of this paper are: (1) the proposal of a novel NEAT feature representation combined with an implicit decoder for joint trajectory planning and BEV semantic prediction, (2) the design of a challenging evaluation setting in CARLA and a detailed empirical analysis of NEAT's driving performance, and (3) the visualization of attention maps and semantic scene interpolations from the interpretable model, providing insights into the learned driving behavior. The code for NEAT is available at https://github.com/autonomousvision/neat.