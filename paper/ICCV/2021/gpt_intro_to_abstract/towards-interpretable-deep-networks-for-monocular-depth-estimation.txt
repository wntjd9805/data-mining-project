Monocular Depth Estimation (MDE) has gained significant attention in the field of computer vision due to its critical role in applications such as 3D scene understanding and autonomous driving. Unlike depth estimation using stereo image pairs, MDE requires less resources and is more cost-effective. While deep neural networks have been successful in improving the state-of-the-art performance of MDE, the interpretability of these networks has been largely overlooked. The lack of interpretability in MDE models can have serious consequences, especially in safety-critical applications like autonomous driving. Previous works on deep network interpretability in computer vision have primarily focused on tasks like image classification or image generation. Although some studies have analyzed how deep networks estimate depth from single images, they still treat the networks as black boxes and do not explore the internal representations learned by the MDE networks. Additionally, existing interpretable models for computer vision tasks do not directly apply to MDE due to the unique characteristics of depth estimation.This paper aims to address the lack of interpretability in MDE models by proposing a method to learn interpretable deep networks for MDE without modifying the original network's architecture or requiring additional annotations. The authors observe that some hidden units in deep MDE networks are selective to certain ranges of depth, which can be interpreted as specific depth cues. To quantify this observation, they compute the depth selectivity of each unit and evaluate its meaningfulness by analyzing the performance drop when units are successively ablated based on their selectivity. The authors argue that a unit's importance in an MDE network is determined by its depth selectivity and that the interpretability of the network can be measured by the selectivity of its internal units.Experimental results demonstrate that the proposed method effectively improves the interpretability of deep MDE networks without compromising depth accuracy or even enhancing it. The findings highlight the possibility of achieving a trade-off between interpretability and model performance in explainable AI. Overall, this work contributes to the quantification and enhancement of interpretability in deep MDE networks, providing insights into the internal representations learned by these models and their applicability in real-world scenarios.