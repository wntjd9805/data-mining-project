Deep Neural Networks (DNNs) have shown remarkable performance in various applications, including computer vision and natural language processing. However, DNNs are vulnerable to adversarial examples, which are crafted by adding malicious perturbations to clean images. Current methods for generating adversarial examples focus on adding adversarial perturbations, but in this paper, we propose a novel approach called AdvDrop that generates adversarial images by dropping imperceptible details from clean images. We demonstrate that AdvDrop can achieve high attack success rates and is effective against current defense methods. We also provide visualizations of the dropped information and analyze the attention of the DNNs to interpret the generated adversaries. This paper introduces a new paradigm for crafting adversarial attacks and contributes to our understanding of the properties and mechanisms of adversarial examples.