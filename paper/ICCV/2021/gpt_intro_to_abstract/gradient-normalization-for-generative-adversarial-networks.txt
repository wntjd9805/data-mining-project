Generative Adversarial Networks (GANs) have achieved great success in synthesizing new data for various applications. However, the unstable training process of GANs remains a challenging problem. One approach to address this issue is to impose a Lipschitz constraint on the discriminator. Previous methods have focused on either model-wise or module-wise constraints, and sampling-based or non-sampling-based constraints, but none have simultaneously satisfied all these properties. In this paper, we propose a new normalization method called gradient normalization (GN) that enforces a Lipschitz constant of 1 for the discriminator model. Unlike previous methods, GN is model-wise, non-sampling-based, and hard-constrained. We theoretically prove that GN is 1-Lipschitz constrained, which helps stabilize the training process. Experimental results demonstrate that GN consistently outperforms state-of-the-art methods in terms of both Inception Score and Frechet Inception Distance. Our implementation is publicly available for further research.