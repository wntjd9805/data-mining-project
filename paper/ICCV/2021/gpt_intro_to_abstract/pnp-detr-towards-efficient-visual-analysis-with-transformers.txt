Object detection is an important task in computer vision that involves recognizing objects in images and accurately localizing them using bounding boxes. Traditional object detection methods rely on heuristics and post-processing techniques, resulting in complex model designs and duplicate removal steps. However, a recent method called DETR eliminates these heuristics and achieves end-to-end object detection using convolution feature maps and transformers.While DETR is effective, applying transformer networks to image feature maps can be computationally expensive due to the attention operation. This is because natural images often contain large background areas that are less informative and can lead to redundant features. Existing methods for improving transformer efficiency mainly focus on accelerating the attention operation, but they do not consider the spatial redundancy in the feature map.To address this limitation, we propose a learnable poll and pool (PnP) sampling module. This module compresses the image feature map by generating a set of fine feature vectors and a small number of coarse feature vectors. The fine feature vectors capture important foreground information, while the coarse feature vectors aggregate background information to aid in object recognition and localization. A transformer then models the interaction between these fine and coarse feature vectors to produce the final detection results. The PnP module consists of two sub-modules: a poll sampler and a pool sampler. The poll sampler uses a content-aware meta-scoring network to predict the informativeness score of each feature vector, and selects the most informative vectors. The subsequent pool sampler dynamically predicts attention weights on the non-sampled vectors and aggregates them into a small number of vectors that summarize the background information. We build a PnP-DETR model using the PnP module, which operates on the fine-coarse feature space and adaptively allocates transformer computation in the spatial domain. Our experiments on the COCO benchmark show that PnP-DETR effectively reduces computation cost while achieving a dynamic trade-off between computation and performance. We also validate the efficiency gain of the PnP sampling module on panoptic segmentation and the Vision Transformer (ViT) model. In summary, our contributions include identifying the spatial redundancy issue in image feature maps, proposing a novel poll-and-pool sampling module for feature abstraction, developing the PnP-DETR model for efficient and adaptive computation, and demonstrating the generalizability of the PnP sampling module across different vision tasks. We believe our method provides valuable insights for future research in efficient solutions for vision tasks using transformers.