Optical flow is a critical task in computer vision that involves estimating the motion between pixels in two images or video frames. It is used as a foundation for various higher-level tasks such as object tracking, scene reconstruction, and video compression. Traditional approaches to optical flow involve computing a cost volume for all possible pixel motions and using this information to infer or refine the motion for each pixel. However, these methods face two key challenges. First, the size of the cost volume grows exponentially with the dimensionality of the search space, resulting in significant memory and computation requirements. This is in contrast to 1D stereo matching tasks where the cost grows linearly with the range of disparity. Second, resolving ambiguities caused by occlusion or lack of texture requires a more holistic understanding of the scene, which cost volumes do not inherently provide.This paper introduces a new approach called Separable Flow which addresses these challenges. The proposed method separates the 2D motion of optical flow into two independent 1D problems (horizontal and vertical motion) and uses a self-adaptive separation layer to compress the 4D cost volume into two smaller 3D volumes. This factorization significantly reduces memory and computing resources without sacrificing accuracy. Additionally, non-local aggregation layers are employed to learn a refined cost volume, leveraging prior knowledge and improving accuracy in ambiguous regions.The Separable Flow module is trained and evaluated on standard optical flow datasets (Sintel and KITTI) and achieves state-of-the-art accuracy on both benchmarks. Notably, it outperforms some deep neural network models fine-tuned on the target KITTI scenes in the cross-domain scenario. An ablation study demonstrates the contributions of each innovation, highlighting the benefits that any optical flow framework can derive from these improvements.