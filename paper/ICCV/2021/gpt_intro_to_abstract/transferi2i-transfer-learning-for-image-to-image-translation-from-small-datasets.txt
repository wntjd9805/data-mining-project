Image-to-image (I2I) translation aims to map an image from a source to a target domain. Although several methods have achieved impressive results in paired, unpaired, scalable, and diverse I2I translation, these approaches heavily rely on large labelled datasets, making them less applicable in practice, especially when faced with small datasets. Few-shot I2I translation has been explored, but it fails to perform multiclass translation, and still requires large training datasets. In this paper, we focus on transfer learning for I2I translation with limited data. Previous works have leveraged transfer learning for I2I translation by initializing certain network components with pretrained models, but still require large datasets for training. We propose a new method that decouples the learning process into two steps: image generation and I2I translation. We improve the transfer efficiency by finetuning the pretrained generative model on source and target data for better source-target initialization, and by self-initializing the weights of the adaptor networks. Additionally, we introduce an auxiliary generator to encourage the usage of deep layers in the I2I network. Extensive experiments demonstrate the superiority of our transfer learning technique, enabling high-quality image synthesis even with limited data. Our contributions include exploring I2I translation with limited data, proposing novel techniques for source-target initialization, self-initialization, and auxiliary generator, and achieving significant performance improvements in two-class and multi-class I2I translation tasks. This paper opens up possibilities for I2I translation in domains with limited data availability.