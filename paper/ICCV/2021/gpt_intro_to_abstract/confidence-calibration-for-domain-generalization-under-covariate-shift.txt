Deep neural networks (DNNs) have shown high accuracy in tasks like classification and detection. However, for real-world applications, it is important to indicate the level of trust users should have in model predictions. Model calibration is a recent development that addresses this issue by ensuring that a classifier's predicted probabilities match its true probabilities. Previous research has explored calibration in the context of transfer learning, particularly in unsupervised domain adaptation scenarios. However, these methods require unlabeled data from the target domain, which may not be available during training and calibration in real-world applications. Additionally, existing methods are designed for a single source domain, leading to disparities between the selected source and target domains. We propose a calibration approach for domain generalization that leverages multiple source domains and clusters their labeled data. We fit post-hoc calibration parameters to each cluster and calibrate the class probabilities of test examples using the calibration parameters of the nearest cluster. We study two calibration methods within each cluster and compare them against a baseline approach. Our contributions include proposing novel solutions for domain generalization calibration, providing theoretical error bounds, and demonstrating improved performance on real-world data without needing any target domain data.