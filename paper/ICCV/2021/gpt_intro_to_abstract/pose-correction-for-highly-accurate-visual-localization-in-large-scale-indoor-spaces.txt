Indoor visual localization is a widely used solution for applications such as autonomous robots, augmented reality, and mixed reality. While recent advancements in visual localization have shown promising results in urban environments and small indoor spaces, long-term localization in large-scale indoor spaces still presents challenges due to various factors such as similar places, repetitive patterns, featureless scenes, occluded scenes, and highly dynamic features. However, recent studies have demonstrated the scaling-up of visual localization in indoor spaces using hierarchical models. These models involve image retrieval, pose estimation, and pose selection to determine the final pose. Despite the success of these models, there is still room for improvement, as the accuracy of current state-of-the-art methods in large-scale indoor spaces is lower compared to outdoor benchmark datasets. The sparsity of image positions in the database is identified as the main reason for this performance gap. To address this issue and improve accuracy, this work proposes a novel module called "Pose Correction" that reorganizes local features observed from the estimated pose. This approach mitigates the view-difference problem and yields an updated pose with superior accuracy. The proposed method is evaluated on the InLoc benchmark dataset and compared to existing state-of-the-art methods, demonstrating significant improvements and achieving new state-of-the-art results in large-scale indoor visual localization. Furthermore, ablation studies are conducted to verify the effectiveness of the extended pose correction and the modified pose verification method. The contributions of this work include addressing the view-difference problem caused by database sparsity, proposing a novel pose correction module, extending pose correction and verifying accuracy improvements, and achieving state-of-the-art results in the benchmark dataset.