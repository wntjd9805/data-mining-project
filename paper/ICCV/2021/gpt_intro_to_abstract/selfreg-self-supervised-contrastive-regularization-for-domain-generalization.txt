Machine learning systems often struggle to generalize to new data that falls outside of their training distribution. This is due to the assumption that the data used for training and testing is independent and identically distributed, which is rarely the case in real-world scenarios where the data is likely to change over time. In this paper, we propose a model that utilizes self-supervised contrastive losses to learn a domain-invariant representation of data. By mapping the latent representations of samples from the same class close together, our model aims to reduce the distance between same-class features in the embedding space. We address the issue of training instability caused by negative pairs by focusing only on positive pair samples. We also introduce three gradient stabilization techniques, namely loss clipping, stochastic weights averaging, and inter-domain curriculum learning, to further improve the model's generalization power. We evaluate our proposed model on the PACS dataset and the DomainBed benchmark, and demonstrate its performance against state-of-the-art methods. Our contributions include the facilitation of metric learning using only positive pairs, the development of a condition-dependent projection layer, and the confirmation of comparable performance to existing domain generalization methods in a fair and realistic environment. Furthermore, we explore the benefits of self-supervised contrastive learning in the domain generalization setting, which has not been extensively studied. We also utilize inter-domain mixup techniques and observe that interpolating same-class features helps achieve robust performance on unseen domain data. Additionally, we discuss other related approaches in the literature, such as domain adaptation, semi-supervised domain adaptation, unsupervised domain adaptation, and meta-learning frameworks.