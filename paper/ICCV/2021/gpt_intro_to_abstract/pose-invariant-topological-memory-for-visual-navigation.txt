Visual navigation planning has been extensively studied in computer science. Traditional methods involve constructing a geometric map of the environment using visual simultaneous localization and mapping (vSLAM) and planning a path to a goal using algorithms like A* search. However, vSLAM requires a lot of effort to obtain an accurate map and often relies on high-precision sensors and expert tuning. Alternatively, topological memory-based planning methods have been proposed, where a memory graph represents past robot observations and their reachability. Recent approaches utilize edge predictors, implemented as neural networks, to predict reachability between nodes. These methods do not aim for accurate mapping and do not require high-precision sensors or expert tuning. In this paper, we focus on topological memory-based planning and address the challenge of edge predictor failure due to different observations poses. We propose a pose invariant topological memory (POINT) that leverages omnidirectional view images and spherical CNNs (SCNNs) to ensure similarity between feature maps, regardless of the robot's pose. We also use contrastive learning with data augmentation for edge predictor training to achieve robust planning in varying environmental conditions. Experimental results demonstrate that POINT outperforms traditional methods and is applicable to real-world environments.