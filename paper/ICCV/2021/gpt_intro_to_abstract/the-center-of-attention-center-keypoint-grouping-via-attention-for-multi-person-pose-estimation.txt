Localizing the anatomical 2D keypoints of humans in images is a fundamental task in computer vision with various applications. Current methods for this task can be categorized into top-down and bottom-up approaches, with each having their advantages and limitations. Top-down approaches require separate object detection and tend to perform poorly under heavy occlusions, while bottom-up approaches lack end-to-end training and can be slow. Keypoint grouping, which involves assigning keypoints to the same human pose, is a critical step in bottom-up methods. Existing grouping algorithms rely on optimization and are not differentiable. One-shot methods offer a faster alternative but sacrifice accuracy and require additional postprocessing techniques. To address these limitations, we propose CenterGroup, a framework based on attention. CenterGroup uses attention to find the best match between person centers and keypoints without the need for test-time optimization. We first obtain proposals for centers and keypoints and then encode them with a transformer for enhanced embeddings. These embeddings are used in a simple grouping scheme that maximizes attention scores, and poses are extracted by assigning keypoints to centers based on the highest score. Our proposed method achieves state-of-the-art results, is 2.5x faster than the current state-of-the-art, and allows for end-to-end training.