The rapid development of digital portrait photography, particularly with the rise of mobile cameras, has led to the emergence of relighting techniques for immersive visual effects in VR/AR experiences. However, achieving consistent relit video results under challenging dynamic illumination conditions remains a challenge. This issue has garnered significant attention from both industry and academia. Early solutions relied on expensive and complex studio setups, while modern approaches utilize color or style transfer techniques to reduce hardware requirements but still suffer from significant time constraints. Recent learning techniques have shown promise for human portrait modeling and relighting from monocular RGB input. However, existing methods focus on single image input and fail to capture temporal consistency, leading to jittery artifacts. Additionally, current approaches for video editing of lighting conditions only modify existing illumination rather than relighting captured video into various scenes. In this paper, we propose a novel real-time and temporally coherent portrait relighting approach that addresses these challenges. Our approach models semantic, temporal, and lighting consistency to enable realistic video portrait light-editing and relighting under dynamic illuminations, all while maintaining real-time performance even on portable devices. To support our approach, we create a high-quality dataset consisting of temporal images and environment lighting maps. We also introduce a neural scheme that disentangles semantic and lighting information, adopts a temporal modeling scheme for dynamic relit effects, and utilizes a lighting sampling scheme based on the Beta distribution. Our approach significantly outperforms existing state-of-the-art methods and is accompanied by the release of our dataset for further research in human portrait and lighting analysis.