Video understanding in computer vision is becoming increasingly important due to the vast amount of videos available online. In particular, human action recognition has seen significant progress with the use of deep convolutional networks in videos. However, a fundamental problem in video understanding is sampling, as it is unnecessary and infeasible to process the entire video due to redundancy in time and limited computational resources. Currently, deep convolutional networks employ fixed hand-crafted sampling strategies for training and testing in videos, which have several limitations. In this paper, we propose a motion-guided sampling strategy for trimmed video action recognition. Our strategy is independent of training data, capable of adapting to various video content, and aims to capture motion information while suppressing irrelevant background distraction. To implement our strategy, we propose critical components for motion estimation and temporal sampling. We conduct experiments on five different trimmed video datasets, which demonstrate significant improvement using our motion-guided sampling strategy. Importantly, our strategy does not greatly increase computation burden or running time and is agnostic to the network architecture, making it highly applicable.