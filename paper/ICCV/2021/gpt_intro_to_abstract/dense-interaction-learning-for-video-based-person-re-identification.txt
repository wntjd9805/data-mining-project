The paper introduces a method called Dense Interaction Learning (DenseIL) for video-based person re-identification (re-ID). Person re-ID is important for surveillance applications such as tracking and retrieval. While image-based re-ID has made progress, it is still insufficient in modeling relationships among misaligned body parts. Video-based re-ID considers adjacent frames to infer occluded or missed parts, but existing methods neglect the spatial-temporal interaction between body parts, limiting their effectiveness. The paper proposes DenseIL, which builds spatial-temporal interaction and densely exploits fine-grained cues. It consists of a CNN encoder and a Dense Interaction (DI) decoder. The DI decoder uses self-attention and a newly proposed Dense Attention module to generate a multi-grained spatial-temporal representation. The Dense Attention module extracts hybrid information from convolution and self-attention features to facilitate coordination between the CNN and Attention mechanism. Additionally, Spatio-Temporal Positional Embedding (STEP-Emb) is proposed to enhance the chronological relation. Experimental results show that DenseIL outperforms state-of-the-art methods on video-based person re-ID datasets, achieving the best performance on large-scale MARS and DukeMTMC-VideoReID datasets.