In this paper, we address the need for interpretability in image recognition models and propose a method called Diverse Valuable Explanations (DiVE) for generating informative explanations. Existing counterfactual explanation methods fail to provide diverse, proximal, sparse, and non-trivial explanations. We leverage the Fisher information matrix in the latent space of a generative model to identify less influential factors and discover spurious correlations. DiVE produces multiple counterfactual explanations that are both valuable and diverse. Our experiments demonstrate that DiVE outperforms previous methods in finding non-trivial explanations, detecting biases in datasets, and generating explanations that are proximal and sparse. The contributions of this work include a new benchmark for evaluating the value of explanations, the proposal of DiVE as an explainability method, and the use of the Fisher information matrix to identify spurious features. Overall, DiVE achieves state-of-the-art results in validity, proximity, and sparsity of explanations, making it a valuable tool for machine learning practitioners.