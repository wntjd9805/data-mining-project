Autonomy in robotic applications requires scene understanding using data from various sensors. This includes tasks such as scene classification and object detection. Semantic and instance segmentation are usually treated separately, but panoptic segmentation combines these tasks by predicting "things" (countable objects) and "stuff" (uncountable semantics) together. However, integrating semantic and instance segmentation results effectively has not been addressed adequately in the literature. This paper presents a novel approach to panoptic segmentation based on LiDAR data. The method uses a graph network to generate instances from over-segmented clusters, achieving state-of-the-art results on benchmark datasets. The approach includes a flexible framework, a clustering and graph convolutional neural network, seamless fusion of semantic and instance results, and thorough experimental results and ablation study.