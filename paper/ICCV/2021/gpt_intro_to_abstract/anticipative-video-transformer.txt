The paper discusses the task of activity anticipation, which involves predicting future human actions based on visual signals. The authors highlight the challenges of anticipation compared to traditional action recognition, including the need to predict the multi-modal distribution of future activities and the importance of modeling the sequential temporal evolution of the video. Existing approaches for predictive reasoning tasks typically use aggregation-based temporal modeling or recurrent architectures, but these have limitations in modeling long-range dependencies and preserving the sequential nature of videos. In this paper, the authors propose a new architecture called Anticipative Video Transformer (AVT) that leverages the transformer architecture with causal masked attention. AVT can predict future actions while processing the input in parallel with long-range attention. The model is trained to jointly predict the next action and future features, encouraging a predictive video representation. The authors also propose using an attention-based frame encoder from the Vision Transformer as a backbone for purely attention-based video modeling. Experimental results demonstrate the performance of AVT on various benchmarks, outperforming prior work and achieving top rankings on popular action anticipation datasets. Overall, the paper introduces a novel architecture for predictive video modeling that addresses the limitations of existing approaches.