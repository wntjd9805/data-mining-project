The paper introduces the problem of long-tailed recognition in computer vision, where training data is highly unbalanced but the test set is kept balanced. The authors argue that standard classifiers trained with cross-entropy loss overfit to highly populated classes and perform poorly on low-shot classes. They propose an approach called Geometric Structure Transfer (GIST) that leverages overfitting to transfer geometric information from many-shot classes to few-shot classes.The GIST approach is based on a deep classifier architecture called GistNet, which consists of an embedding implemented with neural network layers and a parametric classifier implemented with logistic regression. The authors propose a parameter constellation, composed of a class-specific center and shared displacements, to encode the class geometry.To learn the GistNet classifier, a combination of class-balanced and random sampling is used. Class-balanced sampling is used to learn the class-specific center parameters, while random sampling is used to learn the shared geometry parameters. This leverages the overfitting of the standard classifier to the popular classes, allowing the transfer of geometric structure to the few-shot classes.The proposed GistNet classifier is evaluated on two long-tailed recognition datasets and outperforms previous approaches. The paper makes several contributions, including highlighting the advantage of leveraging overfitting for transfer learning, proposing the GistNet architecture, and introducing the GIST learning algorithm that combines class-balanced and random sampling.Overall, the paper addresses the challenge of long-tailed recognition by transferring geometric structure from many-shot classes to few-shot classes, without the need for manual class weighting or heuristic recipes.