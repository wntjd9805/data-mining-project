Arbitrary style transfer involves rendering the content of one image using the style of a different artwork. Previous methods have relied on complex optimization problems, leading to high computational costs. To address this, feed-forward networks with feature transfer modules have been used to produce stylized results in a single pass. However, these methods may not generalize well to unseen images, leading to degraded stylization quality or distorted content structures. In this paper, we propose a new arbitrary style transfer method that generates diversified and coherent stylization results. This approach involves three modules: style bank generation, transformer-driven style composition, and parametric content modulation. Our method, called StyleFormer, produces visually plausible stylization results while ensuring style diversity and content coherence. We train our network on MS-COCO and Wikiart datasets using common style and content losses. Compared to previous approaches, our method achieves state-of-the-art stylization results in terms of both visual quality and efficiency.