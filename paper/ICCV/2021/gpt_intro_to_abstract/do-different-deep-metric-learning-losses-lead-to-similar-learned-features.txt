Deep metric learning (DML) involves training a neural network to map input images to embedding vectors based on their shared class. Various loss functions have been introduced for DML, including ranking based and classification based methods. Recent studies have shown that different loss functions yield similar test performances. However, it remains unclear which input features neural networks focus on while forming an output. This paper aims to analyze and compare the features attended to by neural networks trained with common DML loss functions. Two new analysis methods are proposed, one on a pixel level and one on an image property level. The pixel level analysis utilizes a gradient-based explanation approach to highlight image pixels that influence the network's output. The image property level analysis measures the influence of different image properties on the embeddings and proposes a property-independent extension called Normalized R-Precision. The experiments reveal that different loss functions tend to learn different features depending on the dataset. The contributions of this paper include the introduction of the two analysis methods, the Normalized R-Precision measure, insights into the features learned by common DML loss functions, and the availability of code and data for further research.