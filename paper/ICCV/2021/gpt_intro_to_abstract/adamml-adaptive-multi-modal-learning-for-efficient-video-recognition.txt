Multi-modal learning, which utilizes multiple modalities such as RGB frames, motion, and audio, has gained attention for improving the performance of video recognition models. However, in real-world scenarios with long and untrimmed videos, it becomes computationally impractical to process all modalities. Some modalities may provide irrelevant or redundant information for recognizing action classes, leading to inefficiency. This paper proposes a new approach called AdaMML that adaptively selects optimal modalities for each video segment to achieve efficient video recognition. A multi-modal policy network is trained using Gumbel-Softmax sampling to output the probabilities of using or skipping each modality for each segment. The approach demonstrated significant computational savings and improved accuracy compared to traditional multi-modal learning approaches. Experimental results on various video benchmarks validate the superiority of AdaMML over state-of-the-art methods.