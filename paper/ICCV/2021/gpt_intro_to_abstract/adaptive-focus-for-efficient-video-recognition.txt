The explosive growth of online videos has increased the demand for automatically recognizing human actions, events, or other contents within them. This recognition is beneficial for applications such as recommendation, surveillance, and content-based searching. Previous studies have achieved accurate video recognition by leveraging deep networks, but these models often come with high computational costs. In real-world scenarios, the computational costs translate into power consumption, carbon emission, and practical latency, which need to be minimized. While recent works propose reducing temporal redundancy in video recognition, the exploration of spatial redundancy in image-based data for efficient video recognition has been scarce. This paper addresses this issue by developing a novel adaptive focus (AdaFocus) approach that dynamically localizes and attends to the task-relevant regions of each frame. AdaFocus first obtains cheap and coarse global information by utilizing a lightweight CNN and then trains a recurrent policy network to select the most valuable region for recognition. Finally, a high-capacity deep CNN processes only the selected regions, which leads to significant improvements in efficiency while preserving accuracy. The AdaFocus framework can be extended to model temporal redundancy by introducing an additional policy network that determines whether to skip uninformative frames. Experimental results on multiple video recognition benchmarks demonstrate the effectiveness of AdaFocus, both alone and in conjunction with existing temporal-based techniques. AdaFocus consistently outperforms baselines by large margins, and AdaFocus+ achieves higher efficiency by reducing computation compared to recent approaches. The proposed method can also be applied to state-of-the-art networks, enhancing their computational efficiency.