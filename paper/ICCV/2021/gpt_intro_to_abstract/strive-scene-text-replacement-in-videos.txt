This paper addresses the problem of realistically altering scene text in videos, specifically focusing on creating personalized content for marketing and promotional purposes. The authors propose a method for replacing scene text in videos with a personalized string while preserving the original geometry, appearance, and temporal properties. This technique can be applied to various applications such as language translation, text redaction for privacy, and augmented reality. The ability to manipulate scene text in videos also enables the augmentation of datasets for training text detection, recognition, tracking, erasure, and adversarial attack detection. The traditional manual process of text editing in images by graphic artists is time-consuming and challenging, especially when applied to videos. The authors leverage the progress made in automated text replacement in still images using deep style transfer techniques to tackle the problem of text replacement in videos.Replacing scene text in videos presents additional challenges compared to still images, including respecting temporal consistency and modeling effects such as lighting changes and blur induced by camera and object motion. Furthermore, the pose of the camera relative to the text object can vary widely in videos, requiring the text replacement method to handle diverse geometries. The authors propose a different approach that involves training a spatio-temporal transformer network (STTN) to frontalize text regions of interest (ROI) in a temporally consistent manner. They then select a reference frame with high text quality and perform still-image text replacement on this frame using a state-of-the-art method. To transfer the replacement text onto other frames, they introduce a text propagation module (TPM) that considers changes in lighting and blur effects with respect to the reference frame.The authors present their framework, called STRIVE (Scene Text Replacement In VidEos), which disentangles the problem of text replacement into simpler subtasks to simplify training and reduce computations during inference. They also introduce a learned parametric differential image transformation that captures temporal photometric changes between aligned ROIs and applies it to the text-altered video. This image transformation consists of a learnable blur/sharpness operator and is trained on synthetic data and fine-tuned using self-supervision on real-world images. Additionally, the authors provide new synthetic and real-world datasets comprising diverse annotated scene text objects in videos.Overall, this paper proposes a modular pipeline for text replacement in videos that addresses the challenges posed by temporal consistency and geometric variations. The authors contribute a novel approach to handling video text replacement and provide datasets and tools to facilitate further research in this field.