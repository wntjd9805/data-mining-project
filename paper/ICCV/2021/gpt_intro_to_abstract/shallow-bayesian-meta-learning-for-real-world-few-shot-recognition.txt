Few-shot recognition methods aim to address classification problems with limited labeled training data, and deep meta-learning methods have been widely employed in this field. These meta-learning approaches typically involve training a model using auxiliary data to facilitate rapid adaptation to new categories or synthesizing a classifier for new categories. However, recent studies have questioned the effectiveness of deep meta-learning, suggesting that well-tuned pre-trained convolutional networks combined with simple linear or nearest centroid classifiers can achieve comparable or even better performance than state-of-the-art meta-learners. In this paper, we defend the use of meta-learning for few-shot recognition by showing that even shallow meta-learning can enhance few-shot learning beyond what pre-trained features alone can achieve. Specifically, we propose an amortized Bayesian inference approach called MetaQDA, which utilizes a Bayesian prior and a gradient-based meta-training procedure to enhance the classic Quadratic Discriminant Analysis (QDA) classifier. Our framework offers practical benefits, such as the ability to conduct meta-learning in resource-constrained scenarios without end-to-end training and improved performance compared to fixed-feature approaches. Additionally, MetaQDA exhibits superior calibration and performance in cross-domain settings, making it suitable for practical applications with limited domain-specific data. Overall, our contributions include the introduction of MetaQDA, empirical evidence of its superior performance across various settings, and insights into the meta-learning versus vanilla pre-training debate.