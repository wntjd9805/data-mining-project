Deep neural networks are vulnerable to adversarial examples, which are human-imperceptible perturbations that can fool the network. Universal adversarial perturbations (UAPs) are a type of perturbation that can attack a model for most images. This is concerning because UAPs can be generated beforehand and applied in real-time attacks. This paper revisits UAP by providing an alternative explanation for the phenomenon of a dominant label, where an untargeted UAP causes the misclassification of a large fraction of images. Previous explanations for the dominant label phenomenon were not sufficient, so this paper proposes a new explanation based on the observation that UAPs have a dominant contribution to the model response. The paper also investigates techniques for practical universal attacks under the constraint of not having access to training data. Specifically, the authors propose using self-supervision and artificial jigsaw images for optimizing UAPs and reducing the sample size. They also explore the use of UAPs for black-box attacks in other applications such as object detection and semantic segmentation. Overall, the contributions of this paper include providing a new explanation for the dominant label phenomenon, proposing techniques for practical universal attacks, and demonstrating the effectiveness of UAPs in various applications.