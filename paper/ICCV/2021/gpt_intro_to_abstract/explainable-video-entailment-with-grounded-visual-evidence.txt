This paper introduces the research area of bridging computer vision and natural language processing in tasks including visual captioning, VQA, and visual-textual retrieval. The authors address the video entailment problem, which is the task of determining whether a textual statement is entailed or contradicted by a video. The main challenge is the fine-grained reasoning required to understand complex story-based videos and make correct judgments. The existing method lacks a fine-grained understanding of the video frames and relies heavily on the textual dialog. To overcome this, the authors propose a visual grounding model that links the entities described in a statement to evidence in the video. This grounding module localizes the clips where the entities appear and guides the judgment to focus on those clips and the corresponding sentences in the dialog. The authors also aim to improve the faithfulness of the entailment model by evaluating if the judgment is based on correct evidence. They propose regularizing the training of the entailment judgment module to encourage local explanations on the contribution of words in the statement. The main contributions of the paper are the novel approach to video entailment with visually grounded evidence, the incorporation of explainability into the entailment model, and the outperformance of the proposed method compared to existing methods.