The problem of point cloud registration, which involves aligning two point clouds through rigid transformation, has numerous applications in fields such as autonomous driving, 3D reconstruction, and augmented reality. The most commonly used method for solving this task is the Iterative Closest Point (ICP) algorithm, which has undergone improvements over time. Recently, end-to-end learning-based approaches have emerged, combining point feature extraction, point matching, and point-pairs filtering. These methods, such as Deep Closest Point (DCP) and Deep Global Registration (DGR), utilize deep neural networks to estimate point correspondences and transformations. However, these existing methods do not explicitly control the integration of local fine geometric information and high-level contextual information. In this paper, we propose a novel approach that computes point correspondences at every layer of a deep network using cross-attention matrices. These matrices capture both low-level geometric details and high-level context information, enabling more precise point matching and discrimination between similar points from different scene parts. Moreover, we leverage these cross-attention matrices to exchange information between the point clouds at each layer, allowing the network to exploit context information from both clouds and find the best matching point within overlapping regions. Our method draws inspiration from previous works like DGR and DCP but extends their ideas by computing cross-attention matrices at multiple network layers. The main contribution of our approach is the effective combination of fine and high-level information for point matching, resulting in state-of-the-art results on both synthetic and real datasets. Additionally, our method demonstrates significant improvements in indoor and outdoor scenarios. Overall, our approach provides a more robust and accurate solution for point cloud registration, improving upon existing techniques.