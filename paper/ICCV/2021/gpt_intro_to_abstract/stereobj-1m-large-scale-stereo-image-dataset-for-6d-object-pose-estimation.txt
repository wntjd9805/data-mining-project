This paper focuses on the challenge of estimating the pose of transparent and reflective objects using 3D cues from visual data. Previous approaches have explored deep models that take RGB-D maps as input, but existing depth-sensing methods fail to capture the depths of transparent or reflective surfaces. Therefore, this paper proposes using stereo RGB images as the input modality for object pose estimation. To address the challenge of acquiring a large-scale training dataset, the authors introduce a novel method for capturing and labeling a dataset using multi-view geometry and fiducial markers. This method allows for the accurate localization of cameras, fiducial markers, and object keypoints in the scene. The resulting StereOBJ-1M dataset is the first pose dataset with stereo RGB as input, includes transparent and reflective objects, and is captured in both indoor and outdoor environments. It consists of over 100K frames and over 1.5 million 6D pose annotations of 18 objects. The dataset is sufficient for training large-scale neural networks without additional synthetic images and has the best annotation precision among all public object pose datasets. The paper also presents a novel object-level 6D pose optimization approach called Object Triangulation, which significantly improves pose estimation using stereo input. The experimental results highlight the importance of stereo modality in object pose estimation, and the authors expect that StereOBJ-1M will serve as a benchmark dataset for stereo RGB-based object pose estimation.