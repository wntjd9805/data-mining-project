Batch normalization (BN) has emerged as a significant technique in deep neural networks (DNNs) for improving convergence in computer vision tasks. While previous works have explored the impact of BN on optimization, our study focuses on the consequences of enhanced optimization, particularly on the robustness of DNN models. Most prior research investigating BN and robustness has focused on the covariate shift, but our work examines BN's influence on adversarial robustness from a non-robust feature perspective.We evaluate the behavior of models with and without BN on multiple datasets and observe that BN improves clean accuracy but reduces robust accuracy. We propose a framework for disentangling the robust usefulness and robustness of DNN features, based on the concept of useful features. We find that BN and other normalization variants increase adversarial vulnerability, indicating a shift towards relying more on non-robust features for classification. Our analysis is supported by measuring corruption robustness and feature transferability.To measure the shift in feature robustness, we propose a metric called Local Input Gradient Similarity (LIGS), which evaluates the local linearity of the DNN. Comparing clean accuracy and robust accuracy provides some insights into feature robustness, but LIGS offers direct evidence of BN's influence on feature robustness. We show that introducing BN into DNNs reduces robustness, explaining the lower robust accuracy. Furthermore, we analyze models trained on datasets biased towards robust or non-robust features and find that BN is essential for learning non-robust features. Our framework is not limited to BN analysis, as we also investigate other network structures and optimization factors, with BN being the primary factor that significantly influences the shift towards non-robust features.One practical application of our findings is in boosting transferable attacks. We demonstrate that a substitute model without BN outperforms the variant with BN, and early-stopping the training of the substitute model enhances transferable attacks.Overall, our study contributes to understanding how BN impacts the robustness of DNNs, providing insights into DNN behavior and guiding the design of more robust models.