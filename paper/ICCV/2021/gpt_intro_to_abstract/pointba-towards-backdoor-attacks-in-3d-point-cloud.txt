In recent years, 3D deep learning has gained prominence due to its applications in various fields such as autonomous driving, scene reconstruction, and medical data analysis. However, the security problems associated with 3D deep learning systems have started to receive attention. One of the most damaging threats is the backdoor attack, which involves injecting a small amount of poisoned data during training and activating malicious functionality through a specified trigger during inference. This type of attack poses a significant threat in the 3D domain due to the inherent characteristics of 3D point cloud data, such as noise and sparsity. Existing methods for 2D backdoor attacks cannot be directly applied to the 3D domain due to differences in data structure and model architecture. Hence, this paper proposes a framework to investigate backdoor attacks in the 3D domain, focusing on the design and analysis of backdoor triggers for 3D point cloud data. The paper presents two examples of 3D triggers, namely the orientation trigger and interaction trigger, and analyzes their perturbation effects. The paper also introduces a standard backdoor attack algorithm for 3D triggers, which consistently achieves high attack success rates across different datasets and models. To address security concerns, a clean-label attack algorithm is proposed, leveraging 3D adversarial attack techniques and feature disentanglement to craft perturbed data that is difficult for the model to learn the semantic label from but easy to associate with the implanted trigger. The contributions of this work include a unified framework for investigating backdoor attacks in the 3D domain, experimental evidence of the efficacy of the proposed triggers, and the development of a stealthier clean-label attack technique.