Image perception for low-level visual patterns is crucial in various computer vision tasks, including SLAM, Structure-from-Motion (SfM), and image matching. Local point features have been widely used in these tasks, but researchers have recently started exploring the use of structural features for improved geometric representation. Line segments, being the most prevalent structural features in man-made environments, play a key role in these tasks. To reliably extract and match line segments across frames, convolutional neural networks (CNN) based line segment detection models have been developed, which outperform traditional methods. However, these models consist of two stages and are not suitable for real-time applications due to their slow running speed. In this paper, we propose ELSD, a one-stage architecture that simultaneously predicts line segments and infers line descriptors in an end-to-end manner. ELSD utilizes the Center-Angle-Length (CAL) representation to vectorize a line segment, with a localization module and a regression module for detection. To address the ambiguity of mid-points when lines intersect, we introduce the line-centerness concept and employ modified focal loss to focus more on the mid-points of hard cases. In the regression module, geometric maps are predicted to provide rotation angles and lengths, and the position of mid-points is refined using fine offsets for improved localization accuracy. Additionally, ELSD integrates line pooling to obtain descriptors for each predicted line segment, which are learned through random homography-based self-supervision. The proposed pipeline achieves state-of-the-art performance in terms of accuracy and efficiency on the Wireframe and YorkUrban datasets. The light version of our model achieves a speed of 107.5 FPS on a single GPU (RTX2080Ti) while maintaining comparable performance. To the best of our knowledge, this is the first work that integrates line detector and descriptor in a compact neural network, sharing computational resources and allowing joint training of the two tasks with minimal detection performance loss.