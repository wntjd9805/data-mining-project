Deep Neural Networks (DNNs) have achieved impressive results in various real-world problems such as image classification, natural language processing, and speech recognition. Designing effective architectures for DNNs is crucial for achieving high performance. However, traditional methods for architecture design are time-consuming and require expertise in both DNNs and the specific task domain. Neural Architecture Search (NAS) is an automatic approach that aims to design DNN architectures without the need for expert knowledge.NAS algorithms typically consist of three components: search space, search strategy, and performance estimation. The search strategy is responsible for finding high-performing architectures within the predefined search space. Different techniques, including reinforcement learning, gradient-based algorithms, and Evolutionary Computation, have been used as search strategies in NAS algorithms. However, all NAS algorithms require estimating the performance of the searched architectures, which often results in heavy computational overhead.To address this issue, several algorithms have been proposed to speed up the performance estimation process without requiring extensive computational resources. These algorithms can be categorized into weight inheritance, early stopping policy, reduced training set, and neural predictor. Among these, neural predictors have gained popularity due to their ability to provide satisfactory prediction results. Previous works have focused on designing better regression models for neural predictors, but the limited training dataset remains a significant challenge.In this paper, we propose a novel data augmentation strategy, called Homogeneous Architecture Augmentation for Neural Predictor (HAAP), to address the issue of limited training data. HAAP explores the homogeneous properties of neural architectures by swapping inner orders to generate homogeneous representations. One-hot encoding strategy is used to effectively represent the intrinsic properties of architectures. Experimental results on NAS-Bench-101 and NAS-Bench-201 datasets demonstrate the effectiveness of HAAP and its superiority over existing methods. Additionally, the Homogeneous augmentations can be combined with state-of-the-art neural predictors to further improve their predictions.The remainder of the paper is organized as follows: Section 2 provides a review of related works, Section 3 presents the implementation details of HAAP, Section 4 discusses experiments and presents extensive results, and Section 5 concludes the paper and suggests future directions.