Content-based image retrieval is an important task in computer vision, with recent advancements focusing on enabling users to interact with the retrieval system using additional forms of input, such as sentences, attributes, and clicks. This is particularly relevant in the context of online shopping, where appearance is a key factor in product selection. Existing methods for interactive image retrieval often suffer from the limitation of semantically entangled representations, where changing one aspect of an image can inadvertently affect other aspects. In this paper, we propose the use of disentangled representations in interactive fashion retrieval to overcome this limitation and improve controllability and interpretability of search results. We train convolutional networks to learn attribute-specific subspaces, allowing for operations to be applied directly on the desired subspace without affecting others. We demonstrate the effectiveness of our disentangled representation on various interactive retrieval tasks, including attribute manipulation retrieval, conditional similarity retrieval, and outfit complementary item retrieval. To enable attribute manipulation, we introduce a memory module and a visual-semantic consistency loss to swap attribute representations and preserve the structure of the memory block. Our experimental results show that our method achieves state-of-the-art performance on multiple applications. Overall, our contributions include demonstrating the benefits of disentangled representations in interactive retrieval tasks and proposing novel techniques for attribute manipulation retrieval.