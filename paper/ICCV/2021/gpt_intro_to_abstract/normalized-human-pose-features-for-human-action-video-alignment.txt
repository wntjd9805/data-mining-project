Video alignment is a challenging task in computer science as it involves finding dense temporal correspondences between two videos. When aligning human action videos, there can be significant variations in factors such as scales, orientations, camera viewpoints, and action speeds, making alignment difficult. Previous approaches to video alignment involve estimating human poses from input videos and matching them to reduce interference from video backgrounds and clothing. However, human poses also have variations in scale, bone length ratios, and orientations. Existing 3D pose estimation methods recover poses in camera coordinate systems, making it challenging to apply global orientation normalization. Additionally, joint positions computed by existing methods are dependent on the subjects' anthropometric characteristics, causing differences in distance measurements. This paper proposes using joint angular representations instead of joint position-based representations to describe pose similarities. Joint angles are more consistent among different viewpoints and invariant to camera angles, making them more suitable for comparison. However, using raw joint angles as features for matching suffers from information loss and limitations in directly regressing 3D joint rotations or computing joint rotations from 2D poses. To overcome these limitations, the paper introduces a normalized human pose representation that reflects pose information with respect to joint rotations. This representation preserves the relational context of body configurations and is parameterized by joint positions to reduce ambiguity in comparing pose similarities. The paper presents a neural network that learns to normalize human poses in videos by estimating joint rotations and applying forward kinematics on a predefined 3D skeleton. The resulting normalized poses are then used to learn pose features through metric learning. Experimental results show the robustness of the proposed normalized pose representation in video alignment, pose retrieval, and action detection tasks.