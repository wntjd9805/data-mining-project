Generative modeling of 3D shapes is widely applicable in computer vision, graphics, and robotics. To be effective in downstream applications, a good 3D generative model should be both faithful and probabilistic. A faithful model generates realistic shapes that adhere to human perception and respects partial observations. A probabilistic model captures the multi-modal nature of shape generation and completion by producing diverse shapes from scratch or partial observations. Existing shape generation models can be categorized into voxel-based and point cloud-based approaches, each with limitations in terms of memory usage or inability to capture multi-modality. Recently, probabilistic diffusion models have shown impressive performance in 2D image generation, but extending them to 3D is challenging due to the binary nature of voxels and permutation invariance of point clouds. In this paper, we propose Point-Voxel Diffusion (PVD), a probabilistic shape generation model that overcomes these challenges by integrating denoising diffusion models with the hybrid point-voxel representation of 3D shapes. PVD enables high-quality shape generation by denoising Gaussian noise and produces multiple completion results from partial observations. It offers a unified framework for both unconditional shape generation and conditional, multi-modal shape completion. Experimental results demonstrate that PVD outperforms state-of-the-art methods in synthesizing high-fidelity shapes and achieving multi-modal shape completion from partial observations. The proposed model shows good performance on multiple synthetic and real datasets, including ShapeNet, PartNet, and the Redwood dataset.