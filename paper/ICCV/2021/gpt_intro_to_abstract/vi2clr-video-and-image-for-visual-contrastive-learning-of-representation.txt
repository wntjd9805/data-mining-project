Learning strong and discriminative representations is crucial for various computer vision tasks, including image classification, object detection, image segmentation, activity recognition, video classification, medical imaging, and natural language processing. Unsupervised or self-supervised representation learning has gained attention as it does not rely on manually labeled data and can effectively close the performance gap with supervised training. State-of-the-art methods in this field are driven by instance or prototype discrimination tasks, which involve contrastive loss and image or video augmentation. Data augmentation techniques play a vital role in contrastive representation learning for images and videos. In this paper, we propose a method called Vi2CLR, which simultaneously trains ConvNets for both images and videos using a multi-task learning approach. Our approach optimizes both dynamic and static visual cues using clustering as supervision and enhances representation learning by considering positive and negative pairs within clusters and enforcing proximity to cluster centroids. We evaluate our Vi2CLR-based ConvNets on downstream video and image classification benchmarks and achieve state-of-the-art performance. Our 3D ConvNet achieves top results on UCF101, HMDB51, and Kinetics-400 datasets, while our 2D ConvNet outperforms SimCLR and SwAV on the ImageNet dataset.