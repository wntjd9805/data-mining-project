This paper introduces the concept of egopose estimation in augmented and virtual reality (AR and VR) experiences. Egopose refers to the 3D head and body pose of the camera wearer, which is crucial for creating immersive experiences in AR and VR. Precise characterization of egopose is necessary for seamless interaction between real people and virtual entities. Existing approaches to egopose estimation fall into two categories: non-optical sensor based methods and camera based methods. While sensor-based methods provide robust estimates, they are intrusive and harder to set up. Camera-based methods, on the other hand, are less intrusive but face challenges in capturing the wearer's pose when they are not visible in the camera's field of view.To address these challenges, this paper proposes a deep learning approach that utilizes both camera motion information and visible body parts to estimate egopose. The proposed method can handle situations where the wearer is partially or completely invisible in the camera's field of view. Additionally, the model computes 3D head pose and figure-ground segmentation of the camera wearer.Dataset availability is a major challenge in egopose estimation. To overcome this, the paper leverages existing mocap data to synthesize egocentric images and dynamic pose information. The synthesized datasets show superior generalization power on real videos.The contributions of this paper include a novel perspective on egopose estimation, a joint estimation procedure for ego-head and ego-body poses, an approach for synthesizing egopose data from mocap data, and a real-time pose estimation model suitable for AR and VR applications.