In this paper, we tackle the problem of few-shot visual-semantic learning, which requires an artificial intelligence system to comprehend and learn from a small number of image-text samples. We propose a Hierarchical Graph Attention network (HGAT) that effectively models the intra-modal relationships and inter-modal relationships between visual and semantic modalities. In the first stage of HGAT, we use visual-specific and semantic-specific Graph Neural Networks (GNNs) to capture the intra-relationships within images and texts. We also introduce an attention-based co-learning framework to guide the node feature update of these GNNs, capturing the inter-relationships between the visual and semantic modalities. In the second stage, relation-aware GNNs are used to predict the results of query samples by jointly learning the visual representations, semantic representations, and intra- and inter-modal relationships.We evaluate the performance of HGAT on three widely-used benchmarks and demonstrate its superiority compared to previous methods. HGAT achieves state-of-the-art accuracy in few-shot visual question answering and image captioning tasks. The proposed model also outperforms other graph-based methods in terms of accuracy. Furthermore, ablation experiments confirm the benefits of modeling visual-specific and semantic-specific relationships, the attention-based co-learning framework, and the hierarchical graph-based architecture. Overall, our proposed method offers a promising solution for few-shot multi-modal learning, particularly few-shot visual-semantic learning, and can be easily extended to the semi-supervised setting.