Domain Adaptation (DA) is a challenging task in machine learning, especially when the target domain is significantly different from the source domain. Conventional supervised DA requires a small set of labeled data in the target domain, which is expensive and impractical. In this paper, we focus on Unsupervised DA (UDA) where abundant unlabeled data in the target domain can be leveraged. We consider the assumptions of covariate shift and conditional shift on the domain gap and existing UDA solutions that aim to learn invariant features in both source and target domains. However, due to the lack of supervision in the target domain, capturing domain invariance is challenging. Our goal is to address this challenge and improve the performance of domain adaptation in UDA scenarios.