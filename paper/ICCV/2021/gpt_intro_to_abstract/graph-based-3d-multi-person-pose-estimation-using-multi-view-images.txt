Estimating 3D human poses of multiple individuals from multiple views is a problem with numerous applications, such as sports broadcasting and retail analysis. Previous research on 3D multi-person pose estimation has focused on two main approaches: 2D-to-3D lifting-based methods and direct 3D estimation methods. While the former is efficient for real-time performance, it suffers from limited 3D reconstruction accuracy due to the reliance on 2D pose estimation, which is not robust to occlusion. The latter approach avoids incorrect decisions in 2D camera views but incurs high computational costs and quantization errors. In this paper, we propose a novel approach that combines the advantages of both methods. Our approach utilizes 2D-to-3D lifting for efficient 3D human center detection and direct 3D estimation for accurate single-person 3D pose estimation. Both stages are processed using task-specific graph neural networks, achieving a balance between accuracy and efficiency. We introduce the Multi-view Matching Graph Module for data-driven matching of people across views, considering both visual and geometric cues. We also propose the Center Refinement Graph Module, which operates on the continuous 3D space to refine human center locations. Additionally, we present the Pose Regression Graph Module for fine-level single-person pose estimation by leveraging both spatial and geometric relations. Our approach significantly outperforms previous methods in terms of accuracy and efficiency. Overall, our contributions include the application of task-specific graph neural networks for multi-view 3D pose estimation, the introduction of learnable matching for multi-view human association, the development of a point-based human center refinement module for more accurate localization, and the proposal of a graph-based model for 3D human pose refinement.