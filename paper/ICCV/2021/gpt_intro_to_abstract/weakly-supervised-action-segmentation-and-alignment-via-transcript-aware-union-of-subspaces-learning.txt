Localization and classification of human actions in videos is a challenging task in computer vision. While previous methods have focused on fully-supervised settings with dense annotations, this approach is not scalable for large amounts of video data. As a result, there is growing interest in learning from weakly-annotated videos, such as action transcripts. However, existing weakly-supervised action learning methods still face challenges, including limitations in training models with estimated segmentations and ignoring the low-dimensional structures of videos. Additionally, inference on test videos without transcripts is computationally expensive. In this paper, we propose the Transcript-aware Action Subspace Learning (TASL) framework to address these challenges. TASL models actions using a union of low-dimensional subspaces, learns the subspaces using weak supervision, and refines video features. We introduce a Union-of-Subspaces Network (USN) to capture variations of each action and develop a hierarchical segmentation framework for real-time inference. Experimental results on three datasets demonstrate the effectiveness of our approach in improving action segmentation while speeding up inference.