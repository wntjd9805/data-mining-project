The human visual system is adept at understanding and perceiving scenes, particularly in relation to occlusion. However, machines struggle with reasoning occlusion relationships due to the elusive boundaries of objects. This reasoning is crucial in computer vision applications. Traditional methods rely on low-level visual cues, but these are ineffective in defining boundaries and occlusion cues. Convolution neural networks (CNNs) have improved occlusion relationship reasoning by decomposing the task into boundary extraction and orientation prediction. However, two critical issues have been overlooked: the limited coupling between the two subtasks and the lack of a good representation of occlusion orientation. To address these issues, we propose an Occlusion-shared and Path-separated Network (OPNet) that utilizes abstract feature maps and splits them into task-specific paths. We also propose a robust orthogonal occlusion representation (OOR) to better represent orientation. Experimental results show that our method significantly improves object boundary extraction and occlusion orientation prediction.