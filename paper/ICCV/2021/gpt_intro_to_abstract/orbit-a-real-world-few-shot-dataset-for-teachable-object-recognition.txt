Object recognition systems have made significant advancements in recent years, but they still heavily rely on large training datasets with a high number of labeled examples per object category. This requirement makes collecting training datasets expensive and limits their use to a few application areas. Few-shot learning aims to address this issue by training models to recognize novel objects with only a few examples. This would enable recognition systems to adapt to real-world, dynamic scenarios such as self-driving cars or user-provided training examples. However, most few-shot learning research has been driven by datasets that lack the high variation and quality found in real-world scenarios. Existing datasets assume a fixed number of objects and training examples per object, hindering algorithmic innovation in few-shot learning. To drive further innovation in few-shot learning for real-world impact, it is crucial to have datasets that capture the high variation inherent in real-world applications. The authors introduce the ORBIT dataset, which consists of videos recorded by people who are blind/low-vision on their mobile phones, and an associated few-shot benchmark grounded in teachable object recognizers (TORs) for blind/low-vision individuals. The dataset and benchmark aim to accelerate research in few-shot, high-variation object recognition and explore new research directions in few-shot video recognition. The contributions of this paper include the ORBIT benchmark dataset, which contains videos of objects in various realistic conditions, and the ORBIT teachable object recognition benchmark, which evaluates personalization to individual users and incorporates metrics reflecting the computational cost of real-world deployment. The paper also presents state-of-the-art results on the ORBIT benchmark and highlights the need for algorithmic innovation in few-shot techniques for handling high-variation data. The code and dataset are made available to the research community for further exploration.