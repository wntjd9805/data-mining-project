Supervised video action recognition (AR) has seen significant advancements with the emergence of new models and large-scale video datasets. However, collecting annotated training data for each action class has become increasingly expensive due to the growing number of desired action classes. To address this issue, Zero-Short Action Recognition (ZSAR) has gained attention, aiming to generalize AR models to unseen actions without labeled training data. Existing approaches for ZSAR focus on embedding videos and action classes into a joint semantic space. However, representing action classes semantically poses a challenge due to the complexity and diversity of actions. Early works use manually-defined attributes, but this approach is difficult to generalize. Recent works adopt word embeddings of action names, but these embeddings can be ambiguous and confusing in relating different action classes. Additionally, while deep features have been used for video semantic representations, they often overfit on seen action classes and transfer poorly to unseen ones. Therefore, in this paper, we propose an Elaborative Rehearsal (ER) enhanced model for ZSAR. Inspired by the human memory technique of elaborative rehearsal, we construct Elaborative Descriptions (ED) to define action classes comprehensively and propose two encoding network streams to embed spatio-temporal dynamics and objects in videos. We also introduce Elaborative Concepts (EC) by utilizing a pre-trained image object classification model. To improve generalization, our model rehearses video contents with additional semantic knowledge from EC. Our ER-enhanced ZSAR model achieves state-of-the-art performance on widely used benchmarks and we propose a new ZSAR evaluation protocol based on the Kinetics dataset. This benchmark demonstrates comparable performance to a few-shot learning baseline under a clear split of seen and unseen action classes.