Convolutional neural networks (CNNs) based approaches have shown promising performance in object tracking tasks. However, conventional frame-based cameras, which are commonly used as sensing devices, have limited frame rates and dynamic range. This limits their robustness in degraded conditions. On the other hand, event-based cameras, a bio-inspired sensor, offer high temporal resolution, high dynamic range, and low energy consumption. However, they cannot measure absolute light intensity and provide texture cues. In this paper, we propose a multi-modal sensor fusion-based approach that leverages the complementary advantages of both frame- and event-based cameras to improve tracking performance in degraded conditions. We address the challenge of combining asynchronous events and synchronous images by proposing a simple yet effective event aggregation approach. This approach discretizes the time domain of asynchronous events, allowing them to be easily processed by a CNNs-based model. Experimental results show that our proposed aggregation method outperforms other commonly used event accumulation approaches. We also introduce a novel cross-domain feature integrator that effectively fuses visual cues from both the event- and frame-domain, using a self- and cross-domain attention scheme. Our approach significantly outperforms state-of-the-art frame-based methods in terms of success rate and precision rate. In order to enable more research on multi-modal learning with asynchronous events, we construct a large-scale single-object tracking dataset, FE108, which provides ground truth annotations in both the frame- and event-domain. This dataset is the largest event-frame-based dataset for single object tracking and has the highest annotation frequency in the event domain. Overall, our contributions include the introduction of a novel cross-domain feature integrator, the construction of a large-scale frame-event-based dataset, and the demonstration of the superior performance of our approach compared to state-of-the-art methods.