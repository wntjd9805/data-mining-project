Estimating the 3D pose of human hands from monocular images alone is a challenging problem with various applications in robotics, Human-Computer Interaction, and AR/VR. Many existing approaches have focused on this problem, but difficulties arise due to diverse backgrounds, lighting conditions, self-occlusion, and hand appearances. Acquiring labeled 3D hand joint data is laborious and costly, making it challenging to transfer well to real-world images. To address this, researchers have explored leveraging auxiliary data with weaker or no joint annotations. However, the accuracy of models trained on these annotations is limited by label noise. In recent years, self-supervised learning methods, such as contrastive learning, have shown promising results for image classification tasks. These methods utilize unlabeled data to learn powerful feature representations. However, for 3D pose estimation tasks like hand pose estimation, geometric transformations require equivariance, which standard contrastive learning may not effectively capture. In this paper, we propose a novel self-supervised representation learning technique called Pose Equivariant Contrastive Learning (PeCLR) for 3D hand pose estimation. We introduce a contrastive learning objective that induces equivariance to geometric transformations and can leverage the diverse set of hand images without joint labels. Our method involves pre-training a network with unlabeled data and fine-tuning it for the hand pose estimation task using labeled data. We demonstrate that our approach improves pose estimation accuracy compared to supervised and standard contrastive learning methods. Additionally, we show that our method enhances label efficiency in semi-supervised settings and improves cross-domain generalization. Our contributions include investigating contrastive learning for unlabeled data in hand pose estimation, proposing an equivariant contrastive learning objective, conducting controlled experiments to determine the best augmentations, and demonstrating the superiority of our method over state-of-the-art architectures. The code and models are available for research purposes.