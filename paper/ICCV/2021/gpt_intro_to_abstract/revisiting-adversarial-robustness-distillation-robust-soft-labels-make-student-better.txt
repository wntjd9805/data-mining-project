Deep Neural Networks (DNNs) have become the standard models for solving complex real-world learning problems, but they are also vulnerable to adversarial attacks. Adversarial training (AT) is an effective method to defend against these attacks by generating adversarial examples during model training. However, AT methods have a drawback of relying on large models, which may not be suitable for certain scenarios that require small and lightweight models. To address this limitation, knowledge distillation along with AT, known as Adversarial Robustness Distillation (ARD), has been proposed to boost the robustness of small DNNs by distilling knowledge from robust large models. In this paper, we investigate the use of Robust Soft Labels (RSLs) as a key element in boosting the robustness of small DNNs via distillation. RSLs, which better represent the robust behaviors of the teacher model, can provide more robust information to guide the student's learning. We propose a novel ARD method called Robust Soft Label Adversarial Distillation (RSLAD), which applies RSLs to replace hard labels in all supervision loss terms. We empirically verify the effectiveness of RSLAD in improving the robustness of small DNNs against state-of-the-art attacks and provide a comprehensive understanding of the importance of RSLs for robustness distillation. Our contributions include the identification of the usefulness of the implicit distillation process in AT methods and the proposal of RSLAD as a new method for boosting robustness in small DNNs.