In recent years, convolutional neural networks (CNNs) combined with parallel processing on GPUs have significantly advanced machine learning for image data. However, the memory limitations of parallel hardware pose a challenge for extending these network architectures to handle data with more than 2 dimensions. Sparse convolutions and octrees have been explored as solutions, but practical constraints on input size still exist. In this paper, we propose C-PIC, a framework for learning with tensors while only observing a small fraction of their entries. C-PIC leverages the fact that a learned representation can be constrained to have low rank after suitable non-linear mapping. This constraint allows for a smart sampling strategy that selectively chooses which tensor entries are shown to the architecture. The entire pipeline is end-to-end trainable with back-propagation, enabling the optimal tuning of the low-dimensional embedding for a given prediction task. Crucially, our approach can handle massive spatial resolutions even when the available memory is limited, making it suitable for GPUs. We achieve this through the use of the tensor train (TT) decomposition, which offers efficient cross-approximation algorithms for high-resolution data processing without exceeding memory limits. Our work is the first to employ cross-approximation for learning within a neural network, allowing for operation at high spatial resolution. We present experimental results on two medical image analysis problems, demonstrating that our approach performs on par with state-of-the-art methods. Furthermore, C-PIC can operate at double the resolution compared to other methods, thanks to its ability to handle memory constraints effectively.