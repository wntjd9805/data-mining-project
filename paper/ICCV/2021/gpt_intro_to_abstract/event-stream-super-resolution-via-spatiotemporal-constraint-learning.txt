This paper introduces the concept of event cameras, which are bio-inspired vision sensors that respond to pixel-wise brightness changes asynchronously. Compared to traditional frame-based cameras, event cameras offer advantages such as high dynamic range, high temporal resolution, and freedom from motion blur. However, the spatial resolution of existing event cameras is limited. Therefore, this paper focuses on exploring the algorithm of event stream super-resolution, which aims to increase the spatial resolution of low-resolution event streams. The proposed method is based on a spiking neural network and incorporates a spatiotemporal constraint learning mechanism to accurately describe the spatial and temporal distribution of the event stream. Experimental results demonstrate that the proposed method outperforms existing methods and can be applied to object classification and image reconstruction tasks. Furthermore, the method is implemented on a mobile system, showcasing its real-time performance and potential for deployment on mobile devices. The contributions of this paper include the development of an end-to-end SNN-based model, the demonstration of satisfactory performances on downstream applications, and the implementation of an embedded deployment of the method.