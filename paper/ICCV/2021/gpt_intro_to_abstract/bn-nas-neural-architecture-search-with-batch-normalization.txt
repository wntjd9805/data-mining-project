Neural architecture search (NAS) has greatly improved network performance in computer vision tasks but is limited by high computational costs. To address this, one-shot methods have been proposed, reducing search cost by constructing a supernet and using weight-sharing. However, the training and evaluation process is still time-consuming. In this paper, we propose a novel approach called BN-NAS that utilizes the Batch Normalization (BN) layer to reduce training and searching times. Our BN-based indicator measures the importance of operations and subnets, significantly reducing search cost. Additionally, we only train BN parameters in the supernet stage, further speeding up training. Our experiments demonstrate that BN-NAS achieves faster training and searching times without sacrificing accuracy. The main contributions of this work are the BN-based indicator and the reduction of training epochs, both of which significantly shorten the time required for NAS methods.