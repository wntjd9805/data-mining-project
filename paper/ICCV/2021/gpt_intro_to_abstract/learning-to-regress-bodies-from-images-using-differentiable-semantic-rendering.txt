Estimating 3D human pose and shape from in-the-wild images has garnered significant attention due to its applications in animation, games, and the fashion industry. However, the lack of accurate 3D ground-truth annotations makes this task challenging. Previous methods primarily focus on 2D keypoints with some learned 3D priors, but relying solely on 2D keypoints leads to unrealistic poses and inadequate information about body shape. To address these limitations, recent approaches propose using part-segmentations or silhouettes, but there is a mismatch between these and the common 3D body models. In this paper, we propose an alternative approach that leverages high-level 2D image cues by using detailed clothing segmentation labels to supervise a neural network. To reason about the correspondence between segmentation labels and the SMPL body model, we learn a semantic clothing prior from a large-scale clothed human scan dataset. We introduce Differentiable Semantic Rendering (DSR), a novel loss that supervises the training of 3D body regression with clothing semantics using weak supervision. Our novel loss consists of two components: DSR-C for supervising the clothing region and DSR-MC for the minimally-clothed region. DSR outperforms previous state-of-the-art methods on 3DPW and Human3.6M datasets and obtains comparable results on MPI-INF-3DHP, highlighting the value of using human parsing and semantics for more accurate human body estimation.