Enhancing underexposed images and videos captured in low light is a challenging task in computer vision. Existing methods based on deep neural networks have focused on supervised learning using datasets of low-light images or videos. However, these methods do not address the specific challenges of enhancing underexposed videos taken from dynamic scenes or severe noise corruption. The lack of suitable datasets for dynamic scenes further hinders progress in this area. In this paper, we present a new dataset, called the SDSD dataset, consisting of 150 high-quality spatially-aligned video pairs of dynamic scenes in low- and normal-light conditions. We developed a mechatronic alignment system to capture videos with nearly-identical camera motion, ensuring temporal and spatial consistency. Our dataset is the first to provide high-resolution video pairs for dynamic scenes, enabling the evaluation and comparison of enhancement methods. In addition, we propose an end-to-end framework for enhancing underexposed videos that incorporates noise reduction and illumination enhancement. We employ a self-supervised strategy for noise reduction and predict the illumination map for each input frame using the Retinex theory. Our framework performs well even in extremely low-light conditions and shows superior results compared to state-of-the-art methods on our constructed dataset and the SMID dataset. To validate the visual quality and accuracy of our approach, we conduct a large-scale user study with 100 participants, demonstrating its superiority over previous methods. Our contributions include the construction of a high-quality dataset for enhancing underexposed dynamic scenes and the development of a novel framework that effectively addresses noise and illumination enhancement in videos captured in low-light conditions.