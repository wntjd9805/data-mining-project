This paper introduces a new vision-language task called Spatial-Temporal Video Grounding (STVG), which aims to generate spatio-temporal tubes for target objects in untrimmed videos based on textual descriptions. The existing methods for spatial localization in images/videos rely on pre-generated object proposals, which have limitations in terms of localization performance, generalization to new datasets, and additional training data and computational cost. To address these issues, the authors propose a one-stage visual-linguistic transformer-based framework called STVGBert, which directly generates spatio-temporal object tubes without relying on pre-trained object detectors. The framework utilizes a cross-modal feature learning module called ST-ViLBert, which encodes both spatial and temporal visual information. Experimental results on benchmark datasets demonstrate that STVGBert outperforms state-of-the-art methods in terms of accuracy and efficiency. The contributions of this work include the proposal of a one-stage STVG framework that does not require pre-trained object detectors, the introduction of the ST-ViLBert module for modeling spatio-temporal information, and extensive experiments validating the effectiveness of the proposed framework.