This paper introduces Sim2SG, a self-supervised real-to-sim scene generation technique that addresses the domain gap between synthetic and real data. Synthetic data with automatically generated annotations can help overcome the supervised learning bottleneck in labeling real data. However, to be effective, synthetic data must resemble real data in appearance and content. Existing approaches either manually create synthetic scenes at high cost or intentionally leverage the gap for sim-to-real transfer, limiting their performance. Recent approaches attempt to reduce the content gap but have limitations in aligning the structure and surrounding context of synthetic scenes with real data. Additionally, none of these approaches address the appearance gap. In this paper, Sim2SG is proposed as a self-supervised framework that generates synthetic data similar to real data without the need for real-world labels. It consists of synthesis and analysis stages, where synthetic data is created and then used for training to reduce the content gap. Gradient Reversal Layers are used to align the latent and output distributions and further reduce both the appearance and content gaps. The effectiveness of Sim2SG is demonstrated in the scene graph generation task in three different environments: synthetic CLEVR, synthetic Dining-Sim, and real KITTI. The domain gap is nearly closed in the CLEVR environment, and significant improvements are observed over baselines in Dining-Sim and KITTI. The contributions of this paper include being the first to propose self-supervised aligned scene generation, a synthesis-by-analysis framework that addresses both the content and appearance gaps, and experimental validation of the effectiveness of Sim2SG in different scenarios.