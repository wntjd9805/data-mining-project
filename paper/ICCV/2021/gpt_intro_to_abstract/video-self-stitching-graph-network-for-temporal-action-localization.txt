This paper addresses the challenge of localizing short actions in untrimmed videos, which is crucial for various applications in video understanding. The authors highlight the problem of large variation in action duration and the inferior performance of methods on short actions. They propose a solution that involves temporally upscaling videos and utilizing both the original scale and the enlarged scale to enhance feature representations. This is achieved through a Video self-Stitching Graph Network (VSGN) that incorporates video self-stitching and cross-scale graph pyramid network components. The VSGN shows significant improvement in the localization of short actions compared to other methods, achieving new state-of-the-art performance on THUMOS-14 and ActivityNet-v1.3 datasets. The contributions of this work include addressing the problem of short actions, proposing a novel framework, and demonstrating improved performance in the task of temporal action localization.