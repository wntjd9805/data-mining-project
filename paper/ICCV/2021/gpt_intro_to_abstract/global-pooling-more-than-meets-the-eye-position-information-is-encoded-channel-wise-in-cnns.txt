This paper explores the concept of invariance in neural network architectures, specifically focusing on translation invariance and translation equivariance. The authors examine how convolutional neural networks (CNNs) achieve translation invariance through the use of equivariant operations and global pooling layers. Surprisingly, the authors find that CNNs encode positional information along the channel dimension, even after the spatial dimensions are collapsed during the global pooling operation. This discovery raises the question of how CNNs can contain positional information despite the global pooling layer. The authors provide an answer to this question and demonstrate through quantitative experiments that CNNs encode position information based on the ordering of the channel dimensions. Additionally, the authors propose practical applications of these findings, including improving translation invariance using a loss function and identifying position-specific neurons in a network's latent representation. The authors also show how it is possible to impair the performance of a network in a region-specific manner. The main contributions of this paper are revealing how global pooling layers admit spatial information, proposing strategies to improve translation invariance, identifying position-specific neurons, and demonstrating region-specific attacks on network predictions. These findings have implications for understanding CNN properties and guiding future design.