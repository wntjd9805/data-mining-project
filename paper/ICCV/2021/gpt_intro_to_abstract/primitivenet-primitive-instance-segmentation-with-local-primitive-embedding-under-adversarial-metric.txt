In recent years, 3D scanning techniques with 3D sensors have seen significant advancements, enabling the reconstruction and digitization of real-world environments. However, the quality and complexity of the output models fall below the standards required for applications such as gaming and virtual/augmented reality (AR/VR). Common issues include noise-filled scans and the inability to accurately segment primitive instances. Current approaches, such as mesh decimation and primitive assembly, have limitations in preserving important structures and ensuring high-quality segmentation. To address these limitations, we propose a PrimitiveNet that focuses on improving segmentation quality and generating lightweight models from scans. We transform the global primitive fitting problem into local tasks, making it easier to learn and generalize. By training a primitive embedding network, we extract per-point local surface properties using explicit and implicit features. We also introduce an adversarial metric as a primitive discriminator to distinguish different instances. Our approach combines PointNet and sparse convolution to extract high-resolution point features and regular volume features, respectively. Experimental results demonstrate that our method outperforms existing approaches in various metrics, including mean average precision. We are also able to handle extremely large-scale environments by processing chunks and merging them seamlessly. Ablation studies confirm the importance of local properties and the effectiveness of explicit property supervision. Finally, our algorithm is integrated into a pipeline that produces lightweight models from real scans. Overall, our contributions include the design of an adversarial primitive embedding network, a high-resolution backbone, superior performance compared to state-of-the-art methods, and the ability to handle large-scale environments.