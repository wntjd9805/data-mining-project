Video matting is an important task in computer vision that aims to estimate the foreground opacity (alpha matte) of each frame in a video. This has become particularly relevant with the rise in popularity of video conferencing. Unlike binary segmentation, matting produces soft masks that better represent object boundaries or transparent materials. However, achieving accurate video matting is challenging due to the need for both spatial accuracy and temporal coherence. In this paper, we address these challenges by proposing a Consistency-Regularized Graph Neural Network (CRGNN) framework. We utilize a composited video matting dataset to enhance temporal coherence and introduce a consistency regularization technique to adapt the model to real videos. Our method outperforms existing approaches on both composited and real datasets. We also contribute two large-scale composited datasets and one manually annotated real dataset for future research in this area.