Person image synthesis has gained significant attention in various fields, including the movie industry, e-commerce, and person re-identification. The synthesis of person images involves two critical tasks: pose transfer and pose-guided virtual try-on. Existing methods often fail to explore the correlations between these tasks, resulting in blurry and implausible generated images. To overcome these issues, this paper proposes a structure-transformed texture-enhanced network for person image synthesis. The proposed model consists of a structure-transformed renderer and a texture-enhanced stylizer. The structure-transformed renderer addresses the challenges of pose alignment by using cross-modality deformable convolutions to capture structural motions. This approach avoids artifacts caused by flow-based warping. The texture-enhanced stylizer, on the other hand, enhances texture details by introducing pose-guided high-frequency attention. This attention mechanism improves the visual quality of the generated images by enhancing high-frequency components with spatial contextual information. The proposed approach outperforms existing methods in terms of generating realistic and diversified person images. Ablation experiments are conducted to validate the contributions of the key components. The main contributions of this paper include the proposal of a structure-transformed texture-enhanced network for joint pose transfer and pose-guided virtual try-on tasks, the design of a structure-transformed renderer using cross-modality deformable convolutions, and the introduction of a texture-enhanced stylizer to enhance detailed textures and allow for fashion style manipulation. Experimental results demonstrate the effectiveness and superiority of the proposed approach in generating high-quality person images with diverse poses and fashion styles.