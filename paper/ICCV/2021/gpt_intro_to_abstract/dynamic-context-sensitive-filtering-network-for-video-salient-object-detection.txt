Videos are a highly engaging medium that deeply resonates with humans. Video salient object detection (VSOD) is a fundamental task in video processing that aims to segment visually distinctive regions in videos. This task has garnered significant attention due to its wide range of applications in various fields. However, VSOD is more challenging than still-image based salient object detection (SOD) tasks, as it not only needs to process a large amount of data but also needs to account for temporal dynamics. Existing VSOD methods typically employ fixed parameter layers during inference, which limits their adaptability to dynamic real-world environments. To address this limitation, we propose a dynamic context-sensitive filtering module (DCFM) that can estimate location-related affinity weights to dynamically generate context. This promotes the sensitivity of convolution kernels and enhances the model's adaptability.We also introduce a bidirectional dynamic fusion strategy that encourages interaction between spatial and temporal domains, enabling the network to combine cross-domain features and ensure stability in saliency detection. We conducted extensive experiments on five widely-used datasets and demonstrated that our method outperforms 12 state-of-the-art VSOD approaches in terms of three evaluation metrics. Our approach exhibits high adaptability in different video scenarios, reducing the mean absolute error (MAE) metric by 34.8% and 27.3% on SegV2 and DAVIS datasets, respectively, which are dominated by fast and moderate moving objects.Furthermore, the proposed DCFM can be extended to improve existing still-image SOD models, achieving better performance across all evaluation metrics. Overall, our method presents a promising approach towards accurate VSOD, addressing the challenges of adapting to dynamic changes and distinguishing fine differences in the real-world environment.