Vision-Language Navigation (VLN) tasks, where an agent learns to navigate using natural language instructions, have gained research interest. The key challenge in VLN is perceiving visual scenes and comprehending instructions to make step-by-step actions. Deep learning techniques in feature extraction, attention, and multi-modal grounding have improved environment understanding, while reinforcement learning works have aided in navigation policy development. However, VLN still suffers from biases due to limited data and a large navigation space, impacting generalization. Previous works proposed data augmentation methods within scenes but failed to address the bias across different scenes. Therefore, this paper proposes Random Environmental Mixup (REM), a scene-wise data augmentation method to reduce the domain gap. REM cross-connects scenes, paths, trajectories, and instructions to generate augmented training data that improves navigation generalization. Experimental results on benchmark datasets show that REM significantly reduces the performance gap between seen and unseen environments, surpassing other augmentation methods and achieving new state-of-the-art results in VLN.