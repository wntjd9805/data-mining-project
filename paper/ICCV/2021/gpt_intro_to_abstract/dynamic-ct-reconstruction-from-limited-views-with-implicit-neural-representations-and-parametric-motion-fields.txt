Computed tomography (CT) is a popular imaging technology widely used in various industries and medical settings. It involves capturing x-ray data or sinograms by scanning an object from different angles. Reconstruction algorithms then estimate the scene from these sinograms. While CT imaging of static objects is well-studied, reconstructing dynamic scenes, known as dynamic 4D-CT, is a challenging problem due to spatio-temporal ambiguities caused by aggregated measurements over time. Traditional CT reconstruction algorithms struggle to address this motion-related issue. However, solving 4D-CT problems is crucial for applications such as clinical diagnosis and material evaluation. Several techniques have been proposed to handle periodic and deformable motion, but they assume sparse measurements spanning the full angular range and multiple revolutions around the object, which is not always practical. This paper presents a novel approach for 4D-CT reconstruction that is specifically suited for limited-view scenarios. The method combines an implicit neural representation (INR) model as the static scene prior with a parametric motion field to estimate an evolving 3D object over time. By minimizing the discrepancy between synthesized and observed sinograms, the proposed method optimizes both the INR weights and motion parameters in a self-supervised manner, resulting in accurate dynamic scene reconstructions without training data. The validation of the method is done through a synthetic dataset for parallel beam CT and evaluation on a medical imaging task involving reconstructing a periodically deforming thoracic cavity. The proposed method outperforms competitive baselines in both scenarios.