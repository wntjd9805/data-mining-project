In this work, we aim to build a perception system that can understand the possible interactions with general objects. Our goal is to develop a system that can infer the myriad possible interactions with a novel object. The ability to predict feasible and desirable actions with objects is of central importance in both the robotics and computer vision communities. This skill can assist in motion planning, efficient exploration, interactive learning, and semantic understanding. While passive learning approaches have focused on inferring semantic labels from visual input, they provide limited understanding, especially in terms of actions and prediction models. Our paper takes a step forward in building a common perception system by actively interacting with objects to create its own supervision on successful actions. We approach the problem by parametrizing the predicted action space as a sequence of short-term atomic interactions. We formulate the task as one of dense visual prediction, inferring for each pixel/point whether a certain primitive action can be performed at that location and how it should be executed. Our approach includes a prediction network that can learn to predict actionability scores, action proposals, and success likelihoods for each pixel. We propose an on-policy data sampling strategy to improve the efficiency of exploration. We evaluate our method on a variety of objects and demonstrate that our network successfully learns actionable visual representations and generalizes well to novel shapes and unseen object categories. Our contributions include formulating the task of inferring affordances for manipulating 3D articulated objects, proposing a learning approach that utilizes adaptive sampling, and creating benchmarking environments to evaluate our method.