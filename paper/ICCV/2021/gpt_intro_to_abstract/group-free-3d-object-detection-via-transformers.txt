This paper addresses the problem of 3D object detection on point clouds. Unlike 2D object detection, which works on regular images, 3D object detection requires handling irregular and sparse input point clouds. Previous methods have used hand-crafted grouping strategies to assign points to object candidates, but these can lead to incorrect assignments and degrade detection performance. To overcome these limitations, the paper proposes a novel approach that eliminates the need for handcrafted grouping. The key idea is to compute features for each object candidate by considering all points in the point cloud, with the contribution of each point determined by an automatically learned attention module. The Transformer architecture is adapted to model object-object and object-pixel relationships, allowing for accurate object feature extraction without handcrafted grouping. To further enhance the performance of the Transformer architecture, the paper introduces two improvements. Firstly, the spatial encoding of objects is iteratively refined in different stages, leading to more accurate object predictions. Secondly, the ensemble of detection results from all stages is used during inference, resulting in improved performance with minimal computational overhead. The proposed method is evaluated on the ScanNet V2 and SUN RGB-D benchmarks, and it demonstrates effectiveness and robustness even with simple initial object candidates. Significant performance improvements are achieved with the ensembles scheme on the SUN RGB-D dataset. The proposed approach achieves state-of-the-art performance on both benchmarks without complex additional techniques. The paper also highlights the potential of using the attention mechanism or Transformers for point cloud modeling. Unlike in 2D image modeling, where these modeling tools mainly complement grid modeling techniques, they address the inherent challenges of irregular and sparse distribution in 3D point clouds.