In this paper, we address the fundamental problem of understanding the inner workings of deep neural networks (DNNs), which is crucial in various real-world applications. We highlight the recent findings that DNNs are easily fooled and emphasize the need for interpreting DNNs. We argue that existing approaches, linking semantics to units or layers of DNNs, do not effectively capture the hierarchical inference process and fail to explain the reasoning process in DNNs. To address this issue, we propose a new method called neural architecture disentanglement (NAD) that learns to disentangle a pre-trained DNN into sub-architectures for different tasks. We describe the information flows and inference processes captured by the sub-architectures. Inspired by representation disentanglement, we design an objective function that constrains the information between successive layers of DNNs. We conduct extensive experiments with handcrafted and automatically-searched architectures, including VGG16, ResNet50, DenseNet121, and DARTS-Net. Our experiments on object-based and scene-based datasets (ImageNet and Place365) yield three key findings. First, DNNs can be divided according to independent tasks, indicating the possibility of disentanglement. Second, the disentanglement can end before the last layer, challenging the assumption that deeper layers correspond to higher semantics. Finally, the connection type in a DNN affects the information flow and disentanglement behavior. Dense skip-connections can mix up information for classification, whereas architectures with direct connections or skip-connections amortize the information differently. In addition, misclassified images are more likely to be assigned to tasks with similar sub-architectures as the correct ones. Overall, our study contributes by introducing NAD as a method for understanding DNN inference processes and providing new insights into the inner logic of DNNs.