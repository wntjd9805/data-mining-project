Deep neural networks (DNNs) have shown impressive performance in object classification tasks, but they are highly prone to shortcut learning. Instead of learning holistic representations and generalizable decision rules, DNNs rely on shortcut opportunities (SO) where the target class is highly correlated to a few easily represented input factors. This leads to poor generalization in out-of-distribution (OOD) settings. We observe that humans are also prone to shortcut learning in some cases, but they remain unaffected when predicting basic object factors of variation (FoV) such as shape, hue, or texture. To improve model generalization, we propose to consider generalization opportunities (GO) in addition to shortcut behavior. GO relax the strict correlation between a target class and an input FoV and can be achieved through compositions or frequency-based outliers. We argue that a good vision model should explicitly represent these FoVs rather than being invariant to them. To analyze a model's capability to exploit GO, we introduce a synthetic benchmark suite called DiagViB-61 that enables systematic control over SO and GO for different visual object FoVs. The suite includes an image-generating function and a dataset-generating function that allow control over the correlation between factors and the co-occurrences of factor class combinations. We evaluate various deep learning vision models on this benchmark and show that while they exploit frequency-based GO, their exploitation of compositional-based GO is limited. The benchmark serves as a critical diagnosis for studying a model's shortcut vulnerability and generalization ability under controlled tasks and data setups. Our contributions include the proposal of the benchmark suite, the establishment of suitable metrics to evaluate shortcut vulnerability and GO exploitation, and empirical evidence showing limited exploitation of GO by common vision architectures.