Estimating hand pose has become increasingly important in various applications such as augmented and virtual reality, sign language recognition, and gesture-based interfaces. Recent advancements in deep learning techniques have led to significant progress in skeletal pose estimation for isolated hands. However, there has been limited attention given to estimating the pose of two interacting hands, which is a challenging problem due to self-occlusions caused by the interaction. Most existing approaches for this scenario use generative, model fitting-based methods, but these are limited by the lack of training data. In this paper, we propose a new CNN-based hand pose estimation framework that addresses the challenges of interacting hands. We experimentally validate the hypothesis that visible hands contain useful information for inferring the pose of occluded hands, and leverage this dependence to jointly estimate the pose of interacting hands. We also employ a GAN-type discriminator and explicitly estimate joint visibility to improve pose estimation accuracy. Our system includes a hand detection network to classify hands into interacting and non-interacting categories, enabling tailored pose estimation for challenging cases while still maintaining performance on single-hand cases. To our knowledge, our system is the first end-to-end trainable pipeline that performs both detection and pose estimation for interacting hands. Experimental results demonstrate that our approach significantly improves upon baseline and state-of-the-art systems in joint estimation accuracy.