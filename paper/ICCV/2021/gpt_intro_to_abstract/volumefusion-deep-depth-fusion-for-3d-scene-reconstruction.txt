Multi-view stereo (MVS) is a widely studied topic in computer science, with the goal of reconstructing a 3D scene from multiple images. Traditional MVS techniques have limitations in handling repetitive patterns, low-texture regions, and reflections. Recent advancements in deep neural networks have shown promise in overcoming these limitations. Deep-based MVS methods focus on estimating pixel-wise correspondences between images, but they require depth map fusion for complete 3D reconstruction. A recent approach using a Truncated Signed Distance Function (TSDF) volume for direct regression has advantages but struggles with complex scenes.To address this issue, we propose a two-stage approach for MVS using local reconstruction and global fusion. Our end-to-end framework computes local geometry by generating dense depth maps from neighboring frames. We then merge local depth maps and image features into a single volume representation, regressing TSDF for final 3D scene reconstruction. This approach learns a globally consistent volumetric representation without the need for manually engineered fusion algorithms. Additionally, we introduce a Posed Convolution Layer (Posed-Conv) to enhance the robustness of our depth fusion mechanism by being invariant to both translation and rotation. This allows our method to reconstruct globally consistent shapes even with wide baselines or large rotations between views. Our contributions include a novel network that combines local MVS and global depth fusion for 3D scene reconstruction, and the development of the Posed Convolution Layer, which extracts pose-invariant feature representations. Our method is the first learning-based depth map fusion approach for 3D reconstruction from multi-view images.