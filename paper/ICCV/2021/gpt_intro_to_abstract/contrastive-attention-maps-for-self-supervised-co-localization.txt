Object localization is a fundamental task in computer vision that aims to identify the location of a target object in an image. Deep learning approaches using convolutional neural networks (CNNs) have become popular for object localization, but they typically require costly human-annotated bounding box annotations. To address this limitation, recent attention has focused on weakly-supervised object localization methods that use weaker supervision signals, such as image-level class labels or dataset-level superclass labels. This paper specifically focuses on image co-localization, which aims to locate common objects in a dataset consisting of only one superclass.Existing image co-localization methods can be categorized into multiple instance learning (MIL) and self-supervised representation learning (SSL). MIL-based methods generate candidate boxes and identify if each box contains the target object, but they are computationally expensive and cannot operate in real-time. SSL-based methods use image transformations as pretext tasks for representation learning, but they lack spatial information.This paper proposes a contrastive learning framework for image co-localization. The framework first aggregates the last convolutional feature map across channels to generate an attention map that retains spatial information. Then, it maximizes the similarity between the attention map of the original image and the inverse transformed attention map of the transformed image, while also maximizing dissimilarity with the background. Various image transformations are explored to create positive pairs for contrastive learning.Extensive experiments demonstrate the effectiveness of the proposed method. Qualitative evaluation shows that the method accurately localizes the target object and ignores the background. Quantitative evaluation on benchmark datasets yields new state-of-the-art performance in object localization. The contributions of this paper include the novel adoption of contrastive learning for image co-localization, a study on the definition of positive and negative pairs, a simple yet effective attention extraction method, and an optimal combination of image transformations.