This paper introduces the concept of self-supervised agents in computer science, which learn representations without explicit supervisory labels. These methods have shown impressive performance, surpassing those based on supervised learning. However, they rely on a curated dataset of observations, which deviates from the biological learning process.Biological vision involves active physical exploration of the environment, where infants obtain visual experience by interacting with toys and the surrounding environment. Similarly, learning visual representations in a computational setting can benefit from active exploration. In this paper, the authors propose a framework called Curious Representation Learning (CRL) that allows agents to learn visual representations through curiosity-driven exploration.Unlike traditional reinforcement learning approaches that provide noisy and sparse supervision, CRL leverages self-supervised representation learning techniques to learn representations in embodied environments. The framework includes a separate exploration algorithm that gathers data for representation learning. The challenge lies in effectively exploring the environment and continuously gathering visually salient images.To address these challenges, CRL combines reinforcement learning with self-supervised representation learning. An exploration policy is trained to maximize reward based on the loss of the self-supervised representation learning model. This policy learns to explore the environment and obtain visually distinct images. Simultaneously, the self-supervised model benefits from diverse images that remain visually salient.The paper also discusses the use of embodied visual representations for downstream interactive tasks. Interactive learning, such as reinforcement learning and behavioral cloning, poses challenges due to sparse and noisy feedback. To enable good downstream interactive transfer, it is crucial to freeze the visual network weights before transfer. The authors demonstrate the effectiveness of their approach in improving semantic navigation performance and visual language navigation using imitation learning.In summary, this paper introduces the CRL framework for embodied representation learning and demonstrates its effectiveness in various tasks. The learned representations show transferability to real photographs. The contributions of the paper include the introduction of CRL, the use of learned representations in embodied tasks, and interpretable results on real images.