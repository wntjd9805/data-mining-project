Given its success in the 2D realm, deep learning has expanded to the 3D vision domain, achieving impressive results in classification, segmentation, and detection. While some methods directly operate on 3D data represented as point clouds, meshes, or voxels, others choose to represent 3D information by rendering multiple 2D views. These multi-view methods bridge the gap between 2D and 3D learning by leveraging 2D convolutional architectures and larger image datasets. However, the selection of rendering viewpoints for these methods remains largely unexplored, often relying on heuristics or predefined viewpoints. To address this limitation, we propose a Multi-View Transformation Network (MVTN) that learns to regress view-points and renders views with a differentiable renderer. MVTN is trained in an end-to-end fashion, leading to the most suitable views for the task at hand. Our approach is inspired by the Spatial Transformer Network (STN) and Vision as Inverse Graphics (VIG) paradigms. We demonstrate that integrating a learnable approach to view-point prediction in multi-view methods using a differentiable renderer improves 3D shape recognition tasks, achieving state-of-the-art performance on standard benchmarks. Additionally, MVTN enhances the robustness of multi-view approaches to rotation and occlusion, making it more applicable to realistic scenarios.