Deep Neural Networks (DNNs) have been widely used to enhance the retrieval performance in Cross-Modal Hamming Retrieval (CMHR). These networks capture the implicit structure of cross-modal data and derive binary codes from deeper layers. While DNN architecture effectively builds cross-modal correlations, little attention has been given to the robustness and stability of these structures. Adversarial attacks, which create visually imperceptible perturbations to deceive DNN models, have become a growing concern. This paper focuses on a practical cross-modal Hamming adversarial attack that satisfies two criteria: the attack operates in a black-box setting, where the attacker can only interact with the target model through querying, and the efficiency of querying is prioritized to minimize the risk of detection. Unlike existing adversarial attacks that focus on image-based classification within a single modality, this paper investigates the impact of adversarial examples on deep Hamming learning in CMHR. A triplet-based cross-modal Hamming attack is proposed, which manipulates the similarity structure of the query instance by adding adversarial perturbations. The proposed Adversarial Attack on Deep Cross-Modal Hamming Retrieval (AACH) method attacks deep cross-modal Hamming retrieval models in a black-box setting, without prior knowledge of the target model. A cross-modal triplet construction module is also introduced to boost the learning of adversarial perturbations by exploiting positive and negative instances in Hamming space. Experimental evaluations on popular benchmarks demonstrate the effectiveness of the proposed method in attacking state-of-the-art cross-modal retrieval models.