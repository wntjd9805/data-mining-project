The introduction of this computer science paper discusses the challenges of 3D object detection from point clouds, particularly in the context of autonomous vehicles. The paper highlights the limitations of existing 2D detection algorithms and the need for specialized approaches to process unstructured point clouds. The authors present different methods, such as projection to a birds-eye view representation and rasterization into a 3D voxel grid, but emphasize the trade-offs involved in sacrificing geometric details or dealing with computational bottlenecks. The paper also discusses the use of deep architectures for point cloud representation, as well as the two-stage framework commonly adopted for 3D object detection. The authors identify the limitations and drawbacks of existing models, such as involving hand-crafted designs and sensitivity to hyperparameters. In response to these challenges, the authors propose a novel two-stage 3D object detection framework called CT3D, which incorporates elements of the Transformer architecture to enhance feature extraction and representation. The paper highlights the benefits of the custom Transformer model over traditional feature aggregation mechanisms and discusses the use of self-attention to address missing/noisy detections. The proposed framework is evaluated through extensive experiments and shown to outperform state-of-the-art methods on benchmark datasets.