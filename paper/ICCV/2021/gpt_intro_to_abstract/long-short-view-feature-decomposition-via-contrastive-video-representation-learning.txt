Learning rich video representations is crucial for general video understanding and various downstream tasks in computer vision, such as action recognition, retrieval, and action segmentation. However, supervised learning approaches tend to focus more on static features, limiting their ability to handle more complex tasks that require temporal attributes of videos. To address this, we propose a method that explicitly learns and represents both stationary and non-stationary features in videos. We argue that self-supervised learning, which extracts features based on the underlying structure of the data, can provide a more diverse and descriptive representation. While previous self-supervised methods have focused on capturing temporal features, they do not explicitly differentiate between stationary and non-stationary features. In contrast, our method decomposes the feature space into these two types of attributes, enabling us to solve a broader range of downstream tasks. We apply contrastive learning to long and short views of videos, but instead of assuming similarity between all features, we differentiate between stationary and non-stationary features. We divide the features into two separate subsets and apply separate contrastive losses to each subset. We evaluate our method on the Kinetics dataset without using any labels and show superior performance compared to a strong contrastive learning baseline. Our proposed method also achieves state-of-the-art retrieval results and provides insights into the differences between stationary and non-stationary features. Overall, our work contributes to improving video representation learning and expanding the capabilities of video understanding algorithms.