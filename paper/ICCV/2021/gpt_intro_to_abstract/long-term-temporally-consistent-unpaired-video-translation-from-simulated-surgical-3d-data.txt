This paper introduces a novel approach for unpaired video translation using GAN-based image translation and neural rendering. The focus is on transferring simulated domain images to realistic ones, specifically in the field of computer-assisted surgery. The availability of realistic synthetic data is crucial in CAS, where labeled training data and realistic environments for evaluation are limited. The proposed approach utilizes the geometry and camera trajectories of the simulated domain to achieve long-term temporal consistency. A neural renderer is extended to learn global texture representations, allowing the model to render details consistently across different viewpoints. Lighting-invariant view-consistency loss is introduced to ensure texture consistency, and methods are utilized to maintain label consistency when translating to realistic images. Experimental results show that the generated video sequences retain detailed visual features and preserve label consistency and optical flow between frames.