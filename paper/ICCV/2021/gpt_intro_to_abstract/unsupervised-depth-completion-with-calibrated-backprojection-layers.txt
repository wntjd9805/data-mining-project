This paper addresses the issue of creating a dense depth map by leveraging the complementary information from optical and range sensors. The authors propose a deep neural network architecture that includes a sparse-to-dense module and calibrated backprojection layers. Unlike previous models, the camera intrinsics are treated as inputs to the network, allowing for easier transferability to different sensor platforms. The network is trained unsupervised using the Photometric Euclidean Reprojection Loss, and additional penalties are applied to promote depth map variation and sparse point reconstruction. Experimental results show that the proposed method outperforms baseline and state-of-the-art approaches in terms of depth map accuracy, both outdoors and indoors, with the use of same or different calibrations. The smaller computational footprint of the network is also highlighted, thanks to the inductive bias introduced by the calibrated backprojection layers.