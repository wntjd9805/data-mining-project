Visual object tracking is an important research topic in computer vision, and recent advancements have been made using convolutional neural networks. However, these models are limited in their ability to capture long-range dependencies in image contents and features. Existing trackers primarily rely on convolutional operations, which only model local relationships and struggle to capture global interactions. This limitation hinders their performance in scenarios where global contextual information is important, such as objects undergoing large-scale variations. To address this issue, we propose a new end-to-end tracking architecture using encoder-decoder transformers. Transformers have been successful in sequence modeling tasks and have shown promise in computer vision models. Our architecture integrates both spatial and temporal information for object tracking and generates discriminative spatio-temporal features for localization. The proposed architecture consists of an encoder, a decoder, and a prediction head, which collectively capture and predict the spatial positions of the target object. Extensive experiments demonstrate that our method outperforms existing trackers on both short-term and long-term tracking benchmarks. Notably, our framework is simpler compared to previous long-term trackers and achieves real-time speed. Furthermore, we introduce a new large-scale tracking benchmark to address over-fitting issues on small-scale datasets. Overall, our work makes several contributions in advancing visual object tracking using transformer architectures.