Deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples, which are created by adding imperceptible but malicious noise to natural examples. This vulnerability poses a potential threat to decision-critical deep learning applications. Previous research has explored pre-processing based defenses, but they can suffer from the error amplification effect and may be less effective against unseen adversarial attacks. In this paper, we propose a defense method that leverages class activation features to remove adversarial noise. We design a self-supervised adversarial training mechanism that maximally disrupts the class activation features of natural examples to craft adversarial examples. We then train a denoising model to minimize the distances between the class activation features of natural and adversarial examples. Empirical experiments demonstrate that our method provides significant protection against unseen and adaptive attacks compared to state-of-the-art approaches. The remainder of this paper discusses related work, describes our defense method and its implementation, presents experimental results, and concludes. Our contributions include the observation that adversarial noise significantly disrupts class activation features, the development of a class activation feature-based denoiser, and the proposal of a self-supervised adversarial training mechanism.