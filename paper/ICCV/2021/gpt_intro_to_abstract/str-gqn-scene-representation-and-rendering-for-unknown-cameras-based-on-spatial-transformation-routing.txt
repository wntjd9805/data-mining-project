Understanding the 3D structure of scenes from 2D observations is a crucial task in computer vision. Previous approaches have utilized geometry-based models and machine learning techniques to recover 3D geometry from 2D views. However, these methods do not consider spatial properties and have limited generalization capabilities. In this paper, we propose a novel model, Spatial Transformation Routing Generative Query Network (STR-GQN), that separates spatial transformation and feature extraction processes. Our model learns the content-independent concept of spatial cognition without relying on explicit geometry priors. We introduce the Spatial Transformation Routing (STR) mechanism, which treats the transformation between world space and view space as a message-passing process. Additionally, we develop the Occupancy Concept Mapping (OCM) technique to fuse features from multiple observations and constrain the scene representation scale. Evaluation results on various benchmarks demonstrate that our STR-GQN model outperforms the baseline GQN model in terms of both performance and generalization ability. We also visualize the message passing results to elucidate how our model achieves spatial transformation.