Video inpainting is a problem that involves filling in missing regions in a video sequence with spatial and temporal consistency. This task is useful for video editing purposes, such as removing watermarks and unwanted objects. However, existing approaches to video inpainting struggle to consistently produce visually pleasing results with long-range consistency. Traditional methods rely on patch-based optimization strategies, which have limited ability to capture complex motion or synthesize new content. More recent flow-guided methods use optical flow to achieve temporal consistency, but obtaining accurate optical flow in the missing region can be difficult. Deep models trained on large video datasets have shown promising performance, but the collection process for these datasets is time-consuming and labor-intensive, and they may not perform well on videos from different domains. Another recent approach, internal learning, overcomes the domain gap problem but still relies on externally-trained optical flow estimation for context propagation, leading to incorrect or inconsistent results. In this paper, we propose a new internal learning method for video inpainting that addresses these challenges through implicit long-range propagation. Rather than relying on explicit correspondences like optical flow, we leverage the intrinsic properties of natural videos and convolutional neural networks to propagate information across frames. We demonstrate the effectiveness of our approach on the DAVIS dataset and in various video domains, achieving state-of-the-art performance and providing promising results across different scenarios, including videos with single-frame masks and super high-resolution image sequences. Our contribution includes the development of an internal video inpainting method that achieves state-of-the-art performance and can be applied to videos in various domains, the design of two regularization terms to address ambiguity and deficiency problems in challenging video sequences, and the demonstration of our approach's feasibility in removing objects from 4K videos with only a single-frame mask.