Computer vision models heavily rely on large training image datasets, but most of these datasets are biased in some way. Traditional lab-controlled datasets are often too small and not diverse enough to train robust models. To overcome this, large-scale datasets have been created through web-scraping and crowdsourced annotations. However, this practice introduces dataset bias. It is important to understand the causes of biases, which problems, datasets, or models suffer from biases, and which methods are effective in mitigating biases. Machine learning models have been shown to learn bias from data, leading to biased outcomes that undermine fairness and social trust. In this paper, we investigate the presence of systematic annotation bias in large in-the-wild datasets, specifically focusing on facial expression recognition. We hypothesize that biases exist in these datasets due to cultural or societal biases from annotators. We aim to answer the question of whether annotators are equally likely to assign different expression labels to males and females. Most debiasing techniques focus on dataset bias, but we show that existing techniques fail to fully mitigate biases arising from annotations. Thus, we propose an AU-Calibrated Facial Expression Recognition framework that uses facial action units and the triplet loss to mitigate annotation biases effectively. Our framework outperforms existing debiasing techniques and can be adapted to other applications that require subjective human labeling. The contributions of this paper include comparing annotation bias in lab-controlled and in-the-wild datasets, demonstrating the learned biases by trained models, and proposing a novel framework for removing annotation bias in facial expression recognition.