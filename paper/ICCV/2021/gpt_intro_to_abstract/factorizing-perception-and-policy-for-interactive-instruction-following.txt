Abstract: This paper addresses the challenge of building a robotic assistant that can follow natural language instructions to perform daily chores. Previous research has made significant progress in individual components of instruction following agents, such as navigation and object interaction. However, to create realistic assistants, the agent should possess all these abilities. In this work, we focus on the task of interactive instruction following, which requires the agent to navigate, interact with objects, and complete long-horizon tasks based on egocentric vision and language directives. Inspired by the human visual cortex model, we propose a Modular Object-Centric Approach (MOCA) that factorizes perception and action policy into separate streams. Our agent consists of an action policy module (APM) for sequential action prediction and an interactive perception module (IPM) for object localization. Experimental results on the ALFRED benchmark show that our approach significantly outperforms previous works in terms of various evaluation metrics. We also introduce object-centric localization and an obstruction evasion mechanism to improve the agent's performance. Our proposed method demonstrates the effectiveness of factorizing perception and policy for embodied interactive instruction following tasks. Qualitative and quantitative analyses are presented to support our claims.