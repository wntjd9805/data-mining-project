The increasing popularity and demand for digital videos has led to a growing need for video understanding. Two essential components of video understanding are identifying the action components present in the video and localizing these actions in both the temporal and spatial axes. To effectively perform video understanding tasks, it is crucial to learn good video representations that encode relevant features. This paper focuses on temporal action detection in untrimmed videos, with applications in content-based video searching, video highlight generation, and surveillance.With the availability of large-scale action recognition and detection datasets and high-performance computing services, deep learning approaches have achieved significant success in learning video representations. These approaches involve comprehensive encoder learning processes, such as training an action recognition model on a specific dataset, fine-tuning the model on another dataset, and extracting action class-semantic rich features. These features are then encoded using a localization encoder for action localization tasks.However, the importance of these encoded features can vary for different videos. Previous research on attention mechanisms in convolutional neural networks has shown that attending to more important feature channels or locations can speed up training and improve network performance. Despite this, no prior action localization networks have incorporated attention mechanisms to attend to important encoded features.In this paper, we propose a novel attention mechanism that computes the relative importance of encoded features based on class-specific semantically rich features extracted by the action recognition model. We hypothesize that the importance of encoded features depends on the action classes present in the video. Our attention mechanism considers both the channel and temporal axes of the encoded features and can be easily implemented with self-attention methods.We demonstrate the effectiveness of our attention mechanism by applying it to baseline ConvNets on two major action detection datasets. Our results show considerable improvements in action localization performance. Furthermore, our ablation studies indicate that our attention mechanism can provide additional benefits when used together with self-attention mechanisms.