This paper introduces a new task called Generic Event Boundary Detection (GEBD) in the field of computer vision. The authors argue that current mainstream video models still process short clips and lack the ability to naturally segment events into shorter temporal units, as humans do. To address this, the authors propose the GEBD task which aims to localize moments in videos where humans perceive event boundaries. They create a benchmark dataset called Kinetics-GEBD and compare it with other video boundary datasets, highlighting its advantages such as a larger number of boundaries, a broader range of video domains, and open-vocabulary annotations. The authors outline the challenges in annotating taxonomy-free event boundaries and present novel annotation guidelines. They also evaluate supervised and unsupervised methods on the TAPOS dataset and Kinetics-GEBD. Lastly, the authors discuss the potential applications of event boundaries in video editing, summarization, keyframe selection, and highlight detection. The contributions of the paper include the new GEBD task and benchmark, the annotation task design principles, the evaluation of methods, and the demonstration of the value of event boundaries in downstream applications.