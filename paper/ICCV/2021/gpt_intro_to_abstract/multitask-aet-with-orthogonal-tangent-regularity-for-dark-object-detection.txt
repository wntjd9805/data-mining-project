Low-illumination environments present challenges in computer vision, and current approaches for image enhancement/restoration may generate artifacts that hinder high-level visual understanding tasks. Additionally, existing methods suffer from target inconsistency and data inconsistency, focusing on either human or machine vision without benefiting each other. In this paper, we propose a novel framework called MAET, which encodes the intrinsic structure of low-light-degrading transformations and performs object detection based on this robust representation. We also introduce the disentangling of multitask outputs to prevent overfitting of object-detection features into the degrading parameters. Our MAET framework is compatible with mainstream object architectures and shows superior performance in low-light object detection tasks compared to other methods.