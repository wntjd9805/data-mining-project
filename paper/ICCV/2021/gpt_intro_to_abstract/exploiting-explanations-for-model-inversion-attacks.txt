The recent success of artificial intelligence (AI) has led to its widespread application in various domains. However, with increasing regulatory requirements for responsible AI, these applications must now incorporate explainability and privacy. Many AI applications, such as driver drowsiness detection and classroom facial engagement prediction, require both explanations for users to dispute model predictions and privacy protection to preserve anonymity. However, recent model inversion attacks have demonstrated that attackers can reconstruct sensitive information, such as faces, from model predictions. This paper focuses on the privacy threats of model explanations and proposes attack methods based on saliency maps. The authors also propose an XAI-aware attack on non-explainable target models, which highlights the increased risk due to explanations in surrogate models. The contributions of this work include determining the privacy threat of model explanations, proposing an XAI-aware attack, and analyzing the privacy risk of different explanation types. The study emphasizes the urgent need for privacy defense techniques and models optimized for explainability and privacy in responsible AI.