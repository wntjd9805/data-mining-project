360° videos have become increasingly popular as a medium to record real-life scenery due to their ability to capture entire surroundings without restriction in the field of view. However, the wide field of view of 360° videos presents new challenges in visual understanding, including spherical spatial reasoning and audio-visual reasoning. Previous research has addressed these challenges individually, but no known task incorporates linguistic queries to tackle these tasks in the 360° video domain. In this paper, we propose spatial and audio-visual question answering on 360° videos as a novel benchmark task for 360° video understanding. We introduce the Pano-AVQA dataset, which consists of annotated panoramic videos with audio, video, and relationship description pairs. We contribute 20K spatial and 31.7K audio-visual question-answer pairs with bounding box grounding from 5.4K panoramic video clips. We propose a transformer-based question answering framework that fuses information from the panoramic surroundings, utilizing a quaternion-based coordinate representation for accurate spatial representation and an auxiliary task of audio skewness prediction. Our main contributions include proposing novel benchmark tasks, contributing the Pano-AVQA dataset, and designing a model that effectively fuses multimodal cues from the panoramic sphere.