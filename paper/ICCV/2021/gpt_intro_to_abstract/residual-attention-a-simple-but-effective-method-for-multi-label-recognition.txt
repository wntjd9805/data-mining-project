Convolutional neural networks (CNNs) have been highly successful in image classification tasks, but there have been limited advances in multi-label recognition. Existing approaches focus on semantic relations, object proposals, and attention mechanisms, but suffer from high computational cost or complex implementation. In this paper, we propose a class-specific residual attention module (CSRA) that fully utilizes spatial attention for each object class separately. The CSRA module is simple, easy to train, and has negligible computational cost. We demonstrate its effectiveness on various pretrained models and datasets, achieving state-of-the-art performance in multi-label recognition. The proposed module also provides an intuitive interpretation of how spatial attention is integrated. Our contributions include an effortless method to improve pretrained models, the CSRA module for multi-label recognition, and an intuitive interpretation of the attention mechanism.