Deep Neural Networks have been shown to be vulnerable to adversarial examples, where small perturbations are added to clean inputs, leading to misclassification. Various defenses have been proposed, with adversarial training being one of the most effective. However, it has been found that adversarial training can lead to poor generalization, with a significant gap between training and test accuracy. In this paper, the authors investigate the impact of activation functions on adversarial training and robust generalization. They show that the curvature of the activation function plays a crucial role, and activations with low curvature act as efficient regularizers, reducing the robust generalization gap. The authors also explore the relationship between activation functions and standard generalization, finding that activations with a large robust generalization gap also have a large standard generalization gap. They further demonstrate that non-smooth activations with low approximate curvature can achieve similar regularization effects as smooth activations. Finally, the authors investigate the phenomenon of double descent generalization curves in both standard and robust training and show that activation functions with low curvature, such as SiLU, deviate from the double descent curves observed with ReLU activations. Overall, this work provides insights into the impact of activation functions on adversarial training and highlights findings that can be applied to training adversarially robust models.