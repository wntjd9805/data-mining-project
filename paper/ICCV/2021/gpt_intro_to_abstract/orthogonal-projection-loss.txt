In recent years, deep neural networks (DNNs) have achieved significant progress in computer vision tasks, thanks to effective loss functions for training these networks. The softmax cross entropy (CE) loss has been widely used for classification problems and has played a crucial role in stable and efficient DNN training. However, there are limitations to CE and other existing loss functions in terms of promoting inter-class separation and intra-class clustering.This paper introduces a novel loss function called Orthogonal Projection Loss (OPL), which simultaneously enforces inter-class separation and intra-class clustering through orthogonality constraints on feature representations in the penultimate layer of any DNN. The motivation for this approach comes from the assumption of independent output classes in image classification and the geometric structure provided by orthogonality constraints. Unlike other loss functions that rely on complex negative sample mining procedures or introduce additional learnable parameters, OPL operates directly on mini-batches and does not require any additional parameters. It also integrates well with CE by augmenting its intrinsic angular property.Compared to contrastive loss functions, OPL eliminates the need for negative sample mining and operates directly on mini-batches. Unlike methods that enforce Euclidean margins in feature space, OPL enforces orthogonality through dot-products between feature vectors. It is also architecture-independent and performs well on a wide range of tasks.The main contributions of this paper are:1. The proposal of OPL, a loss function that enforces inter-class separation and intra-class clustering through orthogonality constraints without any additional parameters.2. Efficient formulation of the orthogonality constraints, allowing for mini-batch processing without the need for explicit computation of singular values.3. Extensive evaluation on various image classification tasks, few-shot learning, domain generalization datasets, demonstrating the discriminative ability, transferability, and generalizability of features learned with OPL.4. Improved robustness of learned features to adversarial attacks and label noise.Overall, OPL provides a promising approach to enhancing the discriminativity and generalizability of feature representations learned in deep neural networks for computer vision tasks.