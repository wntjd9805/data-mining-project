Generative Adversarial Networks (GANs) have gained significant attention in the field of computer science since their inception. The advancement of methods like BigGAN and StyleGAN has greatly improved the photorealism of synthetic images, making them useful for content creation and dataset augmentation. However, the problem of GAN replication or memorization is of theoretical and practical importance. Several studies have explored the use of GANs to augment training datasets, especially in medical applications where data scarcity is an issue. The replication of training samples can hinder the performance of downstream machine learning algorithms and raise copyright infringement concerns. It can also pose challenges for preserving patient privacy in de-identification techniques. To gain a deeper understanding of the replication behavior of GANs, researchers have examined various factors contributing to GANs replication/memorization. However, the role of dataset size and complexity in GANs replication has not been extensively explored. This paper aims to bridge this gap by empirically studying the relationship between GANs replication and dataset size/complexity. The findings challenge the common belief that GANs do not tend to memorize training data under normal training setups and provide practical guidance for estimating the minimal dataset size required for image synthesis purposes.