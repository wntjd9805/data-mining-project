3D scene synthesis is a challenging task in deep generative modeling due to the diverse patterns and constraints exhibited by 3D scenes. Existing approaches that develop a single data representation and training approach are insufficient. In this paper, we propose a novel approach that synthesizes 3D scenes by representing objects using attributes such as size, pose, existence indicator, and geometric codes. Our approach combines different viewpoints and strengths to capture diverse feature patterns and enforce different constraints. At one level, instead of synthesizing only the absolute attributes of each individual object, our approach predicts an over-complete set of attributes that includes relative attributes between object pairs. These relative attributes capture spatial correlations among objects and possess consistency constraints, allowing us to prune infeasible attributes. This approach is particularly suitable for neural outputs that exhibit weak correlations and helps to suppress output errors effectively. At another level, our approach combines neural scene synthesis models with conventional scene generation methods. Neural models offer unbounded expressibility but do not provide signals of uncertainties. We address this by learning parametric prior distributions of object attributes and relative attributes, which provide uncertainties and help regulate them and prune outliers. We propose a Bayesian framework to seamlessly integrate neural outputs and parametric prior distributions, optimizing the hyperparameters to maximize the performance of the final output. We evaluate our approach on 3D-FRONT and SUNCG datasets, showing that it can generate 3D scenes with diverse feature patterns, outperforming baseline techniques.