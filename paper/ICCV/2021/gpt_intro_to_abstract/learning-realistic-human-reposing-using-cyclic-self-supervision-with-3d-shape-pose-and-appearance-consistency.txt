This paper introduces the problem of generating realistic images of a person from different viewpoints and poses using a single source image. While this problem is inherently ambiguous, there are statistical regularities in human pose, clothing, and appearance that could make this possible. The solution to this problem has various applications in online fashion, gaming, personal avatar creation, and animation, which has led to significant research interest. Recent work in generative modeling has focused on conditional image synthesis using supervised training with paired data. However, this approach limits the potential size of the training set and can impair robustness and generalization. In this paper, the authors propose a self-supervised approach that does not require paired data. Their method builds on the idea of cycle-consistency, where a source image, source pose, and target pose are used to generate a target image conditioned on pose and appearance. The reverse direction regenerates the source image by switching the source and target conditions. To address the problem of a trivial solution, where the cycle produces the identity mapping, the authors introduce constraints using 3D information extracted from images. They propose a method called SPICE (Self-supervised Person Image CrEation) that leverages the estimated 3D body model to constrain image generation. This is done through pose and shape consistency losses. However, these constraints alone are not sufficient to generate images with the correct appearance. To solve this, the authors introduce pose-dependent appearance consistency on the body surface. Extensive experiments demonstrate the effectiveness of their model, outperforming prior state-of-the-art methods and approaching the accuracy of supervised methods. Overall, this paper proposes a novel self-supervised approach for generating realistic images of people from different viewpoints and poses, leveraging 3D body information in multiple ways.