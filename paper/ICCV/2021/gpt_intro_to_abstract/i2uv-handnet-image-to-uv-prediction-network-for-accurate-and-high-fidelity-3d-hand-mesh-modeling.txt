Observing and understanding the human hand is crucial in the fields of computer vision and human-computer interaction, with applications ranging from gesture recognition to augmented reality. Recent advancements in 3D hand pose and shape estimation have been driven by large-scale data collection, annotation, as well as the development of 3D representations and learning methods. However, existing methods for 3D pose estimation and hand surface reconstruction have limitations, such as sparse joint representation and the need for intermediate representations. In this paper, we propose a novel approach that uses UV position maps as the hand representation and introduces AffineNet for 3D hand shape estimation and SRNet for hand mesh super-resolution reconstruction. Our method achieves accurate and high-fidelity hand reconstruction from RGB inputs, surpassing state-of-the-art methods on multiple datasets. Additionally, we introduce SuperHandScan, a scan dataset, to train SRNet, enabling the reconstruction of high-fidelity hand meshes from coarse meshes. Overall, our contributions include the introduction of UV map representation, the development of AffineNet and SRNet, and the demonstration of superior performance in hand reconstruction.