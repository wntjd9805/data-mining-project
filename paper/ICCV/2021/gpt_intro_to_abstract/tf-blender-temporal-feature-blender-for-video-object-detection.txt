In recent years, there has been a growing interest in extending learning-based computer vision techniques from image tasks to video tasks. Video tasks, such as object detection, video instance segmentation, and multi-object tracking and segmentation, have valuable potentials for real-world applications. However, video object detection poses a challenge due to feature degradation caused by camera jitter or fast motion.Previous studies have explored the use of temporal information in video object detection through post-processing methods. These methods perform still-image detection on single frames and then assemble the detection results across temporal dimensions. However, these methods are not end-to-end and cannot improve weak predictions from single frame detection.Another approach to improving video object detection is through feature aggregation. These methods leverage optical flow to model the feature movement across frames and propagate temporal features to enhance feature representation. However, existing aggregation methods oversimplify the feature aggregation process.This paper proposes TF-Blender, a framework for video object detection that models temporal feature relations and blends neighboring features to enhance the temporal-spatial feature representation. TF-Blender incorporates a temporal relation module and a feature adjustment module to preserve spatial information during feature aggregation. The proposed method achieves significant improvements in detection performance compared to state-of-the-art methods on benchmark datasets.Overall, this paper presents a novel framework for video object detection that effectively utilizes temporal information and improves detection performance through feature aggregation. The proposed method provides a general and flexible solution that can be applied to any detection network.