Abstract:Person re-identification (reID) is the task of retrieving person images from multiple cameras with the same identity as a query person. This paper focuses on visible-IR person re-identification (VI-reID), which involves retrieving infrared (IR) person images that match RGB query images and vice versa. VI-reID is challenging due to various factors such as intra-class variations, noisy samples, and cross-modal discrepancies between RGB and IR images. Existing VI-reID methods based on convolutional neural networks (CNNs) have addressed these challenges to some extent but mainly focus on coarse image-level or rigid part-level representations. This paper proposes a new framework that leverages dense cross-modal correspondences between RGB and IR images for VI-reID. The framework encourages person representations of RGB images to reconstruct those of IR images of the same identity and vice versa, incorporating person masks and introducing novel ID consistency and dense triplet losses. These dense cross-modal correspondences align pixel-level person representations, which improves person representation learning for VI-reID by promoting feature invariance and focus on extracting discriminative local features. The experimental results on standard VI-reID benchmarks demonstrate the effectiveness and efficiency of the proposed framework, achieving state-of-the-art performance. The contributions of this paper include the novel feature learning framework, the introduction of ID consistency and dense triplet losses, and the achieved improvements in VI-reID performance.