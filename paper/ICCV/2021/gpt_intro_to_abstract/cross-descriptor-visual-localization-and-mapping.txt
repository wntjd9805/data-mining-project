Mixed reality and robotics aim to merge physical and digital worlds, necessitating the ability to build and localize against maps of the environment. However, current systems primarily rely on vision sensors and local features for image matching, running in real-time on the devices. The industry has been shifting localization and mapping to the cloud to reduce onboard compute and enable collaborative experiences, but images are not shared across devices due to privacy and storage concerns. This poses limitations on adopting new feature algorithms and co-localization with devices using different features. To address these challenges, this paper defines three novel scenarios: continuous deployment of feature representations, cross-device localization, and collaborative mapping. It proposes a principled and scalable approach that translates descriptors from one representation to another, facilitating matching of features with incompatible dimensionality and distance metrics. The method is computationally efficient and can be deployed on low-compute devices. The paper evaluates the approach on various geometric tasks, demonstrating its effectiveness in real-world localization and mapping systems. Overall, the contributions include introducing novel scenarios, proposing a scalable approach, and demonstrating its efficacy on challenging datasets.