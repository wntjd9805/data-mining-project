Convolutional neural networks (CNNs) have achieved remarkable success in computer vision tasks such as image recognition, object detection, and semantic segmentation. However, the growing depth and width of CNNs require a large number of parameters and computations, making it challenging to deploy these networks on edge devices with limited resources. Network compression techniques, such as pruning and distillation, have been proposed to reduce the size and computational cost of CNNs. Nevertheless, these techniques still rely on floating-point computations, which are not efficient for edge devices that favor fixed-point operations.An alternative approach to network compression is network quantization, which converts the weights and activations of a CNN into low-precision representations. This enables fixed-point inference, reducing memory usage and computational cost. Existing quantization methods often use staircase functions and discretizers to convert full-precision values to their quantized counterparts. However, these approaches suffer from gradient mismatch and quantizer gap problems, making the training process noisy and degrading the quantization performance.In this paper, we propose a distance-aware quantizer (DAQ) that addresses the challenges of gradient mismatch and quantizer gap in a unified framework. Our approach is based on the observation that the discretizer selects the nearest quantized value by computing distances between the full-precision input and the quantized values. We introduce a distance-aware soft rounding (DASR) that accurately approximates the discrete rounding function using a kernel soft argmax. This alleviates the gradient mismatch problem while maintaining differentiability. Additionally, we incorporate a temperature controller that adjusts the temperature parameter in DASR adaptively, ensuring that it matches the behavior of the discrete rounding function. Our DAQ is applicable to various network architectures and achieves state-of-the-art results on standard benchmarks.The main contributions of this paper are: 1) the proposal of DASR, a differentiable approximation of the discrete rounding function that enables end-to-end training of quantized networks while addressing the gradient mismatch problem, 2) the introduction of a temperature controller to mitigate the quantizer gap problem, and 3) the demonstration of state-of-the-art performance on standard benchmarks, accompanied by a thorough analysis of our approach. Our work is the first to jointly alleviate both the gradient mismatch and quantizer gap problems in network quantization.