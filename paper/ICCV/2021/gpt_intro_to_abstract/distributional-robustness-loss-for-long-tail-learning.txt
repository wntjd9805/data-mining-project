This paper addresses the problem of learning representations with unbalanced data in deep models. Existing techniques focus on rebalancing the multi-class classifier, but little attention has been given to the latent representations. The authors propose a new loss, called DRO-LT loss, based on distributionally robust optimization, to balance the representation layer. They show that large gains in accuracy can be achieved by balancing the representation at the last layer. The DRO-LT loss takes into account the uncertainty and variability of tail representations and minimizes the worst-case distribution within a neighborhood of the training distribution. The paper presents an upper bound for the DRO-LT loss that can be computed efficiently and evaluates the proposed approach on four long-tailed visual recognition benchmarks, showing superior performance compared to previous models. The contributions of the paper include formulating learning with unbalanced data as a problem of robust optimization, developing the DRO-LT loss, deriving an efficient upper bound for the loss, and evaluating the approach on multiple benchmarks.