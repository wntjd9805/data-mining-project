Visual place recognition (VPR) is a critical technique in computer vision and robotics applications such as autonomous driving, geo-localization, 3D reconstruction, and virtual reality. The main challenge in VPR is handling changes in appearance caused by different viewpoints, weather conditions, illumination, partial occlusion, and dynamic objects. This paper proposes an attentional encoding strategy called Attentional Pyramid Pooling of Salient Visual Residuals (APPSVR) to construct powerful image representations for VPR. The proposed strategy incorporates triple attention from individual, spatial, and cluster dimensions to identify and embed task-relevant visual cues into discriminative image descriptors. Experiments demonstrate that APPSVR outperforms existing methods and achieves state-of-the-art performance on benchmark datasets. The proposed attention modules learn saliency from both prior knowledge and data-driven fine-tuning, and the visualization shows that the learned attention is consistent with human perception. This work contributes to the development of effective encoding strategies for VPR in large-scale environments.