Prediction of future states in autonomous decision making systems, particularly in motion planning for autonomous driving, is challenging due to the influence of other road users and pedestrians. Advancements in camera-based perception have shown promise in rivaling LiDAR-based perception, providing a leaner and cheaper visual recognition system. However, most camera-based prediction models operate in the perspective view coordinate frame or simplified bird's-eye-view raster representations of the scene. This paper aims to build predictive models that operate in an orthographic bird's-eye-view frame without relying on auxiliary systems for scene representation. The concept of early sensor fusion, generating 3D object detections directly from image and LiDAR data, is explored to improve perception performance. Instead of relying on HD maps and road connectivity, the proposed system, FIERY, learns to predict future motion of road agents using camera driving data in an end-to-end manner. FIERY can reason about the probabilistic nature of the future and predicts multimodal future trajectories. The main contributions of this paper include presenting the first future prediction model in bird's-eye view from monocular camera videos, explicitly reasoning about multi-agent dynamics, predicting plausible and multimodal futures, and outperforming previous prediction baselines on benchmark datasets for autonomous driving.