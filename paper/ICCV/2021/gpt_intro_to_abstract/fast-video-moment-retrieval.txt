Video Moment Retrieval (VMR) is a critical task in video understanding, with applications in various domains such as robotics, autonomous driving, and video entertainment. Despite recent advancements, VMR remains challenging due to factors like complex video scenes and the gap between visual and textual features. This paper proposes a fast video moment retrieval (FVMR) framework that learns an efficient and effective moment-query common space. The framework consists of a video encoder, text encoder, fine-grained semantic extractor, and common space. A hierarchical semantic-guided attention module is introduced to leverage fine-grained semantic structures and improve performance. Experimental results demonstrate that the proposed method achieves high speed and significant performance gains compared to state-of-the-art approaches.