Abstract: This paper addresses the challenge of adversarial vulnerability in deep learning methods used for safety-critical applications. Adversarial training (AT) approaches aim to mitigate this vulnerability by training models on adversarial examples. However, existing AT methods have limitations such as high training time or susceptibility to sophisticated adversaries. To address these limitations, this study proposes a novel approach called single-step latent adversarial training (SLAT), which introduces latent adversarial perturbations in parallel. This approach reduces computational cost and improves the reliability of AT compared to existing single-step AT variants. The study demonstrates that SLAT effectively regularizes the local linearity of neural networks without increasing training time, and outperforms state-of-the-art accelerated AT methods while achieving performance comparable to PGD AT. This work contributes to reducing the computational cost of AT by introducing latent adversarial perturbations, which is the first of its kind.