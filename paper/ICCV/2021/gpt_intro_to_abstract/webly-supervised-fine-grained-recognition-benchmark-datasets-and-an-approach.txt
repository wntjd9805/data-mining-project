Recent advancements in deep learning have shown that utilizing deep networks alongside large, accurately labeled training data is the most promising approach for fine-grained recognition. However, constructing such datasets is still a challenging task, as distinguishing subtle differences among fine-grained categories typically requires domain-specific expert knowledge. To address this challenge and reduce the cost of manual annotation, various methods have been proposed, primarily focusing on semi-supervised learning. These methods involve human intervention and are labor-consuming. As a result, training directly from web images has become popular, but the lack of a benchmark dataset makes it difficult to compare the performances of different algorithms. In this paper, we aim to provide a benchmark dataset, called WebFG-496, for evaluating webly supervised fine-grained recognition algorithms. This dataset addresses three challenges: label noise, small inter-class variance, and class imbalance. To construct the dataset, we leverage categories from existing fine-grained datasets and reuse categories from iNat2017 to build a web version called WebiNat-5089. Furthermore, we propose a learning paradigm called Peer-learning, which involves training two deep neural networks simultaneously and allowing them to mutually correct their classification errors. Experimental results on the benchmark datasets demonstrate the effectiveness of our approach. The main contributions of this work are the construction of the WebFG-496 and WebiNat-5089 datasets, the proposal of the Peer-learning paradigm, and extensive experiments that validate the superiority of our method over existing approaches.