Multi-modal Transformers have the potential to revolutionize computer vision by combining text and image modalities. Prior research has shown that training image-text models using Transformers can enable the performance of multiple downstream tasks without further training, achieving comparable accuracy to the state of the art. However, existing explainability methods for Transformers heavily rely on self-attention and do not provide adaptations to other forms of attention commonly used in multi-modal Transformers. Additionally, Transformer encoder-decoder models, which are not restricted to self-attention, are widely used in object detection, image segmentation, and NLP tasks. In these models, embeddings of position-specific and class-specific queries are crossed with encoded image information. This paper proposes a novel explainability method applicable to all Transformer architectures and demonstrates its effectiveness on three commonly used architectures. The proposed method outperforms existing Transformer explainability methods and is easier to implement, making it suitable for any attention-based architecture.