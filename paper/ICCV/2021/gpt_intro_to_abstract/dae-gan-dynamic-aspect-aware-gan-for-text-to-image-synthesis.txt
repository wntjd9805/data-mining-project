Text-to-image synthesis, the generation of photo-realistic images based on textual descriptions, has gained significant attention in recent years due to its potential applications in various fields. Generative Adversarial Networks (GANs) have been successful in this task, with most existing methods following a two-stage framework. However, these methods often overlook the importance of aspect-level features, which refer to multiple words describing different perspectives of an object or scene. The relationship between aspect terms and sentences is crucial for semantic understanding, and the utilization of aspect information in text-to-image synthesis remains a challenge. Inspired by human learning behaviors, we propose a novel Dynamic Aspect-awarE GAN (DAE-GAN) that comprehensively encodes text information from sentence-level, word-level, and aspect-level granularities. Our approach involves a two-stage image generation process, where a low-resolution image is initially generated using sentence-level embedding and a refinement stage employs an Aspect-aware Dynamic Re-drawer (ADR) module. The ADR module uses word-level embedding for global enhancement and aspect-level embedding for local refinement of image details. We also design a matching loss function to ensure text-image semantic consistency during intermediate synthesis procedures. Experimental results demonstrate the effectiveness, superiority, and interpretability of our proposed method. Overall, our contributions include the integration and utilization of aspect information in text-to-image synthesis, the development of DAE-GAN with comprehensive text representation and aspect-aware refinement, and the validation of our approach through extensive quantitative and qualitative evaluations.