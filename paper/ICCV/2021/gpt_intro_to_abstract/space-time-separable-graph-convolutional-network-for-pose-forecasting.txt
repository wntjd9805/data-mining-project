The task of forecasting future human poses involves modeling the complex spatio-temporal dynamics of the human body. This has significant applications in autonomous driving, healthcare, teleoperations, and collaborative robots. Previous research has focused on modeling space and time separately, using convolutions, recurrent neural networks, or transformer networks for time modeling, and graph convolutional networks for spatial modeling. However, this separate approach limits the understanding of the complex dynamics of the human body. In this paper, we propose a novel Space-Time-Separable Graph Convolutional Network (STS-GCN) that encodes both the spatial and temporal correlations of joint interactions. This single-graph framework allows for better understanding of the body joint interactions and temporal motion patterns. We introduce a factorization technique to optimize the cross-talk between spatial joints and temporal frames, resulting in improved performance and reduced model parameters. We present an encoder-decoder design that utilizes TCN for future pose prediction and demonstrate through extensive experiments that STS-GCN outperforms existing techniques on various large-scale datasets. Our main contributions include the development of the first space-time separable GCN, an exclusively GCN-based human body representation with significantly fewer parameters than existing techniques, and improved performance on challenging long-term predictions. The learned graph edge weights allow for better interpretation of the joint-joint and time-time interactions.