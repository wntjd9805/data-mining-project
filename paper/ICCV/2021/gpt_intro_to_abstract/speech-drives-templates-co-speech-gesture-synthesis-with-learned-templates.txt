In this paper, we address the problem of synthesizing implicit behaviors of humans, specifically co-speech gestures, which refer to the movement of body parts when someone is speaking. While explicit behaviors such as lip syncing, face swapping, and pose retargeting have been extensively explored, synthesizing realistic co-speech gestures has been less explored. Early attempts at co-speech gesture synthesis used rule-based methods, but these suffer from poor naturalness. Later efforts focused on learning human behaviors from collected data, but the multi-modal nature of the mapping from speech audio to gestures poses a challenge. Adversarial learning has been used to narrow the gap between generated and real gestures, but improvements have been limited. In this paper, we propose a method that utilizes a Variational Auto-Encoder (VAE) to model the distribution of gesture sequences, allowing for the encoding of ground-truth gesture sequences into template vectors and the learning of a one-to-one mapping between the template vectors, speech audio, and ground-truth gesture sequences. We also propose including head motion in co-speech gestures for a more unified synthesis and ease of evaluation. To assess the fidelity of generated gesture sequences, we use the VAE to compute a Fr√©chet Template Distance (FTD) measuring the distribution similarity between generated and real gestures. Our contributions include an audio-driven gesture synthesis method that alleviates the ambiguity of synthesis, an objective evaluation measure using lip-sync error, and the demonstration of superior synthesis quality through both subjective and objective tests.