Neural Architecture Search (NAS) aims to automate the design of neural architectures, reducing the need for manual design. Recent advancements, such as Differentiable Architecture Search (DARTS), have made architecture search more efficient by relaxing the search space to be continuous and differentiable. However, DARTS still suffers from a large and redundant space of network architectures, leading to memory and computation overhead. This has prompted researchers to propose alternative formulations, such as PC-DARTS, that reduce redundancy without compromising performance. These alternatives, however, do not consider the intrinsic relationship between different parameters, resulting in sub-optimal architectures. To address this issue, we introduce a bilinear model into DARTS and develop a new backpropagation method to decouple the hidden relationships among variables and facilitate optimization. We formulate DARTS as a bilinear optimization problem and present Interactive Differentiable Architecture Search (IDARTS) as a solution. IDARTS coordinates the training of different parameters, fully exploring their interaction through backtracking. Experimental results on CIFAR10 and ImageNet datasets show that IDARTS outperforms existing DARTS approaches, achieving top-1 accuracies of 97.68% and 76.52%, respectively. Our contributions include formulating DARTS as a bilinear problem, introducing the backtracking method for parameter training coordination, and demonstrating the superior performance of IDARTS compared to prior arts.