In this paper, we address the task of 3D instance segmentation in the field of 3D scene understanding. We propose an end-to-end solution called Semantic Superpoint Tree Network (SSTNet) that directly proposes and evaluates object instances from observed 3D scenes. Our method works with superpoints, which are geometrically homogeneous neighborhoods, to leverage the geometric regularities and improve the consistency and sharpness of segmentations, particularly at object boundaries.SSTNet consists of a backbone that learns point-wise semantic and instance-level features, followed by a point-wise pooling step to efficiently aggregate these features as superpoint-wise ones. The key component of SSTNet is the semantic superpoint tree (SST), which is constructed based on the pooled semantic and instance-level features of superpoints. The SST is then traversed and split by the SSTNet module of binary classification to form proposals of object instances. To compensate for erroneous assignments of superpoints to instances during tree construction and traversal, we introduce a refinement module called CliqueNet. CliqueNet converts each proposed branch of the tree into a graph clique and learns to prune some of the branch nodes, improving the accuracy of the instance segmentation results.We evaluate the effectiveness of SSTNet on the benchmark datasets of ScanNet and S3DIS, and our method outperforms existing methods, ranking top on the ScanNet leaderboard with 2% higher mean average precision (mAP) than the second best method.In summary, our technical contributions include the proposal of an end-to-end solution for 3D instance segmentation using SSTNet, leveraging geometric regularities for improved segmentations, a strategy of divisive grouping for efficient training and inference, and a refinement module to address erroneous assignments.