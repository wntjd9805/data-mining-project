This paper focuses on the problem of few-shot learning, which aims to train networks to understand new concepts based on a few labeled examples. While deep learning approaches typically require a large amount of training data, few-shot learning attempts to mimic human ability to learn from few examples and generalize to new examples. The meta-learning framework for few-shot learning involves learning to learn by sampling few-shot classification tasks and optimizing the model to perform well on these tasks. However, recent works have shown that training for whole-classification, i.e., classification on the entire training label-set, can provide embeddings comparable or even better than meta-learning algorithms. This raises the question of why meta-learning, which aligns training and testing objectives, leads to worse embeddings. This paper explores the edge between whole-classification and meta-learning by decoupling the discrepancies. The authors propose a Classifier-Baseline method, which outperforms many recent meta-learning algorithms. They then investigate the effectiveness of meta-learning compared to whole-classification by performing meta-learning on a converged Classifier-Baseline model. The resulting method, called Meta-Baseline, achieves competitive performance on standard benchmarks. The authors further evaluate the method on two types of generalization and observe a trade-off between base class generalization and novel class generalization. They suggest that meta-learning focuses on improving N-way K-shot tasks, while whole-classification improves class transferability. The results also highlight the importance of dataset selection in comparing meta-learning and whole-classification. Overall, this paper presents a simple yet effective meta-learning method, provides insights into the objectives of meta-learning and whole-classification, and presents a trade-off between these objectives in few-shot learning.