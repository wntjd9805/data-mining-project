Semantic segmentation plays a crucial role in various applications, such as scene understanding and autonomous driving. Prior works in fully supervised semantic segmentation have achieved remarkable success using Convolutional Neural Networks (CNNs) but at the cost of high pixel-wise annotation. In response to the need for less expensive annotations, weakly supervised semantic segmentation (WSSS) approaches have emerged using image-level labels. These approaches typically follow a two-step pipeline of generating pseudo segmentation labels and training segmentation models, with class activation maps (CAM) being a key element in generating pseudo labels. However, CAM maps have coarse boundaries, and existing methods use off-the-shelf saliency maps as fixed binary cues, limiting their potential for improving segmentation performance. To address these limitations, we propose a weakly supervised multi-task deep network that leverages saliency detection and multi-label image classification as auxiliary tasks to assist in learning semantic segmentation. Through joint training, the network adapts pre-trained saliency maps to the target dataset and emphasizes object boundaries in segmentation outputs. Additionally, we propose to learn global pixel-level pairwise affinities from the features of segmentation and saliency tasks to guide the propagation of CAM activations. These affinities are used to refine saliency predictions and CAM maps, enabling multi-stage cross-task iterative learning and label updating. Our proposed method, called AuxSegNet, achieves state-of-the-art results on the PASCAL VOC 2012 and MS COCO datasets for weakly supervised semantic segmentation.