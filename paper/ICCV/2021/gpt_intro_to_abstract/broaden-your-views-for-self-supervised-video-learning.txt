In recent years, self-supervised learning methods have revolutionized representation learning by directly learning from data without the need for manually defined labels. This allows for leveraging large amounts of uncurated data and enables richer training tasks compared to traditional supervised learning approaches. While self-supervised learning has shown success in the domain of images, it has not fully taken advantage of the temporal complexity in video data. Most existing methods in the video domain follow a symmetric approach of extracting two views from a video clip in a symmetric fashion with respect to time, but this overlooks the potential benefits of learning from information at different time scales. In this paper, we propose the BraVe algorithm, which breaks this symmetry by predicting a broad view that spans the longer temporal context of the full video clip. By extrapolating to the general context in which events occur, BraVe improves representation learning from videos. We demonstrate that our framework outperforms alternatives relying on symmetric augmentation procedures and achieves state-of-the-art results on video and audio classification benchmarks. Our algorithm is simple and does not require the creation of explicit negatives, making it a promising approach for representation learning in the video domain.