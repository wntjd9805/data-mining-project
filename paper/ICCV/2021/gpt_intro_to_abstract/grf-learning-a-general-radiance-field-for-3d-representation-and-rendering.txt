This paper introduces a general radiance field (GRF) as a neural network that overcomes limitations in existing methods for 3D reconstruction and rendering. The GRF takes a set of 2D images, camera poses, and intrinsics, along with a 3D query point and viewpoint, and predicts the RGB value and volumetric density at that point. By leveraging multi-view geometry and an attention mechanism, the GRF is able to learn general geometric patterns for each query point, resulting in realistic rendered images with fine-grained details. The GRF outperforms existing approaches in terms of representing 3D content with continuous surfaces, learning 3D representations without 3D data, and generalizing to novel geometries. The paper presents qualitative results and demonstrates improvements over baselines on large-scale datasets.