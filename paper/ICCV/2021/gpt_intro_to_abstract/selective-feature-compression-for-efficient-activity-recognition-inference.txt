Recent advances in action recognition with 3D CNNs have been able to model both motion information and semantic signals, leading to improved performance. However, these models require a large amount of GPU memory and can only operate on short video clips, independently classifying each clip and pooling the results together for a final prediction. Selecting these clips is crucial, and dense sampling is considered the best solution but is highly inefficient. This paper proposes a novel technique called Selective Feature Compression (SFC), which eliminates the need for dense sampling by compressing the rich information of long video sequences into a smaller representation. SFC achieves significant improvement in inference speed without sacrificing accuracy. The proposed method involves splitting a pre-trained action network into two sub-networks and placing SFC in between, compressing the spatial-temporal representations using an attention-based mechanism. This compressed representation is then passed to the second sub-network for action prediction. SFC offers a trade-off between inference speed and accuracy and can be easily fine-tuned. Experimental results on popular datasets demonstrate that SFC maintains the same accuracy as dense sampling while improving inference throughput and reducing memory usage. The applicability of SFC on longer untrimmed videos is also investigated, showing improved performance and runtime. Visual analysis confirms that SFC focuses on the informative parts of a video during feature compression.