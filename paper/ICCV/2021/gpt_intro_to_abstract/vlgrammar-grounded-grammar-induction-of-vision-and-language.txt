This paper introduces the concept of visually grounded grammar induction, which aims to induce the underlying structures and grammars from raw sensory inputs such as vision and language. The authors argue that previous works in this field have focused on aligning constituents of a sentence with a single image, neglecting the hierarchical structures in the image. To address this, the authors propose VLGrammar, a framework that jointly learns image and language grammar using compound probabilistic context-free grammars. They also collect a large-scale dataset called PARTIT, which contains descriptive sentences paired with images of objects and parts, to train and evaluate VLGrammar. Experimental results show that VLGrammar outperforms baselines in image and language grammar induction, and it also improves performance on unsupervised part clustering and image-text retrieval tasks. The authors conclude that VLGrammar has great generalization ability on unseen object categories and is capable of predicting part-whole hierarchies and recursive structures of objects.