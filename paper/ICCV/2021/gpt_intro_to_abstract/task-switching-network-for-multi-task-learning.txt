This paper introduces Task-Switching Networks (TSNs), a new architecture for multi-task learning in computer vision. The authors highlight that existing methods in multi-task learning do not scale well with the number of tasks and can minimize positive interactions amongst tasks. TSNs aim to address these limitations by sharing all parameters among all tasks, avoiding the need for task-specific modules. The proposed architecture performs task-switching by employing a task embedding network to learn task-specific embeddings, which are used to condition the decoder. The authors demonstrate that conditioning a single shared decoder can outperform multi-decoder methods on heterogeneous tasks such as segmentation and regression. The TSNs also offer insights into the relationships between tasks through the structure of the task embeddings. Overall, this work presents a simple and efficient architecture for multi-task learning in computer vision.