Depth measurement is crucial in various applications like robotics, augmented reality, and self-driving vehicles. However, the high cost and continuous operation required by traditional depth measurement devices limit their widespread use. Monocular depth estimation, on the other hand, estimates the depth of pixels in a 2D image without the need for additional measurements, bridging the gap between the physical world and images. Convolutional neural networks (CNNs) have been successfully applied to improve the accuracy, temporal consistency, and depth ranges of monocular depth estimation. While most existing methods rely on supervised training with depth labels, which can be expensive and limited, self-supervised training methods have recently been proposed as alternatives.One such method, SfM-Learner, uses ensembles of consecutive frames in video sequences to train depth and pose networks, achieving performance comparable to supervised methods. However, this approach, as well as other methods based on it, mostly rely on photometric loss and smoothness constraints, resulting in limited supervision of weak texture regions and incorrect depth values, especially at object boundaries. To address these issues, recent methods have employed cross-domain knowledge learning, leveraging scene semantics and modeling object motion to improve monocular depth predictions. Regularization techniques have also been used to enforce depth and semantic consistency.In this paper, we propose a novel approach to self-supervised monocular depth estimation that utilizes semantic segmentation implicitly. We focus on representation enhancement and optimizing the depth network in the representation spaces to produce semantically consistent intermediate depth representations. Inspired by deep metric learning, we introduce a semantics-guided triplet loss that refines depth representations based on local geometric information from scene semantics. We also design a cross-task attention module to enhance the semantic consistency of depth features.Our contributions include the presentation of a training method that extracts semantics-guided local geometry through patch-based sampling and utilizes it in a metric-learning formulation to refine depth features. We also propose a new cross-task feature fusion architecture that fully leverages the implicit representations of semantics for learning depth features. We comprehensively evaluate the performance of our methods using the KITTI Eigen split dataset and demonstrate that our approach outperforms recent state-of-the-art self-supervised monocular depth prediction works in all evaluation metrics.