Visual Question Answering (VQA) is the task of answering natural language questions about an image. To ensure the reliability of VQA systems in real-world applications, such as assisting visually impaired users, it is essential for these systems to be robust to different ways of asking the same question. However, current VQA models are often brittle to minor linguistic variations in input questions. In this paper, we propose a training paradigm called ConClaT that makes VQA models robust to question paraphrases. We achieve this by minimizing contrastive and cross-entropy losses together. The contrastive learning step pulls representations of positive samples (paraphrased questions) closer together while pushing those with different answers farther apart. The cross-entropy step helps make these representations discriminative for accurate visual question answering. Previous approaches have augmented the training data with different variations of the input question but fail to exploit the information that some questions in the augmented dataset are paraphrases of each other. We overcome this limitation by introducing a supervised contrastive loss that encourages joint vision and language representations obtained from paraphrased question samples to be closer together. We also present a variant of this loss that emphasizes rephrased image-question pairs. Our experiments demonstrate that ConClaT outperforms baselines and improves answer consistency and overall accuracy on benchmark datasets. Additionally, VQA models trained using ConClaT perform better than existing approaches across different data augmentation strategies.