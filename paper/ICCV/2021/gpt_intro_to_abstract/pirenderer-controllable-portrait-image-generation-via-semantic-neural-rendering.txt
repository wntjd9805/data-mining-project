Portrait images are widely used in various applications such as virtual reality, film production, and communication. Editing portrait images by controlling the poses and expressions of faces is a challenging task that requires accurate perception of 3D geometric shapes and the generation of photo-realistic images. Recent advancements in Generative Adversarial Networks (GANs) have made progress in synthesizing realistic faces, but most existing methods use indirect and subject-specific motion descriptors, which hinder intuitive editing. To address this, we propose a neural rendering model called PIRenderer that generates photo-realistic portrait images with accurate motions. Our model consists of three parts: Mapping Network, Warping Network, and Editing Network. The Mapping Network produces latent vectors from motion descriptors, the Warping Network estimates deformations between sources and desired targets, and the Editing Network generates final images. Our model enables intuitive image control and realistic results in both direct and indirect portrait editing tasks. Furthermore, we extend our model to audio-driven facial reenactment, demonstrating its potential as an efficient face renderer. Our model generates various and vivid motions from audio streams and transfers them into realistic videos of arbitrary target persons. The contributions of our work include the proposal of PIRenderer, which allows intuitive editing of facial expressions and head motions, tackling indirect image editing tasks, and demonstrating its potential as an efficient face renderer for audio-driven facial reenactment.