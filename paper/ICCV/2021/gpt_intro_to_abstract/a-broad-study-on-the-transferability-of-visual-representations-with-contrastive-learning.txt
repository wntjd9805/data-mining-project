Self-supervised learning has gained significant attention in the field of computer science as a means to learn data representations without labeled supervision. One popular approach in this area is self-supervised contrastive learning, which has shown promising results in image classification tasks. In contrastive learning, a model is trained to recognize different augmented versions of the same image (positives) while discriminating them from other random images (negatives) in the dataset. This paper aims to understand the underlying mechanism of the superior transferability of contrastive learning. The authors conduct a comprehensive study on the transfer learning of contrastive approaches in image classification, few-shot evaluation, and object detection. They compare the performance of different pre-trained models on various downstream datasets and find that contrastive methods outperform supervised cross-entropy models, especially in fixed-feature transfer learning. They also analyze the similarity between hidden representations, intra-class separation, and robustness to image corruption to explain the superior transferability of contrastive approaches. The authors propose a joint objective of self-supervised contrastive loss and supervised cross-entropy/contrastive loss, which consistently outperforms the standard trained counterparts in different downstream tasks. They conclude that contrastive approaches learn more low-level and mid-level information, which is easily adaptable to different domains, compared to supervised cross-entropy models that mainly learn high-level semantics. Additionally, contrastive losses are more robust to image corruptions and provide better-calibrated class probabilities. The paper makes several key contributions, including benchmarking different methods, demonstrating the effectiveness of combining supervised and self-supervised contrastive losses, and showing the presence of more low-level and mid-level information in contrastive models.