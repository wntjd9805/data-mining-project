In recent years, there has been significant progress in video understanding and action recognition, particularly from third person viewpoints. However, action recognition from a first-person perspective lags behind due to the scarcity of large and diverse egocentric datasets. Understanding the positions and movements of hands and manipulated objects is crucial for recognizing egocentric interactions. Existing first-person interaction datasets mostly provide only 2D features without reasoning in 3D about hand and object motions. In this paper, we propose a unified dataset for first-person interaction recognition with markerless 3D annotations of two hands manipulating objects. The dataset includes synchronized RGB-D images, camera poses, hand and object poses, object meshes, scene point clouds, and action labels, allowing for a detailed understanding of 3D hand-object interactions. We also introduce a method to jointly estimate the 3D pose of hands and objects from color images and learn interdependencies within and across hand and object poses using an adaptive graph convolutional network. Our contributions include the creation of the H2O dataset, a semi-automatic annotation pipeline, a unified approach for hand-object interaction recognition, and a novel method for 3D interaction recognition using topology-aware graph convolutional networks. Experimental results demonstrate the effectiveness of our approach and provide baselines for further benchmarking. The dataset and annotations will be made publicly available.