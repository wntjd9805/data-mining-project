Interpretable AI is becoming necessary in safety-critical and high-stakes machine learning applications. In visual recognition tasks, feature attribution methods have been developed to analyze the contributions of individual pixels or visual features. This paper focuses on the class activation mapping (CAM) method, which has been widely used in feature attribution research. However, CAM has limitations in terms of its meaning and violates key minimal requirements for an attribution method. To address these limitations, this paper introduces a novel attribution method called class activation latent mapping (CALM). CALM builds a probabilistic graphical model on the last layers of CNNs and incorporates latent-variable training algorithms. CALM has several advantages over CAM in terms of its interpretability, axiomatic requirements, and accuracy. Experimental analysis demonstrates that CALM outperforms CAM in localizing the correct cues for recognition and advances the state of the art in weakly-supervised object localization tasks. Overall, this paper contributes to the analysis of CAM's lack of interpretability, introduces the more interpretable CALM method, and provides experimental results showing CALM's superior performance. Code for CALM is available at https://github.com/naver-ai/calm.