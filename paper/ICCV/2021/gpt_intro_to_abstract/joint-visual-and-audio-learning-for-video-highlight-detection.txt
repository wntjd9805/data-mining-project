The explosion of online video content has led to increasing demands for video highlight detection, which aims to automatically detect interesting moments within a video. While existing research has primarily focused on visual highlight detection, audio-visual highlight detection remains largely unexplored. In this paper, we propose an approach that jointly learns from visual and audio information to detect highlights in videos. Our approach includes two attention mechanisms: a unimodal self-attention mechanism that models relationships within the same modality, and a bimodal attention mechanism that models the interaction between visual and audio modalities. We also introduce a noise sentinel that allows our model to ignore a modality if it does not contribute to the interestingness of the moment. Experimental results on three benchmark datasets demonstrate the superior performance of our model compared to state-of-the-art methods.