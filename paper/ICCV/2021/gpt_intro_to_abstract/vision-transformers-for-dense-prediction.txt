This paper introduces the dense prediction transformer (DPT), a new architecture for dense prediction tasks in computer vision. While existing architectures for dense prediction typically use convolutional networks, this paper focuses on the choice of backbone architecture and its impact on the overall model's capabilities. The paper highlights the drawbacks of downsampling in dense prediction tasks, such as loss of feature resolution and granularity in the deeper stages of the model.To address these issues, the DPT utilizes a transformer as the basic computational building block of the encoder, specifically adopting the vision transformer (ViT) as the backbone architecture. Unlike fully-convolutional networks, the ViT-based DPT maintains a representation with constant dimensionality throughout all processing stages and has a global receptive field at every stage, leading to fine-grained and globally coherent predictions.Experiments conducted on monocular depth estimation and semantic segmentation tasks demonstrate the effectiveness of the DPT. In monocular depth estimation, DPT outperforms existing fully-convolutional networks by more than 28% on a large-scale training dataset and sets a new state of the art on smaller datasets. Similarly, in semantic segmentation, DPT achieves state-of-the-art results on challenging datasets.Overall, the DPT presents a novel approach to dense prediction tasks by leveraging the transformer architecture, offering improved performance compared to existing convolutional network-based models.