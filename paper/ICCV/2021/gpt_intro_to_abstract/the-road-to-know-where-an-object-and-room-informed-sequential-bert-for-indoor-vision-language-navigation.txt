Abstract: Vision-and-Language Navigation (VLN) enables more flexible interactions with robotic applications by allowing them to navigate based on natural language instructions. One of the first VLN tasks, Room-to-Room navigation (R2R), requires an agent to navigate from a random location to a goal location according to language instructions. Existing methods for indoor VLN tasks encode instructions and visual perceptions at a minimal level, limiting their ability to learn relationships between language elements and visual entities. In this paper, we propose an Object-and-Room Informed Sequential BERT (ORIST) model that encodes instructions and visual perceptions at a fine-grained level, enabling better understanding and navigation. We also incorporate temporal context and incorporate relative directions and room types as important clues for more accurate navigation. Our model achieves state-of-the-art results on three different indoor VLN tasks, demonstrating its generalization ability.