The restoration of clear images from blurred inputs in dynamic scenes is a crucial task in computer vision and image processing. Convolutional neural networks (CNNs), particularly deep learning methods, have shown great success in single image deblurring. Some recent methods have employed strategies such as multi-scale input pyramids and downsampling-upsampling layers to handle the wide range of blur pattern variations. However, these strategies have limitations in terms of their ability to adapt to complex blur patterns with widely distributed scales. Moreover, traditional CNN designs are based on localized filtering operations, which are not ideal for tasks requiring broader reference ranges or full-image self-reference. In this paper, we propose a new deblurring method called SimpleNet to address these challenges. Our method introduces a light-weight non-localized module called Light Global Context Refinement (LGCR) to enrich global detail and improve performance at a lower cost compared to traditional non-local modules. We also introduce the Perceptual Variousness Block (PVB) and PVB-piling strategy to enhance the network's adaptive multi-scale reception ability and broad reception spectrum. We evaluate our method on Go-Pro, RealBlur-J, and RWBI benchmarks, and our experiments demonstrate that SimpleNet achieves state-of-the-art performance. Our contributions include the introduction of LGCR, PVB, and PVB-piling, as well as the development of a robust and effective deblur network with a simple encoder-decoder architecture.