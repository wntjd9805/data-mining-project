Recent advances in 3D human sensing have been made possible by the development of statistical human surface models and various forms of supervised and self-supervised visual inference methods. These models offer advantages in providing anatomically and semantically meaningful representations of the human body during both learning and inference processes. Human anthropometry can be used to regulate the learning and inference process, especially for unfamiliar and complex poses that are not present in training sets. Additionally, semantic models offer correspondences with image detector responses, allowing for essential alignment signals for 3D self-supervision and the elimination of 3D solutions that break the body's symmetry, limb proportions, surface consistency, or anatomical joint angle limits. Evaluation metrics play a crucial role in this field, with the most commonly used representation being "body joints." However, the concept of body joints is not clear anatomically and lacks ground-truth observability. Obtaining body joint positions is typically done by fitting proprietary articulated 3D body models or through human annotators eye-balling joint positions in images, followed by triangulation. While this has been useful for initial 3D predictors, the accuracy of joint predictions cannot be considered anatomically accurate or consistent across a large dataset. In this paper, we propose a hybrid 3D visual learning and reconstruction pipeline that relies on the visually grounded reality of 3D body surface markers and the statistical body model. We utilize multiple novel transformer refinement stages to efficiently and accurately predict key features and incorporate both "model-free" and "model-based" losses to achieve results consistent with human anthropometry. Our model shows state-of-the-art performance in quantitative benchmarks and produces high-quality 3D reconstructions in qualitative testing. The proposed pipeline, named THUNDR, is applicable to both supervised and self-supervised regimes.