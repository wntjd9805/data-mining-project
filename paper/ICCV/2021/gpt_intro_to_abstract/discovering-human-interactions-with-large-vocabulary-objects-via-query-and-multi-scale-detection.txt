This paper focuses on the detection of human interactions with objects in computer vision. The goal is to detect the positions of humans and objects within an image and predict how they interact with each other. While previous studies have made progress in human-object interaction detection, they have primarily focused on a limited variety of objects. In reality, humans interact with a wide range of objects in the visual world, making it necessary to explore this under-explored problem. This paper aims to study the challenges of discovering human interactions with large-vocabulary objects. The current practice for human-object interaction detection involves two parts: person and object instances detection, and instance matching and interaction classification. Existing methods commonly use generic object detectors, but these may not perform well in the low-sample regime of large-vocabulary object detection. This paper proposes a new strategy that formulates the problem as a query problem, using humans as queries to search for corresponding interactions and target objects. The proposed method utilizes Transformers and a cascade detection framework to jointly detect the objects interacting with humans and predict their interactions, eliminating the need for generic object detectors and separate interaction recognition processes. The paper introduces a new dataset and presents experimental results demonstrating that the proposed method outperforms existing one-stage HOI detectors in terms of both accuracy and inference speed. Overall, this work contributes to the understanding of human interactions with large-vocabulary objects in computer vision.