In this paper, we address the task of semantic segmentation for ultra-high resolution geospatial images captured from an aerial view. The accessibility to ultra-high resolution images has expanded due to advanced photography and sensor technologies, leading to increased demand for studying and analyzing such images in various applications such as urban planning and medical image analysis. We propose a deep neural network model that incorporates a locality-aware contextual correlation scheme to improve the segmentation quality of ultra-high resolution geospatial images. Previous segmentation models have targeted full resolution images, requiring significant computation resources. To overcome this challenge, common practices include downsampling the image or separately segmenting partitioned patches and merging the results. However, these practices sacrifice segmentation quality. Our model leverages the contextual information within local patches and their corresponding regions of different scales to capture both semantics and details. We introduce a locality-aware contextual correlation module to enhance relevant features of local patches and an adaptive context fusion scheme to combine these features. Additionally, we propose a contextual semantics refinement network to improve segmentation results and polish segmentation mask contours.Comprehensive experiments demonstrate that our proposed model outperforms existing approaches on public ultra-high resolution aerial image datasets. The contributions of this paper include the development of an ultra-high resolution image segmentation framework that incorporates a novel local segmentation model, the introduction of a contextual semantics refinement network to avoid boundary vanishing artifacts and improve local semantic masks, and achieving state-of-the-art semantic segmentation performance in public ultra-high resolution geospatial image datasets.