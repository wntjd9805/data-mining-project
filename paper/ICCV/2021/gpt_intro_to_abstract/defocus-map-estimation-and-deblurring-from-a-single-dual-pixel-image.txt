Modern DSLR and mirrorless cameras with large-aperture lenses can capture more light, but this also introduces defocus blur, causing objects in images to appear blurred based on their distance from the focal plane. Stopping down the aperture can reduce defocus blur, but it also decreases the amount of light reaching the sensor and increases image noise. Existing techniques for reducing defocus blur either involve impractical hardware additions or rely on focus stacking with long capture times, limiting their applicability to static scenes. In this paper, we propose a method that leverages dual-pixel (DP) sensors, which split each pixel into two halves to capture two sub-images per exposure. By calibrating the spatially-varying blur kernels for the left and right DP images, we can simultaneously recover the defocus map and all-in-focus image from a single DP capture. Our method optimizes a multiplane image (MPI) representation to accurately model occlusions and explain the observed DP images using the calibrated blur kernels. We introduce additional priors to address the under-constrained nature of the problem and a bias correction term to account for image noise. Our method outperforms prior techniques on both defocus map estimation and blur removal, even when tested on images captured using a consumer smartphone camera. Implementation and data are publicly available.