The introduction presents the problem of reconstructing a coherent 3D representation from two views with an unknown relationship. While humans can easily understand and reconstruct scenes from multiple images, existing computer vision methods face challenges in achieving the same level of accuracy and coherence. Traditional tools rely on correspondence and are limited to overlapping regions, while learning-based single view 3D methods produce messy partial reconstructions. The paper proposes a learning-based approach that addresses the challenges of per-view reconstruction, inter-view correspondence, and inter-view 6DOF pose. The method combines a deep neural network architecture and an optimization problem to estimate planes, their relationships, and camera transformations. The proposed approach is validated using realistic renderings from the Matterport3D dataset and compared with various baselines and ablations. The results demonstrate the effectiveness of considering the interrelated problems together and outperform the fusion of existing approaches.