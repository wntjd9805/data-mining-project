Minimum graph cuts have been widely used in machine learning, particularly in natural language processing and computer vision tasks such as segmentation, restoration, and energy minimization. In recent years, they have remained an integral part of deep learning pipelines, playing a crucial role in tasks like image classification, segmentation, and neural style transfer. Karger's contraction algorithm paved the way for randomized algorithms to efficiently solve the problem of finding global minimum cuts, surpassing the efficiency of finding s-t-mincuts. This prompts the question of whether Karger's algorithm can be extended to efficiently find s-t-mincuts or other graph cut problems. In this paper, we prove that a large class of extensions of Karger's contraction algorithm cannot efficiently solve the s-t-mincut problem or the normalized cut problem. However, we demonstrate that a straightforward extension of Karger's algorithm can still be useful for seeded segmentation and semi-supervised learning tasks. We refer to this extension as a forest sampling method, which exhibits similarities to the random walker algorithm. We show through classical experiments that our proposed algorithm compares favorably against the random walker algorithm, which is considered influential in seeded segmentation and semi-supervised learning. With an asymptotic time complexity of O(m) on a graph with m edges, our method offers an efficient alternative while providing probabilistic outputs.