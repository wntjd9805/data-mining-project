Gait recognition is a promising technology for long-distance identification based on walking patterns. However, it remains challenging to extract discriminative temporal representations due to subtle differences in the spatial domain. Existing methods use multi-layer temporal convolutions to model temporal information, but they lack flexibility in adapting to complex motion and realistic factors. To address these issues, we propose a context-sensitive temporal feature learning (CSTL) network that integrates multi-scale temporal features based on contextual information. We also introduce a salient spatial feature learning (SSFL) module to address the misalignment problem caused by temporal operations. CSTL captures frame-level, short-term, and long-term features, which are complementary and provide a comprehensive understanding of gait patterns. The CSTL and SSFL modules work together to enhance motion learning and spatial mining, achieving state-of-the-art performance on gait recognition tasks. Our experimental results on two popular datasets demonstrate the effectiveness of our proposed method.