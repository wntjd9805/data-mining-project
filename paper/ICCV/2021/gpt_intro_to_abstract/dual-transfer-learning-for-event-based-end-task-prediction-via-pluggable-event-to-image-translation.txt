This paper introduces a novel Dual Transfer Learning (DTL) framework for improving the performance of end-task learning using event cameras. Event cameras have unique advantages such as high dynamic range and less motion blur, but learning from sparse event data alone often leads to suboptimal results. On the other hand, generating images from events has been explored but suffers from inference latency and lacks a direct optimization connection between tasks. In this paper, the authors propose a two-stream framework consisting of an Event to End-task Learning (EEL) branch, an Event to Image Translation (EIT) branch, and a Transfer Learning (TL) module. The EIT branch leverages image translation to enhance the EEL branch, while the TL module transfers feature-level and prediction-level information between the two branches. A novel affinity graph transfer loss is introduced to maximize instance similarity and semantic consistency. Experimental results on semantic segmentation and depth estimation tasks demonstrate significant performance improvements without additional inference costs. The proposed DTL framework offers a simple and effective approach for enhancing end-task learning with event cameras.