Deepfake technology, which involves creating convincing fake videos, has become a significant concern due to its potential for spreading misinformation and fake news. In order to create a believable deepfake, both the video and audio channels need to be manipulated. While previous research has primarily focused on identifying visual artifacts and fingerprints, there is a growing need to consider the role of audio in detecting deepfakes. This paper addresses the interplay between video and audio modalities in deepfakes and proposes a novel approach for detecting audiovisual deepfakes. The proposed approach utilizes synchronization patterns between lip motions and pronounced syllables, as well as discrepancies between lip motions and artificially generated phonemes. A two-plus-one-stream model is introduced to separately model the video and audio streams, along with a sync-stream that captures synchronization patterns. By jointly training the network, the model learns to detect both visual and auditory manipulations in deepfakes. To facilitate research in this area, a deepfake dataset containing both visual and auditory manipulations is built. The contributions of this work include the introduction of a joint audiovisual deepfake detection task, the proposal of the sync-stream for modeling synchronization patterns, and the creation of a deepfake dataset for further research in this field.