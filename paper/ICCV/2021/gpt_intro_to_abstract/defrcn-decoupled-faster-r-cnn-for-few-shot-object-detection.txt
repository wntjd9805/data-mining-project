Recently, deep neural networks have achieved remarkable performance on various visual tasks such as image classification and object detection. However, these breakthroughs are only possible when a large amount of annotated data is available, which limits the applicability of current vision systems. Additionally, deep models still cannot match the ability of humans to rapidly learn new concepts from limited examples. This has led to increased research interest in few-shot learning, which aims to learn novel concepts quickly and generalize well in data-scarce scenarios. Few-shot object detection (FSOD) is a more challenging task compared to few-shot classification and object detection, and existing FSOD approaches often rely on meta-learning techniques to acquire task-level knowledge. However, these methods have complicated training processes and data organization, limiting their practical applications. On the other hand, finetune-based methods offer a simpler and more efficient approach to FSOD but struggle with data distribution shifts and underutilization of novel data. In this paper, we propose a new approach called Decoupled Faster R-CNN (DeFRCN) that aims to address the limitations of existing FSOD methods. DeFRCN incorporates two novel modules called Gradient Decoupled Layer (GDL) and Prototypical Calibration Block (PCB) to improve the performance of the Faster R-CNN detector in the few-shot scenario. GDL helps decouple the different modules of Faster R-CNN, while PCB utilizes a pre-trained classification model and novel support prototypes to enhance the classification scores. We demonstrate the effectiveness of our approach on various benchmarks, outperforming state-of-the-art methods. Our contributions include the development of DeFRCN as a straightforward fine-tuning solution for few-shot detection, the introduction of GDL and PCB modules for decoupling and classification performance improvement, and the superior performance of DeFRCN compared to existing approaches.