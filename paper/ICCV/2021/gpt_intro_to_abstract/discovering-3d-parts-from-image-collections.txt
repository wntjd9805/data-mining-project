Recognizing and reasoning about objects in computer vision systems is crucial. While deep learning models have achieved success in recognizing and localizing objects in 2D images, reasoning about 3D attributes from a single image remains challenging. This is due to factors like camera pose, self-occlusions, lighting, and material properties that cause ambiguous object appearance. In this paper, we propose a solution to this problem by discovering faithful and consistent 3D parts from 2D image collections. Previous approaches have leveraged part-based representations, but they either rely on 3D shapes or explicit part annotations as supervision. Our method, on the other hand, automatically discovers 3D parts from image collections in a self-supervised manner. We represent these parts using deep latent embeddings, which allow us to learn a prior distribution of part shapes. We evaluate our approach on several datasets and show that it outperforms state-of-the-art methods in terms of performance and accuracy, while also enabling object manipulation. Our contributions include the development of a part-based single-view 3D reasoning network, the creation of Part-VAE to learn a latent prior over part shapes, and extensive experiments to validate our method.