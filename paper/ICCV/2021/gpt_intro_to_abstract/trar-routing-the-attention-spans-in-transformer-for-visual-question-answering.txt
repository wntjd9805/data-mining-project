This paper introduces Transformer Routing (TRAR), a novel and lightweight routing scheme that achieves automatic selection of attentions in visual and language tasks. TRAR equips each visual self-attention layer with a path controller to predict the next attention span based on the previous output. By treating self-attention as a feature update function for a densely connected graph, TRAR constructs different adjacency masks for defined attention spans, reducing redundancy in parameters and computations. The proposed TRAR is applied to single-stage Transformer networks for visual question answering (VQA) and referring expression comprehension (REC) tasks, achieving new state-of-the-art performance on multiple benchmark datasets. Overall, this paper contributes by addressing the attention span issue in V&L tasks and proposing an effective routing scheme for improved model performance.