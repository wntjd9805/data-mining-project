This paper presents a novel approach to simultaneously estimate LiDAR scene flow and perform motion segmentation using a deep network. Previous works have tackled these problems independently and relied on large amounts of labeled data. However, existing scene flow networks using weak or self-supervised methods have limitations in terms of downsampling and point correspondences. In this study, we propose a method that can handle a larger number of points and does not require 1-to-1 point correspondences. We build on the state-of-the-art optical flow approach RAFT and extend it to perform iterative scene flow estimation and motion segmentation on Bird's-Eye-View feature representations of point clouds. Our method achieves superior performance in scene flow estimation and generalization, outperforming previous methods on multiple datasets. Additionally, our network architecture offers a good balance between generalizability, accuracy, and computational efficiency. The self-supervised training of our method relies solely on the motion profiles of classes and does not require additional data or labels. Overall, our contributions include being the first point-cloud-based scene flow estimation method with self-supervised training for motion segmentation, significant improvement in scene flow estimation compared to previous methods, and the ability to handle a larger number of points.