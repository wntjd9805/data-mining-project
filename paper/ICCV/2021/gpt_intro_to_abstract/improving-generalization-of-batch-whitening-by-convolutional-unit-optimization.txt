This paper introduces the concept of Batch Whitening as an extension of Batch Normalization (BN) in deep neural networks. While BN has been successful in stabilizing and accelerating training by normalizing input features, Batch Whitening aims to remove linear correlation between input channels in addition to centering and scaling. However, previous whitening methods have not been widely adopted due to their complexity and lack of clarity. In this study, the authors investigate the effectiveness of Iterative Normalization (IterNorm), a whitening method based on Newton's iterations, by training ResNet and Wide-Residual Network (WRN) models on CIFAR-10, CIFAR-100, and ImageNet datasets. The results show that IterNorm is not satisfactory on CIFAR-100 despite successful correlation removal. The authors then analyze the theory and practice of Batch Whitening and identify three structural problems: linear transform, position of whitening module, and input misalignment. They propose modifications to the Convolutional Unit, the commonly used block design in neural networks, to address these issues. Empirical analysis and ablation studies confirm that the original Convolutional Unit is well-optimized for BN but not for whitening modules, whereas the modified Convolutional Unit significantly improves performance with whitening modules. The authors also demonstrate the applicability of whitening modules in transfer learning and show improved training stability with their method. The results indicate that the modified Convolutional Unit enhances the efficacy of whitening modules and improves training stability in deep neural networks.