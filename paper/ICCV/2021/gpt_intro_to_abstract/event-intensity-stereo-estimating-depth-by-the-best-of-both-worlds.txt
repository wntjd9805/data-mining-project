Stereo depth estimation is a prominent topic in computer vision, aiming to estimate the depth information of a scene from multiple views. Traditional methods rely on pixel matching and triangulation to recover the scene's 3D geometry. However, stereo matching remains challenging due to factors such as occlusions, imperfect imaging settings, and textureless regions. Recent methods have utilized learning-based approaches to estimate depth, without relying on hand-crafted parameters. Despite significant progress, issues related to poor lighting conditions and complex material properties have received less attention. This paper investigates the use of event-intensity cameras as a complementary data source to enrich the captured details from the scene. Event cameras are vision sensors that report changes of intensity on a per-pixel basis, making them suitable for tasks involving rapid movements. They also have a higher dynamic range compared to traditional intensity cameras, making them applicable for extreme lighting conditions. However, event cameras do not directly capture intensity values, only sensing changes in intensity. To overcome this limitation and utilize the benefits of both event and intensity cameras, the authors propose a network that combines event and intensity images to estimate depth. The network incorporates deformable aggregations and multiscale refinements to achieve precise depth estimation. The authors present the practical advantages of this combination by comparing it with event-only and image-only depth estimation methods on both synthetic and real-world datasets. To the best of their knowledge, this is the first study to investigate the combination of event and intensity images for stereo depth estimation.