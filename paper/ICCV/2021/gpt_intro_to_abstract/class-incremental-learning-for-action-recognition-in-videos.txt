Human activity recognition in video datasets is important for high-level video understanding, and class-incremental learning is necessary when new classes of actions are introduced. Existing approaches have focused on static images and do not capture temporal variations and dynamics in action recognition. This paper presents a framework for class-incremental learning in action recognition based on temporally attentive knowledge distillation. The framework quantifies the importance of subactions within videos and penalizes feature redundancy across frames. The proposed approach utilizes a frame-based video representation method to reduce computational cost. Experimental results show improved accuracy on standard action recognition benchmarks.