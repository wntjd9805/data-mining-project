Visual similarity is a crucial factor in various computer vision tasks, such as image retrieval, person identification, and image clustering. Deep Metric Learning (DML) techniques have been widely used to learn visual similarity by leveraging deep neural networks. However, most existing DML methods focus on learning discriminative embeddings without considering interpretability. Understanding the underlying matching mechanism of DML models is essential for developing reliable intelligent systems for applications like surveillance and access control. Efforts have been made to enhance the transparency of deep visual models, but these methods mainly explain the reasoning process and do not address the composition of visual similarity.In this paper, we propose a Deep Interpretable Metric Learning (DIML) framework to improve the interpretability of DML models. Unlike traditional DML methods that compare feature vectors directly, we leverage the spatial structure of images to enhance interpretability. Our framework measures the similarity of two images by computing an optimal matching flow between their feature maps. This allows us to decompose the similarity into part-wise similarities with different contributions. The DIML framework consists of three key components: Structural Similarity (SS), Spatial Cross-Correlation (CC), and Multi-scale Matching (MM). SS measures the similarity of corresponding parts in the feature maps based on optimal matching flow. CC handles view variance in image retrieval by using spatial cross-correlation as the initial marginal distribution for computing the optimal transport plan. MM enables adaptive adjustment of computational cost in large-scale search problems.Our method is model-agnostic and can be applied to off-the-shelf backbone networks and metric learning methods without additional training. Extensive experimental evaluations on major benchmark datasets demonstrate that our DIML framework improves interpretability while significantly enhancing various metric learning methods.