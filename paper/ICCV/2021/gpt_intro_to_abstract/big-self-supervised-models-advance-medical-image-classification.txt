Learning from limited labeled data is a crucial problem in machine learning, particularly in medical image analysis where annotating medical images is time-consuming and expensive. Two common approaches to learning from limited labeled data are supervised pretraining on a large labeled dataset like ImageNet, and self-supervised pretraining using contrastive learning on unlabeled data. While ImageNet pretraining is widely used in medical image analysis, the use of self-supervised approaches has received limited attention. Self-supervised approaches are attractive because they enable the use of unlabeled domain-specific images during pretraining to learn more relevant representations. This paper focuses on self-supervised learning for medical image analysis and compares it with supervised pretraining on two distinct medical image classification tasks: dermatology skin condition classification and multi-label chest X-ray classification. The authors find that self-supervised pretraining outperforms supervised pretraining, even when the full ImageNet dataset is used. They attribute this to the domain shift and discrepancy between the nature of recognition tasks in medical images compared to general images. In the chest X-ray classification task, self-supervised learning outperforms strong supervised baselines pre-trained on ImageNet by 1.1% in mean AUC. The authors also demonstrate that self-supervised models are robust and generalize better than baselines when subjected to shifted test sets, without fine-tuning. This behavior is desirable for deployment in real-world clinical settings.