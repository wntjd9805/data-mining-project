LiDAR-based 3D object detection is a crucial task in autonomous visual perception systems. However, obtaining large-scale and precise annotations for diverse self-driving scenarios is labor-intensive and time-consuming. This has led to the exploration of self/unsupervised 3D object detection frameworks that can learn 3D representation purely from raw data. While self-supervised pre-training has shown promising results in 2D image recognition and natural language understanding, its application in 3D object detection remains unexplored. This paper presents a novel self-supervised 3D detection framework, named GCC-3D, that integrates geometric-sensitive and semantic-consistent representations. To address the limitations of existing approaches, GCC-3D does not require static partial views, and it employs a geometric-aware contrastive objective and a clustering harmonization phase to capture both local structure and semantic information. Extensive experiments on popular 3D object detection benchmarks demonstrate that GCC-3D achieves significant improvements over random initialized models, with a relative improvement of 4.1% and 1.95% over the previous state-of-the-art method. Furthermore, when compared to PointContrast, GCC-3D demonstrates a relative improvement of over 6.3% and 5.6% in mAP with only 5% labeled data. The proposed framework contributes to the development of a more flexible and scalable self-driving system by reducing the demand for extensive human annotations and achieving state-of-the-art performances in 3D object detection.