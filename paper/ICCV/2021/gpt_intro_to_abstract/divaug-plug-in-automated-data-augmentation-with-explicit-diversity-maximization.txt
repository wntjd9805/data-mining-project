Data augmentation is widely used in deep learning models to improve generalization by generating synthetic data from existing data with controlled perturbation. However, designing data augmentation strategies that work well for different datasets requires human expertise. Recently, efforts have been made to automate the design of augmentation strategies and it has been shown that training models with a learned augmentation policy can significantly improve test accuracy. Despite the success of data augmentation in practice, the underlying theory explaining how it improves model generalization is still lacking. The most well-known hypothesis is that data augmentation imposes a regularization effect on models, making them more consistent within the vicinity of the original data. However, the exact relationship between diversity and regularization is not well-defined, and current evaluation of augmentation strategies is indirect and costly. To address these challenges, we propose a new diversity measure called Variance Diversity, which quantifies the diversity of augmented data by measuring the variance of their corresponding probability vectors. Based on this measure, we introduce a plug-in automated data augmentation framework called DivAug, which maximizes diversity without a separate search process. Our experiments show that the relative gain in model accuracy after applying data augmentation is highly correlated with our proposed diversity measure. Furthermore, compared to state-of-the-art methods, our unsupervised and efficient approach achieves comparable performance gains and can be easily integrated into standard training processes. We also demonstrate that our method enhances the performance of semi-supervised learning algorithms, making it valuable for real-world problems with limited labeled data.