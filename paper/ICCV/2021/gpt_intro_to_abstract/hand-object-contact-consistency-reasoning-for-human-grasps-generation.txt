This paper focuses on the study of hand-object interactions and their applications in virtual reality, human-computer interaction, and imitation learning in robotics. The authors propose a method for generating 3D human hand grasps of 3D objects and highlight the challenges in predicting human grasps compared to robot grasps. To address these challenges, the authors suggest using generative models and large-scale datasets with grasp annotations and contact analysis. However, previous methods overlook the possible contact regions on the object. Therefore, the authors argue that it is crucial for the hand contact points and object contact regions to reach mutual agreement and consistency. To achieve this, they propose a unified model that combines hand grasp synthesis and object contact map estimation. They introduce two components: GraspCVAE, a Conditional Variational Auto-Encoder that predicts hand grasps based on object point clouds, and ContactNet, a network that predicts the contact map on the object. The authors design novel losses and a self-supervised consistency task to optimize the grasp generation during training and adapt the model during test time. The proposed approach is evaluated on multiple datasets and shows significant improvements in both in-domain and out-of-domain object grasps. The contributions of this paper include novel hand-object contact consistency constraints, a self-supervised task for adjusting the generation model at test time, and improved grasp generation performance.