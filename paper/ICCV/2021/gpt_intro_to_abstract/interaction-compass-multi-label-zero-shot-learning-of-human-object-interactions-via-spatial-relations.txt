Multi-label learning is a challenging task in computer science, with applications in various domains such as human-computer interaction, robotics, assistive technologies, and surveillance systems. Traditional multi-label zero-shot learning aims to recognize unseen labels that do not have training images. However, existing works mainly focus on simple labels and ignore intra-label dependencies, spatial relations between the action and the object within an interaction label. This lack of consideration for spatial dependencies leads to difficulties in distinguishing objects in backgrounds and interactions. In this paper, we propose a compositional multi-label zero-shot interaction learning framework that incorporates action-object spatial dependencies without requiring expensive bounding-box annotations. We introduce a novel cross-attention mechanism that learns relational directions between actions and objects to measure their compatibilities. Our framework offers several advantages, including efficient training through differentiable cross-attention, generalization to unseen labels with similar actions, and scalability to thousands of labels without relying on costly bounding-box supervision. We validate our approach on the Visual Genome dataset and demonstrate its effectiveness in enhancing seen and unseen interaction recognition.