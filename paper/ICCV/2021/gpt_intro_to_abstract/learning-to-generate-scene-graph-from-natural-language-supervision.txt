In this paper, the authors explore the problem of learning to detect visual relationships beyond individual concepts in images from image-text pairs. They focus on learning scene graph generation (SGG) from image-sentence pairs, where a scene graph is a graphical representation of an image with objects as nodes and relationships between objects as edges. Most previous methods for scene graph generation rely on fully supervised approaches with human annotations, which are costly and difficult to scale. The authors propose a weakly supervised approach that leverages off-the-shelf object detectors to match object labels with sentence concepts and generate "pseudo" labels for training a deep model. They develop a Transformer-based model that takes visual features and text embeddings as inputs and learns to recognize visual relationships between object pairs. The model is trained on captioning datasets and evaluated on a scene graph benchmark, showing significant improvements over state-of-the-art methods. The authors also demonstrate promising results on open-set scene graph generation. Overall, their work presents a solid step towards structured image understanding.