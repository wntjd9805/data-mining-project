Person re-identification (ReID) is a technology that aims to match a specific person across different camera views, with applications in video surveillance and tracking. However, it faces challenges such as background clutter, occlusion, and viewpoint variations. While image-based person ReID has made progress in extracting discriminative features, it struggles with occlusions and missing parts. Spatial-temporal information can address these limitations and provide more robust results. Several approaches have been proposed to enhance discriminative features using spatial-temporal context, but they have limitations in handling long-range occlusions or capturing global information accurately. In this paper, we propose a novel Pyramid Spatial-Temporal Aggregation (PSTA) framework for video-based person ReID. Our framework utilizes a hierarchical aggregation module to construct long-term dependence without losing short-term information. We introduce a Spatial-Temporal Aggregation Module (STAM) that strengthens foreground features and enforces inter-frame correlations to suppress interference features and enhance discriminative ones. Experimental results demonstrate that our PSTA framework achieves state-of-the-art performance on video-based person ReID benchmarks.