Event cameras, inspired by biological vision systems, record brightness changes as events, asynchronously, and at high temporal resolution. This novel way of acquiring visual information introduces new research directions in computer vision and draws connections to robotics and cognitive sciences. Despite the advantages of event cameras, current vision algorithms face challenges in unlocking their benefits. One of the major challenges is aligning event camera data, especially when the camera is in motion. This paper proposes a method for aligning event camera data based on a probabilistic model. Analogous to image panoramas, the proposed method transforms the location of each individual event into a shared coordinate system. This transformation improves the sharpness of the aggregate event image, leading to improved alignment. The proposed method leverages the idea of congealing, a probabilistic method for aligning traditional images, to measure the joint alignment quality and maximize the likelihood of the event data under a non-parametric distribution. The paper presents new state-of-the-art results, demonstrating higher accuracy and faster performance compared to previously published methods. Additionally, the paper discusses the need for a new approach to evaluating event camera alignment methods.