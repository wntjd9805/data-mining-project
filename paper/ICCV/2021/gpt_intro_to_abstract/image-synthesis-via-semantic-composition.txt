This paper introduces the concept of semantic image synthesis, which involves transforming abstract semantic layouts into realistic images. The paper discusses the use of generative adversarial networks (GAN) in modeling image distribution conditioned on segmentation masks. Despite the substantial achievements in this field, the complexity of characterizing object distributions remains a challenge. Recent advances in GAN-based image synthesis focus on exploiting spatial semantic variance in order to better preserve layout and independent semantics. The paper proposes a semantic encoding and stylization method called semantic-composition generative adversarial network (SC-GAN) to explicitly build connections between objects and stuff in image synthesis. The authors generate semantic-aware and appearance-correlated representations by mapping the discrete semantic layout to the corresponding images. The paper introduces the concept of semantic vectors to abstract objects in images or feature maps based on their appearance similarity, and proposes dynamic operators for semantic stylization that are variant in semantics and correlated in appearance. The learning of semantic encoding and stylization is presented as a two-step process, with semantic encoding trained by estimating natural images from input semantic layouts and semantic stylization trained in an adversarial manner. The proposed method is validated on benchmark datasets for image synthesis and shown to outperform other popular methods in both visual and numerical comparisons. The decoupled design of semantics encoding and stylization also makes the method applicable to other tasks, such as unpaired image translation. Overall, this paper presents a new generator design for GAN-based semantic image synthesis and demonstrates its effectiveness in various applications.