Deep neural networks (DNNs) are widely used in various applications, but they heavily rely on training data quality and computational resources. Many users, therefore, outsource model training or reuse public models from online repositories. However, this raises concerns about the trustworthiness of models provided by third parties. DNNs lack transparency and interpretability, making it difficult to detect malicious behavior. Attackers can exploit this by poisoning the training data or intentionally planting backdoors in the model. Backdoor attacks can be sample-targeted or trigger-based, depending on the activation mechanism. In this paper, we focus on sample-targeted backdoor attacks and propose a defense mechanism called CLEAR. Our approach leverages the observation that sample-targeted attacks create "pockets" around target samples on the decision boundary, allowing us to identify them and remove the backdoor. We demonstrate the effectiveness of CLEAR through extensive experiments on multiple datasets and model architectures, successfully detecting and mitigating sample-targeted backdoor attacks. To the best of our knowledge, this work is the first to achieve such results.