This paper addresses the problem of Video Question Answering (VideoQA) and the limitations of current approaches that rely on manually annotated datasets. The authors propose a new approach for automatically generating a large-scale VideoQA dataset by leveraging cross-modal supervision and text-only tools for question generation. They use transformers trained on a question-answering text corpus to generate a diverse set of non-scripted questions and corresponding open-vocabulary answers from speech transcripts of narrated videos. The resulting dataset, HowToVQA69M, contains 69 million video-question-answer triplets and more than 16 million unique answers. To enable VideoQA with highly diverse questions and answers, the authors introduce a training procedure based on contrastive learning between a video-question multi-modal transformer and an answer transformer that can handle free-form answers. They evaluate their model on multiple existing datasets and demonstrate excellent zero-shot results, particularly for rare answers. Additionally, the authors address the language bias in existing benchmarks for open-ended VideoQA by introducing a new dataset, iVQA, with manually collected questions and answers that require visual information for answering. Overall, this paper makes three main contributions: the introduction of the HowToVQA69M dataset, the development of the contrastive learning-based VideoQA model, and the creation of the iVQA benchmark dataset. The code, datasets, and trained models are made available for further research.