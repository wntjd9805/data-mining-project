In recent years, RGB-D scanning methods have made it possible to track and reconstruct 3D surfaces using consumer-grade devices, resulting in the availability of large-scale 3D datasets with semantic annotations. However, the understanding of the semantics of reconstructed scenes, known as 3D scene understanding, is still an open research problem. Voxel-based methods, inspired by the success of 2D CNN in image semantic segmentation, have been widely used in 3D semantic segmentation. These methods project surface reconstructions to a discrete 3D grid representation and apply 3D convolutions to extract features. However, voxel-based methods suffer from the loss of geodesic information during the voxelization process, leading to ambiguous features and difficulties in learning specific object shapes. In this paper, we propose VMNet, a novel deep hierarchical architecture for geodesic-aware 3D semantic segmentation. VMNet combines the advantages of voxel-based methods in contextual learning with the geodesic information provided by the original meshes. We introduce an intra-domain attentive aggregation module and an inter-domain attentive fusion module to effectively aggregate and fuse features from the Euclidean and geodesic domains. Experimental results on popular benchmark datasets demonstrate the superiority of VMNet over existing voxel-based methods, achieving state-of-the-art performance with a simpler network structure. Our contributions include the proposal of VMNet, the introduction of intra-domain and inter-domain attentive modules, and the demonstration of its effectiveness on 3D semantic segmentation tasks.