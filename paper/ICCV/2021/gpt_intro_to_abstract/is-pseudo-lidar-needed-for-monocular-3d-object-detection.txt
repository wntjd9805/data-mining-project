Detecting and localizing objects in 3D space is essential for various applications in robotics, autonomous driving, and augmented reality. Monocular 3D detection is a rapidly evolving research field due to its potential impact and the widespread availability of cameras. Pseudo-lidar detectors have been developed to leverage advancements in depth estimation, where a pre-trained depth network computes an intermediate point cloud representation that is then fed into a 3D detection network. The strength of pseudo-lidar methods lies in their ability to improve with the quality of depth estimation. However, regressing depth from single images is a challenging problem with inherent errors. These errors contribute significantly to the performance gap between pseudo-lidar and lidar-based detectors. End-to-end monocular 3D detectors offer simplicity and promising results but lack the scalability of unsupervised pre-training. In this paper, we propose DD3D, a novel fully convolutional single-stage 3D detection architecture that combines the benefits of both pseudo-lidar and end-to-end methods. DD3D effectively leverages monocular depth estimation for pre-training, achieving similar scalability to pseudo-lidar methods while maintaining simplicity and generalization performance. We demonstrate the effectiveness of DD3D on monocular 3D detection tasks, surpassing previous state-of-the-art methods on KITTI-3D and nuScenes datasets. Our approach's training procedure and end-to-end optimization enable the effective use of large-scale depth data, resulting in impressive multi-class detection accuracy.