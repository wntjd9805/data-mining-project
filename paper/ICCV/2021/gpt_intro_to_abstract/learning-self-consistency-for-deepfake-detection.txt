Deepfakes are synthetic media generated using image stitching techniques, where the identity or expression of a target subject is replaced by that of another source subject. These deepfakes have had negative social impacts and have spurred the development of methods to detect them. Previous defense methods have focused on identifying artifacts left in the stitching process. In this paper, we propose a new method that detects deepfakes by leveraging inconsistencies in the source features within the forged images. We use a convolutional neural network to extract source features and introduce a novel representation learning method called pair-wise self-consistency learning. This method calculates the cosine similarity between pairs of feature vectors and penalizes those pairs from the same source image for having a low similarity score and those from different source images for having a high similarity score. We train a non-linear binary classifier on the learned source feature map to perform deep-fake detection. We address the issue of pixel-level annotation for the consistency loss by generating synthesized data using an inconsistency image generator. Experimental results show that our method achieves superior detection accuracy on multiple deepfake detection datasets. We also observe that the consistency maps can help localize the modified regions in the images. However, we note that our method's effectiveness is limited to deepfakes generated by existing deepfake detection methods. As the development of deepfake generation techniques evolves, our method may become less effective.