Generic visual object tracking is a fundamental problem in computer vision, involving the estimation of the state of a target object in each frame of a video sequence. While prior research has focused on developing robust appearance models for tracking, current approaches are limited by the quality and discriminative power of these models. One major challenge in tracking is the presence of distractor objects that are visually similar to the target, leading to tracking failures. Appearance-based models struggle in such cases, especially when the target undergoes significant appearance changes. To address these challenges, we propose a target candidate association network that actively tracks both target and distractor objects over time. Our approach uses a base appearance tracker to extract target candidates with distinctive features, and a graph-based candidate embedding network to process and associate these candidates across frames. We also introduce a target detection confidence to improve the robustness of the target classifier. Learning the matching network is challenging due to the lack of ground-truth annotations for associating distractors and the subjective definition of distractors. We address these challenges by utilizing real tracker output and incorporating self-supervised learning and sample-mining strategies. Our experiments demonstrate that our approach improves upon existing tracking methods and achieves state-of-the-art performance on multiple tracking datasets. Our contributions include the proposal of a learnable candidate matching network, the development of an online object association method, and the incorporation of partial supervision, self-supervised learning, and sample-mining to train the association network.