Attention in computer vision is a widely used technique for capturing salient regions in an image. However, the effectiveness of attention in different settings, such as when the testing data is out of the training distribution, is still a matter of debate. In this paper, we address the issue of attention's performance in out-of-distribution (OOD) settings and propose a causal attention module (CaaM) to overcome the limitations of attention. We show that attention can mislead models in OOD settings, leading to incorrect predictions. Our CaaM module generates data partitions and self-annotates confounders to mitigate the confounding bias and improve the accuracy of the models. We conduct extensive experiments on popular attention-based deep models and demonstrate the effectiveness of CaaM in both OOD and identically and independently distributed (IID) settings. Our contributions include the development of a novel visual attention module, a causality-theoretic analysis, and the generic design of CaaM for various deep networks.