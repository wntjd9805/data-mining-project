Human pose estimation is an important technique in computer vision for recognizing and localizing human keypoints in images. Deep neural networks have become the dominant solution for this task, but they often require a significant amount of training and resources. To address this, researchers have focused on designing lightweight and real-time networks. One approach is knowledge distillation, where knowledge from a large pre-trained model is transferred to a compact student model. Previous works in pose estimation have used offline distillation, but training a heavy teacher model can be time-consuming. In this paper, we propose an online pose distillation framework that simplifies the distillation process to one stage. We also introduce both balanced and unbalanced versions of our method, which allows for customization of the target network with different compression rates. Our approach utilizes a feature aggregation unit and multiple auxiliary branches, with the teacher model established through a weighted ensemble of student predictions. We use pixel-wise KL divergence loss to transfer pose structural knowledge. Experimental results on popular benchmark datasets validate the effectiveness of our proposed method. To the best of our knowledge, this is the first online pose distillation approach.