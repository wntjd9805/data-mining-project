Deep learning models are widely used in embedded and mobile devices, but their deployment is hindered by their large size and computational requirements. To address this issue, various model compression techniques have been developed, including neural network quantization, network pruning, and knowledge distillation. However, aggressive compression cannot be performed without data, and collecting user data is often challenging due to privacy concerns. Additionally, the large size of datasets like ImageNet poses memory constraints. Recent works have focused on inverting images from pretrained models to generate model-specific data for compression. However, this approach suffers from bias and lack of generalizability across different models. Moreover, the non-invertibility of pretrained models makes exact synthesis difficult. In this paper, we propose the MixMix data synthesis algorithm, which consists of Feature Mixing and Data Mixing. Feature Mixing utilizes universal features from pretrained models to generate data with high fidelity and generalizability. Data Mixing narrows the inversion solution space, enabling the synthesis of images with exact label information. Our algorithm achieves improved data-free compression performance, as demonstrated through experiments on quantization, pruning, and knowledge distillation tasks.