Convolutional neural networks (CNNs) have become the go-to approach for dense pixel-wise prediction in computer vision tasks. However, existing depth prediction models still struggle to effectively model global contexts. Traditional methods of stacked convolution layers and downsampling have limitations such as washing out low-level features and discarding important local information. To address these issues, various methods have been proposed, including using large kernel sizes, atrous convolutions, image/feature pyramids, and attention modules. While these methods have improved performance, the challenges persist. Transformers, originally used in NLP tasks, have gained interest in computer vision for their ability to capture larger receptive fields. However, using a pure Transformer-based network for pixel-level prediction tasks lacks spatial inductive bias. In this paper, we propose embedding Transformers into the ResNet backbone and designing a unified attention gate decoder to address these limitations. Our method achieves state-of-the-art results on challenging benchmarks for monocular depth estimation and surface normal prediction tasks. Our contributions include introducing Transformers for these tasks, designing an effective attention gate structure, and conducting extensive experiments to demonstrate the superiority of our approach.