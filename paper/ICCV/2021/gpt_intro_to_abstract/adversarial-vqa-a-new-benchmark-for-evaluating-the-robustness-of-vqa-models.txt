Visual Question Answering (VQA) is an important task in computer vision where a model is required to provide an open-ended answer to a given question about an image. A successful VQA system can have practical applications, such as chatbots that assist visually impaired individuals. However, despite advances in VQA models, they are still not robust enough for real-world use. Previous robust VQA benchmarks suffer from limitations in their design, focus on a single type of robustness, and use synthesized images or questions. Additionally, existing benchmarks are often static and do not evolve, which can lead to model performance saturation. To address these issues, we introduce Adversarial VQA (AVQA), a new large-scale VQA dataset collected dynamically using the Human-And-Model-in-the-Loop Enabled Training (HAMLET) approach. AVQA incorporates images from different domains and uses an iterative data collection process. The newly annotated examples, which expose the weaknesses of current models, are added to the training data to train stronger models. The process is repeated several times to create a moving target for VQA systems. We conduct a thorough quantitative evaluation on the robustness of VQA models using the AVQA dataset, including vulnerability to adversarial attacks by humans and benchmarking against state-of-the-art models. The results show the fragility of current VQA models and the need for increased robustness. Our main contributions include the introduction of a dynamically collected VQA benchmark, highlighting the vulnerability of current models, and providing insights into the shortcomings of current models in comparison to other benchmarks.