Modeling the shape and deformation of articulated 3D objects has traditionally been achieved through linear blend skinning (LBS) using polygonal meshes. However, meshes have limitations in terms of resolution-to-memory ratio and fixed topology. To overcome these limitations, recent attention has been focused on neural implicit surface representations, which offer a resolution-independent, smooth, and continuous alternative to discrete meshes. Updating an implicit surface representation based on pose changes is challenging due to the need to modify a continuous function instead of a discrete set of points. In this paper, we propose SNARF (Skinned Neural Articulated Representations with Forward skinning), a novel approach to learning articulated 3D shapes represented by neural implicit surfaces directly from 3D watertight meshes and corresponding bone transformations. SNARF combines the simplicity of skeletal-driven deformation with the flexibility and fidelity of implicit surfaces. Our approach handles topology changes and provides multiple correspondences for any deformed point. We derive the gradients of our forward skinning module, allowing for end-to-end learning of the canonical shape and skinning weights. Importantly, our method does not require pre-defined skinning weights or pose correctives on the surface, making it applicable in scenarios without pre-rigged mesh models. Experimental results show that our method generates high-quality shapes with arbitrary bone transformations, surpassing other recent methods. By conditioning the neural implicit function on poses, our method accurately models pose-dependent deformations. The code for our approach is available.