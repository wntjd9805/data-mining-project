Semi-supervised learning (SSL) is a valuable technique for improving the performance of deep neural networks with limited labeled data by leveraging large amounts of unlabeled data. However, most existing SSL methods assume that labeled and unlabeled data share the same category space, which may not hold true in real-world scenarios. Open-set semi-supervised learning (SSL) is a more practical and challenging setting where outliers that do not belong to the labeled categories may exist in the unlabeled data. Resolving the open-set SSL problem can significantly reduce the data preparation workload in real applications. Previous methods for coping with out-of-distribution (OOD) samples either completely remove them or utilize OOD detection methods that require large amounts of labeled data. However, these methods are not suitable for open-set SSL due to the scarcity of labeled data. In this paper, we propose a novel training framework for open-set SSL that effectively exploits the presence of OOD data for enhanced feature learning while avoiding its adverse impact. We introduce a warm-up training phase that utilizes both labeled and unlabeled data, including OOD samples, to enhance the representation learning of the backbone network. Additionally, we propose a cross-modal matching mechanism for detecting and filtering OOD samples. Our method achieves state-of-the-art performance in open-set SSL benchmarks such as CIFAR-10, CIFAR-100, TinyImageNet, and Animals-10. Our contributions include a novel training pipeline, a warm-up training method utilizing self-supervised learning, a cross-modal matching-based OOD detection algorithm, and improved performance in open-set SSL benchmarks.