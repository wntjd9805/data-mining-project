Deep learning has achieved significant advancements in image recognition tasks, but progress in video understanding has been relatively slower due to the complexity of video data. This research focuses on the design of an effective temporal module for video understanding that can capture complex temporal structures while being computationally efficient. Existing methods combine temporal modules with 2D convolutional neural networks (CNNs) to improve efficiency or design dedicated temporal modules to capture temporal relations. However, the challenge lies in developing a temporal module that has both high efficiency and strong flexibility. To tackle this, an adaptive module is proposed in this paper. The adaptive module incorporates a two-level adaptive modeling scheme that decomposes the video-specific temporal kernel into a location-sensitive importance map and a location-invariant aggregation kernel. This enables the module to focus on enhancing discriminative temporal information and capture temporal dependencies in the video sequence. The proposed temporal adaptive module is designed to be highly efficient and flexible. It is composed of a local branch and a global branch, where the local branch enhances local features using temporal convolutions and the global branch aggregates temporal information using fully connected layers. The proposed module can be integrated into existing 2D CNNs to create an efficient video recognition architecture called TANet. The performance of TANet is evaluated on the task of action classification in videos using the Kinetics-400 dataset, where it outperforms several counterparts in capturing temporal information while maintaining similar computational efficiency. TANet is also tested on the Something-Something dataset, achieving state-of-the-art performance in the motion-dominated dataset.