Recent advancements in self-supervised learning have demonstrated the ability to learn high-level representations of various data types, including images, speech, and text. The widely studied benchmark in this field is ImageNet, where self-supervised representations have shown to outperform supervised representations in terms of data-efficiency and transfer-learning performance. However, a limitation of self-supervised learning on ImageNet is that it relies on a heavily curated dataset, which may introduce biases and restrict its applicability to diverse downstream tasks and larger datasets. In this paper, we explore the performance of recent self-supervised learning methods on downstream tasks, including ImageNet, when pre-trained on less curated datasets like YFCC100M. We observe a significant drop in performance when using the current state-of-the-art self-supervised learning technique on such datasets. We hypothesize that this decline is due to the diverse content and heavy-tailed nature of uncurated datasets, which deviate from the global consistency exploited in previous datasets. To validate this hypothesis, we propose a new method called Divide and Contrast (DnC), which aims to recover the local consistency within subsets of the uncurated dataset. DnC achieves this by training individual "expert" models on each subset and distilling them into a single model. DnC can be used in combination with any self-supervised learning technique and requires the same amount of computation, with the training time being significantly reduced for each expert model. Additionally, the parallelizability of this computation allows for scalability to massive datasets. The remainder of the paper is organized as follows: we provide a review of related work in self-supervised learning, introduce a stronger baseline (MoCLR), present the Divide and Contrast method, and demonstrate its performance on uncurated datasets compared to existing methods.