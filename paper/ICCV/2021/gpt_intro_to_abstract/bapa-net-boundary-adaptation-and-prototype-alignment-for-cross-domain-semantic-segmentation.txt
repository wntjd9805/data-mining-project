Deep convolutional neural networks have greatly improved the performance of computer vision tasks, including image recognition, object detection, and semantic segmentation. However, collecting accurate annotations for semantic segmentation is expensive and laborious. To address this issue, synthetic data from simulation platforms can be used as a cost-effective alternative. However, there is a significant domain discrepancy between synthetic data and real-world data, leading to a drop in performance. To overcome this challenge, unsupervised domain adaptation methods have been proposed. However, current methods often neglect the importance of object boundaries in segmentation results. This is problematic as near-boundary pixels are difficult to classify due to their receptive fields potentially containing pixels from other classes. In this paper, we propose a novel method called Boundary Adaptation and Prototype Alignment Network (BAPA-Net) that properly addresses the issue of object boundaries. Our approach includes a boundary adaptation module to weigh each pixel based on its distance to the nearest boundary pixel and a prototype alignment module to reduce domain mismatch by minimizing distances between class prototypes from source and target domains. Experimental results on benchmark datasets demonstrate the effectiveness of our proposed method in cross-domain semantic segmentation. The contributions of our work include the identification of the neglect of object boundaries in existing methods, the development of the boundary adaptation and prototype alignment modules, and extensive experiments validating the effectiveness of our approach.