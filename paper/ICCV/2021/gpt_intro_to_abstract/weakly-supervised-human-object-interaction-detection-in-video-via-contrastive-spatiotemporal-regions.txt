This paper focuses on the problem of weakly supervised human-object interaction detection in videos. The goal is to identify and localize both the person and object involved in the interaction, as well as the action taken by the human, without the use of bounding box supervision. While previous work has made progress in learning visual-language representations from captioned images or videos, this task is more challenging as it requires detecting both the human and object bounding boxes in multiple frames of the video. Existing methods rely on strong bounding box supervision, which is time-consuming and difficult to scale. To address these issues, the authors propose a weakly supervised approach that leverages verb and noun phrase annotations derived from natural language sentence captions. They introduce a contrastive loss function that associates candidate spatiotemporal regions with action and object vocabulary, allowing for the detection of rare and unseen human-object interactions. The authors also provide a new dataset of over 6.5k videos to evaluate human-object interaction in videos and demonstrate improved performance compared to weakly supervised baselines.