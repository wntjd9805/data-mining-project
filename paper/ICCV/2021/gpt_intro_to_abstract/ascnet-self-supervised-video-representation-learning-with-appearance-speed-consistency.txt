Video analysis has become an important research topic due to the increasing amount of video data. However, labeling this data is expensive and time-consuming. In contrast, there are millions of unlabeled videos available on the internet. Self-supervised learning, which uses unlabeled data to learn meaningful representations, offers a solution to this problem. One popular self-supervised task is predicting the playback speed of video clips, as it can be easily obtained from the video inputs. However, existing methods for this task have limitations, such as the reliance on computationally heavy motion information and the difficulty of maintaining high-quality negative samples. In this paper, we propose two new self-supervised tasks, Appearance Consistency Perception (ACP) and Speed Consistency Perception (SCP), which focus on learning appearance and speed features of video clips, respectively. We also introduce an appearance-based video retrieval strategy to enrich the positive samples and integrate the ACP and SCP tasks. Our experiments show that our method outperforms other self-supervised methods in action recognition and retrieval tasks, while being easier to apply in practice. The proposed tasks and strategy address the limitations of existing methods and improve the quality of learned representations.