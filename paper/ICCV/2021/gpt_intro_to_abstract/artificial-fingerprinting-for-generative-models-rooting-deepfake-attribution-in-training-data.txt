Photorealistic image generation has seen rapid development in recent years, thanks to advancements in generative adversarial networks (GANs). However, this progress has also led to the rise of deepfakes, which are generated media that are difficult for humans to detect and trace. The misuse of deepfakes in social media has become a widespread concern, prompting research efforts in deepfake detection and source attribution. Existing techniques rely on visual patterns or frequency mismatch but struggle to sustainably prevent deepfake misuse as generative models evolve. In this paper, we propose a proactive and sustainable solution for deepfake detection and attribution by introducing artificial fingerprints into generative models. We use image steganography to embed fingerprints into the training data and show that these fingerprints can be decoded from all generated images. By classifying images with matched fingerprints as fake and those with random detected fingerprints as real, we achieve deepfake detection. Additionally, by allocating different fingerprints for different generative models, we enable deepfake attribution. Our solution closes the responsibility loop between generative model inventions and their potential misuses, preventing the misuse of pre-trained generative models. We synergize the domains of image steganography and GANs to propose the first proactive and sustainable solution for deepfake detection and attribution. We also demonstrate the transferability of artificial fingerprints from training data to generative models and empirically validate the beneficial properties of our solution, including universality, fidelity, robustness, secrecy, and anti-deepfake capabilities.