This paper focuses on the challenge of aligning or grounding language to videos in the context of vision-language research. The authors propose a new variant of contrastive learning called Token-Aware Cascade contrastive learning (TACo) to improve video-text alignment for both large-scale pretraining and downstream specific tasks. TACo consists of two modifications to conventional contrastive learning. Firstly, it introduces a token-aware contrastive loss that takes into account the syntactic classes of words, particularly content words such as nouns and verbs. Secondly, it utilizes a cascade sampling method to efficiently select a small set of hard negative examples for training the multi-modal fusion layers. The effectiveness of TACo is empirically validated through experiments on various downstream tasks, including text-video retrieval, video action step localization, and action segmentation. The results demonstrate that TACo enhances text-video retrieval performance and achieves comparable or better results compared to current state-of-the-art methods in transfer learning scenarios.