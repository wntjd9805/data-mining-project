This paper introduces an end-to-end approach for synthesizing novel views from a single 2D image of a scene using an intermediate mesh representation. Previous methods for view synthesis either require multiple images or ground-truth depth at test time, making them impractical for real-world applications. The proposed approach relies on statistical learning from data and introduces the concept of a shrink-wrapped mesh Worldsheet, which wraps a deformable mesh sheet over the 3D world. By generating this Worldsheet for a given view and moving the camera in 3D space, novel views can be obtained. The model is trained end-to-end using rendering losses and a differentiable texture sampler for reconstructing the mesh texture. Additionally, the model incorporates multiple layers of Worldsheets to handle occlusions and depth discontinuities. Experimental results show that the proposed approach outperforms prior state-of-the-art methods on benchmark datasets and is applicable to high-resolution images in real-world scenes. This work is the first to demonstrate mesh recovery for scenes solely based on multi-view supervision.