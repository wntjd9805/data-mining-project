Abstract:Text recognition is a key problem in computer vision with applications ranging from OCR systems to visual question answering. With the advent of deep learning, recognition accuracy has improved significantly. However, the focus has shifted towards the challenging "in-the-wild" setting, particularly irregular scene text recognition. In this paper, we address the understudied problem of unseen word recognition, where no or limited word images are available during training. We propose an iterative framework with a feedback mechanism to enhance performance on rarely or unseen words. Our framework leverages a cross-modal feedback mechanism, where earlier word predictions are fed back to modulate visual features in subsequent iterations. Additionally, we use a conditional variational autoencoder to capture knowledge from textual error distributions and improve the word reconstruction process. Experimental results show that our framework outperforms state-of-the-art methods on various scene-text and handwriting recognition datasets. Ablative studies further demonstrate that our iterative framework is superior to naive spell checking and language model alternatives. Our proposed feedback module is also shown to be compatible with multiple base networks.