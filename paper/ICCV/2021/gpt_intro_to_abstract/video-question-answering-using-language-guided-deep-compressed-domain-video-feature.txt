Recent advancements in artificial intelligence (AI) have drawn considerable attention to the field of computer vision (CV) and natural language processing (NLP). Video question answering (QA) is a multidisciplinary research area that combines visual understanding and language-specific reasoning to provide reasonable answers. This technology has promising applications in human-AI interactions and communication systems. Previous studies in video QA have focused on developing sophisticated deep learning models to address diverse reasoning problems in multimodal data. However, these models often suffer from degraded performance due to insufficient representation capabilities of the extracted features. Current baseline models naively use pre-trained neural networks for feature extraction, which limits their efficiency. In this paper, we propose a time-efficient video QA network called VQAC, which utilizes compressed-domain video features to improve performance at lower complexity. Our approach, which is the first attempt to apply compressed-domain features to video QA tasks, addresses the limitations of conventional video features that require complete video frames for feature extraction. We introduce specific modules to extract appearance and motion features from partially decoded frames, without the need for full reconstruction and decompression of the entire video. The proposed VQAC network produces video QA features that consider different modalities and achieve efficient alignments. We further present a VQAC-integration model that combines our baseline network with existing models to improve performance. Our primary contributions include the development of the VQAC-baseline network, which resolves the major drawbacks of previous video QA features, and the VQAC-integration network, which outperforms previous studies on various video QA datasets.