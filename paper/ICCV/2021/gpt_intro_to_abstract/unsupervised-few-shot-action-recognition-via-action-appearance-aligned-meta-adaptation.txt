Few-shot learning has become increasingly important in the field of computer science, particularly in the context of video data analysis. This is because collecting large-scale labeled video data is challenging and time-consuming. Previous approaches have utilized both 2D and 3D convolutional neural networks (CNNs) to achieve good results in few-shot action recognition in videos. However, these approaches rely on supervised learning and require large amounts of labeled data, which can be expensive and often unattainable. In this paper, we propose MetaUVFS, the first unsupervised meta-learning algorithm for few-shot video action recognition. MetaUVFS leverages a large collection of unlabeled video data to learn video representations through contrastive learning. It then trains an explicit few-shot meta-learner using hard-mined episodes. This episodic meta-learning approach helps bridge the knowledge gap between training and testing phases. MetaUVFS also introduces a novel Action-Appearance Aligned Meta-adaptation module (A3M) to combine action and appearance features, addressing the limitations of direct finetuning. Experimental results show that MetaUVFS outperforms state-of-the-art unsupervised video learning methods on various benchmark datasets and achieves competitive performance compared to existing few-shot action recognition methods. Our contributions include the proposal of MetaUVFS as an unsupervised meta-learning algorithm, the use of a two-stream network to learn action and appearance-specific features, and the introduction of the A3M module for few-shot downstream tasks.