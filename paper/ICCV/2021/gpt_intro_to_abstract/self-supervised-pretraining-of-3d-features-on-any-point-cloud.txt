This paper introduces the use of self-supervised learning on single-view 3D depth scans for training visual features in computer vision tasks. The authors highlight the challenges of acquiring annotated 3D datasets and the limitations of current methods that rely on multi-view depth scans and point correspondences. They propose a simple contrastive framework called DepthContrast, which treats each depth map as an instance and discriminates between them to learn representations. The authors demonstrate that single-view depth scans can be effectively used for self-supervised learning and perform comparably to or better than multi-view approaches. They also show the applicability of their method across different model architectures and types of 3D data. Furthermore, they emphasize the importance of joint training with different input representations and provide insights on optimizing contrastive learning for better results. The authors showcase the performance improvements achieved in nine downstream tasks, including object detection, and achieve state-of-the-art results in two object detection tasks. Overall, their approach offers a promising solution for training visual features using single-view 3D depth scans and enables efficient few-shot learning.