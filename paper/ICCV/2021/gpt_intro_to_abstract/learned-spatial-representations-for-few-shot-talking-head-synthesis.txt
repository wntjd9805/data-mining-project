This paper focuses on the task of learning personalized head avatars in a low-shot setting, also known as "neural talking heads". The goal is to synthesize a photo-realistic video of a source subject, using few-shot images and a driving sequence of facial landmarks. This task has various applications in AR/VR, video conferencing, gaming, animated movie production, and video compression in telecommunication.Traditional graphics-based approaches to this task have high-quality synthesis but do not include the hair area and cannot generalize to new subjects. Recent 2D-based approaches can animate unseen subjects using as few as a single image, but they have lower quality and poor identity preservation compared to 3D-based approaches.The paper proposes a two-step framework that decomposes the synthesis of a talking head into its spatial and style components. First, it predicts a novel spatial layout of the subject under the target pose and expression. Then, it synthesizes the target frame based on the predicted layout. This factorized representation improves subject-agnostic model performance, requires less data for fine-tuning, and achieves better identity preservation. The proposed approach achieves state-of-the-art performance in both single-shot and multi-shot settings.In conclusion, this paper introduces a novel approach for learning personalized head avatars in a low-shot setting. It addresses the challenge of synthesizing realistic and identity-preserving videos by decomposing the process into spatial and style components. The proposed approach outperforms previous methods in terms of subject-agnostic model performance, fine-tuned performance with less data, robustness to pose variations, and improved identity preservation.