This paper focuses on video deblurring, a crucial task in computer vision, as blur is commonly present in videos due to camera movement or moving objects in the scene. The use of event cameras, which record intensity changes at a microsecond level, has shown potential in facilitating deblurring. However, existing video deblurring methods and event-driven restoration methods have limitations. Video deblurring networks often assume consecutively blurry frames, ignoring the presence of sharp frames in a blurry video. On the other hand, event-driven restoration methods are not easily compatible with existing image and video deblurring methods. To address these limitations, this paper proposes a principled framework called D2Nets that leverages non-consecutively blurry frames. D2Nets consists of three steps: distinguishing sharp frames and blurry frames using a bidirectional LSTM, restoring blurry frames using an encoder-decoder deblurring backbone guided by nearby sharp frames, and enhancing temporal consistency through post-processing. Additionally, the paper introduces an event fusion module (EFM) that can be incorporated into D2Nets to bridge the gap between event-driven and video deblurring.Experiments on benchmark datasets demonstrate the effectiveness of D2Nets and EFM in restoring blurry videos. The contributions of this work include the development of D2Nets as a principled deblurring framework, the proposal of EFM to better utilize event information, and the incorporation of EFM into existing image and video deblurring methods for event-driven deblurring.