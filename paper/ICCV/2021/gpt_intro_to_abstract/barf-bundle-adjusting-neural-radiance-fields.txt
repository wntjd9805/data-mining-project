Humans have the ability to reason about 3D geometry through visual perception, allowing us to infer the 3D spatial structures of objects and scenes in videos. However, simultaneously solving the 3D scene representation and camera localization from RGB images is a challenging problem in computer vision. Traditional methods such as structure from motion and SLAM rely on local registration and global bundle adjustment, but these methods are sensitive to registration quality and produce sparse 3D point clouds. Recent advances in view synthesis have focused on recovering dense 3D-aware representations, such as Neural Radiance Fields (NeRF), for high-fidelity view synthesis. However, NeRF requires accurate camera poses, and naive pose optimization with NeRF can lead to suboptimal solutions. This paper proposes a solution to the problem of training NeRF representations from imperfect camera poses. The authors introduce Bundle-Adjusting NeRF (BARF), a strategy for coarse-to-fine registration on coordinate-based scene representations. BARF establishes a theoretical connection between image alignment and joint registration and reconstruction with NeRF. The authors show that positional encoding of input 3D points is critical for NeRF, but also susceptible to suboptimal registration results. BARF successfully recovers scene representations from imperfect camera poses, enabling applications such as view synthesis and localization of video sequences from unknown poses.