Advancements in medical imaging technologies have significantly impacted healthcare practices, but the increasing number of imaging studies has placed a burden on radiologists, affecting the quality and speed of clinical decision-making. Deep learning and computer vision offer a promising solution for automating medical image analysis, but building effective models is hindered by the lack of large-scale labeled datasets. To address this, leveraging medical reports containing detailed information about observed conditions can provide supervision signals for training multimodal representations. However, effectively representing subtle visual cues in medical images remains challenging, as global representations alone may not capture fine-grained semantics. While learning localized features has been explored in other contexts, existing methods rely on pretrained object detectors that are not readily available for medical images. In this paper, we introduce GLoRIA, a framework for learning global-local representations for medical images using attention mechanisms. GLoRIA learns attention weights for significant image sub-regions based on paired report words, creating context-aware localized representations. We also propose a self-attention-based image-text joint representation learning model and a token aggregation strategy to handle abbreviations and typos in medical reports. Our experiments on different datasets demonstrate the generalizability of the learned representations for data-efficient image-text retrieval, classification, and segmentation tasks. GLoRIA consistently outperforms other methods and achieves good performance with limited labels. Our contributions include the GLoRIA framework for learning multimodal global-local representations and demonstrating the label-efficiency of our approach in various tasks.