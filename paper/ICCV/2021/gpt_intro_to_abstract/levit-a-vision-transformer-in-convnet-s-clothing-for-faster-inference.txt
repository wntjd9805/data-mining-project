Transformer neural networks, initially used for Natural Language Processing, have become dominant in various fields due to their ability to handle variable-size sequence data. The introduction of the vision transformer (ViT) architecture has achieved state-of-the-art results in image classification. However, there is a need to further optimize the trade-off between performance and accuracy in small and medium-sized architectures. In this paper, we aim to develop a Vision Transformer-based model, called LeViT, that offers better inference speed on various hardware architectures such as GPUs, Intel CPUs, and ARM hardware. We replace some transformer components with convolutional components to improve performance on highly-parallel architectures. The contributions of this paper include a multi-stage transformer architecture, a computationally efficient patch descriptor, a per-head translation-invariant attention bias, and a redesigned Attention-MLP block to enhance network capacity. These techniques allow for the shrinking of ViT models in terms of width and spatial resolution. Overall, this paper focuses on exploring the speed/accuracy trade-off in Vision Transformers.