Instance segmentation models are widely used for predicting object masks in query images, but they often perform poorly on new classes. This limitation hinders the development of autonomous systems operating in open worlds where there will always be objects that do not belong to known classes. In this paper, we aim to automatically improve the performance of instance segmentation models on static images containing objects from new classes without human intervention. Our approach leverages unlabeled video sequences, which provide rich information and can be easily obtained. We propose a method for automatically generating masks for objects visible in these videos and use these masks to train an instance segmentation model. We focus on localizing and segmenting new objects rather than predicting their categories. Our method differs from previous works as it does not require manual segmentation labels or human intervention for new classes. Instead, it relies on self-supervised learning from unlabeled videos. We developed a novel approach for creating object masks from videos, which takes into account background and motion information. We evaluate our method on a new dataset called Unseen-VIS, which contains objects that do not belong to known classes. Our results show that our method can significantly improve the performance of the instance segmentation model on unseen classes without sacrificing performance on known classes. Our contributions include the proposal of a Bayesian method for generating high-quality masks on unlabeled videos, the creation of a benchmark for evaluating the quality of generated masks, and the demonstration of the effectiveness of our method in improving the performance of an instance segmentation model on unseen classes.