Group activity recognition is a critical problem in computer science, with applications in surveillance systems, video analysis, and social behavior analysis. Unlike conventional action recognition, group activity recognition focuses on understanding the scene of multiple individuals. However, modeling relevant relations between individuals for inferring collective activity is challenging due to spatial and temporal variations in untrimmed scenarios. Previous methods have utilized recurrent neural networks and attention-based methods to capture relation context in videos, but they still face challenges in modeling spatial-temporal contextual information and grouping individuals based on their interconnected relations. In this paper, we propose an end-to-end trainable framework called GroupFormer, which uses a modified Transformer to model individual and group representation for group activity recognition. We introduce a Group Representation Generator to merge individual and scene-wide context, and employ Spatial-Temporal Transformers (STT) to refine the representations. Our STT is enhanced by a clustered attention mechanism, known as Clustered Spatial-Temporal Transformer (CSTT), to model inter-group and intra-group relations. Experimental results on the Volleyball and Collective Activity datasets demonstrate that GroupFormer outperforms state-of-the-art methods. In summary, our contributions include the proposal of GroupFormer, the introduction of a clustered attention mechanism, and extensive experiments showing superior performance.