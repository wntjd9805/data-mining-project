Recently, graph convolutional networks (GCN) have shown impressive performances in 3D representation learning on point clouds. However, these networks face several technical challenges, including the exponential growth in computational cost with the number of points and the expansion of feature dimensions during the graph feature gathering operation. Additionally, the GPU memory requirements and inference speed are negatively impacted. This paper aims to address these challenges by analyzing the fundamental operations in GCNs and proposing optimized strategies. The authors make two key findings: (1) the local geometric structure information propagates smoothly across GCNs, eliminating the need for K-nearest neighbor (KNN) search in every layer, and (2) shuffling the order of the graph feature gathering operation and the MLP leads to equivalent or similar operations. Based on these findings, the authors propose a computationally optimized GCN that reduces the computational burden and accelerates inference. The proposed techniques are applied to four representative GCNs and demonstrate significant improvements in efficiency without loss of accuracy. The contributions of this paper include the analysis of basic operations in GCNs, the development of acceleration theorems, and experimental validation of the proposed methods.