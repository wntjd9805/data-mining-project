Facial expression recognition (FER) is an important task in computer vision research for enabling computers to understand human emotions and interact with humans. However, FER remains challenging due to large inter-class similarities and small intra-class similarities. Existing approaches can be divided into global-based and local-based methods, but they have limitations in capturing critical facial regions and adapting to variations in facial appearance. To address these issues, this paper proposes a TransFER model that leverages attention mechanisms and a Vision Transformer (ViT) to learn diverse relation-aware local representations for FER. The model incorporates Multi-Attention Dropping (MAD) and Multi-head Self-Attention Dropping (MSAD) modules to explore comprehensive local patches and rich relations among different patches. Experimental results demonstrate the effectiveness and usefulness of the proposed TransFER model, achieving state-of-the-art performance on FER benchmarks. The contributions of this work include the application of ViT for FER, the introduction of MSAD and MAD modules, and the exploration of transformer-based relation-aware local patches for FER.