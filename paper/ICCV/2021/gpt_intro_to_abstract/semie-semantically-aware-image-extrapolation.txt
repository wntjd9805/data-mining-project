Image extrapolation, or out-painting, is the task of extending an input image beyond its boundaries. While image inpainting, the counterpart task, has been extensively researched, out-painting remains relatively under-explored. Existing solutions for image inpainting based on deep networks and generative adversarial networks have shown poor results when applied to the out-painting problem. Researchers have explored new solutions to address this issue, particularly for outdoor scenes where existing techniques have focused on extending textures for 'stuff' classes like mountains and trees, or single-object images like faces and cars. However, these methods are not suitable for other domains like traffic scenes and indoor scenes, where successful out-painting requires extending both 'stuff' classes and 'things' classes, as well as adding new objects based on context. Existing techniques fail in these domains by filling the extrapolated region with artifacts and ignoring high-level information like object semantics and co-occurrence relationships. In this paper, we propose a novel approach that extrapolates the image in the semantic label map space, allowing us to generate new objects and maintain texture consistency. We also generate panoptic label maps to improve object boundaries and employ instance-aware context normalization and patch co-occurrence discriminator to ensure texture consistency. Experimental results on Cityscapes and ADE20K datasets demonstrate the superiority of our method in terms of fidelity and object co-occurrence metrics. Our contributions include the proposed paradigm for image out-painting, the generation of panoptic label maps, and the use of instance-aware context normalization and patch co-occurrence discriminator.