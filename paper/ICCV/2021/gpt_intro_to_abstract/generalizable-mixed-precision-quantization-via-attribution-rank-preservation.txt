Deep neural networks have achieved state-of-the-art performance in various computer vision tasks. However, mobile devices with limited resources are unable to handle the high complexity of deep models. To address this issue, network compression strategies need to be designed based on hardware configurations. Network compression techniques such as pruning, quantization, efficient architecture design, and low-rank decomposition have been proposed. Quantization, which restricts network weights and activations to limited bitwidth, is particularly effective for memory saving and fast processing. Mixed-precision quantization has been introduced to search for the optimal bitwidth in each layer, achieving a balance between accuracy and complexity. However, conventional mixed-precision quantization methods require dataset consistency during the bitwidth search and network deployment, resulting in significant search burden for large-scale datasets such as ImageNet. In this paper, we propose a new method called GMPQ (Generalizable Mixed-Precision Quantization) that learns a generalizable quantization strategy by preserving attribution rank for efficient inference. Unlike existing methods, our strategy can be applied across various datasets, reducing the search cost. We enforce the quantized networks to imitate the attribution of full-precision networks, rather than directly minimizing the Euclidean distance between them. This approach allows the attribution of quantized networks to adjust adaptively without capacity inefficiency. Experimental results show that our GMPQ method achieves competitive accuracy and complexity trade-off on ImageNet and PASCAL VOC datasets in only a few GPU hours.