Machine learning models used in scenarios with unknown test data can lead to dangerous consequences when predictions are used for life-threatening occurrences such as medical diagnosis. The distributional gap between training data and test data can result in seriously erroneous predictions. To address this issue, domain generalization (DG) approaches leverage labeled data from multiple training domains. However, existing DG algorithms tend to prioritize the semantic invariance assumption across domains and ignore intra-domain style invariance. In this paper, we propose a novel model called STEAM (STyle and sEmAntic Memory) that incorporates both intra-domain style invariance and inter-domain semantic invariance to improve DG tasks. Unlike existing works, STEAM benefits from the assumption that instances from the same domain should share style information, which helps disentangle style features and ease the search for true semantic features. We achieve intra-domain style invariance using contrastive loss with the goal of reducing overfitting to domain styles. Additionally, STEAM ensures the network respects the conventional semantic invariance among domains by requiring consensus in similarity scores for each pair of samples from the same class. Our contributions include exploring the assumption of intra-domain style invariance, proposing a mechanism for learning domain-agnostic semantic features, and demonstrating the generality and applicability of STEAM in DG and domain adaptation tasks.