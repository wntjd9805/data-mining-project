Artificial neural networks (ANNs) have seen great success in computer vision applications. However, concerns about their vulnerabilities to adversarial attacks have grown, especially in safety-critical and trusted systems. Adversarial images with small perturbations can fool ANNs into making incorrect decisions. Training ANNs with adversarially generated images has been effective in improving robustness against attacks, but it comes with a decrease in clean image accuracy and additional training time. Deep spiking neural networks (SNNs) have gained traction due to their potential for lower power consumption. Recent advances in SNN training have improved accuracy and reduced computation cost. However, the robustness of these SNNs under adversarial attacks is yet to be fully explored. Some earlier works suggest that SNNs may have inherent robustness, but their studies are limited to small datasets and shallow models. This paper aims to examine the inherent robustness of low-latency deep SNNs under adversarial attacks and explore computationally-efficient training algorithms to improve their robustness while retaining high clean-image accuracy. The authors present HIRE-SNN, a training algorithm that optimizes trainable parameters using perturbed images with crafted noise across time steps. Extensive evaluations are conducted with VGG and ResNet SNN models on CIFAR-10 and CIFAR-100 datasets. The remainder of the paper presents the necessary background, analysis of inherent robustness, the training scheme, experimental results, and conclusion.