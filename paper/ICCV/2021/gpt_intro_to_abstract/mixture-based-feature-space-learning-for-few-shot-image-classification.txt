The goal of few-shot image classification is to transfer knowledge from a set of "base" categories to a set of distinct "novel" classes with very few examples. Successful approaches have been able to learn robust feature representations from base training images that can generalize well to novel samples. However, a common assumption in few-shot learning is that classes can be represented with unimodal models, which limits adaptability and can lead to underfitting.To address this limitation, we propose "Mixture-based Feature Space Learning" (MixtFSL) to learn a multimodal representation for the base classes using a mixture of trainable components. This approach allows us to learn both the representation and the mixture model jointly in an online manner, resulting in a discriminative representation that improves performance on novel classes. We present a two-stage approach to train MixtFSL, which initializes the mixture components and progressively refines the learnable mixture model.Our experiments demonstrate that MixtFSL outperforms state-of-the-art methods on four standard datasets and using four different backbones. We show that our approach is flexible and can leverage other improvements in the literature to further boost performance. Additionally, our approach does not suffer from forgetting the base classes. Overall, MixtFSL offers a novel and effective solution for few-shot image classification by learning a flexible representation through the use of mixture components.