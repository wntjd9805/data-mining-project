Recent advancements in vision-based human performance capture have enabled various applications, such as tele-presence, sportscast, gaming, and mixed reality, providing enhanced interactive and immersive experiences. However, the expensive and complex setups of current capture systems limit their popularity. While some lightweight multi-view capture systems have achieved impressive results, they still rely on pre-scanned templates or specialized sensors. Recently, deep implicit function-based methods have been successful in recovering 3D body shape from single RGB images, offering more efficient geometric detail representation. However, existing methods like PIFu and PIFuHD face challenges in multi-person scenarios and multi-view setups due to issues like over-smoothing and occlusion. To address these challenges, we propose a novel framework for multi-person reconstruction that uses a spatial attention-aware module for adaptive information aggregation from multi-view inputs. We combine this module with parametric models to enhance robustness and fine-grained details, particularly in the presence of occlusions. Additionally, we propose a temporal fusion method to enhance the temporal consistency of reconstructed dynamic 3D sequences from video inputs. To better evaluate our system, we contribute a novel dataset called MultiHuman, consisting of high-quality scans of multi-person interactive scenarios, facilitating detailed evaluation. Experimental results demonstrate the state-of-the-art performance and generalization capacity of our approach. This work's main contributions include the proposed framework for high-fidelity multi-view reconstruction, the spatial attention-aware module, the temporal fusion method, and the MultiHuman dataset for training and evaluation purposes in future research.