Semantic scene understanding is a crucial task in applications such as auto-driving and robotics. In the context of auto-driving, semantic scene understanding provides environmental information for motion planning and enhances the safety of autonomous cars. One of the important tasks in semantic scene understanding is semantic segmentation, which involves assigning a class label to each data point in the input data. This information helps autonomous cars better understand their surroundings.Semantic segmentation methods can be categorized into camera-only methods, LiDAR-only methods, and multi-sensor fusion methods. Camera-only methods have made significant progress due to the availability of large open-access datasets and the rich appearance information captured by cameras. However, cameras are vulnerable to changes in lighting conditions, making them less reliable. To address this issue, researchers have turned to LiDAR data, which provides reliable and accurate spatio-depth information about the physical world. However, LiDAR-only semantic segmentation is challenging due to the sparse and irregular distribution of point clouds.In this paper, we propose a perception-aware multi-sensor fusion (PMF) scheme to effectively combine the perceptual information from RGB images and point clouds. Our method tackles segmentation under undesired lighting conditions and sparse point clouds. It also handles adversarial samples of RGB images by integrating information from point clouds. We introduce perception-aware losses to the network, enabling it to capture the perceptual information from two different-modality sensors. Experimental results on benchmark datasets demonstrate that our method outperforms state-of-the-art LiDAR-only methods in terms of mean intersection over union (mIoU).Overall, our contributions include the PMF scheme for effective multi-sensor fusion, addressing segmentation challenges in auto-driving scenarios, and the introduction of perception-aware losses to enhance the network's ability to capture perceptual information. Our experimental results highlight the superior performance of our method compared to existing approaches.