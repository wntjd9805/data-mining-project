This paper addresses the problem of inducing visual concepts for completing visual reasoning tasks in Visual Question Answering (VQA), without the use of concept-level supervision. The proposed Object-Centric Compositional Attention Model (OCCAM) performs object-level visual reasoning and improves interpretability. The trained OCCAM's attention values are used to create classifiers mapping visual objects to words, from which concepts and super concepts are derived. The concept-based visual reasoning framework predicts concepts and performs compositional reasoning using symbolic concept embeddings instead of visual features. Experimental results on the CLEVR and GQA datasets demonstrate the improved interpretability and predictive accuracy of the proposed approach.