The field of computer vision has evolved to include more complex problems in understanding images. In particular, there is a growing need to detect human-object relationships in videos, which has applications in human-centered domains like human-robot interaction, senior care, and healthcare. Unlike previous approaches that focused on static images, this paper proposes a Human-Object Relationship Transformer (HORT) model for detecting human-object relationships in videos. The model consists of two stages: static image prediction and using visual concepts from the first stage to recognize active objects and their relationships. The HORT model leverages transformers with intra- and inter-attention mechanisms to integrate information from neighboring frames and accurately identify human-object interactions. The model has been validated on two video datasets, achieving better performance than state-of-the-art methods in scene graph generation and human-object interaction detection. Ablation studies have also been conducted to analyze the individual contributions of each component of the HORT model.