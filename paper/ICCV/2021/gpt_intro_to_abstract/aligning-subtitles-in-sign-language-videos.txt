Sign languages are a crucial mode of communication for Deaf communities. This paper aims to temporally localize subtitles in continuous signing videos. Automatic alignment of subtitle text to signing content has various potential applications, including assistive tools, translation, indexing of sign language video corpora, subtitling for signing vloggers, and the creation of large-scale sign language datasets. However, the translation between continuous signing and written language remains a challenging problem. Manual alignment of subtitles to sign language video is time-consuming, prompting the need for automated approaches. In this work, we focus on aligning known subtitles within specific temporal signing windows, particularly in sign language interpreted TV broadcasts where the subtitles are synchronized with the audio. Subtitle alignment to continuous signing presents unique challenges due to differences in grammar, word ordering, and duration between signing and speech. Previous work has primarily focused on finding sparse correspondences between keywords in the subtitle and individual signs, rather than localizing the start and end times of complete subtitle texts. We propose using the subtitle text as an additional signal for better alignment and make three contributions: showing the effectiveness of encoding subtitle text in the alignment model, designing a novel formulation based on Transformers, and conducting a comprehensive study with promising results on unseen signers and content.