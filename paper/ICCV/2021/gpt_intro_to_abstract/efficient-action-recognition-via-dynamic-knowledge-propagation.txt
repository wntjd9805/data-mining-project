Video action recognition is a fundamental problem in video understanding, and significant progress has been made in this area thanks to advances in deep learning. However, many state-of-the-art methods for video recognition require high computational costs, making them challenging to apply in practical applications. This paper focuses on the problem of efficient video recognition, aiming to achieve better performance and computational cost trade-off. Two approaches have been proposed: designing efficient versions of 3D-CNN and selecting salient frames/clips. To address these challenges, this paper proposes a knowledge propagation framework with two networks - a light network and a heavy network - that operate together to efficiently recognize actions. This framework, called Dynamic Student-Teacher architecture, allows the student network to process most of the sampled frames while the teacher network processes only a few, enhancing the student features with high-quality features from the teacher model. The main contribution of this work is the dynamic knowledge propagation mechanism, which enables interaction between the student and teacher models during training and inference. The second contribution is the dynamic student-teacher framework itself, which combines light and heavy networks to reduce computational costs without significant performance degradation. Experimental results on popular benchmarks demonstrate the effectiveness of each component of the proposed approach, with improvements in state-of-the-art performance while reducing computational costs.