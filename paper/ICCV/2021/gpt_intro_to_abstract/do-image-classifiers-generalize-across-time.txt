This paper addresses the issue of prediction flicker in image recognition systems applied to videos. It is observed that models correctly recognize objects in one frame but fail to do so in the next frame, leading to catastrophic mistakes in downstream applications. This problem stems from the lack of robustness in current models to small input perturbations. While previous studies have analyzed model robustness using synthetic modifications, this work focuses on evaluating models on perceptually similar images sampled from nearby video frames. Two test sets, ImageNet-Vid-Robust and YTBB-Robust, containing tens of thousands of human-reviewed and perceptually similar image sets, are introduced to evaluate model robustness. Over 47 different models are tested, and experiments reveal significant degradation in accuracy and detection performance in the presence of small, natural perturbations. Even the best-performing models experience accuracy drops, indicating the importance of ensuring reliable predictions in videos, especially in safety-critical environments like autonomous driving. The paper concludes by highlighting the need for future work in addressing this issue and ensuring robust predictions in every frame of a video.