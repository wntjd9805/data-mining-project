Recent advancements in generating high dimensional data, such as images and videos, have provided various practical and commercial applications. However, these applications often require control over the generated visual data based on user inputs. For image manipulation, deep learning models have been utilized in photo editing software to allow users to modify specific portions of an image. Similar approaches have been explored for video generation, where frames can be conditioned on attributes, sentences, sound, or motion information. While existing works have focused on generating videos with single objects, animating images and generating videos with multiple objects present in the scene is more challenging due to the complexity of modeling and controlling their movements.To address this challenge, this paper introduces Click to Move (C2M), the first approach that enables users to generate videos in complex scenes by conditioning the movements of specific objects through mouse clicks. The proposed framework consists of three main modules: an appearance encoder, a motion module, and a generation module. An Graph Neural Network (GCN) is utilized to model object interactions and infer plausible displacements for all objects in the video, while respecting user constraints.Experimental results demonstrate that C2M outperforms previous video generation methods on publicly available datasets. The proposed GCN framework effectively models object interactions in complex scenes. This work improves upon previous video generation methods by allowing users to directly control the video generation process through mouse clicks. Furthermore, the use of semantic information enhances the selection and movement of objects in a temporally consistent manner. The contributions of this paper include the introduction of C2M, a novel approach for generating videos in complex scenes with user interaction, a deep architecture incorporating a GCN for modeling object interactions, and an extensive experimental evaluation demonstrating the superiority of the proposed approach in terms of video quality metrics and adherence to user inputs.