Video semantic segmentation has gained increasing attention as an essential task in video analysis and understanding. Deep neural networks (DNNs) have shown impressive performance in video segmentation. However, most existing methods rely on densely annotated training videos, which are costly and time-consuming to create. To alleviate this constraint, some studies have used self-annotated synthetic videos, but these models often suffer from performance drops when applied to natural scenes due to domain shift. Despite its value in research and practical applications, domain adaptive video segmentation has been largely neglected in the literature. This paper addresses this gap by proposing a domain adaptive video segmentation network (DA-VSN) that introduces temporal consistency regularization (TCR) to bridge the gap between domains. Two regularization modules, cross-domain TCR (C-TCR) and intra-domain TCR (I-TCR), are designed to improve domain adaptive video segmentation. Extensive experiments on synthetic-to-real benchmarks demonstrate the effectiveness of the proposed approach. The contributions of this work include the introduction of TCR to address domain shifts in video segmentation, the design of inter-domain and intra-domain TCR modules, and the achievement of superior domain adaptive video segmentation compared to baselines.