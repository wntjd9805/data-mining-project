Person re-identification (ReID) is a popular technique in computer vision that uses computer vision technology to determine the presence of a specific person from a set of gallery images captured by surveillance cameras. While ReID has been widely studied for visible images, there has been relatively little research on ReID between visible and infrared images, which is important for night-time surveillance applications. The cross-modality visible-infrared person re-identification (cm-ReID) problem is particularly challenging due to the large visual differences between the two modalities and different camera environments. Existing methods for cm-ReID mainly focus on learning common feature representations or generating intermediate modalities or cross-modality images for person matching. However, these approaches often result in noisy or uncertain representations and have limited practical applicability. To address these limitations, this paper proposes a novel end-to-end Modality Confusion Learning network (MCLNet) for cm-ReID that learns modality-invariant features without additional noise or subspace features. MCLNet confuses the modality discrimination in the feature learning process, focusing on modality-irrelevant perspective and minimizing inter-modality discrepancy while maximizing cross-modality similarity. The network incorporates a partially shared two-stream network to simultaneously learn modality-specific and modality-invariant features. Additionally, the paper introduces identity-aware and camera-aware marginal center aggregation strategies to reinforce representation invariance against modality and camera variations, respectively. Experimental results on two cm-ReID datasets demonstrate that the proposed MCLNet framework outperforms state-of-the-art methods in terms of performance.