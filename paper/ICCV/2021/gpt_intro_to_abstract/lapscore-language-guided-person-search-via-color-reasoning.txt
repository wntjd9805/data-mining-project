Language-guided person search is an important application in intelligent surveillance. This paper addresses two main challenges in this task: computing visual-textual affinity and constructing image-text alignments, and handling the fine-grained retrieval aspect with detailed text descriptions and intra-class differences in appearance. Previous methods have focused on learning representations and using attention mechanisms, but they have not explicitly learned cross-modal associations. The paper proposes a novel representation learning method called LapsCore, which solves color reasoning sub-tasks to guide the model in learning fine-grained cross-modal associations. The sub-tasks include text-guided image colorization and image-guided text completion. Another sub-task, image feature channel completion, is also proposed to complete image features with missing channels using captions. The proposed method is evaluated on the CUHK-PEDES dataset and general image-text retrieval datasets, and it demonstrates impressive performance improvements and achieves state-of-the-art results. The paper concludes by highlighting the contributions of the proposed method and its effectiveness in different baselines and cross-modal retrieval tasks.