The novel view synthesis problem involves rerendering a scene from arbitrary viewpoints using a set of sparsely sampled viewpoints. View synthesis requires 3D reconstruction and high-frequency texture synthesis. Previous approaches, such as Neural Radiance Fields (NeRF), have made progress in high-quality view synthesis with many observations. However, when only a few views are available, the problem becomes underconstrained and NeRF often finds degenerate solutions. To address this, we propose DietNeRF, which supervises the radiance field from arbitrary poses using a semantic consistency loss in a feature space capturing high-level scene attributes. We extract semantic representations using CLIP Vision Transformer and maximize similarity with representations of ground-truth views. This approach combines prior knowledge about scene semantics with a 3D representation. NeRF is limited to per-scene estimation and lacks prior knowledge from other images and objects. It requires a large number of input views to reconstruct accurately and struggles when regions of an object are unobserved. We show that DietNeRF can generate realistic reconstructions with as few as 8 views and even produce reasonable reconstructions of occluded regions. To generate novel views with only one observation, we fine-tune pixelNeRF and enhance perceptual quality.