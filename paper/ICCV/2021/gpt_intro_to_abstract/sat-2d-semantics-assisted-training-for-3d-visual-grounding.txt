This paper introduces the concept of 3D visual grounding, which aims to ground a natural language description about a 3D scene to the region referred to by a language query. While previous visual grounding studies have focused on 2D images and videos, the authors propose using 2D semantics to assist 3D visual grounding. They propose a training method called 2D Semantics Assisted Training (SAT), which utilizes 2D image semantics during training to improve the joint representation learning between the 3D scene and language query. SAT introduces auxiliary loss functions and an encoding method for 2D semantics features to align objects in 2D images with the corresponding ones in 3D point clouds or language queries. The authors experiment with a transformer-based model and evaluate SAT on various datasets, showing improved accuracy compared to a non-SAT baseline. The main contributions of this paper are the proposal of SAT, which is the first method to assist 3D tasks with 2D semantics in training without requiring 2D inputs during inference, and the demonstration of its effectiveness in improving 3D object representation and accuracy in 3D visual grounding tasks.