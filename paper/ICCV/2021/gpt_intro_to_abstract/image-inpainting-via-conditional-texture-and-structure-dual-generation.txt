Image inpainting is the process of reconstructing damaged regions of an image while maintaining its overall consistency. This is a common task in computer vision that has practical applications in areas such as photo editing and object removal. Deep learning approaches have been successful in advancing the field of image inpainting, capturing high-level semantics and achieving better results for non-repetitive patterns. There is also a trend of combining deep generative methods with traditional patch-based approaches to obtain realistic textures and plausible semantics. However, these methods often struggle with recovering the global structure of the image. To address this, multi-stage methods have been proposed that explicitly incorporate structure modeling. However, acquiring reasonable structures from corrupted images is challenging, and unstable structural priors can lead to errors. Some recent attempts have focused on integrating structure and texture modeling processes, but the relationship between structures and textures is not fully considered. In this paper, we propose a novel two-stream network for image inpainting that models structure-constrained texture synthesis and texture-guided structure reconstruction in a collaborative manner. This approach allows the two tasks to complement each other and achieve more visually convincing results. We also introduce specific modules to enhance consistency and capture finer details. Experimental results on benchmark datasets demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. The main contributions of this work are the novel two-stream network, the Bi-directional Gated Feature Fusion (Bi-GFF) module for consistency enhancement, and the Contextual Feature Aggregation (CFA) module for capturing spatial dependencies.