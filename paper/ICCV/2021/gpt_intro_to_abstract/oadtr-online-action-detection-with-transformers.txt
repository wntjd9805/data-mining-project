Online action detection, which aims to identify ongoing actions from streaming videos, has gained attention for its potential applications in various domains. Current approaches use recurrent neural networks (RNNs) to model temporal dependencies but face challenges such as non-parallelism and gradient vanishing. In this paper, we propose a new framework called OadTR that applies the power of Transformers to online action detection. Transformers, known for their strong temporal modeling capabilities, have been successful in natural language processing and vision tasks. OadTR is an encoder-decoder architecture that learns long-range historical relationships and future information simultaneously to classify current actions. Our experiments on public datasets show that OadTR outperforms state-of-the-art methods in terms of both mAP and mcAP metrics. This work contributes by introducing Transformers to the online action detection task, designing an efficient encoder-decoder architecture, and achieving significant improvements in performance.