We live in a dynamic world where scenes change rapidly in appearance across time and view angle. To accurately model the world, a scene representation capturing lighting, physics, and 3D structure is needed. Traditional approaches, including state-of-the-art motion capture systems, fail to handle complex occlusions and fine details of motion. High-resolution coverage requires a prohibitive amount of memory. Recent work has used neural networks as a parametrization for scene details, but these representations require static scenes and a large number of images from multiple cameras. In this paper, we propose Neural Radiance Flow (NeRFlow), a novel approach to learning a dynamic scene representation that allows photorealistic novel view synthesis with limited observations. Our approach aggregates sparse observations across time to learn a coherent spatio-temporal scene representation. We achieve this using a radiance flow field represented by implicit neural functions. Our model is fully differentiable and can be trained directly using gradient backpropagation. We evaluate our approach on various challenging scenarios, including fluid dynamics, occlusion-heavy indoor scenes, and human motion capture, and demonstrate high-quality 4D view synthesis. Furthermore, our approach serves as a dynamic scene prior, outperforming classical and internal learning methods in super-resolution and image denoising tasks.