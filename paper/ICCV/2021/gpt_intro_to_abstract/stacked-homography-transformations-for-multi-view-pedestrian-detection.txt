Multi-view object detection is a technique that aims to detect objects in a scene using synchronized images from multiple viewpoints. Compared to single-view detection, multi-view detection allows for the aggregation of information across viewpoints and the inference of 3D structures, making it more robust to occlusions. In this paper, we focus on detecting pedestrians from multi-view images and aim to generate an occupancy map from a bird's eye view perspective. However, estimating the occupancy map from multi-view images poses challenges due to changes in viewpoint, occlusions, and ambiguities in object appearances. Additionally, assembling occupancy information across views is difficult due to incomplete representations of the scene in individual views. Previous work has proposed projecting features from 2D images onto shared spaces to address these challenges, but they either involve expensive 3D convolutions or inaccurate 2D projections. To overcome these limitations, we propose a method called Stacked Homography Transformations (SHOT), which projects feature maps onto different height levels to establish 3D correspondences for different semantic parts of pedestrians. We also design a soft selection module to enable the aggregation of occupancy information across views. Our method achieves state-of-the-art performance on benchmark datasets and demonstrates generalization ability under varying scene and camera viewpoints. Our contributions include the development of SHOT for accurate 3D correspondences, a theoretical analysis of SHOT's properties, and experimental evaluation on standard benchmarks.