This paper introduces a novel approach for dense depth estimation from 2D images using a set of 3D points as constraints. The proposed method leverages a multi-scale 3D point fusion neural network architecture, which is lightweight yet able to efficiently exploit the geometric constraints provided by a sparse set of 3D points. The results demonstrate state-of-the-art performance on the NYU-Depth-v2 and KITTI datasets, with the network using only a fraction of the number of parameters compared to recent architectures. Additionally, when combined with 3D point clouds obtained from COLMAP, the proposed method outperforms recent deep learning-based multi-view stereo and structure-from-motion methods in both accuracy and compactness. This approach offers flexibility and cost savings in depth sensing applications, making it suitable for various scenarios including mobile imaging, AR frameworks, robotics, and autonomous driving.