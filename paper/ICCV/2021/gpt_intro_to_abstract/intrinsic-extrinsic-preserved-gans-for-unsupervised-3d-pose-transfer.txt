Efficient manipulation of 3D meshes using generative models is important in various practical applications in the computer graphics field. In this paper, we focus on unsupervised transfer of 3D pose style from a source mesh to a target mesh. Although some attempts have been made to address this task, current methods still have limitations due to various issues. Most existing methods require strict constraints on mesh correspondences, which often involve manual efforts or specific data requirements. Additionally, previous works that claim to achieve unsupervised 3D pose deformation still rely on constraints in the training datasets. Furthermore, the nature of 3D meshes being embedded in continuous space with arbitrary orders and complex geometric attributes makes the task more challenging compared to 2D images. Moreover, conventional geodesic-based methods for learning deformable 3D shapes suffer from intensive computations, making them unsuitable for large-scale datasets. To address these challenges, we propose a novel Intrinsic-Extrinsic Preserved Generative Adversarial Network (IEP-GAN). The IEP-GAN consists of two-branch discriminators, a global branch to enhance generative capacity and an extrinsic branch to improve pose style learning. We introduce a geodesic-adaptive sampling strategy to compute a regional geometric-preserved loss, which effectively regulates geometric preservation and improves computational efficiency. Our proposed method is the first to achieve unsupervised 3D pose transfer without human supervision. The IEP-GAN framework is the first GAN-based approach for 3D human pose learning. Experimental results on multiple datasets demonstrate that our method achieves state-of-the-art performance in terms of visual quality. Additionally, we explore the possibility of using the resulting embedding space for 3D human mesh manipulation tasks such as smooth pose transfer, interpolation, and shape swapping.