The ability to generate a realistic and continuous video from a single image is a challenging task in computer science. This paper proposes a hybrid framework that combines geometry and image synthesis techniques to tackle this problem. The framework uses disparity maps to represent scene geometry and follows a render-refine-and-repeat process. First, the current frame is rendered from a new viewpoint using disparity to ensure geometric accuracy. Then, the image and geometry are refined to add detail and synthesize new content. This process can be repeated recursively, allowing for perpetual generation with arbitrary trajectories. The framework is trained using a large dataset of drone footage, and experiments show that it outperforms existing methods in generating plausible frames for extended time horizons. However, some limitations, such as a lack of global consistency in the generated world, still exist. The proposed method and dataset are expected to contribute to further advancements in generative methods for large-scale scenes.