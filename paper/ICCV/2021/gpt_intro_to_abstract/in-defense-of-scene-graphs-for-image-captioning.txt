The mainstream approach to image captioning relies on convolutional image features and attention to generate captions. However, recent studies have explored the use of scene graph representations of images to enhance captioning models. The literature has mixed opinions on the effectiveness of scene graphs for captioning. Some studies have reported improvements, while others have found that scene graphs alone yield poor results. In this paper, we identify the challenges in utilizing scene graphs effectively and investigate ways to harness them for captioning. We discuss the different types of scene graph representations and the issues with the current methods that use disparate datasets for training. We also propose several novel approaches to enhance scene graphs for captioning, such as leveraging human-object interaction information and utilizing spatial information of the nodes. We introduce a new captioning model, SG2Caps, that utilizes scene graphs alone for caption generation and achieve competitive performance without visual features. Our experimental results show that scene graphs and textual scene graphs are not compatible with each other for caption generation, and we propose a learnable transformation from scene graphs to captions. We also demonstrate that incorporating spatial locations and human-object interaction information in scene graphs significantly improves captioning performance. Additionally, we devise a light-weight visual feature fusion strategy for our captioning model. Our contributions highlight the potential of scene graphs for generating captions and offer insights into leveraging them effectively in image captioning tasks.