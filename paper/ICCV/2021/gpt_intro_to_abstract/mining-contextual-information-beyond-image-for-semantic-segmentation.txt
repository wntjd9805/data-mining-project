Semantic segmentation is an important task in computer vision that aims to accurately assign semantic labels to each pixel in an image. This task has a wide range of potential applications such as autonomous driving, medical diagnosing, and robot sensing. In recent years, deep neural networks have been the dominant approach for semantic segmentation. These networks typically adopt an encoder-decoder architecture, with methods like FCN serving as the foundation. Various techniques have been proposed to improve segmentation results, including graphical models, novel backbone networks, and aggregation of contextual information. Context modeling is essential for semantic segmentation, with prior methods focusing on surrounding pixels in the input image. However, these approaches neglect to consider potential contextual information beyond the image itself. To address this limitation, we propose a method for mining contextual information beyond the input image. We utilize a feature memory module to store dataset-level representations of different categories, and then aggregate this contextual information by weighting it based on the class probability distribution of pixel representations in the current image. We also introduce a representation consistent learning strategy to enhance intra-class compactness and inter-class dispersion. Our contributions include the exploration of contextual information beyond the input image, the design of a feature memory module, and the development of a representation consistent learning strategy. We demonstrate the effectiveness of our approach by incorporating it into existing segmentation networks and achieving state-of-the-art performance on benchmark datasets. This work presents a novel perspective on addressing the context aggregation problem in semantic image segmentation.