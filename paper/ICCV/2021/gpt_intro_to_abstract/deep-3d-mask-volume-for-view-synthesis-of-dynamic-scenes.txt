In recent years, there have been significant advancements in view synthesis techniques for creating immersive virtual experiences. However, incorporating temporal information into these techniques is crucial for a more compelling and intimate interaction with the virtual scene. This paper focuses on a specific setup where input videos are obtained from static, binocular cameras and novel views are extrapolated from these videos. This setup is particularly useful for scenarios such as 3D teleconferencing, surveillance, and virtual reality playback.While existing image view synthesis algorithms can be applied to each frame of the video, they often suffer from temporal inconsistency and flickering artifacts. These issues arise from the algorithm's inability to predict occluded regions consistently over time. To address this problem, we propose a method that leverages the static background information across time. We introduce a 3D mask volume, which allows for manipulation in 3D space, to reason about moving objects in the scene and reuse static background observations. By blending multiple sets of multiplane images (MPI) derived from instantaneous and background inputs, we are able to render novel views with minimal flickering artifacts.To facilitate the training of our method, we also introduce a new multi-view video dataset. This dataset consists of 96 dynamic scenes captured with a custom camera rig comprised of 10 action cameras. The dataset includes various outdoor environments and human interactions, providing a diverse range of scenarios for evaluation. We demonstrate that our proposed method outperforms previous state-of-the-art techniques in terms of temporal stability, even when using only two input views.In summary, our contributions include a multi-view video dataset and a novel 3D volumetric mask that enables the segmentation of dynamic objects from the static background in a temporally stable manner.