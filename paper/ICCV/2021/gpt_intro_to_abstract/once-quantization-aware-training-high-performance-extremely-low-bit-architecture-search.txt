Quantization Neural Networks (QNN) is a promising research direction in computer science for deploying deep neural networks on edge devices. Existing methods combine Network Architecture Search (NAS) with quantization to improve performance, but they have limitations in terms of sub-optimal performance and instability. In this paper, we propose Once Quantization-aware Training (OQAT), a framework that trains a quantized supernet with shared step size and deploys quantized weights without retraining. OQAT progressively produces a series of quantized models under different bit-widths, leveraging NAS approaches and bit inheritance mechanisms. Experimental results show that our OQATNets achieve state-of-the-art results on the ImageNet dataset, outperforming existing methods in terms of accuracy and computation cost. Our contributions include the OQAT framework, the bit inheritance mechanism, and insights into quantization-friendly architecture design.