Domain adaptation has gained significant attention in computer vision due to its ability to train models without the need for ground truth labels in the target domain. While existing methods have made progress in image-based tasks, such as classification, semantic segmentation, and object detection, challenges still exist in video adaptation tasks due to the complexity of video data. This paper focuses on video domain adaptation for action recognition, utilizing appearance and motion as the two main modalities. Motion modalities, such as optical flow, have been shown to be more domain-invariant, while RGB can capture semantic cues. The authors propose a cross-modal contrastive learning framework that aligns cross-modal representations from the same video and aligns representations between the source and target domains in each modality. They propose two contrastive learning objectives: cross-modal contrastive learning and cross-domain contrastive learning. The proposed framework enables feature learning across different modalities and domains, providing a unified approach that combines the benefits of each modality. The authors evaluate their method on benchmark datasets for video action recognition and demonstrate its effectiveness compared to state-of-the-art domain adaptation techniques. The contributions of this work include the proposed multi-modal framework, the application of contrastive learning, and the achievement of state-of-the-art results in video domain adaptation.