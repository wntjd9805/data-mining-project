Saliency detection is a crucial task in computer vision, with applications ranging from image understanding to action recognition. Although there have been advancements in accurately locating salient objects, challenges still exist, particularly in complex scenes with cluttered backgrounds or low-contrast lighting conditions. The integration of RGB and depth images has gained interest in improving saliency detection performance. Current fusion strategies include early fusion, late fusion, and middle fusion, each with its limitations in accurately capturing the complex interactions between the two modalities. This paper introduces a novel specificity-preserving network for RGB-D saliency detection (SP-Net) that explores shared information and modality-specific characteristics. The proposed network uses two encoder subnetworks to extract multi-scale features for each modality and a cross-enhanced integration module to fuse cross-modal features and learn shared representations. Additionally, a modality-specific decoder and a shared decoder are constructed to combine hierarchical features and integrate modality-specific features. Experimental results on six public datasets demonstrate the superiority of SP-Net over benchmarking methods. The paper also conducts an attribute-based evaluation, studying the performance of RGB-D saliency detection methods under different challenging factors, which has not been done previously.