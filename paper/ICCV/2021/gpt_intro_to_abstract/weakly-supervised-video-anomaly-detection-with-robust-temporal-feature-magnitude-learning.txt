Video anomaly detection has become an important area of research due to its potential applications in autonomous surveillance systems. The goal of video anomaly detection is to identify the time window in which an abnormal event occurs, such as bullying, shoplifting, or violence. While some approaches in this field have explored the use of one-class classifiers trained exclusively with normal videos, the best performing methods employ a weakly-supervised setup using video-level labels for normal or abnormal events. However, weakly supervised anomaly detection faces challenges in identifying abnormal snippets within a whole video labeled as abnormal, as the majority of snippets in abnormal videos are typically normal events that can overwhelm the training process. Additionally, abnormal snippets may not be sufficiently distinct from normal ones, making it difficult to achieve a clear separation between the two. To address these challenges, this paper proposes a novel method called Robust Temporal Feature Magnitude (RTFM) learning. RTFM relies on the temporal feature magnitude of video snippets, where high magnitude features indicate abnormal snippets and low magnitude features represent normal snippets. By using feature magnitude instead of classification scores, RTFM mitigates issues related to multiple-instance learning (MIL) approaches, such as the selection of incorrect abnormal snippets and the weak training signal provided by classification scores. The paper demonstrates that RTFM outperforms state-of-the-art methods in anomaly detection benchmarks and achieves better sample efficiency and anomaly discriminability. The proposed method combines the learning of long and short-range temporal dependencies using pyramid of dilated convolutions (PDC) and a temporal self-attention module (TSA).