The paper introduces the concept of few-shot classification, which allows machine learning algorithms to learn new tasks with limited supervision. It discusses the limitations of current algorithms that require large amounts of labeled examples and proposes few-shot classification as a solution. The paper explores different approaches, such as finetuning-based transfer and meta-learning, to leverage prior knowledge from a base domain to solve a novel task. However, for tasks that lack a related base domain with labeled data, alternative techniques such as unsupervised learning or cross-domain learning are explored. The paper focuses on the problem of effective finetuning specific to the novel task and proposes the use of distractors, unlabelled data exclusive to the task, to improve few-shot generalization. It introduces the ConFT method, based on a contrastive loss, which incorporates distractors to boost generalization. The paper demonstrates the effectiveness of the proposed method in cross-domain and unsupervised prior learning settings, outperforming state-of-the-art approaches.