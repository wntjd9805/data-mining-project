This paper introduces a method called Neural Articulated Radiance Field (NARF) to represent articulated 3D objects using the inverse graphics paradigm. The goal is to learn a disentangled representation that allows for rendering of novel views and poses of objects such as human bodies from images. Previous approaches have focused on static scenes or rigid objects, and there are challenges in efficiently modeling articulated 3D objects with neural networks due to joint variability, self-occlusions, and non-linear transformations. Recent progress in implicit representations of 3D objects has paved the way for the development of NARF. The proposed method extends the Neural Radiance Field (NeRF) framework, which has been successful for static scenes, to handle articulated objects. NARF predicts the radiance field at a 3D location based on the most relevant articulated part, identified using sub-networks that output probabilities for each part. The spatial configurations of parts are computed explicitly with a kinematic model, and NARF predicts the density and view-dependent radiance conditioned on the properties of the selected part. The presented NARF has several advantageous properties, including disentangled representation of camera viewpoint and bone parameters, learning from a sparse set of 2D images with pose annotations, learning part segmentation from images with pose annotations, and the ability to train for articulated objects of various shapes and appearances. Overall, this approach enables the rendering of novel views and poses of articulated 3D objects with little increase in computational complexity.