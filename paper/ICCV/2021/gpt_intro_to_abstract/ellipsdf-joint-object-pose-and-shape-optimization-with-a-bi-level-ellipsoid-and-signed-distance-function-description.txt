Range sensors such as RGB-D cameras and LIDARs have become popular for robot localization and mapping. However, existing map representations in RGB-D SLAM primarily focus on geometric information and lack semantic details about objects in the scene. This paper proposes a model called ELLIPSDF that combines both geometric and semantic information to create expressive yet compact object-level maps. ELLIPSDF utilizes a shared latent representation to predict object shapes and employs a bi-level object model with coarse and fine levels to optimize object pose and shape. The proposed model allows for online computation, low onboard memory use, and preservation of object details. Experimental results demonstrate the effectiveness of ELLIPSDF in inferring object-level maps from multi-view RGB-D camera observations.