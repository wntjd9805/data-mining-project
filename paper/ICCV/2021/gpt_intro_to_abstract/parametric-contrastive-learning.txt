Convolutional neural networks (CNNs) have achieved remarkable success in tasks such as image classification, object detection, and semantic segmentation. The performance of CNNs has been further improved through neural network search. However, when dealing with real-world applications, the long-tailed distribution problem often arises, where a few classes have many instances while most classes have only a few instances. Learning in such an imbalanced setting is challenging, as low-frequency classes can be overwhelmed by high-frequency ones, leading to significant performance degradation in CNNs. Contrastive learning, a research topic in self-supervised representation learning, has shown promise in addressing this issue. However, in an imbalanced dataset, our theoretical analysis reveals that high-frequency classes have a higher lower bound of loss and contribute more importance than low-frequency classes, leading to model bias and increased difficulty in imbalanced learning. To overcome this shortcoming, we propose the Parametric Contrastive Loss (PaCo), an extension of supervised contrastive loss that introduces a set of parametric class-wise learnable centers. We demonstrate that PaCo outperforms supervised contrastive learning in long-tailed recognition benchmarks and achieves new records. Experimental results on full ImageNet and CIFAR datasets validate the effectiveness of PaCo even in a balanced setting.