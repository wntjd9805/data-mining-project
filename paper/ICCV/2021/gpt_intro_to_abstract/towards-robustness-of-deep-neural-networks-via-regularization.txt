Deep Neural Networks (DNNs) are vulnerable to adversarial examples, leading to security concerns in practical applications of deep learning. Despite existing adversarial training methods, we propose a novel defense and detection method based on the observation that adversarial examples often lie outside the natural image data manifold. We introduce a defense framework called Embedding Regularized Classifier (ER-Classifer) that pushes adversarial examples back to the natural data manifold. ER-Classifier incorporates a discriminator to enforce the embedding space to follow a prior distribution, removing the effect of adversarial distortion. Experimental results on various benchmark datasets demonstrate that ER-Classifier outperforms other methods significantly. This paper contributes a novel end-to-end robust DNN framework, an objective for low-dimensional projection, a detection process, and extensive experiments showcasing the robustness of ER-Classfier under white-box attacks.