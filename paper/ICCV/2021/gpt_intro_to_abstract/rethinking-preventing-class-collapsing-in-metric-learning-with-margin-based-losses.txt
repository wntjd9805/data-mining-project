Metric learning is a technique in computer science that aims to learn a function that maps high-dimensional data into a lower-dimensional space, where similar data points are close together. Deep metric learning approaches have shown promising results in various tasks such as face identification, zero-shot learning, image retrieval, and fine-grained recognition. However, when dealing with classes that have diverse appearances, it becomes challenging to learn an embedding representation that accurately captures all modes of intra-class variation. This can result in the collapse of all modes to a single point in the embedding space, reducing the generalization capabilities of the network and leading to sub-par performance. Researchers have proposed different approaches to mitigate this issue. In the case of the SoftMax loss, a multi-center approach that uses multiple centers for each class has been suggested to capture the hidden distribution of the data. For margin-based losses like the triplet loss and margin loss, it was believed that they might offer relief from class collapsing. Theoretically, it has been proven that margin-based losses have minimal solutions without class collapsing under certain assumptions. However, we formulate a noisy framework and demonstrate that, with modest noise assumptions on the labels, margin-based losses still suffer from class collapse, while the easy positive sampling method allows for more diverse solutions.To support our theoretical findings, we conducted an extensive empirical study using real-world datasets. Our study confirms the class-collapsing phenomena and shows that the easy positive sampling method creates a more diverse embedding that leads to better generalization performance. These results suggest that the noisy environment framework better aligns with the training dynamics of neural networks in real-world scenarios. In summary, this paper investigates the challenges of learning an embedding representation that captures diverse class appearances. We propose a noisy framework and demonstrate through theory and experiments that the easy positive sampling method improves the diversity and generalization performance of the embedding. The findings contribute to the understanding of metric learning and have practical implications for real-world applications.