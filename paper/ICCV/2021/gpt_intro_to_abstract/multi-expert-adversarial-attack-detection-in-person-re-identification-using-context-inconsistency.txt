The introduction of this computer science paper discusses the success of deep neural networks (DNNs) in various computer vision tasks and highlights the importance of person ReID, a task aimed at retrieving pedestrians across multiple cameras. However, it is found that DNN-based ReID models are vulnerable to adversarial examples, which are slightly perturbed input images that can lead to incorrect predictions. This vulnerability poses a threat to the security and accuracy of ReID systems. Existing defense methods for image classification do not apply to the person ReID problem. The paper proposes a new strategy called Multi-Expert Adversarial Attack Detection (MEAAD) to detect adversarial attacks in person ReID systems by detecting context inconsistency. MEAAD utilizes multiple ReID networks with different architectures as expert models and considers three types of relations: the relations between the query and its support samples, the relations among the support samples, and the relations between the support samples of different experts. The paper presents an empirical study demonstrating that context inconsistency can be used to detect adversarial attacks. The contributions of this paper include the development of the first adversarial attack detection strategy for ReID systems and the proposal of MEAAD as a method for detecting adversarial attacks by examining context inconsistency. Experimental results on the DukeMTMC-ReID and Market1501 datasets show that MEAAD effectively detects various adversarial attacks with high ROC-AUC scores.