In this paper, we address the challenge of constructing a large-scale audio-visual dataset from uncurated Internet videos without relying on manual annotation or verification. We propose a two-step process for self-supervised learning from unlabeled videos, focusing on the first step of dataset curation. Our approach involves solving a subset selection problem using an information-theoretic measure of audio-visual correspondence as a selection criterion. We introduce a clustering-based solution to estimate mutual information (MI) by measuring the agreement between partitions of data obtained from off-the-shelf models as feature extractors. To ensure scalability, we use SGD for K-means clustering and a mini-batch greedy method for subset maximization. We evaluate our approach on a large collection of videos, producing a dataset of 100 million 10-second clips with high audio-visual correspondence, which is two orders of magnitude larger than existing datasets in the literature. We demonstrate the utility of our approach in self-supervised audio-visual representation learning, showing competitive or better performance compared to baseline datasets. Our contributions include the proposed subset optimization approach, evaluation of pipeline components through controlled experiments, and the release of ACAV100M, a large-scale open-domain dataset for audio-visual representation learning.