The paper discusses the challenges in synthesizing 3D people in 3D scenes, despite progress being made in 3D scene understanding and capturing 3D human motion. The paper introduces SAMP (Scene-Aware Motion Prediction), a stochastic model that generates goal-conditioned and scene-aware motion sequences of virtual humans interacting with 3D scenes. SAMP includes novel components such as MotionNet, a conditional variational autoencoder that models a wide range of styles while performing target actions, and GoalNet, a neural network that generates plausible contact points and orientations on target objects. The paper also incorporates explicit path planning to ensure obstacle avoidance, and presents a new MoCap dataset with diverse human-scene interactions. The contributions of the paper include the novel stochastic model, the method for modeling plausible goal locations and orientations, the incorporation of explicit path planning, and the new MoCap dataset.