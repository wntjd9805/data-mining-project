In recent years, two-stream approaches have emerged as the dominant paradigm in video-based action recognition. These methods utilize two neural modules to process a video stream, with each module capturing different types of information about the scene. The idea behind these approaches is that actions are composed of interactions between people and their environment, which can be broken down into atomic actions with different arguments. However, recognizing human object interactions in video poses a challenge due to the combinatorially large set of labels. Previous attempts to impose a top-down structure using object detections have not shown significant improvements unless combined with end-to-end RGB models or ground truth knowledge. In this paper, a hybrid approach is proposed that combines bottom-up, two-stream action recognition with top-down, structured human-object interaction detection. The approach uses a sequence of object detections to guide the learning of object-centric video features, which are then transferred to a motion pathway using an attention-based mechanism. The model's ability to generalize over object appearance is evaluated and shown to outperform existing approaches, establishing a new state of the art on benchmark datasets. The framework is also shown to transfer readily to a new domain, accurately recognizing humans interacting with IKEA furniture parts and setting a new benchmark for the dataset. The paper's main contributions include a dual-pathway approach, a feature fusion strategy, and state-of-the-art recognition performance on multiple benchmarks.