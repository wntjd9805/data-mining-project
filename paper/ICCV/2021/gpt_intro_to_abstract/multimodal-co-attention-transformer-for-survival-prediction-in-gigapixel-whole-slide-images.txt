Deep learning has revolutionized computer vision in many disciplines, but gigapixel whole-slide imaging (WSI) in computational pathology presents barriers for supervised learning tasks such as cancer prognosis. Pathology WSIs have enormous heterogeneity and can be as large as 150,000 x 150,000 pixels, requiring different approaches for slide-level classification. Current approaches often use a two-stage multiple instance learning-based (MIL) approach for WSI representation learning. MIL is effective for classifying normal tissue versus micro-metastases but struggles with fine-grained visual recognition problems such as survival outcome prediction. Survival outcome prediction requires modeling complex interactions between instances, which MIL cannot handle efficiently. Multimodal learning approaches that integrate WSIs with genomic information face additional challenges due to data heterogeneity. Many approaches use late fusion mechanisms for feature integration, limiting the ability to capture important multimodal interactions. To address these challenges, we propose a weakly-supervised, multimodal learning framework called MCAT (Multimodal Co-Attention Transformer) for survival outcome prediction. MCAT uses a cross-modality attention called genomic-guided co-attention (GCA) for early fusion, enabling the identification of informative instances using genomic features as queries. This allows for more effective feature aggregation and supervision using entire WSIs. Experimental results demonstrate that MCAT outperforms existing methods for survival outcome prediction using WSIs and genomic data, and visualization of gene-guided visual concepts provides insights into feature interactions. The code for MCAT is available at https://github.com/mahmoodlab/MCAT.