This paper introduces a novel self-supervised representation learning paradigm called TupleInfoNCE, which aims to fuse multimodal inputs without manual annotations. Unlike existing methods that only contrast different modalities, TupleInfoNCE contrasts multimodal input tuples, allowing modalities to disambiguate each other and preserve their specific information. The core challenge is determining which tuples to contrast, as always contrasting tuples with corresponding elements can lead to suboptimal results. To address this, TupleInfoNCE generates positive samples through anchor augmentation and utilizes challenging negative samples that do not necessarily correspond to each other. By optimizing hyper-parameters that control the distribution of positive and negative samples, TupleInfoNCE produces better-fused representations. The proposed method is evaluated on various multimodal fusion tasks and achieves significant improvements over state-of-the-art methods.