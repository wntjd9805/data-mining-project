Deep neural networks are extensively used in computer vision tasks, requiring large amounts of training data. However, the collection and annotation of training samples can be expensive and labor-intensive, particularly in domains with scarce publicly available data, such as medical imaging. To address this, models are often trained privately and only provide black-box access to the trained model. However, even black-box access can potentially leak sensitive information about the training data. Membership inference attacks (MIA) aim to detect such leakage by determining if a given data sample was used in the training process. MIA methods exploit the memorization tendencies of deep neural networks, as images used in training tend to have higher prediction confidence. In addition to detecting information leakage, MIA also provides insights into the degree of memorization in the victim model. The accuracy of MIA attacks varies depending on task properties, such as uncertainty in predictions and output dimensionality. In this paper, we focus on supervised image translation and semantic segmentation tasks, which exhibit these properties. We propose a MIA approach that utilizes a predictability error, computed using a linear predictor, to discriminate between hard-to-predict training samples and unseen samples. Our method does not rely on the existence of a large number of in-distribution data samples and operates on a single query to the victim model. We demonstrate the effectiveness of our approach on various benchmarks and discuss potential defenses against MIA. Our main contributions include highlighting vulnerable task properties, presenting the first MIA on image translation models, and proposing a general self-supervised predictability error for MIA.