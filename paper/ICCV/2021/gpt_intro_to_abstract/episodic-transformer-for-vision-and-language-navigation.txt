The goal of achieving an autonomous agent capable of performing household tasks has long been pursued by the research community. In recent years, simulated environments have emerged as benchmarks for assessing research progress in this area. These environments involve agents navigating and interacting with their surroundings based on natural language instructions. The vision-and-language navigation (VLN) task, in particular, requires the agent to ground human instructions in its perception and action space. However, VLN presents two main challenges: handling highly compositional tasks and understanding complex human instructions.To address the first challenge, the agent must be able to perform long compositional tasks with limited environmental visibility. Recurrent architectures have been commonly used to capture previous actions and observations, but they have limitations in capturing long-term dependencies. Inspired by the success of the attention-based transformer architecture, we propose the Episodic Transformer (E.T.) as an alternative. E.T. combines multi-modal inputs (camera observations, language instructions, and previous actions) and has access to the entire episode history, allowing for long-term memory and more efficient execution.The second challenge involves finding effective ways to specify tasks for autonomous agents. We propose using domain-specific language and temporal logic, which can define target states and their temporal dependencies independently of visual appearances and variations in human instructions. By pretraining the language encoder with synthetic instructions and exploring joint training with human instructions, we aim to improve the model's learning and generalization capabilities.To evaluate the performance of E.T., we use the ALFRED dataset, which features longer episodes and object interaction compared to other VLN datasets. Experimental results demonstrate that E.T. benefits from full episode memory and outperforms recurrent models in tasks with long horizons. Pretraining the language encoder with synthetic instructions also yields significant improvements. Additionally, we show that using synthetic instructions as intermediate representations for joint training surpasses conventional data augmentation techniques and image-based annotations.In summary, our contributions consist of proposing the Episodic Transformer architecture for VLN, showcasing its advantages over recurrent models, and introducing synthetic instructions as an intermediate interface between humans and agents. These contributions collectively enable us to achieve a new state-of-the-art performance on the challenging ALFRED dataset. The project page provides access to code and models for further exploration.