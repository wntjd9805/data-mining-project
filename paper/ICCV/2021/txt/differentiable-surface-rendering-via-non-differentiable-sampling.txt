Abstract
We present a method for differentiable rendering of 3D surfaces that supports both explicit and implicit represen-tations, provides derivatives at occlusion boundaries, and is fast and simple to implement. The method ﬁrst samples the surface using non-differentiable rasterization, then ap-plies differentiable, depth-aware point splatting to produce the ﬁnal image. Our approach requires no differentiable meshing or rasterization steps, making it efﬁcient for large 3D models and applicable to isosurfaces extracted from im-plicit surface deﬁnitions. We demonstrate the effectiveness of our method for implicit-, mesh-, and parametric-surface-based inverse rendering and neural-network training appli-cations. In particular, we show for the ﬁrst time efﬁcient, differentiable rendering of an isosurface extracted from a neural radiance ﬁeld (NeRF), and demonstrate surface-based, rather than volume-based, rendering of a NeRF. 1.

Introduction
Computing the derivatives of rendered surfaces with re-spect to the underlying scene parameters is of increasing interest in graphics, vision, and machine learning. Triangle meshes are the predominant shape representation in many industries, but mesh-based derivatives are undeﬁned at oc-clusions or when changing topology. As a result, volu-metric representations have risen in prominence for com-puter vision applications, notably Neural Radiance Fields or NeRF [27]. So far, these volumetric shape representa-tions have been rendered using volume rendering. Volume rendering is naturally differentiable, but is expensive and unnecessary if the underlying shape can be represented well by a surface.
This paper proposes a method to render both explicit (e.g., mesh) and implicit (e.g., isosurface) representations and produce accurate, smooth derivatives, including at oc-clusion boundaries. Our method uses a non-differentiable rasterization step to sample the surface and resolve occlu-sions, then splats the samples using a depth-aware, differen-tiable splatting operation. Because the sampling operation need not be differentiable, any conventional surface extrac-tion and rasterization method (e.g., Marching Cubes [24])
Figure 1. Our method provides efﬁcient, differentiable rendering for explicit and implicit surface representations. Examples in-clude a textured triangle mesh (YCB toy airplane [4]), a cubic B-spline surface, and an isosurface of a density volume (Lego from
NeRF [27]). The Lego is rendered by turning a pretrained NeRF into a surface light ﬁeld. Since surface light ﬁelds only require one evaluation per pixel, we achieve a 128× speed up for render-ing compared with the original NeRF. may be used. The splats provide smooth derivatives of the image w.r.t. the surface at occlusion boundaries. Splatting is performed on a ﬁxed-size pixel grid and is easily expressed using automatic-differentiation, avoiding the need for cus-tom gradients. Since no custom gradients are needed, both forward- and reverse-mode differentiation are immediately supported. We term this method rasterize-then-splat (RtS).
In between the rasterization and splatting steps, the sur-face samples may be shaded by any differentiable function evaluated on a rasterized image buffer – not the original sur-face – using deferred shading [9]. Since the complexity of the shading and splatting computation is bounded by the number of pixels, not the complexity of the surface, RtS is able to scale to highly detailed scenes.
One example of a differentiable shading function is a
NeRF network: given a position in space and a viewing di-rection, it outputs the corresponding radiance. While NeRF is trained using volume rendering, our method can convert a pretrained NeRF into a surface light ﬁeld [28, 39], remov-ing the need for expensive raymarching. We represent the surface as an isosurface of the density ﬁeld extracted from a pretrained NeRF, shade it with the NeRF color prediction
branch, and jointly ﬁnetune the NeRF network and the den-sity ﬁeld. The resulting optimized surface and surface light
ﬁeld matches the original NeRF network in rendering qual-ity (within 0.3 PSNR) but requires only a single network evaluation per pixel, producing a 128× speedup (Fig. 1).
We further demonstrate that RtS provides high-quality derivatives for inverse rendering of meshes and parametric surfaces, while remaining simple to implement. An imple-mentation of RtS for mesh-based rendering is provided as part of TensorFlow Graphics1. 2.