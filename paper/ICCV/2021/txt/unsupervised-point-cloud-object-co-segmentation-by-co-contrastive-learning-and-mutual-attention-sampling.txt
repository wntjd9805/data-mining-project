Abstract
This paper presents a new task, point cloud object co-segmentation, aiming to segment the common 3D objects in a set of point clouds. We formulate this task as an ob-ject point sampling problem, and develop two techniques, the mutual attention module and co-contrastive learning, to enable it. The proposed method employs two point sam-plers based on deep neural networks, the object sampler and the background sampler. The former targets at sam-pling points of common objects while the latter focuses on the rest. The mutual attention module explores point-wise correlation across point clouds. It is embedded in both sam-plers and can identify points with strong cross-cloud cor-relation from the rest. After extracting features for points selected by the two samplers, we optimize the networks by developing the co-contrastive loss, which minimizes feature discrepancy of the estimated object points while maximizing feature separation between the estimated object and back-ground points. Our method works on point clouds of an arbitrary object class. It is end-to-end trainable and does not need point-level annotations.
It is evaluated on the
ScanObjectNN and S3DIS datasets and achieves promis-ing results. The source code will be available at https:
//github.com/jimmy15923/unsup_point_coseg. 1.

Introduction
Point clouds retain 3D geometric structures and are adopted in many 3D vision applications, such as remote sensing [24], autonomous driving [9, 14, 30], and robotics
[21]. As an essential technique for 3D understanding, point cloud segmentation gains significant progress owing to ad-vanced network architectures [32, 33, 38, 43, 45, 51] and large-scale datasets [3, 5, 6, 12, 14, 26, 30, 34, 40]. Despite effectiveness, deep-learning-based methods for point cloud segmentation rely on lots of training data with point-level annotations. The high annotation cost for training data col-lection impedes the utility of point cloud segmentation. 2D image object co-segmentation [16,17,20,23,50] aims to segment the common objects in a set of images without additional annotations. It significantly mitigates the prob-Figure 1: Overview of our method for unsupervised point cloud co-segmentation. The input to our method is a set of point clouds covering objects of a common category (chairs, in this example). Our method only requires 3D co-ordinates as the input. Color is added here for visualization.
Our method picks out the co-segmented points of the com-mon objects (those in red). It formulates co-segmentation as a sampling problem by employing two competitive sam-plers: one for foreground points and the other for the rest.
A mutual attention module is embedded in each sampler for capturing cross-cloud point correlation. The whole network is end-to-end trained by the proposed co-contrastive loss. lem of high annotation costs in object segmentation. How-ever, it is challenging to apply 2D image co-segmentation techniques to 3D point clouds as it must resolve three major issues. First, most image co-segmentation methods rely on object proposal generators or saliency detectors [16,17,50].
These generators and detectors work on the appearance do-main of image pixels, but do not apply to the geomet-ric domain of 3D points. Second, compared to images, point clouds are unordered and unstructured. The extracted point features are typically insufficient for co-segmentation.
Third, most 2D co-segmentation methods adopt a network pre-trained on a large dataset, e.g. ImageNet [35], to ex-tract high-level semantic features. Although point cloud model pre-training has been studied [7,36,47], training from scratch is still widely used in modern point cloud research.
In this paper, we present an unsupervised method for 1
point cloud object co-segmentation. As shown in Figure 1, our method comprises three components to tackle the three aforementioned issues, respectively. First, we cast point cloud co-segmentation as an object point sampling prob-lem. A pair of point samplers are employed: The object sampler targets at sampling points belonging to the com-mon objects, while the background sampler grabs on the rest. Through sampling, object proposals or saliency detec-tors are no longer required. Sampling is non-differentiable.
This issue has been resolved by SampleNet [13, 22], which offers differentiable point sampling from a point cloud. In this work, both object and background samplers are devel-oped upon SampleNet. SampleNet is originally designed for supervised applications. To adapt it to unsupervised co-segmentation, we develop novel co-contrastive learning to derive a pair of competitive samplers. After optimization, the two samplers complete co-segmentation.
Second, a mutual attention module is developed to ex-plore point-wise correlation across different point clouds, and is employed by both samplers. Identifying the common 2D pixels or 3D points in the given images or point clouds is a key component for co-segmentation. To this end, this module computes attention maps across clouds and com-piles informative features for co-segmentation. Compared with the self-attention module [41, 44] focusing on the cor-relation of positions within a point cloud, it computes cross-cloud attention to discover plausible foreground. The idea behind this module is that points belonging to common ob-jects typically have strong cross-cloud correspondences. It turns out that samplers equipped with this module result in better foreground-background separation.
Third, a co-contrastive loss is developed to address the lack of data for pre-training and the absence of supervisory signals for co-segmentation. This loss is designed in both object and point levels. It minimizes the feature discrepancy of points sampled by the object sampler while maximizing the feature discrepancy between points selected by different samplers. We use this loss to derive the samplers as well as their associated mutual attention modules.
The main contribution of this work is threefold. First, to the best of our knowledge, our method is the first attempt to develop an end-to-end trainable network for point cloud ob-ject co-segmentation. Second, we formulate it as a discrim-inative sampling task, which is carried out by the proposed mutual attention module and co-contrastive learning. Third, our approach is evaluated on two real datasets [3, 40], and demonstrates promising results. 2.