Abstract
Deep networks allow to obtain outstanding results in se-mantic segmentation, however they need to be trained in a single shot with a large amount of data. Continual learn-ing settings where new classes are learned in incremental steps and previous training data is no longer available are challenging due to the catastrophic forgetting phenomenon.
Existing approaches typically fail when several incremental steps are performed or in presence of a distribution shift of the background class. We tackle these issues by recreating no longer available data for the old classes and outlining a con-tent inpainting scheme on the background class. We propose two sources for replay data. The first resorts to a generative adversarial network to sample from the class space of past learning steps. The second relies on web-crawled data to re-trieve images containing examples of old classes from online databases. In both scenarios no samples of past steps are stored, thus avoiding privacy concerns. Replay data are then blended with new samples during the incremental steps. Our approach, RECALL, outperforms state-of-the-art methods. 1.

Introduction
A common requirement for many machine learning ap-plications is the ability to learn a sequence of tasks in multi-ple incremental steps, e.g., progressively introducing novel classes to be recognized, instead of using a single-shot train-ing procedure on a large dataset [34]. This problem has been widely studied in image classification and many methods propose to alleviate the forgetting of previous tasks and in-transigence of learning new ones [22, 35, 45]. When the model is exposed to samples of novel classes and is trained on them without additional provisions, the optimization leads to the so-called catastrophic forgetting phenomenon [36, 16], i.e., knowledge about previously seen classes tends to be lost.
Incremental learning on dense tasks (e.g., semantic seg-mentation), where pixel-wise predictions are performed, has
∗These authors share the first authorship.
†Our work was in part supported by the Italian Minister for Education (MIUR) under the “Departments of Excellence” initiative (Law 232/2016).
Figure 1: Replay images of previously seen classes are re-trieved by a web crawler or a generative network and further labeled. Then, the network is incrementally trained with a mixture of new and replay data. only recently been explored and the first experimental stud-ies show that catastrophic forgetting is even more severe than on the classification task [29, 31]. Current approaches for class-incremental semantic segmentation re-frame knowl-edge distillation strategies inspired by previous works on image classification [29, 5, 23, 31]. Although they partially alleviate forgetting, they often fail when multiple incremen-tal steps are performed or when background shift [5] (i.e., change of statistics of the background across learning steps, as it incorporates old or future classes) occurs.
In this paper, we follow a completely different strategy and, instead of distilling knowledge from a teacher model (i.e., the old one) to avoid forgetting, we propose to generate samples of old classes by using replay strategies. We propose
RECALL (REplay in ContinuAL Learning), a method that re-creates representations of old classes and mixes them with the available training data, i.e., containing novel classes be-ing learned (see Fig. 1). To reduce background shift we intro-duce a self-inpainting strategy that re-assigns the background region according to predictions of the previous model.
To generate representations of past classes we pursue two possible directions. The first is based on a pre-trained gener-ative model, i.e., a Generative Adversarial Network (GAN)
[15] conditioned to produce samples of an input class. The
GAN has been trained beforehand on a dataset different than the target one (we chose ImageNet as it comprehends a wide variety of classes and domains), thus requiring a Class Map-ping Module to perform the translation between the two label
spaces. The second strategy, instead, is based on crawling images from the web, querying the class names to drive the search. Both approaches allow to retrieve a large amount of weakly labeled data. Finally, we generate pseudo-labels for semantic segmentation using a side labeling module, which requires only minimal extra storage.
Our main contributions are: 1) we propose RECALL, which is the first approach to use replay data in continual semantic segmentation; 2) to the best of our knowledge, we are the first to introduce the webly-supervised paradigm in continual learning, showing how we can extract useful clues from extremely weakly supervised and noisy samples; 3) we devise a background inpainting strategy to generate pseudo-labels and overcome the background shift; 4) we achieve state-of-the-art results on a wide range of scenarios, especially when performing multiple incremental steps. 2.