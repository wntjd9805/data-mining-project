Abstract
Beneﬁting from large-scale pre-training, we have wit-nessed signiﬁcant performance boost on the popular Visual
Question Answering (VQA) task. Despite rapid progress, it remains unclear whether these state-of-the-art (SOTA) models are robust when encountering examples in the wild.
To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adver-sarial human-and-model-in-the-loop procedure. Through this new benchmark, we discover several interesting ﬁnd-ings. (i) Surprisingly, we ﬁnd that during dataset collection, non-expert annotators can easily attack SOTA VQA mod-els successfully. (ii) Both large-scale pre-trained models and adversarial training methods achieve far worse perfor-mance on the new benchmark than over standard VQA v2 dataset, revealing the fragility of these models while demon-strating the effectiveness of our adversarial dataset. (iii)
When used for data augmentation, our dataset can effec-tively boost model performance on other robust VQA bench-marks. We hope our Adversarial VQA dataset can shed new light on robustness study in the community and serve as a valuable benchmark for future work. 1.

Introduction
Visual Question Answering (VQA) [4] is a task where given an image and a question about it, the model provides an open-ended answer. A successful VQA system can be applied to real-life scenarios such as a chatbot that assists visually impaired people. In these applications, the VQA models are expected to handle diverse question types from recognition to reasoning, and answer questions faithfully based on the evidence in the image.
While model performance on the popular VQA dataset [14] has been advanced in recent years [4, 19, 3, 50, 9, 43, 54], with better visual representations [18, 54],
Figure 1: Illustration of data collection examples. The workers try to attack the VQA model for at most 5 times by asking hard questions about the image, and succeeds at the last attempt. Green (red) indicates a correct (wrong) answer. more sophisticated model designs [12, 27], large-scale pre-training [30, 41, 7, 42, 55] and adversarial training [11], today’s VQA models are still far from being robust enough for practical use. There are some works studying the robust-ness of VQA models, such as their sensitivity to visual con-tent manipulation [1], answer distribution shift [2], linguis-tic variations in input questions [39], and reasoning capa-bilities [13, 38]. However, current robust VQA benchmarks mostly suffer from three main limitations: (i) designed with heuristic rules [13, 2, 1]; (ii) focused on a single type of robustness [38, 39, 13]; (iii) based on VQA v2 [14] images (or questions), which state-of-the-art (SOTA) VQA models are trained on [13, 2, 1, 38, 39]. The images [1] or ques-tions [13, 17] are often synthesized, not provided by human.
In addition, previous data collection procedures on VQA benchmarks are often static, meaning that the data samples in these datasets do not evolve, and model performance can saturate on the ﬁxed dataset without good generalization.
For example, model accuracy on VQA v2 has been im-proved from 50% [4] to 76% [54] since inception. Simi-larly, on robust VQA benchmarks, a recent study [28] has
found that pre-trained models can greatly lift state of the art.
Yet it remains unclear whether such high performance can be maintained when encountering examples in the wild.
To build an organically evolving benchmark, we intro-duce Adversarial VQA (AVQA), a new large-scale VQA dataset dynamically collected with Human-And-Model-in-the-Loop Enabled Training (HAMLET) [47]. AVQA is built on images from different domains, including web im-ages from Conceptual Captions [40], user-generated images from Fakeddit [32], and movie images from VCR [52]. Our data collection is iterative and can be perpetually going.
We ﬁrst ask human annotators to create examples that cur-rent best models cannot answer correctly (Figure 1). These newly annotated examples expose the model’s weaknesses, and are added to the training data for training a stronger model. The re-trained model is subjected to the same pro-cess, and the collection can iterate for several rounds. Af-ter each round, we train a new model and set aside a new test set. In this way, not only is the resultant dataset more challenging than existing benchmarks, but this process also yields a “moving post” target for VQA systems, rather than a static benchmark that will eventually saturate.
With this new benchmark, we present a thorough quan-titative evaluation on the robustness of VQA models along multiple dimensions. First, we provide the ﬁrst study on the vulnerability of VQA models when under adversarial attacks by human. Second, we benchmark several SOTA
VQA models on the proposed dataset to reveal the fragility of VQA models. We observe a signiﬁcant and universal per-formance drop when compared to VQA v2 and other robust
VQA benchmarks, which corroborates our belief that ex-isting VQA models are not robust enough. Meanwhile, this also demonstrates the transferability of these adversarial ex-amples – data samples collected using one set of models are also challenging for other models. Third, as our annotators can ask different types of questions for different types of ro-bustness, our analyses show that SOTA models suffer across various questions types, especially counting and reasoning.
Our main contributions are summarized as follows. (i)
For better evaluation of VQA model robustness, we intro-duce a new VQA benchmark dynamically collected with (ii) Despite a Human-and-Model-in-the-Loop procedure. rapid advances on VQA v2 and robust VQA benchmarks, the evaluation on our new dataset shows that SOTA models are far from being robust. In fact, they are extremely vulner-able when attacked by human annotators, who can succeed within 2 trials on average. (iii) We provide a thorough anal-ysis to share insights on the shortcomings of current models as well as comparison with other robust VQA benchmarks. 2.