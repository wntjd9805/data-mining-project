Abstract
State-of-the-art approaches for visually-guided audio source separation typically assume sources that have char-acteristic sounds, such as musical instruments. These ap-proaches often ignore the visual context of these sound sources or avoid modeling object interactions that may be useful to better characterize the sources, especially when the same object class may produce varied sounds from dis-tinct interactions. To address this challenging problem, we propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning model that embeds the visual struc-ture of the scene as a graph and segments this graph into subgraphs, each subgraph being associated with a unique sound obtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a recursive neural network that emits mutually-orthogonal sub-graph embeddings of the visual graph using multi-head attention. These embeddings are used for conditioning an audio encoder-decoder towards source separation. Our pipeline is trained end-to-end via a self-supervised task consisting of separating audio sources using the visual graph from artiﬁcially mixed sounds.
In this paper, we also introduce an “in the wild” video dataset for sound source separation that contains multiple non-musical sources, which we call Audio Separation in the
Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and provides a challenging, natural, and daily-life setting for source separation. Thorough experiments on the proposed ASIW and the standard MUSIC datasets demon-strate state-of-the-art sound separation performance of our method against recent prior approaches. 1.

Introduction
Real-world events often encompass spatio-temporal in-teractions of objects, the signatures of which leave imprints both in the visual and auditory domains when captured as videos. Knowledge of these objects and the sounds that they produce in their natural contexts are essential when de-signing artiﬁcial intelligence systems to produce meaningful
Figure 1. A schematic illustration of our Audio Visual Scene Graph
Segmenter (AVSGS) framework on frames from our Audio Sep-aration in the Wild (ASIW) dataset. Given an input video and the associated audio, our method builds a spatio-temporal (fully-connected) visual scene graph spanning across the video frames, and learns alignments between sub-graphs of this scene graph and the respective audio regions. The use of the scene graph allows rich characterization of the objects and their interactions, allowing effective identiﬁcation of the sound sources for better separation. deductions. For example, the sound of a cell phone ringing is drastically different from that of one dropping on the ﬂoor; such distinct sounds of objects and their contextual inter-actions may be essential for an automated agent to assess the scene. The importance of having algorithms with such audio-visual capabilities is far reaching, with applications such as audio denoising, musical instrument equalization, audio-guided visual surveillance, or even in navigation plan-ning for autonomous cars, for example by visually localizing the sound of an ambulance.
Recent years have seen a surge in algorithms at the in-tersection of visual and auditory domains, among which visually-guided source separation – the problem of separat-ing sounds from a mixture using visual cues – has made signiﬁcant strides [5, 9, 58, 57]. State-of-the-art algorithms for this task [54, 58, 57] typically restrict the model design to only objects with unique sounds (such as musical instru-ments [5, 57]) or consider settings where there is only a single sound source, and the models typically lack the rich-ness to capture spatio-temporal audio-visual context. For example, for a video with “a guitar being played by a per-son” and one in which “a guitar is kept against a wall”, the context may help a sound separation algorithm to decide whether to look for the sound of the guitar in the audio spec-trogram; however several of the prior works only consider visual patches of an instrument as the context to guide the separation algorithm [9], which is sub-optimal.
From a learning perspective, the problem of audio-visual sound source separation brings in several interesting chal-lenges: (i) The association of a visual embedding of a sound source to its corresponding audio can be a one-to-many map-ping and therefore ill-posed. For example, a dog barking while splashing water in a puddle. Thus, methods such as [5, 9] that assume a single visual source may be misled. (ii) It is desirable that algorithms for source separation are scalable to new sounds and their visual associations; i.e., the algorithm should be able to master the sounds of varied objects (unlike supervised approaches [48, 53]). (iii) Nat-urally occurring sounds can emanate out of a multitude of interactions – therefore, using a priori deﬁned sources, as in [5, 9], can be limiting.
In this work, we rise up to the above challenges using our Audio Visual Scene Graph Segmenter (AVSGS) frame-work for the concrete task of sound source separation. Fig-ure 1 presents the input-output setting for our task. Our setup represents the visual scene using spatio-temporal scene graphs [18] capturing visual associations between objects oc-curring in the video, towards the goal of training AVSGS to infer which of these visual associations lead to auditory grounding. To this end, we design a recursive source sepa-ration algorithm (implemented using a GRU) that, at each recurrence, produces an embedding of a sub-graph of the visual scene graph using graph multi-head attention. These embeddings are then used as conditioning information to an audio separation network, which adopts a U-Net style encoder-decoder architecture [39]. As these embeddings are expected to uniquely identify a sounding interaction, we en-force that they be mutually orthogonal. We train this system using a self-supervised approach similar to Gao et al. [9], wherein the model is encouraged to disentangle the audio corresponding to the conditioned visual embedding from a mixture of two or more different video sounds. Importantly, our model is trained to ensure consistency of each of the separated sounds by their type, across videos. Thus, two guitar sounds from two disparate videos should sound more similar than a guitar and a piano. Post separation, the sepa-rated audio may be associated with the visual sub-graph that induced its creation, making the sub-graph an Audio-Visual
Scene Graph (AVSG), usable for other downstream tasks.
We empirically validate the efﬁcacy of our method on the popular Multimodal Sources of Instrument Combinations (MUSIC) dataset [58] and a newly adapted version of the
AudioCaps dataset [20], which we call Audio Separation in the Wild (ASIW). The former contains videos of perform-ers playing musical instruments, while the latter features videos of naturally occurring sounds arising out of complex interactions in the wild, collected from YouTube. Our experi-ments demonstrate the importance of visual context in sound separation, and AVSGS outperforms prior state-of-the-art methods on both of these benchmarks.
We now summarize the key contributions of the paper:
• To the best of our knowledge, ours is the ﬁrst work to employ the powerful scene graph representation [18] for the task of visually-guided audio source separation.
• We present AVSGS for this task, that is trained to pro-duce mutually-orthogonal embeddings of the visual sub-graphs, allowing our model to infer representations of sounding interactions in a self-supervised way.
• We present ASIW, a large scale in the wild dataset adapted from AudioCaps for the source separation task.
This dataset features sounds arising out of natural and complex interactions.
• Our AVSGS framework demonstrates state-of-the-art performance on both the datasets for our task. 2.