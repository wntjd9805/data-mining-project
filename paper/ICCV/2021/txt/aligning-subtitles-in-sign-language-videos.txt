Abstract
The goal of this work is to temporally align asyn-chronous subtitles in sign language videos. In particular, we focus on sign-language interpreted TV broadcast data comprising (i) a video of continuous signing, and (ii) subti-tles corresponding to the audio content. Previous work ex-ploiting such weakly-aligned data only considered finding keyword-sign correspondences, whereas we aim to localise a complete subtitle text in continuous signing. We propose a Transformer architecture tailored for this task, which we train on manually annotated alignments covering over 15K subtitles that span 17.7 hours of video. We use BERT subti-tle embeddings and CNN video representations learned for sign recognition to encode the two signals, which interact through a series of attention layers. Our model outputs frame-level predictions, i.e., for each video frame, whether it belongs to the queried subtitle or not. Through extensive evaluations, we show substantial improvements over exist-ing alignment baselines that do not make use of subtitle text
*Equal contribution embeddings for learning. Our automatic alignment model opens up possibilities for advancing machine translation of sign languages via providing continuously synchronized video-text data. 1.

Introduction
Sign languages constitute a key form of communication for Deaf communities [50]. Our goal in this paper is to temporally localise subtitles in continuous signing video.
Automatic alignment of subtitle text to signing content has great potential for a wide range of applications including assistive tools for education and translation, indexing of sign language video corpora, efficient subtitling technology for signing vloggers1, and automatic construction of large-scale sign language datasets that support computer vision and linguistic research.
Despite recent advances in computer vision, machine 1Unlike spoken vlogs that benefit from automatic closed captioning on sites such as YouTube, signing vlog creators who wish to provide written subtitles must both translate and align their subtitles manually.
translation between continuous signing and written lan-guage remains largely unsolved [6]. Recent works [11, 12] have shown promising translation results, but to date these have been achieved only in constrained settings where con-tinuous signing is manually pre-segmented into clips, with each clip associated to a written sentence from a limited vo-cabulary. Two key bottlenecks for scaling up translation to continuous signing depicting unconstrained vocabularies are (i) the segmentation of signing into sentence-like units, and (ii) the availability of large-scale sign language training data.
Manual alignment of subtitles to sign language video is tedious – an expert fluent in sign language takes approxi-mately 10-15 hours to align subtitles to 1 hour of continu-ous sign language video. In this work, we focus on the task of aligning a particular known subtitle within a given tem-poral signing window. We explore this task in the context of sign language interpreted TV broadcast footage – a readily available and large-scale source of data – where the subti-tles are synchronised with the audio, but the corresponding sign language translations are largely unaligned due to dif-ferences between spoken and sign languages as well as lags from the live interpretation.
Subtitle alignment to continuous signing remains a very challenging task. First, sign languages have grammatical structures that vary considerably from those of spoken lan-guages [50], and as a result the ordering of words within a subtitle as well as the subtitles themselves is often not maintained in the signing (see Fig. 1). Second, the dura-tion of a subtitle varies considerably between signing and speech due to differences in speed and grammar. Third, the signing corresponds to a translation of the speech that ap-pears in the subtitles as opposed to a transcription: there is no direct one-to-one mapping between subtitle words and signs produced by interpreters, and entire subtitles may not be signed.
Previous work exploiting such weakly-aligned data has mainly focused on finding sparse correspondences between keywords in the subtitle and individual signs [3, 39, 53], as opposed to localising the start and end times of a complete subtitle text in continuous signing. Though, as we show, lo-calising isolated signs identified by keyword spotting never-theless forms a useful pretraining task for full subtitle align-ment. Most closely related to our work, Bull et al. [9] con-sider the task of segmenting a continuous signing video into subtitle units purely based on body keypoints. In fact, sim-ilarly to speech which can be segmented based on prosodic cues such as pauses, sign sentence boundaries can to an ex-tent be detected through visual cues such as lowering the hands, head movement, pauses, and facial expressions [24].
However, as shown in our evaluations in Sec. 4, such ap-proaches based on prosody-only perform poorly in our set-ting, where subtitles do not necessarily correspond to com-plete sign sentences with clear visual boundaries.
In this paper, we instead propose to use the subtitle text as an additional signal for better alignment. We make the following three contributions: (1) we show that encoding the subtitle text as input to the alignment model significantly improves the temporal localisation quality as opposed to only relying on visual cues to segment continuous sign lan-guage videos into subtitle units; (2) we design a novel for-mulation for the subtitle alignment task based on Trans-formers; and (3) we present a comprehensive study ablating our design choices and provide promising results for this new task when evaluating on unseen signers and content. 2.