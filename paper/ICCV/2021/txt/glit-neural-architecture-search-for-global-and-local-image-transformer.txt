Abstract
We introduce the first Neural Architecture Search (NAS) method to find a better transformer architecture for image recognition. Recently, transformers without CNN-based backbones are found to achieve impressive performance for image recognition. However, the transformer is designed for NLP tasks and thus could be sub-optimal when directly used for image recognition. In order to improve the visual representation ability for transformers, we propose a new search space and searching algorithm. Specifically, we in-troduce a locality module that models the local correlations in images explicitly with fewer computational cost. With the locality module, our search space is defined to let the search algorithm freely trade off between global and local infor-mation as well as optimizing the low-level design choice in each module. To tackle the problem caused by huge search space, a hierarchical neural architecture search method is proposed to search the optimal vision transformer from two levels separately with the evolutionary algorithm. Exten-sive experiments on the ImageNet dataset demonstrate that our method can find more discriminative and efficient trans-former variants than the ResNet family (e.g., ResNet101) and the baseline ViT for image classification. The source codes are available at https://github.com/bychen515/GLiT. 1.

Introduction
Convolutional Neural Networks (CNN) -based architec-ture (e.g., ResNet [14]) contributes to the great success of deep learning in computer vision tasks [26, 6, 19] for past several years. By stacking a set of CNN layers, CNN-based models can achieve larger receptive filed and perceive more contextual information on scarifies of the efficiency. Driven by the great success of transformer
[30] in Natural Lan-guage Processing(NLP) tasks, there are increasing interests
*Equal contribution
†Corresponding author
Figure 1. Top-1 accuracy (y-axis) and FLOPs (x-aixs) for different backbones on ImageNet. GLiT is our method. in the computer vision community to develop more efficient architectures based on the transformer [11, 28, 4, 37] which can manipulate the global correlations directly. Among these works, vision transformer (ViT) is a representative one [11] as it does not rely on the CNN-based backbone to extract features and solely relies on self-attention mod-ules in transformer to establish global correlations among all input image patches. While ViT achieves impressive performance, if extra training data is not used, ViT still has lower accuracy than the well-designed CNN models such as
ResNet-101 [14]. To further exploit the potential of trans-former in image recognition tasks, DeiT [28] uses teacher-student strategy for distilling knowledge to the transformer token. These two methods rely on the original transformer architecture but neglect potential gap between NLP tasks and image recognition tasks in architecture.
In this work, we argue that there are unignorable gaps between different kinds of data modalities (e.g., image and text), leading to the disparities between different tasks.
Thus, directly applying the vanilla transformer architecture to other tasks may be sub-optimal. It is natural that there exists better transformer architectures for image recogni-tion. However, hand-designing such an architecture is time consuming since there are too many influential fac-tors to be considered. On one hand, Neural Architecture
Search (NAS) has achieved great progress in computer vi-sion tasks [13, 22, 2]. It can automatically discover an opti-mal network architecture without manual try-and-error. On the other hand, computer vision community still has not in-vestigated NAS for transformers.
Based on the above observations, we intend to discover a better transformer architecture by NAS for specific tasks, e.g., the image classification task in this work.
There are two key factors when designing NAS for trans-former in vision tasks, a well-designed search space that contains candidates with good performance and an efficient searching algorithm to explore the search space.
A na¨ıve search space would only contain the architec-ture parameters in the transformer architecture, such as the feature dimension for query and value, the number of at-tention heads in the Mutli-Head Attention (MHA) mecha-nism, and the number of MHA blocks. However, the search space does not consider two factors. First, the self-attention mechanism in transformer cost quadratic memory and com-putational burden w.r.t the number of input tokens [37] dur-ing the inference stage. Second, local recurrence in human visual system [16, 17] is not realized in the transformers like ViT and DeiT. Inspiration from the local recurrence in human visual system leads to the success of convolu-tional layer, locally connected layers for computer vision tasks [18]. Although theoretically possible, it is hard to model sparse local correlations by the vaninlla self-attention mechanism (e.g., a fixed-size neighbor tokens) in practice.
Considering the two factors mentioned above, we ex-pand the search space of the vanilla transformer by intro-ducing a locality module to the MHA. The locality module only operates on the nearby tokens, requiring fewer param-eters and computation. The locality module and the self-attention module are alternative, which is searched by NAS to decide which one is used. We rename the expanded MHA block as the global-local block as it can capture both global and local correlations among the input tokens. According to our experiments (Table 1), the flexibility of the transformer in capturing global and local information is an important factor for the final performance.
Introducing global-local block should be effective, but poses challenge to the searching algorithm. The NAS al-gorithm for our search space should 1) discover the opti-mal distribution of locality modules and self-attention mod-ules in each global-local block, and 2) find the detailed settings of both locality modules and self-attention mod-ules by searching the module parameters. Such a search space is extremely huge (1018 times of the possible choices in [13] and 1012 times of the possible choices in [22]), which makes it challenging for existing NAS methods like
SPOS [13] in geting an ideal result. To deal with the prob-lem mentioned above, we propose a Hierarchical Neural
Architecture Search method to find the optimal networks.
Specifically, we first train a supernet that contains both lo-cality modules and self-attention modules, and determine the high-level global and local sub-modules distribution with evolutionary algorithm. Then, the detailed architecture within each module are searched in a similar manner. Com-pared with traditional searching strategies, the proposed hi-erarchical searching can stabilize the searching process and improve the searching performance.
Fig. 1 shows that our searched Global Local image
Transfomer (GLiT) achieves up to 4% absolute accuracy in-crease when compared with the state-of-the-art tranformer backbone DeiT on ImageNet.
To summarize, the main contributions are as follows:
• So far as we know, concurrent with [8], we are the first to explore better transformer architecture by NAS for image classification. Our work finds a new trans-former variant that achieves better performance than
ResNet101 and ResNeXt101 using the same training setting without pre-training on extra data.
• We introduce locality modules to the search space of vision transformer model, which not only decreases the computation cost but also enables explicitly local correlation modeling.
• We propose a Hierarchical Neural Architecture Search strategy, which can handle the huge searching space in the vision transformer efficiently and improves the searching results. 2.