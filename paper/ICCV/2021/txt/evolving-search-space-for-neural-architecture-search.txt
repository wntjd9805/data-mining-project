Abstract
Automation of neural architecture design has been a cov-eted alternative to human experts. Various search meth-ods have been proposed aiming to ﬁnd the optimal architec-ture in the search space. One would expect the search re-sults to improve when the search space grows larger since it would potentially contain more performant candidates.
Surprisingly, we observe that enlarging search space is unbeneﬁcial or even detrimental to existing NAS methods such as DARTS, ProxylessNAS, and SPOS. This counterin-tuitive phenomenon suggests that enabling existing methods to large search space regimes is non-trivial. However, this problem is less discussed in the literature.
We present a Neural Search-space Evolution (NSE) scheme, the ﬁrst neural architecture search scheme de-signed especially for large space neural architecture search problems. The necessity of a well-designed search space with constrained size is a tacit consent in existing methods, and our NSE aims at minimizing such necessity. Specif-ically, the NSE starts with a search space subset, then evolves the search space by repeating two steps: 1) search an optimized space from the search space subset, 2) re-ﬁll this subset from a large pool of operations that are not traversed. We further extend the ﬂexibility of obtain-able architectures by introducing a learnable multi-branch setting. With the proposed method, we achieve 77.3% top-1 retrain accuracy on ImageNet with 333M FLOPs, which yielded a state-of-the-art performance among pre-involve vious auto-generated architectures that do not knowledge distillation or weight pruning. When the la-tency constraint is adopted, our result also performs bet-ter than the previous best-performing mobile models with a 77.9% Top-1 retrain accuracy. Code is available at https://github.com/orashi/NSE NAS. 1.

Introduction
Deep neural networks are prevailing in myriad ﬁelds of real-world applications. The emergence of Neural Archi-(a) Traditional NAS (b) Search Space Evolving NAS
Figure 1. Comparison of search schemes. (a) Traditional pipeline. (b) Our proposed search space evolving pipeline. tecture Search (NAS) has brought up a possibility to au-tomate the customization of deep neural network architec-tures for speciﬁc applications. Researchers have investi-gated Reinforcement Learning (RL) and Evolutionary Al-gorithm (EA) based methods [58, 29, 30, 55] to achieve the automation of architecture design. Weight sharing based methods [21, 7, 2, 9, 22, 56, 19, 8, 33] that can substantially reduce the computational cost have been proposed and be-came one of the off-the-shelf approaches of NAS research.
These methods successfully yielded promising results that have surpassed human-designed architectures [41].
In addition to the search method, another key compo-nent of NAS is the search space. When compared with early NAS works [59, 29], the quality of the search space has been improved along with the development of search algorithms [27].
It has been observed that the improve-ment of search space design imposed a positive effect on the performance of many existing works [27, 45]. In partic-ular, the research community has devoted multiple efforts to search space design, from selecting well-suited operations based on the prior knowledge [39, 14] to utilizing channel-level ﬁne-grained model search [40, 35, 14, 49, 6, 1] over
a smaller set of operations. Recent methods often limit the number of candidate operations in each layer to less than ten (excluding decisions on activation functions or SE mod-ules [16]). However, such search space improvements fall back into the paradigm of expertise design, which is a coun-termarch of automated learning of architectures.
A natural question now could be considered is: can we construct a huge search space which is a super-set of a for-merly mentioned space and obtain superior results from it?
If the answer is YES, this approach would solve the problem of search space design by simply using the largest search space one can construct. However, it is less discussed by
NAS literature. Yu et al. [51] and Zhang et al. [53] provide results suggesting that, under the traditional NAS pipeline as shown in Figure 1(a), simply enlarging the search space could be detrimental to the ﬁnal result.
To look into this issue in detail, we set up a search space consists of 27 distinct operations and then tested 4 reason-ably fast NAS algorithms, including DARTS [21], Proxy-less [7], SPOS [14] and One-Shot [2]. We show that all of these methods do not hold the behavior of obtaining bet-ter search results with a larger search space. Furthermore, some have their search cost prohibitively high or failed to converge, while others perform poorly even with their training epochs increased (effectiveness of longer training schedule is suggested by [54, 3]). Another relevant tech-nique is search space simpliﬁcation, which can also assist neural architecture search [54, 17, 13], we will show that such technique is not enough to help NAS algorithm exploit large space effectively.
In this work, we aim at large space neural architec-ture search by proposing a Neural Search-space Evolution (NSE) scheme. Instead of directly confronting the negative impacts derived from a large search space, the NSE starts with a search space subset, which is a reasonably sized ran-dom subset of the full search space, and search an optimized space from this subset, then reﬁll this subset and repeat the search-then-reﬁll steps as shown in Figure 1(b) to traverse the whole space progressively. The resulted NSE enables ever-evolving search space for Neural Architecture Search.
NSE progressively explores extra operation candidates while retaining past knowledge. The search process is con-structed as an iterative process to traverse the pending un-seen operation candidates. During the iterative process, in-stead of keeping a single architecture as the intermediate re-sult, we combine all architectures on the Pareto front found by a supernet trained with One-Shot [2] method to obtain an optimized search space, which will be inherited to the next round of search. By maintaining an optimized search space as knowledge, the search process can always proceed with new candidate operations added to the pending list, which means we can always add newly proposed operations in CNN literature that are less veriﬁed yet potentially efﬁ-cient for speciﬁc tasks.
To effectively exploit more complex architectures, we further adopt the proposed paradigm to the multi-branch scheme, which has orders of magnitude more distinct structures when compared to its single-branch counter-part. Compared with previous single-branch schemes like
DARTS that only allow one operation to be selected, the multi-branch scheme allows multiple operations to be se-lected adaptively. By constructing a probabilistic model for the multi-branch scheme, we can retrieve the ﬁtness of ev-ery candidate operation. Operations with ﬁtness lower than a certain threshold will be dropped from the search space so that the complexity of the search space is progressively reduced and the degree of co-adaptation for the rest of the possible path combinations could also be enhanced.
We conduct experiments on ImageNet [31] with two re-source constraints, i.e. FLOPs and Latency. For these two constraints, NAS under our NSE scheme effectively ex-ploited the potential of a very large search space and secured a continual performance increment in the iterative process, leading to state-of-the-art results.
In summary, the key contributions of this paper are sum-marized as follows:
• We propose NSE, the ﬁrst neural architecture search scheme that designed especially for large space neural architecture search problems, which empowers NAS to minimize the necessity of dedicated search space de-sign. The inheritance property of the evolving process keeps knowledge derived from previous search space while improving that knowledge with new operations added into the current search space.
• We propose a probabilistic modeling of operation-wise
ﬁtness for the multi-branch scheme, which makes it feasible to gradually simplify the multi-branch scheme search space as shared weights converge through one-shot training. Such an annealing paradigm gradually simpliﬁes the complexity of the sub-task and helps the remaining shared weights to be learned better [54]. 2.