Abstract 1.

Introduction
We propose UniT, a Uniﬁed Transformer model to simul-taneously learn the most prominent tasks across different domains, ranging from object detection to natural language understanding and multimodal reasoning. Based on the transformer encoder-decoder architecture, our UniT model encodes each input modality with an encoder and makes predictions on each task with a shared decoder over the en-coded input representations, followed by task-speciﬁc out-put heads. The entire model is jointly trained end-to-end with losses from each task. Compared to previous efforts on multi-task learning with transformers, we share the same model parameters across all tasks instead of separately ﬁne-tuning task-speciﬁc models and handle a much higher vari-ety of tasks across different domains. In our experiments, we learn 7 tasks jointly over 8 datasets, achieving strong performance on each task with signiﬁcantly fewer param-eters. Our code is available in MMF at https://mmf.sh.
First proposed in [59], transformers have shown great success in a wide range of domains including but not lim-ited to natural language, images, video, and audio. Previous works (e.g. [14, 43, 44, 4, 65, 35, 29, 45, 49]) demonstrate that transformers trained on large corpora learn strong rep-resentations for a wide range of downstream language tasks.
In the visual domain, models based on transformers have achieved promising results on image classiﬁcation, object detection, and panoptic segmentation (e.g. [40, 3, 22, 21, 47, 15, 61, 5, 72, 2, 58]). Besides modeling a single modality, transformer models also exhibit strong performance in joint vision-and-language reasoning tasks such as visual question answering (e.g. [31, 38, 39, 57, 9, 30, 55, 71, 23]).
However, despite the above achievements in the applica-tion of transformers to speciﬁc domains, there has not been much prior effort to connect different tasks across domains with transformers. After witnessing the success of trans-formers, various questions naturally arise: could a trans-former model trained for natural language inference on tex-tual input also perform object detection on images, or could 1
an image classiﬁer based on transformers also check textual entailment? Overall, is it possible to build a single model that simultaneously handles tasks in a variety of domains as a step towards general intelligence? Prior work tries to tackle some of these questions but only in limited scope:
• applied only to tasks from a single domain or speciﬁc multimodal domains; ViT [15] and DETR [5] focus on vision-only tasks, BERT [14] and its derivative works
[35, 65, 29, 45] only handle language tasks, while Visu-alBERT, VILBERT [38, 31] and other multimodal trans-formers work only on speciﬁc multimodal domain of vi-sion and language. involve task-speciﬁc ﬁne-tuning for each of the tasks, not leveraging any shared parameters across the tasks, usually ending up with N times the parameters for N tasks, e.g. one has to separately ﬁne-tune a model for each of the tasks with BERT. perform multi-tasking upon related or similar tasks only from a single domain, sometimes with hard-coded train-ing strategies; for example, T5 [45] works only on tasks in the language domain, while VILBERT-MT [39] works only on related vision-and-language tasks.
•
•
In this work, we build a Uniﬁed Transformer (UniT) model that takes images and/or text as inputs and jointly train on multiple tasks ranging from visual perception and natural language understanding to joint vision-and-language rea-soning. UniT consists of transformer encoders which en-code each input modality as a sequence of hidden states (feature vectors), and a transformer decoder over the en-coded input modalities, followed by task-speciﬁc output heads applied on the decoder hidden states to make the ﬁ-nal predictions for each of the tasks. Compared to previous work on multi-task learning with transformers (e.g. [39]), we train UniT and achieve comparable performance to well-established prior work on a much larger variety of tasks; not only joint vision-and-language tasks such as visual question answering, but also vision-only as well as language-only tasks. We make the following contributions in this work:
• We propose UniT, a uniﬁed transformer encoder-decoder architecture that handles multiple tasks and do-mains in a single model with fewer parameters, and a step towards general intelligence.
• We jointly learn the most prominent tasks in the visual and textual domains and their intersections, namely ob-ject detection, visual question answering (VQA), visual entailment, and natural language understanding tasks in the GLUE benckmark [60], including QNLI [46],
MNLI [62], QQP [24], and SST-2 [51]. We show that these diverse tasks can be learned simultaneously and converge properly under our training scheme.
• Through analyses across a variety of tasks, we show that multimodal tasks such as VQA and visual entailment beneﬁt from multi-task training with uni-modal tasks. 2.