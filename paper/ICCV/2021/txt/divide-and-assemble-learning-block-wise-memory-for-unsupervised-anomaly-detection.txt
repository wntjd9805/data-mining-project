Abstract
Reconstruction-based methods play an important role in unsupervised anomaly detection in images. Ideally, we ex-pect a perfect reconstruction for normal samples and poor reconstruction for abnormal samples. Since the general-izability of deep neural networks is difficult to control, ex-isting models such as autoencoder do not work well.
In this work, we interpret the reconstruction of an image as a divide-and-assemble procedure. Surprisingly, by vary-ing the granularity of division on feature maps, we are able to modulate the reconstruction capability of the model for both normal and abnormal samples. That is, finer granu-larity leads to better reconstruction, while coarser granu-larity leads to poorer reconstruction. With proper granu-larity, the gap between the reconstruction error of normal and abnormal samples can be maximized. The divide-and-assemble framework is implemented by embedding a novel multi-scale block-wise memory module into an autoencoder network. Besides, we introduce adversarial learning and explore the semantic latent representation of the discrim-inator, which improves the detection of subtle anomaly.
We achieve state-of-the-art performance on the challenging
MVTec AD dataset. Remarkably, we improve the vanilla autoencoder model by 10.1% in terms of the AUROC score. 1.

Introduction
Anomaly detection in images is an increasingly impor-tant area among the computer vision community. Lacking of abnormal training data makes this task challenging and limits the use of well-established supervised learning meth-ods. Hence, existing works define anomaly detection as an
†Work done as an intern at Hikvision Research Institute.
⋆Corresponding author. This project was supported by the National
Key R&D Program of China (Grant No. 2020AAA010400X) and the Na-tional Natural Science Foundation of China (Grant No. 61803332).
Figure 1. A toy example demonstrating that the reconstruction ca-pability on normality and anomaly varies with different building block sizes. A: a normal sample with diagonal stripes. B: another normal sample with diagonal stripes similar to A. C: an abnormal sample with vertical stripes. unsupervised learning problem that tries to model the dis-tribution of normality without abnormal samples at training time. During inference, samples described as outliers of the normality distribution are then considered to be anomalous.
Reconstruction-based methods have been widely used for unsupervised anomaly detection. Most recent meth-ods are based on convolutional neural networks (CNNs).
In particular, autoencoder (AE) [21] is a natural choice to model the high-dimensional image data in the unsupervised setting.
It is generally assumed [53, 17, 51] that the re-construction error of the normal samples which are similar to the training data will be lower than abnormal samples.
However, due to the existence of down-sampling in AE, the latent coding representation loses the detailed information of the original image.
It may result in blurry output and large reconstruction error even for normal samples. We may strengthen the reconstruction capability of AE by reduc-ing the down-sampling rate or adding skip connections [2].
learned blocks. When the block is small, we learn two sim-ple patterns (i.e. white and dark blue), and succeed to re-construct both normality B and anomaly C. When the block size is too large, we fail to reconstruct both B and C. With a proper medium block size, we are able to reconstruct per-fectly for normality B and poorly for anomaly C, which is ideal for a reconstruction-based anomaly detection system.
Following the above motivation, we propose the Divide-and-Assemble Anomaly Detection (DAAD) framework.
Specifically, we interpret the reconstruction of an image as a divide-and-assemble procedure. The encoded feature maps are divided evenly into a grid of blocks. Accordingly, a block-wise memory module which matches with the size of the division is introduced to model the block-level rather than pixel-level distribution of normal samples. This design makes it possible to modulate the reconstruction capabil-ity of the model. And thus the tradeoff between good re-construction on normal samples and poor reconstruction on abnormal samples can be achieved. Furthermore, skip con-nections can be added safely to improve the reconstruction quality of normal samples without reconstruction of abnor-mal samples. Given an input image, DAAD uses the multi-scale features of multiple encoding layers as queries to re-trieve the most relevant items in the corresponding mem-ory banks. Then those items are aggregated and sent to the decoder through the corresponding skip connections.
The memory items are learned end-to-end together with the model parameters via backpropagation. As shown in Fig-ure 2, DAAD exhibits the desirable property of reconstruct-ing details of normal regions while suppressing the recon-struction of abnormal regions.
In the real world, most anomalies such as scratch and crack tend to be subtle and occupy an extremely small region. Therefore even in abnormal images, the pixels are overwhelmed by normality rather than anomaly. For reconstruction-based methods, the cumulative error from normal pixels contributes more than abnormal pixels, which weakens the discriminativeness of the aggregated anomaly score. Even though the per-pixel reconstruction error of normal pixels is smaller than abnormal pixels, it is still dif-ficult to detect anomaly. Thus we introduce an adversarially learned discriminator and exploit its low-dimensional se-mantic representation. The similarity between the features of the input image and the reconstructed image is treated as a measure of normality. The semantic feature-level anomaly score is complementary with the low-level reconstruction-based score, and their combination further improves the de-tection performance.
The major contributions of this paper are summarized as follows:
• We propose the multi-scale block-wise memory mod-ule which is able to achieve a tradeoff between good reconstruction on normality and poor reconstruction
Figure 2. Exemplary reconstruction results of various models. AE: the vanilla convolutional autoencoder network whose encoder has a down-sampling rate of 16. AE+Skip: AE with skip connections.
MemAE: AE augmented with the memory module proposed by [14] and {2×, 4×, 8×, 16×} indicate the down-sampling rates in our reimplementation. MemAE+Skip: AE+Skip equipped with the memory module [14] in each skip connection.
But due to the uncontrollable generalizability of deep neural networks, it is difficult to improve the reconstruction quality of normal samples on one hand, and suppress the quality of abnormal samples on the other hand (see Figure 2). Thus, the vanilla AE does not work well.
To enlarge the gap of reconstruction error between the normal and abnormal samples, a few works [14, 28] have explored the memory module.
In particular, an external memory module is utilized to explicitly model the distribu-tion of normality. However, according to our investigation, the gap of reconstruction error can not be effectively en-larged in the existing works, leading to limited performance improvement. As shown in Figure 2, on the challenging
MVTec AD [3] dataset, when the down-sampling rate is large (e.g. 16×), MemAE [14] failed to reconstruct both abnormal and some normal regions in the image. When the down-sampling rate is small (e.g. 2×) or with skip con-nections added, it reconstructed both normal and abnormal regions perfectly.
We argue that the major weakness of existing memory-augmented models lies in the fact that they decode the en-coded feature maps in a per-pixel manner. If the basic build-ing block of feature maps is as small as one pixel, then the variation of patterns of each block is limited. The smaller the block is, the more likely that anomaly shares the same block patterns with normality and thus can be reconstructed accurately. This hypothesis can be conceptually explained with a toy example shown in Figure 1. In this example, we try to learn block patterns of different sizes from normal-ity A, and reconstruct normality B and anomaly C using the
Figure 3. The architecture of the proposed DAAD framework. It consists of three major components: an encoder, a decoder and a group of block-wise memory modules. The encoder and decoder are connected with skip connections at multiple scales. And each skip connection is equipped with the block-wise memory module shown on the right. on anomaly and thus make them well separable.
• We introduce an adversarially learned feature repre-sentation that detects anomaly in the semantic space and complements reconstruction-based detection.
• Extensive experiments on common unsupervised anomaly detection benchmarks demonstrate the excel-lent reconstruction and state-of-the-art performance. 2.