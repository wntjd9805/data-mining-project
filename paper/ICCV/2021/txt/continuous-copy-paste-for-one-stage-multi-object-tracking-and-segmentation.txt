Abstract
Current one-step multi-object tracking and segmentation (MOTS) methods lag behind recent two-step methods. By separating the instance segmentation stage from the track-ing stage, two-step methods can exploit non-video datasets as extra data for training instance segmentation. Moreover, instances belonging to different IDs on different frames, rather than limited numbers of instances in raw consecu-tive frames, can be gathered to allow more effective hard example mining in the training of trackers. In this paper, we bridge this gap by presenting a novel data augmentation strategy named continuous copy-paste (CCP). Our intuition behind CCP is to fully exploit the pixel-wise annotations provided by MOTS to actively increase the number of in-stances as well as unique instance IDs in training. Without any modifications to frameworks, current MOTS methods achieve significant performance gains when trained with
CCP. Based on CCP, we propose the first effective one-stage online MOTS method named CCPNet, which generates in-stance masks as well as the tracking results in one shot.
Our CCPNet surpasses all state-of-the-art methods by large margins (3.8% higher sMOTSA and 4.1% higher MOTSA for pedestrians on the KITTI MOTS Validation) and ranks 1st on the KITTI MOTS leaderboard. Evaluations across three datasets also demonstrate the effectiveness of both
CCP and CCPNet. Our codes are publicly available at: https://github.com/detectRecog/CCP. 1.

Introduction
Multi-object tracking (MOT) [30] is of fundamental im-portance in the field of autonomous driving and video surveillance. Recently, multi-object tracking and segmen-tation (MOTS) [23] is introduced as a popular extension
∗ The first two authors contribute equally to this work. of bounding box (bbox) based MOT. MOTS provides per-pixel segmentation masks that locate objects more accu-rately than relative coarse bboxes. As instance masks pre-cisely delineate the visible object boundaries in crowded scenes, MOTS largely eliminates the ambiguities caused by severely overlapped bboxes in both detection and tracking.
Current one-step MOTS approaches struggle to adapt an additional re-ID branch to existing instance segmenta-tion methods to obtain both instance masks and their re-ID features in a single forward pass. Voigtlaender et al. [23] build TRCNN upon Mask-RCNN and adopt a fully con-nected layer to predict association vectors for object propos-als. Also built on Mask-RCNN, MOTSNet [16] proposes a novel mask-pooling layer to focus on the foreground seg-ment area rather than bboxes in the association vector ex-traction. Recently, two-step methods PointTrack [26] and
PointTrack++ [27] separate the tracking phase from the in-stance segmentation phase and convert the compact image representation to un-ordered 2D point cloud representation for learning discriminative instance embeddings. This sepa-ration brings two advantages: (i) Extra frame-level instance segmentation datasets can also be used for training [26]; (ii)
The training data for tracking is not restricted to consecutive frames, thus allowing more unique instance IDs in training mini-batches [27]. When tracking on the instance segmen-tation results produced by TRCNN, PointTrack reduces ID switches (IDS) by 55% [26]. To date, the main successes have been primarily in multi-step approaches [12, 26, 27].
The significant gap between one-step methods and multi-step methods suggests that combing tracking with instance segmentation is a non-trivial problem.
In this paper, we posit that two major causes for the performance gap between one-step methods and multi-step methods [26, 27] are: (i) Challenging training data for in-stance segmentation are limited; (ii) High-quality training samples for tracking are limited. As pixel-level annotations required by MOTS are known to be expensive to obtain,
Figure 1. Comparison between our CCPNet and state-of-the-art methods on KITTI MOTS leaderboard for cars (Left) and pedes-trians (Right). ST denotes self-training (see subsection 4.3). current MOTS datasets [23, 26] usually have limited frames and instances, especially for non-rigid objects like pedestri-ans. Crowded scenes where detection failures often occur are even rarer (66% of pedestrians in KITTI MOTS are not adjacent to anyone). Moreover, for KITTI MOTS pedes-trians, the probability of two adjacent frames containing valid triplets for training trackers in the training set is only 26.8%. Nevertheless, to make the instance segmentation network and the tracker more robust, more instances with different instance IDs that allow for mining harder samples are desired in training. Therefore, current one-step meth-ods [23, 9, 16] put great efforts to increase the number of instances in training by incorporating more frames (e.g. 8 frames [23]) into a mini-batch and reducing the input image size to save GPU memory accordingly. However, we argue that simply stacking more frames does not solve the prob-lem of lacking high-quality training data for tracking. As-suming a training mini-batch for KITTI MOTS pedestrians contains n adjacent frames, when n = 2, the average num-ber of instance IDs is 1.56 and the average number of in-stances is 3.1. When n = 8, though the average number of instances increases to 12.5, the average number of instance
IDs (1.70) has not changed dramatically. The scarcity of in-stance IDs makes it difficult to mine hard triplets, resulting in limited tracking performance.
To bridge the gap between one-step methods and multi-step methods, in this paper, we propose a novel video copy-paste data augmentation strategy named Continuous Copy-Paste (CCP). The intuition behind CCP is to fully exploit the pixel-wise annotations provided by MOTS to actively in-crease the number of instances and instance IDs in training.
To construct a training mini-batch consisting of n frames,
CCP first obtains n consecutive frames as templates from real videos or videos forged from a single image. Then, we retrieve several instance blocks from the database built dur-ing initialization. Each instance block has n crops belong-ing to the same inst ID. These crops are extracted from n frames that are close in time but not necessarily adjacent to each other. To mimic the newly emerging instances and the leaving ones, two of n instance blocks will be shifted to the left and right boundary respectively. Other instance blocks will be pasted on their original positions. It is worth noting that we preserve the relative positions of n crops in each in-stance block for all operations on instance blocks. Lastly, we paste these instance blocks on prepared n templates in descending order of the number of foreground pixels. CCP differs from Copy-Paste proposed in recent works [27, 6] in two aspects. First, we regard the instance block as the basic pasting unit and maintain the relative offset of crops within the instance block. CCP not only increases the instance den-sity but also concentrates on creating high-quality triplets for tracking. Second, we do not model surrounding visual context or randomly choose positions for pasting. Except for instance blocks shifted to image boundaries, instances in instance blocks that are cropped from different images stay in their original positions. As shown in Fig. 1, the effec-tiveness of CCP is examined by combining it with TRCNN
[23] and PointTrack [26].
Based on CCP, we further propose the first effective one-stage method, named CCPNet, for online MOTS. CCPNet follows an encoder-decoder structure and predicts pixel-wise classification confidence, 2D offset pointing to in-stance centers, clustering parameters, foreground recon-struction, and pixel embeddings. In the post-processing pro-cess, pixels are grouped into instances according to their distance from 2D centers. For each grouped instance, we apply max-pooling on embeddings of all foreground pix-els to obtain the instance embedding. After that, instances are associated according to the distance between their em-beddings and the mask IOU between their masks. Thanks to CCP, though CCPNet operates in an online manner, it beats state-of-the-art methods including both 3D tracking methods and offline tracking methods on multiple datasets.
Furthermore, we present a self-training method for CCPNet that brings additional gains (see Fig. 1).
Our main contributions are summarized as follows:
• We propose a novel data augmentation strategy named
CCP for training MOTS approaches. CCP brings sig-nificant performance gains for current MOTS methods without modifying their frameworks.
• The first effective one-stage MOTS approach termed
CCPNet is presented, which performs instance seg-mentation and tracking in one shot.
• Evaluations across three datasets show that CCPNet outperforms all existing MOTS methods by large mar-gins. Moreover, CCPNet ranks 1st on the KITTI
MOTS leaderboard. 2.