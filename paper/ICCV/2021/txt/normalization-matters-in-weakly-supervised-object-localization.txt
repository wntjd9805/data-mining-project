Abstract
Weakly-supervised object localization (WSOL) enables finding an object using a dataset without any localization information. By simply training a classification model using only image-level annotations, the feature map of the model can be utilized as a score map for localization.
In spite of many WSOL methods proposing novel strategies, there has not been any de facto standard about how to normal-ize the class activation map (CAM). Consequently, many
WSOL methods have failed to fully exploit their own capac-ity because of the misuse of a normalization method. In this paper, we review many existing normalization methods and point out that they should be used according to the property of the given dataset. Additionally, we propose a new nor-malization method which substantially enhances the perfor-mance of any CAM-based WSOL methods. Using the pro-posed normalization method, we provide a comprehensive evaluation over three datasets (CUB, ImageNet and Open-Images) on three different architectures and observe signifi-cant performance gains over the conventional min-max nor-malization method in all the evaluated cases (See Fig. 1).
Figure 1: Comparison of several WSOL methods with differ-ent kinds of normalization methods for a class activation map.
The accuracy has been evaluated under the evaluation metric sug-gested in [6] with CUB-200-2011 dataset. All scores in this figure are the average scores of ResNet50, VGG16, and InceptionV3.
In all WSOL methods, the performance using our normalization method, IVR, is the best. 1.

Introduction
Given nothing but the class information of an object, weakly-supervised object localization (WSOL) allows a convolutional neural network (CNN) to localize the object in a scene. Although many fully-supervised object detectors guarantee considerable performance in locating objects in an image, localization techniques in the absence of bound-ing box annotations are still in need.
WSOL using neural networks has been initially intro-duced by the class activation map (CAM) [30] approach.
Training a convolutional neural network (CNN) model with a classification problem enables the model to generate an activation map from the last layer of it. After that, simply cutting out the activation map with a proper threshold en-*Work done as a research scientist at NAVER AI Lab. ables the localization of an object. In spite of the plausible axiom that features contributing better to a specific class are likely to represent the location of the object, the problem still exists that the discriminative parts of an object hoax the activation of the model to make an inaccurate localization of the target object. Many methods have been proposed to overcome this problem [21, 27, 28, 7, 25] and persuasive evidences of performance improvement have been demon-strated qualitatively and quantitatively.
However, prior works have been evaluated under differ-ent conditions and their hyperparameters have been cho-sen empirically. Usually, the feature from the last layer of the model is post-processed to be used as a class activation map. The normalization scheme used by every method dif-fers each other and makes the comparison unfair. The work of [6] has proposed a new evaluation protocol and offered
a thorough comparison of six previous WSOL methods. In their experiments, the min-max normalization has been ap-plied to every method for the fair comparison and the best hyperparameters have been found using a random search
[2]. According to it, all methods have shown almost no im-provement compared to the original CAM [30].
In the work of [1], the authors suggest a thresholding strategy that excludes exceptionally high activation values in each image. In the sense that the valid value range in the activation map changes, this can be compared with other normalization methods. With this new method, some meth-ods such as CAM [30] and HaS [21] have shown perfor-mance improvements in several datasets.
In this paper, we investigate the problem that can oc-cur when using the min-max normalization. Even though the min-max normalization is the most popular scheme in recent WSOL works, we verify that the min-max scheme can deteriorate the performance of most WSOL methods.
We revisit max normalization which has been prevalent for a long time and point out that it can resolve the problem above. Also, although other methods including percentile as a standard for thresholding (PaS) [1] have shown bet-ter results than the min-max normalization, they still suf-fer from problems which will be described in this paper.
To resolve these problems, we propose a new normaliza-tion method inferior value removal (IVR). Through exten-sive experimental results, IVR has been shown to improve the localization performances of almost all WSOL meth-ods. Enabling better exploitation of each WSOL method, a comprehensive re-evaluation and ordering of several WSOL methods have been conducted. The contribution of this pa-per is as follows:
• We provide a thorough investigation about problems of commonly used normalization methods in WSOL.
The problem originating from using min-max normal-ization is explained qualitatively and quantitatively.
• We propose a new normalization method which can better exploit the performance of many WSOL meth-ods.
It can be used in any kind of WSOL methods which use a class activation map.
• A comprehensive evaluation with various kinds of normalization methods in three different datasets and three different architectures has been made. We pro-vide a renewed benchmark of six WSOL methods. 2.