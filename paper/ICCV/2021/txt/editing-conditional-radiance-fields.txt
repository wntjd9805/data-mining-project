Abstract
A neural radiance ﬁeld (NeRF) is a scene model support-ing high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level
NeRF – also known as a conditional radiance ﬁeld – trained on a shape category. Speciﬁcally, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance ﬁeld that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets speciﬁc network components, which balances efﬁciency and accuracy. During user interaction, we formu-late an optimization problem that both satisﬁes the user’s constraints and preserves the original object structure. We demonstrate our editing approach on rendered views of three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a single-view real photograph and show that the edit propagates to extrapolated novel views. 1.

Introduction 3D content creation often involves manipulating high-quality 3D assets for visual effects or augmented reality applications, and part of a 3D artist’s workﬂow consists of making local adjustments to a 3D scene’s appearance and shape [26, 28]. Explicit representations give artists control of the different elements of a 3D scene. For example, the artist may use mesh processing tools to make local adjustments to the scene geometry or change the surface appearance by manipulating a texture atlas [62]. In an artist’s workﬂow, such explicit representations are often created by hand or procedurally generated.
While explicit representations are powerful, there re-main signiﬁcant technical challenges in automatically ac-quiring a high-quality explicit representation of a real-world
Figure 1: Editing a conditional radiance ﬁeld. Given a condi-tional radiance ﬁeld trained over a class of objects, we demonstrate three editing applications: (A) color editing, (B) shape editing, and (C) color/shape transfer. A user provides coarse scribbles over a local region of interest or selects a target object instance. Local edits propagate to the desired region in 3D and are consistent across different rendered views. scene due to view-dependent appearance, complex scene topology, and varying surface opacity. Recently, implicit continuous volumetric representations have shown high-ﬁdelity capture and rendering of a variety of 3D scenes and overcome many of the aforementioned technical chal-lenges [50, 66, 45, 59, 63]. Such representations encode the captured scene in the weights of a neural network. The neural network learns to render view-dependent colors from point samples along cast rays, with the ﬁnal rendering obtained via alpha compositing [58]. This representation enables many photorealistic view synthesis applications [41, 47]. However, we lack critical knowledge in how to enable artists’ control and editing in this representation.
Editing an implicit continuous volumetric representation is challenging. First, how can we effectively propagate sparse 2D user edits to ﬁll the entire corresponding 3D region in this representation? Second, the neural network for an im-plicit representation has millions of parameters. It is unclear
which parameters control the different aspects of the ren-dered shape and how to change the parameters according to the sparse local user input. While prior work for 3D editing primarily focuses on editing an explicit representation [62], they do not apply to neural representations.
In this paper, we study how to enable users to edit and control an implicit continuous volumetric representation of a 3D object. As shown in Figure 1, we consider three types of user edits: (i) changing the appearance of a local part to a new target color (e.g., changing the chair seat’s color from beige to red), (ii) modifying the local shape (e.g., removing a chair’s wheel or swapping in new arms from a different chair), and (iii) transferring the color or shape from a target object instance. The user performs 2D local edits by scrib-bling over the desired location of where the edit should take place and selecting a target color or local shape.
We address the challenges in editing an implicit continu-ous representation by investigating how to effectively update a conditional radiance ﬁeld to align with a target local user edit. We make the following contributions. First, we learn a conditional radiance ﬁeld over an entire object class to model a rich prior of plausible-looking objects. Unexpectedly, this prior often allows the propagation of sparse user scribble edits to ﬁll a selected region. We demonstrate complex ed-its without the need to impose explicit spatial or boundary constraints. Moreover, the edits appear consistently when the object is rendered from different viewpoints. Second, to more accurately reconstruct shape instances, we introduce a shape branch in the conditional radiance ﬁeld that is shared across object instances, which implicitly biases the network to encode a shared representation whenever possible. Third, we investigate which parts of the conditional radiance ﬁeld’s network affect different editing tasks. We show that shape and color edits can effectively take place in the later layers of the network. This ﬁnding motivates us to only update these layers and enables us to produce effective user edits with signiﬁcant computational speed-up. Finally, we introduce color and shape editing losses to satisfy the user-speciﬁed targets, while preserving the original object structure.
We demonstrate results on rendered views of three shape datasets with varying levels of appearance, shape, and train-ing view complexity. We show the effectiveness of our ap-proach for object view synthesis as well as color and shape editing, compared to prior neural editing methods. More-over, we show that we can edit the appearance and shape of a real single-view photograph and that the edit propagates to extrapolated novel views. We highly encourage viewing our video to see our editing demo in action. Code and more results are available at our GitHub repo and website. 2.