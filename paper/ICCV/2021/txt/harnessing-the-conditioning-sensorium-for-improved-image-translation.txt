Abstract 1.

Introduction
Multi-modal domain translation typically refers to syn-thesizing a novel image that inherits certain localized at-tributes from a ‘content’ image (e.g. layout, semantics, or geometry), and inherits everything else (e.g. texture, light-ing, sometimes even semantics) from a ‘style’ image. The dominant approach to this task is attempting to learn disen-tangled ‘content’ and ‘style’ representations from scratch.
However, this is not only challenging, but ill-posed, as what users wish to preserve during translation varies depending on their goals. Motivated by this inherent ambiguity, we de-ﬁne ‘content’ based on conditioning information extracted by off-the-shelf pre-trained models. We then train our style extractor and image decoder with an easy to optimize set of reconstruction objectives. The wide variety of high-quality pre-trained models available and simple training procedure makes our approach straightforward to apply across nu-merous domains and deﬁnitions of ‘content’. Additionally it offers intuitive control over which aspects of ’content’ are preserved across domains. We evaluate our method on traditional, well-aligned, datasets such as CelebA-HQ, and propose two novel datasets for evaluation on more complex scenes: ClassicTV and FFHQ-Wild. Our approach, Sen-sorium, enables higher quality domain translation for more complex scenes.
Our focus in this work is multi-modal, reference-guided, image-to-image domain translation. One well-known vari-ant of this task is seasonal change, for example translating a photograph taken in the summer to a “winter” style, adding snow to the ground and removing leaves from trees. How-ever, it’s not always clear which features should be invariant across domains. For example, when translating from pho-tographic portraits to paintings a user might wish to either keep their pose or to alter it to match the canon of Renais-sance painting. An additional ambiguity arises from the lack of a bijection between most domains. Even within a domain there is typically heterogeneity for a given def-inition of content. These challenges motivate the develop-ment of ﬂexible domain translation models which offer con-trol over what should be preserved as “content” and which mode of the target domain should be used as “style”.
Existing domain translation frameworks produce high quality results on relatively simple meta-domains like hu-man faces, but performance deteriorates both when the complexity of a single domain increases, and when the semantic and morphological gap between translation do-mains grows. Rather than pose domain translation as learn-ing from scratch to disentangle and recombine content and style, we instead pose it as reconstruction from condition-ing information, aided by a tightly bottle-necked ‘style’ representation. Empirically this leads to style represen-tations that capture only what the content does not. Con-tent is deﬁned by selecting conditioning modalities the user wishes to remain invariant during translation (of which a huge variety such as depth-estimation, pose-estimation, and semantic-segmentation, have been well-studied and can be inferred automatically using pre-trained models). This for-mulation is intuitive, and allows practitioners to select a content deﬁnition relevant to their goals.
Our framework, based solely on the well-posed task of image reconstruction, is easy to train and provides the end user with explicit and intuitive control over “content” while improving synthesis quality for complex scenes. To demonstrate the greater robustness of our pipeline on more complex domain translation tasks we introduce two novel datasets: ClassicTV and FFHQ-Wild. In addition to these complex human-centric translation datasets, we show trans-lations from landscape photographs to different seasons and painting styles. 2.