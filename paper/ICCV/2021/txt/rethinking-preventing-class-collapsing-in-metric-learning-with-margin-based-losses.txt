Abstract
Metric learning seeks perceptual embeddings where vi-sually similar instances are close and dissimilar instances are apart, but learned representations can be sub-optimal when the distribution of intra-class samples is diverse and distinct sub-clusters are present. Although theoretically with optimal assumptions, margin-based losses such as the triplet loss and margin loss have a diverse family of so-lutions. We theoretically prove and empirically show that under reasonable noise assumptions, margin-based losses tend to project all samples of a class with various modes onto a single point in the embedding space, resulting in class collapse that usually renders the space ill-sorted for classiﬁcation or retrieval. To address this problem, we pro-pose a simple modiﬁcation to the embedding losses such that each sample selects its nearest same-class counterpart in a batch as the positive element in the tuple. This allows for the presence of multiple sub-clusters within each class.
The adaptation can be integrated into a wide range of met-ric learning losses. Our method demonstrates clear bene-ﬁts on various ﬁne-grained image retrieval datasets over a variety of existing losses; qualitative retrieval results show that samples with similar visual patterns are indeed closer in the embedding space. 1.

Introduction
Metric learning aims to learn an embedding function to lower dimensional space, in which semantic similar-ity translates to neighborhood relations in the embedding space [22]. Deep metric learning approaches achieve promising results in a large variety of tasks such as face identiﬁcation [5, 44, 43], zero-shot learning [9], image re-trieval [14, 10] and ﬁne-grained recognition [47].
In this work we investigate the family of losses which optimize for an embedding representation that enforces that all modes of intra-class appearance variation project to a single point in embedding space. Learning such an embed-ding is very challenging when classes have a diverse ap-pearance. This happens especially in real-world scenarios where the class consists of multiple modes with diverse vi-sual appearance. Pushing all these modes to a single point in the embedding space requires the network to memorize the relations between the different class modes, which could reduce the generalization capabilities of the network and re-sult in sub-par performance.
Recently researchers observed that this phenomena, where all modes of class appearance “collapse” to the same center, occurs in case of the classiﬁcation SoftMax loss [30]. They proposed a multi-center approach, where multiple centers for each class are used with the SoftMax loss to capture the hidden distribution of the data to solve this issue. In the metric learning ﬁeld, a positive sampling method has been proposed [53] with respect to the N-pair loss [41] in order to relax the constraints on the intra-class relations. For margin-based losses such as the triplet loss [3] and margin loss [51], it was believed that they might offer some relief from class collapsing [47, 51]. From a theo-retic perspective, we prove that with optimal assumptions on the hypothesis space and the training procedure, it is in-deed true that the margin-based losses have a minimal so-lutions without class collapsing. However, we formulate a noisy framework and prove that with modest noise assump-tions on the labels, the margin-based losses yet suffer from class collapse and the easy positive sampling method pro-posed in [53] allow more diverse solutions. Adding noise to the labels allow modelling both the aleatoric and the ap-proximation uncertainties of the neural network, therefore it batters represent the training process on real-world datasets with ﬁxed restricted network architecture.
We complement our theoretical study with an extensive empirical study, which demonstrates the class-collapsing phenomena on real-world datasets, and show that the easy positive sampling method is able to create a more diverse embedding which results in better generalization perfor-mances. These ﬁndings suggest that the noisy environment framework better ﬁts the training dynamic of neural net-works in real-world use cases.
Figure 1: Given an anchor (circle with dark ring), our approach samples the closest positive example in the embedding space as the positive element. This results in pushing the anchor only to-wards the closest element direction (green arrow), which allows the embedding to have multiple clusters for each class. 2.