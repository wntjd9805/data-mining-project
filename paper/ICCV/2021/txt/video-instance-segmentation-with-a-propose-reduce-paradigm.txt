Abstract
Video instance segmentation (VIS) aims to segment and associate all instances of predeﬁned classes for each frame in videos. Prior methods usually obtain segmentation for a frame or clip ﬁrst, and merge the incomplete results by tracking or matching. These methods may cause error ac-cumulation in the merging step. Contrarily, we propose a new paradigm – Propose-Reduce, to generate complete sequences for input videos by a single step. We further build a sequence propagation head on the existing image-level instance segmentation network for long-term propa-gation. To ensure robustness and high recall of our pro-posed framework, multiple sequences are proposed where redundant sequences of the same instance are reduced. We achieve state-of-the-art performance on two representative benchmark datasets – we obtain 47.6% in terms of AP on
YouTube-VIS validation set and 70.4 % for J&F on DAVIS-UVOS validation set. 1.

Introduction
Video instance segmentation (VIS), proposed in [53], is a task to segment all instances of the predeﬁned classes in each frame. Segmented instances are linked in the en-tire video. It is important in the ﬁeld of video understand-ing, and can be applied to video editing, autonomous driv-ing, etc. Unlike image-level instance segmentation, VIS re-quires not only detection and segmentation of each frame, but also tracking of objects in the video, which make it a very challenging task.
Recently, several approaches were proposed for this task [53, 7, 1, 2, 29]. Based on the patterns of gen-erating instance sequences, existing frameworks can be roughly categorized into two paradigms: ‘Track-by-Detect’ (Fig. 1(a)) and ‘Clip-Match’ (Fig. 1(b)). The ‘Track-by-Detect’ paradigm detects and segments instances for each individual frame, followed by obtaining instance sequences with frame-by-frame tracking [53, 7]. Differently, ‘Clip-*Equal Contribution.
Figure 1. Four paradigms of generating instance sequences in VIS. (a) Track-by-Detect links detected instances via frame-by-frame tracking. (b) Clip-Match matches overlapped sub-sequences be-tween video-clips. (c) An alternative propagates detected in-stances from one key frame to the rest of a video. (d) Our proposed paradigm, named Propose-Reduce, generates instance sequence proposals based on multiple key frames and reduces redundant se-quences of the same instances.
Match’ adopts the divide-and-conquer strategy. It divides an entire video into multiple short overlapped clips, and obtains VIS results for each clip and generates instance sequences with clip-by-clip matching [2, 1]. Both of the paradigms need two independent steps to generate a com-plete sequence. They both generate multiple incomplete se-quences (i.e., frames or clips) from a video, and merge (or complete) them by tracking/matching at the second stage.
Intuitively, these paradigms are vulnerable to error accumu-lation in the process of merging sequences, especially when occlusion or fast motion exists.
To avoid error accumulation brought by merging incom-plete sequences, one intuitive solution is to generate a com-plete instance sequence for an entire video with only one step. As shown in Fig. 1(c), starting from any key frame of a video, we can obtain instance sequences by propagating the instance segmentation results from this frame to all oth-ers. However, the propagation quality from different start-sequence level. With the above design, our overall frame-work is neat and can be trained in an end-to-end fashion.
The overall contributions are summarized below.
• We propose a new paradigm – Propose-Reduce, for the task of video instance segmentation. This paradigm ensures high recall and does not require error-accumulating tracking/matching modules.
• Based on the paradigm, we propose a variant of Mask
R-CNN for videos, named Seq Mask R-CNN. By adding an extra sequence propagation head upon Mask
R-CNN, temporal relation is established across frames.
• Our framework achieves new state-of-the-art results on
YouTube-VIS [53] validation set with 47.6% in
,
AP as well as DAVIS-UVOS [6] validation set with J&F score 70.4 %. 2.