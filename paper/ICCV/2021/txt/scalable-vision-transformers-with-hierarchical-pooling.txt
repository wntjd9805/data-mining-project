Abstract
The recently proposed Visual image Transformers (ViT) with pure attention have achieved promising performance on image recognition tasks, such as image classiﬁcation.
However, the routine of the current ViT model is to main-tain a full-length patch sequence during inference, which is redundant and lacks hierarchical representation. To this end, we propose a Hierarchical Visual Transformer (HVT) which progressively pools visual tokens to shrink the se-quence length and hence reduces the computational cost, analogous to the feature maps downsampling in Convolu-tional Neural Networks (CNNs). It brings a great beneﬁt that we can increase the model capacity by scaling dimen-sions of depth/width/resolution/patch size without introduc-ing extra computational complexity due to the reduced se-quence length. Moreover, we empirically ﬁnd that the av-erage pooled visual tokens contain more discriminative in-formation than the single class token. To demonstrate the improved scalability of our HVT, we conduct extensive ex-periments on the image classiﬁcation task. With compara-ble FLOPs, our HVT outperforms the competitive baselines on ImageNet and CIFAR-100 datasets. Code is available at https://github.com/MonashAI/HVT. 1.

Introduction
Equipped with the self-attention mechanism that has strong capability of capturing long-range dependencies,
Transformer [37] based models have achieved signiﬁcant breakthroughs in many computer vision (CV) and natural language processing (NLP) tasks, such as machine trans-lation [10, 9], image classiﬁcation [11, 36], segmentation
[43, 39] and object detection [3, 48]. However, the good performance of Transformers comes at a high computa-tional cost. For example, a single Transformer model re-quires more than 10G Mult-Adds to translate a sentence of only 30 words. Such a huge computational complexity hin-ders the widespread adoption of Transformers, especially on resource-constrained devices, such as smart phones.
†Corresponding author. Email: bohan.zhuang@monash.edu
Figure 1: Performance comparisons on ImageNet. With comparable GFLOPs (1.25 vs. 1.39), our proposed Scale
HVT-Ti-4 surpasses DeiT-Ti by 3.03% in Top-1 accuracy.
To improve the efﬁciency, there are emerging efforts to design efﬁcient and scalable Transformers. On the one hand, some methods follow the idea of model compression to reduce the number of parameters and computational over-head. Typical methods include knowledge distillation [19], low-bit quantization [29] and pruning [12]. On the other hand, the self-attention mechanism has quadratic memory and computational complexity, which is the key efﬁciency bottleneck of Transformer models. The dominant solu-tions include kernelization [20, 28], low-rank decomposi-tion [41], memory [30], sparsity [4] mechanisms, etc.
Despite much effort has been made, there still lacks spe-ciﬁc efﬁcient designs for Visual Transformers considering taking advantage of characteristics of visual patterns.
In particular, ViT models maintain a full-length sequence in the forward pass across all layers. Such a design can suffer from two limitations. Firstly, different layers should have different redundancy and contribute differently to the accu-racy and efﬁciency of the network. This statement can be supported by existing compression methods [35, 23], where each layer has its optimal spatial resolution, width and bit-width. As a result, the full-length sequence may contain
huge redundancy. Secondly, it lacks multi-level hierarchi-cal representations, which is well known to be essential for the success of image recognition tasks.
To solve the above limitations, we propose to gradually downsample the sequence length as the model goes deeper.
Speciﬁcally, inspired by the design of VGG-style [33] and
ResNet-style [14] networks, we partition the ViT blocks into several stages and apply the pooling operation (e.g., average/max pooling) in each stage to shrink the sequence length. Such a hierarchical design is reasonable since a re-cent study [7] shows that a multi-head self-attention layer with a sufﬁcient number of heads can express any convo-lution layers. Moreover, the sequence of visual tokens in
ViT can be analogous to the ﬂattened feature maps of CNNs along the spatial dimension, where the embedding of each token can be seen as feature channels. Hence, our design shares similarities with the spatial downsampling of feature maps in CNNs. To be emphasized, the proposed hierarchi-cal pooling has several advantages. (1) It brings consider-able computational savings and improves the scalability of current ViT models. With comparable ﬂoating-point opera-tions (FLOPs), we can scale up our HVT by expanding the dimensions of width/depth/resolution. In addition, the re-duced sequential resolution also empowers the partition of the input image into smaller patch sizes for high-resolution representations, which is needed for low-level vision and dense prediction tasks. (2) It naturally leads to the generic pyramidal hierarchy, similar to the feature pyramid network (FPN) [24], which extracts the essential multi-scale hidden representations for many image recognition tasks.
In addition to hierarchical pooling, we further propose to perform predictions without the class token.
Inherited from NLP, conventional ViT models [11, 36] equip with a trainable class token, which is appended to the input patch tokens, then reﬁned by the self-attention layers, and is ﬁ-nally used for prediction. However, we argue that it is not necessary to rely on the extra class token for image clas-siﬁcation. To this end, we instead directly apply average pooling over patch tokens and use the resultant vector for prediction, which achieves improved performance. We are aware of a concurrent work [6] that also observes the similar phenomenon.
Our contributions can be summarized as follows:
• We propose a hierarchical pooling regime that grad-ually reduces the sequence length as the layer goes deeper, which signiﬁcantly improves the scalability and the pyramidal feature hierarchy of Visual Trans-formers. The saved FLOPs can be utilized to improve the model capacity and hence the performance.
• Empirically, we observe that the average pooled visual tokens contain richer discriminative patterns than the class token for classiﬁcation.
• Extensive experiments show that, with comparable
FLOPs, our HVT outperforms the competitive base-line DeiT on image classiﬁcation benchmarks, includ-ing ImageNet and CIFAR-100. 2.