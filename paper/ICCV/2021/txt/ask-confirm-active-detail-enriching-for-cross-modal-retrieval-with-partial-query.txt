Abstract
Text-based image retrieval has seen considerable progress in recent years. However, the performance of ex-isting methods suffers in real life since the user is likely to provide an incomplete description of an image, which often leads to results filled with false positives that fit the incomplete description.
In this work, we introduce the partial-query problem and extensively analyze its influence on text-based image retrieval. Previous interactive meth-ods tackle the problem by passively receiving users’ feed-back to supplement the incomplete query iteratively, which is time-consuming and requires heavy user effort. Instead, we propose a novel retrieval framework that conducts the interactive process in an Ask-and-Confirm fashion, where
AI actively searches for discriminative details missing in the current query, and users only need to confirm AI’s pro-posal. Specifically, we propose an object-based interac-tion to make the interactive retrieval more user-friendly and present a reinforcement-learning-based policy to search for discriminative objects. Furthermore, since fully-supervised training is often infeasible due to the difficulty of obtain-ing human-machine dialog data, we present a weakly-supervised training strategy that needs no human-annotated dialogs other than a text-image dataset. Experiments show that our framework significantly improves the performance of text-based image retrieval. Code is available at https:
//github.com/CuthbertCai/Ask-Confirm. 1.

Introduction
Recently, cross-modal retrieval, especially text-based image retrieval has gained increasing attention [36]. Al-though significant improvement has been achieved with ex-isting methods [15, 36, 7] for text-based retrieval, we found in practice their retrieval result is barely satisfactory when
*Work done during internship at Youtu Lab
†Corresponding author: Xinyang Jiang, Xing Sun
Figure 1: An illustration of Ask&Confirm. The agent en-riches the textual query and narrows down the retrieval scope by iteratively asking users to confirm more informa-tion. The target image is highlighted with a red rectangle. users only describe some local regions in an image.
In this work, we introduce a new concept of partial-query problem in text-based image retrieval, where the ini-tial text query only describes some objects in the target im-age. Studies [30, 28] have found that when examining an image, people tend to only focus on the objects that stand out the most. This could lead to problems where the objects that people focus on are not the discriminative objects that can distinguish the target image from similar candidates, thus making the user’s input insufficient for retrieving the target image. As shown in Figure 2 (a) and (b), a cross-modal retrieval model performs poorly when a query is only partially given. In both examples, the target image ranks lower than 1000 th, while the other false positives rank top three. A common object (blue box) described by the par-tial query is presented in all images. However, the rest of images are vastly different. For example, in Figure 2 (a), besides the stroller mentioned in the query, the target image consists of umbrellas, chairs, and so on. Whereas the others consist of different objects like trees and buses. If the re-trieval model receives a complete description including all objects, existing methods [15, 17, 32] perform excellently.
To show how the partial query hurts retrieval, we test two
(a) (b) (c) R@10 (d) Mean Rank
Figure 2: Effect of partial queries. (a) and (b) are visual-izations of partial-query retrieval. The target image is sur-rounded by a red box and the others are the top three ranked scenes. The region that matches the query is surrounded by a blue box. (c) and (d) demonstrates R@10 and Mean Rank of a retrieval model as queries decrease. The horizontal axis represents the query number. text-image retrieval models, S-SCAN and T-CMPL on Vi-sual Genome [14], which are modified from SCAN [15] and
CMPL [36]. The implementation is detailed in Section 4.
For each image, its complete description includes 10 cap-tions for different regions. We gradually decrease the num-ber of captions and use them as queries to retrieve the target image. As shown in Figure 2 (c) and (d), for both models,
R@10 decreases and Mean Rank increases as the degree of incompletion increases. These results reveal that partial queries should be tackled for a robust retrieval model.
Existing interactive retrieval models [6, 33, 26, 13, 11, 35, 7, 31] tackle the partial query by involving feedback of users in the retrieval process. Given the initial queries from users, these methods first give several relevant candi-dates that could potentially be the target image. By com-paring the target image with these reference images, users give the retrieval method different forms of feedback to de-scribe the difference between them, such as scores [26, 33], tags [13, 11, 12] or descriptions [7, 29]. The models then refine the retrieval results according to the user feedback and continue next round of iteration until the target image is found. Previous methods only passively receive additional information from users, so users need to have substantial practice and expert knowledge on the retrieval system to give discriminative feedback that can quickly narrow down the retrieval range. Hence, to free users from the burden of analyzing the retrieval results and looking for the discrim-inative information, we propose that the retrieval model it-self should be able to actively search for the discrimina-tive information the current query misses. Another problem of previous interactive retrieval models is time-consuming.
For example, description-based methods [7, 29] require users to input long sentence feedback and tag-based meth-ods [13, 11, 12] require users to input a bunch of attributes.
Hence, we propose a framework where users only need to make simple yes/no confirmation on AI’s question.
In this paper, we propose a novel interactive retrieval framework called Ask&Confirm as shown in Figure 1. The agent first retrieves a set of relevant candidates from the gallery based on initial text queries. Then, it will analyze the retrieval results and the overall status of gallery, and actively select discriminative object candidates for users to confirm their presence. Based on users’ confirmation, the agent nar-rows down the range of candidates and eventually gathers enough information to locate the target image. Instead of passively receiving user feedback, a reinforcement learning (RL) based policy is trained to actively search for the dis-criminative objects missed in the query, and use these ob-jects to distinguish the target image from the rest of gallery.
In this active object-based interaction, users only need to confirm the existence of the proposed objects in the target image, no expert knowledge on the retrieval task and extra effort is needed. Moreover, unlike previous RL based inter-active methods [7, 19] that require human-annotated dialogs which is impractical to widely collect, our Ask&Confirm framework is trained in a weakly-supervised manner, where only text-image pairs are needed.
The contributions of our framework are as follows: 1)
To our knowledge, this is the first work that formally ad-dresses and analyzes the problem of partial query in cross-modal retrieval. 2) Instead of passively receiving missing details from user feedback, we propose a novel interactive retrieval framework Ask&Confirm that introduces an active object-based interaction to actively select the most discrim-inative objects for users to confirm. 3) Rather than using human-annotated dialogs, we propose a weakly-supervised reinforcement learning framework to optimize the interac-tive policy that explores the statistical characteristics of the gallery. Experiments show that our framework is effective and robust with partial queries. 2.