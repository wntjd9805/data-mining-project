Abstract
We propose Anticipative Video Transformer (AVT), an end-to-end attention-based video modeling architecture that attends to the previously observed video in order to anticipate future actions. We train the model jointly to pre-dict the next action in a video sequence, while also learn-ing frame feature encoders that are predictive of succes-sive future frames’ features. Compared to existing tempo-ral aggregation strategies, AVT has the advantage of both maintaining the sequential progression of observed actions while still capturing long-range dependencies—both crit-ical for the anticipation task. Through extensive experi-ments, we show that AVT obtains the best reported per-formance on four popular action anticipation benchmarks:
EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads; and it wins ﬁrst place in the EpicKitchens-100
CVPR’21 challenge. 1.

Introduction
Predicting future human actions is an important task for
AI systems. Consider an autonomous vehicle at a stop sign that needs to predict whether a pedestrian will cross the street or not. Making this determination requires modeling complex visual signals—the past actions of the pedestrian, such as speed and direction of walking, or usage of devices that may hinder his awareness of the surroundings—and us-ing those to predict what he may do next. Similarly, imag-ine an augmented reality (AR) device that observes a user’s activity from a wearable camera, e.g. as they cook a new dish or assemble a piece of furniture, and needs to antici-pate his next steps to provide timely assistance. In many such applications, it is insufﬁcient to recognize what is hap-pening in the video. Rather, the vision system must also anticipate the likely actions that are to follow. Hence, there is a growing interest in formalizing the activity anticipation task [24, 45, 49, 64, 73, 82] along with development of mul-tiple challenge benchmarks to support it [13, 14, 49, 55, 82].
Compared to traditional action recognition, anticipation tends to be signiﬁcantly more challenging. First of all, it re-Temporal Attention  line width) (
//
Spatial Attention
Input Frames wash tomato
Head turn-off tap
Head
…
…
Backbone time
Figure 1: Anticipating future actions using AVT involves en-coding video frames with a spatial-attention backbone, followed by a temporal-attention head that attends only to frames before the current one to predict future actions. In this example, it sponta-neously learns to attend to hands and objects without being su-pervised to do so. Moreover, it attends to frames most relevant to predict the next action. For example, to predict ‘wash tomato’ it attends equally to all previous frames as they determine if any more tomatoes need to be washed, whereas for ‘turn-off tap’ it fo-cuses most on the current frame for cues whether the person might be done. Please see 5.3 for details and additional results.
§ quires going beyond classifying current spatiotemporal vi-sual patterns into a single action category—a task nicely suited to today’s well-honed discriminative models—to in-stead predict the multi-modal distribution of future activi-ties. Moreover, while action recognition can often side-step temporal reasoning by leveraging instantaneous contextual cues [31], anticipation inherently requires modeling the pro-gression of past actions to predict the future. For instance, the presence of a plate of food with a fork may be sufﬁcient to indicate the action of eating, whereas anticipating that same action would require recognizing and reasoning over the sequence of actions that precede it, such as chopping, cooking, serving, etc. Indeed, recent work [23, 77] ﬁnds that modeling long temporal context is often critical for anticipation, unlike action recognition where frame-level modeling is often enough [43, 50, 81]. These challenges are also borne out in practice. For example, accuracy for one of today’s top performing video models [77] drops from 42% to 17% when treating recognition versus anticipation on the
same test clips [13]—predicting even one second into the future is much harder than declaring the current action.
The typical approach to solving long-term predictive rea-soning tasks involves extracting frame or clip level features using standard architectures [12, 86, 91], followed by ag-gregation using clustering [32, 62], recurrence [23, 24, 42], or attention [28, 59, 77, 95] based models. Except the recur-rent ones, most such models merely aggregate features over the temporal extent, with little regard to modeling the se-quential temporal evolution of the video over frames. While recurrent models like LSTMs have been explored for antici-pation [2, 23, 96], they are known to struggle with modeling long-range temporal dependencies due to their sequential (non-parallel) nature. Recent work mitigates this limitation using attention-based aggregation over different amounts of the context to produce short-term (‘recent’) and long-term (‘spanning’) features [77]. However, it still reduces the video to multiple aggregate representations and loses its sequential nature. Moreover, it relies on careful and dataset-speciﬁc tuning of the architecture and the amounts of con-text used for the different aggregate features.
In this work, we introduce Anticipative Video Trans-former (AVT), an alternate video modeling architecture that replaces “aggregation” based temporal modeling with a an-ticipative1 architecture. Aiming to overcome the tradeoffs described above, the proposed model naturally embraces the sequential nature of videos, while minimizing the lim-itations that arise with recurrent architectures. Similar to recurrent models, AVT can be rolled out indeﬁnitely to pre-dict further into the future (i.e. generate future predictions), yet it does so while processing the input in parallel with long-range attention, which is often lost in recurrent archi-tectures.
Speciﬁcally, AVT leverages the popular transformer ar-chitecture [89, 92] with causal2 masked attention, where each input frame is allowed to attend only to frames that precede it. We train the model to jointly predict the next action while also learning to predict future features that match the true future features and (when available) their intermediate action labels. Figure 1 shows examples of how AVT’s spatial and temporal attention spreads over pre-viously observed frames for two of its future predictions (wash tomato and turn-off tap). By incorporating interme-diate future prediction losses, AVT encourages a predictive video representation that picks up patterns in how the vi-sual activity is likely to unfold into the future. This facet of our model draws an analogy to language, where trans-1We use the term “anticipative” to refer to our model’s ability to pre-dict future video features and actions. 2Throughout we use the term “causal” to refer to the constraint that video be processed in a forward, online manner, i.e. functions applied at time t can only reference the frames preceding them, akin to Causal Lan-guage Modeling (CLM) [51]. This is not to be confused with other uses of
“causal” in AI where the connotation is instead cause-and-effect. formers trained with massive text corpora are now powerful tools to anticipate sequences of words (cf. GPT and vari-ants [8, 69, 70]). The incremental temporal modeling aspect has been also been explored for action recognition [53], al-beit with convolutional architectures and without interme-diate self-supervised losses.
While the architecture described so far can be applied on top of various frame or clip encoders (as we will show in experiments), we further propose a purely attention-based video modeling architecture by replacing the backbone with an attention-based frame encoder from the recently intro-duced Vision Transformer [18]. This enables AVT to at-tend not only to speciﬁc frames, but also to spatial features within the frames in one uniﬁed framework. As we see in Figure 1, when trained on egocentric video, the model spontaneously learns to attend to spatial features corre-sponding to hands and objects, which tend to be especially important in anticipating future activities [57].
In summary, our contributions are: 1) AVT, a novel end-to-end purely attention based architecture for predic-tive video modeling; 2) Incorporation of a self-supervised future prediction loss, making the architecture especially applicable to predictive tasks like action anticipation; 3) Ex-tensive analysis and ablations of the model showing its ver-satility with different backbone architectures, pre-trainings, etc. on the most popular action anticipation benchmarks, both from ﬁrst and third person viewpoints. Speciﬁcally, we outperform all published prior work on EpicKitchens-553 [13], EpicKitchens-1003 [14], EGTEA Gaze+ [55], and 50-Salads [82]. Most notably, our method outperforms all submissions to the EpicKitchens-100 CVPR’21 challenge4, and is ranked #1 on the EpicKitchens-55 leaderboard5 for seen (S1) and #2 on unseen (S2) test sets. 2.