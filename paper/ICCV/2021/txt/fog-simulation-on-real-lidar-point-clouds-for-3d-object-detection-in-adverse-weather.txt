Abstract
This work addresses the challenging task of LiDAR-based 3D object detection in foggy weather. Collecting and annotating data in such a scenario is very time, labor and cost intensive. In this paper, we tackle this problem by simu-lating physically accurate fog into clear-weather scenes, so that the abundant existing real datasets captured in clear weather can be repurposed for our task. Our contribu-tions are twofold: 1) We develop a physically valid fog simulation method that is applicable to any LiDAR dataset.
This unleashes the acquisition of large-scale foggy train-ing data at no extra cost. These partially synthetic data can be used to improve the robustness of several perception methods, such as 3D object detection and tracking or si-multaneous localization and mapping, on real foggy data. 2) Through extensive experiments with several state-of-the-art detection approaches, we show that our fog simulation can be leveraged to significantly improve the performance for 3D object detection in the presence of fog. Thus, we are the first to provide strong 3D object detection baselines on the Seeing Through Fog dataset. Our code is available at www.trace.ethz.ch/lidar fog simulation. 1.

Introduction
Light detection and ranging (LiDAR) is crucial for the implementation of safe autonomous cars, because LiDAR measures the precise distance of objects from the sensor, which cameras cannot measure directly. Thus, LiDAR has found its way into many applications, including detec-tion [23, 34], tracking [7, 50], localization [25, 8], and map-ping [48, 15]. Despite the benefit of measuring exact depth information, LiDAR has a significant drawback. The light pulses that LiDAR sensors emit in the invisible near infrared (NIR) spectrum (typically at 850 and 903 to 905 nm wave-length [4]) do not penetrate water particles, as opposed to automotive radars. This means as soon as there are water particles in the form of fog in the air, light pulses emitted by the sensor will undergo backscattering and attenuation. (a) strongest returns (b) last returns
Figure 1: LiDAR returns caused by fog in the (top) scene. (a) shows the strongest returns and (b) the last returns, color coded by the LiDAR channel. The returns of the ground are removed for better visibility of the points introduced by fog.
Best viewed in color (red (cid:98)= low, cyan (cid:98)= high, 3D bounding box annotation in green, ego vehicle dimensions in gray).
Attenuation reduces the received signal power that corre-sponds to the range of the solid object in the line of sight which should be measured, while backscattering creates a spurious peak in the received signal power at an incorrect range. As a result, the acquired LiDAR point cloud will contain some spurious returns whenever there is fog present at the time of capture. This poses a big challenge for most outdoor applications, as they typically require robust per-formance under all weather conditions.
In recent years, several LiDAR datasets for 3D object detection [10, 3, 19, 5, 39, 27, 11, 16] have been presented.
Although many of them contain diverse driving scenarios, none of them allows an evaluation on different kinds of ad-verse weather. Only recently, the Canadian Adverse Driving
Conditions (CADC) Dataset [28] and the Seeing Through
Fog (STF) Dataset [2] address the need for such an eval-uation. While CADC focuses on snowfall, STF is targeted towards evaluation under fog, rain and snow. Consequently, there is still not a large quantity of LiDAR foggy data avail-able that could be used to train deep neural networks.
The reason for this is obvious: collecting and annotating large-scale datasets per se is time, labor and cost intensive, let alone when done for adverse weather conditions.
This is exactly the shortfall that our work addresses. In
Sec. 3, we propose a physically-based fog simulation that converts real clear-weather LiDAR point clouds into foggy counterparts. In particular, we use the standard linear sys-tem [30] that models the transmission of LiDAR pulses. We distinguish between the cases of clear weather and fog with respect to the impulse response of this system and estab-lish a formal connection between the received response un-der fog and the respective response in clear weather. This connection enables a straightforward transformation of the range and intensity of each original clear-weather point, so that the new range and intensity correspond to the measure-ment that would have been made if fog was present in the scene. We then show in Sec. 4 that several state-of-the-art 3D object detection pipelines can be trained on our partially synthetic data to get improved robustness on real foggy data. This scheme has already been applied on images for semantic segmentation [32, 31, 13] and we show that it is also successful for LiDAR data and 3D object detection.
For our experiments, we simulate fog on the clear-weather training set of STF [2] and evaluate on their real foggy test set. Fig. 1 shows an example scene from the
STF dense fog test set, where the noise introduced by fog is clearly visible in the LiDAR data. The authors of STF [2] used a Velodyne HDL-64E as their main LiDAR sensor.
This sensor comes with 64 channels and a so-called dual mode. In this mode, it can measure not only the strongest, but also the last return received for each individual emitted light pulse. Even though the last signal contains less severe noise, fog still causes a significant amount of spurious re-turns. Therefore, even in this dual mode, the sensor cannot fully “see through fog”.
Fig. 2 shows an interesting characteristic of the noise in-troduced by fog, namely that it is not uniformly distributed around the sensor. On the contrary, the presence of noise de-pends on whether there is a target in the line of sight below a certain range from the sensor. If there is a solid object at a moderate range, there are few, if any, spurious returns from the respective pulses. On the other hand, if there is no target in the line of sight below a certain range, there are a lot of spurious returns that are caused by fog. This becomes ap-parent in the example of Fig. 2, where on the left side of the road there is a hill and on the right side there is open space behind the guardrail. Only in the latter case does the noise caused by fog appear in the measurement. This behavior is explained with our theoretical formulation in Sec. 3.
As a side note, similar sensor noise can also be caused by exhaust smoke, but if the future of transportation goes electric, at least this problem may vanish into thin air. (a) channel (b) intensity
Figure 2: LiDAR returns caused by fog in the (top) scene.
Color coded by the LiDAR channel in (a) and by the inten-sity in (b). The returns of the ground are removed for better visibility of the points introduced by fog. Best viewed in color, same color coding as in Fig. 1 applies. 2.