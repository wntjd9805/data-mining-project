Abstract
Recent studies on Generative Adversarial Network (GAN) reveal that different layers of a generative CNN hold different semantics of the synthesized images. However, few
GAN models have explicit dimensions to control the seman-tic attributes represented in a speciﬁc layer. This paper pro-poses EigenGAN which is able to unsupervisedly mine in-terpretable and controllable dimensions from different gen-erator layers. Speciﬁcally, EigenGAN embeds one linear subspace with orthogonal basis into each generator layer.
Via generative adversarial training to learn a target distri-bution, these layer-wise subspaces automatically discover a set of “eigen-dimensions” at each layer corresponding to a set of semantic attributes or interpretable variations. By traversing the coefﬁcient of a speciﬁc eigen-dimension, the generator can produce samples with continuous changes corresponding to a speciﬁc semantic attribute. Taking the human face for example, EigenGAN can discover control-lable dimensions for high-level concepts such as pose and gender in the subspace of deep layers, as well as low-level concepts such as hue and color in the subspace of shal-low layers. Moreover, in the linear case, we theoretically prove that our algorithm derives the principal components as PCA does. Codes can be found in https://github. com/LynnHo/EigenGAN-Tensorflow. 1.

Introduction
Strong evidences [40, 42, 2] show that different layers of a discriminative CNN capture different semantic con-cepts in terms of abstraction level, e.g., shallower layers detect color and texture while deeper layers focus more on objects and parts. Accordingly, we can expect that a generative CNN also has similar property, which is con-ﬁrmed by the recent studies of generative adversarial net-work (GAN) [18, 39, 3]. StyleGAN [18] shows that deeper generator layers control higher-level attributes such as pose and glasses while shallower layers control lower-level fea-tures such as color and edge. Yang et al. [39] found simi-lar phenomenon in scene synthesis, showing that deep lay-ers tend to determine the spatial layout while shallow lay-ers determine the color scheme. Similar conclusion is also made by Bau et al. [3]. All these evidences reveal a prop-Figure 1. Example of interpretable dimensions learned by Eigen-GAN. The smaller the index, the deeper the layer. erty that different generator layers hold different semantics of the synthesized images in terms of abstraction level.
According to this property, one can identify semantic at-tributes from different layers of a well-trained generator by performing post-processing algorithms [3, 12, 36, 39], and then can manipulate these attributes on the synthesized im-ages. For example, Bau et al. [3] identify the causal units for a speciﬁc concept (such as “tree”) by dissection and inter-vention on each generator layer. Turning on or off the causal units causes the concept to appear or disappear on the syn-thesized image. However, these post-processing methods can only be applied to a well-trained and ﬁxed generator.
As for the generator itself, it still operates as a black box and lacks explicit dimensions to directly control the seman-tic attributes represented in different layers. In other words, we do not know what attributes are represented in differ-ent generator layers or how to manipulate them, unless we deeply inspect each layer by these post-processing methods.
Under above discussion, this paper starts with a question: can a generator itself automatically/unsupervisedly learn explicit dimensions that control the semantic attributes rep-resented in different layers? To this end, we propose to em-bed one linear subspace model with orthogonal basis into each generator layer, named as EigenGAN. First, via gener-ative adversarial training, the generator tries to capture the principal variations of the data distribution, and these prin-cipal variations are separately represented in different layers in terms of their abstraction level. Second, with the help of the subspace model, the principal variations of a speciﬁc layer are further orthogonally separated into different ba-sis vectors. Finally, each basis vector discovers an “eigen-dimension” that controls an attribute or interpretable varia-tion corresponding to the semantics of its layer. For exam-ple, as shown at the top of Fig. 1, an eigen-dimension of the subspace embedded in a deep layer controls gender, while another of the subspace embedded in the shallowest layer controls the hue of the image. Furthermore, in the linear case, i.e., one layer model, we theoretically prove that our
EigenGAN is able to discover the principal components as
PCA [15] does, which gives us a strong insight and reason to embed the subspace models into different generator layers.
Besides, we also provide a manifold perspective showing that our EigenGAN decomposes the data generation model-ing into layer-wise dimension expanding steps. 2.