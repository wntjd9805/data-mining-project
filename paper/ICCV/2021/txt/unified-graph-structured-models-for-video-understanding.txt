Abstract
Accurate video understanding involves reasoning about the relationships between actors, objects and their environ-ment, often over long temporal intervals.
In this paper, we propose a message passing graph neural network that explicitly models these spatio-temporal relations and can use explicit representations of objects, when supervision is available, and implicit representations otherwise. Our for-mulation generalises previous structured models for video understanding, and allows us to study how different de-sign choices in graph structure and representation affect the model’s performance. We demonstrate our method on two different tasks requiring relational reasoning in videos – spatio-temporal action detection on AVA and UCF101-24, and video scene graph classiﬁcation on the recent Ac-tion Genome dataset – and achieve state-of-the-art results on all three datasets. Furthermore, we show quantitatively and qualitatively how our method is able to more effectively model relationships between relevant entities in the scene. 1.

Introduction
Deep learning has enabled rapid advances in many im-age understanding tasks such as image classiﬁcation [20], object detection [45] and semantic segmentation [7]. How-ever, progress on recent video understanding datasets such as AVA [17] and Charades [48] has lagged behind in com-parison. Further progress in the video understanding tasks posed by these datasets would facilitate applications in au-tonomous vehicles, health monitoring and automated media analysis and production among others.
A reason why video understanding is so challenging is because, as shown in Fig. 1, it requires understanding the interactions between actors, objects and other context in the scene. Furthermore, these interactions are not always ob-servable from a single frame, and thus require reasoning over long temporal intervals. This is illustrated in Fig. 1, where understanding the actions of the person in the centre-frame is not possible from the target keyframe alone. In or-der to know that the woman is “listening”, we need to con-sider the man who is speaking but no longer in the scene.
And to correctly infer that the woman is “driving” the car, rather than “riding” like the man, we must take into account that she is later holding the steering wheel.
Video is a signiﬁcantly higher-dimensional signal than single images, due to its additional temporal axis, and so we believe learning these unlabelled interactions directly from current datasets with large convolutional networks is not feasible. In this paper, we propose a structured graph neural network to explicitly model these spatio-temporal interac-tions. We model actors and objects (explicitly with bound-ing boxes when we have supervision, and implicitly other-wise) as nodes in our spatio-temporal graph and perform message passing inference to directly model their relations.
Although a wide range of graph-structured models have been proposed for action recognition, we note that there has been no unifying formulation for these models. As such, some works only model spatial relations between actors and objects [14, 53], but not how these interactions evolve over time. Other approaches model long-range temporal interactions [60], but do not capture spatial relations and are not trained end-to-end. And whilst some methods do model spatio-temporal interactions of objects [3, 59], their explicit representations of objects need additional supervi-sion, and are not evaluated on spatio-temporal localisation tasks which requires detailed understanding and is neces-sary for analysing untrimmed videos.
Our graph network formulation based on the message-passing neural network [13] abstraction, allows us to ex-interactions between actors, objects and plicitly model the scene, and how these interactions evolve over time.
Our ﬂexible model allows us to use explicit object rep-resentations from a pretrained Region Proposal Network (RPN) [45], and/or implicitly from convolutional feature maps without additional supervision. Moreover, our general formulation allows us to interpret previous work [14, 53, 59, 60, 69] as special cases, and thus understand how different design choices in object representation, graph connectivity and message passing functions affect the model’s perfor-mance. We demonstrate our versatile model on two dif-ferent tasks: spatio-temporal action detection on AVA [17]
Figure 1: Understanding videos requires reasoning over long-term spatio-temporal interactions between actors, objects and the environ-ment. The actions of the woman in the centre frame are ambiguous given the nearby frames which typical 3D CNN architectures consider.
However, by considering her interactions with the man from nearby frames, we know she is “listening” to him. And the fact that she is later holding the steering wheel indicates she is “driving” in contrast to the man who is “riding”. In this paper, we propose a spatio-temporal graph on which we perform message passing to explicitly model these spatio-temporal interactions. Example from the AVA dataset [17]. and UCF101-24 [52], and video scene graph prediction on the recent Action Genome [23] dataset. Both of these tasks require modelling the spatio-temporal interactions between the actors and/or objects in the scene, and we show con-sistent improvements from using each component of our model and achieve state-of-the-art results on each dataset.
Furthermore, we observe that the largest improvements in AVA are indeed achieved on action classes involving human-to-human and human-to-object interactions, and vi-sualisations of our network show that it is focusing on scene context that is intuitively relevant to its action classiﬁcation. 2.