Abstract
Pedestrian behavior prediction is one of the major chal-lenges for intelligent driving systems. Pedestrians often ex-hibit complex behaviors inﬂuenced by various contextual elements. To address this problem, we propose BiPed, a multitask learning framework that simultaneously predicts trajectories and actions of pedestrians by relying on multi-modal data. Our method beneﬁts from 1) a bifold encoding approach where different data modalities are processed in-dependently allowing them to develop their own representa-tions, and jointly to produce a representation for all modal-ities using shared parameters; 2) a novel interaction mod-eling technique that relies on categorical semantic parsing of the scenes to capture interactions between target pedes-trians and their surroundings; and 3) a bifold prediction mechanism that uses both independent and shared decoding of multimodal representations. Using public pedestrian be-havior benchmark datasets for driving, PIE and JAAD, we highlight the beneﬁts of the proposed method for behavior prediction and show that our model achieves state-of-the-art performance and improves trajectory and action predic-tion by up to 22% and 9% respectively. We further investi-gate the contributions of the proposed reasoning techniques via extensive ablation studies. 1.

Introduction
Predicting road user behavior in complex urban envi-ronments is fundamental for assistive and intelligent driv-ing systems. Prediction is particularly challenging when these systems are encountering pedestrians who exhibit di-verse behaviors [1] that depend on various contextual fac-tors, such as social interactions, road structure, trafﬁc con-dition, and other environmental factors [2].
Pedestrian behavior can be predicted implicitly in the form of future trajectories [3, 4, 5], or explicitly in the form of upcoming actions [6, 7, 8]. It is evident from recent stud-ies [5, 9, 10] that both types of behavior prediction play complementary roles. For instance, predicting pedestrian actions, such as crossing the road, implies the possibility of a lateral motion across the road. Similarly, a pedestrian
Figure 1. Proposed multitask learning for simultaneous prediction of pedestrian trajectory, action and ﬁnal location. Interaction of trafﬁc elements are modeled along with separately and jointly en-coded visual context, pedestrian motion, and ego-motion inputs. approaching a parked vehicle is expected to interact with it. To capture these complementary aspects of behavior, we propose a multitask learning framework that simultaneously predicts trajectories, actions, and ﬁnal locations of pedestri-ans (see Figure 1). To learn complex pedestrian behavior, our model relies on multiple data modalities including vi-sual context, pedestrian motion, and ego-vehicle dynamics.
The proposed method independently and jointly processes different input modalities and tasks. Independent process-ing allows each modality or task to learn its own parameters, whereas joint processing acts as a regularizer, inducing the model to learn more representative features. Since pedestri-ans’ behaviors are often inﬂuenced by what is around them, we introduce a novel technique to model interactions be-tween target pedestrians and their surroundings based on the semantic composition of the scenes. The proposed tech-nique relies on visuospatial semantic representations of the scenes divided into categories based on object classes.
We evaluate the performance of the proposed method using public pedestrian behavior benchmark datasets, PIE
[10] and JAAD [11], and show that our method signiﬁcantly improves over state-of-the-art algorithms on both pedestrian trajectory and action prediction tasks. 2.