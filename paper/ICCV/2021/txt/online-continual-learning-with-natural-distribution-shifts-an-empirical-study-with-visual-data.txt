Abstract
Continual learning is the problem of learning and re-taining knowledge through time over multiple tasks and environments. Research has primarily focused on the in-cremental classification setting, where new tasks/classes are added at discrete time intervals. Such an “offline” setting does not evaluate the ability of agents to learn effectively and efficiently, since an agent can perform multiple learning epochs without any time limitation when a task is added.
We argue that “online” continual learning, where data is a single continuous stream without task boundaries, enables evaluating both information retention and online learning efficacy. In online continual learning, each incoming small batch of data is first used for testing and then added to the training set, making the problem truly online. Trained models are later evaluated on historical data to assess in-formation retention. We introduce a new benchmark for online continual visual learning that exhibits large scale and natural distribution shifts. Through a large-scale analysis, we identify critical and previously unobserved phenomena of gradient-based optimization in continual learning, and propose effective strategies for improving gradient-based online continual learning with real data. The source code and dataset are available in: https://github.com/
IntelLabs/continuallearning. 1.

Introduction
Supervised learning aims to find models that can predict labels given input data, with satisfactory performance when evaluated on a specific population of interest. This popula-tion is typically sampled to create training data for the model to learn over. The critical requirement for successful learning is a set of training data points that are independent and iden-tically distributed (iid.). Continual learning challenges this assumption and considers a sequence of learning problems where the distribution changes dramatically through time.
This setting is crucial when learned models are deployed in interactive systems, since the environment with which agents interact continually evolves.
There are two critical performance metrics for a continual learner: learning efficacy and information retention. Learn-ing efficacy entails the simple question, “is learning the n-th thing easier than learning the first thing?” [31]. This ability is critical for fast learning and quick adaptation. Information retention considers the model’s ability to quickly recall when faced with a historical task that had been previously consid-ered. This question is also studied to understand a property of neural networks called catastrophic forgetting [21].
Continual learning algorithms are typically evaluated in an incremental classification setting, where tasks/classes ar-rive one-by-one at discrete time intervals. Multiple learning epochs over the current task are permitted and the learner can spend as much time as desired to learn each task. This setting is appropriate for evaluating information retention, because access to previous tasks is prohibited. However, learning efficacy is not evaluated in this setting, because models can easily learn each task from scratch and still be successful [23]. We refer to this incremental classification setting as offline continual learning.
Our work focuses on online continual learning, which aims to evaluate learning efficacy in addition to information retention.
In online continual learning, there is a single online stream of data. At every time step, a small batch of data arrives. The model needs to immediately predict labels for the incoming data as an online testing step. After the prediction, this batch of data is added to the dataset. The model needs to be updated before the next batch of data arrives using a fixed budget of computation and memory; this is an online training step. Testing and training are on the fly. Moreover, as we are in a continual learning setting, the data distribution changes over time. A successful online test performance requires efficient learning and adaptation in this non-stationary setting.
To study online continual visual learning, we construct a new benchmark where the data distribution evolves naturally over time. To do so, we leverage images with geolocation tags and time stamps. We use a subset of YFCC100M [30] with 39 million images captured over 9 years. Our task is online continual geolocalization. We empirically evaluate the natural distribution shift and validate that the benchmark
is appropriate to study online continual learning. We further use this benchmark to analyze the behavior of gradient-based optimization in online continual learning. Our experiments suggest that the non-stationarity in the data results in a sig-nificantly different behavior from what was previously ob-served in offline continual learning. Surprisingly, learning efficacy and information retention turn out to be conflicting objectives from an optimization perspective, necessitating a careful trade-off. We also found that mini-batching is a non-trivial problem for online continual learning. Increas-ing batch sizes in SGD, even by a small factor, significantly hurts both learning efficacy and information retention. Based on the analysis, we propose simple yet effective strategies, such as online learning rate and replay buffer size adaptation algorithms, that significantly improve gradient-based opti-mization for online continual learning. We will share our benchmark and code with the community in order to support future research in online continual visual learning. 2.