Abstract
The core of visual place recognition (VPR) lies in how to identify task-relevant visual cues and embed them into dis-criminative representations. Focusing on these two points, we propose a novel encoding strategy named Attentional
Pyramid Pooling of Salient Visual Residuals (APPSVR). It incorporates three types of attention modules to model the saliency of local features in individual, spatial and cluster (1) To inhibit task-irrelevant lo-dimensions respectively. cal features, a semantic-reinforced local weighting scheme is employed for local feature reﬁnement; (2) To leverage the spatial context, an attentional pyramid structure is con-structed to adaptively encode regional features according to their relative spatial saliency; (3) To distinguish the dif-ferent importance of visual clusters to the task, a para-metric normalization is proposed to adjust their contribu-tion to image descriptor generation. Experiments demon-strate APPSVR outperforms the existing techniques and achieves a new state-of-the-art performance on VPR bench-mark datasets. The visualization shows the saliency map learned in a weakly supervised manner is largely consistent with human cognition. 1.

Introduction
Visual place recognition (VPR) has become the core technique of many promising applications in the ﬁeld of computer vision [1,2,5,39,41,45] and robotics [7,10,11,25], such as autonomous driving [6,24,28], geo-localization [22, 23, 40], 3D reconstruction [9] and virtual reality [26].
VPR in large-scale environments is typically solved as an instance retrieval task [1, 2, 18, 19, 40, 41, 45], where the goal is to ﬁnd the most visually similar database images for a given query image. The main challenge is different viewpoints, weather and illumination may cause dramatic changes in the appearance of the same scene. Partial occlu-sion and dynamic objects also bring additional challenges to the task. Therefore, how to construct powerful image representations has raised widespread concerns in the ﬁeld.
In the exploratory work carried out over the past decades,
Figure 1. APPSVR consists of two main steps: local reﬁnement and global integration. Reﬁning the prior knowledge of “preserv-ing the semantics of buildings”, local reﬁnement (b) can adaptively highlight billboards and inhibit repeated structures on buildings.
Certain visual cues improperly retained in (b) (e.g., vehicle parts that look like architectural windows) can be suppressed in the sub-sequent global integration (c). Finally, the overall attention (d) of APPSVR is consistent with human perception habit of valuing static structures and omitting misleading visual elements.
VLAD [3] and its variants [1, 19, 48] stand out from other counterparts by introducing residual that can better charac-terize the nuances of local details. Drawing on their wis-dom, we follow the basic idea of aggregating cluster-wise residuals for feature embedding. Considering that not every visual element in the image is helpful to the VPR task, it is necessary to emphasize the task-relevant ones in the image representation.
With similar motivations, early attempts [2, 20, 39] have been made to accentuate task-relevant local features. Fol-lowing the pace of deep learning, the recent attention-aware methods for VPR can be broadly divided into two cate-gories. The data-driven methods [19, 31, 51] usually in-tegrate trainable attention modules into the encoding net-work. Through end-to-end learning, these modules essen-tially act as a black box weighting of local features. The rule-based methods [27, 29, 35] typically use artiﬁcial rules to ﬁlter speciﬁc visual cues for subsequent encoding. Their performance is susceptible to the bias of rough prior knowl-edge. To combine the advantages of both categories, we adopt a semantic reinforced attention module [32] for local feature reﬁnement, where semantic priors can be reﬂected by the initial weights of the parametric model. With further
ﬁne-tuning, the model can learn comprehensive reasoning habits from prior knowledge and data-driven training.
Besides the individual distinctiveness, local features’ task relevance also greatly depends on their context in the scene. Some existing methods, such as contextual reweight-ing [19] and multi-scale regional pooling [44, 48, 51], have demonstrated the advantages of incorporating spatial infor-mation into the encoding strategy.
Inspired by them, we develop an attentional pyramid pooling to leverage the fea-tures’ regional context. Speciﬁcally, an overlapping pyra-mid structure is constructed, where regional features are for-mulated by aggregating salient visual residuals within each grid. Then through a spatial attention module, regional fea-tures are weighted by their relative spatial saliency before being embedded into the visual word vector. In particular, our spatial saliency weight is derived based on the global context, not just based on the regions in the rigid grid.
Moreover, the ﬁnal representation of VLAD variants [1, 17,34] is normally the concatenation of visual word vectors, whose scales are equalized by intra-normalization [17].
Consequently, all visual words contribute the same to the descriptor generation and similarity metric. To distinguish their different importance to the task, we propose a paramet-ric normalization, through which visual word vectors are rescaled according to their task relevance and then concate-nated as a unit image descriptor. In this way, the different saliency of visual clusters can be intuitively highlighted in the similarity interpretation during indexing.
In summary, we propose an attentional encoding strategy for VPR, named Attentional Pyramid Pooling of Salient Vi-sual Residuals (APPSVR). Particularly, we introduce three types of attention modules to model the saliency of local features in individual, spatial and cluster dimensions re-spectively. Incorporating the triple attention, our model can adaptively identify and embed salient visual cues into dis-criminative image descriptor. Experiments verify the effec-tiveness of all proposed modules, and demonstrate that our model signiﬁcantly outperforms baseline methods on city-scale benchmark datasets. In summary, our contributions are as follows:
• We propose an encoding strategy APPSVR for VPR, which integrates the triple attention from individual, spatial and cluster saliency into feature embedding.
• For local feature reﬁnement, we adopt a semantic-reinforced local weighting scheme, where crompre-hensive local saliency can be learned from both prior knowledge and data-driven ﬁne-tuning.
• We propose an attentional pyramid pooling and a para-metric normalization for global integration, through which spatial and cluster saliency can be incorporated into the encoding strategy.
• Experiments demonstrate that APPSVR outperforms existing methods and achieves a new state-of-the-art performance on benchmark datasets. The visualization shows the attention learned under weak supervision is highly consistent with human cognition. 2.