Abstract
We present MetaUVFS as the first Unsupervised Meta-learning algorithm for Video Few-Shot action recogni-tion. MetaUVFS leverages over 550K unlabeled videos to train a two-stream 2D and 3D CNN architecture via con-trastive learning to capture the appearance-specific spatial and action-specific spatio-temporal video features respec-tively. MetaUVFS comprises a novel Action-Appearance
Aligned Meta-adaptation (A3M) module that learns to fo-cus on the action-oriented video features in relation to the appearance features via explicit few-shot episodic meta-learning over unsupervised hard-mined episodes. Our action-appearance alignment and explicit few-shot learner conditions the unsupervised training to mimic the down-stream few-shot task, enabling MetaUVFS to significantly outperform all state-of-the-art unsupervised methods on few-shot benchmarks. Moreover, unlike previous few-shot action recognition methods that are supervised, MetaUVFS needs neither base-class labels nor a supervised pretrained backbone. Thus, we need to train MetaUVFS just once to perform competitively or sometimes even outperform state-of-the-art supervised methods on popular HMDB51,
UCF101, and Kinetics100 few-shot datasets. 1.

Introduction
Few-shot learning [36, 53, 61, 17, 51, 48, 36, 14, 10, 27, 66] has emerged as a school of approaches that train a model to transfer-learn or adapt quickly on novel, of-ten out-of-domain, classes using as few labeled samples as possible to mitigate the lack of large-scale supervision for these novel classes. Few-shot learning is highly rele-vant for videos because collecting large-scale labeled video data is extra challenging with the additional temporal di-mension. There has been work utilizing both 2D and 3D
CNNs [74, 5, 15, 68, 4, 71] to achieve strong results on few-⋆ Authors with equal contribution.
Work done when Jay Patravali was a research intern at Microsoft.
Figure 1. The above example shows a 1-support 5-way few-shot video action recognition task to classify a query sample of novel
Tennis Swing class. Using only appearance with a 2D CNN in-correctly predicts Jump Rope as it relies on only frame-level spa-tial cues. Using only action with a 3D CNN incorrectly predicts
Golf Swing as it matches based on just the swinging action with-out paying attention to the spatial cues. Whereas MetaUVFS pre-dicts the correct class via the Action-Appearance Aligned Meta-adaptation (A3M) module that learns to align and relate the ac-tion with the appearance attributes via few-shot meta-learning. All three methods are trained using unlabeled videos. shot action recognition in videos. However, these are super-vised approaches and require large amounts of labeled base-class data and/or large-scale supervised pretrained back-bones [5, 4, 15, 68] that are not only prohibitively expen-sive to scale but also oftentimes unattainable. Meanwhile, there is virtually infinite unlabeled video data at our disposal through the rise of multi-media social networking. This mo-tivates us to address the question, “Can we develop models for video action recognition that perform competitively on few-shot benchmarks without the use of either base-class labels or any external supervision?”
Existing unsupervised video representation learning methods [47, 55, 24] provide task-agnostic representations that apply to various downstream tasks. However, as we
show in later sections, these methods are not specialized for the few-shot learning task with novel classes and therefore perform sub-optimally on them.
To this end, we propose MetaUVFS as the first method for unsupervised meta-learning for few-shot video action recognition. MetaUVFS leverages large-scale (over half a million) unlabeled video data to learn video representations via contrastive learning and then trains an explicit few-shot meta-learner using episodes that are hard-mined over the learned representations. The episodic meta-learning helps mimic the episodic few-shot meta-testing during the train-ing phase. This imposes a downstream task-specific prior on the learned video representations and reduces the knowl-edge gap between training and testing.
We introduce an unsupervised two-stream action-appearance network in MetaUVFS to learn fine-grained spatio-temporal 3D features over video segments via an ac-tion stream and spatial 2D features over video frames via an appearance stream. Direct finetuning of either feature alone can be sub-optimal in a challenging few-shot sce-nario as illustrated in Fig. 1. Instead, we design an Action-Appearance Aligned Meta-adaptation module (A3M) in the few-shot meta-learner of MetaUVFS that combines the two streams by learning a spatio-temporal alignment of appear-ance over action features. A3M learns an attention map conditioned on the action and appearance features to bet-ter focus on the action-specific features in the frame-level appearance embeddings. This helps to improve intra-class similarity and reduce inter-class confusion for few-shot.
Consequently, MetaUVFS outperforms all state-of-the-art (SoTA) unsupervised video learning methods on multi-ple benchmark datasets and also outperforms or performs competitively against the SoTA few-shot action recognition methods. To summarize, our main contributions are, 1. We propose MetaUVFS as the first unsupervised meta-learning algorithm for few-shot video action recogni-tion. 2. MetaUVFS uses a two-stream network to learn action and appearance-specific features via contrastive learn-ing over 550K unlabeled videos. It employs a novel
Action-Appearance Aligned Meta-adaptation (A3M) module that is episodically trained via hard-mined episodes to specialize for few-shot downstream tasks. 3. MetaUVFS outperforms all SoTA unsupervised meth-ods across multiple few-shot benchmarks and per-forms competitively to or even outperforms some of the SoTA few-shot action recognition methods. 2.