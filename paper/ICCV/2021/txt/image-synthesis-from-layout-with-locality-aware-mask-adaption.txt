Abstract
This paper is concerned with synthesizing images con-ditioned on a layout (a set of bounding boxes with ob-ject categories). Existing works construct a layout-mask-image pipeline. Object masks are generated separately and mapped to bounding boxes to form a whole semantic seg-mentation mask (layout-to-mask), with which a new image is generated (mask-to-image). However, overlapped boxes in layouts result in overlapped object masks, which reduces the mask clarity and causes confusion in image generation.
We hypothesize the importance of generating clean and se-mantically clear semantic masks. The hypothesis is sup-ported by the finding that the performance of state-of-the-art LostGAN decreases when input masks are tainted. Mo-tivated by this hypothesis, we propose Locality-Aware Mask
Adaption (LAMA) module to adapt overlapped or nearby object masks in the generation. Experimental results show our proposed model with LAMA outperforms existing ap-proaches regarding visual fidelity and alignment with input layouts. On COCO-stuff in 256 256, our method improves the state-of-the-art FID score from 41.65 to 31.12 and the
SceneFID from 22.00 to 18.64.
× 1.

Introduction
This paper is concerned with image generation from lay-outs, a specific task of conditional image synthesis. A lay-out is a set of bounding boxes with object categories, repre-senting the positions, sizes and classes of objects in an im-age. The layout-to-image generation task is to convert the bounding boxes to a photorealistic image without segmenta-tion annotation [37]. This task remains a challenging prob-lem but provides a promising approach to understanding vi-sual relations in images via analysis-by-synthesis. It also has a wide range of applications such as human-computer collaborative creation, where a potentially desired picture is generated according to the layout given by a human.
Existing works construct an effective layout-mask-image
Figure 1. An illustration of image generation from layouts (bounding boxes with categories). Based on the input layout as shown in column (a), the generator first synthesizes a semantic mask in (b) and then translates the mask to an image in (c). (d) summarizes the generation process. Masks and images are gener-ated by our model with the Locality-Aware Mask Adaption. pipeline [32, 38]. As a layout only provides a coarse con-figuration of the desired image, a semantic segmentation mask is generated first and then translated to the final image (Fig. 1). A semantic segmentation mask specifies the cate-gory distribution of each pixel [7]. In the degraded and sim-ple case, each pixel belongs to only a single category. How-ever, a generated mask may not be “clean” and semantically clear but has overlapping masks and mixed categories of pixels. This is because the overall semantic mask is formed by aggregating object masks generated separately [32, 38].
As a result, when the bounding boxes overlap, which is a common situation, the object masks also overlap and the category of pixels can be confusing. Such an ambiguous semantic mask causes difficulty for image generation, as it is not clear for the generator to synthesize which object in the overlapped area. Therefore, we hypothesize that a se-mantically clear mask with little overlaps is important for high-quality image generation.
Hypothesis justification. To test our hypothesis, we train the mask-to-image component of LostGAN [32] with ground-truth masks (Sec. 3). When also tested on ground-truth masks, this mask-to-image model has a significantly better performance than the original layout-to-image one on visual fidelity and layout alignment. When tested with in-creasingly tainted masks, the model’s performance decays gradually (Fig. 2). This shows the impact of mask clarity on image quality and supports our hypothesis.
Our method. Motivated by the above hypothesis, we propose Locality-Aware Mask Adaption (LAMA) module in the context of layout-to-image generation (Sec. 4). Af-ter a raw semantic mask is formed by combining object masks, LAMA aims to adapt the raw mask to a cleaner one by considering objects’ local relation (Fig. 3). It scales the mask values of each object in each pixel individually with a learned matching mechanism. Empirically, when two ob-ject masks originally overlap, LAMA enables the mask of the background category to shrink precisely in pixels where the foreground mask is located (Fig. 4). Finally, the gen-erated mask and the category are injected into the image generation pipeline via normalization layers.
×
Experimental results. Empirically the proposed model outperforms the state-of-the-art methods in terms of visual quality of images and layout alignment (Sec. 6.1). On 256, our method improves the state-of-COCO-stuff in 256 the-art FID [10] score from 41.65 to 31.12 and the Scene-FID [35] from 22.00 to 18.64. Besides, a quantitative com-parison show LAMA refines the raw masks to cleaner ones with smaller entropies (Sec. 6.2). In addition, the contri-bution of LAMA module is exemplified in a control ex-periment. The performance of our model decays without
LAMA, while that of LostGAN [32] is improved when combined with LAMA (Sec 6.3).
Our main contribution lies in three aspects: 1. Conceptual contribution. A hypothesis on the im-portance of generating clean semantic masks is presented and preliminarily verified by experimental results. 2. Technical contribution. Locality-Aware Mask Adap-tion (LAMA) is proposed for generating a clean and sharp semantic mask to facilitate image generation. Our contribu-tion of LAMA is orthogonal to existing works, and LAMA can be easily integrated with other methods. 3. Metric contribution. An new and challenging eval-uation metric termed YOLO scores is proposed to measure the layout alignment of objects.
We provide a complete implementation with PyTorch
[27] including source code and evaluation metrics1.
Table 1. A brief summary of how existing methods aggregate over-lapped object masks/features. ⋄ means masks are generated with
BiConvLSTM [31] modeling mask relations during generation.
Methods
Layout2Im [37, 38]
LostGAN [32, 34]
OC-GAN [35]
Hong et al. [13]
Obj-GAN [20]
OP-GAN [11, 12]
SG2IM [15]
Ashual and Wolf [1]
Ours
Trained with
GT masks
Aggregating overlapped object masks/features
No
No
No
Yes
Yes
Yes
Yes
Yes
No
ConvLSTM
Normalize
Normalize and concaten-ate with layout boundaries
Sum ⋄
Maxpooling ⋄
Sum in global pathway and replacement in object path
Sum
Normalize
Adapt and normalize 2.