Abstract
Image generation has been heavily investigated in com-puter vision, where one core research challenge is to gen-erate images from arbitrarily complex distributions with lit-tle supervision. Generative Adversarial Networks (GANs) as an implicit approach have achieved great successes in this direction and therefore been employed widely. How-ever, GANs are known to suffer from issues such as mode collapse, non-structured latent space, being unable to com-pute likelihoods, etc. In this paper, we propose a new unsu-pervised non-parametric method named mixture of inﬁnite conditional GANs or MIC-GANs, to tackle several GAN is-sues together, aiming for image generation with parsimo-nious prior knowledge. Through comprehensive evalua-tions across different datasets, we show that MIC-GANs are effective in structuring the latent space and avoiding mode collapse, and outperform state-of-the-art methods. MIC-GANs are adaptive, versatile, and robust. They offer a promising solution to several well-known GAN issues. Code available: github.com/yinghdb/MICGANs. 1.

Introduction
GANs have achieved great successes in a fast-growing number of applications [19]. The success lies in their abil-ity to capture complex data distributions in an unsupervised, non-parametric and implicit manner [13]. Yet, such ability comes with limitations, such as mode collapse. Despite a range of methods attempting to address these issues, they are still open. This motivates our research aiming to mit-igate several limitations collectively including mode col-lapse, unstructured latent space, and being unable to com-pute likelihoods, which we hope will facilitate follow-up
GAN research and broaden their downstream applications.
GANs normally consist of two functions: a generator and a discriminator. In image generation, the discrimina-tor distinguishes between real and generated images, while the generator aims to fool the discriminator by generating
*Corresponding author. The authors from Zhejiang University are af-ﬁliated with the State Key Lab of CAD&CG. images that are similar to real data. The widely known mode collapse issue refers to the generator’s tendency to only generate similar data which aggregate around one or few modes in a multi-modal data distribution, e.g., only generating cat images in a cat/dog dataset. There has been active research in distribution matching to solve/mitigate mode collapse [31, 45, 50, 55], which essentially explic-itly/implicitly minimizes the distributional mismatch be-tween the generated and real data. In parallel, it is found that latent space structuring can also help, e.g. by intro-ducing conditions [39], noises [23], latent variables [5] or latent structures [15]. In comparison, latent space structur-ing does enable more downstream applications such as con-trolled image generation, but they normally require strong prior knowledge of the data/latent space structure, such as class labels or the cluster number in the data or the mode number in the latent space. In other words, they are either supervised, or unsupervised but parametric and prescribed.
We simultaneously tackle the latent space structure and mode collapse by proposing a new, unsupervised and non-parametric method, mixture of inﬁnite conditional GANs or MIC-GANs. Without loss of generality, we assume an image dataset contains multiple (unlabelled) clusters of im-ages, with each cluster naturally forming one mode. Instead of making a GAN avoid mode collapse, we make use of it, i.e. exploiting GAN’s mode collapse property, to let one
GAN cover one mode so that we can use multiple GANs to capture all modes. Next, doing so naturally brings the question of how many GANs are needed.
Instead of re-lying on the prior knowledge [3, 15], we aim to learn the
In other words, number of GANs needed from the data.
MIC-GANs model the distribution of an inﬁnite number of
GANs. Meanwhile, we also construct a latent space accord-ing to the data space by letting each GAN learn to map one latent mode to one data mode. Since there can be an inﬁnite number of modes in the data space, there are also the same number of modes in the latent space, each associated with one GAN. The latent space is then represented by a convex combination of GANs and is therefore structured.
To model a distribution of GANs, our ﬁrst technical nov-elty is a new Bayesian treatment on GANs, with a family 1
of non-parametric priors on GAN parameters. Speciﬁcally, we assume an inﬁnite number of GANs in our reservoir, so that for each image, there is an optimal GAN to gener-ate it. This is realized by imposing a Dirichlet Process [11] over the GAN parameters, which partitions the probabilistic space of GAN parameters into a countably inﬁnite set where each element corresponds to one GAN. The image genera-tion process is then divided into two steps: ﬁrst choose the most appropriate GAN for an image and then generate the image using the chosen GAN.
Our second technical novelty is a new hybrid inference scheme. Training MIC-GANs is challenging due to the in-ﬁnity nature of DP. Not only do we need to estimate how many GANs are needed, we also need to compute their parameters.Some speciﬁc challenges include: 1) unable to compute likelihoods from GANs (a fundamental ﬂaw of
GANs) [9]; 2) lack of an explicit form of GAN distribu-tions; 3) prohibitive computation for estimating a poten-tially inﬁnite number of GANs. These challenges are be-yond the capacity of existing methods. We therefore pro-pose a new hybrid inference scheme called Adversarial Chi-nese Restaurant Process.
MIC-GANs are unsupervised and non-parametric. They automatically learn the latent modes and map each of them to one data mode through one GAN. MIC-GANs not only avoid mode collapse, but also enable controlled image gen-eration, interpolation among latent modes, and a systematic exploration of the entire latent space. Through extensive evaluation and comparisons, we show the superior perfor-mance of MIC-GANs in data clustering and generation. 2.