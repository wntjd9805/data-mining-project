Abstract
Human Pose Estimation (HPE) aims at retrieving the 3D position of human joints from images or videos. We show that current 3D HPE methods suffer a lack of view-point equivariance, namely they tend to fail or perform poorly when dealing with viewpoints unseen at training time. Deep learning methods often rely on either scale-invariant, translation-invariant, or rotation-invariant oper-ations, such as max-pooling. However, the adoption of such procedures does not necessarily improve viewpoint gener-alization, rather leading to more data-dependent methods.
To tackle this issue, we propose a novel capsule autoen-coder network with fast Variational Bayes capsule rout-ing, named DECA. By modeling each joint as a capsule entity, combined with the routing algorithm, our approach can preserve the joints’ hierarchical and geometrical struc-ture in the feature space, independently from the viewpoint.
By achieving viewpoint equivariance, we drastically re-duce the network data dependency at training time, result-ing in an improved ability to generalize for unseen view-points. In the experimental validation, we outperform other methods on depth images from both seen and unseen view-points, both top-view, and front-view. In the RGB domain, the same network gives state-of-the-art results on the chal-lenging viewpoint transfer task, also establishing a new framework for top-view HPE. The code can be found at https://github.com/mmlab-cv/DECA.
1.

Introduction
Human pose estimation is key for many applications, such as action recognition, animation, gaming, to name a few [16, 29, 28]. State of the art methods [2, 32] that rely on RGB images can correctly localize human joints (e.g. torso, elbows, knees) in images, also in presence of occlu-sions. However, they tend to fail when dealing with chal-lenging scenarios. The top-view perspective, in particular, turns out to be a difﬁcult task; on the one hand, it causes the largest amount of joints occlusions, and on the other hand, it suffers the scarcity of suitable training data, as shown in
Fig. 1.
When presented with unseen viewpoints, humans dis-play a remarkable ability to estimate human poses, even in the presence of occlusions and unconventional joints con-ﬁgurations. This is not always true in computer vision.
In fact, available methods are trained in relatively con-strained settings [15], with a limited variability between dif-ferent viewpoints. Limited data, especially from the top-viewpoint, along with limited capabilities of modeling the hierarchical and geometrical structure of the human pose, results in poor generalization capabilities.
This generalization problem, known as the viewpoint problem, depends on how the network activations vary with the change of the viewpoint, usually after a transforma-tion (translation, scaling, rotation, shearing). Convolutional
Neural Networks (CNNs) scalar activations are not suitable to effectively manage these viewpoint transformations, thus needing to rely on max-pooling and aggressive data aug-mentation [4, 9, 22, 36]. By doing so, CNNs aim at achiev-ing viewpoint invariance, deﬁned as f (T x) = f (x) (1)
According to this formulation, applying a viewpoint transformation T on the input image x, does not change the outcome of the network activations.
However, a more desirable property would be to capture and retain the transformation T applied to the input image x, thus obtaining a network that is aware of the different trans-formations applied to the input. Being able to model net-work activations that change in a structured way according to the input viewpoint transformations is also called view-point equivariance and it is deﬁned as: f (T x) = T f (x). (2)
This is achieved by introducing capsules: groups of neu-rons that explicitly encode the intrinsic viewpoint-invariant relationship existing between different parts of the same ob-ject. Capsule networks (CapsNets) can learn part-whole re-lationships between so-called entities across different view-points [12, 26, 13], similarly to how our visual cortex sys-tem operates, according to the recognition-by-components theory [1]. Unlike traditional CNNs, which usually re-tain viewpoint invariance, capsule networks can explicitly model and jointly preserve a viewpoint transformation T through the network activations, achieving viewpoint equiv-ariance (Eq. 2).
Developing viewpoint-equivariant methods for 3D HPE networks leads to multiple advantages: (i) the learned model is more robust, interpretable, and suitable for real-world applications, (ii) the viewpoint is treated as a learn-able parameter, allowing to disentangle the 3D data of the skeleton from each speciﬁc view, (iii) the same annotated data can be used to train a network for different viewpoints, thus less training data is required.
In this work, we address the problem of viewpoint-equivariant human pose estimation from single depth or
RGB images. Our contribution is summarised as follows:
• We present a novel Deep viewpoint-Equivariant
Capsule Autoencoder architecture (DECA) which jointly addresses multiple tasks, such as 3D and 2D human pose estimation.
• We show how our network works with limited training data, no data augmentation, and across different input domains (RGB and depth images).
• We show how the feature space organization, deﬁned by routing the input information to build capsule enti-ties, improves when the tasks are jointly addressed.
• We evaluate our method on the ITOP [9] dataset for the depth domain and on the PanopTOP31K [5] dataset for the RGB domain. We establish a new baseline for the viewpoint transfer task and in the RGB domain. 2.