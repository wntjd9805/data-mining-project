Abstract
The generalization capability of deep neural networks has been substantially improved by applying a wide spec-trum of regularization methods, e.g., restricting function space, injecting randomness during training, augmenting data, etc. In this work, we propose a simple yet effective regularization method named progressive self-knowledge distillation (PS-KD), which progressively distills a model’s own knowledge to soften hard targets (i.e., one-hot vec-tors) during training. Hence, it can be interpreted within a framework of knowledge distillation as a student becomes a teacher itself. Specifically, targets are adjusted adaptively by combining the ground-truth and past predictions from the model itself. We show that PS-KD provides an effect of hard example mining by rescaling gradients according to difficulty in classifying examples. The proposed method is applicable to any supervised learning tasks with hard tar-gets and can be easily combined with existing regularization methods to further enhance the generalization performance.
Furthermore, it is confirmed that PS-KD achieves not only better accuracy, but also provides high quality of confidence estimates in terms of calibration as well as ordinal rank-ing. Extensive experimental results on three different tasks, image classification, object detection, and machine trans-lation, demonstrate that our method consistently improves the performance of the state-of-the-art baselines. The code is available at https://github.com/lgcnsai/PS-KD-Pytorch. 1.

Introduction
The recent progress made in deep neural networks (DNNs) has significantly improved performance in various tasks related to computer vision as well as natural language processing, e.g., image classification [14, 18, 22, 37], object detection / segmentation [13, 32], machine translation [41] and language modeling [20]. Scaling up of DNN is widely
*Corresponding author adopted as a promising strategy to achieve higher perfor-mances [12, 14, 39]. However, deeper networks require a large number of model parameters that need to be learned, which could make models more prone to overfitting. Thus,
DNNs typically produce overconfident predictions even for incorrect predictions, and this is because the predictions are highly miscalibrated [10, 26].
To improve generalization performance and training ef-ficiency of DNNs, a number of regularization methods have been proposed. The widely employed methods in practice include: L1- and L2-weight decay [23, 27] to restrict the function space, dropout [36] to inject randomness during training, batch normalization [19, 33] to accelerate train-ing speed by normalizing internal activations in every layer.
There also have been several methods that are specifically designed for a particular task. For example, advanced data augmentation techniques that are specific to computer vi-sion tasks such as Cutout [5], Mixup [51], AugMix [16] and
CutMix [49] have shown to boost classification accuracy and also improve robustness and uncertainty of a model.
Another effective regularization method is to adjust the tar-gets when they are given in the form of one-hot coded vec-tors (i.e., hard targets), including label smoothing (LS) [38], label perturbation [43], etc.
Among those methods about adjusting targets, LS [38] has been widely applied to many applications [31, 41, 53] and has shown to improve generalization performance as well as the quality of confidence estimates (in terms of cal-ibration) on image classification and machine translation tasks [25]. LS softens a hard target as a smoothed dis-tribution by assigning a small amount of probability mass to non-target classes. However, it is also empirically con-firmed that it is not complementary to current advanced reg-ularization techniques. For example, if we utilize LS and
CutMix simultaneously for image classification, the perfor-mance on both classification and confidence estimation is substantially degraded [3].
One natural question raised on LS could be: is there a more effective strategy to soften hard targets so as to obtain
Figure 1. A schematic of PS-KD. At epoch t, a student at epoch (t − 1) becomes a teacher and a model at epoch t is trained with the soft targets computed as a linear combination of hard targets and the predictions from the teacher. more informative labels? To answer this question, we pro-pose a simple regularization technique named progressive self-knowledge distillation (PS-KD) that distills the knowl-edge in a model itself and uses it for training the model. It means that a student model becomes a teacher model itself, which gradually utilizes its own knowledge for softening hard targets to be more informative during training. Specif-ically, the model is trained with the soft targets which are computed as a linear combination of the hard targets and the past predictions at a certain epoch, which are adjusted adaptively as training proceeds.
To justify our proposed method, we have shown that PS-KD gives more weights to hard-to-learn examples by a gra-dient rescaling scheme during training, which clearly re-veals that a student model can be enhanced even if trained with a teacher worse than the student (e.g., past predictions).
The proposed method is easy to implement, can be applied to any supervised learning tasks where the hard targets are given as the ground-truth labels. Moreover, it can be easily combined with current advanced regularization techniques, thereby enhancing further their generalization performance.
With this simple method, the generalization ability of DNNs can be greatly improved regarding the target metrics (e.g., accuracy) as well as the quality of confidence estimates in terms of both calibration and ordinal ranking (i.e., misclas-sification detection) [9, 24].
To rigorously evaluate the advantages of PS-KD, we con-duct extensive experiments on diverse tasks with popular benchmark datasets: image classification on CIFAR-100 and ImageNet, object detection on PASCAL VOC, and ma-chine translation on IWSLT15 and Multi30k. The exper-imental results demonstrate that training with PS-KD fur-ther enhances the generalization performance of the state-of-the-art baselines. For image classification on CIFAR-100 and ImageNet, our results show that the model trained with
PS-KD provides not only better predictive performance, but also high quality of confidence estimates. We further con-firm that the advanced image augmentation techniques such as Cutout and CutMix also benefit from PS-KD. From the evaluation on object detection with PASCAL VOC, it is ob-served that PS-KD makes a model learn better representa-tions compared to the existing approaches. To show the wide applicability of PS-KD, we also conduct the experi-ments on machine translation, which improve BLEU scores of baselines significantly. 2.