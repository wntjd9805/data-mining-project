Abstract
Is a geometric model required to synthesize novel views from a single image? Being bound to local convolutions,
CNNs need explicit 3D biases to model geometric transfor-mations. In contrast, we demonstrate that a transformer-based model can synthesize entirely novel views without any hand-engineered 3D biases. This is achieved by (i) a global attention mechanism for implicitly learning long-range 3D correspondences between source and target views, and (ii) a probabilistic formulation necessary to capture the ambi-guity inherent in predicting novel views from a single image, thereby overcoming the limitations of previous approaches that are restricted to relatively small viewpoint changes. We evaluate various ways to integrate 3D priors into a trans-former architecture. However, our experiments show that no such geometric priors are required and that the trans-former is capable of implicitly learning 3D relationships between images. Furthermore, this approach outperforms the state of the art in terms of visual quality while covering the full distribution of possible realizations. 1.

Introduction
Imagine looking through an open doorway. Most of the room on the other side is invisible. Nevertheless, we can es-timate how the room likely looks. The few visible features enable an informed guess about the height of the ceiling, the position of walls and lighting etc. Given this limited infor-mation, we can then imagine several plausible realizations of the room on the other side. This 3D geometric reasoning and the ability to predict what the world will look like before we move is critical to orient ourselves in a world with three spatial dimensions. Therefore, we address the problem of novel view synthesis (NVS) [35, 23, 9] based on a single initial image and a desired change in viewpoint. In partic-ular, we aim at specifically modeling large camera trans-formations, e.g. rotating the camera by 90â—¦ and looking at previously unseen scenery. As this is an underdetermined problem, we present a probabilistic generative model that learns the distribution of possible target images and synthe-sizes them at high fidelity. Solving this task has the potential to transform the passive experience of viewing images into an interactive, 3D exploration of the depicted scene. This requires an approach that both understands the geometry of the scene and, when rendering novel views of an input, con-siders their semantic relationships to the visible content.
Interpolation vs. Extrapolation Recently, impressive syn-thesis results have been obtained with geometry-focused ap-proaches in the multi-view setting [54, 55, 44], where not just a single but a large number of images or a video of a scene are available such that the task is closer to a view in-terpolation than a synthesis of genuinely novel views. In contrast, if only a single image is available, the synthesis of novel views is always an extrapolation task. Solving this task is appealing because it allows a 3D exploration of a scene starting from only a single picture.
While existing approaches for single-view synthesis make small camera transformations, such as a rotation by a few
degrees, possible, we aim at expanding the possible camera changes to include large transformations. The latter neces-sitates a probabilistic framework: Especially when apply-ing large transformation, the problem is underdetermined because there are many possible target images which are consistent with the source image and camera pose. This task cannot be solved with a reconstruction objective alone, as it will either lead to averaging, and hence blurry synthe-sis results, or, when combined with an adversarial objec-tive, cause a significant mode-dropping when modeling the target distribution. To remedy these issues, we propose to model this task with a powerful, autoregressive transformer, trained to maximize the likelihood of the target data.
Explicit vs. Implicit Geometry The success of transform-ers is often attributed to the fact that they enforce less in-ductive biases compared to convolutional neural networks (CNNs), which are biased towards local context. Relying mainly on CNNs, this locality-bias required previous ap-proaches for NVS to explicitly model the overall geometric transformation, thereby enforcing yet another inductive bias regarding the three dimensional structure. In contrast, by modeling interactions between far-flung regions of source and target images, transformers have the potential to learn to represent the required geometric transformation implicitly without requiring such hand engineered operations. This raises the question whether it is at all necessary to explic-itly include such biases in a transformer model. To address this question, we perform several experiments with varying degrees of inductive bias and find that our autoregressively trained transformer model is indeed capable of learning this transformation completely without built-in priors and can even learn to predict depth in an unsupervised fashion.
To summarize our contributions, we (i) propose to learn a probabilistic model for single view synthesis that prop-erly takes into account the uncertainties inherent in the task and show that this leads to significant benefits over previ-ous state-of-the-art approaches when modeling large cam-era transformations; see Fig. 1. We (ii) also analyze the need for explicit 3D inductive biases in transformer archi-tectures for the task of NVS with large viewpoint changes and find that transformers make it obsolete to explicitly code 3D transformations into the model and instead can learn the required transformation implicitly themselves. We also (iii) find that the benefits of providing them geometric information in the form of explicit depth maps are relatively small, and investigate the ability to recover an explicit depth representation from the layers of a transformer which has learned to represent the geometric transformation implicitly and without any depth supervision. 2.