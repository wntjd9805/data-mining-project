Abstract 1.

Introduction
While neural symbolic methods demonstrate impressive performance in visual question answering on synthetic im-ages, their performance suffers on real images. We iden-tify that the long-tail distribution of visual concepts and un-equal importance of reasoning steps in real data are the two key obstacles that limit the models’ real-world potentials.
To address these challenges, we propose a new paradigm,
Calibrating Concepts and Operations (CCO), which en-ables neural symbolic models to capture underlying data characteristics and to reason with hierarchical importance.
Specifically, we introduce an executor with learnable con-cept embedding magnitudes for handling distribution im-balance, and an operation calibrator for highlighting im-portant operations and suppressing redundant ones.
Our experiments show CCO substantially boosts the per-formance of neural symbolic methods on real images. By evaluating models on the real world dataset GQA, CCO helps the neural symbolic method NSCL outperforms its vanilla counterpart by 9.1% (from 47.0% to 56.1%); this re-sult also largely reduces the performance gap between sym-bolic and non-symbolic methods. Additionally, we create a perturbed test set for better understanding and analyzing model performance on real images. Code is available at https://lizw14.github.io/project/ccosr.
Visual question answering (VQA) aims to develop a model that can answer open-ended questions from images.
Currently, end-to-end methods, which directly make predic-tions over dense visual and textual features [37, 19], repre-sent the most effective class of models for VQA. Nonethe-less, such methods have been criticized for exploiting short-cuts (e.g., statistical dataset bias [1, 11], question prior [2] or isolated text and image elements [25]) to answer ques-tions; these shortcuts often make them unable to generalize well on out-of-domain data.
In contrast, neural symbolic methods [5, 18, 38, 26] are equipped with strong reasoning ability, enabling them to an-swer multi-hop and complex questions in a compositional and transparent manner—they first parse each question into a program with a series of reasoning steps, and then com-pose neural modules on the fly to execute the program on the image. While symbolic methods achieve nearly perfect performance on synthetic dataset, they perform poorly on real-world datasets. For instance, neural symbolic concept learner (NSCL) [26] achieves 98.9% accuracy on the syn-thetic CLEVR dataset [17], but only 47.0% accuracy on the real-world GQA dataset [15]. Note the original NSCL can-not be directly applied to GQA; this 47.0% accuracy is ob-tained from our own re-implementation, where minimal but
necessary modifications are made (e.g., adding in same and common modules), for making models runnable on GQA.
As summarized in Figure 1, we note there are two ma-jor differences between synthetic datasets and real-world datasets. First, while visual concepts are well-balanced in the synthetic datasets, they follow a long-tail distribution in real-world datasets. For example, as shown in Figure 1(a), in GQA, common concepts like “man”, “window”,
“black”, “white” are far more frequent than uncommon ones like “pink” and “eraser”, in both questions and an-swers. Second, unlike in synthetic data, the reasoning steps on real data have varying importance, mainly because of redundancy/over-specification in question description. For example, as shown in Figure 1(b), in the question ”What is the little boy doing?”, the noun (i.e., boy) itself is enough to select the person being asked about while the adjective (i.e., little) only serves as a nuisance factor.
We identify that this mismatch of dataset characteristics is the main obstacle for adapting neural symbolic methods from synthetic datasets to real-world datasets. More con-cretely, we find that the original architecture designs of neu-ral symbolic methods (which were designed/verified mainly on synthetic datasets) are no longer suitable for the real-world setting. For examples, as shown in Section 3, even simple operations like removing the normalization on con-cept embeddings or manually assigning larger weights to less discriminative modules are effective to improve the per-formance of neural symbolic methods on real images.
To better cope with real images, we propose Calibrat-ing Concepts and Operations (CCO), which enables neural symbolic methods to explicitly learn weights for concept embedding and reason with contextual module importance.
Specifically, CCO learns different concept embedding mag-nitudes for each execution module, and learns an operation weight predictor to contextually predict weights for each operation in the reasoning program. In this way, the model will be able to handle unbalanced concept distributions and to reason with varying operation importance.
Our empirical results show that CCO substantially boosts the applicability of neural symbolic methods on real images. For example, on the real-world GQA dataset,
CCO outperforms the baseline NSCL by a large margin of 9.1% (from 47.0% to 56.1%). Moreover, the proposed
CCO largely reduces the performance gap between the sym-bolic method and the state-of-the-art non-symbolic methods
[32, 16] on real-world GQA dataset.
Additionally, based on the proposed operation weight calibrator, we create a perturbed test set by progressively re-moving the operations with low weights from testing ques-tions. Our purpose is to verify whether the learned operation weights are able to highlight important operations and sup-press redundant ones, and simultaneously to access the ro-bustness of different models regarding this operation infor-mation erasing. Our analysis reveals 1) GQA questions con-tain superfluous information by way of over-specification and 2) the ability to effectively handle this extraneous infor-mation is crucial for models to improve performance. We hope this perturbed test set can facilitate researchers to bet-ter understand the compositionality of VQA questions and to further improve symbolic reasoning over real images. 2.