Abstract
Can a user create a deep generative model by sketch-ing a single example? Traditionally, creating a GAN model has required the collection of a large-scale dataset of ex-emplars and specialized knowledge in deep learning. In contrast, sketching is possibly the most universally accessi-ble way to convey a visual concept. In this work, we present a method, GAN Sketching, for rewriting GANs with one or more sketches, to make GANs training easier for novice users. In particular, we change the weights of an original
GAN model according to user sketches. We encourage the model’s output to match the user sketches through a cross-domain adversarial loss. Furthermore, we explore different regularization methods to preserve the original model’s di-versity and image quality. Experiments have shown that our method can mold GANs to match shapes and poses speciﬁed by sketches while maintaining realism and diversity. Finally, we demonstrate a few applications of the resulting GAN, including latent space interpolation and image editing. 1.

Introduction
The power and promise of deep generative models such as
GANs [19] lie in their ability to synthesize endless realistic, diverse, and novel content with minimal user effort. The potential utility of these models continues to grow thanks to the increased quality and resolution of large-scale generative models [30, 6, 49, 47] in recent years.
Nonetheless, the training of high-quality generative mod-els demands high-performance computing platforms, putting the process out of reach for most users. Furthermore, training 1
a high-quality model requires expensive large-scale data col-lection and careful pre-processing. Commonly used datasets such as ImageNet [12] and LSUN [64] require human an-notation and manual ﬁltering. The specialized FFHQ Face dataset [29] requires delicate face alignment and super-resolution pre-processing. Moreover, the technical effort is not trivial: developing an advanced generative model re-quires the domain knowledge [52, 30] of a team of experts, who often invest months or years into a single model on speciﬁc datasets.
This leads to the question: how can an ordinary user cre-ate their own generative model? A user creating artwork with cats might not want a generic model of cats, but a bespoke model of special cats in a particular desired pose: nearby, reclining, or all looking left. To obtain such a cus-tomized model, must the user curate thousands of reclining left-looking cat images and then ﬁnd an expert to invest months of time in model training and parameter tuning?
In this paper, we propose the task of creating a gener-ative model from just a handful of hand-drawn sketches.
Ever since Ivan Sutherland’s SketchPad [57], computer sci-entists have recognized the usefulness of guiding computer-generated content using sketching interface. This tradition has continued in the area of sketch-based image synthesis and 3D modeling [26, 10, 27]. But rather than creating a single image or a 3D shape from a sketch, we wish to under-stand if it is possible to create a generative model of realistic images from hand-drawn sketches. Unlike sketch-based con-tent creation, where both the input and output are 2D or 3D visual data, in our case, the input is a 2D sketch and the output is a network with millions of opaque parameters that control algorithm behavior to make images. We ask: with such a different output domain, which parameters shall we update, and how? How do we know whether the model’s output will resemble the user sketch?
In this paper, we aim to answer the above questions by developing a method to tailor a generative model to a small number of sketch exemplars provided by the user. To achieve this, we take advantage of off-the-shelf generative models pre-trained on large-scale data, and devise an approach to ad-just a subset of the model weights to match the user sketches.
We present a new cross-domain model ﬁne-tuning method that encourages the new model to create images that resem-ble a user sketch, while preserving the color, texture, and background context of the original model. As shown in Fig-ure 1, our method can change the object pose and zoom in cat faces with only four hand-drawn sketches.
We use our method to create several new customized
GAN models, and we show that these modiﬁed models can be used for several applications such as generating new sam-ples, interpolating between two generated images, as well as editing a natural photograph. Our method requires mini-mal user input. Instead of collecting a new dataset through manual ﬁltering and image alignment, a user only needs to provide one or a few exemplar sketches for our method to work effectively. Finally, we benchmark our method to fully characterize its performance. Code and models are also available on our webpage. 2.