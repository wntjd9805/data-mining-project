Abstract
Crowd counting is a difﬁcult task because of the diver-sity of scenes. Most of the existing crowd counting meth-ods adopt complex structures with massive backbones to enhance the generalization ability. Unfortunately, the per-formance of existing methods on large-scale data sets is not satisfactory. In order to handle various scenarios with less complex network, we explored how to efﬁciently use the multi-expert model for crowd counting tasks. We mainly fo-cus on how to train more efﬁcient expert networks and how to choose the most suitable expert. Speciﬁcally, we pro-pose a task-driven similarity metric based on sample’s mu-tual enhancement, referred as co-ﬁne-tune similarity, which can ﬁnd a more efﬁcient subset of data for training the ex-pert network. Similar samples are considered as a clus-ter which is used to obtain parameters of an expert. Be-sides, to make better use of the proposed method, we design a simple network called FPN with Deconvolution Count-ing Network, which is a more suitable base model for the multi-expert counting network. Experimental results show that multiple experts FDC (MFDC) achieves the best per-formance on four public data sets, including the large scale
NWPU-Crowd data set. Furthermore, the MFDC trained on an extensive dense crowd data set can generalize well on the other data sets without extra training or ﬁne-tuning.1 1.

Introduction
Crowd counting is a task that tries to estimate the number of objects in an image, such as people, cars or animals. This
*Corresponding author 1Code will be available at https://github.com/streamer-AP
Figure 1. The stability of the category pseudo-labels during 5 epochs of alternate training. The ﬁrst row is result of our method, and the second row is result of 8-way divide and grow training method from IG-CNN[16]. task has attracted signiﬁcant attention because of its various scenarios application, such as security surveillance, human trafﬁc control, hot spot discovery etc.
Earlier data sets such as UCSD data set [2], Mall data set [5], World Expo’10 data set [30] contain low density crowd image with balanced number[19]. Previous meth-ods have achieved reliable performance on such data set
[28, 29, 19, 22]. However, when it comes to more com-plex scenarios[32, 1, 11], the performance of existing meth-ods will be signiﬁcantly humiliated. This is because un-der these scenarios, there is large diversity of visual ap-pearances because of distorted perspectives, variable scales, unbalanced distributions and wide range of brightness, etc.
Consequently, single network based methods [12, 15, 17] may perform well in one special scenes, but worse on oth-ers.
To deal with complex scenarios, some methods [16, 12, 17, 30] explored multi-expert structure, which usually
includes a base feature extractors, a routing network or weighting network and multiple experts networks. Each expert was designed to handle a speciﬁc scale or density, alleviating the complexity of dealing with the diversity of scenes with only one expert. The router, essentially a clas-siﬁer, selects the optimal expert for every testing sample.
To obtain effective experts, most of multi-expert count-ing methods [17, 16] applied a differential training technol-ogy. This technology only backwards the loss on the ex-pert which gives the most accurate prediction of the current sample. After training, each training sample is assigned a pseudo class label, which is the index of the most suitable experts, to train the router. Existing experimental results in [16] show that the router’s accuracy in assigning sam-ples to the optimal expert network is not high. Moreover, as shown in Figure 1, the generated pseudo labels are unsta-ble and hard to converge during differential training. This means after training, the most suitable expert for many sam-ples is the expert that didn’t use this sample to train. This goes against the original intention of the algorithm design, that is, the expert network trained with a speciﬁc set will be more suitable for the samples in this set. This phenomenon demonstrates that samples on which an expert performs the best may be not suitable for training this expert together to further boost the performance. From Figure 1, it can be seen that the category pseudo-labels obtained by our method are more stable. This means that expert networks trained us-ing our method are indeed more suitable for the training the samples used by these networks.
Actually, since each expert is generated through ﬁne-tuning the model on a subset of the training data, the key problem is how to divide the training set into several sub-sets. A good subset should bring performance improvement when ﬁne-tune the model on it. However, it is difﬁcult to divide the entire data set directly. We considered a easier problem, i.e. how to evaluate the similarity between two samples. We propose a novel metric to evaluate the simi-larity between the samples ( referred co-ﬁne-tune similar-ity), which can reﬂect the correlation between the parame-ters of their optimal experts. This similarity can approxi-mately describe the model’s performance improvement af-ter ﬁne-tuning on the subset containing these two samples.
Therefore, if a cluster is composed of similar samples, it is conducive to optimize the base model in a consistent direc-tion, generating a effective expert for all the samples in this subset. To obtain this kind of cluster, based on co-ﬁne-tune similarity, we design a simple clustering method to ﬁnd po-tential clusters in dense crowd data sets. Then each cluster is used to obtain the parameters of an expert. In order to select the optimal weights for testing data during the infer-ence, we consider each cluster as a class and train a CNN classiﬁer as router, which predicts the class label for a test-ing data. The prediction result is used to retrieve the optimal expert. In this way, we can dynamically select a suitable expert for the test image according to its characteristics and improve the performance greatly.
Furthermore, to reduce the storage space of the param-eters and avoid over-ﬁtting on clusters, we design a sim-ple yet efﬁcient crowd counting model (referred to as FDC) which has a tiny density map regressor. Using FDC as a base model, we obtain an multi-expert FDC ( referred to as an MFDC) with our training strategy.
Our main contributions can be summarised as:
• We proposed a novel multi-experts training frame-work for crowd counting task, which exploits relations within samples. The proposed pipeline can be inte-grated with existing methods and improve their perfor-mance signiﬁcantly.
• To obtain multiple representative weights, we develop an effective take-driven similarity and a clustering method to obtain multiple clusters of the training data.
Each cluster is used to learn a set of parameters, which is effective for testing samples similar to this cluster.
• Extensive experiments are conducted on four data sets, namely, STA, STB[32], UCF-QNRF [11], and NWPU-Crowd [23], to demonstrate that the proposed method can achieve the state-of-the-art performance. 2.