Abstract
Benchmark datasets that measure camera pose accu-racy have driven progress in visual re-localisation research.
To obtain poses for thousands of images, it is common to use a reference algorithm to generate pseudo ground truth. Popular choices include Structure-from-Motion (SfM) and Simultaneous-Localisation-and-Mapping (SLAM) us-ing additional sensors like depth cameras if available.
Re-localisation benchmarks thus measure how well each method replicates the results of the reference algorithm.
This begs the question whether the choice of the reference algorithm favours a certain family of re-localisation meth-ods. This paper analyzes two widely used re-localisation datasets and shows that evaluation outcomes indeed vary with the choice of the reference algorithm. We thus ques-tion common beliefs in the re-localisation literature, namely that learning-based scene coordinate regression outper-forms classical feature-based methods, and that RGB-D-based methods outperform RGB-based methods. We argue that any claims on ranking re-localisation methods should take the type of the reference algorithm, and the similarity of the methods to the reference algorithm, into account. 1.

Introduction
The availability of benchmark datasets [14,34,38,39,43, 56, 64, 69, 72, 74, 77, 82] has been a driving factor for re-search on visual re-localisation, a core technology to make autonomous robots [40], self-driving cars [28], and aug-mented / mixed reality (AR / MR) systems [1, 10, 42] a re-ality. These benchmarks provide camera poses for a set of training and test images. The training images can be used to create a scene representation, and the test images serve as queries to determine the 3D position and 3D orientation (6DoF pose) of the camera with respect to the scene. Due to the challenge of jointly estimating the poses of thousands or more images, benchmark datasets are typically gener-ated by a reference algorithm such as SfM or (RGB-)D
SLAM [34, 38, 39, 64, 74]. As such, benchmarks measure how well visual re-localisation methods are able to repli-cate the results of the reference algorithm.
Ideally, the choice of reference algorithm should not
Figure 1. Visualisation of the same scene reconstructed by two different reference algorithms. We show the trajectories of test images estimated by state-of-the-art visual localisation algorithms (left) and the percentage of images localised within 1cm and 1◦ of error w.r.t. the pseudo ground truth (right, higher is better).
While the underlying 3D scene models are very similar, the rel-ative ranking of methods is inverted. In this paper, we show that the reference algorithm used to create the pseudo ground truth has a signiﬁcant impact on which method achieves the best results. matter as long as it faithfully estimates the camera poses of the training and test images. In particular, the choice of reference algorithm should not affect the ranking of meth-ods on a benchmark. In practice however, different refer-ence algorithms optimise different cost functions, e.g., re-projection errors of sparse point clouds for SfM [60, 80] or alignment errors in 3D space for depth-based SLAM meth-ods [16, 30, 47, 62], leading to different local minima. We ask to what degree the choice of reference algorithm im-pacts the ranking of methods on a benchmark. This is an important question as it pertains to whether or not we can draw absolute conclusions, e.g., algorithm A is better than algorithm B or using component C improves accuracy. In-terestingly, to the best of our knowledge, this question has not received much attention in the re-localisation literature.
The main focus of this paper is to investigate how
the choice of reference algorithms impacts the measured performance of visual re-localisation algorithms.
To this end, we compare two types of reference algorithms (depth-based SLAM and SfM) on two popular benchmark datasets [64, 74]. Detailed experiments with state-of-the-art re-localisation algorithms show that the choice of refer-ence algorithm can have a profound impact on the ranking of methods. In particular, as illustrated in Fig. 1, we show that depending on the reference algorithm, a modern end-to-end-trainable approach [7] either outperforms or is outper-formed by a classical, nearly 10 year-old baseline [54, 55].
Similarly, the choice of whether to use depth maps or SfM point clouds to represent a scene can improve or decrease performance depending on the reference algorithm. Our re-sults show that we as a community should be careful when drawing conclusions from existing benchmarks.
Instead, it is necessary to take into account that certain approaches more closely resemble the reference algorithm than others.
The former are better able to replicate imperfections in a reference algorithm’s pseudo ground truth (pGT). This nat-ural advantage should be discussed when evaluating locali-sation results and designing new benchmarks.
In detail, this paper makes the following contributions: 1) we show that the choice of a reference algorithm for ob-taining pGT poses can have a signiﬁcant impact on the rel-ative ranking of methods, to the extend that the rankings of methods can be (nearly) completely reversed. This im-plies that published results for visual re-localisation should always be considered under the aspect of which algorithm was used to create the pGT. 2) we provide a comparison of pGT generated by RGB-only SfM and (RGB-)D SLAM on the 7Scenes [64] and 12Scenes [74] datasets, which are widely used [2–6, 8, 11, 12, 26, 33, 34, 64, 74–76]. We show that none is clearly su-perior than the other. We show that commonly accepted re-sults from the literature (RGB-D variants of re-localisation methods outperform their RGB-only counterparts; scene coordinate regression is more accurate than feature-based methods) are not absolute but depending on the pGT. 3) we are not aware of prior work aimed at evaluating the extent to which conclusions about localisation performance can be drawn from existing benchmarks. As such, this paper is the ﬁrst to raise awareness that the limitations of the pGT for re-localisation need to be discussed in order to make valid comparisons across methods.
Our new pGT and our evaluation pipeline are available at github.com/tsattler/visloc pseudo gt limitations/. 2.