Abstract
Growing at a fast pace, modern autonomous systems will soon be deployed at scale, opening up the possibility for cooperative multi-agent systems. Sharing information and distributing workloads allow autonomous agents to better perform tasks and increase computation efﬁciency. How-ever, shared information can be modiﬁed to execute adver-sarial attacks on deep learning models that are widely em-ployed in modern systems. Thus, we aim to study the robust-ness of such systems and focus on exploring adversarial at-tacks in a novel multi-agent setting where communication is done through sharing learned intermediate representations of neural networks. We observe that an indistinguishable adversarial message can severely degrade performance, but becomes weaker as the number of benign agents increases.
Furthermore, we show that black-box transfer attacks are more difﬁcult in this setting when compared to directly per-turbing the inputs, as it is necessary to align the distribution of learned representations with domain adaptation. Our work studies robustness at the neural network level to con-tribute an additional layer of fault tolerance to modern se-curity protocols for more secure multi-agent systems. 1.

Introduction
With rapid improvements of modern autonomous sys-tems, it is only a matter of time until they are deployed at scale, opening up the possibility of cooperative multi-agent systems. Individual agents can beneﬁt greatly from shared information to better perform their tasks [26, 59]. For ex-ample, by aggregating sensory information from multiple viewpoints, a ﬂeet of vehicles can perceive the world more clearly, providing signiﬁcant safety beneﬁts [52]. More-over, in a network of connected devices, distributed pro-cessing across multiple agents can improve computation ef-*Equal contribution.
Work done while all authors were at UberATG.
ﬁciency [18]. While cooperative multi-agent systems are promising, relying on communication between agents can pose security threats as shared information can be malicious or unreliable [54, 3, 37].
Meanwhile, modern autonomous systems typically rely on deep neural networks known to be vulnerable to adver-sarial attacks. Such attacks craft small and imperceivable perturbations to drastically change a neural network’s be-havior and induce false outputs [48, 21, 8, 30]. Even if an at-tacker has the freedom to send any message, such small per-turbations may be the most dangerous as they are indistin-guishable from their benign counterparts, making corrupted messages difﬁcult to detect while still highly malicious.
While modern cyber security algorithms provide ade-quate protection against communication breaches, adversar-ial robustness of multi-agent deep learning models has yet to be studied. Meanwhile, when it comes to safety-critical applications like self-driving, additional layers of redun-dancy and improved security are always welcome. Thus, by studying adversarial robustness, we can enhance modern security protocols by introducing an additional layer of fault tolerance at the neural network level.
Adversarial attacks have been studied extensively but ex-isting approaches mostly consider attacks on input domains like images [48, 21], point clouds [7, 50], and text [44, 14].
On the other hand, multi-agent systems often distribute computation across different devices and transmit inter-mediate representations instead of input sensory informa-tion [52, 18]. Speciﬁcally, when deep learning inference is distributed across different devices, agents will communi-cate by transmitting feature maps, which are activations of intermediate neural network layers. Such learned commu-nication has been shown to be superior due to transmitting compact but expressive messages [52] as well as efﬁciently distributing computation [18].
In this paper, we investigate adversarial attacks in this novel multi-agent setting where perturbations are applied to learned intermediate representations. An illustration is
Figure 1. Overview of a multi-agent setting with one malicious agent (red). Here the malicious agent attempts to sabotage a victim agent by sending an adversarial message. The adversarial message is indistinguishable from the original, making the attack difﬁcult to detect. shown in Figure 1. We conduct experiments and showcase vulnerabilities in two highly practical settings: multi-view perception from images in a ﬂeet of drones and multi-view perception from LiDAR in a ﬂeet of self-driving vehicles (SDVs). By leveraging information from multiple view-points, these multi-agent systems are able to signiﬁcantly outperform those that do not exploit communication.
We show, however, that perturbed transmissions which are indistinguishable from the original can severely degrade the performance of receivers particularly as the ratio of ma-licious to benign agents increases. With only a single at-tacker, as the number of benign agents increase, attacks be-come signiﬁcantly weaker as aggregating more messages decreases the inﬂuence of malicious messages. When mul-tiple attackers are present, they can coordinate and jointly optimize their perturbations to strengthen the attack.
In terms of defense, when the threat model is known, adver-sarial training is highly effective, and adversarially trained models can defend against perturbations almost perfectly and even slightly enhance performance on natural exam-ples. Without knowledge of the threat model, we can still achieve reasonable adversarial robustness by designing more robust message aggregation modules.
We then move on to more practical attacks in a black box setting where the model is unknown to the adversary. Since query-based black box attacks need to excessively query a target model that is often unaccessible, we focus on query-free transfer attacks that are more feasible in practice. How-ever, transfer attacks are much more difﬁcult to execute at the feature-level than on input domains. In particular, since perturbation domains are model dependent, vanilla transfer attacks are ineffective because two neural networks with the same functionality can have very different intermediate rep-resentations. Here, we ﬁnd that training the surrogate model with domain adaptation is key to aligning the distribution of intermediate features and achieve much better transferabil-ity. To further enhance the practicality of attacks, we pro-pose to exploit the temporal consistency of sensory infor-mation processed by modern autonomous systems. When frames of sensory information are collected milliseconds apart, we can exploit the redundancy in adjacent frames to create efﬁcient, low-budget attacks in an online manner. 2.