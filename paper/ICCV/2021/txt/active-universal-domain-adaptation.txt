Abstract
Most unsupervised domain adaptation methods rely on rich prior knowledge about the source-target label set rela-tionship, and they cannot recognize categories beyond the source classes, which limits their applicability in practi-cal scenarios. This paper proposes a new paradigm for unsupervised domain adaptation, termed as Active Univer-sal Domain Adaptation (AUDA), which removes all label set assumptions and aims for not only recognizing target samples from source classes but also inferring those from target-private classes by using active learning to annotate a small budget of target data. For AUDA, it is challenging to jointly adapt the model to the target domain and select informative target samples for annotations under a large domain gap and significant semantic shift. To address the problems, we propose an Active Universal Adaptation Net-work (AUAN). Specifically, we first introduce Adversarial and Diverse Curriculum Learning (ADCL), which progres-sively aligns source and target domains to classify whether target samples are from source classes. Then, we pro-pose a Clustering Non-transferable Gradient Embedding (CNTGE) strategy, which utilizes the clues of transferabil-ity, diversity, and uncertainty to annotate target informative sample, making it possible to infer labels for target samples of target-private classes. Finally, we propose to jointly train
ADCL and CNTGE with target supervision to promote do-main adaptation and target-private class recognition. Ex-tensive experiments demonstrate that the proposed AUDA model equipped with ADCL and CNTGE achieves signifi-cant results on four popular benchmarks. 1.

Introduction
Recent advances in deep neural networks have convinc-ingly demonstrated the high capability of learning effec-tive models on large datasets. The impressive achievements heavily rely on quantities of labeled training instances,
Figure 1. Comparison between Active Universal Domain Adapta-tion and representative domain adaptation settings with respect to classification tasks and assumptions on target label set. AUDA re-moves all label set assumptions and aims for not only recognizing target samples belonging to the shared common label set but also inferring labels for those belong to target-private label set by using active learning to annotate a small budget of target data. which requires expensive and time-consuming labor work of collection and annotation. A reasonable question is why not directly recycling the off-the-shelf knowledge or models from a source domain to new domains. As data from differ-ent domains are sampled from different data distributions, there is probably a large domain gap [60] which may de-grade the model performance in the target domain [35, 40].
An appealing way to address this issue is Unsupervised Do-main Adaptation (UDA) [44], which aims to learn a classifi-cation model with source labeled data and target unlabeled data to ensure that the learned model could perform well in the target domain.
Most unsupervised domain adaptation methods can be divided into four categories, namely, closed set domain adaptation [25, 42, 47, 52, 57, 23], partial domain adapta-tion [4, 5, 6], open set domain adaptation [36, 46, 65, 28], and universal domain adaptation [61, 11, 45], as shown in the top of Figure 1. Specifically, closed set domain adap-tation [16, 30, 33] supposes that the source and target do-mains share the same label set. The partial domain adapta-tion [4, 64, 6] assumes that the source label set contains the target label set. The open set domain adaptation assumes that common classes between two domains are known [36] or the source label set is a subset of the target label set
[46]. Recently, Universal Domain Adaptation [61, 11, 45] removes all assumptions about source-target label set rela-tionship, and classifies target samples as labels contained in the source label set or marks them as “unknown” similar to the open set domain adaptation. However, the “unknown” category is still unknown, which is inapplicable for practical applications, e.g., new products recommendation or rare an-imal/plant recognition. Therefore, it is necessary for prac-tical domain adaptation algorithms to infer actual labels for samples belonging to the “unknown” category.
To achieve this goal, we propose to define a new paradigm for unsupervised domain adaptation, referred as
Active Universal Domain Adaptation (AUDA). As shown in the bottom of Figure 1, a labeled source domain and a tar-get domain without any explicit restrictions on the classes are provided for model training. Classes are defined as
“known” if they belong to the source label set. Otherwise, they are defined as “unknown”. Since target samples of “un-known” classes are much more difficult to recognize than the ones of “known” classes, AUDA algorithms need to draw knowledge from the source domain to firstly recognize the “known”/“unknown” label for the test samples from the target domain. Then, actual class labels should be inferred for both “known” and “unknown” samples. However, it is nearly impossible to infer labels for the “unknown” sam-ples without any labeled training data. Since practical ap-plications offer the possibility of annotating a small budget of target instances, termed as Active Learning (AL), we are motivated to acquire labels for a subset of target data from an oracle, especially, labels of target “unknown” samples, to assist the unknown category inference.
To design algorithms for active universal domain adap-tation, we are exposed to two aspects of technical chal-(1) Without any prior knowledge of the source-lenges: target label set relationship, there exist a large domain gap and significant semantic shift problems in AUDA. Specifi-cally, source and target data are sampled from different dis-tributions and the domain gap makes it hard to recognize
“known”/“unknown” instances in the target domain. More-over, the unexpectable semantic shift means that many un-known classes are contained in the target domain, making it extremely difficult to reduce the domain gap between the shared classes. If the domain gap and semantic shift cannot be well reduced, it is challenging for active learning to an-notate informative instances to infer target “unknown” in-(2) During active learning, the most informative stances. target instances should be annotated and used for learning to infer target “unknown” instances. Most existing AL ap-proaches prefer to annotate instances that are highly un-certain [10, 12, 24, 54] or diverse [49, 15]. As these ap-proaches perform active learning without considering do-main gap and semantic shift, uncertainty and diversity may be wrongly estimated [34]. Therefore, directly applying the traditional AL approaches easily lead to select outliers, re-dundant instances, or uninformative instances for annota-tion, which is detrimental for further reducing the domain gap and semantic shift, and damages the performance of in-ferring target “unknown” samples. Although the prior work in active domain adaptation [53] tries to deal with the prob-lem of domain gap, it dose not consider the semantic shift problem, making it inapplicable for AUDA. As a result, it is advisable to design active learning strategies that can an-notate the most informative target instances with the joint consideration of domain gap and semantic shift.
Motivated by the above observations, we propose an Ac-tive Universal Adaptation Network, which simultaneously adapts the model from the source domain to the target do-main, and performs active learning towards target informa-tive instances for unknown category inference. Specifically, we first propose Adversarial and Diverse Curriculum Learn-ing (ADCL), which designs an adversarial curriculum loss and a diverse curriculum loss to align source and target do-mains, and learn the ability of target “known”/“unknown” instances recognition1. Thus, the negative effects of do-main gap and semantic shift in active learning can be al-leviated, which helps to select more informative instances for annotation. Then, we propose an active learning strat-egy named Clustering Non-transferable Gradient Embed-ding (CNTGE), which utilizes the clues of transferabil-ity, diversity, and uncertainty to annotate target samples of target-private classes and assign pseudo labels to highly confident target “known” instances. The labeled and pseudo labeled target instances could provide better supervision for
ADCL, which helps to learn better curriculums. Finally, jointly training with ADCL and CNTGE could further rein-force the adaptation process, and learn to infer actual labels for target “unknown” instances.
The main contributions of this paper are: (1) We in-troduce a more practical unsupervised domain adaptation paradigm, Active Universal Domain Adaptation, which re-quires no assumptions about the target label set and aims for not only recognizing target samples belonging to the shared label set but also inferring those of target-private classes via active learning. (2) To address the AUDA task, we pro-pose Active Universal Adaptation Network, an end-to-end model, which performs adversarial and diverse curriculum learning and clustering non-transferable gradient embed-ding to cooperatively promote domain adaptation and active 1Here, we define target samples from the common label set as the tar-get “known” instances while others from target private label set are target
“unknown” instances.
(3) Extensive experiments demonstrate that the learning. proposed AUDA model equipped with ADCL and CNTGE achieves significant classification results. 2.