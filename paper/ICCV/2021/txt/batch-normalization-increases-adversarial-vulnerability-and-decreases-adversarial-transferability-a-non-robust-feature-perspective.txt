Abstract
Batch normalization (BN) has been widely used in mod-ern deep neural networks (DNNs) due to improved conver-gence. BN is observed to increase the model accuracy while at the cost of adversarial robustness. There is an increasing interest in the ML community to understand the impact of
BN on DNNs, especially related to the model robustness.
This work attempts to understand the impact of BN on DNNs from a non-robust feature perspective. Straightforwardly, the improved accuracy can be attributed to the better uti-lization of useful features. It remains unclear whether BN mainly favors learning robust features (RFs) or non-robust features (NRFs). Our work presents empirical evidence that supports that BN shifts a model towards being more depen-dent on NRFs. To facilitate the analysis of such a feature robustness shift, we propose a framework for disentangling robust usefulness into robustness and usefulness. Extensive analysis under the proposed framework yields valuable in-sight on the DNN behavior regarding robustness, e.g. DNNs first mainly learn RFs and then NRFs. The insight that RFs transfer better than NRFs, further inspires simple techniques to strengthen transfer-based black-box attacks. 1.

Introduction
Batch normalization (BN) [18] has been considered as a milestone technique in the development of deep neural net-works (DNNs) pushing the frontier in computer vision due to improved convergence. Numerous works have attempted to understand the impact of BN on DNNs from various perspec-tives. In contrast to previous works, investigating why (or how) BN helps the optimization [31, 1], our work focuses on the consequence of such enhanced optimization, especially on the model robustness. Our work is not the first one to study BN and robustness together. Most of the previous
*Equal contribution
Figure 1. Schematic of disentangling F usefulness and robustness with ball color representing robust usefulness, i.e. the darker, the more robustly useful. Ball size indicates usefulness while the red line divides RFs and NRFs. works are focusing on the covariate shift [3, 33, 43, 42]. For example, [3] adapts the BN statistics to improve the model robustness against common corruptions. On the contrary, our work studies BN by focusing on its impact on adversarial robustness from the non-robust feature perspective.
We evaluate the behavior of models with and w/o BN on multiple datasets in Table 1. As expected, BN improves the clean accuracy, i.e. accuracy on clean images. How-ever, this comes at the cost of lower robust accuracy, i.e. accuracy on adversarial images [35]. Straightforwardly, the
DNN can be seen as a set of useful features, consisting of robust features (RFs) and non-robust features (NRFs) [17], and the improved accuracy can be roughly interpreted as
BN facilitating utilization of more useful features. Yet, it remains unclear whether BN mainly favors learning RFs or
NRFs. Our empirical investigation shows that BN and other normalization variants all increase adversarial vulnerability in standard training, suggesting BN shifts the model to rely
more on NRFs than RFs for classification. Our claim is further corroborated by the analysis of corruption robustness and feature transferability.
With the above empirical evidence supporting our main claim that BN shifts the model towards being more depen-dent on NRFs, it is still necessary yet non-trivial to define and measure such a feature robustness shift. Inspired by [17], with a classifier DNN defined as a feature set F , we propose a framework, as shown in Figure 1, for disentangling F robust usefulness into F robustness and F usefulness. Fol-lowing [17], F usefulness and F robust usefulness can be measured by clean accuracy and robust accuracy, respec-tively. F usefulness can be seen as the amount of total useful features, indicated by the ball size and F robustness indi-cates the ratio of RFs to NRFs (see Figure 1). Conceptually,
F robustness is orthogonal to F usefulness. The core differ-ence between our feature analysis framework and that in [17] lies in the disentangled F robustness which can be utilized to measure how much BN shifts the model towards NRFs.
In practice, however, it is very difficult to directly measure
F robustness. Inspired by [26, 28] demonstrating a positive correlation between robustness and local linearity, we pro-pose a metric termed Local Input Gradient Similarity (LIGS) (see Sec. 4), measuring the local linearity of a DNN as an in-dication for F robustness. Admittedly, comparing the clean accuracy and robust accuracy also sheds some light on the
F robustness, however, they are heavily influenced by the dimension of usefulness. Measuring LIGS provides direct evidence on how BN influences the robustness of learned F , which facilitates analysis under the above framework.
Such analysis yields insight on the DNN behavior regard-ing robustness. On a normal dataset, introducing BN (or
IN/LN/GN) into the DNN consistently reduces F robustness, which naturally explains their induced lower robust accuracy.
We investigate and compare the behaviour of models trained on a dataset that mainly has either RFs or NRFs, which shows that NRFs are difficult to learn w/o BN, suggesting that BN is essential for learning NRFs. Further investigation on the dataset with RFs and NRFs cued for conflicting labels reveals that the model learns first RFs and then NRFs, and the previous learned RFs can be partially forgotten while the model learns NRFs in the later stage. The proposed frame-work is not limited for analyzing the impact of BN, and we also analyze other network structures and optimization factors. Interestingly, we find that most of them have no significant influence on F robustness indicated by the LIGS metric, leaving BN (and other normalization variants) among our investigated factors as the only one that have significant influence on the shift towards more NRFs. One practical use case of our key findings is to boost transferable attacks. We demonstrate that a substitute model w/o BN outperforms its counterpart with BN and that early-stopping the training of the substitute model can also boost transferable attacks.
Table 1. Comparison of models with and w/o BN on accuracy and robustness. [11] reports a similar phenomenon.
Network
Acc
PGD l2 PGD l∞ CW l2 CW l∞ 1/255 1/255 0.25 0.25 73.37
VGG16 (None) 71.59 15.55 6.04
VGG16 (BN)
VGG19 (None) 72.38 16.52 6.94
VGG19 (BN)
ResNet18 (None) 66.51 30.44
ResNet18 (BN) 70.50 16.79
ResNet50 (None) 71.60 28.00
ResNet50 (BN) 76.54 19.50 74.24
VGG11 (None) 95.42 63.91 96.27 51.22
VGG11 (BN)
VGG16 (None) 95.76 62.24 96.43 52.90
VGG16 (BN)
VGG11 (None) 90.06 51.30 92.48 39.31
VGG11 (BN)
VGG16 (None) 91.89 34.01 28.61
VGG16 (BN)
ResNet50 (None) 92.15 29.24 9.15
ResNet50 (BN) 95.6 93.7 t e
N e g a m
I
N
H
V
S 0 1
R
A
F
I
C 1.79 0.55 2.18 0.69 1.24 0.14 2.17 0.53 83.20 77.50 82.76 80.24 70.47 63.87 63.18 56.05 49.33 36.37 16.66 6.82 17.46 7.66 30.43 17.40 28.26 20.19 64.64 51.13 62.97 52.88 51.75 39.04 34.37 24.01 17.09 8.72 0.23 0.02 0.30 0.03 0.93 0.07 0.88 0.19 83.24 77.61 82.92 79.93 70.40 63.85 63.46 54.58 49.24 36.64 2.