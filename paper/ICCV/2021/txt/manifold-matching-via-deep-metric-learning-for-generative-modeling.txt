Abstract
We propose a manifold matching approach to genera-tive models which includes a distribution generator (or data generator) and a metric generator. In our framework, we view the real data set as some manifold embedded in a high-dimensional Euclidean space. The distribution generator aims at generating samples that follow some distribution condensed around the real data manifold.
It is achieved by matching two sets of points using their geometric shape descriptors, such as centroid and p-diameter, with learned distance metric; the metric generator utilizes both real data and generated samples to learn a distance metric which is close to some intrinsic geodesic distance on the real data manifold. The produced distance metric is further used for manifold matching. The two networks learn simultaneously during the training process. We apply the approach on both unsupervised and supervised learning tasks: in uncondi-tional image generation task, the proposed method obtains competitive results compared with existing generative mod-els; in super-resolution task, we incorporate the framework in perception-based models and improve visual qualities by producing samples with more natural textures. Experiments and analysis demonstrate the feasibility and effectiveness of the proposed framework. 1.

Introduction
Deep generative models including Variational Autoen-coder (VAE) [21], Generative Adversarial Networks (GAN)
[13] and their variants [3, 26, 24, 42, 10, 55, 41] have achieved great success in generative tasks such as image and video synthesis, super-resolution (SR), image-to-image text generation, neural rendering, etc. The translation, above approaches try to generate samples which mimic real data by minimizing various discrepancies between their cor-responding statistical distributions, such as using KL diver-gence [21], Jensen-Shannon divergence [13], Wasserstein distance [3], Maximum Mean Discrepancy [26] and so on.
These approaches focused on the data distribution aspect and did not pay enough attention to the underlying metrics of these distributions. The interplay between distribution measure and its underlying metric is a central topic in opti-mal transport (cf. [47]). Despite that researchers have suc-cessfully employed optimal transport theory in generative models [3, 50, 9], simply assuming the underlying metric to be Euclidean metric may neglect some rich information lying in the data [1]. In addition, although the above ap-proaches are validated to be effective, successful training setups are mostly based on empirical observations and lack of physical interpretations.
In this paper we bring up a geometric perspective which serves as an important parallel view of generative models as
GANs. Table 1 summarizes the main differences between classic GANs and our proposed (so-called MvM) frame-work.
Instead of directly matching statistical discrepan-cies under Euclidean distance, we provide a more ﬂexible framework which is built upon learning the intrinsic dis-tances among data points. Speciﬁcally, we treat the real data set as some manifold embedded in high-dimensional Eu-clidean space, and generate a fake distribution measure con-densed around the real data manifold by optimizing a Man-ifold Matching (MM) objective. The MM objective is built on shape descriptors, such as centroid and p-diameter with respect to some proper metric learnt by a metric generator using Metric Learning (ML) approaches. During training process, the (fake data) distribution generator and the metric generator work interchangeably and produces better distri-bution (metric) that facilitates the efﬁcient training of metric (distribution) generator. The learned distances can not only be used to formulate energy-based loss functions [22] for
MM, but can also reveal meaningful geometric structures of real data manifold.
Table 1. Main differences between GANs and MvM.
Differences
Main point of view
Matching terms
Matching criteria
Underlying metric
Objective functions
GANs statistics means, moments, etc. statistical discrepancy default Euclidean one min-max value function
MvM geometry centroids, p-diameters learned distances learned intrinsic two distinct objectives
*Equal contributions.
We apply the proposed framework on two tasks: uncon-ditional image generation and single image super-resolution (SISR). We utilize unconditional image generation task as a validation of the feasibility of the approach; and further implement a supervised version of the framework on SISR task to demonstrate its advantage. Our main contributions are: (1) We propose a manifold matching approach for gen-erative modeling, which matches geometric descriptors be-tween real and generated data sets using distances learned during training; (2) We provide a ﬂexible framework for modeling data and building objectives, where each genera-tor has its own designated objective function; (3) We con-duct experiments on unconditional image generation task and SISR task which validates the effectiveness of the pro-posed framework. 2.