Abstract
The conventional detectors tend to make imbalanced classiﬁcation and suffer performance drop, when the dis-tribution of the training data is severely skewed.
In this paper, we propose to use the mean classiﬁcation score to indicate the classiﬁcation accuracy for each category dur-ing training. Based on this indicator, we balance the clas-siﬁcation via an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method. Speciﬁcally,
EBL increases the intensity of the adjustment of the decision boundary for the weak classes by a designed score-guided loss margin between any two classes. On the other hand,
MFS improves the frequency and accuracy of the adjust-ment of the decision boundary for the weak classes through over-sampling the instance features of those classes. There-fore, EBL and MFS work collaboratively for ﬁnding the classiﬁcation equilibrium in long-tailed detection, and dra-matically improve the performance of tail classes while maintaining or even improving the performance of head classes. We conduct experiments on LVIS using Mask
R-CNN with various backbones including ResNet-50-FPN and ResNet-101-FPN to show the superiority of the pro-posed method. It improves the detection performance of tail classes by 15.6 AP, and outperforms the most recent long-tailed object detectors by more than 1 AP. Code is available at https://github.com/fcjian/LOCE. 1.

Introduction
Object detection plays an important role in computer vi-sion, and recent object detectors [6, 9, 15, 23] have achieved promising performance on several common datasets with a few categories and balanced class distribution, such as PAS-CAL VOC (20 classes) [5] and COCO (80 classes) [16].
However, most real-world data contains a large number of categories and its distribution is long-tailed: a few head classes contain abundant instances while a great number of tail classes only have a few instances.
∗Corresponding author.
Figure 1. Statistics of mean classiﬁcation score and classiﬁcation accuracy for each category on LVIS v1.0 training set and COCO training set tested by Mask R-CNN with ResNet-50-FPN. The x-axis represents the sorted category index and the number of in-stances from the corresponding category.
Recently, LVIS [7] is released for exploring long-tailed object detection. Not surprisingly, the performance of the state-of-the-art detectors designed for balanced data is sig-niﬁcantly degraded [13, 27] if they are directly applied to such datasets. The reason for the performance degradation mainly comes from two aspects: (1) The long-tailed dis-tribution of data. The number of the instances from tail classes (e.g., only 1 instance for one class) is insufﬁcient for training a deep learning model, resulting in under-ﬁtting of these classes. Moreover, the tail classes will be over-whelmed by the head classes during training, because the number of the instances of head classes is much larger than that of tail classes (e.g., thousands of times). As a result, the detectors cannot learn the tail classes well, and recognize those tail classes with very low conﬁdence, as demonstrated in Figure 1. (2) The large number of categories. With the in-crease of the number of categories, it brings a higher chance of misclassiﬁcation, especially for the tail classes with a very low classiﬁcation score.
Several works [7, 19, 22, 25] attempted to cope with the problem of long-tail learning by re-sampling training
data or re-weighting loss function. However, most of them assign the sampling rate and the loss weight according to the sampling frequency of each category, which is model-agnostic and sensitive to hyper-parameters [13, 19]. It may bring the following problems: (1) the model-agnostic data re-sampling is prone to over-ﬁt the tail classes and under-represent the head classes; (2) the dataset-based loss re-weighting may cause excessive gradients and unstable train-ing especially when the category distribution is extremely imbalanced. Recently, wang et al. [24] introduce Seesaw loss to adaptively re-balance the gradients of positive and negative samples by dynamically accumulating the number of class instances during training. However, the number of the training samples cannot accurately reﬂect the learning quality of the classes, due to the diversity and complexity of instances and categories, e.g., training a classiﬁer for the categories with visual similarity usually requires more train-ing samples than the categories with very different visual appearance.
To address the above problems, we propose to use the mean classiﬁcation score to monitor the learning status (i.e., classiﬁcation accuracy) of each category during training. As shown in Figure 1, the mean classiﬁcation score has an ap-proximate positive correlation with the classiﬁcation accu-racy. Thus, it can be used as an effective indicator to re-ﬂect the classiﬁcation accuracy during training. Based on this indicator, we design an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method, to dynamically balance the classiﬁcation. Equilibrium Loss:
To balance the classiﬁcation of different classes, EBL as-signs different loss margins between any two classes based on the statistical mean classiﬁcation score.
It increases the loss margin between weak (with low mean score) pos-itive classes and dominant (with high mean score) negative classes, and vice versa. Thus, the designed loss margin increases the intensity of the adjustment of the classiﬁca-tion decision boundary for the weak classes, resulting in a more balanced classiﬁcation. Memory-augmented Fea-ture Sampling: In addition to increasing the intensity of the adjustment of the decision boundary, we design MFS to in-crease the frequency and accuracy of the adjustment of the decision boundary for the weak classes. Speciﬁcally, rich instance features are ﬁrstly extracted based on a set of dense bounding boxes generated by a model-agnostic bounding box generator, and then stored by a feature memory mod-ule for feature reuse across training iterations. Finally, a probabilistic sampler is used to access the feature memory module to sample more instance features of weak classes to improve the training.
We codename the proposed method Long-tailed Object detector with Classiﬁcation Equilibrium (LOCE). In sum-mary, our contributions are as follows: (1) we propose to use the mean classiﬁcation score to monitor the classiﬁca-tion accuracy of each category during training; (2) we de-velop a score-guided equilibrium loss that improves the in-tensity of the adjustment of the decision boundary for the weak classes. (3) we design a memory-augmented fea-ture sampling to enhance the frequency and accuracy of the adjustment of the decision boundary for the weak classes. (4) we conduct experiments on LVIS [7] using Mask R-CNN [9] with various backbones including ResNet-50-FPN and ResNet-101-FPN [10, 14]. Extensive experiments show the superiority of LOCE. It improves the tail classes by 15.6 AP based on the Mask R-CNN with ResNet-50-FPN [10, 14] and outperforms the most recent long-tailed object detectors by more than 1 AP on LVIS v1.0. 2.