Abstract
We propose a Transformer-based framework for 3D hu-man texture estimation from a single image. The proposed
Transformer is able to effectively exploit the global infor-mation of the input image, overcoming the limitations of existing methods that are solely based on convolutional neural networks.
In addition, we also propose a mask-fusion strategy to combine the advantages of the RGB-based and texture-flow-based models. We further intro-duce a part-style loss to help reconstruct high-fidelity col-ors without introducing unpleasant artifacts. Extensive ex-periments demonstrate the effectiveness of the proposed method against state-of-the-art 3D human texture estima-tion approaches both quantitatively and qualitatively. The project page is at https://www.mmlab-ntu.com/ project/texformer. 1.

Introduction
In this paper, we study the problem of estimating 3D hu-man texture from a monocular image. This is an important problem that plays a key role in single-image 3D human reconstruction and has wide applications in virtual and aug-mented reality, film industry, gaming, and biometrics.
Most of existing methods for this problem [13, 23, 29, 44, 47, 53] use deep convolution neural networks (CNNs) to predict 3D human texture (i.e., a UV map) from the input image (Figure 1(a)). While these methods have achieved impressive results, their network architectures suffer from an inherent shortcoming: convolution layers are by design local operations and inefficient in processing global infor-mation that is crucial in 3D human texture estimation. More specifically, the input and output in this task do not have strictly-aligned spatial correspondences and may even have totally different shapes as shown in Figure 1(a). This is in sharp contrast to 2D computer vision tasks such as image super-resolution [51] and image-to-image translation [21] where the input and output are well aligned.
We believe that one should not simply follow the com-mon practice of using only CNNs for the 3D task in this
Figure 1. We propose a Transformer for 3D human texture estima-tion from a single image. Compared with the existing method [44] that solely relies on CNNs (a), the proposed Transformer uses the attention mechanism to more effectively exploit global informa-tion (b), which leads to higher-quality 3D human texture estima-tion. See the text for more detailed explanations. work, and global operations should be introduced for bet-ter 3D human texture reconstruction. Note that some recent approaches [47, 53] attempt to address this issue with fully-connected layers (or MLPs), which however often leads to loss of fine spatial information. Moreover, this information loss cannot be easily remedied by skip connections [38] due to the misalignment of the different-layer features. A suc-cessful solution should be able to more effectively distribute features of the input into suitable locations in the UV space while preserving fine spatial information.
Towards this end, we propose a Transformer-based framework for 3D human texture estimation from a single image. The Transformer allows processing information of the input in a global manner, which is in particular suitable for our task. At the core of the Transformer is an attention module that involves three basic components: Query, Key, and Value. For the Query, we use a pre-computed color map that has the same shape as the output UV map. Each pixel in the query map corresponds to a vertex on 3D hu-man mesh [31]. The Value is the input image that has all the source pixels. For the Key, we use a 2D part-segmentation map obtained with an off-the-shelf model [16], which im-plies the mapping from the image to the UV space [34, 53].
For an intuitive understanding of the relationship be-tween these three components, we elucidate the working mechanism of our Transformer in Figure 1(b). A pixel in the Query (marked as yellow star) is first used to correlate with the Key, which produces an attention map. With this attention map, the source information in the Value can then be effectively aggregated by weighted averaging to gener-ate the corresponding pixel in the output UV map. Such an attention mechanism allows us to exploit global infor-mation of the input image without losing fine details, which is the key factor that distinguishes our method from exist-ing algorithms. Note that the above explanations have been simplified for ease of understanding. As will be introduced in Section 3, we use more channels for the Value and Key in real implementation and perform multi-head attention [43] in feature space.
To summarize, we make the following contributions: 1) We propose a Transformer-based framework, termed as
Texformer, for 3D human texture estimation from a single image. Based on the attention mechanism, the proposed network is able to effectively exploit global information of the input. It naturally overcomes the limitations of existing algorithms that solely rely on CNNs and effectively facili-tates higher-quality 3D human texture reconstruction. 2) Existing algorithms output either RGB values [44, 47] or texture flow [23, 53] to synthesize the final UV map. We analyze the limitations of these two strategies and propose a new method to combine the best of both worlds. We show that the proposed method is able to significantly reduce vi-sual artifacts while preserving fine details. 3) The estimated textures of previous approaches often suf-fer from noticeable color differences from the input. To remedy this issue, we propose a part-style loss that enforces the Gram-matrix similarity [12] for each human body part and encourages closer appearances to the input image.
Extensive experiments on the Market-1501 dataset [54] demonstrate the effectiveness of the proposed method against state-of-the-art 3D human texture estimation ap-proaches both quantitatively and qualitatively. 2.