Abstract
How to make the appearance and motion information in-teract effectively to accommodate complex scenarios is a fundamental issue in flow-based zero-shot video object seg-In this paper, we propose an Attentive Multi-mentation.
Modality Collaboration Network (AMC-Net) to utilize ap-pearance and motion information uniformly. Specifically,
AMC-Net fuses robust information from multi-modality fea-tures and promotes their collaboration in two stages. First, we propose a Multi-Modality Co-Attention Gate (MCG) on the bilateral encoder branches, in which a gate func-tion is used to formulate co-attention scores for balancing the contributions of multi-modality features and suppress-ing the redundant and misleading information. Then, we propose a Motion Correction Module (MCM) with a visual-motion attention mechanism, which is constructed to em-phasize the features of foreground objects by incorporat-ing the spatio-temporal correspondence between appear-ance and motion cues. Extensive experiments on three pub-lic challenging benchmark datasets verify that our proposed network performs favorably against existing state-of-the-art methods via training with fewer data. The code is released at https://github.com/isyangshu/AMC-Net. 1.

Introduction
Zero-shot Video Object Segmentation (ZVOS) aims to automatically separate the primary object(s) from the back-ground in a video sequence without any human interaction.
Since ZVOS does not require manual intervention, it has significant value in a wide range of applications, such as video compression [11], visual tracking [45], and person re-identification [51]. How to distinguish the target object(s) from complex and diverse background without any prior knowledge is an open challenge in ZVOS.
To address this issue, several methods [38, 21, 1, 19, 53]
Figure 1. Illustration of various multi-modality interaction ap-proaches for fusing appearance and motion information. design various multi-modality interaction schemes to lever-age external object motion information, which hypothesizes that the object moving across the video sequence is highly related to the primary object. Despite the impressive perfor-mance, there remain some issues in existing interaction ap-proaches. Early methods [38, 21, 1] directly execute feature alignment such as concatenation or addition to produce ob-ject masks (see Figure 1 (a)). Due to the redundant and in-valid information in flow maps and video frames, the direct feature alignment of multi-modality features would limit the accuracy of segmentation (see the third column in Figure 2).
Several methods [53, 19] propose to build a motion-based attention mechanism to enhance the feature learning of ob-ject appearance (see Figure 1 (b)). These methods learn to
Figure 2. Visual comparison of different flow-based methods. We show the video frames, optical flow maps, SegFlow [1] predictions,
MATNet [53] predictions, and our predictions in the bmx-trees, motocross-jump, and car-roundabout video sequences. The optical flow maps are predicted by PWCNet [35]. enhance the appearance features of the significant motion areas, which makes them rely on the quality of optical flow.
However, when complex motion conditions (e.g., deforma-tion, motion blur, fast motion, and clutters) occur in the video sequence, optical flow might fail to capture the object location and influence the accuracy of object segmentation (see the fourth column in Figure 2).
Motivated by the above observations, in this paper, we propose an Attentive Multi-Modality Collaboration Net-work (AMC-Net) for zero-shot video object segmentation, which builds a novel co-attention mechanism for effective multi-modality interaction. AMC-Net adaptively fuses ro-bust spatio-temporal representations from multi-modality features and promotes their collaboration in two stages to thoroughly combine the merits of appearance and motion features (see Figure 1 (c)).
In the first stage, we pro-pose a Multi-Modality Co-Attention Gate (MCG), which is used to unify the appearance and motion information into valid spatio-temporal feature representations. Considering the disparity of the contributions of different modality fea-tures, we utilize a gate function to predict the co-attention scores, which are used to balance the contributions of multi-modality features and suppress the redundant and mislead-ing information. In the second stage, we propose a Motion
Correction Module (MCM) to perform adaptive feature fu-sion, in which a visual-motion attention mechanism is con-structed to emphasize the features of foreground objects by incorporating the spatio-temporal correspondence between appearance and motion cues. Specifically, different from the single-directional attention guidance from motion to ap-pearance, we model the attention based on visual saliency and motion saliency to facilitate the feature learning of fore-ground objects.
To investigate the effectiveness of our proposed model, we conduct comprehensive experiments including over-all comparison and ablation studies on three benchmark datasets [31, 33, 29]. The results show that our proposed method can achieve superior performance against the state-of-the-arts by using only DAVIS-16 [31] for training.
Our contributions can be summarized as follows:
• We propose an Attentive Multi-Modality Collaboration
Network for zero-shot video object segmentation, which promotes deep collaboration of appearance and motion in-formation to generate accurate object segmentation.
• We propose a Multi-Modality Co-Attention Gate to unify the multi-modality information. A gate function is used to produce co-attention scores to adaptively balance the contributions of appearance and motion information.
• Our proposed method performs favorably against the state-of-the-art methods on three public challenging bench-mark datasets (DAVIS-16 [31], Youtube-Objects [33], and
FBMS [29]). 2.