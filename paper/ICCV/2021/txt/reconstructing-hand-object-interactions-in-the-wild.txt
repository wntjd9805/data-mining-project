Abstract
We study the problem of understanding hand-object in-teractions from 2D images in the wild. This requires recon-structing both the hand and the object in 3D, which is chal-lenging because of the mutual occlusion between the hand and the object. In this paper we make two main contribu-tions: (1) a novel reconstruction technique, RHO (Recon-structing Hands and Objects), which reconstructs 3D mod-els of both the hand and the object leveraging the 2D image cues and 3D contact priors; (2) a dataset MOW (Manipu-lating Objects in the Wild) of 500 examples of hand-object interaction images that have been ”3Dﬁed” with the help of the RHO technique. Overall our dataset contains 121 distinct object categories, with a much greater diversity of manipulation actions, than in previous datasets. 1.

Introduction
Our hands are the primary way we interact with objects in the world. In turn, we designed our world with hands in mind. Therefore, understanding hand-object interactions
*Equal contribution. More examples available at this project page. is an important ingredient for building agents that perceive and act in the real world. For example, it can allow them to learn object affordances [10], infer human intents [27], and learn manipulation skills from humans [34, 26, 33].
What does it mean to understand hand-object interac-tions? We argue that fully capturing the richness of hand-object interactions requires 3D understanding.
In gen-eral, recovering 3D from a single RGB image is an under-In the case of hand-object interac-constrained problem. tions, the problem is even more challenging due to heavy occlusions that occur during object manipulation, a wide range of small daily objects that are not even present in labeled recognition datasets, and ﬁne-grained interactions with complex contacts that are difﬁcult to model.
Overall, our community has made substantial progress toward this goal. However, due to the difﬁculty in obtaining 3D annotations in the wild, the data collection efforts have focused mainly on in-the-lab setting [12, 61, 9, 2, 47]. As shown in Figure 2, there is a large reality gap between the existing in-the-lab settings and the richness of environments
Indeed, as and interactions found in images in the wild. shown in Table 1, existing datasets have a limited number of participants and objects. 1
setting data type particip. objects
HO3D [12] lab video 10 10
CP [2] lab video 50 25
GRAB [47] lab
MoCap 10 51
Ours wild image 450 121
Table 1: Existing 3D hand-object datasets. Our dataset contains in-the-wild images, as shown in Figure 2 right, and a large number of different participants and objects. of Hands [40] datasets. These depict a rich diversity of ma-nipulation actions, which we augment with newly collected 3D object models from 121 object categories, 3D object poses, and 3D hand poses.
The resulting dataset can be used in a number of ways.
We highlight two potential use-cases next. First, the dataset can be used to evaluate hand-object reconstruction methods on challenging in-the-wild data. Second, it can be used to learn more about human manipulation from images in the wild. For example, we present initial analysis of our data and observe interesting trends (Figure 8).
In summary, our key contributions are: (1) We present a novel optimization-based procedure, RHO, that is able to reconstruct hand-object interactions in the wild across di-verse object categories; (2) We show quantitative and qual-itative improvements over existing methods, especially on in-the-wild setting; (3) We contribute a new 3D dataset,
MOW, of 500 images in the wild, spanning 121 object cate-gories with annotation of instance category, 3D object mod-els, 3D hand pose, and object pose annotation.
We encourage the readers to see the extended version of this work on arXiv which includes the supplementary mate-rials, and also to check the project page. 2.