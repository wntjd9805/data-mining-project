Abstract
Most recent video super-resolution (SR) methods either adopt an iterative manner to deal with low-resolution (LR) frames from a temporally sliding window, or leverage the previously estimated SR output to help reconstruct the cur-rent frame recurrently. A few studies try to combine these two structures to form a hybrid framework but have failed to give full play to it. In this paper, we propose an omniscient framework to not only utilize the preceding SR output, but also leverage the SR outputs from the present and future.
The omniscient framework is more generic because the it-erative, recurrent and hybrid frameworks can be regarded as its special cases. The proposed omniscient framework enables a generator to behave better than its counterparts under other frameworks. Abundant experiments on public datasets show that our method is superior to the state-of-the-art methods in objective metrics, subjective visual ef-fects and complexity. 1.

Introduction
Super-Resolution (SR) aims at reconstructing high-images from the corresponding low-resolution (HR) resolution (LR) images. As the most basic problem in S-R, single image super-resolution (SISR) has been relative-ly studied thoroughly, where under a unified framework, the researchers only have to design different kinds of con-volutional neural networks (CNNs) [4, 14, 32, 31, 3] to solve this issue. Based on SISR, video super-resolution (VSR) has also been developed, albeit a lot of works
[1, 22, 19, 27, 29, 11, 10] have been proposed, there is not a unified framework being dominant in VSR yet. Figure 1
∗Corresponding author. Code: https://github.com/psychopa4/OVSR.
Figure 1: Performances on Vid4 [1] dataset and time costs.
The Red circle denotes iterative methods, and the blue cir-cle indicates recurrent and hybrid methods. Please refer to
Table 4 and Table 6 for more details. illustrates dozens of state-of-the-art (SOTA) VSR methods in terms of performance and speed.
As SISR requires only one input image, most SISR methods focus on exploring different generator networks to extract features from this one image under a unified frame-work. Nevertheless, since VSR involves consecutive video frames as input, different schemes for handling the temporal information have emerged. We demonstrate different kinds of frameworks for VSR in Figure 2.
As illustrated in Figure 2(a), most recent VSR methods
[1, 22, 25, 30, 12, 24, 29, 23, 11] apply an iterative manner to deal with LR frames from a temporally sliding window, where we only show the case of window size as 3. Given a sequence of video frames, the iterative framework con-siders the whole VSR processing as multiple independent sub-processes. Theoretically, these sub-processes are not temporally correlated and can be handled simultaneously, which means they enjoy the advantage of parallel comput-(a) Iterative VSR (b) Recurrent VSR (c) Hybrid VSR (d) Local Omniscient VSR (e) Global Omniscient VSR
Figure 2: Different kinds of frameworks for VSR, where “G” represents the generator network. Red, black and blue arrows denote information from the past, present and future respectively. ing [29]. However, the iterative framework can only obtain more neighboring LR frames by increasing the window size but omits the previously estimated SR output, which is the exact reason that prevents it from a better performance.
As demonstrated in Figure 2(b), the recurrent framework
[19, 10] processes video frames by the order, while it can n-ever exploit the subsequent frames to assist with recovering the current frame, which has limited its potential. Although a few studies [5, 27] have tried to combine these two frame-works to form a hybrid framework, as shown in Figure 2(c), it can only receive the estimated hidden states from the past, and they have not achieved satisfying results.
The recurrent and hybrid frameworks only leverage pre-vious hidden states, which inspires us to wonder what if we further try to involve the hidden states from the present and future. To this end, we propose the omniscient frame-work. Specifically, we integrate two sub-networks: a pre-cursor network N etp and a successor network N ets into the omniscient framework. The successor network inherits the hidden states generated by the precursor network, and thus manages to leverage the LR frames and hidden states from the past, present and future. As shown in Figure 2(d) and
Figure 2(e), the omniscient framework can be further divid-ed into two categories: local omniscient and global omni-scient. Local omniscient framework processes video frames unidirectionally while global omniscient framework does it bidirectionally. The global omniscient framework enables any LR frame to receive information from all other frames in a same video sequence, however, it is not appropriate for delay-sensitive real-time tasks like live broadcasts, where the local omniscient framework suits well instead.
Overall, in this paper, we propose a more generic om-niscient framework to exploit both LR frames and estimat-ed hidden states from the past, present and future. In fact, as shown in Figure 2, the iterative, recurrent and hybrid frameworks can be regarded as the special counterparts of our proposed omniscient framework. We have explored a same kind of generator network under different framework-s, and we have found the omniscient framework (local and global) superior to the existing iterative, recurrent and hy-brid frameworks. Our models surpass other SOTA meth-ods in both performance and complexity, and thus we hope this framework will become a standard framework in VSR, under which researchers are free to design more effective generator networks, explicit or implicit motion information capturing modules, or loss functions to tap its potential. 2.