Abstract
Extensive Unsupervised Domain Adaptation (UDA) studies have shown great success in practice by learning transfer-able representations across a labeled source domain and an unlabeled target domain with deep models. However, current work focuses on improving the generalization ability of UDA models on clean examples without considering the adversarial robustness, which is crucial in real-world ap-plications. Conventional adversarial training methods are not suitable for the adversarial robustness on the unlabeled target domain of UDA since they train models with adver-sarial examples generated by the supervised loss function.
In this work, we propose to leverage intermediate represen-tations learned by robust ImageNet models to improve the robustness of UDA models. Our method works by align-ing the features of the UDA model with the robust features learned by ImageNet pre-trained models along with domain adaptation training. It utilizes both labeled and unlabeled domains and instills robustness without any adversarial in-tervention or label requirement during domain adaptation training. Our experimental results show that our method signiﬁcantly improves adversarial robustness compared to the baseline while keeping clean accuracy on various UDA benchmarks. 1.

Introduction
Transferring knowledge from a labeled source domain to an unlabeled target domain is desirable in many real-world applications. However, deep learning models do not perform well in the presence of such domain shifts. For example, a
*This work was done while being at Huawei Noah’s Ark Lab. The webpage for the project: awaisrauf.github.io/robust_uda
†Corresponding Author model trained on synthetic data may fail to generalize well on real-world data. Unsupervised Domain Adaptation (UDA) seeks to solve this problem by learning domain-invariant features. Recent UDA methods harness transferable features learned by deep models pre-trained on large datasets like
ImageNet [12, 17, 29, 28, 49, 26, 40, 15, 21, 22]. However, a large body of work has shown that these deep models are vulnerable to small adversarial changes in the input that can easily fool the trained models [5, 39, 14, 7]. The widespread use of these models in sensitive applications requires them to be robust against these changes.
Signiﬁcant attention has been devoted to counter adversar-ial examples, and many defense methods have been de-vised [14, 16, 42, 30, 6, 25, 33, 37, 41, 46]. Supervised adver-sarial training is among the most successful approaches [30].
It is based on the simple idea of training a model on ad-versarial examples. It utilizes min-max optimization where adversarial examples are ﬁrst generated by iterative maxi-mization of the loss, and the model is then trained on these examples. However, the generation of these adversarial ex-amples requires labels and adversarial training implicitly assumes inputs from a single domain. These issues limit the applicability of adversarial training in UDA.
In this paper, we propose a simple, unsupervised, and do-main agnostic method for robustness in UDA. Our method does not require labels and utilizes data from both domains, making it feasible for UDA. Our work is motivated by the recent line of work on transferability of robustness [13, 9], and observation that adversarially trained models learn "fun-damentally different" features from normally trained counter-parts [43, 20, 36]. The ﬁrst line of work has demonstrated the transferability of adversarial robustness from a pre-trained robust model. The authors in [18, 38] show that adversarially pre-trained models can improve robustness in transfer learn-ing; [13] shows that adversarial robustness can be distilled by matching softened labels produced by robust pre-trained
Figure 1. An overview of the proposed method. Source and target images are passed through the backbone model and robust teachers to get features at different blocks. The intermediate features are transferred to the robust feature adaptation (RFA) module, which adapts the robustness. The output of the backbone model goes through the domain adaptation module, which utilizes an unsupervised domain adaption algorithm. The parameters of the UDA feature extractor are updated to minimize both domain adaptation and robust feature adaptation loss.
Light colors show the features extracted for source domain inputs and dark colors for target domain inputs. models; [9] shows that robustness can be distilled by match-ing input gradients of robust models to those of a non-robust model. These works focus on cutting the computational cost of adversarial training for single domain classiﬁcation and require labeled data.
Our proposed method, Robust Feature Adaptation (RFA), embeds the adaptation of robustness in the domain adaptation training by leveraging the feature space of robust pre-trained models. RFA uses ImageNet adversarially pre-trained mod-els to extract robust features for inputs of source and target domains. It then instills robustness in UDA’s feature extrac-tor by minimizing its discrepancy with robust features. RFA enables the model to learn both domain invariant and robust features.
Unlike previous works on transferability, our method does not require labeled data as it only uses intermediate fea-tures of the robust models and a label-free distance measure between the feature spaces of the two models. Similarly,
RFA does not require any adversarial intervention during the domain adaptation training as it does not generate adver-sarial examples. These characteristics make it possible to harnesses both labeled source and unlabeled target domains.
Moreover, the RFA is a plug-in method that can be used with any UDA method to enhance its robustness. It only requires adversarially pre-trained models similar to the UDA meth-ods that need normally pre-trained models. Our experiments show that RFA can equip UDA models with high adversarial robustness while keeping good generalization ability. Our contributions can be summarized as follows:
• We propose a plug-in method that aligns the features of a UDA model with the robust features of multiple adversarially pre-trained ImageNet models. This way, it instills robustness in UDA models without adversarial intervention or label requirement.
• To the best of our knowledge, we are the ﬁrst to show that the adversarial robustness for a target task can be distilled from intermediate representations of robust models adversarially trained on a different task without any ﬁne-tuning.
• Comprehensive experimental results show that our method consistently improves the robustness of various
UDA algorithms on widely-used benchmark datasets.
For instance, it improves the adversarial robustness from 0% to 43.49% while maintaining the clean accu-racy for CDAN as the UDA algorithm on challenging simulation-to-real adaptation task of the VisDA-2017 dataset. 2.