Abstract
How would a static scene react to a local poke? What are the effects on other parts of an object if you could lo-cally push it? There will be distinctive movement, despite evident variations caused by the stochastic nature of our world. These outcomes are governed by the characteris-tic kinematics of objects that dictate their overall motion caused by a local interaction. Conversely, the movement of an object provides crucial information about its underlying distinctive kinematics and the interdependencies between its parts. This two-way relation motivates learning a bijec-tive mapping between object kinematics and plausible future image sequences. Therefore, we propose iPOKE – invert-ible Prediction of Object Kinematics – that, conditioned on an initial frame and a local poke, allows to sample object kinematics and establishes a one-to-one correspondence to the corresponding plausible videos, thereby providing a con-trolled stochastic video synthesis. In contrast to previous works, we do not generate arbitrary realistic videos, but provide efﬁcient control of movements, while still capturing the stochastic nature of our environment and the diversity of plausible outcomes it entails. Moreover, our approach can transfer kinematics onto novel object instances and is not conﬁned to particular object classes. Our project page is available at https://bit.ly/3dJN4Lf. 1.

Introduction
Imagine a 3-year-old standing next to a stacked pyramid of glasses in a shop. Can you sense the urge to pull one glass out—just to observe what happens. We have an inborn curiosity to understand how the world around us reacts to our actions, so we can eventually imagine and predict their out-come beforehand. This ability to predict is the prerequisite for targeted, goal-oriented interaction with our world rather than random manipulation of our environment. Once we are older, we have also learned to generalize and predict the dynamics of previously unseen objects when they are pulled or poked; and the less audacious have understood that it is often more effective to have others do daring experiments like the one above (and pay the bill) while they are learning by merely watching the outcome. While such experiments
Figure 1. iPOKE: Conditioned on a local poke controlling desired object motion in a static image, our invertible model learns a rep-resentation of the remaining object kinematics for arbitrary object classes. Once learned, our framework allows users to locally con-trol intended movements while sampling diverse realistic motion for the remainder of the object and to even transfer kinematics to unseen object instances. are not just fun to watch, they also help to imagine the many possible outcomes caused by the stochastic nature of the many factors beyond our control.
Given a single static image, how can an artiﬁcial vision sys-tem imagine, i.e. synthesize, the many possible outcomes when locally manipulating the scene? It needs to learn how a local poke affects different parts of an object and the re-sulting kinematics [49]. Conditioned on only the start frame and the displacement of a single pixel, we want to synthesize multiple videos, each showing the different plausible future dynamics. To render this generative, stochastic approach widely applicable, training should only require videos of objects in motion, but no ground truth information regarding the forces acting on an object such as a local poke. The representation of the kinematics should then generalize to similar objects not seen during training in contrast to instance speciﬁc models [15]. Moreover, the method should work for arbitrary objects, rather than being tuned to just a single class [1, 21]. Therefore, no prior motion model is available, but all kinematics have to be learned from the unannotated
video data. Previous work on video synthesis has mainly explored two opposing research directions: (i) uncontrolled future frame prediction [24, 10, 53, 59] synthesizing videos based on a start frame, but with no control of scene dynamics, and (ii) densely controlled video synthesis [50, 73, 77, 74] demanding tedious, per-pixel guidance how the video will evolve such as by requiring the object motion to be provided per pixel [50, 73, 77] or a future target frame [74]. Our sparsely controlled video synthesis based on few local user interactions constitutes the rarely investigated midground in between, allowing for speciﬁc but still efﬁcient control of kinematics.
In this paper, we present a model for exercising local con-trol over the kinematics of objects observed in an image.
Indicating movements of individual object parts with a sim-ple mouse drag provides sufﬁcient input for our model to synthesize plausible, holistic object motion. To capture the ambiguity in the global object articulation, we learn a dedi-cated latent kinematic representation. The synthesis problem is then formulated as an invertible mapping between object kinematics and video sequences conditioned on the observed object manipulation. Due to its stochastic nature, our latent representation allows to sample and transfer diverse kine-matic realizations ﬁtting to the sparse local user input to then infer and synthesize plausible video sequences as shown in
Fig. 1.
To evaluate our model on controlled stochastic video syn-thesis, we conduct quantitative and qualitative experiments on four different datasets exhibiting complex and highly ar-ticulated objects, such as humans and plants. Comparisons with the state-of-the-art in stochastic and controlled video prediction demonstrate the capability of our model to predict and synthesize plausible, diverse object articulations inferred from local user control. 2.