Abstract
We present a novel and flexible architecture for point cloud segmentation with dual-representation iterative learning.
In point cloud processing, different representa-tions have their own pros and cons. Thus, finding suitable ways to represent point cloud data structure while keeping its own internal physical property such as permutation and scale-invariant is a fundamental problem. Therefore, we propose our work, DRINet, which serves as the basic net-work structure for dual-representation learning with great flexibility at feature transferring and less computation cost, especially for large-scale point clouds. DRINet mainly con-sists of two modules called Sparse Point-Voxel Feature Ex-traction and Sparse Voxel-Point Feature Extraction. By uti-lizing these two modules iteratively, features can be prop-agated between two different representations. We further propose a novel multi-scale pooling layer for pointwise lo-cality learning to improve context information propagation.
Our network achieves state-of-the-art results for point cloud classification and segmentation tasks on several datasets while maintaining high runtime efficiency. For large-scale outdoor scenarios, our method outperforms state-of-the-art methods with a real-time inference time of 62ms per frame. 1.

Introduction
Point cloud data plays a significant role in various real-world applications, from autonomous driving to augmented reality (AR). One of the critical tasks in point cloud un-derstanding is point cloud semantic segmentation, which can facilitate self-driving cars or AR applications to inter-act with the physical world. For real-world applications, an accurate and real-time point cloud segmentation method is highly desirable. Therefore, in this work, we will study a new framework for high-quality point cloud segmentation in real-time.
‡Part of the work was done during an internship at DEEPROUTE.AI.
∗Equal contributions.
Figure 1. The mIoU performance vs. speed on the SemanticKITTI test set. Projection methods are drawn as blue triangles and other kinds of methods are drawn as red rectangles. Methods close to the right top location mean that achieving better per-formance within less runtime cost. Drawn methods are RA:
RandLA [13], PV: PVCNN [23], SPV: Sparse PVCNN [31],
DASS [34] PO: PolarNet [45], D53: Darknet53 [2], D21: Dark-net21 [2], RN: RangeNet++ [24], SQ321: SqueezeSegV3-21 [37],
SQ353: SqueezeSegV3-53 [37], O: our DRINet. Our DRINet out-performs all the existing methods while maintaining high runtime efficiency at 15Hz.
Although we have witnessed great progress in vision tasks on 2D images with convolutional neural networks (CNN), point cloud processing with deep learning still faces lots of challenges. Due to its sparsity and irregularity, it is difficult to directly apply 2D CNNs or some other pop-ular operations in image processing for point cloud data.
PointNet [26] is a pioneering work that directly operates on raw point clouds. PointNet++ [27] extends the PointNet by aggregating local features at different scales of neigh-borhoods to capture more context information and fine ge-ometry structures. Further, VoxelNet [50] firstly combines learning-based point cloud feature extraction with a stan-these aspects, we propose the DRINet that serves as a better and novel framework for dual representations point cloud segmentation learning. Our DRINet has better flexibility in converting between dual representations, with which we can learn features iteratively between point and voxel rep-resentations (shown in the Fig. 2) by our proposed novel modules: Sparse Point-Voxel Feature Extraction (SPVFE) and Sparse Voxel-Point Feature Extraction (SVPFE). Each module takes the features of the other module as input. As such, we can preserve the fine details by pointwise features and explore more context information with large receptive fields by voxelwise features. Beyond these two modules, we explore multi-scale feature extraction and aggregation for pointwise feature learning in our SPVFE to maintain its locality for better context information. Furthermore, we replace the bilinear and trilinear gathering operations with an attentive gathering layer to reduce the computation cost of feature transformation from voxelwise features to point-wise features under the SVPFE module while maintaining the performance.
In summary, our contributions include
• We propose a novel network architecture for point cloud learning that can flexibly transform representa-tions between pointwise and voxelwise features. Both pointwise and voxelwise features can be aggregated and propagated iteratively.
• A multi-scale pooling layer is proposed at the voxel level to efficiently extract multi-scale pointwise fea-tures to gain better context information of point clouds.
• We propose a novel attentive gathering layer to gain better pointwise features from voxel features at a low memory access cost.
• To demonstrate the effectiveness of our method, extensive experiments are conducted on both in-door and outdoor datasets including ModelNet [48],
ShapeNet [48], S3DIS [1], and SemanticKITTI [2].
Compared with existing methods, our DRINet achieves the state-of-the-art performance on Se-manticKITTI, one of the most challenging datasets for outdoor scene parsing, while running at a real-time speed of 62ms per frame on an Nvidia RTX 2080 Ti
GPU. 2.