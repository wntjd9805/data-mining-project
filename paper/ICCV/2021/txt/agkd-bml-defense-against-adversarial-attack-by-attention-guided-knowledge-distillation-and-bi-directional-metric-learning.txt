Abstract
While deep neural networks have shown impressive per-formance in many tasks, they are fragile to carefully de-signed adversarial attacks. We propose a novel adversarial training-based model by Attention Guided Knowledge Dis-tillation and Bi-directional Metric Learning (AGKD-BML).
The attention knowledge is obtained from a weight-ﬁxed model trained on a clean dataset, referred to as a teacher model, and transferred to a model that is under training on adversarial examples (AEs), referred to as a student model.
In this way, the student model is able to focus on the cor-rect region, as well as correcting the intermediate features corrupted by AEs to eventually improve the model accu-racy. Moreover, to efﬁciently regularize the representation in feature space, we propose a bidirectional metric learn-ing. Speciﬁcally, given a clean image, it is ﬁrst attacked to its most confusing class to get the forward AE. A clean image in the most confusing class is then randomly picked and attacked back to the original class to get the backward
AE. A triplet loss is then used to shorten the representation distance between original image and its AE, while enlarge that between the forward and backward AEs. We conduct extensive adversarial robustness experiments on two widely used datasets with different attacks. Our proposed AGKD-BML model consistently outperforms the state-of-the-art approaches. The code of AGKD-BML will be available at: https://github.com/hongw579/AGKD-BML. 1

Introduction
Deep neural networks (DNNs) have achieved great break-through on a variety of ﬁelds, such as computer vision [22], speech recognition [17], and natural language process-ing [8]. However, their vulnerability against the so-called
*This work was done during the research assistantship at BNL.
†Corresponding author.
Figure 1. A clean image (“German shepherd”) and its adversarial example (incorrectly classiﬁed as “Planetarium”) are in the ﬁrst column. The class relevant attention maps (Grad-CAM) of correct and incorrect labels, and the class irrelevant attention maps are shown in the second, third and fourth columns, respectively.
It shows that the adversarial perturbations corrupt the attention maps. adversarial examples (AEs), which are the data with care-fully designed but imperceptible perturbations added, has drawn signiﬁcant attention [38]. The existing of AEs is a potential threat for the safety and security of DNNs in real-world applications. Thus, many efforts have been made to defend against adversarial attacks as well as improve the adversarial robustness of the machine learning model. In particular, adversarial training [16, 27]-based models are among the most effective and popular defending methods.
Adversarial training solves a min-max optimization prob-lem, in which the inner problem is to ﬁnd the strongest AE within an (cid:15)−ball by maximizing the loss function, while the outer problem is to minimize the classiﬁcation loss of the AE. Madry et al. [27] provided a multi-step pro-jected gradient descent (PGD) model, which has become the standard model of the adversarial training. Follow-ing PGD, a number of recent works have been proposed to improve adversarial training from different aspects, e.g.,
[6, 11, 28, 32, 35, 42, 49, 51].
However, the adversarial training-based models still suf-fer from relatively poor generalization on both clean and adversarial examples. Most of the existing adversarial train-ing based models focus only on the on-training model that utilizes adversarial examples, which may be corrupted, but have not well explored the information from the model trained on clean images. In this work, we aim to improve the model adversarial robustness by distilling the attention knowledge and utilizing bi-directional metric learning.
The attention mechanism plays a critical role in human visual system and is widely used in a variety of application tasks [34, 53]. Unfortunately, one of our observations shows that the perturbations in the adversarial example (AE) will be augmented through the network, and thus signiﬁcantly corrupts the intermediate features and attention maps.
It is shown in the Figure 1, the AE confuses the model by letting it focus on different regions from the clean image.
Intuitively, if we can transfer the knowledge of clean images from the teacher model to the student model to 1) obtain right attention information, and 2) correct the intermediate features corrupted by AE, we should be able to improve the model’s adversarial robustness.
With this motivation, we propose an Attention Guided
Knowledge Distillation (AGKD) module, which applies knowledge distillation (KD) [18] to efﬁciently transfer at-tention knowledge of the corresponding clean image from the teacher model to the on-training student model. Specif-ically, the teacher model is pre-trained on the original clean images and will be ﬁxed during training, while the student model is the on-training model. The attention map of a clean image obtained from the teacher model is used to guide the student model to generate the attention map of the corresponding AE against the perturbations.
We further use t-distributed Stochastic Neighbor Embed-ding (t-SNE) to study the behavior of the AE in the latent feature space (see Figure 3), and observe that the repre-sentations of the AE are usually far away from their orig-inal class, similar as shown in [28]. While AGKD trans-fers information of clean image to the student model from the teacher model and thus provides the constraints on the similarity between the AE and its corresponding clean im-age, there is no constraint of samples from different classes taken into account. Previous works [24, 28, 52] proposed using metric learning to regularize the latent representations of different classes. Speciﬁcally, a triplet loss is utilized, in which latent representations of the clean image, its cor-responding AE and an image from another class are con-sidered as the positive, anchor, and negative example, re-spectively. However, this strategy only considers the one-directional adversarial attack, i.e., from the clean image to its adversarial example, making it less efﬁcient.
To address the above issue, we propose a Bi-directional attack Metric Learning (BML) to provide a more efﬁcient and strong constraint. Speciﬁcally, the original clean im-age (positive) is ﬁrst attacked to its most confusing class, which is the class that has the smallest loss other than the correct label, to get the forward adversarial example (an-chor). Then, a clean image is randomly picked from the most confusing class and is attacked to the original image to get the backward adversarial example as the negative.
By integrating AGKD and BML, our AGKD-BML model outperforms the state-of-the-art models on two widely used datasets, CIFAR-10 and SVHN, under differ-ent attacks. In summary, our contribution is three-fold:
• An attention guided knowledge distillation module is proposed to transfer attention information of clean im-age to the student model, such that the intermediate features corrupted by adversarial examples can be cor-rected.
• A bidirectional metric learning is proposed to efﬁ-ciently constrain the representations of the different classes in feature space, by explicitly shortening the distance between original image and its forward adver-sarial example, while enlarging the distance between the forward adversarial example and the backward ad-versarial example from another class.
• We conduct extensive adversarial robustness experi-ments on the widely used datasets under different at-tacks, the proposed AGKD-BML model outperforms the state-of-the-art approaches with both the qualita-tive (visualization) and quantitative evidence. 2