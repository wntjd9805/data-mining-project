Abstract
The key point of language-guided person search is to construct the cross-modal association between visual and textual input. Existing methods focus on designing mul-timodal attention mechanisms and novel cross-modal loss functions to learn such association implicitly. We propose a representation learning method for language-guided person search based on color reasoning (LapsCore).
It can ex-plicitly build a ﬁne-grained cross-modal association bidi-rectionally. Speciﬁcally, a pair of dual sub-tasks, image colorization and text completion, is designed. In the for-mer task, rich text information is learned to colorize gray images, and the latter one requests the model to understand the image and complete color word vacancies in the cap-tions. The two sub-tasks enable models to learn correct alignments between text phrases and image regions, so that rich multimodal representations can be learned. Extensive experiments on multiple datasets demonstrate the effective-ness and superiority of the proposed method. 1.

Introduction
Language-guided person search has attracted consider-able attention because of its promising application in intel-ligent surveillance. As shown in Figure 1, it aims to retrieve the person from a large image database that best matches the natural language description query. Compared with image-based and attribute-based person ReID, language queries are easier to obtain than image queries and provide more comprehensive and accurate descriptions than attributes.
There exist two main challenges in the task of language-guided person search. First, it is difﬁcult to compute the visual-textual afﬁnity and construct the image-text align-ments, resulting from the cross-modal gap. Secondly, per-∗Equal contribution
†Corresponding author
Figure 1: The task of language-guided person search is to retrieve the person that best matches the given textual query from a large image database. son search is a ﬁne-grained retrieval task: (i) Text provides very detailed descriptions to the target person; (ii) Person images have ﬁne intra-class differences in appearance.
After the pioneering work [20] of language-guided per-son search, many efforts have been devoted to handling the
[2, 39, 28, 36] design advanced challenges of this task. models to learn better representations of image and text. At-tention mechanisms are developed in [20, 10, 24, 5] to build the local image-text association. [19, 33, 39, 28] propose novel loss functions to narrow the distance between visual and textual features. However, all of these methods implic-itly learn the cross-modal local association, which leaves a rigorous test to the models’ learning capability. From numerous experiments of language-guided person search, we observe that colors play a signiﬁcant role in retrieval.
Faced with personal images, human beings tend to accept visual colors to extract the appearance information, and then understand the clothes or ornaments related to these col-ors. Thus we are inspired to propose a novel representa-tion learning method LapsCore, by solving color reasoning sub-tasks, which guide the model to explicitly learn ﬁne-grained cross-modal associations.
As displayed in Figure 1, the ﬁrst sub-task, text-guided image colorization (IC), is to colorize a gray image accord-ing to its text description. In this task, models are facili-tated to correctly probe rich color information from the text and align them to the corresponding image regions. For instance in Figure 1, not only the word “red” should be ex-tracted, but also the semantic meaning of “shirt” requires to be paired with “red”, and the spatial region in the image indicating “shirt” should be colored in red. Hence the text-to-image local association can be constructed. As for the opposite direction, image to text, the other sub-task image-guided text completion (TC) is designed. Speciﬁcally, in each description sentence, all color words are removed, and these vacancies are required to be completed by exploiting the paired colorful images. In this way, valid image regions can be saliently represented and then associated with related text phrases. Although the color reasoning tasks are uncom-plicated for human beings, they require models’ compre-hensive cross-modal understanding to solve them. By using these two sub-tasks, better multimodal representations can be exploited in the main task, image-text matching. Fur-thermore, we propose another “color” reasoning sub-task,
ICf , aiming to complete image features with missing chan-nels using captions, which generalizes the IC task from im-age color channel completion into feature semantic channel completion. Given feature representations of the input im-age, we partially mask some channels, and the caption is utilized to recover them.
In this process, general textual information including colors can be probed and exploited.
Therefore it endows our method the robustness in cases that colors are not the dominant information in captions.
To tackle the ﬁrst sub-task IC, we convert it into a pixel-wise regression problem. The original images are pro-cessed into gray ones as input, and paired captions are used to recover the original images. The TC task can be treated as a Visual Question Answering (VQA) problem, where the question is a sentence with a color word va-In the cancy and the answer is one of candidate colors. image feature channel completion sub-task, we ﬁrst pre-train a feature extractor on the person ID classiﬁcation task, then visual feature maps are masked for recovering using captions. Extensive experiments are conducted on the language-guided person search dataset, CUHK-PEDES
[20]. The proposed method is proved to produce impressive performance improvements. Veriﬁcation on general image-text retrieval datasets also conﬁrms its effectiveness, includ-ing Caltech-UCSD Birds [26], Oxford-102 Flowers [26],
Flickr30k [25], and MSCOCO [21].
In summary, the main contributions of our work include: reasoning sub-tasks, image colorization, text completion, and image feature channel completion.
• Extensive experiments are conducted on the challenging language-guided person search dataset, CUHK-PEDES.
LapsCore proves effective to bring considerable perfor-mance gain and achieves the state-of-the-art results.
• The proposed method is demonstrated as generic to be incorporated into different baselines and bring improve-ments. The effectiveness is also conﬁrmed on other cross-modal retrieval tasks, which is expected to give inspira-tions to other researchers. 2.