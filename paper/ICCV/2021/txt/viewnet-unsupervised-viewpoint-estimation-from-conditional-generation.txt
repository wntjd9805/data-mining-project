Abstract
Understanding the 3D world without supervision is cur-rently a major challenge in computer vision as the annota-tions required to supervise deep networks for tasks in this domain are expensive to obtain on a large scale.
In this paper, we address the problem of unsupervised viewpoint estimation. We formulate this as a self-supervised learning task, where image reconstruction provides the supervision needed to predict the camera viewpoint. Specifically, we make use of pairs of images of the same object at train-ing time, from unknown viewpoints, to self-supervise train-ing by combining the viewpoint information from one image with the appearance information from the other. We demon-strate that using a perspective spatial transformer allows efficient viewpoint learning, outperforming existing unsu-pervised approaches on synthetic data, and obtains com-petitive results on the challenging PASCAL3D+ dataset. 1.

Introduction
Object viewpoint estimation is one of the key compo-nents required to enable autonomous systems to understand the 3D world. Earlier methods [20, 21, 4] were successfully demonstrated to work in controlled environments. While more recent work, benefiting from modern learnable repre-sentations [55, 17, 32], have been shown to help other vision tasks such as object detection and 3D reconstruction [31] and have been deployed in various applications [46, 16, 33].
In this paper, we focus on recovering the 3D pose of an object relative to the camera viewing it from a single image.
This requires an accurate understanding of the 3D structure of the objects depicted in an image, and is can be compli-cated by challenges such as object symmetry. While large-scale image datasets annotated with semantic supervision (e.g. [11, 34]) have been a key enabler for modern deep net-works, obtaining viewpoint annotations can be extremely laborious, expensive, and error-prone. For instance, the an-notation procedure for the commonly used object viewpoint benchmark, PASCAL3D+ [59], required annotators to man-ually select an appropriate 3D CAD model from a pool of
Figure 1: ViewNet learns to extract the camera viewpoint via self-supervised training on a collection of image pairs.
At inference it can estimate viewpoint from a single image. models and click on the 2D locations for a set of prede-fined landmarks for each object instance. Similarly, creat-ing 3D face pose estimation datasets such as [63] involves a time consuming step of fitting morphable 3D face models to 2D face images. As a result, creating large-scale view-point datasets for a diverse set of objects, especially when 3D CAD models are unavailable, is challenging using exist-ing annotation pipelines.
To overcome this problem, we propose a self-supervised object viewpoint estimation method, ViewNet, that learns from an unlabeled (i.e. without ground truth pose) collec-tion of image pairs from a given object category (see Fig-ure 1). Our method exploits multi-view consistency and does not require any manual viewpoint annotations. Our work is inspired by the analysis by synthesis and conditional generation paradigms [61, 54, 35, 37] where we learn to dis-entangle viewpoint and 3D appearance of objects and recon-struct images based on these disentangled factors. To this end, we leverage the information contained in image pairs of the same object with different viewpoints. Such pairs may be generated synthetically or obtained from videos.
Given such an image pair, our method extracts 3D appear-ance from the first image and viewpoint from the second one, and reconstructs the second one based on this factor-ization. At test time, our method can be used to predict the viewpoint relative to the camera of objects from single im-ages. Unlike previous work [56, 23], we are able to leverage supervision directly from pixel values, allowing for more efficient supervision, as well as enabling the generation of images from new viewpoints.
Our main contributions are: (i) A new conditional gener-ation approach for estimating viewpoint from single images via self-supervision. Our model encodes strong geometric consistency, enabling it to accurately generate novel views that can be used to refine its own predictions. (ii) A detailed evaluation of our method on ShapeNet and PASCAL3D+, where we outperform related approaches. (iii) We highlight the limitations of current evaluation procedures by show-ing that a large portion of object instances in certain cate-gories are captured from similar viewpoints rendering their evaluation biased. We also show that the calibration step commonly used to align estimated unsupervised viewpoints with the ground-truth introduces additional biases. 2.