Abstract
Video-based person re-identification aims to associate the video clips of the same person across multiple non-overlapping cameras. Spatial-temporal representations can provide richer and complementary information between frames, which are crucial to distinguish the target per-son when occlusion occurs. This paper proposes a novel
Pyramid Spatial-Temporal Aggregation (PSTA) framework to aggregate the frame-level features progressively and fuse the hierarchical temporal features into a final video-level representation. Thus, short-term and long-term temporal information could be well exploited by different hierar-chies. Furthermore, a Spatial-Temporal Aggregation Mod-ule (STAM) is proposed to enhance the aggregation capa-bility of PSTA. It mainly consists of two novel attention blocks: Spatial Reference Attention (SRA) and Temporal
Reference Attention (TRA). SRA explores the spatial cor-relations within a frame to determine the attention weight of each location. While TRA extends SRA with the cor-relations between adjacent frames, temporal consistency information can be fully explored to suppress the inter-ference features and strengthen the discriminative ones.
Extensive experiments on several challenging benchmarks demonstrate the effectiveness of the proposed PSTA, and our full model reaches 91.5% and 98.3% Rank-1 accu-racy on MARS and DukeMTMC-VID benchmarks. The source code is available at https://github.com/
WangYQ9/VideoReID-PSTA. 1.

Introduction
Person re-identification (ReID) aims to match a particu-lar person from non-overlapping camera views, which is an important technology in many applications, such as video surveillance, tracking, and smart city. However, it is chal-lenging due to many practical obstacles, such as background clutter, blur, occlusion and viewpoint variations.
Recently, image-based person ReID has achieved im-pressive progress [2, 39, 54, 26, 43]. Most of these works
*Corresponding Author
Figure 1. Illustration of various solutions that employ temporal context information to aggregate frame-level features. (a) Trans-ferring information from adjacent frames. (b) Using a global ref-erence to guide the attention of each frame. (c) Fusing frame-level features with a pyramid structure (ours). The green lines indicate the clean features of the target person while the red ones indicate the features interfered by occlusions. The pyramid structure can alleviate the irrelevant feature by aggregating progressively. focus on extracting more discriminative features within a single image. Therefore, it is difficult to retrieve the target person when occlusion occurs, or missing crucial parts. In contrast, the richer spatial-temporal information can allevi-ate the limitation of image-based ReID and is more power-ful to obtain discriminative features and robust results.
Several works [17, 38, 49, 32] have been proposed to en-hance the discriminative features of the target person and suppress the irrelevant features with the help of spatial-temporal context information. Subramaniam et al. [38] pro-pose a co-segmentation module to activate a common set of features across multiple frames. Hou et al. [17] aim to solve the problem of partial occlusion. They use the infor-mation from adjacent frames to reconstruct the occlusion part, thus could alleviate the interference of irrelevant fea-tures. The message passing flow of these methods is shown in Fig. 1 (a). Although the information from the tempo-ral adjacent frames can somewhat suppress the occluded features, it would lose efficacy when long-range occlusion occurs because it lacks the long-term dependence. Yan et
al. [53] propose to learn the attention from a global view by constructing a global reference. The message passing flow of these methods is shown in Fig. 1 (b). Although the refer-ence can capture global information, there is no guarantee that it can represent the target person well. For example, this method may focus on the occlusion parts when most of the frames are occluded. Jiang et al. [21] propose to infer attentions of frame-level features by constructing a relation embedding for each pair of frames in the tracklet. This kind of method fully exchanges the message among the whole tracklet. However, it would suffer from the high redundancy among frames and the large distribution space for relation embedding. On the other hand, it is not computationally efficient to construct all relations of frames.
To alleviate the problems mentioned above, we propose a novel Pyramid Spatial-Temporal Aggregation (PSTA) framework for high-performance video-based person ReID.
Fig. 1 (c) illustrates our basic idea. The adjacent frame-level feature maps are grouped into pairs and then sent into a hier-archical aggregation module. As the process goes on, long-term dependence could be constructed in the later stage without losing the short-term information from the previ-ous stage. It is obvious that after aggregating by our PSTA, the proportions of the clean features are increased. There-fore, the key to improving performance is how to aggregate the adjacent features, such that the fused features can be more discriminative and with less interference from occlu-sions. We argue that a well-designed aggregation module should satisfy two requirements: 1) foreground features can be strengthened with the intra-frame information. 2) fea-tures of the target person can be enhanced, and the non-target information can be suppressed with the inter-frame correlations. We propose a Spatial-Temporal Aggregation
Module (STAM) according to the above two requirements.
More specifically, it consists of two key components: Spa-tial Reference Attention (SRA) and Temporal Reference At-tention (TRA). SRA explores the spatial correlations within a frame to determine the attention weight of each location.
While TRA extends SRA with the correlations between ad-jacent frames, such that temporal consistency information can be fully explored to suppress the interference features and strengthen the discriminative ones.
In summary, the main contributions are as follow:
• We propose a novel Pyramid Spatial-Temporal Aggre-gation (PSTA) framework to aggregate the frame-level features step-by-step, establishing the long-term de-pendence while maintaining the short-term informa-tion effectively and efficiently.
• We propose a novel feature aggregation module (STAM) which considers both intra-frame and inter-frame correlations to suppress the interference features and enhance the discriminative ones.
• Extensive experiments demonstrate that our PSTA achieves state-of-the-art performance on several video-based person ReID benchmarks. Our full model reaches 91.5% and 98.3% Rank-1 accuracies on
MARS and DukeMTMC-VID benchmarks. 2.