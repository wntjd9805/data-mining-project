Abstract
We consider the shuffled linear regression problem where the correspondences between covariates and responses are unknown. While the existing formulation assumes an ideal underlying bijection in which all pieces of data should match, such an assumption barely holds in real-world applications due to either missing data or outliers. Therefore, in this work, we generalize the formulation of shuffled linear regression to a broader range of conditions where only part of the data should correspond. Moreover, we present a remarkably simple yet effective optimization algorithm with guaranteed global convergence. Distinct tasks validate the effectiveness of the proposed method.1 1.

Introduction
Linear regression has always been a powerful tool for parameter estimation problems in science and engineering.
In the conventional form, it is designed to estimate the pa-rameters of a linear system given pairs of matched data. Its objective function is written as2 min x∈X
∥Ax − b∥2 2 , (1) where A ∈ Rn×d (n ≫ d) and b ∈ Rn denote the collec-tions of covariates and responses, respectively, and x is the regression variable belonging to set X .
There also exist cases where the correspondences between
A and b are unknown. For example, in computer vision tasks such as point cloud registration, shape matching, perspective-n-points, and image registration, the regression variable x stands for the rigid transformation, spectral mapping, cam-era projection, and transformation matrices, respectively. In such cases, correspondences among points, vertices, or pix-els are often unknown. Similar situations also appear in other fields, such as data de-anonymization [25] in data sci-ence, artifacts dating [12] in archaeology, and sampling with jitter [28] in signal processing. 1Source code can be found at https://github.com/SILI1994/
Generalized-Shuffled-Linear-Regression. 2Without loss of generality, we assume the Euclidean norm as a metric.
All these issues lead to a recently raised variant of linear regression, namely, the shuffled linear regression (SLR) prob-lem. It aims at simultaneously recovering both correspon-dences between data and the regression variable x [28, 45].
Formally, its objective function is written as min
P∈Π⋆, x∈X
∥PAx − b∥2 2 , (2) where P is an n-dimensional square permutation matrix from the discrete and finite set Π⋆ satisfying
P1 = PT 1 = 1, Pij ∈ {0, 1}, where 1 is the all-one vector, and Pij denotes the ijth en-try of P. Intuitively, constraints (3) bring some plausible properties on the relations between each covariate Ai and response bj: (3)
• Pij ∈ {0, 1} ensures Ai and bj are either absolutely matched or completely independent from each other.
• Together with the binary property, P1 = PT 1 = 1 fur-ther guarantees bijectivity, i.e., each Ai and bj possess exactly one match.
This vanilla formulation of SLR demands exactly the same cardinalities of {Ai} and {bj}, and all of them to be inliers.
However, such requirements are seldom satisfied in practice due to missing data and outliers. Therefore, it is desired to extend SLR to work well for real-world applications.
In this work, we generalize SLR to a broader range of cases where the cardinalities of covariates {Ai} and re-sponses {bj} can be different, and only part of them should match. We denote this generalized setup of SLR as General-ized Shuffled Linear Regression (GSLR). Our contributions are summarized as follows:
• We present GSLR, a generalized formulation of SLR, making it applicable to practical situations, where only parts of the data should correspond.
• We propose a remarkably simple yet effective algorithm with a detailed theoretical analysis for optimization.
• We employ distinct examples to demonstrate how
GSLR can benefit computer vision tasks in achieving state-of-the-art accuracies.
such as iterative closet point (ICP) [3] and its variants, are infeasible algorithms for the SLR problem. In detail, as shown in Fig. 1, their nearest neighbors (NN)-based match-ing strategy may either link a point to multiple queries or leave it unmatched, which consequently violates the one-to-one matching constraint of permutation matrices. As we will show later in Sec. 5, it is this violation that prevents them from achieving stably accurate results. For a solution, some literature proposes to strengthen the bijectivity. For example, both Jing et al. [30] and Szymon [32] propose to symmetrize the registration process; and Gold et al. [14] convert the assignment matrix to a doubly stochastic one to balance the weights of correspondences. However, contrary to our GSLR formulation, these attempts still cannot guarantee strict one-to-one correspondences. There also exist works [38, 39] that cast the matching problem into a linear assignment problem (LAP) by maximizing kernel densities in product spaces, whereas their generalizability is limited for assuming the correspondences between the covariates {Ai} and responses
{bj} to be partial-to-all instead of partial-to-partial.
Robust estimation Robust estimation aims at alleviating the effects of outliers on regression problems. A popular tool to realize so are the M-estimators [36], which propose to either down-weight or completely reject suspected outliers.
The ability of a robust estimator is typically assessed by two criteria, namely, breakdown point and efficiency [48]. Specif-ically, the breakdown point, theoretically upper-bounded by 0.5, demonstrates the proportion of outliers an estimator can tolerate before providing an incorrect result. The efficiency, calculated as the ratio of the theoretically minimal variance provided by the Cramer-Rao bound w.r.t. the actual one of the estimator, stands for the quality of an estimator.
Relation to QAP QAP (e.g., second-order graph match-ing [46]) is another popular problem subject to permutational constraints. However, different from QAP, (G)SLR addition-ally requires to estimate the regression variable x. Thus, converting (G)SLR to QAP is hardly possible unless the optimal x can be explicitly expressed by A, b, and P [45] (e.g., when x ∈ Rd, we can eliminate x from (G)SLR by x = (cid:0)AT PT PA(cid:1)−1
AT PT b). However, in general cases where x ∈ X ̸= Rd, such an elimination can only present coarse results as mentioned in [22]. 3. Formulation and optimization of GSLR
Following the formulation of SLR shown in objective (2), the GSLR problem can be similarly written as min
P∈Π, x∈X
∥PAx − b∥2 2 , (4) where A ∈ Rm×d, b ∈ Rn (m ̸= n in general), and Π is the set of generalized permutation matrices.
Figure 1: NN-based corresponding strategy may violate the one-to-one correspondences by either assigning a single point to multiple queries or leaving points unmatched. 2.