Abstract
Traditional methods for RGB hand mesh recovery usu-ally need to train a separate model for each dataset with the corresponding ground truth and are hardly adapted to new scenarios without the ground truth for supervision. To ad-dress the problem, we propose a self-supervised framework for hand mesh estimation, where we pre-learn hand priors from existing hand datasets and transfer the priors to new scenarios without any landmark annotations. The proposed approach takes binocular images as input and mainly re-lies on left-right consistency constraints including appear-ance consensus and shape consistency to train the model to estimate the hand mesh in new scenarios. We conduct ex-periments on the widely used stereo hand dataset, and the experimental results verify that our model can get compa-rable performance compared with state-of-the-art methods even without the corresponding landmark annotations. To further evaluate our model, we collect a large real binocu-lar dataset. The experimental results on the collected real dataset also verify the effectiveness of our model qualita-tively. 1.

Introduction
Hand mesh recovery from RGB images has always been an important research task in the field of computer vision. It has a wide range of applications, such as virtual reality, hu-man computer interaction, robotics and so on. With the de-velopment of deep learning [6, 8] and the existence of some large hand datasets [19, 11], hand mesh recovery from RGB images has made great progress. However, a challenge of the hand mesh recovery is that many existing methods usu-ally need a large labeled hand dataset [10, 5] for training. In the case of insufficient annotations or even in the most ex-treme case without any landmark annotations in unseen real scenarios, it is very difficult to obtain accurate hand mesh prediction results. In this paper, we are specifically inter-*Corresponding author. ested in self-supervised hand mesh recovery to deal with the extreme case without any landmark annotations in un-seen real scenarios.
It is very challenging to tackle this self-supervised mesh recovery task. Some previous methods generate synthetic datasets [5] for estimating the hand mesh coordinates. But the model trained on the synthetic hand dataset can hardly be adapted to the real data due to the domain gap, which limits their application in the real environment. Other meth-ods more or less need landmark annotations for supervi-sion. They propose several weakly-supervised methods and use 2D hand joints and/or depth map to train their net-work [1, 17, 4]. Since these approaches heavily rely on large-scale 2D pose annotations, they also have limited gen-eralizability when applied to unseen images in new scenar-ios.
Unlike previous methods, our approach is based on the insight in cognitive science that human baby can adapt to a novel concept by correlating it with old concepts with-out receiving an explicit supervision. Therefore, we present a novel self-supervised learning approach that transfer the hand priors learned from past hand estimation tasks into new scenarios and regresses the network parameters of hand mesh prediction with no ground truth landmark available.
In order to preserve certain past experience and adapt to a new unlabeled environment, our approach explores a form of self-supervised objective which learns hand mesh from easily accessible binocular images by left-right consistency without ground truth landmark.
The most crucial objective in this paper is to move away any kind of landmark supervision to improve generalizabil-ity in new scenarios. Since there are already some exist-ing large labeled hand datasets [10, 5], our approach can make better use of these datasets to obtain the initial mesh estimate. For more accurate estimates in an unseen envi-ronment without any landmark annotations, we propose to learn from previous experience and regress the hand mesh vertices with binocular images by left-right consistency.
Both the appearance consensus and shape consistency of self-supervised objectives are carefully designed. As will
be shown in Section 4, our method can accurately recover dense meshes and achieves comparable results to weakly supervised and even some supervised methods. The main contributions of our paper can be summarized as follows: 1) We propose a self-supervised framework for hand mesh recovery. The model is pre-learned from an exist-ing labeled dataset, and then is continually transferred to an unseen environment with binocular images by left-right consistency without any ground truth of 2D / 3D joint coor-dinates, depth maps, and mesh vertex coordinates. 2) We carefully design several self-supervised con-straints to enforce the appearance consensus and the shape consistency. These constraints enable the model to dig into underlying spatial relations in binocular images, so as to generalize the model to new scenarios. 3) Taking binocular RGB images as input, our model can estimate the absolute hand mesh vertex coordinates which is especially useful in virtual reality and robot grasping where 3D absolute coordinates are required. 4) Our model achieves comparable performance with ex-isting weakly supervised and even some supervised meth-ods on the widely used stereo dataset. We also collect a large real binocular dataset and the experimental results on this dataset also verify the effectiveness of the proposed model. 2.