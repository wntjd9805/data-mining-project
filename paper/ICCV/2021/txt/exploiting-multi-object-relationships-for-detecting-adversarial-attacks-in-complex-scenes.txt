Abstract
Vision systems that deploy Deep Neural Networks (DNNs) are known to be vulnerable to adversarial exam-ples. Recent research has shown that checking the intrin-sic consistencies in the input data is a promising way to detect adversarial attacks (e.g., by checking the object co-occurrence relationships in complex scenes). However, ex-isting approaches are tied to specific models and do not of-fer generalizability. Motivated by the observation that lan-guage descriptions of natural scene images have already captured the object co-occurrence relationships that can be learned by a language model, we develop a novel approach to perform context consistency checks using such language models. The distinguishing aspect of our approach is that it is independent of the deployed object detector and yet of-fers very high accuracy in terms of detecting adversarial examples in practical scenes with multiple objects. Exper-iments on the PASCAL VOC and MS COCO datasets show that our method can outperform state-of-the-art methods in detecting adversarial attacks. 1.

Introduction
Deep neural networks (DNNs) are widely used in vision tasks such as object detection and classification, for their ability to achieve state-of-the art (SOTA) performance in such tasks. DNN-based vision systems are also known to be vulnerable to adversarial examples [13, 38, 14, 20, 4, 1, 31]; specifically, it is possible to add (quasi-)imperceptible per-turbations that can cause DNN-based vision systems to out-put incorrect results, while projecting high confidence with regard to the results. For example, adversarial examples can misclassify STOP signs to speed limit signs [10] and a school bus to an ostrich [38].
One promising defense strategy proposed recently is to capture the intrinsic dependencies within the input data, and to check for violations of such dependencies to detect adver-sarial examples. For instance, in scene images with multiple objects, the intrinsic relationships between objects, com-monly known as the context of the scene, can be used to detect adversarial attacks [21]. Similarly, the dependen-cies between video frames can be used to detect adversarial frames in video classification [18, 41]. To illustrate, let us consider the STOP sign attack as an example. A STOP sign is a part of a road intersection scene wherein it typically co-exists with a stop line and/or a street name sign; in contrast, a speed limit sign is rarely, if ever, seen at intersections and thus does not co-exist with the latter objects.
While context has been used extensively for object recognition problems and scene understanding, there is little work with respect to detection of adversarial attacks using context.
In our previous work [21], we proposed model-ing context as a fully connected graph, where each node is an object proposal from a Region Proposal Network (RPN), and edges encode how other regions (including the back-ground and the whole scene) affect the current node in its feature space. Then we train a bank of auto-encoders (each corresponds to a category of objects) to check for consis-tency with respect to the distribution of context features.
While this approach performs well, it is deeply coupled with Faster R-CNN [36] and cannot be applied to single-stage detectors like YOLO [35]; besides, it requires retrain-ing when there is any change to the Faster R-CNN model (e.g., when switching to another CNN model). In summary, while prior approaches have tried to utilize context to detect adversarial attacks, they do so in a way that intricately ties the context to the model in use, which limits their applica-bility.
In this paper, we propose a novel model-agnostic adver-sarial attack detector based on object co-occurrence. Our observation is that the language description of a natural scene image (i.e., of the output of an object detection net-work) can readily capture the dependencies between ob-Figure 1: High-level idea of how our language model-based context consistency check works. First, we use a language model to learn the object co-occurrence context (e.g., bus and signal in the example) from descriptions of scene images. At the test time, we mask off detected objects in the scene description and ask the language model to predict the object based on context (i.e., other objects). By measuring the consistency between the detection results and the prediction results, we assess if the input scene image is adversarial or not. jects. We exploit recent advances in natural language mod-els to learn the dependencies between objects based on co-occurrence and to detect adversarial attacks as violations of the learned context model. Figure 1 depicts the high-level idea of our approach. Given an unknown scene image, we first encode the output of an object detection network into a sentence describing the object co-occurrence relationships (e.g., “bus and signals”). Then we use a trained language model to predict each detected object instance purely based on the context. Finally, we evaluate the context consistency of the scene image by comparing the language model pre-diction the and the detection results. If the results are dif-ferent, we conclude that the input image is adversarial.
The key contributions of our work are as follows.
• To the best of our knowledge we are the first to propose a model-agnostic, context-consistency based approach to detect adversarial perturbations against object detectors.
• We design and realize a language-based model to learn the object co-occurrence relationships in complex scenes, which serves as our novel context model to detect adver-sarial attacks.
• We conduct extensive experiments with three different types of adversarial attacks (misclassification, hiding, and appearing) on two large-scale datasets - PASCAL
VOC [9] and Microsoft COCO [24]. Our method yields high detection performance in all the test cases; the ROC-AUC is over 0.72 in most cases, which is 12-69% higher than a state-of-the-art attack detection method [44] that does not use context and is comparable to (only 5% worse) previous context-inconsistency-based adversarial attack detection approach [21] that is model-dependent (tightly coupled with the Faster R-CNN architecture thus cannot be applied to other architectures, like YOLO). 2.