Abstract
Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-deﬁned properties.
We extend neural radiance ﬁelds (NeRF) to jointly en-code semantics with appearance and geometry, so that com-plete and accurate 2D semantic labels can be achieved us-ing a small amount of in-place annotations speciﬁc to the scene. The intrinsic multi-view consistency and smoothness of NeRF beneﬁt semantics by enabling sparse labels to ef-ﬁciently propagate. We show the beneﬁt of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in var-ious interesting applications such as an efﬁcient scene la-belling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view seman-tic label fusion in visual semantic mapping systems. 1.

Introduction
Enabling intelligent agents, such as indoor mobile robots, to plan context-sensitive actions in their environ-ment requires both a geometric and semantic understanding of the scene. Machine learning methods have proven to be valuable in both geometric and semantic prediction tasks, but the performance of these methods suffers when the dis-tribution of the training data does not match the scenes ob-served at test-time. Though the issue can be mitigated by gathering costly annotated data or semi-supervised learn-ing, it is not always feasible in open-set scenarios with var-ious known and unknown classes. For this reason, it is ad-vantageous to have methods that can self-supervise. In par-ticular, there has been recent success in using scene-speciﬁc methods (e.g. NeRF [16]) that represent the shape and ra-diance of a single scene with a neural network trained from scratch using only images and associated camera poses.
Semantic scene understanding means attaching class la-Figure 1: Neural radiance ﬁelds (NeRF) jointly encoding appearance and geometry contain strong priors for segmen-tation and clustering. We build upon this to create a scene-speciﬁc 3D semantic representation, Semantic-NeRF, and show that it can be efﬁciently learned with in-place super-vision to perform various potential applications. bels to a geometric model. The tasks of estimating the geometry of a scene and predicting its semantic labels are strongly related, as parts of a scene that have similar shape are more likely to belong to the same semantic category than those which differ greatly. This has been shown in work on multi-task learning [9, 33], where networks that simultane-ously predict both shape and semantics perform better than when the tasks are tackled separately.
Unlike scene geometry, however, semantic classes are a human-deﬁned concept and it is not possible to semanti-cally label a novel scene in a purely self-supervised man-ner. The best that could be achieved would be to cluster self-similar structures of a scene into categories; but some labelling would always be needed to associate these clusters with human-deﬁned semantic classes.
In this paper, we show how to design a scene-speciﬁc network for joint geometric and semantic prediction and train it on images from a single scene with only weak se-mantic supervision (and no geometric supervision). Be-cause our single network must generate both geometry and
semantics, the correlation between these tasks means that semantics prediction can beneﬁt from the smoothness, co-herence and self-similarity learned by self-supervision for geometry. In addition, multi-view consistency is inherent to the training process and enables the network to produce accurate semantic labels of the scene, including for views that are substantially different from any in the input set.
Our system takes as input a set of RGB images with as-sociated known camera poses. We also supply some partial or noisy semantic labels for the images, such as ground truth labels for a small fraction of the images, or noisy or coarse label maps for a higher number of images. We train our net-work to jointly produce implicit 3D representations of both the geometry and semantics for the whole scene.
We evaluate our system both quantitatively and qualita-tively on scenes from the Replica dataset [28], and quali-tatively on real-world scenes from the ScanNet dataset [3].
Generating dense semantic labels for a whole scene from partial or noisy input labels is important for practical appli-cations, like when a robot encounters a new scene and either only a small amount of in-situ labelling is feasible, or only an imperfect single-view network is available. 2.