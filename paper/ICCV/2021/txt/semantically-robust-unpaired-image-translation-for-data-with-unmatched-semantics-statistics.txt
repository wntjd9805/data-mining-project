Abstract
Many applications of unpaired image-to-image transla-tion require the input contents to be preserved semantically during translations. Unaware of the inherently unmatched semantics distributions between source and target domains, existing distribution matching methods (i.e., GAN-based) can give undesired solutions. In particular, although pro-ducing visually reasonable outputs, the learned models usu-ally ﬂip the semantics of the inputs. To tackle this without using extra supervisions, we propose to enforce the trans-lated outputs to be semantically invariant w.r.t. small per-ceptual variations of the inputs, a property we call “seman-tic robustness”. By optimizing a robustness loss w.r.t. multi-scale feature space perturbations of the inputs, our method effectively reduces semantics ﬂipping and produces transla-tions that outperform existing methods both quantitatively and qualitatively. 1.

Introduction
Recently, unpaired image-to-image translation [11] has been very popular in the computer vision community. Due to its general assumptions on the inputs (unlabelled im-ages collected from different domains) and the easy ac-cessibility of training data (does not use paired images), it is widely used in ﬁelds such as image manipulations, style transfer, domain adaptation, data augmentation, etc.
[13, 58, 43, 22, 28, 9, 30, 41, 45, 2]. On the other hand, unpaired image translation remains a very challenging task owing to its unsupervised learning nature. Without paired images that specify the exact domain mapping, one has to rely on visual cues to perform distribution matching (i.e., via GANs [16]). Existing GAN-based methods all rely on the adversarial loss that aims to optimally align image statistics between translation images and the target ones (in
*Co-ﬁrst authors; work partly done during an internship at X. Code available at https://github.com/SeanJia/SRUNIT.
Figure 1: The class distributions in GTA vs. Cityscapes.
During unpaired image translation, the generator has to ﬂip the inputs’ semantics to match the target distributions. In-stances from over-represented semantic classes in the source domain (e.g., sky) can be ﬂipped to those from underrepre-sented classes (e.g., vegetation). (top) Conceptually, forcing the distribution of
Figure 2: translated images to match the target one causes seman-tics (the different colored shapes) of the input images to get
ﬂipped. (bottom) An example of semantics ﬂipping (high-lighted in red boxes) from the GTA to Cityscapes task. the marginal sense). However, what if the two distributions should not be the same? In fact, the underlying distributions of semantics from the two domains are usually different, let alone the image distribution of translated images and target one. We call this the unmatched semantics statistics prob-lem, which is under-explored yet both critical and common for unpaired image translation tasks. 1
Similar to the language translation, the semantics of an image should be preserved during translations. For in-stance, in the GTA to Cityscapes dataset [12], while trees look different across domains, their identity/semantics re-mains the same. In the Horse to Zebra task [11], a horse or zebra remains in the class Equus, not turning into a shack. Consider the translation as a two-stage process:
ﬁrstly project an image from one domain to the shared semantics space, and then project it to the other domain.
When the source and target images are projected to the same semantic space and have different distributions in that space, we say the data have unmatched semantics statistics.
Unpaired data from different domains generally have un-matched semantics statistics, unless they are very carefully constructed. For example, in the Horse to Zebra dataset, there are more zebras than horses; in the GTA to Cityscapes dataset, more trees in Cityscapes than in GTA (see Fig. 1). Given unmatched semantics statistics, forcibly match-ing distributions between the translated and the target im-ages can only give spurious solutions, where semantics get
ﬂipped only to match the target semantics statistics (see Fig. 2 for an example). In Sec. 5, we demonstrate that semantics
ﬂipping is a critical and common issue in various GAN-based unpaired image translation frameworks.
There are a few direct attempts at preserving the seman-tics during translations and thus reducing ﬂipping. How-ever, they either require extra supervision or pre-trained models [22, 51] or are too restrictive (dataset-speciﬁc) and prone to artifacts [7, 60, 56].
In this paper, we propose to tackle this problem by encouraging that, during im-age translations, perceptually similar contents should be mapped to contents with high semantic similarity. We call this property of the mapping as semantic robustness.
In essence, semantic robustness ensures a consistent map-ping that prevents the semantics of the inputs from be-ing ﬂipped easily. Speciﬁcally, based on the recently pro-posed framework CUT [40], we propose a semantic robust-ness loss w.r.t. multi-scale feature space perturbations of the input images. We call our method SRUNIT (Semanti-cally Robust Unpaired Image Translation) and empirically demonstrate its effectiveness in reducing semantics ﬂipping.
SRUNIT outperforms existing GAN-based methods both qualitatively and quantitatively on several common datasets. 2.