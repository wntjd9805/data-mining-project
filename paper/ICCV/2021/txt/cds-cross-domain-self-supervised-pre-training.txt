Abstract
We present a two-stage pre-training approach that im-proves the generalization ability of standard single-domain pre-training. While standard pre-training on a single large dataset (such as ImageNet) can provide a good initial rep-resentation for transfer learning tasks, this approach may result in biased representations that impact the success of learning with new multi-domain data (e.g., different artis-tic styles) via methods like domain adaptation. We propose a novel pre-training approach called Cross-Domain Self-supervision (CDS), which directly employs unlabeled multi-domain data for downstream domain transfer tasks. Our approach uses self-supervision not only within a single do-main but also across domains. In-domain instance discrim-ination is used to learn discriminative features on new data in a domain-adaptive manner, while cross-domain matching is used to learn domain-invariant features. We apply our method as a second pre-training step (after ImageNet pre-training), resulting in a significant target accuracy boost to diverse domain transfer tasks compared to standard one-stage pre-training. 1.

Introduction
Real-world image data can come from many sources: different weather, viewpoints, lighting, artistic styles, etc.
Therefore, many tasks require visual representations that generalize across multiple domains. For example, domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain [32, 12]. Cross-domain image retrieval aims to match semantically related images regardless of domain shift (e.g., see Fig. 1-(b, c)).
Pre-training has been very effective for deep neural net-works across many visual tasks, providing strong initial representations [21, 7]. Typically, prior work pre-trains a model on a large-scale supervised auxiliary domain (mostly on ImageNet [31]) and assumes the learned features are a good starting point for downstream tasks. However, Im-ageNet pre-training learns biased representations [13] and suffers from domain shift caused by changes in background,
Figure 1: Top: Two-stage pre-training for domain transfer meth-ods. To learn discriminative and domain-invariant features on downstream domains, we propose Cross-Domain Self-supervised pre-training (CDS) by leveraging unlabeled data from multiple do-mains. Bottom: An application of CDS to unsupervised cross-domain image retrieval. CDS learns a better semantic relationship across domains compared to ImageNet pre-training. rotation, and viewpoints [1]. This suggests that pre-training on a single domain does not encourage domain-invariant features and may not be a good match for downstream tasks encountering new domains (e.g., Fig. 1-(b), domain adapta-tion). In this paper, we address the problem of pre-training representations that are robust to domain shift and useful for downstream methods that operate on multiple domains.
We propose a two-stage pre-training approach to im-prove standard ImageNet pre-training with respect to gen-eralization to new domains for downstream tasks. After the standard pre-training on a generic supervised dataset (e.g.,
ImageNet), we add a second self-supervised pre-training stage that uses unlabeled downstream data from multiple domains as illustrated in Fig. 1-(a). Our second pre-training stage ensures that the representation gains discriminative power on the new domains and invariance to domain shift.
After this two-stage pre-training, the representation can be used directly for tasks like cross-domain image retrieval or used to initialize a model for existing transfer methods (e.g., train with both labeled source and unlabeled target data).
We compare our two-stage pre-training with standard one-stage pre-training and show significant gains across mul-tiple tasks and methods. For example, Fig. 1-(c) shows that our method is better at learning class-semantic similar-ity across new domains compared to ImageNet pre-training (Fig. 1-(b)) and improves cross-domain image retrieval.
Self-supervised learning (SSL) has been shown to be very effective for pre-training on unlabeled data. SSL solves pre-text tasks such as predicting rotation [14] or in-stance discrimination [42, 7]. However, state-of-the-art SSL (e.g., [42, 7, 16]) focuses on learning from a single do-main. Naively adapting SSL to multiple domains cannot learn domain-invariant representations as the images of the same class across domains can have different visual charac-teristics, as we will show in our experiments.
To address the issue, we propose a new pre-training method called Cross-Domain Self-supervision (CDS) that overcomes the limitations of the prior single-domain SSL methods. CDS effectively learns the relationship between domains using unlabeled data (i.e., unsupervised). Specifi-cally, we devise two types of self-supervision to extract dis-criminative1 and domain-invariant features across domains.
First, we propose in-domain instance discrimination. This is motivated by recent SSL [42, 7], but we apply it in a do-main adaptive manner to learn discriminative features in each domain. Second, we propose cross-domain match-ing. This objective matches each sample to a neighbor in the other domain while forcing it to be far from unmatched samples. While in-domain instance discrimination encour-ages a model to learn discriminative features by separating every instance within a domain, the cross-domain match-ing enables better knowledge transfer across domains by performing domain alignment. We hypothesize that such pre-training optimized for downstream multi-domain data can gain domain-invariance and discriminability to new do-mains.
CDS is applicable to a variety of domain transfer tasks encountering new domains, where domain-invariant repre-sentations across downstream multi-domain should be con-sidered. We present three tasks to evaluate SSL baselines: (1) unsupervised cross-domain image retrieval, (2) univer-sal domain adaptation, and (3) few-shot domain adaptation.
In our experiments, we show CDS improves various do-main transfer methods by providing a better pre-training approach that outperforms the existing state-of-the-art SSL methods.
In summary, our work has the following contributions: 1. We present two-stage pre-training to improve the gen-1This term refers to instance-level discriminative representations [42]. eralization ability of the standard single-stage pre-training for downstream multi-domain tasks. 2. We propose novel a Cross-Domain Self-supervised pre-training, which learns discriminative and domain-invariant features using unlabeled multi-domain data. 3. We show that CDS outperforms standard ImageNet pre-training and state-of-the-art SSL baselines on var-ious domain transfer tasks. 2.