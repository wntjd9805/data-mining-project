Abstract
Multi-view pedestrian detection aims to predict a bird’s eye view (BEV) occupancy map from multiple camera views.
This task is confronted with two challenges: how to estab-lish the 3D correspondences from views to the BEV map and how to assemble occupancy information across views.
In this paper, we propose a novel Stacked HOmography
Transformations (SHOT) approach, which is motivated by approximating projections in 3D world coordinates via a stack of homographies. We first construct a stack of trans-formations for projecting views to the ground plane at dif-ferent height levels. Then we design a soft selection mod-ule so that the network learns to predict the likelihood of the stack of transformations. Moreover, we provide an in-depth theoretical analysis on constructing SHOT and how well SHOT approximates projections in 3D world coordi-nates. SHOT is empirically verified to be capable of esti-mating accurate correspondences from individual views to the BEV map, leading to new state-of-the-art performance on standard evaluation benchmarks. 1.

Introduction
Multi-view detection, a.k.a. multi-camera detection [20, 1], aims to detect objects from a set of synchronized images from different sensing viewpoints of a scene. Compared with the single view detection, multi-view object detection enables to aggregate the information across multiple view-points and infer 3D structures of the scene [8, 5], hence is generally robust to occlusions, which is a major challenge to single-view detection in crowded scenes. In this paper, as shown in Figure 1, we focus on detecting pedestrians from multi-view images, where the input is a batch of images from different viewpoints and the output is an occupancy map from the bird’s eye view (BEV) of the plane.
Estimating the occupancy map from a set of multi-view images is challenging in two aspects. First, due to the change of view point, occlusions and ambiguities in object appearances often present in different views, therefore it is not a trivial problem to match the features of pedestrians ac-Figure 1. The task of multi-view pedestrian detection: Given a batch of synchronized images captured from different view angles, our goal is to predict an occupancy map of the scene. curately among input views. Second, even if the correspon-dences are adequately estimated, one view only provides an incomplete representation for the whole scene, bringing dif-ficulties in assembling the knowledge of occupancy across all views. For example, due to occlusions, an area may be only visible in one view, so we have to identify that view and exclude the distractions from other views according to the pre-established correspondences.
To optimize the correspondence and feature extraction jointly in an end-to-end manner, recent work [13, 12] pro-posed to project the features extracted from 2D images to a shared space for aggregating information from all views, while keep the framework differentiable. However, previ-ous works either project the features to large 3D grids [13], or only project the features to the ground plane [12]. The full 3D projection proposed by [13] is expensive since 3D convolutions are involved in dealing with the projected fea-tures. Meanwhile, 2D projection used in [12] is not accurate due to misalignments.
In this paper, for the first challenge of establishing the 3D correspondence, we propose to project the feature maps onto different height levels according to different semantic parts of the pedestrian. As illustrated in Figure 2, our mo-Figure 2. Illustration of different projection schemes: (a) the proposed stacked homography transformations (SHOT) approximates 3D projection with a stack of homographies; (b) 3D projection proposed in [13] project 2D feature points to 3D grids; (c) 2D projection proposed in [12] projects 2D feature onto the 2D ground plane. Our method achieves a better tradeoff between projection efficiency and accuracy than the other two schemes. tivation is that each pixel should be projected to a ground plane at a proper height. To achieved this, we construct a stack of homography projections, which are H0, H1 and
H2 in the figure.
For the second challenge, assembling occupancy infor-mation across views, we design a soft selection module to ensure the network differentiable, thus learning how to aggregate the occupancy information end-to-end. Specifi-cally, for each pixel of the features extracted from individ-ual views, we design a likelihood map prediction module to softly select projections from the stack of transformations.
Since each pixel is projected with a stack of homography transformations, our method is named Stacked HOmogra-phy Transformations (SHOT).
Intuitively, SHOT can be viewed as an approximation of a 3D projection with a stack of homographies. Then we theoretically analyze the properties of SHOT in two as-pects: (1) the requirements of acquiring the pre-computed homographies and (2) the requirements of serving as a 3D projection. SHOT achieves the state-of-the-art performance of 90.2% MODA on WILDTRACK and 88.3% MODA on
MultiviewX, which outperforms the recent method [12] by 2% on WILDTRACK and 4.2% on MultiviewX, respec-tively. Moreover, we investigate the performance of SHOT under a practical yet challenging setting: both the scene and the camera locations are distinctly different between train-ing and testing. The results validate the generalization abil-ity of SHOT. To sum up, our contributions are as follows:
• We propose a novel stacked homography transfor-mations (SHOT) to establish accurate 3D correspon-dences between individual input views and the BEV occupancy map.
• We theoretically analyze the geometry property of
SHOT and demonstrate two properties: 1) The stack of transformations can be effectively constructed without knowing extrinsic parameters; 2) SHOT can project all human body parts to the same grid on the BEV map with proper hyperparameters.
• We conduct experiments on standard benchmarks and achieve new state-of-the-art results. Moreover, we in-vestigate the performance of SHOT under a new chal-lenging setting: training and testing involve different scenes and camera viewpoints. 2.