Abstract
By estimating 3D shape and instances from a single view, we can capture information about an environment quickly, without the need for comprehensive scanning and multi-view fusion. Solving this task for composite scenes (such as object stacks) is challenging: occluded areas are not only ambiguous in shape but also in instance segmenta-tion; multiple decompositions could be valid. We observe that physics constrains decomposition as well as shape in occluded regions and hypothesise that a latent space learned from scenes built under physics simulation can serve as a prior to better predict shape and instances in oc-cluded regions. To this end we propose SIMstack, a depth-conditioned Variational Auto-Encoder (VAE), trained on a dataset of objects stacked under physics simulation. We for-mulate instance segmentation as a centre voting task which allows for class-agnostic detection and doesn’t require set-ting the maximum number of objects in the scene. At test time, our model can generate 3D shape and instance seg-mentation from a single depth view, probabilistically sam-pling proposals for the occluded region from the learned latent space. Our method has practical applications in providing robots some of the ability humans have to make rapid intuitive inferences of partially observed scenes. We demonstrate an application for precise (non-disruptive) ob-ject grasping of unknown objects from a single depth view. 1.

Introduction
While humans are intuitively able to interpret partially observed scenes using geometric reasoning and prior expe-rience, estimating 3D shape from RGB or depth images is challenging in computer vision due to ambiguity — many 3D shapes can explain a 2D observation. The classical ap-proach to generate 3D shapes from depth images involves
*Research presented in this paper has been supported by Dyson
Technology Ltd.
†1Zoe Landgraf, Raluca Scona, Tristan Laidlow, Stephen James and Andrew J. Davison are with the Dyson Robotics Labora-tory, Department of Computing, Imperial College London, UK. zoe.landgraf15@imperial.ac.uk
‡2Stefan Leutenegger is with the Smart Robotics Lab, Department of
Computing, Imperial College London, UK.
Figure 1: Top to bottom: SIMstack outputs object shapes and instances from a single depth view on two YCB sequences and two real data examples, one with a fully occluded object supporting a leaning box (3rd row), for which our model predicts a plausible proposition. Right: Grasping demo setup (green: target object). taking images from all sides of an object and fusing the re-projected points into a common 3D representation such as a truncated signed distance function (TSDF) [29]. How-ever, apart from the exhaustive nature of the task, it is often impossible to reach all required viewpoints to generate a watertight surface reconstruction [12].
This drawback has led researchers to explore learning based approaches to reconstruct 3D shapes, such as learn-ing to complete partial reconstructions [12, 11, 40] and pre-dicting scenes [39] or objects [52, 51] from a single depth image. In parallel, researchers explore 3D shape prediction for scenes [30, 15, 38] and objects [9, 49, 53] from single or multi-view RGB data. While most of these approaches work with pointclouds, voxel occupancy grids or TSDF rep-resentations, some have explored alternative representations such as parametric surface elements [16], 2D sketches [45], graph neural networks [13], and the increasingly popular implicit neural surface representations [31, 26, 5].
To our knowledge, no existing work has explored 3D shape prediction with instance segmentation for multiple objects from a single depth image. Single view shape pre-diction allows to quickly estimate 3D occupancy, while in-stance segmentation is crucial for interactive tasks such as
object manipulation. We aim to solve this task for a tabletop scene with a variable number of stacked household objects and propose a VAE whose latent space is learned from sta-ble (under physics simulation) scenes, with the aim to better reason about shape and instance decomposition in occluded regions. Conditioned on depth, the VAE learns to predict 3D shape and instances given a depth view, completing oc-cluded regions using its latents, which can be thought of as an ‘intuitive physics’ prior. Our refinement method further optimises the reconstruction for collision-free decomposi-tion. We aim to reconstruct scenes composed of unknown objects and train our model on randomly assembled piles of 3D parametric shapes (superquadrics). At test time, our conditional VAE (C-VAE) generates 3D shape and instance segmentation, as well as realistic reconstruction proposals for occluded regions in one forward pass. Our method can also integrate multiple views for improved reconstruction: the VAE can be conditioned on multiple views and our la-tent space can be further optimised against novel views us-ing differentiable rendering. We show an application of our method for non-disruptive grasping using a robot arm.
In summary, our contributions are:
• A depth-conditioned VAE for scenes of stacked ob-jects which can generate 3D shape, instance segmenta-tion and probabilistic reconstruction proposals for oc-cluded regions.
• A center voting scheme based on 3D Hough Voting allowing for class-agnostic 3D instance segmentation for scenes with an arbitrary number of objects.
• A shape refinement procedure to generate a compact scene representation of parametric shapes for down-stream applications. 2.