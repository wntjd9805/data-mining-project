Abstract
Event cameras are ideally suited to capture HDR vi-sual information without blur but perform poorly on static or slowly changing scenes. Conversely, conventional im-age sensors measure absolute intensity of slowly chang-ing scenes effectively but do poorly on high dynamic range or quickly changing scenes.
In this paper, we present an event-based video reconstruction pipeline for High Dy-namic Range (HDR) scenarios. The proposed algorithm in-cludes a frame augmentation pre-processing step that de-blurs and temporally interpolates frame data using events.
The augmented frame and event data are then fused using a novel asynchronous Kalman ﬁlter under a unifying un-certainty model for both sensors. Our experimental results are evaluated on both publicly available datasets with chal-lenging lighting conditions and fast motions and our new dataset with HDR reference. The proposed algorithm out-performs state-of-the-art methods in both absolute intensity error (48% reduction) and image similarity indexes (aver-age 11% improvement). 1.

Introduction
Event cameras offer distinct advantages over conven-tional frame-based cameras: high temporal resolution, high dynamic range (HDR) and minimal motion blur [24]. How-ever, event cameras provide poor imaging capability in slowly varying or static scenes, where despite some efforts in ‘gray-level’ event cameras that measure absolute inten-sity [35, 6], most sensors predominantly measure only the relative intensity change. Conventional imaging technol-ogy, conversely, is ideally suited to imaging static scenes and measuring absolute intensity. Hybrid sensors such as the Dynamic and Active Pixel Vision Sensor (DAVIS) [4] or custom-built systems [53] combine event and frame-based cameras, and there is an established literature in video reconstruction fusing conventional and event camera data
[43, 32, 31, 53]. The potential of such algorithms to en-hance conventional video to overcome motion blur and in-crease dynamic range has applications from robotic vision (a) Input LDR Image (b) E2VID [39] (c) CF [43] (d) Our AKF Reconstruction
Figure 1. An example with over exposure and fast camera mo-tion causing blur taken from the open-source event camera dataset
IJRR [29]. Image (a) is the low dynamic range (LDR) and blurry input image.
Image (b) is the result of state-of-the-art method
E2VID [39] (uses events only). Image (c) is the result of ﬁlter-based image reconstruction method CF [43] that fuses events and frames. Our AKF (d) generates sharpest textured details in the overexposed areas. systems (e.g., autonomous driving), through ﬁlm-making to smartphone applications for everyday use. (AKF)
In this paper, we propose an Asynchronous Kalman
Filter to reconstruct HDR video from hybrid event/frame cameras. The key contribution is based on an explicit noise model we propose for both events and frames. This model is exploited to provide a stochastic framework in which the pixel intensity estimation can be solved using an Extended Kalman Filter (EKF) algorithm
[17, 18]. By exploiting the temporal quantisation of the event stream, we propose an exact discretisation of the EKF equations, the Asynchronous Kalman Filter (AKF), that is computed only when events occur. In addition, we propose a novel temporal interpolation scheme and apply the estab-lished de-blurring algorithm [31] to preprocess the data in a step called frame augmentation. The proposed algorithm demonstrates state-of-the-art hybrid event/frame image re-construction as shown in Fig. 1.
We compare our proposed algorithm with the state-of-the-art event-based video reconstruction methods on the popular public datasets ACD [43], CED [46] and IJRR [29] with challenging lighting conditions and fast motions.
However, existing public datasets using DAVIS event cam-eras do not provide HDR references for quantitative evalu-ation. To overcome this limitation, we built a hybrid sys-tem consisting of a high quality RGB frame-based camera mounted alongside a pure event camera to collect high qual-ity events, and HDR groundtruth from multiple exposures taken from the RGB camera. Thus, we also evaluate the qualitative and quantitative performance of our proposed al-gorithm on our proposed HDR hybrid event/frame dataset.
Our AKF achieves superior performance to existing event and event/frame based image reconstruction algorithms.
In summary, our contributions are:
•
•
•
•
An Asynchronous Kalman Filter (AKF) for hybrid event/frame HDR video reconstruction
A unifying event/frame uncertainty model
Deblur and temporal interpolation for frame augmen-tation
A novel real-world HDR hybrid event/frame dataset with reference HDR images and a simulated HDR dataset for quantitative evaluation of HDR perfor-mance. 2.