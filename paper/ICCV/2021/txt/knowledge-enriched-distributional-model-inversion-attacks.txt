Abstract
Model inversion (MI) attacks are aimed at reconstruct-ing training data from model parameters. Such attacks have triggered increasing concerns about privacy, espe-cially given a growing number of online model repositories.
However, existing MI attacks against deep neural networks (DNNs) have large room for performance improvement. We present a novel inversion-specific GAN that can better distill knowledge useful for performing attacks on private models from public data. In particular, we train the discriminator to differentiate not only the real and fake samples but the soft-labels provided by the target model. Moreover, unlike previous work that directly searches for a single data point to represent a target class, we propose to model a private data distribution for each target class. Our experiments show that the combination of these techniques can signif-icantly boost the success rate of the state-of-the-art MI at-tacks by 150%, and generalize better to a variety of datasets and models. Our code is available at https://github. com/SCccc21/Knowledge-Enriched-DMI. 1.

Introduction
Many attractive applications of machine learning (ML) techniques involve training models on sensitive and propri-etary datasets. One major concern for these applications is that models could be subject to privacy attacks and re-veal inappropriate details of the training data. One type of privacy attacks is MI attacks, aimed at recovering training data from the access to a model. The access could either be black-box or white-box. In the blackbox setting, the at-tacker can only make prediction queries to the model, while in the whitebox setting, the attacker has complete knowl-edge of the model. Given a growing number of online plat-forms where users can download entire models, such as
Tensorflow Hub1 and ModelDepot2, whitebox MI attacks
*Correspondence to G.-J. Qi, guojunq@gmail.com 1https://www.tensorflow.org/hub 2https://modeldepot.io/ have posed an increasingly serious threat to privacy.
Effective MI attacks have been mostly demonstrated on simple models, such as linear models, and low-dimensional feature space [5, 4]. MI attacks are typically cast as an opti-mization problem that seeks for the most likely input exam-ples corresponding to a target label under the private model.
When the target model is a DNN, the underlying attack op-timization problem becomes intractable and solving it via gradient methods in an unconstrained manner may easily end in a local minima. Previous MI attack models like [31] explore the idea of distilling a generic prior from potential public data via a GAN generator and using it to guide the inversion process. For instance, to attack a face recogni-tion classifier trained on private face images, one can train a GAN with public face datasets to learn generic statistics of real face images and then solving the attack optimization over the latent space of the GAN rather than in an uncon-strained ambient space.
However, there still exists a large room to improve the attack performance. For instance, the top-one identification accuracy of face images inverted from the state-of-the-art face recognition classifier is 45%. A natural question is: Is the underperformance of MI attacks against DNNs because
DNNs do not memorize much about private data or it is sim-ply an artifact of imperfect attack algorithm design? This paper shows that it is the latter.
We reveal a variety of drawbacks associated with the the current MI attacks against DNNs. Particularly, we notice that the previous state-of-the-art approach suffers from the two key limitations: 1) The information about private clas-sifier is not sufficiently explored for distilling knowledge from public data. Previous works ignore the important role of the target classifier in adapting the knowledge distilled from the public data for training the MI attack model on the target classifier. Indeed, given a target classifier to attack, we can also use its output labels to distill which public data are more useful in inverting the target model to recover the private training examples of the given labels. 2) Prior works made a simplified one-to-one assumption in recovering a single example for a given label of the target model. How-ever, in real scenarios, inverting a model should naturally result in a distribution of training examples corresponding to the given label. This inspires us to recover a data dis-tribution in the MI attack in line with such a many-to-one assumption.
To address the first limitation, we propose to tailor the training objective of the GAN to the inversion task. Specif-ically, for the discriminator, we propose to leverage the tar-get model to label the public dataset and train the discrimi-nator to differentiate not only the real and fake samples but also the labels. This new training scheme will force the generator to retain image statistics that are more relevant to infer the classes of the target model, which are likely to oc-cur in the unknown private training data. To overcome the second limitation, we propose to explicitly parameterize the private data distribution and solve the attack optimization over the distributional parameters. Moreover, this will lead us to explore a distribution in which each point with large probability mass will achieve a good attack performance.
We perform experiments on various datasets and network architectures and show that such a distributional MI attack by distilling public-domain knowledge tailored for private labels can significantly improve the previous state-of-the-art attack against DNNs, even when the public data have no overlap with the private labels of the target network.
The paper is organized as follows. We introduce related works in Section 2 and describe our proposed inversion-specific GAN and distributional recovery in Section 3.
In Section 4, we assess the performance of the proposed method and show the extend application to a new attack setting: multi-target MI attacks. Finally, we conclude and discuss our key findings in Section 5. 2.