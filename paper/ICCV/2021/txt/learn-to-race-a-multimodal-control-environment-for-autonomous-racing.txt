Abstract
Existing research on autonomous driving primarily fo-cuses on urban driving, which is insufficient for charac-terising the complex driving behaviour underlying high-speed racing. At the same time, existing racing simula-tion frameworks struggle in capturing realism, with respect to visual rendering, vehicular dynamics, and task objec-tives, inhibiting the transfer of learning agents to real-world contexts. We introduce a new environment, where agents Learn-to-Race (L2R) in simulated competition-style racing, using multimodal informationâ€”from virtual cameras to a comprehensive array of inertial measure-ment sensors. Our environment, which includes a sim-ulator and an interfacing training framework, accurately
In this models vehicle dynamics and racing conditions. paper, we release the Arrival simulator for autonomous racing. Next, we propose the L2R task with challenging metrics, inspired by learning-to-drive challenges, Formula-style racing, and multimodal trajectory prediction for au-tonomous driving. Additionally, we provide the L2R frame-work suite, facilitating simulated racing on high-precision models of real-world tracks. Finally, we provide an offi-cial L2R task dataset of expert demonstrations, as well as a series of baseline experiments and reference implementa-tions. We make all code available: https://github. com/learn-to-race/l2r.
Figure 1: Learn-to-Race interfaces with a racing simulator, which features numerous real-world racetracks such as the Thrux-ton Circuit (top-left) and Las Vegas Motor Speedway (top-right).
Simulated race cars (bottom) are empowered with learning agents, tasked with the challenge of learning to race for the fastest lap-times and best metrics. 1.

Introduction
Progress in the field of autonomous driving relies on the existence of challenging tasks and well-defined evalua-*Equal contribution. tion metrics, which enable researchers to effectively assess and improve algorithms. Models developed in learning-to-drive settings continue to struggle with issues in sample-complexity, safety, and unseen generalisation, calling for
more suitable benchmarks [9, 16, 28]. We hypothesise that high-fidelity simulation environments, together with well-defined metrics and evaluation procedures, are conducive to developing more sophisticated agents; and, in turn, such agents will be better-suited to real-world deployment.
Simulated autonomous racing exhibits task complexity on several factors: (i) agents must perform real-time deci-sion making, requiring computationally-efficient policy up-dates as well as robustness to latency; (ii) agents must be able to deal with realistic vehicle and environmental dynam-ics (whereas agents in less-realistic environments have been able to achieve super-human performance); (iii) agents must leverage more informative intrinsic reward schemes that en-able replication of human-like driving behaviour, e.g., trad-ing off safety and performance; and (iv) agents must use offline demonstrations effectively, without overfitting, and must leverage interactions with the environment sample-efficiently. We highlight simulated racing (Figure 1) as an opportunity for developing learning strategies that are capa-ble of meeting these stringent requirements.
In this work, we release the Arrival Autonomous Rac-ing Simulator, which includes numerous interfaces for both simulated and real vehicle instrumentation. Furthermore, we introduce Learn-to-Race (L2R), a multimodal and continuous control environment for training and evaluat-ing autonomous racing agents. Through the L2R envi-ronment, we simulate competition-style racetracks that are based off real-world counterparts, we provide mechanisms for fully-characterising realistic racing agents (e.g., flexible sensor placements, multimodal cameras, and various ve-hicle dynamics profiles), and we provide numerous tools for fine-grained agent evaluation (e.g., random and fixed spawn locations, custom racing map construction, and in-jection of external disturbances). Using these facilities, we enable research in problems that require agents to make safety-critical, sub-second decisions in dynamically unsta-ble contexts, such as autonomous racing, real-time uncer-tainty analysis in highway driving, and trajectory forecast-ing. In this paper, we exemplify algorithm development and benchmarking of methods under learning from demonstra-tions, reinforcement learning, and model-predictive control.
Concretely, our contributions include: (i) the Arrival Au-tonomous Racing Simulator, which simulates high-fidelity competition-style tracks, vehicles, and various sensor sig-nals; (ii) the Learn-to-Race (L2R) framework, a plug-and-play environment, which defines interfaces for vari-ous sensor modalities and provides an OpenAI-gym com-pliant training and testing environment for learning-based agents; (iii) an official L2R task and dataset with expert demonstrations, metrics, and reference evaluation proce-dures; and (iv) an academic release of the simulator, code for the L2R framework, and implementations of baseline agents to facilitate full reproducibility and extension. 2.