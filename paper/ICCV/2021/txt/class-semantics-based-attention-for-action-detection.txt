Abstract
Action localization networks are often structured as a feature encoder sub-network and a localization sub-network, where the feature encoder learns to transform an input video to features that are useful for the local-ization sub-network to generate reliable action proposals.
While some of the encoded features may be more useful for generating action proposals, prior action localization approaches do not include any attention mechanism that enables the localization sub-network to attend more to the more important features. In this paper, we propose a novel attention mechanism, the Class Semantics-based Attention (CSA), that learns from the temporal distribution of se-mantics of action classes present in an input video to ﬁnd the importance scores of the encoded features, which are used to provide attention to the more useful encoded fea-tures. We demonstrate on two popular action detection datasets that incorporating our novel attention mechanism provides considerable performance gains on competitive action detection models (e.g., around 6.2% improvement over BMN action detection baseline to obtain 47.5% mAP on the THUMOS-14 dataset), and a new state-of-the-art of 36.25% mAP on the ActivityNet v1.3 dataset. Further, the
CSA localization model family which includes BMN-CSA, was part of the second-placed submission at the 2021 Ac-tivityNet action localization challenge. Our attention mech-anism outperforms prior self-attention modules such as the squeeze-and-excitation in action detection task. We also ob-serve that our attention mechanism is complementary to such self-attention modules in that performance improve-ments are seen when both are used together. 1.

Introduction
The creation of digital videos and the need for video understanding has exploded over the last decade due to
*denotes equal contribution
†Corresponding author widespread availability and presence of digital cameras.
Two fundamental components of video understanding are identifying the action components that are present in the video [12, 22, 42] and localizing these actions across the temporal axis [7, 13, 29, 46, 23] and also the spatial axis
[18, 37]. For video understanding tasks, it is crucial to learn good video representations, learning relevant encoded features that are useful for the video understanding tasks
[12, 23, 25]. These rich video representations or encoded features can then be utilized to perform various video un-derstanding tasks (e.g. a localization sub-network can use these features for detecting action segments, etc.). This paper focuses on the task of temporal action detection in untrimmed videos, which has several applications such as content-based video searching [21], video highlight gener-ation [15] and surveillance [9].
With the availability of large-scale action recognition and detection datasets (e.g. Kinetics-400 [5], Activi-tyNet v1.3 [4], etc.) and the availability of high perfor-mance computing services, deep learning approaches have achieved enormous success in learning video representa-tions or learning encoders that generate features which are useful for video understanding tasks such as temporal ac-tion detection [22, 12, 42, 35]. These encoder learning the encoder processes can be very comprehensive - e.g. learning process of BMN [23] (a recently proposed com-petitive approach) ﬁrst involves training a TSN-ResNet101 action recognition model [42] on Kinetics-400 action recog-nition task [52] followed by ﬁne-tuning the action recogni-tion model on ActivityNet v1.3 [4], followed by extracting action class-semantic rich features using the action recogni-tion model, and ﬁnally using a localization encoder to en-code features that are useful for video understanding tasks such as action localization. These encoded features are pro-cessed by the localization sub-network for generating action proposals [51, 25, 23]. It is unlikely that all of these encoded features will be equally useful or important for the local-In fact, prior ization sub-network for a particular video. works on attention in convolutional neural networks (Con-vNets) have shown that attending to more important feature
channels [19] or locations [43] can accelerate training and usually improve network performances. Despite the possi-bility that the importance of the encoded feature can vary for different videos and the existing motivation from prior works that have demonstrated improved ConvNet perfor-mances via attention mechanisms [43, 19, 40], there are no prior action localization networks that have attention mech-anisms for attending to the more important encoded features for accelerating training and improving performance.
Incorporating attention at the encoded feature, while not used before for action localization ConvNets, can eas-ily be implemented with prominent self-attention meth-ods (e.g. squeeze-and-excitation (SE) based attention [19], transformer [40]), which learn the inter-dependencies of the encoded feature to estimate the relative importance of the features. In contrast, we propose a novel attention mecha-nism that computes the relative importance of the features based on class-speciﬁc semantically rich features that are extracted by the action recognition model and that are used at the input of the encoder in the action localization network (Fig. 1). Our rationale on using these class-speciﬁc seman-tically rich features is that the distribution of importance of the encoded feature will likely depend based on which ac-tion class (or action classes) the video contains. Evidence for such class-speciﬁc dependency of importance distribu-tion was demonstrated in an ablation study in [19] which showed that some speciﬁc feature channels were more im-portant than others for one particular class while having low importance for another class within the ImageNet dataset
[36]. For action localization tasks, the class-semantics can vary across the temporal axis since different action classes can be present at different time points. Prior works on ac-tion localization in the fully and weakly supervised setting (where only class-level supervision is used to learn the tem-poral boundaries of an action), use class semantic-rich clas-siﬁcation features as input and have successfully pushed the state of the art localization performances. This shows that the class-semantics features contain useful information for reliable action localization. To learn the importance of fea-tures from the temporally varying class-semantics in videos, our novel attention mechanism jointly learns from both the channel and temporal axes of the encoder input features, and provides attention both along the channel and temporal axes of the encoded features. Our attention mechanism is generic and can be easily applied to prior action localiza-tion networks that have an encoder and a localization sub-[23, 8, 46]) (Fig. 1). We demonstrate that network (e.g. our attention mechanism considerably improves on baseline
ConvNets [23, 46, 8] on two major action detection datasets (Thumos[20] and ActivityNet v1.3 [4]). Our ablation stud-ies also show that our novel attention mechanism can pro-vide complementary beneﬁts when used together with self-attention mechanisms (such as [19]). 2.