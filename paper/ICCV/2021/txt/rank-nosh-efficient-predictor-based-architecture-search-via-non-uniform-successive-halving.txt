Abstract
Predictor-based algorithms have achieved remarkable performance in the Neural Architecture Search (NAS) tasks.
However, these methods suffer from high computation costs, as training the performance predictor usually requires training and evaluating hundreds of architectures from scratch. Previous works along this line mainly focus on re-ducing the number of architectures required to ﬁt the pre-dictor. In this work, we tackle this challenge from a differ-ent perspective - improve search efﬁciency by cutting down the computation budget of architecture training. We pro-pose NOn-uniform Successive Halving (NOSH), a hierar-chical scheduling algorithm that terminates the training of underperforming architectures early to avoid wasting bud-get. To effectively leverage the non-uniform supervision sig-nals produced by NOSH, we formulate predictor-based ar-chitecture search as learning to rank with pairwise com-parisons. The resulting method - RANK-NOSH, reduces the search budget by while achieving competitive or even better performance than previous state-of-the-art predictor-based methods on various spaces and datasets.
⇥
⇠ 5 1.

Introduction
Neural Architecture Search has demonstrated its effec-tiveness in discovering high-performance architectures for various computer vision tasks, including image classiﬁca-tion [6, 23, 44], semantic segmentation [21, 32], and image generation [14, 15, 16]. Concretely, NAS methods attempt to identify the best architecture from a vast search space ac-cording to a predeﬁned performance metric (e.g., accuracy, latency, etc.). Pioneering works in this ﬁeld require train-ing and evaluating thousands of architectures in the search space [26, 50, 51]. For example, the reinforcement learning method proposed by Zoph et al. [51] trains over 20,000 net-works. The tremendous amount of computation overhead largely limits their practical usage. Since then, improving the efﬁciency of architecture search algorithms has become a central topic in the NAS community.
Recently, weight-sharing technique witnesses much suc-cess in improving the search efﬁciency of NAS [2, 7, 23, 29, 44]. Those methods train a supernet that encompasses all architectures in the search space, and use the pretrained supernet to evaluate the performance of architectures. De-spite their search efﬁciency, weight-sharing methods are not generally applicable to arbitrary search spaces due to the restrictions in constructing supernets [27, 46]. Moreover, they also suffer from various inductive biases caused by the weight-sharing mechanism [6, 33, 38, 47, 48], which has a tendency towards parameter-free operations and wide, shal-low structures.
On the other hand, predictor-based NAS methods are free from the aforementioned disadvantages. Starting from a pool of randomly selected architectures, previous methods iteratively conduct the following steps: 1) train and evalu-ate all the architectures in the pool fully; 2) ﬁt a surrogate performance predictor; 3) use the predictor to propose new architectures and add them to the pool for the next round
[12, 40, 45]. Compared with previous RL and evolution-based NAS methods, using a performance predictor can re-duce the number of networks evaluated from scratch. How-ever, training all the architectures in the candidate pool fully is still extremely computationally expensive. Most comple-mentary advances alone this line focus on developing better predictors that require a smaller training pool [12, 40, 45], but the potential to further cut down the search cost by re-ducing the training length of individual architectures in the pool has not drawn much attention.
In this work, we aim to investigate the possibility of re-ducing the search cost of predictor-based NAS by reduc-ing the number of epochs required to train every architec-ture in the candidate pool. Inspired by successive halving
[17], our key idea is that the learning process of poor archi-tectures can be terminated early to avoid wasting budgets.
However, it is non-trivial to integrate successive halving to predictor-based NAS formulations. Firstly, predictor-based algorithms iteratively add new architectures to the candidate pool [12, 40, 45], whereas regular successive halving only removes underperforming candidates from the initial pool.
Secondly, with successive halving, architectures in the pool will be trained for different number of epochs, so their val-idation accuracy are not directly comparable in a semanti-cally meaningful way. Standard regression-based predictor
ﬁtting, which requires the exact validation accuracy for each architecture when fully trained, will be problematic in this setting.
To tackle those challenges in a uniﬁed way, we propose
RANK-NOSH, an efﬁcient predictor-based framework with signiﬁcantly improved search efﬁciency. RANK-NOSH consists of two parts. The ﬁrst part is NOn-Uniform Suc-cessive Halving (NOSH), which describes a multi-level scheduling algorithm that allows adding new candidates and resuming terminated training process. It is non-uniform in the sense that NOSH maintains a pyramid-like candidate pool of architectures trained for various epochs without dis-carding any candidates. For the second part, we construct architecture pairs and use a pairwise ranking loss to train the performance predictor. The predictor is essentially a rank-ing network and can efﬁciently distill useful information from our candidate pool consisting of architectures trained for different epochs. Moreover, the proposed framework naturally integrates recently developed proxies that measure architecture performance without training [1, 5, 25], which allows more architectures to be included in the candidate pool at no cost.
Extensive experimental evaluations on multiple search spaces, datasets, and budgets demonstrate the effectiveness and generality of the proposed method. On DARTS space,
NAS-Bench-101, and NAS-Bench-201, RANK-NOSH can reduce the search budget of SOTA predictor-based methods by 5x while achieving similar or even better results. 2.