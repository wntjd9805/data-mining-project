Abstract
ObjectGoal Navigation (OBJECTNAV) is an embodied task wherein agents are to navigate to an object instance in an un-seen environment. Prior works have shown that end-to-end
OBJECTNAV agents that use vanilla visual and recurrent modules, e.g. a CNN+RNN, perform poorly due to overfitting and sample inefficiency. This has motivated current state-of-the-art methods to mix analytic and learned components and operate on explicit spatial maps of the environment. We instead re-enable a generic learned agent by adding auxil-iary learning tasks and an exploration reward. Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8% rel-ative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge [35]. From our analy-sis, we propose that agents will act to simplify their visual inputs so as to smooth their RNN dynamics, and that aux-iliary tasks reduce overfitting by minimizing effective RNN dimensionality; i.e. a performant OBJECTNAV agent that must maintain coherent plans over long horizons does so by learning smooth, low-dimensional recurrent dynamics.
Site: joel99.github.io/objectnav/ 1.

Introduction
Consider how a robot placed in a novel home environment should find a target object, e.g. ‘Find a chair’. This task, known as ObjectGoal Navigation (OBJECTNAV), requires the agent to search through the unseen environment and, upon seeing a goal, navigate around obstacles to reach it.
Current state-of-the-art OBJECTNAV agents [5, 35] build explicit spatial maps of the environment and leverage a mix of analytic and learned planning on top of these maps. This is in contrast to the state-of-the-art methods in POINTNAV, a related task where agents navigate to specified goal coordi-nates (instead of object categories). In POINTNAV, the best method [39] scales a generic architecture (i.e. a CNN and
RNN) to 2.5 billion frames of experience. This approach out-paces methods with explicit spatial grounding, to the point
*Correspondence to joel.ye@gatech.edu
Figure 1. Overview of the OBJECTNAV agent architecture. The agent receives RGBD input, a GPS+Compass sensor, and must navigate to a goal instance. Building on [40], we introduce new auxiliary tasks, a semantic segmentation visual input, a Semantic
Goal Exists (SGE) feature that describes the fraction of the frame occupied by the goal object, and a method for tethering secondary policies that learns from its own reward signal with off-policy updates. We encourage the acting policy to explore, and the tethered policy to perform efficient OBJECTNAV. of essentially solving POINTNAV in Habitat [28].
In this work, we explore how to enable such a generic agent for OBJECTNAV. OBJECTNAV is considerably more chal-lenging than POINTNAV; the agent that nearly solves POINT-NAV only achieves 6% success on OBJECTNAV [35]. While
POINTNAV agents can use the onboard GPS+Compass sen-sor as a compact representation to measure progress towards the goal, OBJECTNAV agents cannot; they instead need to be competent at exploration since the target object can be anywhere in the environment. As such, a vanilla CNN+RNN agent tasked with learning these complex representations for
OBJECTNAV is likely to be under-equipped with only an RL reward, even if the reward is shaped to encourage navigation towards a goal [35]. Even with an additional exploration reward and no other feedback to learn better environment representations, efficient exploration is hard [6].
Our approach builds on a recent advance in POINTNAV [40] that combines simple architectures with auxiliary learning tasks to greatly improve sample complexity in visually com-plex environments. Specifically, 1) we update the agent’s inputs to incorporate egocentric semantic segmentation and a feature that explicitly describes how much of the target object class is in view. 2) We additionally introduce three new auxiliary learning tasks: two general task for learning
an inverse dynamics model and one exploration-centric task that predicts map coverage, and 3) to teach the agent to explore, we add an exploration term to the reward.
However, exploration is different than OBJECTNAV and we find that agents trained with an exploration reward will con-tinue to explore after locating goal instances. To mitigate this task mis-alignment, we propose a ‘tethered-policy’ multitask learning technique. In this method, we learn a second policy adapted only for the sparse OBJECTNAV reward while still acting via a primary policy given the exploration reward.
Tethering allows faster adaptation to the sparse OBJECTNAV reward, which in turns improves agent efficiency over simple fine-tuning of a single policy on the sparse reward.
Our results show that end-to-end learning can achieve state-of-the-art results in OBJECTNAV once equipped with generic representation learning and exploration objectives. Nonethe-less we are far from human-like competence at the task. To guide future work, we analyze various aspects of our agent, but center in particular around its recurrent dynamics. We propose that zero-shot transfer to RedNet segmentation is greatly degraded by the chaotic dynamics it causes in agent
RNNs, that several failure modes are caused by lapses in agent memory, and that auxiliary tasks reduce overfitting by constraining the effective dimensionality of agent RNNs.
Concretely, our contributions are: 1) Objectives to train a simple CNN+RNN architecture that lead to 24.5% success on OBJECTNAV (+37% relative improvement over prior state-of-the-art). This suggests that, despite their current prevalence, explicit maps need not be necessary to learn complex Embodied AI tasks. 2) A method for tethering secondary policies to an agent.
Tethered policies learn from separate reward signals with off-policy updates. We use a tethered policy with no exploration reward to achieve 8.1% SPL (+8% relative improvement over prior work). 3) An analysis of the agent. Through examination of the agent’s behavior, representations, and recurrent dynam-ics, we find a) the agent seeks out simple visual inputs corresponding to smoother RNN dynamics, b) agent fail-ures are often related to agent memory, and c) auxiliary tasks regularize agent RNNs by constraining their effec-tive dimensionality. We hypothesize a key component of performant OBJECTNAV agents will be the ability to plan with smooth, low-dimensional recurrent dynamics. 2. Approach
OBJECTNAV Definition and Dataset. [2] In OBJECTNAV, an agent must navigate to an instance of a specified object category in an unseen environment. The agent does not receive a map of the environment and must navigate us-ing its (noiseless) onboard sensors: an RGBD camera, a
GPS+Compass sensor which provides location and orienta-tion relative to start of episode. The agent also receives a goal object category ID. The full action space is discrete and consists of MOVE_FORWARD, TURN_LEFT, TURN_RIGHT,
LOOK_UP, LOOK_DOWN, and STOP. We experiment with both the full 6-action setting and a restricted 4-action setting that excludes LOOK_UP and LOOK_DOWN. To our knowl-edge, no other works have considered the 6-action setting (perhaps due to the increased complexity of specifying tilt actions in 2D map-based planners). The agent must stop at a location within 1m of an object of the specified category and be able to view the object from that location. There is a 500 step limit to succeed on the episode.
We experiment on the Matterport dataset (MP3D [4]), which has 90 scenes and 40 labeled semantic object categories.
There are 21 goal categories, as specified in the Habitat 2020
Challenge [35]. We train our agent with a 3D GPS (i.e. with vertical localization) to encourage the agent to reason about 3D exploration, as a considerable number of MP3D scenes incorporate elevation changes. However, we evaluate the agent with a 2D GPS for compatibility (by zero-ing vertical axis) with the Habitat Challenge parameters. We provide a comparison of these settings in (Section A.4).
Additional Features. We accelerate agent learning by pro-viding two semantic features: semantic segmentation (segm.) for the visual input, and a “Semantic Goal Exists” (SGE) scalar which equals the fraction of the visual input that is occupied by the goal category (computed from the semantic segm.). During training, we use the ground truth semantic segm. directly from the MP3D annotations. During evalua-tion, the segm. is predicted from RGBD using a RedNet [18] model finetuned to predict the 21 goal categories. The Red-Net model is taken off-the-shelf (trained on SUNRGBD [32]) and finetuned on 100k randomly sampled forward-facing views from MP3D. We provide additional details in Section
A.15. The SGE feature distills the domain knowledge that the agent should navigate to the goal once it is seen; this knowledge is built into the planners in prior works [5, 20]. 2.1. Agent Architecture
Our agent uses the split-belief architecture introduced in [40], shown in Fig. 1. This approach first embeds all sensory inputs with feed-forward modules. The visual inputs are first downsampled by 0.5 i.e. from 480 × 640 to 240 × 320, and the semantic segm. channel is also projected to 4D by associating each semantic ID with a learnable vector.
After this preprocessing, visual inputs are fed through a
ResNet18 [15]. The goal object ID is directly embedded into a 32D vector. The embedded visual and goal vectors are concatenated with the GPS-Compass and SGE inputs to form an observation embedding. Next, a set of recurrent
“belief modules” integrate these observation embeddings over time. These belief modules are independent GRUs [7], each associated with a separate auxiliary task; Ye et al. [40]
proposes that the split modules, as opposed to one larger recurrent network, enable orthogonal auxiliary tasks to be learned while minimizing task conflict. We term output cell states from all GRUs as “beliefs.” Beliefs are fused using an attention layer conditioned on the observation embedding.
The fused belief is then directly used in a linear actor-critic policy head. We refer to this agent as the base agent. 2.2. Learning Signals
Rewards. The base agent receives a sparse success reward rsuccess, a slack reward rslack to encourage faster goal-seeking, and an exploration reward rexplore. Measuring exploration is a dense indicator of progress, and encourages an intuitively prerequisite skill for OBJECTNAV. We use a visitation-based coverage reward, in which we first divide the map into a voxel grid with 2.5m × 2.5m × 2.5m voxels and reward the agent for visiting each voxel. This form of visitation-based exploration bonus was found to work well for map-based agents [26]. We smooth rexplore by decaying it by the number of steps the agent has spent in the voxel (visit count v). Further, to ensure the agent eventually prioritizes
OBJECTNAV, we decay rexplore based on episode timestep t with a decay constant d = 0.995. In summary: rtotal = rsuccess + rslack + rexplore rsuccess = 2.5 on success rslack = −10−4 per step rexplore = 0.25 × dt v (1a) (1b) (1c) (1d)
Tethering to an Exploration Policy. Using exploration to guide behavior can distract the agent from properly termi-nating at the goal point, e.g. when it is standing next to the goal but can instead choose to continue to explore a wide open space (we observe this in Section 4.1). To alleviate this pathology, we introduce a tethered-policy method which attaches an additional policy head to the fused belief output.
Each of the two policy heads can be fed their own desired rewards, and the agent can act according to any mix of the two policies. Note this means that each policy share the same action space. We implement a simple schedule in which the agent first acts with a policy optimized with the full shaped reward (rtotal) for an initial period of training, and then with a second policy which only uses the sparse rsuccess. The first phase is intended to teach the agent how to explore, and the second to encourage efficient OBJECTNAV. We use importance-weighted VTrace [10] returns to update the non-behavioral policy to account for experience being off-policy from its perspective (details in Section A.13). Tethering is similarly motivated as SAC-X [27], but implemented as a lightweight extension to a standard RL agent.
Auxiliary Tasks. We use 6 auxiliary tasks. As in Ye et al. [40], we use 2 instances of CPC|A [14], a self-supervised contrastive task, at horizons of k = {4, 16} steps. We also use PBL [13] with a horizon of k = 8 steps. These tasks use agent states to make predictions about the environment.
We also introduce two general-purpose inverse tasks, which predict agent actions given environment transitions: Action
Distribution Prediction (ADP) and Generalized Inverse Dy-namics (GID). ADP and GID both predict actions taken between two observations k frames apart (i.e. {a[t:t+k]}), and are both conditioned on the belief at the first frame ht and the visual embedding (i.e. CNN output) ϕt+k. ADP uses a 2-layer MLP to directly predict an action distribution and evaluates the KL-divergence between this prediction and the empirical distribution of the next k actions:
LADP = KL(MLP(ht, ϕt+k), a[t:t+k]) (2)
GID predicts individual actions using a separate GRU with state gGID and two linear layers f, f ′. The GRU is initialized with the same inputs as in ADP: gGID t = f (ht, ϕt+k)
The GRU is updated with actions taken: t+i = GRU(at+i−1, gGID gGID t+i−1)
LGID = k−1 (cid:88) i=1
CrossEnt(f ′(gGID t+i ), at+i) (3) (4) (5)
Finally, we introduce an exploration-specific Coverage Pre-diction (CP) task that leverages the GPS sensor provided to the agent. CP follows a similar structure as GID, i.e. using a GRU for sequence prediction. However, this GRU’s ini-tial state is conditioned not on a visual embedding, but on the number of steps the agent has spent in its current voxel, v(st). This conditioning helps the GRU anticipate how close the agent is to the edge of the voxel. The GRU predicts the change in coverage at each of the next k steps: t = f ′′(ht, v(st)) gCP t+i = GRU(at+i−1, gCP gCP t+i−1)
LCP = k (cid:88) i=1
CrossEnt(f ′′′(gCP t+i), ∆cov(t, t + i)) (6) (7) (8) where f ′′, f ′′′ are linear layers, gCP denote the GRU’s state, and ∆cov(t, t+i) denotes the change in coverage, i.e. number of new voxels visited between time t and t + i1. Perfect performance would require the belief to remember all prior locations. The horizons for ADP, GID, and CP are k = 6, 4, 16, respectively; details in Section A.14. 1Note that ∆cov(t, t + i) ∈ {0, . . . , i}
VAL
TEST-STD
Success % (↑) SPL % (↑)
Success % (↑) SPL % (↑) 1) 4-Action 34.4±2.0 9.58±0.75 19.9 2) 4-Action + RedNet FT 33.1±2.0 6.89±0.56 3) 6-Action 30.8±1.9 7.60±0.64 4) 6-Action + Rednet FT 34.6±2.0 7.93±0.64 5) 6-Action + Tether 26.6±1.9 9.79±0.82 6) 6-Action + Tether + RedNet FT 30.3±1.9 10.8±0.84 7) E2E Baseline (DD-PPO [39]) 8) SemExp [5] 9) SRCB-robot-sudoer
--------24.5
-21.1 6.2 17.9 14.4 6.7
--6.4
-8.1 2.1 7.1 7.5
Table 1. Primary variants on the MP3D VAL and TEST-STD splits, along with prior SoTA from the Habitat Challenge leaderboard [35].
± intervals provide 95% CI; VAL bold values are significantly better than non-bold values (p < 0.05, paired t-test), TEST bolding is for emphasis. Our auxiliary task and exploration enabled agent (row 1) outperforms prior state of the art success (over row 7) and triples the vanilla learning agent (row 6). Fine-tuning the agent with RedNet segmentation reverses the ranks of 4 and 6-action agents (rows 1 and 3 vs 2 and 4, analysis in Section 3.1), helping the 6-action agent to achieve a new state-of-the-art 24.5% success. Finally, tuning on the sparse
OBJECTNAV success reward helps the agent set state-of-the-art 8.1% SPL (row 6 vs 9). 3. Results
In our experiments, we train each of our agents for 8 GPU-weeks (192 GPU-hours), amounting to ∼125M frames per agent. For the tethered variant, we use the sparse policy for the final 25M frames. The two primary metrics we report are Success and Success weighted by Path Length (SPL), a measure of trajectory efficiency, defined over N episodes as
SPL = 1
N
N (cid:88) i=1
Si li max(pi, li) where Si is a binary indicator of success on episode i, li is length of optimal path, and pi is length of agent path. We evaluate checkpoints every ∼4M frames and report metrics from the checkpoint with the highest SPL on VAL.
Auxiliary Tasks and Exploration produce an effective
OBJECTNAV agent. We compare against results from the
Habitat Challenge 2020 leaderboard in Table 1. We first note the large drop from VAL to TEST-STD (e.g.-14% success in row 1). Since we cannot access the test split, we con-jecture that the splits were randomly sampled such that the 9 test scenes were disproportionately challenging.2 While other works do not highlight this shift, we note that even agents with strong priors (rows 6, 7) have high performance variance, e.g. 5% success gaps between the two test splits (TEST-STD, TEST-CHALLENGE [35]). We provide additional commentary and check for agent biases in Section A.1. 2Distribution shift is expected given small VAL, TEST splits.
Using RedNet segm. directly (i.e. zero-shot transfer from
GT , row 1), the 4-action agent reaches 19.9% success (+11% relative over prior best, row 1 vs 8). This is 3x the perfor-mance of the E2E baseline that is rewarded for approaching a goal, a method which “solved” POINTNAV (row 7).
We tune the 4-action agent with RedNet segm. for an addi-tional 10M frames to account for the ground-truth-to-RedNet distribution shift, but surprisingly performance drops (row 1 vs 2). Conversely, the 6-action agent which initially per-forms worse than the 4-action agent (row 1 vs 3) overtakes the 4-action agent after finetuning with RedNet (row 2 vs 4). We analyze this reversal in Section 3.1. Though these tuned agents match in VAL (row 1 vs 4), the 6-action agent improves on TEST-STD, to 24.5%, +37% relative over prior state-of-the-art (row 4 vs 8).
However, both 4 and 6-Action agents are less efficient than prior methods (SPL column). Qualitative examination of these agents indicate they prefer to wander around goals for some time before stopping at them correctly, likely due to the exploration reward. After finetuning on the sparse
OBJECTNAV reward through tethered training, SPL rises and
Success falls (row 3 vs 5). We show the Success drop is likely due to reduced exploration in Section A.2. Nonetheless, after tuning this agent on RedNet, we achieve +0.6% (+8% relative) over state of the art SPL (row 6 vs 9).
We provide ablations for our design choices in Table 2. Our auxiliary task ablations are compared against the 4-action agent (rows 1-4). These tasks provide modest gains in both metrics (<1% for ADP, 1% GID, 2% CP). Next, ablating the
Success % (↑) SPL % (↑) 1) Base (4-Action) 34.4±2.0 9.58±0.75 2) - Action Distribution Prediction 34.1±2.0 9.27±0.69 3) - Generalized Inverse Dynamics 33.7±2.0 9.05±0.69 4) - Coverage Prediction 32.4±2.0 8.7s7±0.69 5) - Semantic Goal Exists 20.3±1.7 4.14±0.47 6) 6-Action + Tether 26.6±1.9 9.79±0.82 7) 6-Action + Sparse Tuning 23.1±1.8 8.43±0.76
Table 2. Ablations on VAL. Bold values are significantly better than unbolded values in the same group. New auxiliary tasks pro-vide moderate performance gains, and tethered training preserves
Success better than directly tuning on sparse OBJECTNAV reward.
Success % (↑)
SPL % (↑)
TRAIN
VAL
TRAIN
VAL 1) 4-Act 2) 6-Act 50.3 (36.0) 43.3 (34.4) 18.1 (12.4) 12.3 (9.6) 56.0 (21.7) 58.0 (30.8) 21.5 (8.2) 16.9 (7.6) 3) 6-Act + Tether 54.0 (27.3) 52.2 (26.0) 27.9 (11.5) 26.0 (9.8)
Table 3. Performance on a 300-episode subset of TRAIN and VAL splits, reported as “with GT segmentation (with RedNet segmen-tation)”. Best metrics are bolded for emphasis. In both splits, all agents degrade significantly with RedNet segmentations, but 6-action agents more so. Under GT segmentation (numbers out-side parentheses), agents are minimally overfit, though SPL does degrade slightly from TRAIN to VAL. In the GT setting, 6-action agents outperform 4-action agents.
SGE sensor drops performance by a large amount (-14%, row 1 vs 5). In Section A.5, we note that SGE is much more effectively incorporated as a feature than as an auxiliary task.
Finally, we also compare tethered policy training against direct finetuning (row 6 vs 7) for the 6-action agent. While direct finetuning also improves SPL (vs SPL reported in row 3 in Table 1), tethered training is overall more effective at preserving agent performance. 3.1. Stable segmentation is critical to untuned 6-action agents.
We found that 4-action agents suffer a performance drop after finetuning on RedNet, while 6-action agents improve.
To understand why, we first observed that the RedNet is greatly overfit (Section A.15), we decouple our agent from
RedNet by evaluating agent performance on both TRAIN and
VAL with GT and RedNet segm. , shown in Table 3. We first note that most agents are minimally overfit to TRAIN, with a moderate impact to SPL and < 3% drop in success.
RedNet Seg. degrades performance significantly, but in particular, RedNet Seg. hurts 6-action agents more, even on
TRAIN, where the effects of an overfit RedNet are reduced.
This suggests 6-action agents are more sensitive to RedNet statistics in particular, and hence able to overtake the 4-action agent once finetuned with RedNet.
We examine agent trajectories to understand this sensitivity (videos provided in supplement). We first find that during training with GT semantics, 6-action agents learn to navigate while spending most steps facing downwards towards the floor (while the 4-action agent can only look straight ahead).
This happens despite there being minimal obstacles on the floor while looking straightforward offers greater chance to see more distant goals. Intuitively, the visual input provided by the floor is simpler than the view of a room. We thus posit a stability hypothesis: the 6-action agent learns to exploit a simpler signal to enable stabler recurrent dynamics. That is, because it is harder to maintain memory over long term tra-jectories when faced with noisy inputs, agents will simplify their input even at slight cost to information gained. Such a hypothesis may relate to results on improved generalization when restricting the visual field of a gridworld agent [17] and when withholding inputs from parts of a recurrent mod-ule [12]. This hypothesis suggests that standard RNN gating is imperfect at preventing input-induced information decay in the RNN state, and presents an avenue for future work.
We next observe that RedNet segm. is low quality and unsta-ble, flickering across consecutive frames and without well-defined object boundaries. These are expected failures: poor segm. is due in part to the noisiness of the underlying MP3D meshes, and consistent segm. is a broader challenge [36].
Predicted segm. of frames of the floor are particularly poor, likely since RedNet is trained with front-facing views. Con-sequently, the 6-action agent behavior was very erratic, with repeated alternating turning actions. If the 6-action agent did learn to exploit the floor for simple visual inputs, it would have a reduced incentive to filter noisy frontal views, and thus would be particularly vulnerable to degraded segm.
Then, one possible reason to explain earlier trends is that 1. RedNet overfitting implies that previously general agents which are tuned with RedNet will in turn overfit, 2. Untuned 6-action agents avoid this overfit transfer but suffer signifi-cantly more from unstable semantics, and 3. Tuned 6-action agents overfit, but gain performance by maintaining stable behavior while receiving unstable semantics. We quantify this instability in Section A.7. The fact that tuning agents with RedNet improves agent performance suggests robust-ness to noisy inputs is partially learnable, though the agent will not do so if it has a simpler option (facing the ground). 4. Agent Analysis
These agents demonstrate promising performance (∼ 55% val success) in navigating large, complex environments, es-pecially when granted GT segm. Nonetheless, their perfor-mance is far from perfect, and they still display qualitative failure modes that a human would not have, such as repeated circling inside a room. In this section we analyze agent be-Figure 2. Breakdown of failure modes for base 6-action agent in 300 episodes of VAL. havioral modes, knowledge, and internal dynamics to inform future directions towards human-level OBJECTNAV, and to develop an intuition for how a competent OBJECTNAV agent operates. We scope our analysis to GT segm. and the base 6-action agent, unless otherwise noted. 4.1. Behavioral Analysis
In our behavioral analysis we aim to understand: How can
OBJECTNAV agents approach 1.0 Success? To identify prominent modes of the remaining failures, we conduct a qualitative coding of agent behavior for both the base and tethered 6-action agent. Specifically, we sample 300 vali-dation episodes and manually label the failures (there are 125 for base, 151 for tether) and group them according to common trends. A subset of failure modes are described in Table 4 and their relative prevalence for the base agent is shown in Fig. 2. A full list of failure modes are in Section
A.6, and example videos are available in the supplement.
Name
Description
Explore
Plateau
Loop
Detection
A generic failure to find the goal despite steady exploration. Includes semantic fail-ures e.g. going outdoors to find a bed.
Repeated collisions against the same piece of debris causes a plateau in coverage. In-cludes debris which traps agent in spawn.
Poor exploration due to looping over the same locations or backtracking.
Despite positive SGE, the agent does not notice nor successfully navigate to the goal.
Last mile
Gets stuck near the goal.
Commitment
Sees and approaches the goal but passes it.
Open
Explores an open area without any objects.
Table 4. Description of prominent failure modes.
We find that agent failures are diverse and beyond only un-Figure 3. We probe time and max SGE seen on each belief, the fused belief, and the observation embedding (“Obs”) as a baseline.
Error bars show 95% CI across single beliefs. Both concepts are encoded, though imprecisely and with high variance across beliefs. successful exploration, which at best comprises ‘explore’ and ‘loop’. They get stuck (‘plateau’ and ‘last mile’), suffer pathologies that might be attributed to the exploration reward (‘commitment’ and ‘open’), and deal with a handful of issues due to scene quality (labeled as ‘misc’ here). However, some failure modes do preclude others. For e.g., an agent which is trapped by reconstruction debris at spawn (‘plateau’) is still liable to not find the goal due to general exploration issues.
Additionally, the agent does not make panoramic turns to identify promising goal locations, as a human might. We attribute this to the visitation-based exploration reward; a different reward which promotes simply viewing more areas instead of visiting them may mitigate this. Alternately, non-episodic exploration rewards may promote the appropriate behavior with less misalignment with OBJECTNAV.
These exploration-related pathologies are not cured by swap-ping to the sparse OBJECTNAV reward, which degrades success (Table 2, row 6 and 7). Tethered agents explore worse, and behavioral coding finds a new dominant failure mode wherein the agent quits early (see Section A.2). 4.2. Probing Learned Knowledge
How well do agent representations match our intuitions?
Does its behavior match represented features? e.g., “com-mitment” failures, which generally occur when the agent spots the goal early in the episode, should only be feasible if the agent maintains a time representation and knows it has time to continue gaining coverage rewards. Conversely, does the agent represent prominent variables that a human might? e.g., “Commitment” and “Detection” failures both imply that agents do not remember SGE features from prior timesteps. To test these questions, we train linear decoders to probe our agent for time and the most goal seen. We first record representations from 300 episodes on both TRAIN and
VAL. We train the decoders using TRAIN representations and test them on VAL representations (∼ 60 − 70K steps each).
We plot decoder performance in Fig. 3, aggregating perfor-mance across individual beliefs. Though the features are represented across beliefs, there is high variance in probe performance across beliefs. Moreover, probe performance on the fused belief is worse than in individual beliefs. These results reveal semantic differences among individual beliefs and the fused belief, suggesting some abstraction is lost in fusion despite it only being a weighted sum. Separately, the tethered agent represents Max SGE more prominently than the base agent, matching the intuition that the base agent does not retain goals seen in previous steps perfectly.
With the same approach, we find little evidence of features that a human might use in OBJECTNAV: time spent in loca-tion, room ID, and distance to goal (Section A.8). Supplying these features directly could improve the agent. 4.3. Dynamical Analysis
Though a behavioral analysis provides a sense of how the agent fails, it does not explain why, and probing techniques fall short when it is unclear what knowledge an agent might encode. Through our experiments, dynamics have played an important role – the 6-action agent looking downward, acting chaotic with predicted segm., and overtaking 4-action after tuning. Important failure modes appear dynamic in nature: ‘loop’ failures indicate forgetting of visited locations,
‘commitment’ failures could be attributed to procrastination through the agent’s internal clock. We were thus motivated to characterize the agent’s dynamic computations.
Belief Dynamics correlate with agent observations. Be-fore we study RNN dynamics proper, we first validate that
RNN dynamics reflect trajectory dynamics, as represented by sequences of observation embeddings. This helps quantify to what extent agent beliefs are grounded in agent observations and actions; a strong connection would support a stability hypothesis (that agents need smooth inputs for smooth RNN dynamics). Alternately, RNN dynamics could be driven by intermediate computation, and thus the RNNs would conser-vatively incorporate inputs, i.e. through gating, so as to best preserve hidden state information. We measure this with the curvature of the agent belief trajectories and the observation embeddings, by computing the dot product similarity of suc-cessive displacements in a sequence of representations ([16], details in Section A.9). We report the correlation between each belief curvature and observation curvature.
Fig. 4 shows that observation and belief curvatures are closely correlated, i.e. that varying observations track large changes within agent hidden state. This suggests that RNNs do incorporate most inputs. It is possible non-reactive com-putation (e.g. such as time tracking probed in Section 4.2) which occurs independently from observations might not manifest because they are relatively stable; however, the CP belief, which should track coverage variables that are rela-tively independent from moment-to-moment observations, is
Figure 4. We measure curvature of observation embeddings and beliefs of the base 6-action agent, plotting the distribution from transitions in 300 episodes. All but CP closely correlate.
Figure 5. Fixed point (FP) memories. Top: For each of 6 agents (4-Act, 6-Act, 6-Act + Tether, No CP, No ADP, No GID), we sort agent beliefs by the span of their memory. This span is computed by sampling 50 FPs and counting at each FP the number of eigenvalues with time constants above 10 steps. The more eigenvalues past threshold, the larger the span, and the lower the rank. Beliefs are labeled by their task, and beliefs without tasks (from ablations) are labeled with BASE. We count the number of times a belief appears in a given rank (e.g. since BASE only appears in 3 agents, it only has 3 counts). Consistent rank indicates relative span is stable. Bottom:
We measure how memory changes over time for the CPC|A-4 and
CP tasks by counting the number of eigenvalues with time constants above 100 steps at several points during training. less correlated with observations than the other beliefs. This suggests non-reactive computation needn’t be hidden.
Auxiliary tasks specify fixed point memory spans. We would ideally like to understand what computational struc-ture the agent RNNs use beyond coarse-grained statistics like curvature. To approach this question, we employ fixed point
(FP) analysis [11, 33, 34]. FP analysis simplifies the study of a full RNN into a study of its “slow points,” where the RNN update is small and well-approximated by a linear function.
We provide details on methodology in Section A.10. FPs are typically studied both through their overall layout (the manifold) and their local dynamics. While FP manifolds have been used to directly link RNN computation to dynam-ical structures, e.g. ring attractors, we find our RNNs are much higher-dimensional than those in prior work and thus difficult to clearly classify (notes in Section A.11). This leaves local dynamic structure. At an identified FP h, the
RNN update is well described by a first order approximation.
Let ∆ht := ht − h, for ht near h, then
∆ht+1 ≈ J rec h ∆ht. (9)
Then the decomposition of J rec assesses FP “memory.” h
Specifically, a delta along an eigenvector v with correspond-ing eigenvalue λ ≪ 1 would decay to 0, while an eigenvector v′ with λ′ ≈ 1 would persist indefinitely. A simple method to quantify FP memory is to count the eigenvalues past a threshold value. For interpretability we count eigenvalue time constants instead of the values themselves, as in [21] (Section A.10); we call this quantity a memory span.
We measure and rank these spans for all beliefs of several different agents, producing Fig. 5. The auxiliary task cor-responding to a belief appears to play a consistent role in specifying FP memory span, and thus in organizing recurrent dynamics, across seeds, action suites (4 vs 6 action), and RL objectives (tether or not). Among the tasks, only CP induces larger spans than the base belief. This matches the intuition that CP could require the tracking of an unbounded number of variables. Separately, the fact that the base belief has larger span than other tasks reveals a potential mechanism by which auxiliary tasks encourage generalization. Note that smaller memories imply there are more dimensions along which RNN state decays towards fixed points, which in turn should imply preference towards lower-dimensional trajectories (even if variable inputs prevent this empirically).
Then, the fact that most auxiliary task-augmented beliefs achieve smaller memories than the vanilla “Base” belief demonstrates auxiliary tasks may preferentially select for low-dimensional trajectories. Such low-dimensional trajec-tories could be necessary for generalization; indeed this is theorized to be the reason why monkeys engage low-d dy-namics in working memory tasks[8]. Note that this hypothe-sis concerns how auxiliary tasks promote generalization in recurrent beliefs (i.e. beyond general visual representations).
In the bottom figure of Fig. 5 we plot memory span for several agents at various points in training. Memory span trends upward over training. Consistent with the first plot,
CPC|A-4 appears to constrain the span to be low. CP, on the other hand, may be constrained by the capacity of the RNN (the belief hidden size is 196). It is possible that an ideal
OBJECTNAV agent would maintain stable low-d dynamics, but this would need to be reconciled with how such an agent would need to track its path to keep exploring new areas. 5.