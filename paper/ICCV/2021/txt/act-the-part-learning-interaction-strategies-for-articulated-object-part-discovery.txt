Abstract
People often use physical intuition when manipulating articulated objects, irrespective of object semantics. Moti-vated by this observation, we identify an important embod-ied task where an agent must play with objects to recover their parts. To this end, we introduce Act the Part (AtP) to learn how to interact with articulated objects to dis-cover and segment their pieces. By coupling action selec-tion and motion segmentation, AtP is able to isolate struc-tures to make perceptual part recovery possible without se-mantic labels. Our experiments show AtP learns efﬁcient strategies for part discovery, can generalize to unseen cat-egories, and is capable of conditional reasoning for the task. Although trained in simulation, we show convincing transfer to real world data with no ﬁne-tuning. A sum-mery video, interactive demo, and code will be available at https://atp.cs.columbia.edu. 1.

Introduction
How do people and animals make sense of the physi-cal world? Studies from cognitive science indicate observ-ing the consequences of one’s actions plays a crucial role
[17, 38, 3]. Gibson’s inﬂuential work on affordances argues visual objects ground action possibilities [14]. Work from
Tucker et al. goes further, suggesting what one sees affects what one does [44]. These ﬁndings establish a plausible biological link between seeing and doing. However, in an age of data-driven computer vision, static image and video datasets [40, 24, 2] have taken center stage.
In this paper, we aim to elucidate connections between perception and interaction by investigating articulated ob-ject part discovery and segmentation. In this task, an agent must recover part masks by choosing strategic interactions over a few timesteps. We do not assume dense part labels or known kinematic structure [1, 23]. We also do not in-teract randomly [33]. Rather, we learn an agent capable of holding and pushing, allowing us to relax the assumption that objects are ﬁxed to a ground plane [28]. Our task and
Figure 1. Interaction for Part Discovery. Passive part segmenta-tion algorithms require detailed annotation and cannot generalize to new categories. While motion can help discover new objects, prior work cannot infer actions for understanding individual parts.
Our work, Act the Part, learns interaction strategies that expose parts and generalize to unseen categories. approach novelty are highlighted in Fig. 1.
Segmentation from strong supervision and random inter-action is widely studied; however, creating informative mo-tion to enable category level generalization while relaxing supervision is less explored in the community. We iden-tify the following hurdles, which make this direction salient and difﬁcult. Motion cannot be assumed in a scene as ob-jects seldom move spontaneously. Even with agent interac-tion, not all actions create perceivable motion to give insight about articulation. Actions might activate only a small num-ber of parts, so diversity of action and aggregation of poten-tially noisy perceptual discoveries is necessary. Generaliza-tion of interaction and perception to unseen categories with-out retraining or ﬁne-tuning is also desirable. These facets are often overlooked in prior work but are at the heart of this paper.
To address these challenges, we introduce Act the Part (AtP), which takes visual observations, interacts intelli-gently, and outputs part masks. Our key insight is to couple
action selection and segmentation inference. Given an RGB input image and the part segmentation belief, our interac-tion network reasons about where to hold and push to move undiscovered parts. By reasoning about changes in visual observations, our perception algorithm is able to discover new parts, keep track of existing ones, and update the part segmentation belief.
We evaluate our approach on eight object categories from the PartNet-Mobility dataset [9, 29, 49] and a ninth multilink category, which we conﬁgure with three links.
Our experiments suggest: (1) AtP learns effective interac-tion strategies to isolate part motion, which makes articu-lated object part discovery and segmentation possible. (2)
Our method generalizes to unseen object instances and cat-egories with different numbers of parts and joints. (3) Our model is capable of interpretable conditional reasoning for the task—inferring where and how to push given arbitrary hold locations.
We also demonstrate transfer to real images of unseen categories (without ﬁne-tuning) and introduce a toolkit to make PartNet-Mobility more suitable for future research. 2.