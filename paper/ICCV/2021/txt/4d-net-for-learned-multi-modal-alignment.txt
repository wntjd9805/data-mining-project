Abstract
We present 4D-Net, a 3D object detection approach, which utilizes 3D Point Cloud and RGB sensing informa-tion, both in time. We are able to incorporate the 4D infor-mation by performing a novel dynamic connection learning across various feature representations and levels of abstrac-tion, as well as by observing geometric constraints. Our approach outperforms the state-of-the-art and strong base-lines on the Waymo Open Dataset. 4D-Net is better able to use motion cues and dense image information to detect distant objects more successfully. We will open source the code. 1.

Introduction
Scene understanding is a long-standing research topic
It is especially important to the au-in computer vision. tonomous driving domain, where a central point of inter-est is detecting pedestrians, vehicles, obstacles and potential hazards in the environment. While it was traditionally un-dertaken from a still 2D image, 3D sensing is widely avail-able, and most modern vehicle platforms are equipped with both 3D LiDAR sensors and multiple cameras producing 3D Point Clouds (PC) and RGB frames. Furthermore, au-tonomous vehicles obtain this information in time. Since all sensors are grounded spatially, their data collectively, when looked at in time, can be seen as a 4-dimensional en-tity. Reasoning across these sensors and time clearly of-fers opportunities to obtain a more accurate and holistic un-derstanding, instead of the traditional scene understanding from a single 2D still-image or a single 3D Point Cloud.
While all this 4D sensor data is readily available on-board, very few approaches have utilized it. For example, the majority of methods targeting 3D object detection use a single 3D point cloud as an input [17], with numerous ap-proaches proposed [24, 52, 36, 31, 39, 40, 55, 57, 56]. Only more recently has point cloud information been consid-ered in time, with approaches typically accumulating sev-eral point clouds over a short time horizon [21, 20, 55, 31].
Furthermore, the sensors have complementary character-Figure 1. 4D-Net effectively combines 3D sensing in time (PCiT) with RGB data also streamed in time, learning the connections between different sensors and their feature representations. istics. The point cloud data alone may sometimes be insuf-ﬁcient, e.g., at far ranges where an object only reﬂects a handful of points, or for very small objects. More infor-mation is undoubtedly contained in the RGB data, espe-cially when combined with the 3D Point Cloud inputs. Yet, relatively few works attempted to combine these modali-ties [34, 50, 19, 23]. Notably, only 2 of the 26 submissions to the Waymo Open Dataset 3D detection challenge oper-ated on both modalities [46]. No methods have attempted combining them when both are streamed in time. The ques-tions of how to align these very different sensor modalities most effectively, as well as how to do so efﬁciently, have been major roadblocks.
To address these challenges, we propose 4D-Net, which combines Point Cloud information together with RGB cam-era data, both in time, in an efﬁcient and learnable man-ner. We propose a novel learning technique for fusing infor-mation in 4D from both sensors, respectively building and learning connections between feature representations from different modalities and levels of abstraction (Figure 1). Us-ing our method, each modality is processed with a suit-able architecture producing rich features, which are then aligned and fused at different levels by dynamic connec-tion learning (Figure 2). We show that this is an effective and efﬁcient way of processing 4D information from multi-ple sensors. 4D-Nets provide unique opportunities as they naturally learn to establish relations between these sensors’ features, combining information at various learning stages.
This is in contrast to previous late fusion work, which fuse
already mature features that may have lost spatial informa-tion, crucial to detecting objects in 3D.
Our results are evaluated on the Waymo Open
Dataset [47], a challenging Autonomous Driving dataset and popular 3D detection benchmark. 4D-Net outperforms the state-of-the-art and is competitive in runtime. Impor-tantly, being able to incorporate dense spatial information and information in time improves detection at far ranges and for small and hard to see objects. We present several insights into the respective signiﬁcance of the different sen-sors and time horizons, and runtime/accuracy trade-offs.
Our contributions are: (1) the ﬁrst 4D-Net for object de-tection which spans the 4-Dimensions, incorporating both point clouds and images in time, (2) a novel learning method which learns to fuse multiple modalities in 4D, (3) a simple and effective sampling technique for 3D Point
Clouds in time, (4) a new state-of-the-art for 3D detection on the Waymo Open Dataset and a detailed analysis for un-locking further performance gains. 2.