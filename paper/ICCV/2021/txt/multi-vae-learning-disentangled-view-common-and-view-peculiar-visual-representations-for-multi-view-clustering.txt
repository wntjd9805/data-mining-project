Abstract
Multi-view clustering, a long-standing and important re-search problem, focuses on mining complementary infor-mation from diverse views. However, existing works often fuse multiple views’ representations or handle clustering in a common feature space, which may result in their entan-glement especially for visual representations. To address this issue, we present a novel VAE-based multi-view clus-tering framework (Multi-VAE) by learning disentangled vi-sual representations. Concretely, we define a view-common variable and multiple view-peculiar variables in the gen-erative model. The prior of view-common variable obeys approximately discrete Gumbel Softmax distribution, which is introduced to extract the common cluster factor of multi-ple views. Meanwhile, the prior of view-peculiar variable follows continuous Gaussian distribution, which is used to represent each view’s peculiar visual factors. By controlling the mutual information capacity to disentangle the view-common and view-peculiar representations, continuous vi-sual information of multiple views can be separated so that their common discrete cluster information can be effectively mined. Experimental results demonstrate that Multi-VAE enjoys the disentangled and explainable visual represen-tations, while obtaining superior clustering performance compared with state-of-the-art methods. 1.

Introduction
Clustering analysis is a fundamental research topic in many fields, such as computer vision, machine learning, and data mining, etc. Its goal is to partition data items with sim-ilar patterns or characteristics into the same group. With
∗Corresponding author the unprecedented growth of deep learning, deep cluster-ing methods [9, 37, 44, 47] overcome the shortcomings of shallow models and make considerable progress in cluster-ing performance. In real-world applications, however, vi-sual data is often collected from multiple views or diverse sources, e.g., 1) various writing styles of one digit written by different people, 2) multiple views of an object captured from cameras in multiple directions. Compared with single-view clustering, accordingly, multi-view clustering (MVC) can access to more comprehensive characteristics contained in multi-view data and thus attracts increasing attention.
Existing MVC methods can be roughly divided into three categories: 1) The first category is multi-view spectral clus-tering [18, 23, 32, 33], where multiple graph structures are constructed for clustering. 2) The second category [25, 52] uses non-negative matrix factorization to decompose the feature matrix and obtain cluster assignments. 3) The third category is based on subspace clustering [21, 53], which conducts self-representation on a subspace shared by multi-ple views. More researches on MVC can be found in [49].
For many MVC methods, the central bottleneck is their high complexity that makes it unrealistic for handling large-scale data clustering tasks. Recent approaches have achieved inspirational progress by applying deep models
[3, 7, 34, 45, 50, 55]. However, most of them learn the clus-ter structures by exploring common representations or fus-ing features of all views. Although complementary infor-mation can be fetched in this way, the interference caused by the entanglement among multiple views is also ignored.
We are inspired by two observations: 1) Cluster infor-mation is discrete, which is an abstraction of the maximum common visual information of all views. 2) Each view’s peculiar visual information is often continuous, which has different effects on clustering. For example, the observa-tions from multiple sides of an object are conducive to bet-Figure 1. The framework of Multi-VAE. Inference process: zv extracts the v-th view’s peculiar visual information that is contained in the embedding transformed by the corresponding encoder. c represents the cluster information among all views’ embeddings. Generative process: the v-th view’s latent variable is composed of zv and c, which is fed into the corresponding decoder to generate samples. ter describe itself. Nevertheless, the two writing styles of a digit have no complementary effect for clustering, instead, which may even cause interference. How to disentangle them and learn explainable multi-view visual representa-tions? This is an interesting but challenging problem. For-tunately, some advancements about disentangled represen-tation learning [1] have been made. Some generative mod-els, such as variational autoencoders (VAE) [2] and gener-ative adversarial networks (GAN) [4], are used to learn the explainable representations, each unit of which corresponds to a single factor of variation of the data. However, learning disentangled visual representations has been rarely studied for multi-view clustering.
In this paper, we propose a novel VAE-based framework for multi-view clustering (dubbed Multi-VAE), which can learn disentangled and explainable visual representations and tackle large-scale data clustering problems. Different from the existing multi-view clustering methods, as shown in Figure 1, we introduce a view-common variable c and multiple view-peculiar variables {z1, z2, . . . , zV } in a mul-tiple VAEs architecture. In order to learn the common visual representation across views (i.e., cluster information), the view-common variable c is inferred from all views’ embed-dings. Meanwhile, each view-peculiar variable zv is only inferred from the corresponding view’s embedding so as to learn peculiar visual representations (like angle, styles, and size, etc). For each view, its latent variable is made up of c and zv and is used to generate examples. Since the clus-ter information is discrete and peculiar visual information is continuous, the prior distributions of c and zv we se-lected are Gumbel Softmax distribution and Gaussian dis-tribution, respectively. By controlling the mutual informa-tion capacity of KL divergence between the posterior of the latent variables and their prior during training, the common and peculiar visual representations of multiple views can be disentangled, which are further used for clustering.
Specifically, the contributions of this work include:
• We propose a novel multi-view VAE framework, namely Multi-VAE, where the view-common and view-peculiar variables are introduced to mine the dis-crete clusters and continuous visual factors.
• Our model can disentangle all views’ common cluster representation and each view’s peculiar visual repre-sentations.
In this way, the interference of multiple views’ superfluous information is reduced when min-ing their complementary information for clustering.
• Multi-VAE shows clearly superior clustering perfor-mance compared with other methods. Moreover, its complexity is linear to data size. To our knowledge, this is the first attempt to implement MVC by learning disentangle and explainable representations. 2.