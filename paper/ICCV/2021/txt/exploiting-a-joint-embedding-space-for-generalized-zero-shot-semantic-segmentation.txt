Abstract
We address the problem of generalized zero-shot se-mantic segmentation (GZS3) predicting pixel-wise seman-tic labels for seen and unseen classes. Most GZS3 meth-ods adopt a generative approach that synthesizes visual features of unseen classes from corresponding semantic ones (e.g., word2vec) to train novel classiﬁers for both seen and unseen classes. Although generative methods show de-cent performance, they have two limitations: (1) the visual features are biased towards seen classes; (2) the classiﬁer should be retrained whenever novel unseen classes appear.
We propose a discriminative approach to address these lim-itations in a uniﬁed framework. To this end, we leverage visual and semantic encoders to learn a joint embedding space, where the semantic encoder transforms semantic fea-tures to semantic prototypes that act as centers for visual features of corresponding classes. Speciﬁcally, we intro-duce boundary-aware regression (BAR) and semantic con-sistency (SC) losses to learn discriminative features. Our approach to exploiting the joint embedding space, together with BAR and SC terms, alleviates the seen bias problem.
At test time, we avoid the retraining process by exploiting semantic prototypes as a nearest-neighbor (NN) classiﬁer.
To further alleviate the bias problem, we also propose an in-ference technique, dubbed Apollonius calibration (AC), that modulates the decision boundary of the NN classiﬁer to the
Apollonius circle adaptively. Experimental results demon-strate the effectiveness of our framework, achieving a new state of the art on standard benchmarks. 1.

Introduction
Recent works using convolutional net-works (CNNs) [6, 36, 45, 57] have achieved signiﬁcant success in semantic segmentation.
They have proven effective in various applications such as image editing [34] and autonomous driving [54], but semantic segmentation neural
*Equal contribution, †Corresponding author.
Figure 1: In contrast to generative methods [3, 32] (top), we up-date both visual and semantic encoders to learn a joint embedding space, and leverage a nearest neighbor classiﬁer in the joint em-bedding space at test time (bottom). This alleviates a bias problem towards seen classes, and avoids re-training the classiﬁer. We visu-alize visual features and semantic prototypes by circles and stars, respectively. Best viewed in color. in the wild still has two limitations. First, existing methods fail to generalize to new domains/classes, assuming that training and test samples share the same distribution. Sec-ond, they require lots of training samples with pixel-level ground-truth labels prohibitively expensive to annotate.
As a result, current methods could handle a small set of pre-deﬁned classes only [23].
As alternatives to pixel-level annotations, weakly-supervised semantic segmentation methods propose to ex-ploit image-level labels [19], scribbles [35], and bounding boxes [8], all of which are less labor-intensive to anno-tate. These methods, however, also require a large num-ber of weak supervisory signals to train networks for novel classes. On the contrary, humans can easily learn to recog-nize new concepts in a scene with a few visual examples, or even with descriptions of them. Motivated by this, few-and zero-shot learning methods [29, 42, 48] have been pro-posed to recognize objects of previously unseen classes with
a few annotated examples and even without them, respec-tively. For example, few-shot semantic segmentation (FS3) methods [47, 49] typically exploit an episode training strat-egy, where each episode consists of randomly sampled sup-port and query sets, to estimate query masks with a few annotated support examples. Although these FS3 methods show decent performance for unseen classes, they are ca-pable of handling a single unseen class only. Recently, the work of [56] ﬁrst explores the problem of zero-shot seman-tic segmentation (ZS3), where it instead exploits pre-trained semantic features using class names (i.e., word2vec [38]).
This work, however, focuses on predicting unseen classes, even if a given image contains both seen and unseen ones.
To overcome this, generalized ZS3 (GZS3) has recently been introduced to consider both seen and unseen classes in a scene during inference. Motivated by generative ap-proaches [2, 50, 52] in zero-shot image classiﬁcation, many
GZS3 methods [3, 15, 32] ﬁrst train a segmentation net-work that consists of a feature extractor and a classiﬁer with seen classes. They then freeze the feature extractor to extract visual features, and discard the classiﬁer. With the ﬁxed feature extractor, a generator [14, 25] is trained to produce visual features from semantic ones (e.g., word2vec) of corresponding classes. This enables training novel clas-siﬁers with real visual features of seen classes and gener-ated ones of unseen classes (Fig. 1 top). Although genera-tive methods achieve state-of-the-art performance in GZS3, they have the following limitations: (1) the feature extractor is trained without considering semantic features, causing a bias towards seen classes. The seen bias problem becomes even worse through a multi-stage training strategy, where the generator and novel classiﬁers are trained using the fea-ture extractor; (2) the classiﬁer needs to be re-trained when-ever a particular unseen class is newly included/excluded, hindering deployment in a practical setting, where unseen classes are consistently emerging.
We introduce a discriminative approach for GZS3, dubbed JoEm, that addresses the limitations of generative methods in a uniﬁed framework (Fig. 1 bottom). Specif-ically, we exploit visual and semantic encoders to learn a joint embedding space. The semantic encoder transforms semantic features into semantic prototypes acting as centers for visual features of corresponding classes. Our approach to using the joint embedding space avoids the multi-stage training, and thus alleviates the seen bias problem. To this end, we propose to minimize the distances between visual features and corresponding semantic prototypes in the joint embedding space. We have found that visual features at object boundaries could contain a mixture of different se-mantic information due to the large receptive ﬁeld of deep
CNNs. Directly minimizing the distances between the vi-sual features and semantic prototypes might distract dis-criminative feature learning. To address this, we propose a boundary-aware regression (BAR) loss that exploits seman-tic prototypes linearly interpolated to gather the visual fea-tures at object boundaries along with its efﬁcient implemen-tation. We also propose to use a semantic consistency (SC) loss that transfers relations between seen classes from a se-mantic embedding space to the joint one, regularizing the distances between semantic prototypes of seen classes ex-plicitly. At test time, instead of re-training the classiﬁer as in the generative methods [3, 15, 32], our approach to learning discriminative semantic prototypes enables using a nearest neighbor (NN) classiﬁer [7] in the joint embedding space. In particular, we modulate the decision boundary of the NN classiﬁer using the Apollonius circle. This Apollo-nius calibration (AC) method also makes the NN classiﬁer less susceptible to the seen bias problem. We empirically demonstrate the effectiveness of our framework on standard
GZS3 benchmarks [10, 40], and show that AC boosts the performance signiﬁcantly. The main contributions of our work can be summarized as follows:
• We introduce a simple yet effective discriminative ap-proach for GZS3. We propose BAR and SC losses, which are complementary to each other, to better learn discrim-inative representations in the joint embedding space.
• We present an effective inference technique that modu-lates the decision boundary of the NN classiﬁer adap-tively using the Apollonius circle. This alleviates the seen bias problem signiﬁcantly, even without re-training the classiﬁer.
• We demonstrate the effectiveness of our approach exploit-ing the joint embedding space on standard benchmarks for GZS3 [10, 40], and show an extensive analysis with ablation studies. 2.