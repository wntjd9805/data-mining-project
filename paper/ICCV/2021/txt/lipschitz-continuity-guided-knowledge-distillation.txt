Abstract
Knowledge distillation has become one of the most im-portant model compression techniques by distilling knowl-edge from larger teacher networks to smaller student ones.
Although great success has been achieved by prior dis-tillation methods via delicately designing various types of knowledge, they overlook the functional properties of neu-ral networks, which makes the process of applying those techniques to new tasks unreliable and non-trivial. To al-leviate such problem, in this paper, we initially leverage
Lipschitz continuity to better represent the functional char-acteristic of neural networks and guide the knowledge dis-tillation process. In particular, we propose a novel Lips-chitz Continuity Guided Knowledge Distillation framework to faithfully distill knowledge by minimizing the distance between two neural networks’ Lipschitz constants, which enables teacher networks to better regularize student net-works and improve the corresponding performance. We de-rive an explainable approximation algorithm with an ex-plicit theoretical derivation to address the NP-hard prob-lem of calculating the Lipschitz constant. Experimental re-sults have shown that our method outperforms other bench-marks over several knowledge distillation tasks (e.g., classi-fication, segmentation and object detection) on CIFAR-100,
ImageNet, and PASCAL VOC datasets. Our code is avail-able at https://github.com/42Shawn/LONDON/ tree/master. 1.

Introduction
Recently, deep learning models have driven great ad-vances in computer vision [13, 8], natural language pro-information retrieval [42, 43] and multi-cess [36, 33], the buoyant de-modal modelling [19, 18]. mand of equipping those cumbersome models in resource-To meet
* Equal contribution. † Corresponding author. constrained edge devices, researchers have proposed sev-eral network compression paradigms, such as network prun-ing [24, 12], network quantization [20] and knowledge dis-tillation (KD) [16]. Among these compression methods,
KD helps the training process of a smaller network (student) by transferring knowledge from a larger one (teacher). As one of the first innovators, Hinton et al. [16] proposed using soft labels of the larger networks to supervise the training process of the smaller ones. These soft labels are usually interpreted as a form of unseen knowledge distilled from teachers.
Apart from treating soft labels as distilled knowledge, various kinds of knowledge are designed in [45, 14, 38, 41].
For example, Romero et al. [34] presented to train interme-diate layers of students with guidance of the corresponding layers of teachers, which initiates the subsequent flourish-ing studies on feature-based knowledge distillation. Re-searchers [45, 25, 39] also modulated the relations among adjacent feature maps as additional knowledge to assist the training of student networks. Unfortunately, most of these feature-based KD methods solely focus on aligning the shallow information but overlook the high-level infor-mation of both networks, i.e., the students mechanically mimicking teachers’ actions while neglecting their interior qualities. Thereby, previous studies consider networks as black-boxes and heuristically select features without any functional properties [38, 41, 48], which impedes a univer-sal representative of knowledge to be distilled. To address this problem, we argue that leveraging networks’ func-tional properties to derive high-level knowledge is able to strengthen the performance of KD.
In this paper, we incorporate Lipschitz continuity into
KD, considering neural networks as functions rather than black-boxes. By definition in Eq. 4, Lipschitz constant1 is the upper bound of the relationship between input pertur-1The Lipschitz constant of a function ∥f ∥Lip is the maximum norm of its gradient in the domain set, which reflects Lipschitz continuity of the function.
bation and output variation for a given distance, represent-ing the robustness and expressiveness of neural networks
[1, 29, 28]. Specifically, authors in [29, 46] demonstrated the effectiveness of the Lipschitz constant by constraining the weights of the discriminator in a generative adversar-ial network (GAN). Besides, many studies in representa-tion learning [2, 37] demonstrate that deep neural networks are competent in learning high-level information with in-creasing abstraction. Inspired by this, we devise a scheme to capture the Lipschitz continuity (i.e., calculate the Lips-chitz constant for every intermediate block) of the teacher networks and adopt the captured continuity as knowledge to guide the training of student networks. It is worth not-ing that Lipschitz constant computation is a NP-hard prob-lem [40]. We address this problem by proposing an ap-proximation algorithm with a tight upper bound.
In par-ticular, we design a Transmitting Matrix (TM) for each block and calculate the spectral norm of TM through an adopted iteration method to avoid the high complexity of learning large intermediate matrices. We then aggregate all
Lipschitz constants calculated from TMs as the knowledge of the Lipschitz continuity that are transferred to student networks.
Importantly, Lipschitz continuity loss function is backpropagation-friendly for training deep networks be-cause of its differentiability.
Overall, the contributions of this paper are four-fold:
• To the best of our knowledge, we are the first on utiliz-ing a high-level functional property, Lipschitz continuity in knowledge distillation, to supervise student networks’ training process. In addition, we theoretically explain the effectiveness of our method from the perspective of net-work regularization and then empirically consolidate this explanation.
• We propose a novel knowledge distillation framework,
Lipschitz cONtinuity Guided Knowledge DistillatiON (LONDON) for distilling knowledge from the Lipschitz constant.
• To avoid the NP-hard Lipschitz constant calculation, we devise a Transmitting Matrix to numerically approximate the Lipschitz constant of networks in the KD process.
• We perform experiments on different knowledge distil-lation tasks such as classification, object detection, and segmentation. Our proposed method achieves the state-of-the-art results in these tasks on CIFAR-100, ImageNet, and VOC datasets. 2.