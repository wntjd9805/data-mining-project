Abstract scale environments).
For several emerging technologies such as augmented reality, autonomous driving and robotics, visual localiza-tion is a critical component. Directly regressing cam-era pose/3D scene coordinates from the input image using deep neural networks has shown great potential. However, such methods assume a stationary data distribution with all scenes simultaneously available during training. In this pa-per, we approach the problem of visual localization in a continual learning setup – whereby the model is trained on scenes in an incremental manner. Our results show that similar to the classification domain, non-stationary data in-duces catastrophic forgetting in deep networks for visual localization. To address this issue, a strong baseline based on storing and replaying images from a fixed buffer is pro-posed. Furthermore, we propose a new sampling method based on coverage score (Buff-CS) that adapts the existing sampling strategies in the buffering process to the problem of visual localization. Results demonstrate consistent im-provements over standard buffering methods on two chal-lenging datasets – 7Scenes, 12Scenes, and also 19Scenes by combining the former scenes1. 1.

Introduction
Camera relocalization is a fundamental problem aimed at estimating 6 degree-of-freedom (DoF) camera pose with respect to a known environment. Visual localization aims to solve this problem requiring only RGB images as input [52– 54, 56]. Traditional methods [50–56, 61, 62] require build-ing a 3D map of the environment followed by an explicit matching stage [58, 59] to establish 2D pixels to 3D coordi-nates. Recently with the success of deep neural networks, the problem can now be solved end-to-end by directly re-gressing the camera pose [12, 31–33, 42, 47, 57, 63, 67] or 3D scene coordinates [9–11, 37, 68]. This has shown to be more accurate than feature-based methods (at least for small
*The first two authors contributed equally. 1Code and materials are available at https://github.com/
AaltoVision/CL_HSCNet.
One of the limitations of end-to-end regression methods for visual localization is limited scalability to larger envi-ronments with several scenes. Although the methods per-formed well when trained and evaluated on a single scene, the performance quickly degraded when jointly trained on multiple scenes. This was mitigated by considering a hi-erarchical approach [37] to localize a given input image – first obtain a coarse localization in terms of the scene or sub-scene, followed by estimating a finer camera pose esti-mate. In this work, we push the methods further towards a general intelligence setting – learn continually from the in-coming stream of data. Under this setting, all the scenes are not available during training but encountered sequentially one after the other as shown in Figure 1. There are several benefits in terms of sample and memory efficiency to learn-ing tasks in a continual manner over the setting of training jointly over all tasks. In the joint training setting, each time a scene has changed, the model needs to be retrained on all the scenes in the database – even the ones that have not undergone any change. Adding new scenes to the database also requires model retraining which affects the scalability.
Due to the above issues, the full dataset needs to be stored in memory. In contrast, continual learning (CL) [1, 2, 19, 35] aims to reduce the computational costs by fine-tuning the model only on the changed/new scene and images from the previous scenes stored in a small buffer. Furthermore, the memory costs are also reduced as only the data for the cur-rent scene needs to be stored in memory along with a small buffer of images from previous scenes. This is of particular importance for mobile applications where storage capacities are device constrained.
Solely training on the images from the current scene leads to catastrophic forgetting of knowledge gained from prior scenes. This is attributed to the interference of the gradients from the current task images with the model pa-rameters learned on previous scenes. The performance of neural networks in such non-stationary data distribution set-ting is well studied under the domain of continual learning.
The CL problem is broadly categorized into i) class/task
CL: all the data from current class/task is available and the
Figure 1. Overview of our replay-based continual learning approach for visual localization. During each scene (task) iteration, the model is updated using the current and previous task samples. The former is sampled from a small fixed-size buffer. After the training is over, a small subset of the current task samples are stored in the buffer by replacing parts of stored data from previous tasks. model is allowed to have repeated passes through the whole dataset, and ii) online CL: the task boundary changes sud-denly. To mitigate the challenges of CL, several approaches are proposed: i) regularization methods [1, 35, 45] that pe-nalize changes in weights considered important for previ-ous scenes or directly impose orthogonality constraints in the training objective, ii) modular methods [2, 28, 49] that increase the model capacity assigning new parameters for each task, and iii) replay methods [22, 48] that perform ex-perience replay by storing samples from previous scenes in a fixed size buffer or using generative models to generate images of past scenes. All the three methods incur limited memory and computational costs as regularization meth-ods require past gradients or feature maps to be stored in memory like the replay-based approaches, while modular approaches require an increase in model size. For a fixed model size, experience replay based methods have shown superior performance compared to regularization methods, and in some recent works [19], a combination of both also leads to good results.
In this work, we consider the task CL setting for the problem of visual localization in the context of experience-replay based solutions. We adopt some of the buffer-ing methods from literature – Reservoir [66], and Class-balance [22] to perform experience replay. A strong base-line is created using these methods and challenges specific to the visual localization problem highlighted. Unlike the classification domain where each image is representative of the whole class, visual localization scenes consist of diverse sets of images spanning a whole 3D environment. Exist-ing buffering methods do not take the 3D scene layout into account. Storing images from just one part of the scene does not guarantee generalization to images from other dis-joint parts. To retain performance on several parts of the scene, we propose a buffering process that ensures the im-ages stored in the buffer will have higher scene coverage.
This is done by computing a coverage score factor that in-dicates if buffering new incoming image will improve the existing coverage score of buffer images. The proposed buffering algorithm outperforms existing methods on chal-lenging datasets – 7Scenes, 12Scenes, and also 19Scenes obtained by combining the former scenes.
To summarize, we make the following contributions:
• Introduce the problem of continual learning for visual localization.
• Create a strong experience-replay baseline from exist-ing buffering methods across several indoor datasets.
• Propose a new buffering strategy conditioned on the 3D geometry of the scene. 2.