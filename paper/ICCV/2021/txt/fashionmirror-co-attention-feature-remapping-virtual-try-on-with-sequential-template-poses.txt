Abstract
Virtual try-on tasks have drawn increased attention.
Prior arts focus on tackling this task via warping clothes and fusing the information at the pixel level with the help of semantic segmentation. However, conducting semantic segmentation is time-consuming and easily causes error ac-cumulation over time. Besides, warping the information at the pixel level instead of the feature level limits the perfor-mance (e.g., unable to generate different views) and is un-stable since it directly demonstrates the results even with a misalignment. In contrast, fusing information at the fea-ture level can be further refined by the convolution to ob-tain the final results. Based on these assumptions, we pro-pose a co-attention feature-remapping framework, namely
FashionMirror, that generates the try-on results according to the driven-pose sequence in two stages. In the first stage, we consider the source human image and the target try-on clothes to predict the removed mask and the try-on clothing mask, which replaces the pre-processed semantic segmen-tation and reduces the inference time. In the second stage, we first remove the clothes on the source human via the re-moved mask and warp the clothing features conditioning on the try-on clothing mask to fit the next frame human. Mean-while, we predict the optical flows from the consecutive 2D poses and warp the source human to the next frame at the feature level. Then, we enhance the clothing features and source human features in every frame to generate realistic try-on results with spatio-temporal smoothness. Both quali-tative and quantitative results show that FashionMirror out-performs the state-of-the-art virtual try-on approaches. 1.

Introduction
In this paper, we envisage a new shopping scenario.
Imagine that we stand in front of a fashion mirror inside a shopping mall. The fashion mirror shows the real-time
Figure 1. Virtual try-on results in sequential poses. virtual try-on results of the selected clothes. Therefore, we can exhibit arbitrary poses to guide the synthesized try-on result in the fashion mirror for viewing how suitable the gar-ments are in multi-aspects as demonstrated in Fig. 1. To achieve this goal, one intuitive way is to apply the single-pose virtual try-on methods (e.g., [41]) for the first frame and apply the sequential pose transformation (e.g., [31]) for the following frames. However, since the clothing informa-tion only depends on the first frame, errors may accumulate in sequential generation. Another possible solution is to use the multi-pose virtual try-on methods (e.g., [19]) in a frame-by-frame manner. Nevertheless, the results of consecutive frames may be inconsistent (i.e., flickering artifacts) since they are generated independently.
To consider the sequential information for video-based virtual try-on, FWGAN [10] proposes a flow-navigated warping GAN, which (i) warps the clothing image and re-fines the clothing texture at the pixel level, (ii) warps the previous frame via the optical flow generated by [11], and (iii) conducts the pre-processed semantic segmentation to remove the clothes from the source human image. How-ever, the performance is limited by the generated optical flow and warping the clothes at the pixel level prohibits
the network from generating new contents. For example, the clothing contents in the side view cannot be obtained from the front view. Meanwhile, fusing the refined clothes at the pixel level easily generates unstable results (e.g., the critical occlusion problem). For example, when the try-on model conducts the fusion between humans with limbs in front of the torso and warped clothes at the pixel level, the clothes veil the limbs as the green box shown in Fig. 4.
Most previous works [38, 14, 41, 28, 9, 45, 19, 10] require the pre-processed semantic segmentation, which is time-consuming, and the quality of the segmentation highly af-fects the follow-up try-on results. To reduce the time cost,
[22] proposes a parsing-free virtual try-on method. How-ever, it cannot transfer the users’ pose to obtain the infor-mation from different views. This is important for the real-world try-on scenario since users usually try on the clothes and exhibit different poses for evaluating whether the gar-ment is suitable.
To address these issues, we propose a co-attention feature-remapping try-on framework, namely FashionMir-ror. Given a source human image, a target try-on cloth-ing image, and a guiding pose sequence, the goal is to synthesize the try-on results in sequential poses according to the guiding pose sequence. The proposed FashionMir-ror framework consists of two stages: (I) parsing-free co-attention mask prediction and (II) human and clothing fea-ture remapping. In stage (I), instead of using the semantic body parts, FashionMirror directly leverages a co-attention mechanism to learn the relation between consecutive human frames and the target try-on clothes for finding the regions related to the try-on clothes. Based on the co-attended re-sults, FashionMirror predicts i) the removed mask, repre-senting the clothing region that should be removed from the source human, and ii) the target try-on clothing mask, repre-senting the region that the target clothing should fit. In stage (II), the goal is to synthesize the target try-on results based on the removed and target try-on clothing masks. Since remapping the visual information of target try-on clothes at the pixel level suffers from different-view and unstable is-sues as mentioned earlier, we warp the human and clothing information at the feature level for achieving the realistic try-on results. Specifically, the skeleton flow extraction net-work learns the feature-level optical flows among consecu-tive frames. By warping the current-frame human features with the extracted feature flows, we conduct the human se-quence generation to transfer the current-frame human to the next pose. Furthermore, we enhance the source human feature and the target clothing feature within every frame for improving detailed information.
We evaluate the efficacy of the proposed FashionMir-ror with several state-of-the-art methods on both subjec-tive and objective experiments. The results manifest that
FashionMirror outperforms state-of-the-art methods quali-tatively and quantitatively. The contributions are summa-rized as follows:
• We propose a co-attention feature-remapping virtual try-on framework, namely FashionMirror, to envisage a new shopping scenario via synthesizing the realistic try-on results in sequential poses with spatio-temporal smoothness.
• The proposed parsing-free co-attention mask mecha-nism replaces the commonly-used semantic segmenta-tion for virtual try-on and reduces the inference time by 42.84%. 2.