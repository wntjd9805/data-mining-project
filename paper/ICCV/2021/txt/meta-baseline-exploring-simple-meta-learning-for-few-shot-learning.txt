Abstract
Meta-learning has been the most common framework for few-shot learning in recent years. It learns the model from collections of few-shot classiﬁcation tasks, which is believed to have a key advantage of making the training objective consistent with the testing objective. However, some re-cent works report that by training for whole-classiﬁcation, i.e. classiﬁcation on the whole label-set, it can get compa-rable or even better embedding than many meta-learning algorithms. The edge between these two lines of works has yet been underexplored, and the effectiveness of meta-learning in few-shot learning remains unclear. In this paper, we explore a simple process: meta-learning over a whole-classiﬁcation pre-trained model on its evaluation metric.
We observe this simple method achieves competitive per-formance to state-of-the-art methods on standard bench-marks. Our further analysis shed some light on understand-ing the trade-offs between the meta-learning objective and the whole-classiﬁcation objective in few-shot learning. Our code is available at https://github.com/yinboc/ few-shot-meta-baseline. 1.

Introduction
While humans have shown incredible ability to learn from very few examples and generalize to many different new examples, the current deep learning approaches still rely on a large scale of training data. To mimic this hu-man ability of generalization, few-shot learning [3, 27] is proposed for training networks to understand a new con-cept based on a few labeled examples. While directly learn-ing a large number of parameters with few samples is very challenging and most likely leads to overﬁtting, a practical setting is applying transfer learning: train the network on common classes (also called base classes) with sufﬁcient samples, then transfer the model to learn novel classes with a few examples.
The meta-learning framework for few-shot learning fol-lows the key idea of learning to learn. Speciﬁcally, it sam-ples few-shot classiﬁcation tasks from training samples be-longing to the base classes and optimizes the model to per-form well on these tasks. A task typically takes the form of N -way and K-shot, which contains N classes with K support samples and Q query samples in each class. The
Q query samples into the goal is to classify these N
⇥
K support samples. Under
N classes based on the N
⇥ this framework, the model is directly optimized on few-shot classiﬁcation tasks. The consistency between the ob-jectives of training and testing is considered as the key ad-vantage of meta-learning. Motivated by this idea, many re-cent works [25, 5, 24, 28, 4, 21, 10, 30] focus on improving the meta-learning structure, and few-shot learning itself has become a common testbed for evaluating meta-learning al-gorithms.
However, some recent works ﬁnd that training for whole-classiﬁcation, i.e. classiﬁcation on the whole training label-set (base classes), provides the embedding that is compa-rable or even better than many recent meta-learning algo-rithms. The effectiveness of whole-classiﬁcation models has been reported in both prior works [5, 1] and some con-current works [29, 26]. Meta-learning makes the form of training objective consistent with testing, but why it turns out to learn even worse embedding than simple whole-classiﬁcation? While there are several possible reasons, e.g. optimization difﬁculty or overﬁtting, the answer has not been clearly studied yet. It remains even unclear that whether meta-learning is still effective compared to whole-classiﬁcation in few-shot learning.
In this work, we aim at exploring the edge between whole-classiﬁcation and meta-learning by decoupling the discrepancies. We start with Classiﬁer-Baseline: a whole-classiﬁcation method that is similarly proposed in concur-rent works [29, 26].
In Classiﬁer-Baseline, we ﬁrst train a classiﬁer on base classes, then remove the last fully-connected (FC) layer which is class-dependent. During test time, it computes mean embedding of support samples for each novel class as their centroids, and classiﬁes query sam-ples to the nearest centroid with cosine distance. We ob-serve this baseline method outperforms many recent meta-learning algorithms.
In order to understand whether meta-learning is still ef-fective compared to whole-classiﬁcation, a natural experi-ment is to see what happens if we perform further meta-learning over a converged Classiﬁer-Baseline on its evalu-ation metric (i.e. cosine nearest-centroid). As a resulting method, it is similar to MatchingNet [27] or ProtoNet [23] with an additional classiﬁcation pre-training stage. We observe that meta-learning can still improve Classiﬁer-Baseline, and it achieves competitive performance to state-of-the-art methods on standard benchmarks. We call this simple method Meta-Baseline. We highlight that as a method, all the individual components of Meta-Baseline have been proposed in prior works, but to the best of our knowledge, it has been overlooked that none of the prior works studies them as a whole. We further decouple the discrepancies by evaluating on two types of generaliza-tion: base class generalization denotes performance on few-shot classiﬁcation tasks from unseen data in the base classes, which follows the common deﬁnition of general-ization (i.e. evaluated in the training distribution); and novel class generalization denotes performance on few-shot clas-siﬁcation tasks from data in novel classes, which is the goal of the few-shot learning problem. We observe that: (i) During meta-learning, improving base class generaliza-tion can lead to worse novel class generalization; (ii) When training Meta-Baseline from scratch (i.e. without whole-classiﬁcation training), it achieves higher base-class gener-alization but much lower novel class generalization.
Our observations suggest that there could be a trade-off between the objectives of meta-learning and whole-classiﬁcation. It is likely that meta-learning learns the em-bedding that works better for N -way K-shot tasks, while whole-classiﬁcation learns the embedding with stronger class transferability. We ﬁnd that the main advantage of training for whole-classiﬁcation before meta-learning is likely to be improving class transferability. Our further ex-periments provide a potential explanation of what makes
Meta-Baseline a strong baseline: by inheriting one of the most effective evaluation metrics of the whole-classiﬁcation model, it maximizes the reusing of the embedding with strong class transferability. From another perspective, our results also rethink the comparison between meta-learning and whole-classiﬁcation from the perspective of datasets.
When base classes are collected to cover the distribution of novel classes, novel-class generalization should converge to base-class generalization and the strength of meta-learning may overwhelm the strength of whole-classiﬁcation.
In summary, our contributions are as following:
• We present a simple Meta-Baseline that has been over-looked in prior work. It achieves competitive perfor-mance to state-of-the-art methods on standard bench-marks and is easy to follow.
• We observe a trade-off between the objectives of meta-learning and whole-classiﬁcation, which potentially explains the success of Meta-Baseline and rethinks the effectiveness of both objectives in few-shot learning. 2.