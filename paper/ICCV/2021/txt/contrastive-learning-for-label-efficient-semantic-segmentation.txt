Abstract
Collecting labeled data for the task of semantic segmen-tation is expensive and time-consuming, as it requires dense pixel-level annotations. While recent Convolutional Neural
Network (CNN) based semantic segmentation approaches have achieved impressive results by using large amounts of labeled training data, their performance drops significantly as the amount of labeled data decreases. This happens be-cause deep CNNs trained with the de facto cross-entropy loss can easily overfit to small amounts of labeled data. To address this issue, we propose a simple and effective con-trastive learning-based training strategy in which we first pretrain the network using a pixel-wise, label-based con-trastive loss, and then fine-tune it using the cross-entropy loss. This approach increases intra-class compactness and inter-class separability, thereby resulting in a better pixel classifier. We demonstrate the effectiveness of the proposed training strategy using the Cityscapes and PASCAL VOC 2012 segmentation datasets. Our results show that pretrain-ing with the proposed contrastive loss results in large per-formance gains (more than 20% absolute improvement in some settings) when the amount of labeled data is limited.
In many settings, the proposed contrastive pretraining strat-egy, which does not use any additional data, is able to match or outperform the widely-used ImageNet pretraining strat-egy that uses more than a million additional labeled images. 1.

Introduction
In the recent past, various approaches based on Convo-lutional Neural Networks (CNNs) [6, 8, 64] have reported excellent results on several semantic segmentation datasets by first pretraining their models on the large-scale Ima-geNet [13] classification dataset and then fine-tuning them with large amounts of pixel-level annotations. This training
*This work was done when Xiangyun Zhao was interning at Google. strategy has several disadvantages: First, collecting a large, pixel-level annotated dataset is time-consuming and expen-sive. For example, the average time taken to label a single image in the Cityscapes dataset is 90 minutes [11]. Second, the ImageNet dataset can only be used for non-commercial research, making the ImageNet pretraining strategy unsuit-able for building real-world products. Collecting a propri-etary large-scale classification dataset similar to ImageNet would be expensive and time-consuming. Third, ImageNet pretraining does not necessarily help segmentation of non-natural images, such as medical images [44].
To reduce the need for large amounts of dense pixel-level annotations and the additional large-scale labeled ImageNet dataset, this work focuses on training semantic segmenta-tion models using only a limited number of pixel-level an-notated images (no ImageNet dataset). This is challenging since CNN models can easily overfit to limited training data.
Typical semantic segmentation models consist of a deep
CNN feature extractor followed by a pixel-wise softmax classifier, and are trained using a pixel-wise cross-entropy loss. While these models perform well when trained with a large number of pixel-level annotated images, their perfor-mance drops significantly as the number of labeled train-ing images decreases (see Fig. 1). This happens because
CNNs trained with the cross-entropy loss can easily overfit to small amounts of labeled data, as the cross-entropy loss focuses on creating class decision boundaries and does not explicitly encourage intra-class compactness or large mar-gins between classes [15, 37, 49].
To address this issue, we propose to first pretrain the feature extractor using a pixel-wise, label-based contrastive loss (referred to as contrastive pretraining), and then fine-tune the entire network including the pixel-wise softmax classifier using the cross-entropy loss (referred to as soft-max fine-tuning). This approach increases both intra-class compactness and inter-class separability as the label-based contrastive loss [32] encourages the features of pixels from the same class to be close to each other and the features of
Figure 1. When trained with pixel-wise cross-entropy loss, the per-formance of a semantic segmentation model drops significantly as the number of labeled training images decreases. Here, we use a
DeepLabV3+ [8] model with the ResNet50-based encoder of [7]. pixels from different classes to be far away. The increased intra-class compactness and inter-class separability natu-rally lead to a better pixel classifier in the fine-tuning stage.
Figures 2 and 3 show the distributions of various classes in the softmax input feature spaces of models trained with the cross-entropy loss and the proposed strategy, respectively, using 2118 labeled images from the PASCAL VOC 2012 dataset. The mean IOU values of the corresponding models on the PASCAL VOC 2012 validation dataset are 39.1 and 62.7, respectively. The class support regions are more com-pact and separated when trained with the proposed strategy, leading to a better performance. We use t-SNE [52] for gen-erating the visualizations.
Various existing semi-supervised and weakly-supervised semantic segmentation approaches also focus on reducing the need for pixel-level annotations by leveraging additional unlabeled images [5, 20, 27, 41] or weaker forms of anno-tations such as bounding boxes [12, 31, 42, 47] and image-level labels [1, 34, 42]. In contrast to these approaches, the proposed contrastive pretraining strategy does not use any additional data, and is complimentary to them.
Pixel-wise cross-entropy loss ignores the relationships between pixels. To address this issue, region-based loss functions such as region mutual information loss [65] and affinity field loss [30] have been proposed. Different from these loss functions which model pixel relationships in the label space, the proposed contrastive loss models pixel rela-tionships in the feature space. Also, while these loss func-tions only model relationships between pixels within a local neighborhood, the proposed loss encourages the features of same class pixels to be similar and features of different class pixels to be dissimilar irrespective of their image locations.
Some recent works such as [4, 43, 55, 58, 59] also used pixel-wise contrastive loss for the task of semantic segmen-tation. However, these works focus on leveraging unlabeled data through self-supervised contrastive learning, and make
Figure 2. Distribution of various classes in the softmax input fea-ture space of a model trained using only cross-entropy loss on 2118 labeled images from the PASCAL VOC 2012 dataset.
Figure 3. Distribution of various classes in the softmax input fea-ture space of a model trained using the proposed training strategy on 2118 labeled images from the PASCAL VOC 2012 dataset. use of the labels only in the fine-tuning stage. In contrast, we focus on supervised contrastive learning, and make use of the labels in both pretraining and fine-tuning stages.
We perform experiments on two widely-used semantic segmentation benchmark datasets, namely, Cityscapes and
PASCAL VOC 2012, and show that pixel-wise, label-based contrastive pretraining results in large performance gains when the amount of labeled data is limited.
Our main contributions are as follows:
• New loss functions: We extend supervised contrastive learning [32] to the task of semantic segmentation.
We propose and evaluate three variants of pixel-wise, label-based contrastive loss.
• Simple training approach: We propose a simple con-trastive learning-based pretraining strategy for improv-ing the performance of semantic segmentation models.
We consider the simplicity of our pretraining strategy as its main strength since it can be easily adopted by existing and future semantic segmentation approaches.
• Strong results: We show that label-based contrastive pretraining results in large performance gains on two widely-used semantic segmentation datasets when the amount of labeled data is limited. We also show that, in most settings, the proposed contrastive pretrain-ing which does not use any additional data, outper-forms the widely-used ImageNet pretraining which uses more than a million additional labeled images.
• Detailed analyses: We show visualizations of class distributions in the feature spaces of trained models to provide insights into why the proposed training strat-egy works better (Fig. 2 and 3). We also present abla-tion studies that justify our two-stage training strategy. 2.