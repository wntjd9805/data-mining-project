Abstract
We consider the scalable recognition problem in the fine-grained expert domain where large-scale data collection is easy whereas annotation is difficult. Existing solutions are typically based on semi-supervised or self-supervised learning. We propose an alternative new framework, MEM-ORABLE, based on machine teaching and online crowd-sourcing platforms. A small amount of data is first labeled by experts and then used to teach online annotators for the classes of interest, who finally label the entire dataset.
Preliminary studies show that the accuracy of classifiers trained on the final dataset is a function of the accuracy of the student annotators. A new machine teaching algo-rithm, CMaxGrad, is then proposed to enhance this accu-racy by introducing explanations in a state-of-the-art ma-chine teaching algorithm. For this, CMaxGrad leverages counterfactual explanations, which take into account stu-dent predictions, thereby proving feedback that is student-specific, explicitly addresses the causes of student confu-sion, and adapts to the level of competence of the student.
Experiments show that both MEMORABLE and CMaxGrad outperform existing solutions to their respective problems. 1.

Introduction
The success of deep learning in computer vision has been largely driven by large-scale datasets. Many breakthroughs, made across various tasks, have benefited from large-scale and well-curated datasets like ImageNet for object recogni-tion [6], COCO for object detection and segmentation [23],
Kinetics for action recognition [19], etc. These datasets usually contain common objects, scenes, or actions and thus can be scalably annotated on crowdsourcing platforms such as Amazon Mechanical Turk (MTurk) [14]. When this is possible, we say that learning is scalable. However, this is usually not the case for expert domains, such as biol-ogy or medical imaging. While data collection can still be easy in these domains, annotations require highly special-ized and domain-specific knowledge. For example, while it
Figure 1: The proposed MEMORABLE framework is a new solu-tion to the problem of large-scale recognition in fine-grained do-mains. 1⃝ A large-scale raw dataset (D) is collected and a small subset delivered to experts, who produce a labeled dataset Dl; 2⃝
A neural network is trained on Dl or semi-supervised trained on
Dl ∪ Du, where Du = D − Dl; 3⃝ A teaching tutorial, composed of a teaching set L of images and associated explanations, is cre-ated from Dl with the CMaxGrad algorithm, and used to teach human annotators the target categories; 4⃝ human annotators label the unlabeled data Du. Finally, the classifier is re-trained on the fully labeled datset D. (Red and blue cylinders represent whether data is labeled or not.) is easy to crawl the web or deploy cameras in the wild to collect a large number of animal images, it is usually ex-pensive to recruit the biologists or taxonomists needed to label them. The resulting lack of large annotated datasets hampers the application of deep learning to expert domains.
For example, the largest existing bird dataset, NAbirds, only contains about 48k instances [37]. Even the recent and largest biological dataset, iNaturalist, contains only about 850k instances [38]. This is smaller than ImageNet, pro-posed about 10 years ago, and pales in comparison to the largest datasets of everyday objects, e.g. Open Images with 9M images [20].
Since labeling is difficult in expert fine-grained domains, scalable learning must take advantage of small expert-labeled datasets and large amounts of unlabeled data. This motivated extensive research on less label-intensive forms of learning, including few-shot learning, transfer learning, semi-supervised learning, and self-supervised learning. For example, models pre-trained on an everyday domain by su-pervised learning are frequently transferred to a target fine-grained domain by fine-tuning. Another strategy is to learn a good feature extractor by self-supervised learning, which
requires no labels, and then fine-tune a classifier at the top of it on a small set of labeled target data. However, these approaches usually underperform scalable supervised learn-ing. For example, state-of-the-art self-supervised learn-ing with SimCLR [3] underperforms a supervised baseline when only a subset of the samples are labeled, especially on fine-grained domains [43].
Unlike all these approaches, we pursue the alternative so-lution of scaling up the process of data annotation. While this was a pie in the sky idea in the past, two recent develop-ments now make it promising. First, several crowdsourcing platforms, like Amazon Mechanical Turk, Sama [15], mi-croWorkers [13], or Clickworker [12], have appeared in re-cent years, making it easier to recruit large numbers of im-age annotators online. Second, research has been steadily increasing in the area of machine teaching [48, 47, 28], showing potential to develop algorithms capable of teaching these annotators the domain-specific knowledge needed to label expert data. While these developments are promising, there have been so far no efforts to study how they can be combined into a complete framework for scalable learning.
Typically, machine teaching papers only evaluate the accu-racy of the labeling produced by the annotators taught by their algorithms. While this is informative, it does not fully address the scalable learning problem, which also includes the design of deep learning systems using those annotations.
This raises an additional set of questions, such as what qual-ity must the labels have to guarantee effective deep learn-ing performance, how can the machine teaching algorithms achieve that quality, and whether noisy label learning algo-rithms [22, 9] have a role in the process.
In this work, we address these questions in the context of scalable learning of recognition systems, which we de-note as scalable recognition. We propose a new Machine tEaching fraMewORk for scAlaBLe rEcognition (MEMO-RABLE) in fine-grained expert domains, illustrated in Fig-ure 1. A large raw dataset (D) is first collected for a target fine-grained task, e.g. by deploying cameras in the wild or crawling archived medical images in a hospital database.
A small subset Dl ⊂ D and |Dl| ≪ |D| is then labeled by experts. Machine teaching is next used to teach non-experts, e.g. Amazon MTurk workers, how to label for the target categories. The unlabeled data Du = D/Dl is finally labeled by these humans and the complete dataset used to train an image recognition system. To identify critical areas of this framework, we perform an initial study with simu-lated noisy annotations. This shows that the accuracy of the machine teaching plays a significant role in the accuracy of the final recognition system. We then hypothesize that bet-ter machine teaching performance can be achieved by in-troducing explanations in the machine teaching algorithm.
State-of-the-art machine teaching algorithms [25, 26, 18] tend not to use explanations. Although there is literature do-ing [28], it tends to rely on attributive explanations [33, 46] that do not take into account the student predictions. To address this problem, we propose the addition of counter-factual explanations to machine teaching.
Counterfactual explanations [41, 8] take into account both ground-truth labels and student predictions, highlight-ing image regions that are most discriminant of student mis-takes. They are thus most instructive for humans to learn from their errors. Furthermore, because the explanatory feedback varies according to the student’s prediction, they naturally adjust to the level of competence of the student.
We seek to leverage all these benefits by introducing a gen-eralization of the recent MaxGrad machine teaching algo-rithm [40], denoted Counterfactual MaxGrad (CMaxGrad), which is endowed with counterfactual explanations. Exper-iments show that this algorithm both achieves state-of-the-art machine teaching performance and enables significant scalable recognition gains for the MEMORABLE frame-work. The latter is itself shown to outperform other scal-able recognition strategies, such as semi-supervised learn-ing. It is also shown that deep learning systems trained with
MEMORABLE can leverage noisy label training schemes with surprising effectiveness.
The contributions of the paper are summarized as 1) a study of the importance of labeling accuracy for the accu-racy of scalable recognition; 2) the MEMORABLE frame-work to solve the fine-grained scalable recognition problem, by leveraging crowdsourcing platforms and machine teach-ing algorithms; 3) the new CMaxGrad machine teaching algorithm that introduces counterfactual explanations into machine teaching; and 4) new benchmarks, based on two challenging datasets, for the evaluation of scalable recogni-tion. 2.