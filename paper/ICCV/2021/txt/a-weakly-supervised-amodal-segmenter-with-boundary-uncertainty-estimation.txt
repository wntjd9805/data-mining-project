Abstract
This paper addresses weakly supervised amodal instance segmentation, where the goal is to segment both visible and occluded (amodal) object parts, while training pro-vides only ground-truth visible (modal) segmentations. Fol-lowing prior work, we use data manipulation to generate occlusions in training images and thus train a segmenter to predict amodal segmentations of the manipulated data.
The resulting predictions on training images are taken as the pseudo-ground truth for the standard training of Mask-RCNN, which we use for amodal instance segmentation of test images. For generating the pseudo-ground truth, we specify a new Amodal Segmenter based on Boundary
Uncertainty estimation (ASBU) and make two contribu-tions. First, while prior work uses the occluder’s mask, our ASBU uses the occlusion boundary as input. Second,
ASBU estimates an uncertainty map of the prediction. The estimated uncertainty regularizes learning such that lower segmentation loss is incurred on regions with high uncer-tainty. ASBU achieves significant performance improve-ment relative to the state of the art on the COCOA and
KINS datasets in three tasks: amodal instance segmenta-tion, amodal completion, and ordering recovery. 1.

Introduction
This paper addresses weakly supervised amodal instance segmentation (WAIS). Our goal is to segment both vis-ible and occluded (amodal) parts of object instances in images. The weak supervision in training provides only ground-truth visible (modal) instance segmentations.
Im-portant applications of amodal segmentation include au-tonomous driving and robot path planning, where identify-ing the whole spatial extents of partially occluded objects is critical. Considering this problem under weak supervision is also important because human annotators often cannot provide reliable ground truth. For example, different anno-tators are likely to have very different and sometimes poor guesses of occluded object parts.
There is scant prior work on WAIS. Following recent
PCNet [36], our training consists of two stages. First, we use data augmentation to train a common image segmenter – UNet [32] – on manipulated training images to predict their amodal segmentations. As input to UNet, we use the available ground-truth modal segmentation and information about where the data augmentation generated the occlusion in the training image. In the second training stage, UNet’s amodal segmentations are taken as a pseudo-ground truth for learning a standard instance segmenter – Mask-RCNN
[13], as in [36]. On test images with occlusions, Mask-RCNN trained on the pseudo-ground truth is expected to output correct amodal instance segmentation.
Our contributions are aimed at advancing the first train-ing stage, and include: (1) a new way to exploit the weak supervision for training of UNet; and (2) enabling UNet to estimate uncertainty of the predicted amodal segmentation, and enforcing the training of UNet to explicitly minimize this uncertainty.
Our first contribution is motivated by the following lim-itation of PCNet [36]. For manipulating training images, as illustrated in Fig. 1, PCNet randomly places an occluder object onto an occludee object based on their ground-truth modal segmentation masks, and in this way artificially gen-erates the occluded mask of the occludee. Then, as three inputs to UNet, PCNet uses the manipulated training im-age, the occluded mask of the occludee, and the occluder’s mask. However, using the occluder’s mask as input to UNet puts the restrictive constraint that the occluder itself cannot be occluded by another object. To address this limitation,
PCNet estimates an object ordering graph in the image, and for the occluder selects a union of all objects estimated as closer to the camera than the occludee (i.e., a union of mul-tiple occluders). Our novelty is in replacing the occluder’s mask with the occlusion boundary in the input to UNet, as depicted in Fig. 1. Thus, our occluders are allowed to be themselves partially occluded by some other objects. This reduces complexity of estimating the pseudo-ground-truth amodal segmentation, as we do not need to estimate the ob-ject ordering graph.
Figure 1. (Top) A recent approach to WAIS [36] where UNet [32] is trained with binary cross-entropy loss to predict amodal segmentation from the manipulated training image, the occluder’s mask, and the occluded mask of the occludee. (Bottom) Our approach, called ASBU, differs [36] in terms of input, output, and loss. For input, instead of the occluder’s mask, we use the occlusion boundary, and instead of
“zeros” we use matting to superimpose the occluder onto the image. This means that we remove the information about the occluder’s spatial extent from the input to UNet. For output, along with amodal segmentation, we additionally estimate uncertainty of prediction. For loss, we use uncertainty to appropriately weight loss. GT stands for ground truth.
Our second contribution is aimed at accounting for shape priors. As shown in Fig. 4, we implicitly capture a “shape prior” through learning to estimate an uncertainty map for the predicted amodal segmentation. In our experiments, we observe that the estimated uncertainty typically takes low (high) values over regions far away (close) to the occlusion boundary. This suggests that our uncertainty map is capable of representing a spatial distribution of object shapes, and hence can be used for regularizing our learning. Our reg-ularization uses the estimated uncertainty map to appropri-ately modulate a difference between the predicted amodal segmentation and the original ground-truth mask of the oc-cludee (before the occlusion), such that lower loss is in-curred on regions with high uncertainty.
Our two contributions are incorporated in the new
Amodal Segmenter with Boundary Uncertainty estimation (ASBU). ASBU is evaluated on the COCOA [37] and KINS
[31] datasets on three tasks: amodal instance segmentation, amodal completion, and ordering recovery. ASBU signifi-cantly outperforms the state of the art in all three tasks.
In the following, Sec. 2 reviews previous work; Sec. 3 specifies ASBU; Sec. 4 formalizes our uncertainty estima-tion and uncertainty weighted loss; Sec. 5 presents our im-plementation details and experimental results; and Sec. 6 concludes the paper. 2.