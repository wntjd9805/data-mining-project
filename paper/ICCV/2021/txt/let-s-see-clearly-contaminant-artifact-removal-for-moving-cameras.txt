Abstract
Contaminants such as dust, dirt and moisture adhering to the camera lens can greatly affect the quality and clar-ity of the resulting image or video. In this paper, we pro-pose a video restoration method to automatically remove these contaminants and produce a clean video. Our ap-proach first seeks to detect attention maps that indicate the regions that need to be restored. In order to leverage the corresponding clean pixels from adjacent frames, we pro-pose a flow completion module to hallucinate the flow of the background scene to the attention regions degraded by the contaminants. Guided by the attention maps and com-pleted flows, we propose a recurrent technique to restore the input frame by fetching clean pixels from adjacent frames.
Finally, a multi-frame processing stage is used to further process the entire video sequence in order to enforce tempo-ral consistency. The entire network is trained on a synthetic dataset that approximates the physical lighting properties of contaminant artifacts. This new dataset and our novel framework lead to our method that is able to address differ-ent contaminants and outperforms competitive restoration approaches both qualitatively and quantitatively. 1.

Introduction
As imaging devices have become ubiquitous, the abil-ity to take photographs and videos everywhere and anytime has increased significantly. Mobile cameras, action cam-eras, surveillance cameras, and the sensors of autonomous driving cars are often exposed to the harsh environment in which contaminants will cause deterioration of image qual-ity. Figure 1 shows some examples of dirty lens artifacts, where the visibility of the scene radiance is partially af-fected by the absorption and reflection of the contaminants along the light path [17]. These undesired artifacts are not only aesthetically disturbing, but also bring difficulty for subsequent computer vision tasks. Although one can phys-ically clean the lens sporadically, doing this frequently is by no means a handy solution and sometimes infeasible for real-time situations.
Since the contaminants adhere to the lens surface and thereby lie out of focus, their imaging effect can be mod-eled by a low-frequency light modulation [17], i.e., the dirty lens artifacts appear diffuse and semi-transparent with the high-frequency textures of the background scene partially preserved. This makes image or video inpainting meth-ods [6, 55, 19, 46, 54] inadequate for our task as they com-pletely ignore the underlying structures and the hallucinated content. Albeit visually plausible, they may deviate signif-icantly from the real scene. Furthermore, these works as-sume the completion regions are prescribed by a user-given mask, whereas our task automatically identifies the degra-dation region, which is inferred from camera motion.
This work is more closely related to single image artifact
removal for raindrops [11, 18, 32, 33], reflection [3, 12, 45, 57] and thin obstructions [29]. These works typically adopt learning approaches, utilizing the spatial prior of natural images to restore the spatial variant degradation. Nonethe-less, the artifact removal for a single image is inherently ill-posed, and the learned spatial prior often fails to generalize to scenes with domain gaps. To solve this, multi-frame ap-proaches [2, 28, 47] decouple the occlusion and background scene by leveraging the fact that there exists motion differ-ence between the two layers, and the pixels occluded in one frame are likely to be revealed in other frames. In particular, the recent learning-based approach [28] achieves remark-able quality in removing unwanted reflection and obstruc-tions. However, this method only considers a fixed number of adjacent frames as input, which should be varied depend-ing on the magnitude of the motion and obstruction size, whereas our recurrent scheme supports an arbitrary number of adjacent frames for restoration until convergence.
In this work, we propose a learning-based framework tailored for removing the contaminant artifacts of moving cameras. To this end, we first train the network to auto-matically spot the contaminant artifacts which are usually prominent in the flow maps of a video with a moving cam-era. As opposed to layer decomposition, we only focus on the background motion, of which the degraded region by the contaminants is hallucinated and softly blended by our flow completion network, depending on how much of the background is occluded.
In order to leverage information spanning an arbitrary number of frames, the restoration for each frame is recur-rent. That is, to restore one frame, we recurrently feed the adjacent frames one by one. Guided by the completed back-ground flow, the pixels within the artifact region can be pro-gressively restored by referring to the corresponding clean pixels from other frames. So far the restoration operates on each input frame individually, utilizing only the infor-mation of their adjacent frames. To produce the temporally consistent result for the whole video, we propose another multi-frame processing stage, in which we follow the same pipeline again but this time using the restored results from the last recurrent stage as input.
We train the entire framework in a supervised fashion.
To achieve this, we propose a synthetic dataset that follows the imaging physics of contaminant artifacts. Extensive ex-periments prove that the proposed model can generalize to real dirty lens videos (as shown in Figure 1), outperforming strong baselines both qualitatively and quantitatively. Our contributions can be summarized as follows:
• We propose the first deep learning approach to specif-ically address the contaminant artifacts for moving cameras. The proposed method performs better than general restoration methods on real videos.
• A physics-inspired synthetic dataset is proposed to mimic real contaminant artifacts.
• We propose a flow completion module to effectively hallucinate the background motion given the partially visible structure clue within the degraded region.
• The proposed recurrent scheme not only helps lever-age the multiple adjacent frame information to restore individual frames but can also be reused to refine such frame-wise output and ultimately yield temporally co-herent video results. 2.