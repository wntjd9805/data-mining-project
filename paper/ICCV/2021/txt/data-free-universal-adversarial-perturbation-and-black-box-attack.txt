Abstract
Universal adversarial perturbation (UAP), i.e. a single perturbation to fool the network for most images, is widely recognized as a more practical attack because the UAP can be generated beforehand and applied directly during the at-tack stage. One intriguing phenomenon regarding untargeted
UAP is that most images are misclassified to a dominant la-bel. This phenomenon has been reported in previous works while lacking a justified explanation, for which our work attempts to provide an alternative explanation. For a more practical universal attack, our investigation of untargeted
UAP focuses on alleviating the dependence on the original training samples, from removing the need for sample labels to limiting the sample size. Towards strictly data-free untar-geted UAP, our work proposes to exploit artificial Jigsaw images as the training samples, demonstrating competitive performance. We further investigate the possibility of ex-ploiting the UAP for a data-free black-box attack which is arguably the most practical yet challenging threat model.
We demonstrate that there exists optimization-free repetitive patterns which can successfully attack deep models. Code is available at https://bit.ly/3y0ZTIC. 1.

Introduction
Deep neural networks [28] are widely known to be vul-nerable to adversarial examples [52]. This intriguing phe-nomenon of human imperceptible perturbation fooling the
DNN has inspired active research for studying the model robustness against adversarial attack techniques [18, 36, 3].
More surprisingly, [38] shows that a single perturbation can be generated to attack the model for most images. Due to its image-agnostic nature, it is often termed universal adversar-ial perturbation (UAP). The existence of UAP is especially
*Equal contribution worrisome because, unlike image-dependent adversarial per-turbations, after being generated beforehand the UAP can be applied directly for performing a real-time attack [38].
Our work revisits UAP by providing an alternative expla-nation on the phenomenon of dominant label, i.e. an untar-geted UAP causing the model to misclassify a large fraction of images to a dominant label, which has been reported in [38, 43] but still lacks a justifiable explanation. Note that this phenomenon is counter-intuitive because unlike targeted
UAP [25, 60], the untargeted UAP is not optimized towards any predefined target label. In the targeted setting, [25, 60] have shown that the UAP alone leads the model to output the target class. We observe a similar phenomenon for the untargeted UAP by perceiving the dominant label, which is the result of the optimization algorithm instead of being predefined, as the pseudo-target class. When added to the images, the UAP causes the average logit values over all adversarial examples to be somewhat proportional to the logit values with the UAP alone as the input. This result is further collaborated by the layer-wise and step-wise model response analysis, suggesting untargeted UAP has a dom-inant contribution to the model response and there exists a positive correlation between this dominant influence and fooling ratio as the training evolves. This observation moti-vates to further investigate simple yet effective techniques towards more practical universal attacks under the data-free constraint, in both white-box and black-box settings.
Overall our contributions are shown as follows:
• We revisit the mechanism behind the dominant label phenomenon caused by an untargeted UAP. Specifically, we show that the existing explanation [38] hypothesiz-ing a dominant label occupying a large image space can not justify some observed phenomena based on some reasonable assumptions. We provide an alternative ex-planation with the observation that untargeted UAP has a dominant contribution to the model response of adver-sarial examples. Nonetheless, the untargeted UAP still does not lead to the misclassification of all images, for which we find that some samples tend to be systemati-cally more robust against the explored untargeted UAP and they tend to have repetitive semantic content.
• Our findings motivate the investigation of untargeted
UAP towards a more practical attack by alleviating the dependence on the original training samples in a progressive manner from removing the need for sample labels to limiting the sample size. Specifically, we adopt a self-supervision cosine similarity loss to optimize the untargeted UAP and reduce the sample size by common augmentation techniques. Towards strictly data-free
UAP, we propose to adopt artificial jigsaw images of variable frequency as the training samples. Our work suggests the benefit of designing artificial images that mimic the properties of natural images.
• We further investigate whether the UAP can be ex-ploited for facilitating practical data-free black-box at-tack, also termed no-box attack in [31]. Interestingly, we find that optimization-free repetitive content, such as vertical/horizontal or checkerboard pattern, is sufficient enough for a strong attack. It outperforms an existing sophisticated optimization-based method [31] which is resource-intensive and not strictly data-free. Beyond the deep classifier, we further demonstrate this attack is effective for attacking DNNs in other applications, such as object detection and semantic segmentation. 2.