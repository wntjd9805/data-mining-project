Abstract 1.

Introduction
This paper introduces a model for producing stylized line drawings from 3D shapes. The model takes a 3D shape and a viewpoint as input, and outputs a drawing with tex-tured strokes, with variations in stroke thickness, deforma-tion, and color learned from an artist’s style. The model is fully differentiable. We train its parameters from a single training drawing of another 3D shape. We show that, in contrast to previous image-based methods, the use of a ge-ometric representation of 3D shape and 2D strokes allows the model to transfer important aspects of shape and texture style while preserving contours. Our method outputs the re-sulting drawing in a vector representation, enabling richer downstream analysis or editing in interactive applications.
Our code and dataset are available at our project page: www.github.com/DifanLiu/NeuralStrokes
Understanding and creating stylized outline drawings is a key task for stylization [34, 4], sketch understanding [16], and human vision [5, 18]. Artists and amateurs alike draw pictures of 3D objects in many different styles, whether for art, animation, architectural design, 3D authoring, or simply the pleasure of drawing. However, most recent research in image stylization does not take 3D geometry into account, producing drawings that frequently lose detail and do not capture image outlines. Conversely, there is a long his-tory of 3D drawing algorithms that create precise line draw-ings in hand-authored procedural styles, but they cannot be learned from data, making them difﬁcult to control and in-applicable for analysis of existing sketches. While there is a long literature on analysis and shape reconstruction from sketches, these methods typically assume that artists draw with plain line styles.
This paper introduces differentiable learning for styliza-tion of 3D shapes, combining ideas from classic 3D line drawing algorithms with modern differentiable rendering.
The algorithm produces a stylized vector rendering from a 3D shape, in a style learned from a single drawing made by an artist of a reference shape (Figure 1). There are sev-eral challenges in making such a system work. To gener-ate stylized strokes, our method must disentangle several stroke attributes, including spatially-varying thickness, ge-ometric deformations and smoothness, and texture. These elements may often be quite noisy, with strokes being wig-gly or messy; the ﬁnal pixel values are an entangled combi-nation of these factors. Although we train from a drawing paired with a 3D shape, the individual components of thick-ness, deformation and texture are not labeled in the data.
Training with a purely pixel-based image translation model fails to disentangle these factors, producing noisy results that lose image details. Moreover, due to the difﬁculties of producing skilled artistic drawings, our training set is nec-essarily small, with only one drawing per distinct style.
To address these challenges, we propose a differentiable rendering formulation of stroke attributes. This allows the model to learn to accurately predict stroke thickness, defor-mation, and texture. Because our method works with 3D ge-ometry, it can accurately capture ﬁne details of a shape that purely image-based methods cannot. Our model is trained on pixel inputs, yet it produces output in a vector graph-ics format, which provides the beneﬁts of a compact repre-sentation with inﬁnite resolution and is suitable for further editing and use by graphic designers.
Our technical contributions include: (a) a convolutional network operating along parameterized stroke paths, (b) the combination of 3D geometry, 2D image, and 1D curve fea-ture maps to learn stroke properties with a differentiable vector renderer, and (c) learning from a single example based on multi-scale patches of the training drawing. We show that our method produces substantially better results than existing image-based methods, in terms of predicting artists’ drawings, and in user evaluation of results. 2.