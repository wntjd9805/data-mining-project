Abstract
Deep learning has achieved remarkable progress for vi-sual recognition on large-scale balanced datasets but still performs poorly on real-world long-tailed data. Previous methods often adopt class re-balanced training strategies to effectively alleviate the imbalance issue, but might be a risk of over-fitting tail classes. The recent decoupling method overcomes over-fitting issues by using a multi-stage train-ing scheme, yet, it is still incapable of capturing tail class
In this paper, information in the feature learning stage. we show that soft label can serve as a powerful solution to incorporate label correlation into a multi-stage training scheme for long-tailed recognition. The intrinsic relation between classes embodied by soft labels turns out to be helpful for long-tailed recognition by transferring knowl-edge from head to tail classes.
Specifically, we propose a conceptually simple yet par-ticularly effective multi-stage training scheme, termed as
Self Supervised to Distillation (SSD). This scheme is com-posed of two parts. First, we introduce a self-distillation framework for long-tailed recognition, which can mine the label relation automatically.
Second, we present a new distillation label generation module guided by self-supervision. The distilled labels integrate information from both label and data domains that can model long-tailed distribution effectively. We conduct extensive experiments and our method achieves the state-of-the-art results on three long-tailed recognition benchmarks:
ImageNet-LT,
CIFAR100-LT and iNaturalist 2018. Our SSD outperforms the strong LWS baseline by from 2.7% to 4.5% on various datasets. 1.

Introduction
Deep learning has achieved remarkable progress for vi-sual recognition in both image and video domains by train-ing powerful neural networks on large-scale balanced and curated datasets (e.g., ImageNet [8] and Kinetics [21]). Dis-tinct from these artificially balanced datasets, real-world data always follows long-tailed distribution [30, 29], which makes collecting balanced datasets more challenging, es-pecially for classes naturally with rare samples. However, learning directly from long-tailed data induces significant performance degeneration due to the highly imbalanced data distribution.
A common series of approaches to alleviate the dete-rioration caused by long-tailed training data is based on class re-balanced strategies [13, 22, 2, 10, 33], including re-sampling training data [13, 10, 6, 33] and designing cost-sensitive re-weighting loss functions [22, 39]. These meth-ods can effectively diminish the domination of head classes during the training procedure, and thus can yield more pre-cise classification decision boundaries. However, they are often confronted with the risk of over-fitting tail classes since the original data distribution is distorted and over-parameterized deep networks easily fit this synthetic distri-bution. To overcome these issues, the recent work [20, 48] decouples the tasks of representation learning and classi-fier training. This two-stage training scheme first learns visual representation under the original data distribution, and then trains a linear classifier on frozen features under class-balanced sampling. This simple two-stage training scheme turns out to be able to handle the over-fitting issue and sets new state-of-the-art performance on the standard long-tailed benchmarks. Nevertheless, this two-stage train-ing scheme fails to deal with imbalance label distribution issues well, particularly for representation learning stage.
In this paper, our objective is to design a new learning paradigm for long-tailed visual recognition, with the hope of sharing the merits of both types of long-tailed recognition methods, i.e., robust to the over-fitting issue, and effectively handling imbalance label issue. To meet this objective, our idea is to study how to incorporate the label correlation into a multi-stage training scheme? Inspired by the work of knowledge distillation [16] in model compression, we observe that soft labels produced by a teacher network are able to capture the inherent relation between classes, which might be helpful for long-tailed recognition by transferring knowledge from head classes to tail classes, as shown in
Figure 1. Thus, soft labels provide a practical solution for the multi-stage training strategy with label modeling. (cid:66): Corresponding author (lmwang@nju.edu.cn).
Based on the above analysis, we present a conceptu-Figure 1. Real-world data always follows long-tailed data distribution, which is dominated by several head classes with abundant samples (i.e, blue cubes) but also contains many tail classes with scarce data (i.e. green and yellow cubes), termed as original distribution.
Learning directly from long-tailed data can cause a significant performance drop. A common way to deal with the imbalance problem is re-sampling by randomly dropping images from head classes and repeatedly sampling images from tail classes (identical image is marked by unique Roman numeral), resulting in a re-balanced distribution. This strategy might lead to over-fitting tailed classes and under-fitting head classes. Inspired by the work of knowledge distillation in model compression, we propose to use the soft labels to deal with imbalance distribution that reflect the inherent relation between classes. The distilled distribution acts as a naturally balanced distribution by transferring knowledge from data-rich classes to data-poor classes. Best viewed in color. ally simple yet particularly effective multi-stage training scheme for long-tailed visual recognition, termed as Self
Supervision to Distillation (SSD). The key contribution of our SSD is two folds: (1) a self-distillation framework for learning effective long-tailed recognition network; (2) a self-supervision guided distillation label generation mod-ule to provide less biased but more informative soft la-bels for self-distillation. Specifically, we first streamline the multi-stage long-tailed training pipeline within a simple self-distillation framework, in which we are able to natu-rally mine the label relation automatically and incorporate this intrinsic label structure to improve the generalization performance of multi-stage training. Then, to further im-prove the robustness of the self-distillation framework, we present an enhanced distillation label generation module by self-supervision from the long-tailed training set itself.
Self-supervised learning learns effective visual representa-tion without labels, and can treat each image equally, thus relieving the effect of imbalanced label distribution on soft label generation.
Specifically, we first train an initial teacher network un-der label supervision and self-supervision simultaneously using instance-balanced sampling. Then, we train a sepa-rate linear classifier on top of the visual representation by refining the class decision boundaries with class-balanced sampling. This new classifier yields soft labels of train-ing samples for self-distillation. Finally, we train a self-distillation network under the hybrid supervision of soft la-bels from previous stages and hard labels from the original training set. As a semantic gap exists between hard labels and soft labels on whether it is biased to head classes, we adopt two classification heads for these two supervisions respectively. We evaluate our SSD training framework for long-tailed visual recognition on the datasets of ImageNet-LT [28], CIFAR100-LT [2], and iNaturalist 2018 [37]. Our approach outperforms other methods on these datasets by a large margin, which verifies the effectiveness of our pro-posed multi-stage training scheme.
To sum up, the main contribution of this paper is as fol-lows:
• We introduce a simple yet effective multi-stage train-ing framework (SSD). In this framework, we share the merits of re-balanced sampling and decoupled training strategy, by leveraging soft labels modeling into the feature learning stage.
• We propose a self-supervision guided soft label gener-ation module which produces robust soft labels from both data and label domains. These soft labels provide effective information by transferring knowledge from head to tail classes.
• Our SSD achieves the state-of-the-art performance on three challenging long-tailed recognition benchmarks including ImageNet-LT, CIFAR100-LT and iNaturalist 2018 datasets. 2.