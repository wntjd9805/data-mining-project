Abstract
Temporal action proposal generation is an important and challenging task in video understanding, which aims at detecting all temporal segments containing action in-stances of interest. The existing proposal generation ap-proaches are generally based on pre-defined anchor win-dows or heuristic bottom-up boundary matching strategies.
This paper presents a simple and efficient framework (RTD-Net) for direct action proposal generation, by re-purposing a Transformer-alike architecture. To tackle the essential visual difference between time and space, we make three important improvements over the original transformer de-tection framework (DETR). First, to deal with slowness prior in videos, we replace the original Transformer en-coder with a boundary attentive module to better capture long-range temporal information. Second, due to the am-biguous temporal boundary and relatively sparse annota-tions, we present a relaxed matching scheme to relieve the strict criteria of single assignment to each groundtruth. Fi-nally, we devise a three-branch head to further improve the proposal confidence estimation by explicitly predicting its completeness. Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effective-ness of RTD-Net, on both tasks of temporal action pro-posal generation and temporal action detection. Moreover, due to its simplicity in design, our framework is more ef-ficient than previous proposal generation methods, without non-maximum suppression post-processing. The code and models are made available at https://github.com/
MCG-NJU/RTD-Action. 1.

Introduction
As large numbers of videos are captured and uploaded online (e.g., YouTube, Instagram, and TikTok), video un-derstanding is becoming an important problem in computer vision. Action recognition [33, 39, 5, 36, 47, 37] has re-ceived much research attention from both academia and in-*: Equal contribution. (cid:66): Corresponding author.
Figure 1. Overview of RTD-Net. Given an untrimmed video,
RTD-Net directly generates action proposals based on boundary-attentive features without hand-crafted design, such as dense an-chor placement, heuristic matching strategy, and non-maximum suppression. dustry, with a focus on classifying trimmed video clip into action labels. However, these action recognition methods cannot be directly applied for realistic video analysis due to the fact that these web videos are untrimmed in nature.
Therefore, temporal action detection [26, 24, 42, 41] is a demanding technique, which aims to localize each action instance in long untrimmed videos with the action category and as well its temporal duration. In general, temporal ac-tion detection task is composed of two subtasks: temporal action proposal generation and action classification.
As for temporal proposal generation task, there are two mainstream approaches. The first type is an anchor-based [3, 19, 11, 15] method, which generates action pro-posals based on dense and multi-scale box placement. As the duration of action instances varies from seconds to min-utes, it is almost impossible for these anchor-based meth-ods to cover all these ground-truth instances under a rea-sonable computation consumption. The second type is a boundary-based [46, 26, 24] method, which first predicts the boundary confidence at all frames, and then employs a bottom-up grouping strategy to match pairs of start and
end. These methods extract the boundary information at a local window and simply utilize the local context for mod-eling. Therefore, these boundary-based methods might be sensitive to noise and fail to yield robust detection results, as they easily produce incomplete proposals. Furthermore, the performance of these two kinds of methods is highly dependent on the carefully-designed anchor placement or sophisticated boundary matching mechanisms, which are hand-crafted with human prior knowledge and require spe-cific tuning.
We contend that long-range temporal context modeling is vital for proposal generation. Viewing videos as tem-poral sequences and employing Transformer architecture to model global one-dimensional dependencies boosts local-ization performance. We propose a direct action proposal generation framework with Transformers. This direct action proposal generation with parallel decoding allows us to bet-ter capture inter-proposal relationships from a global view, thus resulting in more complete and precise localization re-sults. Moreover, our temporal detection framework stream-lines the complex action proposal generation pipeline with a neat set prediction paradigm, where hand-crafted designs such as anchor box placement, boundary matching strat-egy, and time-consuming non-maximum suppression are re-moved. As a result, our framework conducts inference with a noticeable faster speed. However, due to the essential vi-sual property difference between time and space, it is non-trivial to adapt the image detection Transformer architec-ture for videos.
We observe that the feature slowness in videos [45] and ambiguous temporal boundaries [31] are two key issues that require specific consideration for building a direct action proposal generation method with Transformers. First, al-though there are many frames along the temporal dimen-sion, their features change at a very low speed. Direct employment of self-attention mechanism as in Transformer encoder will lead to an over-smoothing issue and reduce the discrimination ability of action boundary. Second, due to the high-level semantic for action concept, its temporal boundary might be not so clear as object boundary, and the ground-truth labels might also contain some noise due to inconsistency among different labors. So a strict set match-ing loss might have a negative effect on the convergence of
Transformer, and not be optimal for training and general-ization.
To address the above issues, we present a Relaxed
Transformer Decoder (RTD) architecture for direct action proposal generation, as shown in Figure 1. Compared with the original object detection Transformer, we make three notable improvements to adapt for the video task.
First, we replace the original Transformer encoder with a customized boundary-attentive architecture to overcome the over-smoothing issue. Second, we propose a relaxed matcher to relieve the strict criteria of single assignment to a ground-truth. Finally, we devise a three-branch detec-tion head for training and inference. A completeness head is added to explicitly estimate the tIoU between regressed temporal box and ground-truth box. We observe that this tIoU loss can guide the training of Transformer and regular-ize three heads to converge to a stable solution.
In summary, our main contributions are as follows:
• For the first time, we adapt the Transformer architec-ture for direct action proposal generation in videos to model inter-proposal dependencies from a global view, and reduce the inference time greatly by streamlining temporal action proposal generation pipeline with a simple and neat framework, removing the hand-crafted designs.
• We make three improvements over important
DETR [4] to address the essential difference between temporal location in videos and spatial detection in images, including boundary attentive representation, relaxation mechanism, and three-branch head design.
• Experiments demonstrate that our method outperforms the existing state-of-the-art methods on THUMOS14 and achieves comparable performance on ActivityNet-1.3, in both temporal action proposal generation task and temporal action detection task. 2.