Abstract
With the strength of deep generative models, 3D pose transfer regains intensive research interests in recent years.
Existing methods mainly rely on a variety of constraints to achieve the pose transfer over 3D meshes, e.g., the need for manually encoding for shape and pose disentangle-ment. In this paper, we present an unsupervised approach to conduct the pose transfer between any arbitrate given 3D meshes. Speciﬁcally, a novel Intrinsic-Extrinsic Preserved
Generative Adversarial Network (IEP-GAN) is presented for both intrinsic (i.e., shape) and extrinsic (i.e., pose) in-formation preservation. Extrinsically, we propose a co-occurrence discriminator to capture the structural/pose in-variance from distinct Laplacians of the mesh. Meanwhile, intrinsically, a local intrinsic-preserved loss is introduced to preserve the geodesic priors while avoiding heavy com-putations. At last, we show the possibility of using IEP-GAN to manipulate 3D human meshes in various ways, including pose transfer, identity swapping and pose interpolation with latent code vector arithmetic. The extensive experiments on various 3D datasets of humans, animals and hands qualita-tively and quantitatively demonstrate the generality of our approach. Our proposed model produces better results and is substantially more efﬁcient compared to recent state-of-the-art methods. Code is available: https://github. com/mikecheninoulu/Unsupervised_IEPGAN 1.

Introduction
Efﬁcient 3D mesh manipulation with high-ﬁdelity gen-erative models is crucial in the computer graphics ﬁeld and enjoys great potential in various practical applications, ranging from 3D human activity understanding, 3D aug-mented reality to robotics. In this work, the focus is made to transfer the 3D pose style from the source mesh to the target mesh in an unsupervised manner. Although a few attempts have been proposed for this task, due to several issues, lim-itations still exist in the state-of-the-art methods.
*Corresponding Author.
Figure 1: Visualized latent pose space distribution learned by our IEP-GAN. Dots in each color stands for a latent code of a pose class from FAUST dataset [2]. We observe the clusters in the latent space ﬁt their pose representations, suggesting that the IEP-GAN has generalized the cohesive ability of projection.
To date, strict constraints of re-enforced correspon-dences of meshes are inevitable in almost all the existing methods [26, 8]. However, to provide those correspon-dences needs either extra manual efforts or speciﬁc require-ments for data. Although some existing works claimed that they can achieve 3D pose deformation in an unsupervised setting, a constraint on the training datasets is still needed that different poses performed by the same subject should be given to successfully disentangle the shape and pose in-formation [4, 39]. This constraint actually serves as a strong supervision with manually labeled prior. Thus a real unsu-pervised setting for pose transfer learning is not achieved in any existing work yet. Besides, compared to 2D im-ages that have ﬁxed rigid permutation [29, 38, 27, 28], 3D meshes are embedded in continuous space with arbitrat-ing orders and complex geometric attributes, which makes the task more challenging. This innately structural dif-ference from 2D images precludes the standard discrete convolution operator to be immediately applicable to 3D meshes/points. A model speciﬁcally designed for geometric characters is desirable [14]. Lastly, intrinsic (i.e., geodesic) distances are powerful metrics for learning latent represen-tations of deformable 3D shapes [16, 3, 2], but conventional geodesic-based methods suffer from the intensive computa-tions, which makes them unsuitable for learning on large-scale datasets.
Learning unsupervised 3D pose transfer without any constraints needed on training sets, this target drives our current research efforts. To this end, we propose a GAN-based framework motivated by three key observations.
First, a variety of deep generative models are proposed to encode and regenerate pose-transferred meshes by disen-tangling pose and shape information. However, extra con-straints on the training data are inevitable in all the existing methods, otherwise the shape and pose latent representa-tions cannot be successfully disentangled, which leads to degenerate solutions. In an ideal case, a model should be able to learn the shape and pose representations without any enforced constraint on training data, see Fig. 1. Second, to obtain the geodesic distance priors for the intrinsic preser-vation of meshes, various differentiable intrinsic metrics are adopted, but they either need intensive computational costs or large-scale training sets [11, 31]. At last, to our knowl-edge, there is no existing deep generative model proposed speciﬁcally for extrinsic information learning on 3D data.
In this paper, we propose a novel Intrinsic-Extrinsic Pre-served Generative Adversarial Network (IEP-GAN) (see
Fig. 2). Two-branch discriminators are introduced to lib-erate the learning from the needs of data constraints. A global-branch discriminator is introduced to substitute the ground-truth meshes by enforcing generated meshes to con-verge to realistic ones. Besides, an extrinsic-branch dis-criminator is incorporated to enhance the pose style learn-ing through the co-occurrence statics of the Laplacians of the meshes. Furthermore, a geodesic-adaptive sampling strategy is introduced to compute a regional geometric-preserved loss instead of the global geodesic prior, which can form an effective geometric regulation as intrinsic preservation while avoiding the heavy computations.
To summarize, the novelties can be listed as below:
• To the best of our knowledge, the proposed work is the
ﬁrst that can achieve unsupervised 3D pose transfer with-out any human supervision and the intrinsic-extrinsic pre-served generative adversarial network is the ﬁrst GAN-based framework for 3D human pose learning.
• The IEP-GAN consists of newly proposed two-branch discriminators. The global branch enhances the gener-ative capacity of the model and substitutes the need for the manual encoding of the training data. The extrinsic branch utilizes the Laplacian to strengthen extrinsic geo-metric learning in a co-occurrence static way.
• At last, a regional geometric-preserved loss is proposed to preserve the regional intrinsic priors via a geodesic-adaptive sampling strategy, which guarantees both an effective geometric regulation and substantial improve-ments in the computational efﬁciency. i.e.,
• Experimental results on four different datasets,
DFAUST [3], FAUST [2], ANIMAL [26], and
MANO [23], show that the proposed method achieves the new state-of-the-art performances with satisfying visual qualities. Moreover, we further represent the possibility of using the resulting embedding space for the 3D human mesh manipulation, such as the smooth pose transfer, in-terpolation and swapping of the given shapes. 2.