Abstract
Retrieving accurate 3D reconstructions of objects from the way they reﬂect light is a very challenging task in com-puter vision. Despite more than four decades since the deﬁnition of the Photometric Stereo problem, most of the literature has had limited success when global illumina-tion effects such as cast shadows, self-reﬂections and am-bient light come into play, especially for specular surfaces.
Recent approaches have leveraged the capabilities of deep learning in conjunction with computer graphics in order to cope with the need of a vast number of training data to in-vert the image irradiance equation and retrieve the geom-etry of the object. However, rendering global illumination effects is a slow process which can limit the amount of train-ing data that can be generated.
In this work we propose a novel pixel-wise training pro-cedure for normal prediction by replacing the training data (observation maps) of globally rendered images with in-dependent per-pixel generated data. We show that global physical effects can be approximated on the observation map domain and this simpliﬁes and speeds up the data cre-ation procedure. Our network, PX-NET, achieves state-of-the-art performance compared to other pixelwise methods on synthetic datasets, as well as the DiLiGenT real dataset on both dense and sparse light settings. 1.

Introduction
Photometric Stereo (PS) is a classical problem in com-puter vision since the early ’80s [42]. PS assumes multiple images from the same viewpoint along with varied illumina-tion and calculates local geometrical features (e.g. normal or depth) at each pixel by exploiting the relation between surface orientation and intensity of reﬂected light. This is essentially an inverse rendering problem requiring at least three input images in order to have a unique solution.
Most of the difﬁculty in retrieving the 3D shape from
‘Reading’
[16] MAE=12.6o
Proposed MAE=9.78o
Figure 1. Comparison of the proposed approach versus [16] on
‘Reading’ of the DiLiGenT real benchmark [35]. The evaluation metric is the mean angular error (MAE) of the computed normal map compared with the ground truth. the light reﬂected off the object is due to the type of reﬂec-tion and its non-linear dependence on material properties.
This is mathematically expressed though the surface bidi-rectional reﬂectance distribution function (BRDF) which is determined by the material of the object. Over the last forty years a very wide spectrum of BRDF equations have been proposed to model the light reﬂection phenomena. Starting from the basic linear light response for diffuse reﬂection
[21, 13], more specular behaviour of reﬂected light have been proposed [31, 3, 8, 20, 38, 40]. Comparison among numerous BRDFs can be found in [41, 11, 30, 29]. Finally, the recently proposed Disney BRDF [4, 12] was invented to unify most physical reﬂection effects including gloss reﬂec-tion, subsurface scattering and metallic/specular roughness into a uniﬁed formulation.
The above mentioned advancements in computer graph-ics have enabled convolutional neural network (CNN)-based approaches to be useful for solving PS by rendering large number of images of various surfaces under numerous light and material conﬁgurations. They often parametrise the PS problem as normal regression from light intensity observations (i.e. observational map [16] ), effectively per-forming an inversion of the irradiance equation. CNN-based approaches have been shown to outperform classical optimisation based methods [17, 32] mainly due to the abil-ity of CNNs to learn how to deal with a great variety of real-istic reﬂectances which lead classical optimisation methods into intractable computations and thus simpliﬁcations (e.g. assuming Lambertian reﬂection).
In addition, CNNs can gain robustness to deviations from the irradiance equation such as global illumination effects (cast shadows, self re-ﬂections) if the training data includes them [16]. This can be achieved using 3D creation suite (like Blender [2]) which can render data containing that level of realism. However, exhaustively sampling global illumination effects (which are a function of the overall surface geometry) requires a huge number of meshes to be rendered. Furthermore, the rendering requirements grow exponentially if the rendered data are to be covering all the materials/lights conﬁgurations as well. The rendering computational cost can be reduced by rendering multi-material objects ([16]) to the detriment of realistic ray-traced self reﬂections. Finally, it is noted that rendering full objects is computationally expensive, there-fore relatively slow and somehow inefﬁcient, as there is a large amount of correlation among neighbouring pixels (es-pecially for shadow/self reﬂection patterns).
In order to maximise the combinations of sampled mate-rials, lights and normal directions, instead of pre-rendering training data, we train our CNN with highly efﬁcient, in-dependently generated pixelwise, observational maps. This allows to widen the training data variation as all of these pa-rameters (e.g materials) can be sampled independently for every data point. Moreover, we show how global light ef-fects can be approximated in the observational map domain, and propose a strategy that includes variations in the maps that model ambient light, cast shadows, self-reﬂections and reﬂectance mixing in discontinuity boundaries. This strat-egy helps to reduce the synthetic to real gap making our data applicable for challenging real data [35].
Contribution: Our CNN based approach for solving PS problem has the following main contribution: we propose a per-pixel observation map generation strategy which can re-place slow-to-obtain full image rendering while still allow-ing the network to learn global illumination effects. Fur-thermore, we also propose an improvement to the CNN-PS [16] architecture termed PX-NET which beneﬁts from the increase in the training data variation. Finally, we show that including the RGB channels in the observation map can further boost performance.
The rest of this work is divided as follows. Section 2 discusses the relevant literature. Section 3 provides details of our proposed CNN approach. Sections 4 and 5 describe the experiment setup and corresponding results. 2.