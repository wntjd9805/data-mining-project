Abstract
Spatio-temporal action detection is an important and challenging problem in video understanding. The exist-ing action detection benchmarks are limited in aspects of small numbers of instances in a trimmed video or low-level atomic actions. This paper aims to present a new multi-person dataset of spatio-temporal localized sports actions, coined as MultiSports. We first analyze the important ingre-dients of constructing a realistic and challenging dataset for spatio-temporal action detection by proposing three crite-ria: (1) multi-person scenes and motion dependent identifi-cation, (2) with well-defined boundaries, (3) relatively fine-grained classes of high complexity. Based on these guide-lines, we build the dataset of MultiSports v1.0 by selecting 4 sports classes, collecting 3200 video clips, and annotat-ing 37701 action instances with 902k bounding boxes. Our datasets are characterized with important properties of high diversity, dense annotation, and high quality. Our Multi-Sports, with its realistic setting and detailed annotations, exposes the intrinsic challenges of spatio-temporal action detection. To benchmark this, we adapt several baseline methods to our dataset and give an in-depth analysis on the action detection results in our dataset. We hope our
MultiSports can serve as a standard benchmark for spatio-temporal action detection in the future. Our dataset website is at https://deeperaction.github.io/multisports/. 1.

Introduction
Spatio-temporal human action detection in untrimmed videos is of great importance for many applications, such as surveillance and sports analysis. Recently, recognizing ac-tions from short trimmed videos has achieved considerable progress [44, 3, 40, 35, 41, 42], but these classification mod-els can not be directly applied for video analysis in a multi-person scene. Meanwhile, although temporal action detec-tion methods [56, 26, 25, 50, 53] for untrimmed videos can distinguish intervals of human actions from background, (cid:66): Corresponding author (lmwang@nju.edu.cn). they are still unable to spatially detect multiple concurrent human actions, which is important in real-world applica-tions of video analysis.
Current spatio-temporal action detection benchmarks can be mainly classified into two categories: 1) Densely annotated high-level actions such as J-HMDB [17] and
UCF101-24 [38]. Their clips only have a single person doing some semantically simple and temporally repeated actions. Typically, the scene context can provide enough cues for recognizing these coarse-grained action categories.
Thus, these benchmarks might be impractical for real-world applications such as surveillance, where it is required to deal with more fine-grained actions in a multi-person scene; 2) Sparsely annotated atomic actions such as AVA [12].
They fail to provide clear temporal action boundaries, and simply focus on frame-level spatial localization of atomic actions. This setting removes the requirements of tempo-ral localization for action detection algorithms. Meanwhile, their atomic actions rarely require the complex reasoning over the actors and their surrounding environment.
Based on the analysis above, we argue that a new benchmark is necessary to advance the research of spatio-temporal action detection. The benchmark should satisfy several important requirements to cover the realistic chal-lenges of this task. 1) There should be multiple persons performing different actions concurrently in the same scene, where the background information is not sufficient for ac-tion recognition and motion itself of the actor plays a sig-nificant role. 2) To address the inherently confusing human action boundaries in time, actions should be both semanti-cally and temporally well-defined with a consensus among humans. 3) Considering the complexity of real-world appli-cations, actions should be fine-grained which requires accu-rate human pose and motion information, long-term tempo-ral structure, possible interactions between humans, objects and scenes, and reasoning over their relations.
Following the above guidelines, we develop the Multi-Sports dataset, short for Multi-person Sports Actions. The dataset is large-scale, high-quality, multi-person, and con-tains fine-grained action categories with precise and dense annotations in both spatial and temporal domains. The ac-Figure 1. The 25fps tubelets of bounding boxes and fine-grained action category annotations in MultiSports dataset. Multiple concurrent action situations frequently appear in MultiSports with many starting and ending points in the long untrimmed video clips. The frames are cropped and sampled by stride 5 or 7 for visualization propose. Tubes with the same color represent the same person. tion vocabulary consists of 66 action classes collected from 4 sports (basketball, volleyball, football and aerobic gym-nastics). An example clip has been visualized in Figure 1.
We choose these four sports for the following reasons. 1)
There are plenty of multiple concurrent action instances in sports competitions. Also, the background is far less char-acteristic and cannot provide sufficient information for fine-grained action recognition. 2) Sports actions have well-defined categories and boundaries. These boundaries are defined by either professional athletes or official documen-tations [7]. 3) Due to the complex competition rules, rec-ognizing sports action generally requires to model the long-term structure and the human-object-scene interactions. For example, in football, although the athlete may take only 0.5s to kick the ball, we may need up to 5s context to recognize whether it is pass, long ball, through ball, or cross.
In practice, we conduct exhaustive annotations of 25 fps frame-wise bounding boxes and fine-grained action cate-gories in a two-stage procedure: 1) a team of professional athletes of corresponding sport to annotate the temporal and category labels, and 2) a team of crowd-sourced annota-tors to finish the bounding boxes with the help of tracking method FCOT [6]. This two-stage annotation procedure as well as careful quality control together can guarantee con-sistent and clean annotations. To ensure the visual qual-ity, all videos in our dataset are high-resolution records of professional competitions from a diversity of countries and different performance levels.
Given the well-defined and dense-annotated action in-stances in MultiSports v1.0, we benchmark spatio-temporal action detection on this challenging dataset. We perform empirical studies with several recent state-of-the-art action detector methods. Compared with previous action detection benchmarks such as J-HMDB [17] and UCF101-24 [38], our MultiSports is quite challenging with a much lower frame mAP and video mAP. We also introduce a detailed error analysis on detection results and try to provide more insights on spatio-temporal action detection. According to our analysis on MultiSports benchmark, we figure out several challenges of spatio-temporal action detection that needs to be addressed, such as capturing subtle differences between fine-grained action categories, performing accurate temporal localization, dealing with action occlusion and modeling long-range context. We hope MultiSports could serve as a standard benchmark to advance the area of spatio-temporal action detection in the future. MultiSports sptatio-temporal action detection is currently a track of DeeperAc-tion challenge at ICCV 2021 https://deeperaction.github.io/.
In summary, our main contribution is twofold. 1) We de-velop a new benchmark MultiSports of spatio-temporal ac-tion detection for well-defined and realistically difficult hu-man actions in a multi-person scene, providing high-quality and 25fps frame-wise annotations from four sports. 2) We conduct extensive studies and systematic error analysis on
MultiSports, which reveals the key challenges of spatio-temporal action detection and hopefully can facilitate future research in this area.
2.