Abstract
Simulators can efﬁciently generate large amounts of la-beled synthetic data with perfect supervision for hard-to-label tasks like semantic segmentation. However, they in-troduce a domain gap that severely hurts real-world per-formance. We propose to use self-supervised monocu-lar depth estimation as a proxy task to bridge this gap and improve sim-to-real unsupervised domain adaptation (UDA). Our Geometric Unsupervised Domain Adaptation method (GUDA)1 learns a domain-invariant representation via a multi-task objective combining synthetic semantic su-pervision with real-world geometric constraints on videos.
GUDA establishes a new state of the art in UDA for seman-tic segmentation on three benchmarks, outperforming meth-ods that use domain adversarial learning, self-training, or other self-supervised proxy tasks. Furthermore, we show that our method scales well with the quality and quantity of synthetic data while also improving depth prediction. 1.

Introduction
Self-supervised learning from geometric constraints is used to learn tasks like depth and ego-motion directly from unlabeled videos [13, 22, 24, 51, 76]. However, tasks like semantic segmentation and object detection in-herently require human-deﬁned labels. A promising al-ternative to expensive manual labeling is to use synthetic datasets [4, 9, 14, 45, 46]. Simulators can indeed be pro-grammed to generate large quantities of diverse data with accurate labels (cf. Fig. 1), including for optical and scene
ﬂow [30, 44], object detection [42], tracking [14], action recognition [9], and semantic segmentation [45, 46]. How-ever, no simulator is perfect. Hence, effectively using syn-thetic data requires overcoming the sim-to-real domain gap, a distribution shift between a source synthetic domain and a target real one due to differences in content, scene geom-etry, physics, appearance, or rendering artifacts.
The goal of Unsupervised Domain Adaptation (UDA) is to improve generalization across this domain gap with-out any real-world labels. Most methods use adversar-1https://github.com/tri-ml/packnet-sfm
Cityscapes (a) Synthetic datasets (ground-truth)
KITTI
DDAD (b) Real-world datasets (predictions)
Figure 1: Our GUDA approach uses geometric self-supervision on videos in a multi-task setting to achieve state-of-the-art results in UDA for semantic segmentation. ial learning for pixel or feature-level adaptation [3, 15, 27, 35, 54, 59, 69] or self-training by reﬁning pseudo-labels [31, 32, 73, 77, 78, 50]. These methods yield clear improvements, but require learning multiple networks be-yond the target one, are hard to train (adversarial learn-ing), or limited to semantically close domains (iterative diffusion of high-conﬁdence pseudo-labels). Alternatively, few works [53, 67] have explored simple image-level self-supervised proxy tasks [20, 40, 33] to improve generaliza-tion across domains, but with only limited success for UDA of semantic segmentation.
In this work, we introduce self-supervised monocu-lar depth as a proxy task for UDA in semantic seg-mentation. We propose a multi-task mixed-batch train-ing method combining synthetic supervision with a real-1
world self-supervised depth estimation objective to learn a domain-invariant encoder. Although it is not obvious that geometric constraints on videos can help overcome a se-mantic gap on images, our method, called GUDA for Ge-ometric Unsupervised Domain Adaptation, outperforms other UDA methods for semantic segmentation. Further-more, we can directly combine our method with self-trained pseudo-labels, leading to a new state of the art on the stan-dard SYNTHIA-to-Cityscapes benchmark. In addition, we show on Cityscapes [7], KITTI [18], and DDAD [24] that our method scales well with both the quantity and qual-ity of synthetic data (cf. Fig. 1), from SYNTHIA [46] to
VKITTI2 [4], and a new large-scale high-quality dataset [1].
Finally, we show that GUDA is also capable of state-of-the-art monocular depth estimation in the real-world domain. 2.