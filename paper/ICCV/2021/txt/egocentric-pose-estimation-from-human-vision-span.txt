Abstract
Estimating camera wearer’s body pose from an egocen-tric view (egopose) is a vital task in augmented and virtual reality. Existing approaches either use a narrow ﬁeld of view front facing camera that barely captures the wearer, or an extended head-mounted top-down camera for maxi-mal wearer visibility. In this paper, we tackle the egopose estimation from a more natural human vision span, where camera wearer can be seen in the peripheral view and de-pending on the head pose the wearer may become invisible or has a limited partial view. This is a realistic visual ﬁeld for user-centric wearable devices like glasses which have front facing wide angle cameras. Existing solutions are not appropriate for this setting, and so, we propose a novel deep learning system taking advantage of both the dynamic fea-tures from camera SLAM and the body shape imagery. We compute 3D head pose, 3D body pose, the ﬁgure/ground separation, all at the same time while explicitly enforcing a certain geometric consistency across pose attributes. We further show that this system can be trained robustly with lots of existing mocap data so we do not have to collect and annotate large new datasets. Lastly, our system estimates egopose in real time and on the ﬂy while maintaining high accuracy. 1.

Introduction
Truly immersive experiences in augmented and virtual reality (AR and VR) are driven by explicit characterization of user’s (i.e., the device wearer) pose. In particular, this user’s pose needs to be estimated from the perspective of the device, which implicitly corresponds to their egocen-tric perspective. Typically referred to as the egopose, this corresponds to the 3D head and body pose of the camera wearer. Egopose drives the necessary inputs required for constructing naturalistic experiences in AR and VR. For in-stance, world locked egopose representations provide the necessary inputs for user’s interacting with the audio and visual objects in a virtual scene. In particular, for conver-sations involving a combination of real people and virtual entities (like avatars or holograms), a precise characteriza-tion of egopose is necessary to enable seamless switching between multiple speakers while retaining immersion. 0.5 0
-0.5 s t l u s e
R r u
O 0.1 0
-0.1
-0.2 0.5 0
-0.5 h t u r
T d n u o r
G 0.1 0
-0.1
-0.2 0.5 0
-0.5 0.1 0.2 0.1 0
-0.1
-0.2 0.5 0
-0.5 0.1 0.2 0.1 0
-0.1
-0.2 0
-0.1
-0.2 0
-0.1
-0.2 0.5 0
-0.5 0.5 0
-0.5 0.2 0.1 0.1 0
-0.1
-0.2 0.1 0
-0.1 0.1 0
-0.1
-0.2 0
-0.1
-0.2 0
-0.1
-0.2 0.5 0
-0.5 0.1 0.2 0.1 0
-0.1
-0.2 0
-0.1
-0.2
-0.3 0.5 0
-0.5 0.1 0.2 0.1 0
-0.1
-0.2 0
-0.1
-0.2 0
-0.1
-0.2 0.5 0
-0.5 0.1 0.2 0.1 0.5 0
-0.5 0.1 0.2 0.1 0
-0.1
-0.2
-0.3 0
-0.1
-0.2
-0.3
-0.4 0.2 0.1 0
-0.1
-0.2 0.1 0
-0.1
-0.2
Figure 1. Egopose estimation from a human vision span. The head-mounted front facing ﬁsheye camera often sees the wearer only partially in the peripheral view. Sometimes, the wearer is completely invisible in the camera’s ﬁeld of view. We extract the body part segmentation (row one), motion history image (row two) and estimate the body and head pose of the wearer in real time (row three). Row four shows the ground truth egoposes.
Egopose estimation is a challenging task. Existing ap-proaches generally fall under two categories: non-optical sensor based methods, and camera based approaches. Sen-sors based approaches relying on magnetic and inertial at-tributes give robust estimate of the egopose [13, 14]. How-ever, they need specially designed equipment, are usually harder to set up, and reasonably intrusive, inhibiting the user’s general movement. Camera based methods are less intrusive and can work in different environments. One cate-gory of these approaches relies on top-down head-mounted camera to have the best view of the wearer [2, 3, 6, 11], while the other uses the narrow ﬁeld of view (FOV) front facing cameras in which camera wearer is mostly invisi-ble [8, 9, 4, 5]. The former setting leads to reliable re-sults as long as they can ‘see’ body parts clearly. However, the head-mounted downward cameras need to extend to the front to avoid the occlusion of the nose and cheek. When
the wearer is missing from the FOV, the pose estimation would completely fail. The later setting has the advantage of estimating egopose without seeing the wearer, although it cannot resolve some ambiguous body poses, especially the arm poses.
In AR and VR devices, it is natural to have cameras close to the wearer’s face and have a visual ﬁeld similar to human eyes: for the most part, the camera can see the wearer’s hands and some other parts of the body only in the peripheral view, and for a signiﬁcant portion of the time it cannot see the wearer at all, for instance when the camera wearer looks up. This presents a new setting for egopose – a human-eye-like vision span, which we believe has not been studied. Our solution framework, as shown in Fig. 1, takes advantage of both the camera motion and visible body parts to give robust egopose estimation no matter the wearer is visible or not in the camera’s FOV. We propose a deep learning approach to tackle this problem.
Firstly, our proposal uses both the dynamic motion infor-mation obtained from camera SLAM, and the occasionally visible body parts for predictions.
In addition to predict-ing the egopose, the model computes 3D head pose and the
ﬁgure-ground segmentation of camera wearer in the ego-centric view. Because of this joint estimation of head and body pose, we can enforce certain geometrical consistency during the inference, which can further improve results and enable us to reposition the egopose in a global coordinate system with camera SLAM information.
Secondly, the proposed method allows wearer to be in-visible in the ﬁeld of view; and in cases where the camera wearer is partially visible, our method can take advantage of both motion and visible shape features to further improve the results.
Thirdly, one of the biggest challenges in egopose es-It takes a timation is the availability of good datasets. lot of effort to capture synchronized egocentric video and body/head poses for hundreds of subjects.
In this work, we instead utilize existing datasets to the best extent pos-sible, speciﬁcally leveraging mocap data collected over the past decades. These mocap data usually only capture the body joints movement and they do not include the egocen-tric video. Building on [6], in this work, we also propose an approach to synthesize not only the virtual view egocentric images, but also the dynamic information associated with the pose changes. We show that such synthetically gener-ated datasets already have superior generalization power on real videos. Lastly, our main application is in AR and VR setting, and hence, we propose the model with low latency design so it can be deployed in real-time applications.
The contributions of this paper are:
• An egopose estimation model from a novel perspec-tive of the human vision span, critical to small factor
AR/VR glasses, where the FOV covers very limited and sometimes no view of the wearer;
• A joint estimation procedure for ego-head and ego-body poses;
• An approach for synthesizing data for egopose from existing mocap data, which is generalizable to real sce-narios; and
• A pose estimation model that is real-time, thereby en-abling real-world AR and VR applications. 2.