Abstract
We present a dual-pathway approach for recognizing fine-grained interactions from videos. We build on the suc-cess of prior dual-stream approaches, but make a distinc-tion between the static and dynamic representations of ob-jects and their interactions explicit by introducing separate motion and object detection pathways. Then, using our new Motion-Guided Attention Fusion module, we fuse the bottom-up features in the motion pathway with features cap-tured from object detections to learn the temporal aspects of an action. We show that our approach can generalize across appearance effectively and recognize actions where an ac-tor interacts with previously unseen objects. We validate our approach using the compositional action recognition task from the Something-Something-v2 dataset where we outperform existing state-of-the-art methods. We also show that our method can generalize well to real world tasks by showing state-of-the-art performance on recognizing hu-mans assembling various IKEA furniture on the IKEA-ASM dataset. 1.

Introduction
In recent years, “two-stream” approaches have emerged as a dominant paradigm in video-based action recognition
[16, 6, 15]. Such methods process a video stream using two different neural modules whose scores are fused to pro-duce a final prediction. Each module has its own purpose: typically, one module captures temporal information about motion in the scene, and the other captures spatial informa-tion about the appearance of relevant objects, actors, and perhaps background context.
Although it is not always explicit in their formulation, two-stream models capture the idea that actions fundamen-tally describe compositional interactions between people and their environment. These interactions are made up of atomic actions (verbs) which can take a variety of argu-ments (in analogy to syntactic analyses, subjects or objects).
For instance, pick up a mug might be represented as
Figure 1: Do current video models have the ability to rec-ognize an unseen instantiation of an interaction defined us-ing a combination of seen components? We show that it is possible by specifying the dynamic structure of an action using a sequence of object detections in a top-down fash-ion. When the top-down structure is combined with a dual-pathway bottom-up approach, we show that the model can then generalize even to unseen interactions. the triple (person, pick up, mug). Due to this composi-tionality, automated recognition of human object interac-tions in video thus faces the fundamental challenge that the set of labels is combinatorially large. As a result, enumer-ating all possible descriptions to train end-to-end methods
[32, 6, 15, 49, 24] is impractical. As illustrated in Figure 1, the compositional nature of interactions ultimately requires a vision system that can generalize to actions with previ-ously seen structure but instantiated with possibly unseen combinations of components.
In response to this challenge, there have been attempts to impose an explicit top-down structure to decompose an action into its actors (objects) and spatial-temporal rela-tion among them using object detections [48, 36, 25, 26].
However, such methods do not exhibit clear and obvi-ous improvements unless ensembled with end-to-end RGB
models at test time [36] or ground truth knowledge about actor-object relations is provided to the model by an ora-cle [26]. This suggests that enforcing a top-down structure does not fully capture the range of variations among interac-tions. Moreover, the fact that an ensemble of independently trained models consistently outperforms an RGB+object feature fusion approach [36] for recognizing interactions suggests that features from the two domains are not effec-tively fused during training. Nonetheless, the motivation for using object detections remains strong because objects nat-urally define a structure of an interaction, and off-the-shelf object detections [21, 41] have become robust enough to be consumed as-is to define complex actions [28].
In this paper, we present a hybrid approach for rec-ognizing fine-grained interactions that borrows ideas from bottom-up, two-stream action recognition and top-down, structured human-object interaction detection. The key idea behind our approach is to make use of a sequence of object detections to guide the learning of object-centric video fea-tures that capture both static relations and dynamic move-ment patterns of objects. The learned object-centric repre-sentation is then transferred to a motion pathway using an attention-based Motion-Guided Attention Fusion (MGAF) mechanism. The MGAF module guides the RGB represen-tation from the motion pathway to develop representation of the dynamic aspects of an action.
In the remainder of the paper, we evaluate the model’s ability to generalize over object appearance when recog-nizing interactions. We show that our approach leads to a video model that can recognize unseen interactions (novel verb-noun compositions) at test time better than existing ap-proaches. We evaluate our approach using the Something-Else task [36] from the Something-Something-V2 dataset
[18] where we establish a new state of the art. Moreover, we show that our framework is a general concept and transfers readily to a new domain. Using the recently released IKEA-ASM dataset [3], we show that our model accurately recog-nizes humans interacting with numerous parts to assemble various IKEA furniture and also sets the new state-of-the-art benchmark for the main task of the dataset. Further, we present the first results on the compositional task using the
IKEA-ASM dataset where a model is tested on novel verb-noun compositions. In summary, the main contributions of the paper are: 1. A dual-pathway approach that leverages dynamic rela-tions of objects. 2. MGAF: A feature fusion strategy to use motion-centric object features to guide the RGB feature learning pro-cess. 3. A state of the art recognition performance on multiple benchmarks including compositional tasks. 2.