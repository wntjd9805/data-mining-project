Abstract
Deep neural networks have significantly improved appearance-based gaze estimation accuracy. However, it still suffers from unsatisfactory performance when general-izing the trained model to new domains, e.g., unseen envi-ronments or persons. In this paper, we propose a plug-and-play gaze adaptation framework (PnP-GA), which is an en-semble of networks that learn collaboratively with the guid-ance of outliers. Since our proposed framework does not re-quire ground-truth labels in the target domain, the existing gaze estimation networks can be directly plugged into PnP-GA and generalize the algorithms to new domains. We test
PnP-GA on four gaze domain adaptation tasks, ETH-to-MPII, ETH-to-EyeDiap, Gaze360-to-MPII, and Gaze360-to-EyeDiap. The experimental results demonstrate that the
PnP-GA framework achieves considerable performance improvements of 36.9%, 31.6%, 19.4%, and 11.8% over the baseline system. The proposed framework also outper-forms the state-of-the-art domain adaptation approaches on gaze domain adaptation tasks. Code has been re-leased at https://github.com/DreamtaleCore/
PnP-GA. 1.

Introduction
Eye gaze is an important indicator of human atten-tion.
It has been exploited in various applications, such as human-robot interaction [1, 28, 30], virtual/augmented reality games [2, 20, 32], intelligent cockpit systems [12], medical analysis [4], gaze/head redirection [40], etc. With the development of deep learning techniques, appearance-based gaze estimation attracts much attention recently. To improve gaze estimation accuracy, many large-scale gaze estimation datasets have been proposed [10, 19, 35, 37].
Due to the differences of subjects, background environ-*Corresponding Author. † denotes equal contribution. This work was supported by the National Natural Science Foundation of China (NSFC) under Grant 61972012 and China Postdoctoral Science Foundation under
Project 2020TQ0156.
Figure 1. Overview of the proposed Plug-and-Play (PnP) adaption framework for generalizing gaze estimation to a new domain. ments, and illuminations between these datasets, the per-formance of gaze estimation models that are trained on a single dataset usually dramatically degrades when testing on a new dataset.
To tackle such a gap in gaze estimation, several works have been proposed by utilizing adversarial training (ADV) [26, 31], few-shot learning (FSL) [25, 33], or em-bedding with prediction consistency [14]. However, these works mainly focus on the inter-person gap (i.e., personal calibration) from the same dataset, where the environmen-tal conditions are often very similar. When generalizing the gaze estimation models to new datasets, the differences be-tween the image capture devices and the environments can-not be neglected, as illustrated in Fig. 1. Consequently, a gaze estimation model trained in the source domain cannot achieve satisfactory performance by directly applying to the target domain. In this paper, we focus on domain adaptation between different datasets, which is more common in prac-tical applications.
Unlike most of the existing works in gaze domain adap-tation following supervised learning paradigm, this work aims to generalize the gaze estimation in an unsupervised manner. This is because we usually do not have access to the ground-truth labels in a new domain. The task can be viewed as an unsupervised domain adaptation (UDA) prob-lem. Table 1 shows the differences between the existing gaze domain adaptation approaches and UDA. While the
ADV and FSL approaches are model-specific, the UDA and fine-tuning approaches are model-agnostic.
It means that these methods are capable of adapting arbitrary network ar-chitectures. However, the fine-tuning approach requires a number of target labels. The UDA approach is more chal-lenging than conventional approaches since the lack of an-notated data in a new domain prohibits fine-tuning of the pre-trained models. Many UDA methods for other com-puter vision tasks have explored to take additional priors, e.g., shape priors for hand segmentation [3], pseudo labels for person re-identification [11], random sampler [21] for head pose estimation, etc. However, the aforementioned priors are not available on gaze estimation tasks, since gaze directions have no specific shapes or labels.
Table 1. Differences among the domain adaptation approaches.
# Target images
■■■■■
■
Methods
Fine-tuning
FSL [25, 33]
ADV [26, 31] ■■
UDA (Ours)
■
# Target labels Model
■■■■■
Agnostic
■
Specific
Not require
Specific
Agnostic
Not require
In this paper, we propose an outlier-guided plug-and-play collaborative learning framework for generalizing gaze estimation with unsupervised domain adaptation. Optimiz-ing the error-prone outliers has been proved to be helpful for model generalization [18, 34]. We follow the similar idea and take the outliers of pre-trained models’ outputs in the target domain as noisy labels. We employ two groups of networks that collaboratively teach each other. While these two groups share the same architecture, one group is the momentum version of another group. The primary contri-butions of this paper are summarized as follow:
• We propose a plug-and-play gaze adaptation frame-work (PnP-GA) for generalizing gaze estimation in new domains. Existing gaze estimation networks can be easily plugged into our framework without modifi-cation of their architectures.
• We propose an outlier-guided collaborative learning strategy for the unsupervised domain adaptation.
It only requires a few samples from the target domain without any labels. We specifically design an outlier-guided loss to better characterize the outliers and guide the learning.
• The PnP-GA framework shows exceptional perfor-mances with plugging of the existing gaze estima-tion networks. Our system achieves performance improvements over the baseline system of 36.9%, 31.6%, 19.4%, and 11.8% on four gaze adaptation tasks: ETH-to-MPII, ETH-to-EyeDiap, Gaze360-to-MPII, and Gaze360-to-EyeDiap. 2.