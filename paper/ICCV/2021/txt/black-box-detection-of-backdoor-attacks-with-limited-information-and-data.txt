Abstract
Although deep neural networks (DNNs) have made rapid progress in recent years, they are vulnerable in adversarial environments. A malicious backdoor could be embedded in a model by poisoning the training dataset, whose intention is to make the infected model give wrong predictions during inference when the specific trigger appears. To mitigate the potential threats of backdoor attacks, various backdoor de-tection and defense methods have been proposed. However, the existing techniques usually require the poisoned training data or access to the white-box model, which is commonly unavailable in practice. In this paper, we propose a black-box backdoor detection (B3D) method to identify backdoor attacks with only query access to the model. We introduce a gradient-free optimization algorithm to reverse-engineer the potential trigger for each class, which helps to reveal the existence of backdoor attacks. In addition to backdoor de-tection, we also propose a simple strategy for reliable pre-dictions using the identified backdoored models. Extensive experiments on hundreds of DNN models trained on several datasets corroborate the effectiveness of our method under the black-box setting against various backdoor attacks. 1.

Introduction
Despite the unprecedented success of Deep Neural Net-works (DNNs) in various pattern recognition tasks [17], the reliability of these models has been significantly challenged in adversarial environments [2, 5], where an adversary can cause unintended behavior of a victim model by malicious attacks. For example, adversarial attacks [4, 13, 18, 42] ap-ply imperceptible perturbations to natural examples with the purpose of misleading the target model during inference.
Different from adversarial attacks, backdoor (Trojan) at-tacks [9, 19, 33] aim to embed a backdoor in a DNN model by injecting poisoned samples into its training data. The in-*Corresponding author.
Figure 1: Illustration of backdoor attack and detection. By speci-fying the target class and the trigger pattern, the adversary poisons a portion of training data to have the trigger stamped and the label changed to the target. During inference, the model predicts nor-mally on clean inputs but misclassifies the triggered inputs as the target class. Our detection method reverse-engineers the potential trigger for each class and judges whether any class induces a much smaller trigger, which can be used to detect backdoor attacks. fected model performs normally on clean inputs, but when-ever the embedded backdoor is activated by a backdoor trig-ger, such as a small pattern in the input, the model will out-put an adversary-desired target class, as illustrated in Fig. 1.
As many users with insufficient training data and computa-tional resources would like to outsource the training proce-dure or utilize commercial APIs from third parties for solv-ing a specific task, the vendors of machine learning services with malicious purposes can easily exploit the vulnerability of DNNs to insert backdoors [9, 19]. From the industry per-spective, backdoor attacks are among the most worrisome
Accessibility
White-box model
Poisoned training data
Clean validation data
Training-stage
Inference-stage
[6, 7, 43, 47]
✓
✓
✗
[32, 35, 49]
✓
✗
✓
[20, 22, 24, 36, 45]
✓
✗
✓
[8, 10, 11]
✓
✗
✗
B3D (Ours)
✗
✗
✓
B3D-SS (Ours)
✗
✗
✗
Table 1: Model and data accessibility required by various backdoor defenses. We detail on some most related defenses in Sec. 2. security threats when using machine learning systems [29].
Due to the threats, tremendous effort has been made to detect or defend against backdoor attacks [7, 14, 16, 20, 27, 32, 36, 43, 45]. Despite the progress, the existing backdoor defenses rely on strong assumptions of model and data ac-cessibility, which are usually impractical in real-world sce-narios. Some training-stage defenses [7, 43] aim to identify and remove poisoned samples in the training set to mitigate their effects on the trained models. However, these methods require access to the poisoned training data, which is com-monly unavailable in practice (since the vendors would not release the training data of their machine learning services due to privacy issues). On the other hand, some inference-stage defenses [8, 20, 36, 45] attempt to reverse-engineer the trigger through gradient-based optimization approaches and then decide whether the model is normal or backdoored based on the reversed triggers. Although these methods do not need the poisoned training data and could be applied to any pre-trained model, they still require the gradients of the white-box model to optimize the backdoor trigger. In this work, we focus on a black-box setting, in which neither the poisoned training data nor the white-box model can be ac-quired, while only query access to the model is attainable.
Justification of the black-box setting. Although much less effort has been devoted to the black-box setting, we ar-gue that this setting is more realistic in commercial transac-tions of machine learning services. For example, a lot of or-ganizations (e.g., governments, hospitals, banks) purchase machine learning services that are applied to some safety-critical applications (e.g., face recognition, medical image analysis, risk assessment) from vendors. These systems po-tentially contain backdoors injected by either the vendors, the participants in federated learning, or even someone who posts the poisoned data online [1, 19]. Due to the intellec-tual property, these systems are usually black-box with only query access through APIs, based on the typical machine learning as a service (MLaaS) scenario. Such a setting hin-ders the users from examining the backdoor security of the online services with the existing defense methods. Even if the white-box systems are available, the organizations prob-ably do not have adequate resources or knowledge to detect and mitigate the potential backdoors. Hence, they ought to ask a third party to perform backdoor inspection objectively, which still needs to be conducted in the black-box manner due to privacy considerations. Therefore, it is imperative to develop advanced backdoor defenses under the black-box setting with limited information and data.
In this paper, we propose a black-box backdoor detec-tion (B3D) method. Similar to [45], B3D formulates back-door detection as an optimization problem, which is solved using clean data to reverse-engineer the potential trigger for each class, as illustrated in Fig. 1. Differently, we solve the problem by adopting a gradient-free algorithm, which min-imizes the objective function through model queries solely.
Moreover, we demonstrate the applicability of B3D when using synthetic samples (denoted as B3D-SS) in the case that the clean samples for optimization are unavailable. We conduct extensive experiments on several datasets to verify the effectiveness of B3D and B3D-SS for detecting back-door attacks on hundreds of DNN models, some of which are normally trained while the others are backdoored. Our methods achieve comparable and even better backdoor de-tection accuracy than the previous methods based on model gradients, due to the appropriate problem formulation and efficient optimization procedure, as detailed in Sec. 3.
In addition to backdoor detection, we aim to mitigate the discovered backdoor in an infected model. Under the black-box setting, the typical re-training or fine-tuning [32, 43, 45] strategies cannot be adopted since we are unable to modify the black-box model. Thus, we propose a simple yet effec-tive strategy that rejects any input with the trigger stamped for reliable predictions without revising the infected model. 2.