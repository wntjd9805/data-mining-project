Abstract
Despite the fast progress in training specialized mod-els for various tasks, learning a single general model that works well for many tasks is still challenging for computer vision. Here we introduce multi-task self-training (MuST), which harnesses the knowledge in independent specialized teacher models (e.g., ImageNet model on classiﬁcation) to train a single general student model. Our approach has three steps. First, we train specialized teachers indepen-dently on labeled datasets. We then use the specialized teachers to label an unlabeled dataset to create a multi-task pseudo labeled dataset. Finally, the dataset, which now contains pseudo labels from teacher models trained on different datasets/tasks, is then used to train a student model with multi-task learning. We evaluate the feature rep-resentations of the student model on 6 vision tasks including image recognition (classiﬁcation, detection, segmentation) and 3D geometry estimation (depth and surface normal es-timation). MuST is scalable with unlabeled or partially la-beled datasets and outperforms both specialized supervised models and self-supervised models when training on large scale datasets. Lastly, we show MuST can improve upon already strong checkpoints [23] trained with billions of ex-amples. The results suggest self-training is a promising di-rection to aggregate labeled and unlabeled training data for learning general feature representations. 1.

Introduction
Visual representation learning is a core problem in com-puter vision. Supervised and self-supervised pre-training have shown promising results in transferring the learned feature representations to downstream tasks. Typically, a model is pre-trained with a supervised [28, 10] or a self-supervised objective [5, 16, 17]. Despite the wide adoption of transfer learning from supervised training, the features may not necessarily be useful for downstream tasks. For example, He et al. found that ImageNet pre-training fails
∗Authors contributed equally.
Figure 1. An overview of Multi-Task Self-Training (MuST).
Specialized Teacher represents a supervised model trained on a single task and dataset (e.g., classiﬁcation model trained on Im-ageNet). Specialized Teacher models are trained independently on their own tasks and datasets. They then generate pseudo la-bels on a shared dataset. Finally, a single General Student model is trained jointly using the pseudo (and supervised) labels on the shared dataset. to improve COCO instance segmentation [18]. In contrast,
Shao et al. showed features learned from Objects365 de-tection dataset improve COCO instance segmentation by a large margin [46]. Pre-training with a specialized task that aligns with the downstream target task still yields the best performance in object detection [32, 46] and semantic seg-mentation [4].
Intuitively, it is possible to learn general features by training a model to simultaneously do well on multiple tasks. Recent work in NLP started to show promising re-sults on learning a generalist model with multi-task learn-ing [55, 9].
In computer vision, the biggest challenge of training a multi-task model is in the data collection and annotation. Despite datasets like COCO [34], collecting a wide variety of annotations (e.g., instance segmentation, person keypoints, image caption) for the same image dataset is quite challenging. Due to the time consuming nature of annotating images with labels, it is hard to scale such efforts with the number of images and the number of tasks. The lack of large scale multi-task datasets impedes the progress in multi-task learning for computer vision.
In this work, we study using self-training to remedy the issue. We propose to use pseudo labeling to enable large scale multi-task feature learning for computer vision. Zoph
et al. [62] observed that self-training further improves pre-training for transfer learning, and that self-training works even when pre-training fails to outperform a randomly ini-tialized model. The gap between pre-training and self-training suggests that self-training can learn better features from pseudo labels. Inspired by this observation, we ﬁrst investigate whether good features can be learned by only using pseudo labels. We train teacher models using datasets such as COCO or Objects365 to generate pseudo labels on unlabeled images. Figure 2 shows example pseudo labels on ImageNet. Surprisingly, we ﬁnd a student model trained with only these pseudo labels preserves most of the trans-fer learning performance of its specialized teacher model.
This ﬁnding suggests pseudo labels are effective at distill-ing the knowledge in a supervised dataset. Therefore, we can use pseudo labels to transfer knowledge from multiple teacher models to a single student model for representation learning.
We propose Multi-Task Self-Training (MuST) to train a generalist student model on the information distilled from teacher models trained on different tasks and datasets. Fig-ure 1 shows the overview of the algorithm. MuST has three steps. First, it trains specialized teachers independently on labeled datasets. For example, one teacher can be trained with depth prediction and another teacher can be trained with object detection. The specialized teachers are then used to label a larger unlabeled dataset to create a multi-task pseudo labeled dataset. For example, these teachers can generate depth estimations and object detections on the
ImageNet dataset. Finally, the dataset, which now con-tains pseudo labels from teacher models trained on different datasets/tasks, is used to train a student model with multi-task learning. Hence the student, for example, can do depth prediction and object detection at the same time.
In our experiments, we have four teacher models: clas-siﬁcation, semantic segmentation, object box detection, and depth estimation. We design a simple model architecture (Figure 3) based on ResNet [20] and feature pyramid net-works (FPN) [33]. The parameters in the ResNet-FPN backbone are shared across different tasks. For each in-dividual task, it has a small task-speciﬁc head consisting of a few convolution layers followed by a linear prediction layer. Our experiments show that this simple model archi-tecture is able to absorb the knowledge of different tasks in the shared backbone. The generalist student model is on par with/outperforms its specialist teacher models for all trans-fer learning tasks.
The recent self-supervised algorithms like SimCLR [5],
MoCo [17] are shown to create representations that are on par or better than its supervised counterpart. In our experi-ments, MuST also outperforms SimCLR [5] by a large mar-gin on segmentation and depth estimation tasks. We also observe that the representations learned by SimCLR is on
Figure 2. Examples of pseudo labels on ImageNet. Left: bound-ing boxes labeled with an Objects365 teacher model. Middle: se-mantic segmentation labeled with a COCO teacher model. Right: depth labeled with a MiDaS teacher model. par with those of supervised learning on ImageNet (1.3M images) but does not scale as well on JFT (300M images).
On the contrary, MuST outperforms SimCLR [5] on both
ImageNet and JFT. Moreover, MuST also outperforms su-pervised JFT pre-training for 5 out of 6 tasks except the im-age classiﬁcation task. The results indicate the potential of
MuST in learning general feature representations that im-prove with more unlabeled data.
Lastly, we show MuST can improve upon already strong checkpoints such as ALIGN [23]. We ﬁne-tune ALIGN checkpoints, previously trained with billions of supervised examples, with MuST pseudo labels and ﬁnd improvements on a suite of downstream tasks: detection, segmentation, and depth estimation tasks.
We summarize our contributions below:
• We propose Multi-Task Self-Training (MuST), a sim-ple algorithm for creating general visual representa-tions by multi-task learning with pseudo labels.
• We conduct experiments by jointly training across several datasets (e.g., ImageNet, Objects365, COCO,
JFT) to learn general feature representations that out-performs representations learned by supervised and self-supervised methods.
• We perform experiments to compare supervised, self-supervised, and MuST on 6 computer vision tasks in-cluding tasks in image recognition (classiﬁcation, de-tection, segmentation) and 3D geometry estimation (depth and surface normal estimation).
• MuST can be used to improve upon already strong checkpoints and achieve competitive results on a va-riety of tasks compared to task-speciﬁc state-of-the-art models. 2.