Abstract
Most existing brain tumor segmentation methods usually exploit multi-modal magnetic resonance imaging (MRI) im-ages to achieve high segmentation performance. However, the problem of missing certain modality images often hap-pens in clinical practice, thus leading to severe segmenta-tion performance degradation. In this work, we propose a
Region-aware Fusion Network (RFNet) that is able to ex-ploit different combinations of multi-modal data adaptively and effectively for tumor segmentation. Considering dif-ferent modalities are sensitive to different brain tumor re-gions, we design a Region-aware Fusion Module (RFM) in RFNet to conduct modal feature fusion from available image modalities according to disparate regions. Benefit-ing from RFM, RFNet can adaptively segment tumor re-gions from an incomplete set of multi-modal images by ef-fectively aggregating modal features. Furthermore, we also develop a segmentation-based regularizer to prevent RFNet from the insufficient and unbalanced training caused by the incomplete multi-modal data. Specifically, apart from obtaining segmentation results from fused modal features, we also segment each image modality individually from the corresponding encoded features. In this manner, each modal encoder is forced to learn discriminative features, thus improving the representation ability of the fused fea-tures. Remarkably, extensive experiments on BRATS2020,
BRATS2018 and BRATS2015 datasets demonstrate that our
RFNet outperforms the state-of-the-art significantly. 1.

Introduction
Brain tumor segmentation, aiming to segment different brain tumor regions, is vital for clinical assessment and sur-gical planning. In order to improve the segmentation accu-racy, most existing methods [16, 43, 17, 29, 11, 4, 38] use four modalities simultaneously, namely Fluid Attenuation
*This work was done when Yuhang Ding interned at Baidu Research.
Yi Yang is the corresponding author.
Figure 1. Illustration of different sensitivities of modalities to dif-ferent brain tumor regions. From left to right: Images of four modalities, i.e., Flair, T1c, T1 and T2, and the corresponding la-bels of three patients are shown. In the segmentation results, dif-ferent colors denote different brain tumor regions.
Inversion Recovery (Flair), contrast enhanced T1-weighted (T1c), T1-weighted (T1) and T2-weighted (T2). However, the missing modality problem is very common in clinical practice due to different scanning protocols and patient con-ditions. Therefore, these standard brain tumor segmentation networks cannot be deployed directly in practice.
Incomplete multi-modal brain tumor segmentation ap-proaches [3, 10, 14, 44] have been proposed to deal with various missing situations. Havaei et al. [14] and Dorent et al. [10] compute the mean and variance across accessi-ble multi-modal features as fused features. However, this fusion treats each modality equally regardless of different missing scenarios and thus may fail to aggregate features ef-fectively. Later, Chen et al. [3] and Zhou et al. [44] leverage attention mechanisms to emphasize contributions from dif-ferent accessible modalities. However, they do not fully ex-ploit the relations between tumor regions and image modal-ities. In particular, different modalities contain distinct ap-pearances and thus have different sensitivities to diverse tu-mor regions. For example, as visible in Fig. 1, T1c is more sensitive to the red and blue tumor areas while Flair and T2
provide more information for the green tumor area. This observation motivates us that we should pay different atten-tion to different modalities and different regions in order to achieve accurate brain tumor segmentation.
Taking the relations between modalities and regions into account, we propose a Region-aware Fusion Network (RFNet) to aggregate various accessible multi-modal fea-tures from different regions adaptively. Our RFNet is con-structed by an encoder-decoder architecture, where four encoders are employed to extract features from different modal images.
In order to establish the relations be-tween image modalities and tumor regions, we introduce a
Region-aware Fusion Module (RFM) into our RFNet. RFM first divides modal features into different regions (i.e., tu-mor sub-structure) via a learned probability map. The prob-ability map indicates the probabilities of tumor regions at each pixel. Then, RFM generates corresponding attention weights in each region to adaptively control the contribu-tions of different image modalities.
Since brain tumors usually occupy a small part of brains, we introduce a region-norm pooling operation to obtain a normalized global feature from each region. Thereby, we prevent the global feature from being numerically too small.
Then, we employ two fully-connected layers and a sigmoid activation to attain attention weights from the global feature
In this fashion, for image modalities and tumor regions.
RFM will generate larger weights for the modalities which are more sensitive to certain tumor regions, thus leading to discriminative fused features for accurate segmentation.
Due to the missing hetero-modal data, RFNet will face the problem of unbalanced training. To be specific, RFNet might try to seek the easiest way to segment brain tumors from the multi-modal data.
In other words, the network segments each region mainly by exploiting the modalities which are sensitive to the region rather than all the modality information. However, this will lead to poor segmentation accuracy when some modalities are missing. To tackle this problem, we develop a segmentation-based regularizer. In particular, a weight-shared decoder is employed to segment each modality individually.
In this manner, each modal encoder is forced to learn discriminative features for all the tumor regions. Therefore, RFNet can segment differ-ent regions well even when some modalities are missing.
Benefit from the proposed fusion module and regularizer,
RFNet achieves higher accuracy than the state-of-the-art methods on BRATS2020, BRATS2018 and BRATS2015.
This demonstrates the superiority of our method.
Overall, our contributions are threefold:
• We propose a Region-aware Fusion Network (RFNet) for incomplete multi-modal brain tumor segmentation.
Particularly, we introduce a novel a Region-aware Fu-sion Module (RFM) by explicitly taking the relations between modalities and regions into account. With the help of RFM, RFNet effectively aggregates diverse combinations of modal features and produces discrim-inative fused features for segmentation.
• To address the unbalanced training problem of RFNet, we propose a segmentation-based regularizer. The pro-posed regularizer enforces each modal encoder to pro-duce discriminative features for segmenting all the tu-mor regions, thus further improving the discrimina-tiveness of the fused features.
• Taking advantage of the proposed fusion module and regularizer, RFNet achieves superior segmentation ac-curacy compared to the state-of-the-art on the widely-used BRATS2020, BRATS2018 and BRATS2015 benchmarks. 2.