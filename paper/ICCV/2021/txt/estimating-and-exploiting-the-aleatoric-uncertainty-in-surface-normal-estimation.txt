Abstract
Surface normal estimation from a single image is an important task in 3D scene understanding. In this paper, we address two limitations shared by the existing meth-ods: the inability to estimate the aleatoric uncertainty and lack of detail in the prediction. The proposed network es-timates the per-pixel surface normal probability distribu-tion. We introduce a new parameterization for the distribu-tion, such that its negative log-likelihood is the angular loss with learned attenuation. The expected value of the angu-lar error is then used as a measure of the aleatoric uncer-tainty. We also present a novel decoder framework where pixel-wise multi-layer perceptrons are trained on a subset of pixels sampled based on the estimated uncertainty. The proposed uncertainty-guided sampling prevents the bias in training towards large planar surfaces and improves the quality of prediction, especially near object boundaries and on small structures. Experimental results show that the proposed method outperforms the state-of-the-art in Scan-Net [4] and NYUv2 [33], and that the estimated uncer-tainty correlates well with the prediction error. Code is available at https://github.com/baegwangbin/ surface_normal_uncertainty. 1.

Introduction
The ability to estimate surface normal from a single
RGB image plays a crucial role in understanding the 3D scene geometry. The estimated normal can be used to build augmented reality (AR) applications [18] or to control au-tonomous robots [41]. In this work, we address two limita-tions shared by the state-of-the-art methods. (1) Inability to estimate the aleatoric uncertainty. State-of-the-art learning-based approaches [39, 7, 1, 31, 14, 18, 42, 24, 32, 6, 38] train deep networks by minimizing some distance metric (e.g., L2) between the predicted normal and the ground truth. However, the ground truth normal, calcu-lated from a measured depth map, can be sensitive to the
Figure 1. Comparison between our method and TiltedSN [6]. The proposed network estimates the surface normal probability distri-bution, from which the expected angular error can be inferred. The prediction made by our method shows clearer object boundaries and preserves a higher level of detail. This is due to the proposed uncertainty-guided sampling which prevents the bias in training towards large planar surfaces. depth noise and to the algorithm used to compute the nor-mal (see Fig. 2 for examples of inaccurate ground truth).
The network should be able to capture such aleatoric uncer-tainty, in order to be deployed in real-world applications. (2) Lack of detail in the prediction. An indoor scene generally consists of large planar surfaces (e.g., walls and floors) and small objects with complex geometry. There-fore, if the training loss is applied to all pixels, the learn-ing becomes biased to large surfaces, resulting in an over-smoothed output. Such bias can be solved by applying the loss on a carefully selected subset of pixels. For example, in [40], pair-wise ranking loss was applied to the pixels near instance boundaries to improve the quality of monocular depth estimation. However, such effort has not been made for surface normal estimation.
â€¢ State-of-the-art performance. Experimental results show that the proposed method achieves state-of-the-art performance on ScanNet [4] and NYUv2 [33].
Qualitatively, the prediction made by our method con-tains a higher level of detail (see Fig. 1). 2.