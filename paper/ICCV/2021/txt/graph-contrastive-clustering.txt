Abstract
Recently, some contrastive learning methods have been proposed to simultaneously learn representations and clus-improvements. tering assignments, achieving signiﬁcant
However, these methods do not take the category infor-mation and clustering objective into consideration, thus the learned representations are not optimal for clustering and the performance might be limited. Towards this issue, we ﬁrst propose a novel graph contrastive learning frame-work, and then apply it to the clustering task, resulting in the Graph Constrastive Clustering (GCC) method. Dif-ferent from basic contrastive clustering that only assumes an image and its augmentation should share similar repre-sentation and clustering assignments, we lift the instance-level consistency to the cluster-level consistency with the assumption that samples in one cluster and their augmen-tations should all be similar. Speciﬁcally, on the one hand, we propose the graph Laplacian based contrastive loss to learn more discriminative and clustering-friendly features.
On the other hand, we propose a novel graph-based con-trastive learning strategy to learn more compact cluster-ing assignments. Both of them incorporate the latent cat-egory information to reduce the intra-cluster variance as well as increase the inter-cluster variance. Experiments on six commonly used datasets demonstrate the superiority of our proposed approach over the state-of-the-art methods.1 1.

Introduction
Figure 1. Motivation of the proposed GCC. (a) Existing contrastive learning based clustering methods mainly focus on instance-level consistency, which maximizes the correlation between self-augmented samples and treats all other samples as negative sam-ples. (b) GCC incorporates the category information to perform the contrastive learning at both the instance and the cluster levels, which can better minimize the intra-cluster variance and maximize the inter-cluster variance.
Based on a large number of annotated training sam-ples, deep learning achieves signiﬁcant success in the past
*H. Zhong and J. Wu contributed equally and are joint ﬁrst authors. This work was done when H. Zhong worked full time at DAMO Academy, Alibaba Group.
†Corresponding author. 1Code address: https://github.com/mynameischaos/GCC decade [15]. However, it is very expensive and time-consuming to manually label a large training dataset. It is also impractical to collect a labeled dataset for each domain or task. In this case, clustering attracts more attention re-cently, which aims to divide the samples into separate clus-ters without knowing the label information.
Clustering [3, 36, 16, 17] is a very challenging task since samples in the same class have various appearances and su-pervision signals are lacked to train the model. Classic clus-tering methods [43, 10, 2, 35, 37], such as spectral cluster-ing [26] and subspace clustering [24, 9], suffer from two ob-vious limitations, including indiscriminative feature repre-sentation and sub-optimal solution for clustering caused by the separation of feature extraction and clustering. Some re-cent deep learning based methods can well handle the above issues. For example, auto-encoder related methods [34, 19] minimize the reconstruction error and assign various reg-ularization terms in the latent feature space, such as the
KL-divergence [39]. Deep adaptive clustering (DAC) [3] maximizes the similarity between self-augmented samples to adaptively train the neural network. Deep comprehensive correlation mining (DCCM) [36] thoroughly investigates various kinds of correlation among samples and features.
These approaches achieve good clustering performance, but their upper bound accuracy is limited since the learned fea-tures are not discriminative enough.
Recently, contrastive learning [4] has received much at-tention in unsupervised feature learning, which emphasizes the importance of data augmentation and maximizes the agreement between two augmented samples. Because of its success, a few approaches [16, 44, 23, 36] are proposed to jointly optimize the contrastive learning and clustering. For instance, partition conﬁdence maximisation (PICA) [16] learns the most semantically plausible clustering solution by maximizing partition conﬁdence, which corresponds
Instead of only to the cluster-wise contrastive learning. using the cluster contrast in PICA, deep robust cluster-ing (DRC) [44] adopts the conventional contrastive learning in feature and cluster space simultaneously. These methods signiﬁcantly improve the clustering performance, but they still face another obvious issue: both of them still follow the basic framework of contrastive learning and only as-sume that a sample and its augmentations should be similar in the feature space, which does not incorporate the latent category information into clustering.
In view of the above limitations, we propose the graph contrastive framework and apply it to the clustering task, re-sulting in the Graph Contrastive Clustering (GCC) method.
As shown in Figure 1, we assume that samples in one cluster and their augmentations should share similar fea-ture representations and clustering assignments, which lifts the commonly-used instance-level consistency in PICA and
DRC to the cluster-level consistency. By incorporating the latent category/cluster information, GCC can help to learn more discriminative features and better clustering as-signments, which is more suitable for the clustering task.
Speciﬁcally, we ﬁrst construct a similarity graph based on the current features, then we apply it to both representation learning and clustering learning. For representation learn-ing, the graph Laplacian based contrastive loss is proposed to learn more clustering-friendly features. For clustering learning, a novel graph-based contrastive learning strategy is proposed to learn more compact clustering assignments.
Both of them can help to decrease the intra-class variance and increase inter-class variance. Experimental results on six challenging datasets validate the effectiveness of the proposed method. We also perform extensive ablation anal-ysis to demonstrate the superiority of graph contrastive.
Our main contributions are summarized as follows: 1. By incorporating the latent category information, we propose a novel graph contrastive framework, which assumes that samples in one cluster and their augmen-tations should share similar representations and clus-tering assignments. This framework lifts the tradition instance-level consistency to cluster-level consistency, thus can better reduce the intra-class variance as well as increase the inter-class variance. 2. We apply the proposed graph contrastive framework to the clustering task, and come up with the graph contrastive clustering method (GCC), which consists of two graph contrastive modules. For representation graph contrastive module, a graph Laplacian based contrastive loss is proposed to learn more discrimina-tive and clustering-friendly features. For assignment graph contrastive module, a novel graph-based con-trastive learning strategy is proposed to learn more compact clustering assignments. 3. We conduct extensive experiments on image cluster-ing and our proposed method achieves signiﬁcant im-provement on various datasets. We also conduct an ex-tensive ablation study to validate the effectiveness of each proposed module. 2.