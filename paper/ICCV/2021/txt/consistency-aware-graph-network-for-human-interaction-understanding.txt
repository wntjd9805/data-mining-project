Abstract
Compared with the progress made on human activity classification, much less success has been achieved on hu-man interaction understanding (HIU). Apart from the latter task is much more challenging, the main cause is that re-cent approaches learn human interactive relations via shal-low graphical models, which is inadequate to model com-plicated human interactions. In this paper, we propose a consistency-aware graph network, which combines the rep-resentative ability of graph network and the consistency-aware reasoning to facilitate HIU. Our network consists of three components, a backbone CNN to extract image fea-tures, a factor graph network to learn third-order interac-tive relations among participants, and a consistency-aware reasoning module to enforce labeling and grouping consis-tencies. Our key observation is that the consistency-aware-reasoning bias for HIU can be embedded into an energy, minimizing which delivers consistent predictions. An effi-cient mean-field inference algorithm is proposed, such that all modules of our network could be trained jointly in an end-to-end manner. Experimental results show that our ap-proach achieves leading performance on three benchmarks.
Code is available at https://git.io/CAGNet. 1.

Introduction
Analyzing human activities in natural scenes is a fun-damental task to many potential applications like video surveillance [35], key-event retrieval [12], social behavior interpretation [2] and sports analysis [28]. Abundant tech-niques have been developed for human activity recognition (HAR, where the goal is to assign an activity label to each image or video) [7, 25, 16, 33, 21, 43, 43, 27], which have gained impressive progress on recognition accuracy. How-ever, the task of human interaction understanding (HIU) is much less successful mainly because current methods learn
∗Jianhua Zhang {zjh@ieee.org} is the corresponding author.
Figure 1. The graphical representation of HIU in a scene with three people. We decompose HIU into two sub-tasks: recognizing person-wise actions (as denoted by the node labels, with KK, BK,
HG, NI indicating kick, be-kicked, hug, no-interaction, respective-ly) and predicting if any pair of people are interacting (solid edges) or not (dashed edges). Applying consistency-unaware models to such cases can lead to inconsistent predictions as highlighted by the red edges and labels (see Section 1 for details). We address such issue by presenting a consistency-aware graph network with two types of third-order dependencies incorporated. human interactive relations via shallow graphical represen-tations [42, 41, 40, 25, 7, 46], which is inadequate to model complicated human interactions, e.g. fighting and chasing as two concurrent activities happening in the same scene.
As commonly done in literature [25, 42, 40, 41], we decompose HIU into two sub-tasks illustrated by Figure 1 middle: 1) The individual action prediction task assigning each participant an action label; 2) The pairwise interac-tive prediction task determining if any pair of participants are interacting or not. Solving the two sub-tasks provides a way to disentangle concurrent human activities with multi-ple participants, as well as a comprehensive understanding to surveillance scenes. Also note that natural scene may in-clude either/both physical interactions (e.g., handshaking, hugging and punching) and non-physical interactions (e.g., chasing, talking and queuing). People performing such in-teractions naturally form a group. In this sense, the action of a person can be defined as “what is she/he doing under such interactive circumstances?”, which is challenging to answer if only considering the local representations. Though HI-U performance had been lifted a lot by a conjunctive us-age of deep features and rich contextual information, there still exist two main challenges. Since most existing work-s perform piece-wise learning of deep feature representa-tions and contextual models [42, 40], the first challenge is how to learn deep features and contextual relations jointly.
The second challenge is how to ensure prediction consis-tency for the two sub-tasks of HIU. In this paper, we tackle two types of prediction inconsistencies illustrated by Fig-ure 1 right. The first type is called the labeling inconsis-tency, e.g. the action label of B (i.e. kick) is inconsistent with the action label of C (i.e. hug) as they are interacting (denoted by a solid edge). The second type is called the grouping inconsistency, under the assumption that interact-ing people belong to the same group while non-interacting ones belong to separate groups. Consequently, the predic-tion (A, C) are not interacting (denoted by the dashed edge) is inconsistent with the prediction that (A, B) are interact-ing and (B, C) are interacting as well. To address the two challenges, we present a consistency-aware graph network (CAGNet), which consists of a backbone CNN to extrac-t image features, a third-order graph network (TOGN) to learn human interactive context, and a consistency-aware reasoning (CAR) module to improve the consistency with-in action and interaction predictions. All components of
CAGNet could be trained jointly and efficiently with GPU acceleration. We empirically validate the effectiveness of these three components on three benchmarks of human in-teraction understanding.
Our contributions are of three aspects. First, we propose a TOGN for HIU, which is more powerful than the widely adopted pairwise graph networks in terms of representing the interactive relations among people. Second, we present an efficient CAR module to resolve the labeling and group-ing inconsistencies within HIU predictions. Third, our pro-posed CAGNet, which takes the TOGN and CAR modules as its building-blocks, outperforms the state-of-the-art re-sults by salient margins on three evaluated benchmarks. 2.