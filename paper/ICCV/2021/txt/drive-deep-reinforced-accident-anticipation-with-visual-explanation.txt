Abstract
Traffic accident anticipation aims to accurately and promptly predict the occurrence of a future accident from dashcam videos, which is vital for a safety-guaranteed
To encourage an early and accu-self-driving system. rate decision, existing approaches typically focus on cap-turing the cues of spatial and temporal context before a future accident occurs. However, their decision-making lacks visual explanation and ignores the dynamic interac-tion with the environment. In this paper, we propose Deep
ReInforced accident anticipation with Visual Explanation, named DRIVE. The method simulates both the bottom-up and top-down visual attention mechanism in a dashcam ob-servation environment so that the decision from the pro-posed stochastic multi-task agent can be visually explained by attentive regions. Moreover, the proposed dense an-ticipation reward and sparse fixation reward are effective in training the DRIVE model with our improved reinforce-ment learning algorithm. Experimental results show that the DRIVE model achieves state-of-the-art performance on multiple real-world traffic accident datasets. Code and pre-trained model are available at https://www.rit.edu/ actionlab/drive. 1.

Introduction
With increasing demand for autonomous driving, antic-ipating possible future accidents is becoming the central consideration to guarantee a safe driving strategy [3, 40, 2].
Given a dashcam video, an accident anticipation model aims to tell the driving system if and when a traffic accident will occur in the near future. Despite remarkable advances in visual perception [10, 14, 20], the decision-making of driving control has long been studied in isolation with vi-sion perception research for the autonomous driving sce-nario [26, 36]. We target at bridging this gap by investi-gating a key research question: where do drivers look when predicting possible future accidents? This will lead to a vi-sually explainable model that associates the low-level visual attention and high-level accident anticipation.
Figure 1: The Markov decision process of the DRIVE model. The neural network agent (left) learns to exploit visual attentive state (bottom) to predict the actions includ-ing the accident score and the next fixation (top), which in return explore the driving environment (right) to maximize the total reward (middle).
The traffic accident anticipation is far from being solved due to the following challenges. First, the visual cues of a future accident are vital to training a discriminative model but in practice, they are difficult to be captured from the lim-ited and noisy video data before the accident occurs. Previ-ous works take advantage of object detection and learn the accident visual cues by either soft attention in [3] or graph relational learning in [2]. In this paper, we propose to ex-plicitly learn the visual attention behavior to address where to look such that accident risky regions can be localized.
Second, it is intrinsically a trade-off between an early de-cision and a correct decision since the earlier to anticipate an accident, the harder to make the decision right due to fewer accident-relevant cues. Existing works [3, 2] simply address the trade-off by training supervised deep learning models with an exponentially weighted classification loss.
In this paper, we address this trade-off by formulating the task as a Markov Decision Process (MDP), where explo-ration and exploitation can be dynamically balanced in a driving environment.
In the context of accident anticipa-tion, the MDP model aims to exploit the immediate visual cues for accident anticipation and also explore more possi-bilities of accident scoring and attention allocation.
Our proposed DRIVE model is illustrated with the MDP
perspective in Fig. 1. The DRIVE model simultaneously learns the policies of accident anticipation and fixation pre-diction based on a deep reinforcement learning (DRL) al-gorithm. At each time step, the agent takes actions to predict the occurrence probability of a future accident, as well as the fixation point indicating where drivers will look in the next time step. Our environmental model dynam-ically provides the observation state by considering both the bottom-up and top-down visual attention, which is re-currently modulated by the actions from the previous time step. We develop a novel dense anticipation reward to en-courage early and accurate prediction, as well as a sparse fixation reward to enable visual explanation. Moreover, to effectively train the DRIVE model on real-world datasets, substantial improvements are made based on the DRL algo-rithm SAC [18]. Our method is demonstrated to be effective on the DADA-2000 dataset [11], and can be easily extended to the DAD dataset [3] without fixation annotations.
The proposed approach differs from existing works [3, 40, 6, 2] that are formulated within the supervised learn-ing (SL) framework. The proposed DRL-based solution is fundamentally superior to SL in that the DRL could uti-lize immediate observations to achieve a long-term goal, i.e., making early decision for anticipating future accidents.
Moreover, according to [27], our method is introspectively explainable as compared to [3, 6], which simply provide rationalizations (post-hoc explanation), since we explicitly formulate drivers’ visual attention during model learning.
Our experimental results also validate that the learned vi-sual attention serves as the causality of the outcome from the agent. The main contributions are threefold:
• The DRIVE model is proposed for traffic accident an-ticipation from dashcam videos based on deep rein-forcement learning (DRL).
• The DRIVE model is visually explainable by explicitly simulating the human visual attention within a unified
DRL framework.
• The proposed dense anticipation reward and sparse fix-ation reward are effective in training the model by our improved DRL training algorithm. 2.