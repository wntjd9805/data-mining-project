Abstract
Deep neural networks achieve great success in many vi-sual recognition tasks. However, the model deployment is usually subject to some computational resources. Model pruning under computational budget has attracted growing attention.
In this paper, we focus on the discrimination-aware compression of Convolutional Neural Networks (CNNs). In prior arts, directly searching the optimal sub-network is an integer programming problem, which is non-smooth, non-convex, and NP-hard. Meanwhile, the heuris-tic pruning criterion lacks clear interpretability and doesn’t generalize well in applications. To address this problem, we formulate sub-networks as samples from a multivari-ate Bernoulli distribution and resort to the approximation of continuous problem. We propose a new ﬂexible search scheme via alternating exploration and estimation. In the exploration step, we employ stochastic gradient Hamilto-nian Monte Carlo with budget-awareness to generate sub-networks, which allows large search space with efﬁcient computation.
In the estimation step, we deduce the sub-network sampler to a near-optimal point, to promote the generation of high-quality sub-networks. Unifying the ex-ploration and estimation, our approach avoids early falling into local minimum via a fast gradient-based search in a larger space. Extensive experiments on CIFAR-10 and Im-ageNet show that our method achieves state-of-the-art per-formances on pruning several popular CNNs. 1.

Introduction
With the advance of high-performance GPUs, Convolu-tional Neural Networks (CNNs) achieve great success in computer vision tasks [20, 38, 39]. Via exploiting deeper structure and over-parameterization, modern CNNs have strong generalization abilities. However, the complexity of CNNs is also growing, both in the computational cost and the size of model parameters. These requirements re-∗ Equal contributions. This work was partially supported by NSF IIS 1845666, 1852606, 1838627, 1837956, 1956002, IIA 2040588. strict the deployment of modern CNNs to resource-sensitive platforms, such as mobile devices and low-end facilities.
Another severe issue also arises for mobile devices, as the computational burden typically leads to high energy costs, which depletes the battery quickly. To address these prob-lems, pruning weights or structures [13, 23] from computa-tional heavy models is an effective solution.
Among existing methods, structural pruning, particu-larly channel pruning, is an efﬁcient scheme since post-processing is not necessary. Although various effective methods are proposed to target this problem, these meth-ods still suffer from several problems. In some works, the pruned networks are obtained via solving the relaxation of the integer optimization. However, bias may be introduced into the reduced pruning criterion due to the small but in-eluctable impacts of certain channels.
Importance sam-pling [27, 30] attempts to estimate the global or relative im-portance of neurons, but the performance is heavily affected by the priors. Recently, discrimination power [46, 10] is used as an effective criterion for channel pruning. Never-theless, the intra-neuron relation is delicate and difﬁcult to deal with.
To address these problems, we propose a new continuous formulation for the pruning problem of pretrained deep con-volutional networks, which can be solved via alternating ex-ploration and estimation. We characterize the sub-network space with an interpretable probabilistic model, that shares the same optimum with the naive formulation and avoids the ambiguity of non-integer values and the potential bias in naive relaxation methods. Instead of directly searching the optimal sub-networks, we compute the parameter to gen-erate high-quality sub-networks via alternating exploration and estimation. More speciﬁcally, in the exploration step, we sample sub-networks from the temporary distribution by virtue of stochastic gradient Hamiltonian Monte Carlo (SGHMC), which can explore more sub-networks and avoid falling into sub-optimal areas encoded by the temporary dis-tribution. Of note, the exploration is gradient-based and computationally efﬁcient, particularly for high-dimension parameter space. In the estimation step, we compute the op-timal distribution that generates high-quality sub-networks,
which is then used as a warm-start for the next exploration, to guide the search focusing on the informative areas. Dif-ferent from importance sampling, our prior is learned from the data, which has better generalization ability. To accel-erate the computation, we further construct a proximal ob-jective in the estimation, which implicitly considers the re-lation between neurons.
Our main contributions are summarized as follows:
• We propose a probabilistic formulation of the model compression problem. Our new formulation is inter-pretable and avoids potential sub-optimal or biased eval-uation of pruned networks.
• We propose a method to solve the probabilistic model via alternating exploration and estimation. We also design a correction term to ensure the sampled sub-networks obeying the FLOPs budgets.
• The proposed algorithm can search the parameter space effectively. Guided by the estimation step, the search fo-cuses on high-quality sub-networks. Via the exploration step, the search is more likely to jump out the local min-imum, beneﬁting from both the larger search space of sampling methods and the computational efﬁciency of gradient methods.
• Extensive experimental results demonstrate our approach achieves state-of-the-art performances in pruning VGG and ResNet on CIFAR10 and ImageNet.
Notations: We use the bold capital and bold lowercase symbols to represent matrices and vectors, respectively. In denotes a n × n-identity matrix. 1n is a n-dimension one vector, and 0n is a n-dimension zero vector. The weights of layer l are represented by Fl ∈ Rcl×wl×hl , where cl is the number of channels, wl and hl are height and width of the feature map. E(·) represents the expectation. 2.