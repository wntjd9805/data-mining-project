Abstract
In this paper, we propose a balancing training method to address problems in imbalanced data learning. To this end, we derive a new loss used in the balancing training phase that alleviates the influence of samples that cause an overfitted decision boundary. The proposed loss efficiently improves the performance of any type of imbalance learn-ing methods. In experiments on multiple benchmark data sets, we demonstrate the validity of our method and re-veal that the proposed loss outperforms the state-of-the-art cost-sensitive loss methods. Furthermore, since our loss is not restricted to a specific task, model, or training method, it can be easily used in combination with other recent re-sampling, meta-learning, and cost-sensitive learning meth-ods for class-imbalance problems. Our code is made avail-able at https://github.com/pseulki/IB-Loss. 1.

Introduction
Despite the remarkable success of deep neural networks (DNNs) these days, many areas of computer vision suffer from highly imbalanced datasets. Many real-world data ex-hibit skewed distributions [23, 16, 11, 24, 10], in which the number of samples per class differs greatly. This imbalance between classes can be problematic, since the model trained on such imbalanced data tends to overfit the dominant (ma-jority) classes [18, 14, 4]. That is, while the overall perfor-mance appears to be satisfactory, the model performs poorly on minority classes. To overcome the class imbalance prob-lem, extensive research has recently been conducted to im-prove the generalization performance by reducing the over-whelming influence of the dominant class on the model.
The research on imbalanced learning can be divided into three approaches: data-level approach, cost-sensitive re-weighting approach, and meta-learning approach. The data-level approach aims to directly balance the training data distributions via re-sampling (i.e., under-sampling or over-sampling) [6, 32] or by generating synthetic samples
[28]. Meanwhile, the cost-sensitive re-weighting approach aims to design new loss functions to re-weight samples by considering their importance [33, 17, 22]. Finally, the meta-learning approach enhances the performance of the data-level and/or cost-sensitive re-weighting approach via meta-learning [31, 25, 30]. Most recent data-level approaches require a heavy computational burden. Moreover, under-sampling can lose some valuable information, and over-sampling or data generation can cause overfitting on certain repetitive samples. The meta-learning approach requires ad-ditional unbiased data [31] or a meta-sampler [30], which is computationally expensive in practice. Therefore, our work focuses on the cost-sensitive re-weighting approach to de-sign a new loss function that is simple but efficient.
The cost-sensitive re-weighting approach aims to assign class penalties to shift the decision boundary in a way that reduces the bias induced by the data imbalance. For this pur-pose, the most commonly adopted method is to re-weight samples inversely to the number of training samples in each class to assign more weights for the minority classes
[17, 33, 8]. These methods have focused on only global-level class distribution and assign the same fixed weight to all samples belonging to the same class. However, not all samples in a dataset play an equal role in determining the model parameters [7]. That is, some samples have greater influences on forming a decision boundary. Hence, each sample needs to be re-weighted differently according to its impact on the model.
Recently, numerous studies have been conducted in which each sample is considered to design sample-wise loss functions
[9, 22, 27]. Specifically, these methods down-weight well-classified samples and assign more weights to hard examples, which yield high errors. This re-weighting might lead to the complete training when the high capacity of DNNs is sufficient to finally memorize the whole train-ing data [34, 3]. This implies that DNN is overfitted to hard samples, which are located at the overlapping region be-tween the majority and minority classes. In the imbalanced data, most hard samples are majority samples that enforce the decision boundary to be complex and shift to the minor-ity region.
To address the aforementioned problem, in this paper, we propose a loss-sensitive method to down-weight sam-ples that cause overfitting of a DNN trained with highly im-balanced data. To this end, we derive a formula that mea-sures how much each sample influences the complex and biased decision boundary. To derive the formula, we uti-lize the influence function [7], which has been widely used in robust statistics. Using the derived formula, we design a novel loss function, called influence-balanced (IB) loss, that adaptively assigns different weights to samples accord-ing to their influence on a decision boundary. Specifically, we re-weight the loss proportionally to the inverse of the influence of each sample. Our method is divided into two phases: standard training and fine-tuning for influence bal-ancing. During the fine-tuning phase, the proposed IB loss alleviates the influence of the samples that cause overfitting of the decision boundary.
Through extensive experiments on multiple benchmark data sets, we demonstrate the validity of our method, and show that the proposed method outperforms the state-of-the-art cost-sensitive re-weighting methods. Furthermore, since our IB loss is not restricted to a specific task, model, or training method, it can be easily utilized in combination with other recent data-level algorithms and hybrid methods for class-imbalance problems.
The main contributions of this paper are as follows:
• We discover that the existing loss-based loss methods can lead a decision boundary of DNNs to eventually overfit to the majority classes.
• We design a novel influence-balanced loss function to re-weight samples more effectively in such a way that the overfitting of the decision boundary can be allevi-ated.
• We demonstrate that simply substituting our proposed loss for the standard cross-entropy loss significantly improves the generalization performance on highly imbalanced data. 2.