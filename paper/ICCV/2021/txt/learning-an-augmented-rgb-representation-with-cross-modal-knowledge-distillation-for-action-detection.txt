Abstract
In video understanding, most cross-modal knowledge distillation (KD) methods are tailored for classification focusing on the discriminative representation of tasks, the trimmed videos. However, action detection requires not only categorizing actions, but also localizing them in untrimmed videos. Therefore, transferring knowledge per-taining to temporal relations is critical for this task which is missing in the previous cross-modal KD frameworks. To this end, we aim at learning an augmented RGB representation for action detection, taking advantage of additional modal-ities at training time through KD. We propose a KD frame-work consisting of two levels of distillation. On one hand, atomic-level distillation encourages the RGB student to learn the sub-representation of the actions from the teacher in a contrastive manner. On the other hand, sequence-level distillation encourages the student to learn the temporal knowledge from the teacher, which consists of transferring the Global Contextual Relations and the Action Boundary
Saliency. The result is an Augmented-RGB stream that can achieve competitive performance as the two-stream network while using only RGB at inference time. Extensive experi-mental analysis shows that our proposed distillation frame-work is generic and outperforms other popular cross-modal distillation methods in action detection task. 1.

Introduction
Learning representation from untrimmed videos for ac-tion detection is a challenging vision task. Action detec-tion aims at categorizing all the frames corresponding to every action occurring in an untrimmed video. The two main challenges for action detection are to tackle compos-ite action patterns and fine-grained details [47]. These chal-lenges are especially difficult in cases of real-world scenar-ios, where actions are densely distributed and overlapping each other [63]. To address these challenges, a typical set-ting, called two-stream network [49], consists in combining
RGB with additional modalities like optical flow [41, 62],
Figure 1. Proposed cross-modal distillation framework for action detection. Our distillation framework is composed of three loss terms corresponding to different types of knowledge to transfer across modalities. LAtomic: Atomic KD loss; LGlobal: Global
Contextual Relation loss; LBoundary: Boundary Saliency loss. 3D poses [68, 11] to take into account the complementary nature of each modality. However, using such setting is contingent upon the availability of multiple modalities and of expensive processing resources. The cost of computing additional modalities could be prohibitive, especially for long untrimmed videos. These constraints limit the usage of multi-modal action detection methods for real-world ap-plications.
Previous studies [20, 21] have shown that cross-modal
Knowledge Distillation (KD) is an effective mechanism to avoid the computation of the additional modalities during test time, while preserving the complementary informa-tion from the additional modalities. However, most previ-ous works [6, 18, 17] in the video understanding domain have investigated solely the classification of short trimmed videos [28, 50, 30].
In these works, each video corre-sponds to a single action and the distillation framework in-fuses the aggregated knowledge of an action instance from one modality into another. In contrast to trimmed videos, untrimmed videos contain rich sequential knowledge with complex temporal relations. Untrimmed videos in real-world scenarios tend to have cluttered background and mul-tiple correlated actions either in sequence [36] or in par-allel [63, 48]. Therefore, distillation mechanisms tailored for classification tasks and extended for detection tasks lack in capturing fine-grained details along the temporal dimen-sion. Now the question remains, what should be the right strategy to distillate cross-modal knowledge for action de-tection in untrimmed videos?
In this work, we propose a distillation framework to combine cross-modal information for detecting actions with high precision and minimal resource. The goal is to reach the two-stream performance while using only the RGB stream at inference time. The proposed distillation frame-work consists of a traditional teacher-student network ar-chitecture which operates in a Seq2Seq fashion [42, 10], thanks to three new distillation losses dedicated to the ac-tion detection task as illustrated in Fig. 1. The first loss in our formulation is the Atomic KD loss, which enables the RGB student network to mimic the feature representa-tion of every individual snippet from the teacher network in a contrastive manner. This loss-term extends the cross-modal KD mechanism fabricated for the classification tasks to the temporal domain [38], by transferring the knowl-edge only between one-to-one corresponding snippets of different modalities. As a snippet is often shorter than the action instance in an untrimmed video, so this loss en-courages a transfer of sub-representation [19] of the ac-tion, for example, ”raising arm” in the ”drinking” action.
Here, such sub-representation w.r.t. the entire video corre-sponds to an atomic piece of knowledge within the com-plete action feature distribution. However, the untrimmed video is composed of a sequence of snippets, distilling only the atomic representation is not sufficient for learning dis-criminative action representation. Thus, distillation mech-anisms dedicated to represent specifically an action within an untrimmed video are required.
We therefore introduce two loss-terms for sequence-level
KD so as to transfer the cross-snippet relations between dif-ferent modalities. Firstly, we propose a Global Contextual
Relation loss to transfer the contextual information of the sequence between modalities. In our work, contextual in-formation is defined as the embedding of the correlation be-tween all the snippet features. Thanks to this loss term, ev-ery student snippet feature can learn in the latent space from all the correlated teacher snippets within the untrimmed videos (Fig. 1). With this loss-term, detecting one action in a snippet can benefit from the information in the correlated snippets (corresponding to related actions, e.g. take and eat sandwich) across modalities, resulting in better action de-tection performance. Secondly, we propose another KD loss to distillate the boundary saliency from the teacher to RGB student network, dubbed Boundary Saliency loss. This en-sures a more precise action boundary detection of the RGB student which is prone to imprecise action boundary detec-tion due to weak temporal signals. In an untrimmed video, the start and end moments of the action are more salient than other parts (see Fig. 1). Intuitively, the feature vari-ation across consecutive snippets in the video can reflect such saliency of the action boundaries. Therefore, learn-ing this variation from a modality that can better capture the movement (e.g. optical flow, 3D poses) encourages the
RGB stream representation to be more sensitive to the ac-tion boundaries.
Contributions. To summarize, we take a step towards the little-explored but crucial, cross-modal KD for action detection. We build a Seq2Seq KD framework for action detection with a novel formulation. This formulation con-sists of an atomic-level KD loss and two sequence-level
KD losses. The three loss terms in our formulation are jointly optimized in an end-to-end fashion. To the best of our knowledge, we are the first to propose a formu-lation containing sequential KD loss for the action detec-tion task. We perform comprehensive experiments on five benchmarks. Our joint formulation significantly improves the vanilla RGB baseline (up to +6.8% improvement on
MultiTHUMOS w.r.t. vanilla-RGB) and achieves the Two-stream performance while using only RGB at inference time. The consistent improvement on all these datasets cor-roborates the effectiveness and robustness of our distillation framework. 2.