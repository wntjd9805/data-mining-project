Abstract
From an early age, humans perceive the visual world as composed of coherent objects with distinctive properties such as shape, size, and color. There is great interest in build-ing models that are able to learn similar structure, ideally in an unsupervised manner. Learning such structure from com-plex 3D scenes that include clutter, occlusions, interactions, and camera motion is still an open challenge. We present a model that is able to segment visual scenes from complex 3D environments into distinct objects, learn disentangled representations of individual objects, and form consistent and coherent predictions of future frames, in a fully un-supervised manner. Our model (named PARTS) builds on recent approaches that utilize iterative amortized inference and transition dynamics for deep generative models. We achieve dramatic improvements in performance by introduc-ing several novel contributions. We introduce a recurrent slot-attention like encoder which allows for top-down inﬂu-ence during inference. We argue that when inferring scene structure from image sequences it is better to use a ﬁxed prior which is shared across the sequence rather than an auto-regressive prior as often used in prior work. We demon-strate our model’s success on three different video datasets (the popular benchmark CLEVRER; a simulated 3D Play-room environment; and a real-world Robotics Arm dataset).
Finally, we analyze the contributions of the various model components and the representations learned by the model. 1.

Introduction
Object-level perception plays a key role in human vision
[13]. People interpret scenes as consisting of discrete, inde-pendent and coherent objects and a broad set of experiments has shown this ability is developed very early in life [16].
While most present computer vision models lack any explicit object-level structure [7], there is an ongoing effort to create models that would, at least in some sense, represent scenes
Figure 1. Unsupervised segmentation outputs from PARTS (above).
The model is able to extract meaningful segments of complex, cluttered and partially observable scenes. It is also able to predict future frames along with their corresponding segmentation (below). in terms of their constituent objects [1, 5, 2, 10].
There are good reasons why this is worth pursuing.
Object-level representations are inherently combinatorial, allowing complex scenes to be explained by different com-binations of simpler elements. Such representations may also allow better generalization by learning general rules related to what objects are, how they move and how they interact. Finally, objects may help in building models that allow planning, take into account actions and their effect on the environment and produce future predictions that are useful and actionable for agents.
Most current models that push in this direction introduce a structural inductive bias in the form of slots. These are
a set of latent variables (deterministic or stochastic) which are inferred from an observation and, if things work well, should each capture a different object or entity in the scene.
One way to encourage this to happen is through architectural biases such as making the slots “compete” in explaining different parts of the scene (using a softmax, for example), processing each slot independently when decoding or the use of attention which may help in choosing which parts of the scene go to each slot. Probabilistic tools like different auxiliary losses [22] and choice of priors [3] are another way to encourage the emergence of objects in the representations.
When working with image sequences the problem, in some sense, becomes easier. When objects move in the world they usually do it in a coherent, consistent and predictable manner. This makes the problem of inferring which parts of the visual input belong to an object and which objects are in a scene easier. This can also help in avoiding useful, but wrong, heuristics like extracting objects purely by color. Ego-motion, especially if accompanied by action information can also reveal a lot about the structure of a 3D scene – for instance, motion parallax ensures that objects which lie farther from an observer will move differently than nearby objects when the motion is parallel to the camera plane.
While progress has been made in recent years along this front, extracting meaningful objects from complex 3D envi-ronments is still a challenge. We introduce Predict, Attend,
Reﬁne with Transformers and Slots (PARTS) – a model which is able to, unsupervised, extract object-level segmen-tations and representations of complex 3D environments and make predictions about future observations. We achieve this by extending and improving recently proposed models
[5, 18]. We introduce a recurrent slot-attention mechanism which is able to incorporate top-down information and assist in the inference process. We show that the use of transform-ers and other architectural changes can help performance.
Finally, we show how treating frames as independent in the generative process (but not in the inference process) can help resulting representations, especially when coupled with expressive pair-wise prediction models such as transformers.
We analyse the contribution of each part of the model and study the resulting learned representations. 2.