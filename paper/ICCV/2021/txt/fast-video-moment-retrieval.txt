Abstract
This paper targets at fast video moment retrieval (fast
VMR), aiming to localize the target moment efﬁciently and accurately as queried by a given natural language sentence.
We argue that most existing VMR approaches can be divid-ed into three modules namely video encoder, text encoder, and cross-modal interaction module, where the last module is the test-time computational bottleneck. To tackle this is-sue, we replace the cross-modal interaction module with a cross-modal common space, in which moment-query align-ment is learned and efﬁcient moment search can be per-formed. For the sake of robustness in the learned space, we propose a ﬁne-grained semantic distillation framework to transfer knowledge from additional semantic structures.
Speciﬁcally, we build a semantic role tree that decomposes a query sentence into different phrases (subtrees). A hierar-chical semantic-guided attention module is designed to per-form message propagation across the whole tree and yield discriminative features. Finally, the important and discrim-inative semantics are transferred to the common space by a matching-score distillation process. Extensive experimental results on three popular VMR benchmarks demonstrate that our proposed method enjoys the merits of high speed and signiﬁcant performance. 1.

Introduction
Video Moment Retrieval (VMR) aims to localize a tem-poral segment from an untrimmed video, as queried by a natural language sentence [14, 1]. It plays a crucial role in video understanding and has various downstream applica-tions such as robotic navigation, autonomous driving, video entertainment, and so forth. Despite great successes in re-cent years [63, 57, 65, 40, 62, 64, 38], effective VMR re-mains challenging due to many factors including complex video scenes, ﬁne-grained semantic query structures, and huge cross-modal gap between visual and textual features. (cid:28573) (cid:28569) (cid:28572) (cid:28564) (cid:28653) (cid:28631) (cid:28629) (cid:28646) (cid:28649) (cid:28631) (cid:28631) (cid:28597) 50 45 40 35 30 25 20
TACos: R1@0.3
ANetCap: R1@0.5 0               5               10             20              25                50             200           350           500  750         2100 (cid:28616)(cid:28633)(cid:28647)(cid:28648)(cid:28564)(cid:28616)(cid:28637)(cid:28641)(cid:28633)(cid:28564)(cid:28572)(cid:28641)(cid:28647)(cid:28573) (cid:28609)(cid:28633)(cid:28648)(cid:28636)(cid:28643)(cid:28632)(cid:28647) (cid:28582)(cid:28600)(cid:28577)(cid:28616)(cid:28597)(cid:28610)(cid:28572)(cid:28597)(cid:28597)(cid:28597)(cid:28605)(cid:28673)(cid:28582)(cid:28580)(cid:28573) (cid:28600)(cid:28614)(cid:28610)(cid:28572)(cid:28599)(cid:28618)(cid:28612)(cid:28614)(cid:28673)(cid:28582)(cid:28580)(cid:28573) (cid:28608)(cid:28603)(cid:28605)(cid:28572)(cid:28599)(cid:28618)(cid:28612)(cid:28614)(cid:28673)(cid:28582)(cid:28580)(cid:28573) (cid:28618)(cid:28615)(cid:28608)(cid:28610)(cid:28633)(cid:28648)(cid:28572)(cid:28597)(cid:28599)(cid:28608)(cid:28673)(cid:28582)(cid:28580)(cid:28573) (cid:28599)(cid:28598)(cid:28612)(cid:28572)(cid:28597)(cid:28597)(cid:28597)(cid:28605)(cid:28673)(cid:28582)(cid:28580)(cid:28573) (cid:28616)(cid:28609)(cid:28608)(cid:28603)(cid:28597)(cid:28572)(cid:28619)(cid:28597)(cid:28599)(cid:28618)(cid:28673)(cid:28582)(cid:28580)(cid:28573) (cid:28615)(cid:28599)(cid:28600)(cid:28609)(cid:28572)(cid:28610)(cid:28633)(cid:28649)(cid:28646)(cid:28605)(cid:28612)(cid:28615)(cid:28673)(cid:28581)(cid:28589)(cid:28573) (cid:28599)(cid:28616)(cid:28614)(cid:28608)(cid:28572)(cid:28605)(cid:28599)(cid:28599)(cid:28618)(cid:28673)(cid:28581)(cid:28587)(cid:28573) (cid:28602)(cid:28618)(cid:28609)(cid:28614)(cid:28572)(cid:28611)(cid:28649)(cid:28646)(cid:28647)(cid:28573) (cid:28600)(cid:28629)(cid:28648)(cid:28629)(cid:28647)(cid:28633)(cid:28648)(cid:28647) (cid:28597)(cid:28610)(cid:28633)(cid:28648)(cid:28599)(cid:28629)(cid:28644) (cid:28616)(cid:28597)(cid:28599)(cid:28643)(cid:28647)
Figure 1. Test time and accuracy plot of state-of-the-art VMR approaches on TACos and ActivityNet Captions (ANetCap). We report the metrics R1@0.3 and R1@0.5 for the two datasets re-spectively. Our proposed FVMR achieves the best accuracy-speed balance among all the competitors. Best viewed in color.
To tackle the above challenges, the current state-of-the-art VMR pipeline can be divided into three modules name-ly video encoder, text encoder, and cross-modal interaction module. The ﬁrst two encoders utilize convolutional neural networks (e.g. C3D [49] or I3D [4]) and recurrent neural networks (e.g. BiLSTM [27] or GRU [12]) to extract visual and textual features respectively. Then, to predict the target moment, the cross-modal interaction module is designed to jointly consider both modalities by using different architec-tures such as cross attention [36, 35, 7, 64], graph neural networks [63, 67, 42], and temporal adjacent networks [65].
Despite the above achievements, we emphasize, fast video moment retrieval (fast VMR) is in fact often neces-sary, since localizing the target moment is usually employed only as a part of time-critical video retrieval systems. For example, to localize moments in a video corpus [46, 30], it usually requires us to perform efﬁcient VMR on hundreds to thousands of candidate videos for a given natural lan-guage query. Also, fast video moment retrieval on embed-ded devices may enable many additional applications, such as intelligent robot service and smart home [68]. However, as shown in Figure 1, high-speed and effective VMR al-gorithms remain scarce. Taking the recently state-of-the-art
approach 2D-TAN [65] as an example, it will cost about 100 milliseconds when performing VMR on one single video.
We argue that among the three modules in the VMR pipeline, cross-modal interaction is the test-time computa-tional bottleneck. The reasons are three-fold: (1) Before model testing, it is convenient to pre-extract and store video features in an ofﬂine manner. As a result, the video encoder does not affect the test time. (2) The text encoder is highly efﬁcient (∼3 ms per sentence) and irreplaceable for all the
VMR methods. Therefore, targeting at a low cost of text encoder is unadvisable. (3) Cross-modal interaction takes up most of test time due to the complex feature fusion oper-ation [54, 61, 34, 62, 63, 40, 42, 8] and subsequent feature transformations [65, 50, 23]. The above observations moti-vate us to design an efﬁcient and effective cross-modal in-teraction module. Ideally, this module can be simpliﬁed to a cross-modal common space, where moment-query align-ment is learned. In such a space, for the given query, it is nearly cost-free to obtain the matching score of each mo-ment proposal, i.e., we can simply calculate the similarities between video moment features and the query sentence fea-ture in the common space by using efﬁcient vector opera-tions like dot product. A few early work [1, 26] has ex-plored common space learning for VMR. However, their performance is far below the current state-of-the-arts.
In fact, without a well-designed cross-modal interaction, it is difﬁcult to ground a textual query onto the video effectively.
Therefore, the common space learning strategy has become an underdog in VMR, which motivates us to solve the fol-lowing problem: How to learn a common space that can not only yield efﬁcient moment/query features for fast VM-R but also improve the discriminative ability by leveraging
ﬁne-grained semantic structures?
To this end, we propose a ﬁne-grained semantic distil-lation framework for fast video moment retrieval, which learns an efﬁcient and effective moment-query common s-pace by transferring knowledge from additional semantic structures. Speciﬁcally, our proposed approach consists of four modules, namely video encoder, text encoder, ﬁne-grained semantic extractor, and common space. Here, in addition to the text encoder (a Bi-LSTM in our proposed approach), we introduce a ﬁne-grained semantic extractor to facilitate the learning of common space. This extrac-tor decomposes a query sentence into ﬁne-grained semantic structures (phrases) by building a semantic role tree, where each phrase is represented as a subtree. Then, a hierarchical semantic-guided attention module is designed to propagate semantic information across the whole tree and yield dis-criminative features for each phrase. Note that the learned
ﬁne-grained phrase features serve as complementary cues to provide an enhanced supervisory signal when learning the common space. During model training, the video and text encoders are required to learn from the ﬁne-grained se-mantic extractor by matching score distillation. As a re-sult, ﬁne-grained semantic information is injected into the common space for robust moment-query alignment. During testing, we only exploit the text and video encoders to per-form VMR, which does not adds computational overhead.
As shown in Figure 1, our proposed method achieves the best accuracy-speed balance among state-of-the-arts.
The main contributions of this paper are three-fold:
• We introduce fast video moment retrieval (FVMR) that aims for retrieving target moments efﬁciently and ac-curately. To this end, a simple yet effective common space learning paradigm is designed, not only speeding up VMR, but also improving the performance.
• We design a novel ﬁne-grained semantic distillation framework for FVMR. Here, a hierarchical semantic-guided attention module is designed to leverage the ﬁne-grained semantic structures by optimizing a matching-score distillation loss.
• Extensive experimental results on three popular VM-R benchmarks demonstrate that our proposed method enjoys the merits of high speed and signiﬁcant perfor-mance. Compared with the recent state-of-the-arts, 2D-TAN [65], our proposed model is 40× faster and obtains 5.5% absolute gains on the TACos dataset [44]. 2.