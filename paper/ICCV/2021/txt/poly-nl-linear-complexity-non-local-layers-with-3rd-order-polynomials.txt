Abstract
Spatial self-attention layers, in the form of Non-Local blocks, introduce long-range dependencies in Convolu-tional Neural Networks by computing pairwise similarities among all possible positions. Such pairwise functions un-derpin the effectiveness of non-local layers, but also deter-mine a complexity that scales quadratically with respect to the input size both in space and time. This is a severely limiting factor that practically hinders the applicability of non-local blocks to even moderately sized inputs. Previ-ous works focused on reducing the complexity by modifying the underlying matrix operations, however in this work we aim to retain full expressiveness of non-local layers while keeping complexity linear. We overcome the efficiency limi-tation of non-local blocks by framing them as special cases of 3rd order polynomial functions. This fact enables us to formulate novel fast Non-Local blocks, capable of reducing the complexity from quadratic to linear with no loss in per-formance, by replacing any direct computation of pairwise similarities with element-wise multiplications. The pro-posed method, which we dub as “Poly-NL”, is competitive with state-of-the-art performance across image recognition, instance segmentation, and face detection tasks, while hav-ing considerably less computational overhead. 1.

Introduction
Convolutional Neural Networks (CNNs) have led to a revolution in machine learning, and, specifically, are cur-rently the undisputed state of the art in computer vision on various tasks. Nonetheless, CNNs, even if composed by a deep stack of convolutional operators, have a limited recep-tive field [34], which makes the crucial long-range depen-dencies hard to capture.
Recent work on spatial self-attention ameliorated this is-sue with a novel set of modules for neural networks [52, 47].
These blocks extract non-local interactions among all spa-tial positions of the input and weight them with a set of learnable parameters. Passing through a Non-local block, each input position takes into account the contribution of all the others, scaled by their similarity with a given refer-ence. These blocks introduce the possibility to reason about the whole space in one glance and make non-local behavior easier to be captured by the network. Inserting Non-local blocks in neural architectures has been proven very effec-tive [2, 15, 51, 39, 40, 38], but the computation of a similar-ity score for each pair of points scales quadratically with the number of spatial positions. As such, the expensive com-putational and storage complexity makes non-local blocks impractical to compute even upon moderately sized input.
Recent works tackle such limitation via an efficient com-putation of the similarity matrix [59, 33, 44] but miss to pro-vide a theoretical overview of the Non-local block formula-tion. In this work, we build upon the aforementioned line of research, and revisit Non-local layers under the lens of poly-nomials, framing them as special cases of 3rd order poly-nomials. Powered by this intuition, we derive an efficient version of Non-local neural networks, Poly-NL which takes into account long-range dependencies without the need to compute explicitly any pairwise similarity. Poly-NL layers perform computations using the same set of interactions as the Non-local block of [52], and at the same time reduce the overall complexity drastically from O(N 2) to O(N ) with no loss in performance.
In this work, we link polynomials and the Non-Local layer. Our goal is to efficiently extract high-order interac-tions from the input while capturing long-range spatial de-pendencies. Thus, our contribution can be summarized as follows:
• We bridge the formulations between high-order poly-nomials and non-local attention.
In particular, we prove that self-attention (in the form of Non-local blocks) can be seen as a particular case of general 3rd order polynomials. 1
• We propose ”Poly-NL” a novel building block for neu-ral networks, which can be seen as polynomials of the input matrix. In particular, we propose an alternative
Non-local block that reduce complexity from quadratic to linear with respect to the spatial dimensions.
• We showcase the efficiency and the effectiveness of our method across a range of tasks: image recognition, instance segmentation, and face detection. 2.