Abstract
We present a multiview pseudo-labeling approach to video learning, a novel framework that uses complementary views in the form of appearance and motion information for semi-supervised learning in video. The complementary views help obtain more reliable “pseudo-labels” on unla-beled video, to learn stronger video representations than from purely supervised data. Though our method capital-izes on multiple views, it nonetheless trains a model that is shared across appearance and motion input and thus, by design, incurs no additional computation overhead at in-ference time. On multiple video recognition datasets, our method substantially outperforms its supervised counterpart, and compares favorably to previous work on standard bench-marks in self-supervised video representation learning. 1.

Introduction 3D convolutional neural networks (CNNs) [55, 8, 56, 17] have shown steady progress for video recognition, and particularly human action classiﬁcation, over recent years.
This progress also came with a shift from traditionally small-scale datasets to large amounts of labeled data [31, 6, 7] to learn strong spatiotemporal feature representations. Notably, as 3D CNNs are data hungry, their performance has never been able to reach the level of hand-crafted features [57] when trained ‘from-scratch’ on smaller scale datasets [49].
However, collecting a large-scale annotated video dataset [21, 7] for the task at hand is expensive and tedious as it often involves designing and implementing annotation platforms at scale and hiring crowd workers to collect an-notations. For example, a previous study [48] suggests it takes at least one dollar to annotate a single video with 157 human activities. Furthermore, the resource intensive annota-tion process needs to be repeated for each task of interest or when the label space needs to be expanded. Finally, another dilemma that emerges with datasets collected from the web is that the underlying content is vanishing over time, and
Figure 1: Multiview pseudo-labeling (MvPL) takes in mul-tiple complementary views of a single unlabeled video clip, in the form of RGB (V ), optical-ﬂow (F ), and temporal gradients ( ∂V
∂t ) and uses a shared model to perform semi-supervised learning. After training, a single RGB view is used for inference. therefore the datasets need to be replenished in a recurring fashion [50].
The goal of this work is semi-supervised learning in video to learn from both labeled and unlabeled data, thereby re-ducing the amount of annotated data required for training.
Scaling video models with unlabeled data is a setting of high practical interest, since collecting large amounts of unlabeled video data requires minimal human effort. Still, thus far this area has received far less attention than fully supervised learning from video.
Most prior advances in semi-supervised learning in com-puter vision focus on the problem of image recognition.
“Pseudo-labeling” [38, 64, 61, 51] is a popular approach to utilize unlabeled images. The idea is to use the predic-tions from a model as target labels and gradually add the
unlabeled images (with their inferred labels) to the training set. Compared to image recognition, semi-supervised video recognition presents its own challenges and opportunities.
On the one hand, the temporal dimension introduces some ambiguity, i.e. given a video clip with an activity label, the activity may occur at any temporal location. On the other hand, video can also provide a valuable, complementary sig-nal for recognition by the way objects move in space-time, e.g. the actions ‘sit-down’ vs. ‘stand-up’ cannot be discrimi-nated without using the temporal signal. More speciﬁcally, video adds information about how actors, objects, and the environment change over time.
Therefore, directly applying semi-supervised learning al-gorithms designed for images to video could be sub-optimal (we will verify this point in our experiments), as image-based algorithms only consider appearance information and ignore the potentially rich dynamic structure captured by video.
To address the challenge discussed above, we introduce multiview pseudo-labeling (MvPL), a novel framework for semi-supervised learning designed for video. Unlike tradi-tional 3D CNNs that implicitly learn spatiotemporal features from appearance, our main idea is to explicitly force a single model to learn appearance and motion features by ingesting multiple complementary views1 in the form of appearance, motion, and temporal gradients, so as to train the model from unlabeled data that augments labeled data.
We consider visual-only semi-supervised learning and all the views are computed from RGB frames. Therefore our method does not require any additional modalities nor does it require any change to the model architecture to accommo-date the additional views. Our proposed multiview pseudo-labeling is general and can serve as a drop-in replacement for any pseudo-labeling based algorithm [38, 64, 61, 51] that currently operates only on appearance, namely by aug-menting the model with multiple views and our ensembling approach to infer pseudo-labels.
Our method rests on two technical insights: 1) a single model that nonetheless beneﬁts from multiview data; and 2) an ensemble approach to infer pseudo-labels.
First, we convert both optical ﬂow and temporal differ-ence to the same input format as RGB frames so that all the views can share the same 3D CNN model. The 3D CNN model takes only one view at a time and treats optical ﬂow and temporal gradients as if they are RGB frames. The ad-vantage is that we directly encode appearance and motion in the input space and distribute the information through multiple views, to the beneﬁt of the 3D CNN.
Second, when predicting pseudo-labels for unlabeled data, we use an ensemble of all the views for prediction. We show that predicting pseudo-labels from all the views is more ef-fective than predicting from a single view alone. Our method 1We use the term view to refer to different input types (RGB frames, optical ﬂow, or RGB temporal gradients), as opposed to camera viewpoints. uses a single model that can seamlessly accommodate differ-ent views as input for video recognition. See Figure 1 for an overview of our approach.
In summary, this paper makes the following contributions:
• This work represents an exploration in semi-supervised learning for video understanding, an area that is heavily researched in image understanding [10, 22, 28, 51, 60].
Our evaluation establishes semi-supervised baselines on Kinetics-400 (1% and 10% label case), and UCF101 (similarly as the image domain which uses 1% and 10% of labels on ImageNet [10, 28, 51, 60]).
• Our technical contribution is a novel multiview pseudo-labeling framework for general application in semi-supervised learning from video, that delivers consistent improvement in accuracy on multiple pseudo-labeling algorithms.
• On several challenging video recognition benchmarks, our method substantially improves its single view coun-terpart. We obtain state-of-the-art performance on
UCF101 [52] and HMDB-51 [35] when using Kinetics-400 [31] as unlabeled data, and outperform video self-supervised methods in this setting. 2.