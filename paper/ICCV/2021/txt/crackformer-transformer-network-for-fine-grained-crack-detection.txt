Abstract
Cracks are irregular line structures that are of interest in many computer vision applications. Crack detection (e.g., from pavement images) is a challenging task due to inten-sity in-homogeneity, topology complexity, low contrast and noisy background. The overall crack detection accuracy can be significantly affected by the detection performance on fine-grained cracks. In this work, we propose a Crack
Transformer network (CrackFormer) for fine-grained crack detection. The CrackFormer is composed of novel atten-tion modules in a SegNet-like encoder-decoder architecture.
Specifically, it consists of novel self-attention modules with 1x1 convolutional kernels for efficient contextual informa-tion extraction across feature-channels, and efficient posi-tional embedding to capture large receptive field contextual information for long range interactions. It also introduces new scaling-attention modules to combine outputs from the corresponding encoder and decoder blocks to suppress non-semantic features and sharpen semantic ones. The Crack-Former is trained and evaluated on three classical crack datasets. The experimental results show that the Crack-Former achieves the Optimal Dataset Scale (ODS) values of 0.871, 0.877 and 0.881, respectively, on the three datasets and outperforms the state-of-the-art methods. 1.

Introduction
Pavement crack detection from images is a challenging issue due to intensity inhomogeneity, topology complexity, low contrast, and noisy texture background [18]. In addi-tion, crack’s diversity (thin, grid or thick crack etc.) makes it more difficult.
There are a large number of studies on crack detec-tion [6, 22, 2, 36, 37, 35, 10]. Recent studies have employed convolutional neural networks (CNNs) to boost detection accuracy to a higher level. In this study, we consider the problem of detecting thin cracks from the image of an as-phalt surface. In general, it is much easier to detect thick
*Corresponding author
Figure 1. Crack prediction from our CrackFormer model (Best viewed in color). The upper left is a classical crack image. The upper right is the predicted result. The bottom shows a profile slice with normalized grey scale, its ground truth and corresponding crack predicted probabilities. cracks than thin cracks. Thus, crack detection performance is largely affected by how well one method can detect thin cracks.
The state-of-the-art (SOTA) methods heavily rely on
Fully Convolutional Networks (FCNs) [9], such as Seg-Net [31], U-Net [27] and their variants [21]. SegNets and U-Nets use an encoder-decoder architecture, where the encoder extracts high-level semantic representations by using a cascade of convolution and pooling layers, and the decoder leverages memorized pooling indices or skip connections to re-use high-resolution feature maps from the encoder in order to recover lost spatial informa-tion from high-level representations. Despite their out-standing performance, these methods suffer from limita-tion in complex segmentation tasks, e.g. when dealing with thin cracks or when there exists low contrast between crack and background.
In general, these models rely on stacked 3 × 3 convolution and pooling operations, and could not achieve pixel-level segmentation precision in the convolution-pooling pipeline, resulting in blur and coarse crack segmentation. Moreover, suffering from the limited receptive field by using 3 × 3 convolutional kernels, these
methods tend to fail in detecting long cracks, resulting in discontinuous crack detection.
In this work, we propose a Crack Transformer net-work (CrackFormer) by combining novel self-attention and
It ex-scaling-attention mechanisms for crack detection. plores to leverage the merits of Transformer models [30] to capture long-range interactions and simultaneously adopt small convolution kernels for fine-grained attentive percep-tion. CrackFormer keeps the regular layout by using a
SegNet-like architecture, but introduces attention mecha-nisms in two different ways. Fig. 2 shows our network structure. The main contribution of this paper can be sum-marized as follows, 1. A new self-attention block (Self-AB) is proposed (Fig. 3). The Self-AB can fully extract contextual in-formation across feature-channels by leveraging the 1×1 convolution kernels, and capture large receptive field contextual information across spatial-domain by an efficient position embedding. 2. A new scaling-attention block (Scal-AB) is proposed (Fig. 4), where a set of scaling-attention masks are generated by nonlinearizing the encoder’s feature maps, and used to suppress non-semantic features and sharpen the semantic cracks. 3. We propose a Transformer encoder-decoder structure integrating the proposed Self-AB and Scal-AB blocks, where the Self-AB is embedded into different levels of the encoder and decoder modules, and the Scal-AB is introduced between the encoder feature maps and corresponding decoder ones.
A crack prediction result by our method is shown in
Fig. 1, where the original image is shown in the upper left, the predicted result shown in the upper right. In the lower row, we can observe from the profile that the cracks are pre-dicted precisely. 2.