Abstract
Recently, backdoor attacks pose a new security threat to the training process of deep neural networks (DNNs). At-tackers intend to inject hidden backdoors into DNNs, such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if hidden backdoors are activated by the attacker-deﬁned trigger. Ex-isting backdoor attacks usually adopt the setting that trig-gers are sample-agnostic, i.e., different poisoned samples contain the same trigger, resulting in that the attacks could be easily mitigated by current backdoor defenses. In this work, we explore a novel attack paradigm, where backdoor triggers are sample-speciﬁc. In our attack, we only need to modify certain training samples with invisible perturba-tion, while not need to manipulate other training compo-nents (e.g., training loss, and model structure) as required in many existing attacks. Speciﬁcally, inspired by the recent advance in DNN-based image steganography, we generate sample-speciﬁc invisible additive noises as backdoor trig-gers by encoding an attacker-speciﬁed string into benign images through an encoder-decoder network. The mapping from the string to the target label will be generated when
DNNs are trained on the poisoned dataset. Extensive ex-periments on benchmark datasets verify the effectiveness of our method in attacking models with or without defenses.
The code will be available at https://github.com/ yuezunli/ISSBA. 1.

Introduction
Deep neural networks (DNNs) have been widely and successfully adopted in many areas [11, 25, 49, 19]. Large amounts of training data and increasing computational power are the key factors to their success, but the lengthy and involved training procedure becomes the bottleneck for
† indicates corresponding authors.
Corresponds to wubaoyuan@cuhk.edu.cn and siweilyu@buffalo.edu.
Figure 1. The comparison of triggers in previous attacks (e.g.,
BadNets [8]) and in our attack. The triggers of previous attacks are sample-agnostic (i.e., different poisoned samples contain the same trigger), while those of our method are sample-speciﬁc. users and researchers. To reduce the overhead, third-party resources are usually utilized in training DNNs. For exam-ple, one can use third-party data (e.g., data from the Internet or third-party companies), train their model with third-party servers (e.g., Google Cloud), or even adopt third-party APIs directly. However, the opacity of the training process brings new security threats.
Backdoor attack1 is an emerging threat in the training process of DNNs.
It maliciously manipulates the predic-tion of the attacked DNN model by poisoning a portion of training samples. Speciﬁcally, backdoor attackers in-ject some attacker-speciﬁed patterns (dubbed backdoor trig-gers) in the poisoned image and replace the corresponding label with a pre-deﬁned target label. Accordingly, attack-ers can embed some hidden backdoors to the model trained with the poisoned training set. The attacked model will behave normally on benign samples, whereas its predic-1Backdoor attack is also commonly called ‘neural trojan’ or ‘trojan at-tack’ [26]. In this paper, we focus on the poisoning-based backdoor attack
[21] towards image classiﬁcation, although the backdoor threat could also happen in other scenarios [1, 46, 43, 20, 29, 36, 44].
tion will be changed to the target label when the trigger is present. Besides, the trigger could be invisible [3, 18, 34] and the attacker only needs to poison a small fraction of samples, making the attack very stealthy. Hence, the insidi-ous backdoor attack is a serious threat to the applications of
DNNs.
Fortunately, some backdoor defenses [7, 41, 45] were proposed, which show that existing backdoor attacks can be successfully mitigated. It raises an important question: has the threat of backdoor attacks really been resolved?
In this paper, we reveal that existing backdoor attacks were easily mitigated by current defenses mostly because their backdoor triggers are sample-agnostic, i.e., different poisoned samples contain the same trigger no matter what trigger pattern is adopted. Given the fact that the trigger is sample-agnostic, defenders can easily reconstruct or de-tect the backdoor trigger according to the same behaviors among different poisoned samples.
Based on this understanding, we explore a novel attack paradigm, where the backdoor trigger is sample-speciﬁc.
We only need to modify certain training samples with invis-ible perturbation, while not need to manipulate other train-ing components (e.g., training loss, and model structure) as required in many existing attacks [34, 27, 28]. Speciﬁcally, inspired by DNN-based image steganography [2, 51, 39], we generate sample-speciﬁc invisible additive noises as backdoor triggers by encoding an attacker-speciﬁed string into benign images through an encoder-decoder network.
The mapping from the string to the target label will be gen-erated when DNNs are trained on the poisoned dataset. The proposed attack paradigm breaks the fundamental assump-tion of current defense methods, therefore can easily bypass them.
The main contributions of this paper are as follows: (1)
We provide a comprehensive discussion about the success conditions of current main-stream backdoor defenses. We reveal that their success all relies on a prerequisite that (2) We explore a backdoor triggers are sample-agnostic. novel invisible attack paradigm, where the backdoor trig-ger is sample-speciﬁc and invisible. It can bypass existing (3) defenses for it breaks their fundamental assumption.
Extensive experiments are conducted, which verify the ef-fectiveness of the proposed method. 2.