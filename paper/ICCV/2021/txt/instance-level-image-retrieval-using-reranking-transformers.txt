Abstract
Instance-level image retrieval is the task of searching in a large database for images that match an object in a query image. To address this task, systems usually rely on a retrieval step that uses global image descriptors, and a subsequent step that performs domain-speciﬁc reﬁnements or reranking by leveraging operations such as geometric veriﬁcation based on local features. In this work, we pro-pose Reranking Transformers (RRTs) as a general model to incorporate both local and global features to rerank the matching images in a supervised fashion and thus replace the relatively expensive process of geometric veriﬁcation.
RRTs are lightweight and can be easily parallelized so that reranking a set of top matching results can be performed in a single forward-pass. We perform extensive experiments on the Revisited Oxford and Paris datasets, and the Google
Landmarks v2 dataset, showing that RRTs outperform pre-vious reranking approaches while using much fewer local descriptors. Moreover, we demonstrate that, unlike exist-ing approaches, RRTs can be optimized jointly with the fea-ture extractor, which can lead to feature representations tai-lored to downstream tasks and further accuracy improve-ments. The code and trained models are publicly available at https://github.com/uvavision/RerankingTransformer. 1.

Introduction
Instance recognition is a challenging task that aims to vi-sually recognize an object instance. This is distinct from category-level recognition that identiﬁes only the object class.
Instance recognition is important in e-commerce where it is often desired to ﬁnd a speciﬁc product in a large image collection, or in place identiﬁcation where the objec-tive is to infer the identity of a public landmark. As the number of instances is much larger than the number of ob-ject categories, instance recognition is typically cast as im-age retrieval instead of classiﬁcation, and usually involves both metric learning and local feature based reranking.
Over the last decade, instance recognition continues to be a major focus of research. Pioneering systems lever-Figure 1. Top performing instance recognition methods often rely on reranking the top results using a score such as the number of inlier correspondences from geometric veriﬁcation. We propose to replace this step with a Reranking Transformer (RRT) that can be learned with the underlying representations of the images. aged hand-crafted local descriptors and matching algo-rithms [52, 42]. More recent approaches incorporate both global and local descriptors extracted from deep learning models [3, 37]. Global descriptors summarize an image into a single vector, leading to a compact representation for large-scale search. Local descriptors encode detailed spatial features for patch-level matching, and are shown to be im-portant for high retrieval precision [56, 9]. The best meth-ods typically use a global descriptor to reduce the solution space to a set of candidate matching images, and local de-scriptors to re-rank the nearest images [51, 9]. While exten-sive progress has been made to improve image retrieval us-ing global features, fewer efforts have been made to develop similarity metrics based on local features. State-of-the-art approaches still rely on classic matching techniques, such as geometric veriﬁcation [42] and aggregated selective match kernels (ASMK) [55]. Geometric veriﬁcation assumes ob-ject instances are rigid and local matches between images can be estimated as a homography using RANSAC [20]. It is also an expensive process that requires iterative optimiza-tion on a large set of local descriptors. ASMK aggregates the similarities of features without modeling the geomet-ric alignment, but requires ofﬂine clustering and encoding procedures. It was mainly used as a global retrieval tech-nique in previous literature. Both geometric veriﬁcation and
ASMK require large amounts of local descriptors to ensure retrieval performance.
In this work, we propose Reranking Transformers (RRTs), which learn to predict the similarity of an im-age pair directly. Our method is general and can be used as a drop-in replacement for other reranking approaches such as geometric veriﬁcation. We conduct detailed ex-periments showing that as either a drop-in replacement or trained together with a global retrieval approach, the pro-posed method is the top-performing across the standard benchmarks for instance recognition. RRTs leverage the transformer architecture [59] which has led to signiﬁcant improvements in natural language processing [16, 30] and vision-and-language tasks [28, 12, 33]. Most recently, it has also been used for purely vision tasks, notably for im-age recognition [18] and object detection [10]. To the best of our knowledge, our work is the ﬁrst to adapt transformers for a visual task involving the analysis of image pairs in the context of reranking image search results.
Reranking Transformers are lightweight. Compared with typical feature extractors which have over 20 million parameters (e.g. 25 million in ResNet50 [24]), the proposed model only has 2.2 million parameters. It can also be easily parallelized such that re-ranking the top 100 neighbors re-quires a single pass. As shown in Fig. 1, our method directly predicts a similarity score for the matching images, instead of estimating a homography, which may be challenging un-der large viewpoint changes or infeasible for deformable objects. Our method requires much fewer descriptors but achieves superior performance, especially for challenging cases.
In current state-of-the-art models, the feature ex-traction and matching are optimized separately, which may lead to suboptimal feature representations. In this work, we
ﬁrst perform experiments using pretrained feature extrac-tors, then demonstrate the beneﬁt of jointly optimizing the feature extractor and our model in a uniﬁed framework.
Contributions. (1) We propose Reranking Transformers (RRTs), a small and effective model which learns to predict the similarity of an image pair based on global and local descriptors; (2) Compared with existing methods, RRTs re-quire fewer local descriptors and can be parallelized so that reranking the top neighbors only requires a single forward-pass; (3) We perform extensive experiments on Revisited
Oxford/Paris [44], Google Landmarks v2 [61], and Stan-ford Online Products, and show that RRTs outperform prior reranking methods across a variety of settings. of these works learn feature detection and representation jointly by non-local maximum suppression [19, 56], or at-tention [37, 54, 9]. The detected local descriptors are usu-ally used for geometric veriﬁcation [42] or ASMK [55].
Compared to local features, global descriptors provide a compact representation of an image for large-scale search.
Current global descriptors are typically extracted from CNN models [3, 58, 43, 21] by spatial pooling [2, 26, 58, 43, 35], which may not be ideal for modeling region-wise relations across images. Recent systems either use global descriptors to reduce the solution space and then local descriptors to re-rank the nearest neighbors, or encode local descriptors using a large visual codebook, followed by image matching with an aggregated selective match kernel [55, 54, 56]. This work mainly follows the retrieve-and-rerank paradigm.
Reranking for instance recognition/retrieval. Geomet-ric veriﬁcation is the dominant image reranking approach and widely used in both traditional [42] and more recent works [51, 37, 9]. Inspired by text retrieval, query expan-sion techniques have also been introduced for image re-trieval [14, 13, 57, 22]. These methods differ from geo-metric veriﬁcation and our work as they rely on analysing the local nearest neighbor graph for each query during test-ing. Diffusion based approaches [17, 63, 25, 5, 4] aim to learn the structure of the data manifold by similarity propa-gation over the global afﬁnity graph built on a query and all the gallery images, which is nontrivial to scale. Overall, the motivation of image reranking is to make better use of test-time knowledge to reﬁne retrieval results. Our work shares the same vision with this line of research but focuses more on learning the similarity of an image pair directly.
Transformers for visual tasks. Transformers have be-come the dominant architecture for representing text [16, 30]. Recently, it has also been introduced to vision-and-language [28, 33] and pure vision tasks [40, 10, 27]. As the key ingredient of the transformer architecture, the self-attention mechanism has also been studied for visual recog-nition [6, 45, 64]. These works apply transformers for single image predictions while we leverage transformers to learn the visual relation of an image pair. Our work is also closely related to SuperGlue [50], which while not a Transformer, also relies on self-attention. SuperGlue aims to learn local correspondences between images with pixel-level supervi-sion. Our work differs from SuperGlue in that it learns the similarity of an image pair with image-level supervision.
We provide a best-effort comparison with this approach. 2.