Abstract
Modern cameras are equipped with a wide array of sen-sors that enable recording the geospatial context of an im-age. Taking advantage of this, we explore depth estimation under the assumption that the camera is geocalibrated, a problem we refer to as geo-enabled depth estimation. Our key insight is that if capture location is known, the corre-sponding overhead viewpoint offers a valuable resource for understanding the scale of the scene. We propose an end-to-end architecture for depth estimation that uses geospatial context to infer a synthetic ground-level depth map from a co-located overhead image, then fuses it inside of an en-coder/decoder style segmentation network. To support eval-uation of our methods, we extend a recently released dataset with overhead imagery and corresponding height maps. Re-sults demonstrate that integrating geospatial context signif-icantly reduces error compared to baselines, both at close ranges and when evaluating at much larger distances than existing benchmarks consider. 1.

Introduction
Accurately estimating depth is important for applications that seek to interpret the 3D environment, such as aug-mented reality and autonomous driving. The traditional ge-ometric approach for solving this problem requires multiple views and infers depth by triangulating image correspon-dences. Lately, more attention has been paid to the single-image variant, which has great potential value but is known to be ill-posed. Ranftl et al. [29] point out that to solve this problem “one must exploit many, sometimes subtle, visual cues, as well as long-range context and prior knowledge.”
One of the primary difficulties with inferring depth from a single image is that there is an inherent scale ambiguity.
In other words, different sized objects in the world can have the same projection on the image plane (simply by adjusting the focal length or position in space). Despite this, meth-ods that take advantage of convolution neural networks have shown promise due to their ability to capture prior informa-tion about the appearance and shape of objects in the world.
There are broadly two classes of methods in this space.
Figure 1: We explore a new problem, geo-enabled depth estimation, in which the geospatial context of a query image is exploited during the depth estimation process.
Supervised approaches assume a ground-truth labeling is provided during training, often obtained from another sen-sor such as LiDAR. This labeling could be absolute (met-ric values) or have an unknown scale. Self-supervised ap-proaches, on the other hand, do not require ground-truth depth. Instead, the consistency of multiple inputs (e.g., se-quences of images from a video, or a stereo pair) are used to derive depth up to a scaling factor, often by formulat-ing the problem as a novel view synthesis task. For both of these classes of methods, it is common to make strong assumptions about the scale of the scene during training, or to require computation of a scaling factor at inference time in order to interpret the predicted depths.
For example, supervised methods often presume to know the maximum observed depth of the scene, by constrain-ing the output of the network using a sigmoid activation and scaling by the maximum depth [9, 19]. If the scale is unknown, i.e., a scale-invariant loss was used during train-ing, then a scaling factor must be computed at inference to interpret the predictions relative to the world. Such ob-jective functions have been proposed when metric depth is not available or for combining training datasets with differ-ent properties. For example, Ranftl et al. [29] align their predictions with the ground-truth via a least squares crite-rion before computing error metrics. These caveats limit
the generalizability of such methods when applying them to real-world imagery from novel locations (e.g., varying depth ranges or lack of ground truth).
A similar phenomena occurs in self-supervised monocu-lar approaches that estimate depth up to an unknown scale.
The maximum observed depth of the scene is often used to constrain the predicted depths during training, and a scaling factor is computed at inference to bring the predictions in line with the ground truth. As before, the common strat-egy in the current literature is to compute this scaling fac-tor using the ground-truth directly (per image), in this case by computing the ratio of the median predicted values and median ground-truth values [12]. The issue of how to cali-brate self-supervised monocular depth estimation networks has only recently been highlighted by McCraith et al. [26], who point out that current approaches severely limit practi-cal applications.
Beyond these issues, estimating depth at long ranges is known to be extremely challenging. Zhang et al. [41] note the limitations of LiDAR (sparse, reliable up to 200m) and argue the need for “dense, accurate depth perception beyond the LiDAR range.” Most state-of-the-art depth estimation networks assume a maximum depth of 100 meters for out-door scenes [12]. Further, popular benchmark datasets for depth estimation are constrained to small ranges, typically below 100 meters (using a depth cap to filter pixels in the ground truth). For example, Ranftl et al. [29] evaluate on four different datasets, ETH3D, KITTI, NYU, and TUM, with the depth caps set to 72, 80, 10, and 10 meters, respec-tively. Reza et al. [30] have similarly pointed out the need for depth estimation to function at much larger distances.
In this work we explore how geospatial context can be used to augment depth estimation, a problem we refer to as geo-enabled depth estimation (Figure 1). Modern cam-eras are commonly equipped with a suite of sensors for estimating location and orientation. Kok et al. [17] pro-vide an in-depth overview of algorithms for recovering po-sition/orientation from inertial sensors, concluding that as quality has improved and cost has decreased “inertial sen-sors can be used for even more diverse applications in the future.” Accordingly, a great deal of work has shown that geo-orientation information is extremely valuable for aug-menting traditional vision tasks [24, 25, 35, 39, 40].
Given a geocalibrated camera, we explore how to inject geospatial context into the depth estimation process. In this scenario, our goal is to develop a method that takes advan-tage of the known geocalibration of the camera to address the previously outlined weaknesses. Specifically, we want to use geospatial context to 1) reduce the inherent scale am-biguity and to 2) enable more accurate depth estimation at large distances. Our key insight is that if the location of the capturing device is known, the corresponding overhead viewpoint is a valuable resource for characterizing scale.
We propose an end-to-end architecture for depth esti-mation that uses geospatial context to infer an intermedi-ate representation of the scale of the scene. To do this, we estimate a height (elevation) map centered at the query im-age and transform it to a synthetic ground-level depth map in a differentiable manner via a sequence of voxelization and ray casting operations. This intermediate representa-tion is metric, and we fuse it inside of an encoder/decoder segmentation architecture that outputs absolute depth esti-Importantly, our approach makes no assumptions mates. during training about the maximum observed depth and re-quires no post-processing step to align predictions.
To support evaluating our methods, we extend the re-cently released HoliCity dataset [44] to include overhead imagery and corresponding height data from a composite digital surface model. Extensive experiments show that when geospatial context is available our approach signifi-cantly reduces error compared to baselines, including when evaluating at much longer depth ranges than considered by previous work. 2.