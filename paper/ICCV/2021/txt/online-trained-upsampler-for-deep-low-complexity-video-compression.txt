Abstract
Deep learning for image and video compression has demonstrated promising results both as a standalone tech-nology and a hybrid combination with existing codecs.
However, these systems still come with high computational costs. Deep learning models are typically applied directly in pixel space, making them expensive when resolutions be-come large.
In this work, we propose an online-trained upsampler to augment an existing codec. The upsampler is a small neural network trained on an isolated group of frames. Its param-eters are signalled to the decoder. This hybrid solution has a small scope of only 10s or 100s of frames and allows for a low complexity both on the encoding and the decoding side.
Our algorithm works in ofﬂine and in zero-latency set-tings. Our evaluation employs the popular x265 codec on several high-resolution datasets ranging from Full HD to 8K. We demonstrate rate savings between 8.6% and 27.5% and provide ablation studies to show the impact of our de-sign decisions.
In comparison to similar works, our ap-proach performs favourably.
Conventional 
Codecs
Hand-tuning
Online-trained 
Upsampler (this work)
Motion Vectors
Frame-Level 
Filters
Scope
Learned 
Codecs
Sequence 
Datasets
Machine Learning 100s of 
Frames 10s of 
Frames
Several 
Frames
Single 
Frame (Optical) Flow
Blockwise
Transform
Block or
Recep. Field
Learned Transf. 
& Dist.
Figure 1. Exploitation of different Scopes. Qualitative degree and key methods that exploit redundancies in various scopes for conventional and learned codecs. 1.

Introduction
Since the introduction of deep learning to image and video compression, steady improvements have led to learning algorithms outperforming many commonly used codecs. However, their principles are similar to those of conventional codecs. The main algorithmic drivers are modelling the distribution within a receptive ﬁeld, some-times also its neighbours, and the motion between two suc-cessive images. A large-scale video corpus enables learning the necessary functions. On the other hand, conventional codecs are hand-tuned and use block-wise transforms and predictions together with motion estimation.
This work proposes a method to exploit an extended scope of 10s or 100s of frames using an online-trained upsampler. The online-trained design has the advantage that computational expenses can be kept low, especially on the decoding side, compared to deep learning compression models or deep denoisers. The latter two may require 100K operations for a single pixel, while we add less than 600 op-erations to the conventional codec, which in turn requires only several dozens of operations.
Compared to how machine learning is most often ap-plied, the scope of data we consider is small. However, it is large compared to what conventional codecs typically deal with (see Fig. 1 for a qualitative overview). Note that there are exceptions for some data, for example, a sequence of
images with little change where the ﬁrst image is as good a predictor for the last image as any other. In such a case, a codec would implicitly cover a more extensive scope. How-ever, this does not hold for signiﬁcant motion or evolving textures like particle systems (water, smoke, ﬁre).
We use a conventional codec to provide compression for a lower resolution signal to deal with these more difﬁcult redundancies. At the same time, our proposed upsampler is trained on a group of frames to reconstruct the high-resolution signal efﬁciently. The encode then signals the upsampler’s parameters to the decoder. They typically make up a small fraction of the bitstream and therefore have little effect on the coding gain.
Our experiments cover the ofﬂine setting used for con-ventional non-interactive video transmission and the zero-latency setting, which is suitable for interactive environ-ments.
In summary, we
•
•
• propose a new architecture for an online-trained up-sampler incorporating internal features and position encoding, demonstrate signiﬁcant improvement over the com-monly used x265 codec, and conduct extensive ablation studies to show how our proposed approach compares under different en-coding settings. 2.