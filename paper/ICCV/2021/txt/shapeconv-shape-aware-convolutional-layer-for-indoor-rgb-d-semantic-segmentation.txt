Abstract
RGB-D semantic segmentation has attracted increasing attention over the past few years. Existing methods mostly employ homogeneous convolution operators to consume the
RGB and depth features, ignoring their intrinsic differences.
In fact, the RGB values capture the photometric appearance properties in the projected image space, while the depth fea-ture encodes both the shape of a local geometry as well as the base (whereabout) of it in a larger context. Compared with the base, the shape probably is more inherent and has a stronger connection to the semantics, and thus is more critical for segmentation accuracy. Inspired by this obser-vation, we introduce a Shape-aware Convolutional layer (ShapeConv) for processing the depth feature, where the depth feature is ﬁrstly decomposed into a shape-component and a base-component, next two learnable weights are in-troduced to cooperate with them independently, and ﬁnally a convolution is applied on the re-weighted combination of these two components. ShapeConv is model-agnostic and can be easily integrated into most CNNs to replace vanilla convolutional layers for semantic segmentation. Ex-tensive experiments on three challenging indoor RGB-D se-mantic segmentation benchmarks, i.e., NYU-Dv2(-13,-40),
SUN RGB-D, and SID, demonstrate the effectiveness of our
ShapeConv when employing it over ﬁve popular architec-tures. Moreover, the performance of CNNs with ShapeConv is boosted without introducing any computation and mem-ory increase in the inference phase. The reason is that the learnt weights for balancing the importance between the shape and base components in ShapeConv become con-stants in the inference phase, and thus can be fused into the following convolution, resulting in a network that is identi-cal to one with vanilla convolutional layers. 1.

Introduction
With the widespread use of depth sensors (such as Mi-crosoft Kinect [31]), the availability of RGB-D data has
*Corresponding Author
Figure 1. Visual demonstration of why the shape of an RGB-D im-age matters. Regarding the images on the top, lines with the same color share a same shape, yet with different base. The correspond-ing patches are shown on the bottom. boosted the advancement of RGB-D semantic segmenta-tion, which contributes to an indispensable task in the com-puter vision community. Thanks to the ﬂourishing of Con-volutional Neural Networks (CNNs), recent studies mostly resort to CNNs for tackling this problem. Convolutional layers, deemed as the core building blocks of CNNs, are accordingly the key elements in RGB-D semantic segmen-tation models [6, 13, 15, 17, 21].
However, RGB and depth information are inherently dif-ferent from each other. In particular, RGB values capture the photometric appearance properties in the projected im-age space, while the depth feature encodes both the shape of a local geometry as well as the base (whereabout) of it in a larger context. As a result, the convolution operator that is widely adopted for consuming RGB data might not be the optimal for processing the depth data. Taking Figure 1 as an example, we would expect the corresponding patches of the same chairs to have the same features, as they share
the same shape. The shape is a more inherent property of the underlying object and has stronger connection to the se-mantics. We would expect to achieve shape invariance in the learning process. When a vanilla convolution operator is applied on these corresponding patches, the resulting fea-tures are different due to the differences in their base com-ponent, hindering the learning from achieving shape invari-ance. On the other hand, the base components cannot be simply discarded for pursuing the shape invariance in the current layer, as they form the shape in a followup layer with a larger context.
To address these problems, we propose a Shape-aware
Convlutional layer (ShapeConv), to learn the adaptive bal-ance between the importance of shape and base informa-tion, giving the network the chance to focus more on the shape information whenever necessary for beneﬁting the
RGB-D semantic segmentation task. We ﬁrstly decom-pose a patch1 into two separate components, i.e., a base-component and a shape-component. The mean of patch val-ues depicts the whereabout of the patch in a larger context, thus constitutes the base component, while the residual is the relative changes in the patch, which depicts the shape of the underlying geometry, thus constitutes to the shape component. Speciﬁcally, for an input patch (such as P1 in
Figure 1), the base describes where the patch is, i.e., the dis-tance from the observation point; while the shape expresses what the patch is, e.g., a chair corner. We then employ two operations, namely, base-product and shape-product, to re-spectively process these two components with two learn-able weights, i.e., base-kernel and shape-kernel. The output from these two is then combined in an addition manner to form a shape-aware patch, which is further convolved with a normal convolutional kernel. In contrast to the original patch, the shape-aware one is capable of adaptively learn-ing the shape characteristic with the shape-kernel, and the base-kernel serves to balance the contributions of the shape and the base for the ﬁnal prediction.
In addition, since the base-kernel and shape-kernel be-come constants in the inference phase, we can fuse them into the following convolution kernel, resulting in a network that is identical to the one with vanilla convolutional layers.
The proposed ShapeConv can be easily plugged into most
CNNs as a replacement of the vanilla convolution in seman-tic segmentation without introducing any computation and memory increase in the inference phase. This simple re-placement transforms CNNs designed for RGB data into ones better suited for consuming RGB-D data.
To validate the effectiveness of the proposed method, we conduct extensive experiments on three challenging
RGB-D indoor semantic segmentation benchmarks: NYU-Dv2 [25](-13,-40), SUN RGBD [26], and SID [1]. We ap-1The operation unit of input features for the convolutional layer, whose spatial size is the same as the convolution kernel. ply our ShapeConv to ﬁve popular semantic segmentation architectures and can observe promising performance im-provements compared with baseline models. We found that
ShapeConv can signiﬁcantly improve the segmentation ac-curacy around the object boundaries (see Figure 5), which demonstrates the effective leveraging of the depth informa-tion2. 2.