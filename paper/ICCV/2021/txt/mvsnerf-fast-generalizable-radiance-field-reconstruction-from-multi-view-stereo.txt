Abstract
We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on
∗ Equal contribution
Research done when Anpei Chen was in a remote internship with UCSD. generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF. 1.

Introduction
Novel view synthesis is a long-standing problem in computer vision and graphics. Recently, neural rendering approaches have significantly advanced the progress in this area. Neural radiance fields (NeRF) and its following works [34, 31, 27] can already produce photo-realistic novel view synthesis results. However, one significant drawback of these prior works is that they require a very long per-scene optimization process to obtain high-quality radiance fields, which is expensive and highly limits the practicality.
Our goal is to make neural scene reconstruction and rendering more practical, by enabling highly efficient radiance field estimation. We propose MVSNeRF, a novel approach that generalizes well across scenes for the task
of reconstructing a radiance field from only several (as few as three) unstructured multi-view input images. With strong generalizability, we avoid the tedious per-scene optimization and can directly regress realistic images at
If further novel viewpoints via fast network inference. optimized on more images with only a short period (5-15 min), our reconstructed radiance fields can even outperform
NeRFs [34] with hours of optimization (see Fig. 1).
We take advantage of the recent success on deep multi-view stereo (MVS) [50, 18, 10].
This line of work can train generalizable neural networks for the task of 3D reconstruction by applying 3D convolutions on cost volumes. Similar to [50], we build a cost volume at the input reference view by warping 2D image features (inferred by a 2D CNN) from nearby input views onto sweeping planes in the reference view’s frustrum. Unlike MVS methods [50, 10] that merely conduct depth inference on such a cost volume, our network reasons about both scene geometry and appearance, and outputs a neural radiance field (see Fig. 2), enabling view synthesis. Specifically, leveraging 3D CNN, we reconstruct (from the cost volume) a neural scene encoding volume that consists of per-voxel neural features that encode information about the local scene geometry and appearance. Then, we make use of a multi-layer perceptron (MLP) to decode the volume density and radiance at arbitrary continuous locations using tri-linearly interpolated neural features inside the encoding volume.
In essence, the encoding volume is a localized neural representation of the radiance field; once estimated, this volume can be used directly (dropping the 3D CNN) for final rendering by differentiable ray marching (as in [34]). rendering that allows for
Our approach takes the best of the two worlds, learning-based MVS and neural rendering. Compared with existing MVS methods, we enable differentiable neural training without 3D supervision and inference time optimization for further quality improvement.
Compared with existing neural rendering works, our MVS-like architecture is natural to conduct cross-view correspondence reasoning, facilitating the generalization to unseen testing scenes and also leading to better neural scene reconstruction and rendering. Our approach can, therefore, significantly outperform the recent concurrent generalizable NeRF work [54, 46] that mainly considers 2D image features without explicit geometry-aware 3D structures (See Tab. 1 and Fig. 4). We demonstrate that, using only three input images, our network trained from the DTU dataset can synthesize photo-realistic images on testing DTU scenes, and can even generate reasonable results on other datasets that have very different scene distributions. Moreover, our estimated three-image radiance field (the neural encoding volume) can be further easily optimized on novel testing scenes to improve the neural reconstruction if more images leading to photo-realistic results that are are captured, comparable or even better than a per-scene overfit NeRF, despite of ours taking substantially less optimization time than NeRF (see Fig. 1).
These experiments showcase that our technique can be used either as a strong reconstructor that can reconstruct a radiance field for realistic view synthesis when there are only few images captured, or as a strong initializor that significantly facilitates the per-scene radiance field optimization when dense images are available.
Our approach takes an important step towards making realistic neural rendering practical. We have released the code at mvsnerf.github.io. 2.