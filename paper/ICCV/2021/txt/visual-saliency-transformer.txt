Abstract
Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Trans-former (VST), for both RGB and RGB-D salient object de-tection (SOD). It takes image patches as inputs and lever-ages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in
Vision Transformer (ViT), we leverage multi-level token fu-sion and propose a new token upsampling method under the transformer framework to get high-resolution detec-tion results. We also develop a token-based multi-task de-coder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspec-tive for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is avail-able at https://github.com/nnizhang/VST. 1.

Introduction
SOD aims to detect objects that attract peoples’ eyes and can help many vision tasks, e.g., [58, 19]. Recently, RGB-D
SOD has also gained growing interest with the extra spatial structure information from the depth data. Current state-of-the-art SOD methods are dominated by convolutional ar-chitectures [28], on both RGB and RGB-D data. They often adopt an encoder-decoder CNN architecture [47, 57], where the encoder encodes the input image to multi-level features and the decoder integrates the extracted features to predict the final saliency map. Based on this simple architecture,
*Equal contribution.
†Corresponding author. most efforts have been made to build a powerful decoder for predicting better saliency results. To this end, they in-troduced various attention models [37, 80, 7], multi-scale feature integration methods [24, 49, 16, 43], and multi-task learning frameworks [67, 77, 82, 69, 25]. An additional de-mand for RGB-D SOD is to effectively fuse cross-modal information, i.e., the appearance information and the depth cues. Existing works propose various modality fusion meth-ods, such as feature fusion [22, 4, 16, 18, 89], knowledge distillation [53], dynamic convolution [48], attention mod-els [31, 78], and graph neural networks [43]. Hence, CNN-based methods have achieved impressive results [66, 88].
However, all previous methods are limited in learning global long-range dependencies. Global contexts [21, 83, 56, 44, 37] and global contrast [75, 2, 8] have been proved crucial for saliency detection for a long time. Nevertheless, due to the intrinsic limitation of CNNs that they extract fea-tures in local sliding windows, previous methods can hardly exploit the crucial global cues. Although some methods uti-lized fully connected layers [36, 22], global pooling layers
[44, 37, 65], and non-local modules [38, 7] to incorporate the global context, they only did such in certain layers and the standard CNN-based architecture remains unchanged.
Recently, Transformer [61] was proposed to model global long-range dependencies among word sequences for machine translation. The core idea is the self-attention mechanism, which leverages the query-key correlation to relate different positions in a sequence. Transformer stacks the self-attention layers multiple times in both encoder and decoder, thus can model long-range dependencies in every layer. Hence, it is natural to introduce the Transformer to
SOD, leveraging the global cues in the model all the way.
In this paper, for the first time, we rethink SOD from a new sequence-to-sequence perspective and develop a novel unified model for both RGB and RGB-D SOD based on a pure transformer, which is named Visual Saliency Trans-former. We follow the recently proposed ViT models
[12, 74] to divide each image into patches and adopt the
Transformer model on the patch sequence. Then, the Trans-former propagates long-range dependencies between image patches, without any need of using convolution. However,
it is not straightforward to apply ViT for SOD. On the one hand, how to perform dense prediction tasks based on pure transformer still remains an open question. On the other hand, ViT usually tokenizes the image to a very coarse scale. How to adapt ViT to the high-resolution prediction demand of SOD is also unclear.
To solve the first problem, we design a token-based transformer decoder by introducing task-related tokens
Then, we propose a to learn decision embeddings. novel patch-task-attention mechanism to generate dense-prediction results, which provides a new paradigm for using transformer in dense prediction tasks. Motivated by pre-vious SOD models [82, 87, 79, 25] that leveraged bound-ary detection to boost the SOD performance, we build a multi-task decoder to simultaneously conduct saliency and boundary detection by introducing a saliency token and a boundary token. This strategy simplifies the multitask pre-diction workflow by simply learning task-related tokens, thus largely reduces the computational costs while obtain-ing better results. To solve the second problem, inspired by the Tokens-to-Token (T2T) transformation [74], which re-duces the length of tokens, we propose a new reverse T2T transformation to upsample tokens by expanding each token into multiple sub-tokens. Then, we upsample patch tokens progressively and fuse them with low-level tokens to obtain the final full-resolution saliency map. In addition, we also use a cross modality transformer to deeply explore the inter-action between multi-modal information for RGB-D SOD.
Finally, our VST outperforms existing state-of-the-art SOD methods with a comparable number of parameters and com-putational costs, on both RGB and RGB-D data.
Our main contributions can be summarized as follows:
• For the first time, we design a novel unified model based on the pure transformer architecture for both
RGB and RGB-D SOD, from a new perspective of sequence-to-sequence modeling.
• We design a multi-task transformer decoder to jointly conduct saliency and boundary detection by introduc-ing task-related tokens and patch-task-attention.
• We propose a new token upsampling method for transformer-based framework.
• Our proposed VST model achieves state-of-the-art results on both RGB and RGB-D SOD benchmark datasets, which demonstrates its effectiveness and the potential of transformer-based models for SOD. 2.