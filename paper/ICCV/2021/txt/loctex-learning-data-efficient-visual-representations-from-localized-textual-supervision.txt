Abstract
Computer vision tasks such as object detection and se-mantic/instance segmentation rely on the painstaking an-notation of large training datasets. In this paper, we pro-pose LocTex that takes advantage of the low-cost localized textual annotations (i.e., captions and synchronized mouse-over gestures) to reduce the annotation effort. We introduce a contrastive pre-training framework between images and captions, and propose to supervise the cross-modal attention map with rendered mouse traces to provide coarse local-ization signals. Our learned visual features capture rich semantics (from free-form captions) and accurate localiza-tion (from mouse traces), which are very effective when transferred to various downstream vision tasks. Compared with ImageNet supervised pre-training, LocTex can reduce the size of the pre-training dataset by 10× or the target dataset by 2× while achieving comparable or even improved performance on COCO instance segmentation. When pro-vided with the same amount of annotations, LocTex achieves around 4% higher accuracy than the previous state-of-the-art “vision+language” pre-training approach on the task of
PASCAL VOC image classiﬁcation. 1.

Introduction
The tremendous success of deep learning in computer vision can be credited in part to the existence of large anno-tated datasets, such as ImageNet [7, 47]. However, acquiring high-quality annotations is usually very expensive and time-consuming, especially for dense, pixel-wise labeling tasks.
For instance, segmenting instances in a single image from the
COCO dataset takes more than 10 minutes on average [29]*.
Pre-training plus ﬁne-tuning is a widely-adopted solution to reduce the need for costly annotations. In the computer vision community, a convolutional neural network (CNN) backbone is ﬁrst pre-trained to perform image classiﬁca-tion on ImageNet. Then, the learned features can be trans-*70k hours / 320k images = 0.22 hours/image = 13 minutes/image
Figure 1: LocTex pre-trains the visual CNN backbone with (a) localized textual annotations, which consists of free-form captions associated with synchronized mouse traces. With our contrastive and localization loss, the model learns (b) rich semantics and accurate localization. This is very useful when transferred to (c) downstream tasks that are sensitive to localization (e.g., object detection, instance segmentation). ferred to other downstream tasks by ﬁne-tuning on the target dataset. Over the past few years, this paradigm has enabled state-of-the-art performance on many computer vision tasks, including object detection [46], semantic segmentation [31] and instance segmentation [20].
Though effective, ImageNet pre-training has its caveats. (i) Its annotations (i.e., 1000-class labels) are very expensive to acquire. Annotating ImageNet is not as easy as it seems because differentiating among a ﬁne-grained class taxonomy requires expert knowledge, which makes it hard to scale up or repeat. (ii) It is not as effective for those tasks that are more sensitive to localization than classiﬁcation. As ImageNet pre-training only takes the object existence into consideration, its learned visual representations are supposed to be invariant to different object locations. Some recent research [19] has demonstrated competitive performance on object detection and instance segmentation with models trained from scratch. 1
To solve (i), researchers have explored pre-training back-bone networks with coarse, freely available labels, such as metadata and hashtags [23]. There has also been increased attention in self-supervised pre-training that learns visual representations from unlabeled images [18, 5, 16]. Some of them have been successfully scaled up to hundreds of mil-lions or even billions of images [18]. However, (ii) remains unsolved as they usually rely on some low-level visual cues (e.g., color, texture) and lack semantic understanding. In addition to this, (iii) self-supervised pre-training methods tend to be trained with prohibitively long schedules to ex-ploit their potential. For instance, the recent approach of
BYOL [16] requires 170 TPU days for a single training run.
In this paper, we propose LocTex to learn data-efﬁcient visual representations using localized textual supervision, which is composed of free-form captions associated to syn-chronized mouse traces (see Figure 1a). This form of an-notation can be easily acquired from non-expert workers, leading to (i) lower cost and better scalability. Technically, we propose to bridge the vision and language modalities with contrastive learning and supervise the cross-modal at-tention map with rendered mouse traces, providing (ii) coarse localization information that improves the performance of localization-sensitive downstream tasks. Finally, our method requires (iii) a similar amount of training time as ImageNet pre-training: it can be trained under a day with 8 GPUs.
After the pre-training, we transfer our learned feature rep-resentations to various downstream vision tasks, including image classiﬁcation, object detection and instance segmenta-tion. Compared with the ImageNet supervised pre-training, our proposed LocTex can reduce the size of the pre-training dataset by 10× or the target dataset by 2× while achieving comparable or better performance on the COCO instance seg-mentation. With the same amount of annotations, our LocTex achieves around 4% higher accuracy than the previous state-of-the-art “vision+language” pre-training approach [8] on the PASCAL VOC image classiﬁcation. 2.