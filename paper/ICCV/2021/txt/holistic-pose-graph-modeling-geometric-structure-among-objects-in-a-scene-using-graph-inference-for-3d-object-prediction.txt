Abstract
Due to the missing depth cues, it is essentially ambigu-ous to detect 3D objects from a single RGB image. Existing methods predict the 3D pose for each object independently or merely by combining local relationships within limited surroundings, but rarely explore the inherent geometric re-lationships from a global perspective. To address this issue, we argue that modeling geometric structure among objects in a scene is very crucial, and thus elaborately devise the
Holistic Pose Graph (HPG) that explicitly integrates all ge-ometric poses including the object pose treated as nodes and the relative pose treated as edges. The inference of the HPG uses GRU to encode the pose features from their corresponding regions in a single RGB image, and passes messages along the graph structure iteratively to improve the predicted poses. To further enhance the correspondence between the object pose and the relative pose, we propose a novel consistency loss to explicitly measure the deviations between them. Finally, we apply Holistic Pose Estimation (HPE) to jointly evaluate both the independent object pose and the relative pose. Our experiments on the SUN RGB-D dataset demonstrate that the proposed method provides a significant improvement on 3D object prediction. 1.

Introduction 3D object prediction from a single RGB image is ex-tremely challenging, which estimates 3D bounding boxes for each object in a scene. The main difficulty of this task is to predict the depth information that a single RGB im-age loses during the projection from 3D real world to the 2D image. How are humans capable of making precise esti-mations when looking at an image? Humans not only have rich prior knowledge about the category-specific object, but also can leverage the geometric relationships among differ-Figure 1. The effect of using geometric relationships. (a) some results from our baseline method [27], which make inaccurate es-timation about the relative pose. (b) our model makes more rea-sonable prediction after using HPG. ent objects in a scene to alleviate the uncertainties of predic-tion for each object. Existing methods [12,14,16,17,20,37] consider using the prior knowledge to reason about the ob-ject pose independently but it is rough sometimes. As a result of the inevitable deviation of prediction per object, there will be a certain amount of inaccurate estimations, as shown in Figure 1 (a), which cause humans’ misunderstand-ing about the scene. These results mainly are caused by the improper relative pose estimation and could have been ef-fectively avoided by using the geometric relationships.
Inspired by the above observations, we seek to leverage the geometric relationship to add more constraints on each object for more reasonable and precise estimation. Mod-eling the geometric relationships explicitly will help us ex-clude many impossible solutions in the 3D space. For exam-ple, as shown in top of Figure 1 (a), we should have gotten
calculating the 3D Intersection over Union (IoU) between the predicted 3D bounding box and ground truth, but fail to treat all objects in the image as a whole. Under these metrics, the results in the top row of Figure 1 (a) and (b) can not be distinguished because all the objects’ IoU are above the preset threshold. However, the results of Figure 1 (b) are indeed more rational in human cognition. Conse-quently, we introduce Holistic Pose Estimation (HPE) that more holistically evaluates both the 3D bounding box of the independent object and the relative pose of each pair of ob-jects.
We evaluate our model 1 on SUN RGB-D dataset [32] to verify the effectiveness of the Holistic Pose Graph. Re-sults show that our proposed method outperforms previous methods both on the existing metrics and HPE. 2.