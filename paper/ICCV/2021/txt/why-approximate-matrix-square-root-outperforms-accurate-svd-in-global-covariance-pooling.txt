Abstract
Global Covariance Pooling (GCP) aims at exploiting the second-order statistics of the convolutional feature. Its effectiveness has been demonstrated in boosting the clas-siﬁcation performance of Convolutional Neural Networks (CNNs). Singular Value Decomposition (SVD) is used in
GCP to compute the matrix square root. However, the approximate matrix square root calculated using Newton-Schulz iteration [14] outperforms the accurate one com-puted via SVD [15]. We empirically analyze the reason behind the performance gap from the perspectives of data precision and gradient smoothness. Various remedies for computing smooth SVD gradients are investigated. Based on our observation and analyses, a hybrid training proto-col is proposed for SVD-based GCP meta-layers such that competitive performances can be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP meta-layer that uses SVD in the forward pass, and Pad´e approx-imants in the backward propagation to compute the gra-dients. The proposed meta-layer has been integrated into different CNN models and achieves state-of-the-art perfor-mances on both large-scale and ﬁne-grained datasets. 1.

Introduction
Global Covariance Pooling (GCP) explores the second-order statistics by normalizing the covariance matrix of the convolutional features before feeding them to the fully-connected layer.
It has been shown to outperform the
ﬁrst-order pooling methods (e.g., max-pooling and average-pooling) [9, 17, 15, 16, 14]. Generally, a GCP meta-layer computes the covariance matrix of the features as the global representation, and then performs eigendecomposi-tion to derive the corresponding eigenvalues and eigenvec-tors, followed by normalization using either matrix loga-rithm [9, 17] or the matrix square root [15, 16, 14]. How-Figure 1. (Left) The training top-5 error of AlexNet. The perfor-mance gap is gradually minimizing as the learning rate decays. (Right) The validation top-1 error in the last stage. These SVD remedies marginally improve the performance but are not compa-rable against iSQRT-COV [14]. Our proposed SVD-Pad´e achieves the best results among the SVD methods. ever, the logarithm may change the magnitude of eigen-values signiﬁcantly and over-stretch the small ones. Be-sides, the matrix square root has been proved to amount to robust covariance estimation and to approximately exploit
Riemannian geometry [15]. Thus, the matrix square root is often preferred over logarithm normalization.
One can either use SVD to compute the accurate square root [15] or use the Newton-Schulz iteration method to de-rive the approximate square root [14, 16]. Intuitively, the accurate one should yield better performance. Surprisingly, the approximate square root outperforms the exact one con-tinuously [14]. Our paper starts with this intriguing obser-vation and intends to ﬁnd out the underlying reasons.
One crucial issue of SVD is the numerical instability of its gradients which is derived from the skew-symmetric matrix K whose off-diagonal elements are deﬁned as
Kij=1/(λi−λj), where λi and λj are eigenvalues. Kij 1 can be reformulated as 1
. When the two eigenval-1−λj /λi
λi ues are very small and close to each other, 1 1 1−λj /λi
λi will move towards inﬁnity. As a consequence, the gradient
Kij will explode. To avoid this issue, several attempts have been made to smooth the gradients [16, 7, 25]. These meth-ods consistently outperform ordinary SVD in other tasks, but none of them has been validated in GCP yet. and
The gradient instability issue becomes more critical for
GCP as it usually deals with very large matrices. According to our observation, when the covariance matrix dimension is very large (>200), it is more likely to have many small eigenvalues. Single precision (i.e., ﬂoat32) usually zeros out small eigenvalues and cannot guarantee the convergence of the network. Therefore, the data type of the covariance is set to double precision (i.e., ﬂoat64) such that the small eigenvalues can be well represented. However, the high pre-cision can also represent very subtle differences between the small eigenvalues. This can easily result in large gradients and aggravate the instability issue, and thus lead to inferior performance. Therefore, a couple of questions arise: 1) Is the performance gap between MPN-COV [15] (with accurate SVD) and iSQRT-COV [14] (with approxi-mate one) related to their gradient smoothness? 2) Can we smooth the gradient of the accurate SVD to help MPN-COV [15] to achieve competitive perfor-mance against iSQRT-COV [14]?
To answer these questions, we introduce several SVD back-ward remedies into MPN-COV [15] which use different tricks (e.g., gradient truncation, abandoning small eigenval-ues, and Taylor polynomial approximation [25]) to smooth the gradients. Fig. 1 shows the training and validation er-ror curves of the modiﬁed SVD remedies and the ordinary
SVD. We can see that although the modiﬁed SVD functions bring marginal performance gain over the ordinary SVD, still none of them is comparable against the Newton-Schulz based iSQRT-COV [14]. This implies that gradient smooth-ness does not fully account for the disparity.
Another interesting observation is that the performance gap between the modiﬁed SVD functions and iSQRT-COV [14] is gradually mitigating when the learning rate decreases (see Fig. 1 left). This is probably because with a small learning rate and stable network weights, the co-variance matrices are more likely to be well-conditioned, and thus the smallest eigenvalues λmin are larger than EPS (i.e., the smallest positive number the data precision al-lows). This could beneﬁt SVD for stable eigendecom-postion as smaller round-off errors and smoother gradients are obtained. Otherwise, if λmin is smaller than EPS, the small eigenvalues would be zeroed out and the gradient
Kij would go to inﬁnity. We empirically show that the covariance matrices of MPN-COV [15] are indeed becom-ing better-conditioned as the learning rate decreases, which is coherent with the trend of performance gap. This has enlightened us to combine the SVD function with iSQRT-COV [14] and to develop a hybrid training protocol, i.e., use Newton-Schulz iteration to train the network until the learning rate is sufﬁciently small and the network weights are relatively stable, then switch to the ordinary/modiﬁed
SVD for accurate matrix square root calculation. By doing so, SVD only deals with well-conditioned matrices in the later stage. This hybrid strategy fully explores the potential of SVD for eigendecomposition, and thus these SVD meth-ods achieve competitive and sometimes better performance than iSQRT-COV [14].
When the hybrid training protocol is used, unlike the situation in the standalone training, the marginal perfor-mance improvement of the modiﬁed SVD remedies over ordinary SVD no longer holds. The ordinary SVD func-tion can sometimes outperform the modiﬁed SVD reme-dies with smooth gradients. This phenomenon makes us re-consider the effectiveness and necessity to smooth the gra-dients. As the large gradients can be close to inﬁnity and are very likely to cause overﬂow, these modiﬁed SVD remedies drastically change the gradient at the order of magnitude for smoothness. However, the large but accurate gradients are important for learning robust representation and improving the generalization performance. We argue that the large gradients should be closely approximated while avoiding singularities. Among the SVD remedies, Wang et al. [25] suggested a promising direction to approximate the gradi-ents using the Taylor polynomial, but their truncated Taylor series cannot converge under certain circumstances. Moti-vated by this work, we propose to use Pad´e approximants, a rational approximation technique that has larger conver-gence radii and more powerful approximation abilities, to estimate the gradients. We show the appealing conver-gence property of Pad´e approximants over Taylor polyno-mial. The proposed meta-layer outperforms all the existing spectral methods and its combination with Newton-Schulz iteration achieves state-of-the-art performances. Our con-tributions are threefold:
• We empirically analyze the reason behind the superior performance of the approximate matrix square root over the accurate one from data precision and gradient smooth-ness view points. Various remedies for computing smooth
SVD gradients are investigated and validated in GCP.
• A hybrid training protocol is proposed for GCP meta-layers and competitive performance can be achieved com-pared with Newton-Schulz iteration. We justiﬁed this strategy using the metric condition number to measure the ill-condition of covariance matrices.
• We propose a SVD backward algorithm that relies on
Pad´e approximants for fast and robust gradient approx-imation. It consistently achieves state-of-the-art perfor-mance on different datasets and different models.
Finally, to promote the easy applicability of the relevant
SVD techniques, we will release the source codes of all the methods implemented in PYTORCH upon acceptance1. 1The code is available at https://github.com/
KingJamesSong/DifferentiableSVD.
2.