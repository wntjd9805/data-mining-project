Abstract
We present a new versatile building block for deep point cloud processing architectures that is equally suited for diverse tasks. This building block combines the ideas of spatial transformers and multi-view convolutional networks with the efﬁciency of standard convolutional layers in two and three-dimensional dense grids. The new block oper-ates via multiple parallel heads, whereas each head dif-ferentiably rasterizes feature representations of individual points into a low-dimensional space, and then uses dense convolution to propagate information across points. The results of the processing of individual heads are then com-bined together resulting in the update of point features.
Using the new block, we build architectures for both dis-criminative (point cloud segmentation, point cloud classiﬁ-cation) and generative (point cloud inpainting and image-based point cloud reconstruction) tasks. The resulting ar-chitectures achieve state-of-the-art performance for these tasks, demonstrating the versatility of the new block for point cloud processing. 1.

Introduction
In this work, we consider recognition and generation tasks for point clouds, such as semantic segmentation or image-based reconstruction. Most state-of-the-art architec-tures for point cloud processing are derived from convolu-tional neural networks (ConvNets) [20] and are inspired by the success of ConvNets in image processing tasks. Such
ConvNet adaptations are based on direct rasterization of point clouds onto regular grids followed by convolutional pipelines [38, 8], as well as on generalizations of the con-volutional operators to irregularly sampled data [25, 50] or non-rectangular grids [18, 16].
In this work, we propose a new building block (a cloud transform block) for point cloud processing architectures that combines the ideas of ConvNets and Transformers [48] (Figure 3). Similarly to the (self)-attention layers within transformers, our cloud transform blocks take unordered
*VL is currently with Yandex and Skoltech.
Figure 1: Sample outputs of cloud transformers across four di-verse cloud processing tasks including recognition tasks (left) and generation tasks (right). sets of vectors as inputs, and process such input using multi-ple parallel heads. For an input set element, each head com-putes two- or three-dimensional key and a higher dimen-sional value, and then uses the computed keys to rasterize the respective values onto a regular grid. A two- or three-dimensional convolution is then used to propagate the in-formation across elements. The results of parallel heads are then probed at key locations and are recombined together, producing an update to element features.
We show that multiple cloud transform blocks can be stacked sequentially and trained end-to-end, as long as spe-cial care is taken when implementing forward and back-ward pass through the rasterization operations. We then de-sign cloud transformer architectures that concatenate mul-tiple cloud transform blocks together with task-speciﬁc 3D convolutional layers. Speciﬁcally, we design a cloud trans-former for semantic segmentation (which we evaluate on the S3DIS benchmark [1]), classiﬁcation (which we eval-uate on the ScanObjectNN benchmark [47]), point cloud inpainting (which we evaluate on ShapeNet-based bench-mark [58]), and a cloud transformer for image-based ge-ometric reconstruction (which we evaluate on a recently introduced ShapeNet-based benchmark [42]). In the eval-uation, the designed cloud transformers achieve state-of-the-art accuracy for semantic segmentation and point cloud completion tasks and considerably outperform state-of-the-art for image-based reconstruction and point cloud classi-ﬁcation (Figure 1). We note that such versatility is rare among previously introduced point cloud processing archi-tectures, which can handle either recognition tasks (such as semantic segmentation, classiﬁcation) or generation tasks (such as inpainting and image-based reconstruction) but usually not both.
To sum up, our key contributions and novelty are:
• We propose a new approach to point cloud processing based on repeated learnable projection, rasterization and de-rasterization operations. We investgate how to make rasterizations and de-rasterizations repeatable sequentially within the same architecture through the gradient balancing trick. Additionally, we show that aggregating rasterizations via element-wise maximum performs better than additive accumulation at least in the context of our approach.
• We propose and validate an idea of multi-head self-attention for point clouds that performs parallel pro-cessing by rasterization and de-rasterization to sep-arate low-dimensional grids. Additionally, we pro-pose an idea of using both two-dimensional and three-dimensional grids in parallel with each other.
• Based on the two ideas above, we propose architec-tures for semantic segmentation, classiﬁcation, point cloud inpainting, and image-based reconstruction. The proposed architectures are all based on the same Cloud
Transform blocks and achieve state-of-the-art perfor-mance on standard benchmarks in each case despite the diversity of tasks. 2.