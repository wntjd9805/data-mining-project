Abstract
Point cloud completion is an interesting and challeng-ing task in 3D vision, aiming to recover complete shapes from sparse and incomplete point clouds. Existing learning-to based methods often require vast computation cost achieve excellent performance, which limits their practi-cal applications.
In this paper, we propose a novel Re-current Forward Network (RFNet), which is composed of three modules: Recurrent Feature Extraction (RFE), For-ward Dense Completion (FDC) and Raw Shape Protection (RSP). The RFE extracts multiple global features from the incomplete point clouds for different recurrent levels, and the FDC generates point clouds in a coarse-to-ﬁne pipeline.
The RSP introduces details from the original incomplete models to reﬁne the completion results. Besides, we propose a Sampling Chamfer Distance to better capture the shapes of models and a new Balanced Expansion Constraint to re-strict the expansion distances from coarse to ﬁne. Accord-ing to the experiments on ShapeNet and KITTI, our network can achieve the state-of-the-art with lower memory cost and faster convergence. 1.

Introduction
With the rapid development of real-time 3D sensors like
LiDAR and depth camera, 3D data has attracted more and more attention in computer vision and robotics. As a repre-sentation which describes the scene better than 2D images, 3D point clouds have been widely used in applications such as SLAM [1] and object detection [6, 18, 22]. However, point clouds acquired from sensors are often incomplete and sparse due to the limitation of resolution and occlusion.
As a consequence, recovering complete and high-resolution models from incomplete inputs has been an important and challenging task, known as the point cloud completion.
Since the work of PCN [32], many deep learning based researches have been explored on the 3D point cloud com-pletion work. Some of them are based on 3D grids and 3D
* means the corresponding author
Figure 1. Comparison of U-Net (left) and our framework (right).
Rectangles denote operations in networks, while same colors mean same parameters. Our framework recurrently send the coarse com-pleted results and incomplete input to next level, while parameters are shared in some layers. convolution neural networks(CNNs), such as GRNet [28].
The others are built on the structure of PointNet [19] and
PointNet++ [20], such as TopNet [24] and SANet [27].
These networks need high-dimensional global features or multiple local features to acquire enough shape informa-tion from the inputs. Most of them have plenty of pa-rameters and suffer from great memory cost to reach good performance. To address these problems, we propose a novel well-performed recurrent forward point cloud com-pletion framework with designed lightweight modules and parameter-shared operations to reduce the parameters and memory cost. Besides, most works above pay less attention to details of incomplete point clouds, which will cause a dis-tortion of the outputs. Large distortions will lead to mean-ingless completion results. On this occasion, we merge original shapes from incomplete models with outputs of dif-ferent resolutions to prevent our completion results from large distortions.
In this paper, we propose a novel approach named
RFNet. As shown in Fig. 1, it is organized in a “forward” framework different from the “backward” framework like
U-Net [21], which has been proved working well on seg-mentation [17, 3] and point cloud completion [27]. How-ever, U-Net is computationally expensive to search and ag-gregate local features of different resolutions, especially for dense point clouds. In our framework, operations are or-ganized in multiple recurrent levels, which can be divided
into three modules: Recurrent Feature Extraction (RFE),
Forward Dense Completion (FDC) and Raw Shape Protec-tion (RSP). In the RFE, we do not extract high-dimensional global features or local features for completion, but lever-age multiple short global features in different recurrent lev-els to decrease the computational cost. We introduce the
FDC to generate completed point clouds of different reso-lutions. The FDC generates the results by creating an initial model and lifting it to high resolutions with a larger lifting ratio than most previous methods. Therefore, our network needs fewer lifting levels to generate dense point clouds.
Some layers in the RFE and FDC are parameter-shared to further reduce the model size. We propose the RSP to pre-serve details of the original incomplete model. Points from the FDC are driven towards their nearest neighbors in orig-inal point clouds in the RSP. The driving distances can be controlled by a learnable parameter.
In this way, we can preserve more information of the original models. To better capture the shapes and improve the uniformity of results, we apply Chamfer Distance in randomly divided subsets of dense point clouds, named Sampling Chamfer Distance.
Besides, we improve the generation continuity in the FDC by constraining expansion distances balanced with their es-timated expectation.
The contributions can be summarized as follows: 1. We propose a novel point cloud completion network based on a new recurrent forward framework. In addition to the improvement of the completion result, the memory cost is greatly reduced beneﬁt from this structure; 2. We propose a Raw Shape Protection module to pre-serve original shapes in a learnable way; 3. We propose a Sampling Chamfer Distance to better capture the shape differences between models and a new
Balanced Expansion Constraint to restrict the expansion distances from coarse to ﬁne; 4. The experiments on ShapeNet and KITTI demonstrate that our network outperforms existing methods on the 3D completion task. 2.