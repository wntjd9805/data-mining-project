Abstract 1.

Introduction
We tackle the problem of monocular 3D reconstruction of articulated objects like humans and animals. We con-tribute DensePose 3D, a method that can learn such re-constructions in a weakly supervised fashion from 2D im-age annotations only. This is in stark contrast with previ-ous deformable reconstruction methods that use paramet-ric models such as SMPL pre-trained on a large dataset of 3D object scans. Because it does not require 3D scans,
DensePose 3D can be used for learning a wide range of ar-ticulated categories such as different animal species. The method learns, in an end-to-end fashion, a soft partition of a given category-speciﬁc 3D template mesh into rigid parts together with a monocular reconstruction network that pre-dicts the part motions such that they reproject correctly onto 2D DensePose-like surface annotations of the object. The decomposition of the object into parts is regularized by ex-pressing part assignments as a combination of the smooth eigenfunctions of the Laplace-Beltrami operator. We show signiﬁcant improvements compared to state-of-the-art non-rigid structure-from-motion baselines on both synthetic and real data on categories of humans and animals.
Recent advances in deep learning have produced impres-sive results in monocular 3D reconstruction of articulated and deformable objects, at least for particular object cat-egories such as humans. Unfortunately, while such tech-niques are general in principle, their success is rather dif-ﬁcult to replicate in other categories. Before learning to reconstruct 3D objects from images, one must ﬁrst learn a model of the possible 3D shapes of the objects. For hu-mans, examples of such models include SMPL [37] and
GHUM [62]. Constructing these requires a large dataset of 3D scans of the objects deforming and articulating over time, which have to be acquired with specialised devices such as domes. Not only this hardware is uncommon, com-plex and expensive, but it is also difﬁcult if not impossi-ble to apply to many objects of interest, such as wild ani-mals or even certain types of deformable inanimate objects.
Then, after building a suitable 3D shape model, one still has to train a deep neural network regressor that can pre-dict the shape parameters given a 2D image of the object as input [29, 63, 26]. Supervising such a network requires in turn a dataset of images paired with the corresponding ground-truth 3D shape parameters. Images with paired re-constructions are also very difﬁcult to obtain in practice. 1
Some images may be available from the same scanners that have been used to construct the 3D model in the ﬁrst place, but these are limited to ‘laboratory condition’ by deﬁnition.
Thus, while there is abundance of ‘in the wild’ images of di-verse object categories that can be obtained from the Inter-net, they are lacking 3D ground-truth and are thus difﬁcult to use for learning 3D shape predictors.
In this paper, we are interested in bootstrapping 3D mod-els and monocular 3D predictors without using images with corresponding 3D annotations or even unpaired 3D scans.
Fortunately, other modalities can provide strong cues for re-construction. For example, previous work [26, 43, 30, 15] leveraged 2D annotations for semantic keypoints to accu-rately reconstruct various object categories. While these keypoints provide a supervisory signal at sparse image lo-cations, DensePose [21, 40, 50] provides dense correspon-dences between the images of humans or other animals and 3D templates of these categories. Example of these anno-tations are shown on the left of Figure 1, where the colours encode the indices of corresponding points on the template mesh. DensePose annotations can be seen as generalising sparse joint locations, with two important differences: the density is much higher, and the correspondences are deﬁned on the surface of the object rather than in its skeleton joints.
Such dense annotations can be obtained manually or with detectors pre-trained on those manual 2D annotations, with the same degree of ﬂexibility and generality as sparse 2D landmarks, while providing much stronger cues for learning detailed 3D models of the objects. However, such annota-tions do not appear to have been used to bootstrap 3D object models before.
The main goal of this work is thus to leverage dense surface annotations, such as the ones provided by Dense-Pose, in order to learn a parametric model of a 3D ob-ject category without using any 3D supervision. As done in [26, 43, 30, 15], we further aim to learn a deep neural network predictor that aligns the model to individual 2D in-put images containing the object of interest. Our method assumes only having an initial rigid canonical 3D template of the object category generated by a 3D artist. There is no loss of generality here since knowledge of the template is re-quired to collect DensePose annotations in the ﬁrst place.1
Thus, pragmatically, we include this template in our model.
Our main contribution is a novel parametric mesh model for articulated object categories, which we call Dense-Pose 3D (DP3D). In a purely data-driven manner, DP3D learns to softly assign the vertices of the initial rigid tem-plate to one of a number of latent parts, each of which mov-ing in a rigid manner. The parametrization of the mesh ar-ticulation is then given by a set of per-part rigid transforms 1The 3D template is used by the human annotators as a reference to mark correspondences and deﬁnes the canonical surface mapping for the object category. expressed in the space of the logarithms of SE(3). In or-der to pose the mesh, each vertex of the template shape is deformed with a vertex-speciﬁc transformation deﬁned as a convex combination of the part-speciﬁc transforms, where the weights are supplied by the soft segmentation of the corresponding vertex. In order to prevent unrealistic shape deformations, we enforce smoothness of the part segmen-tation, and consequently of the vertex-speciﬁc offsets, by expressing the part assignment as a function of a truncated eigenbasis of the Laplace-Beltrami operator computed on the template mesh, which varies smoothly along the mesh surface. We further regularise the mesh deformations with the as-rigid-as-possible (ARAP) soft constraint.
DP3D is trained in a weakly supervised manner, in the sense that our pipeline (including DensePose training) does not require 3D annotations for the input images. In an end-to-end fashion, we train a deep pose regressor that, given a
DensePose map extracted from an image, predicts the shape deformation parameters, poses the mesh accordingly, and minimises the distance between the projection of the posed mesh to the image plane and the input 2D DensePose an-notations. We show that our method does not need manual
DensePose annotations for the training images; it can learn even from the predictions of a DensePose model trained on a different dataset. This way, DP3D can learn to infer the shape of humans and animals from an unconstrained dataset containing diverse poses. Since DP3D does not use images directly but only the DensePose annotations or predictions, it is robust to changes in the object appearance statistics, which makes it suitable for transfer learning.
We conduct experiments on a synthetic dataset of hu-man poses, and on the popular Human 3.6M benchmark, showing that the model trained on staged Human 3.6M generalises to a more natural 3DPW dataset. We also ﬁt the models to animal categories in the LVIS dataset. Note that learning reconstruction of LVIS animals would be im-possible with any method requiring 3D supervision since there are no scans or parametric models available for species like bears or zebras. DP3D produces more accurate recon-structions than a state-of-the-art Non-rigid Structure-from-Motion (NR-SfM) baseline and compares favourably with fully-supervised approaches. 2.