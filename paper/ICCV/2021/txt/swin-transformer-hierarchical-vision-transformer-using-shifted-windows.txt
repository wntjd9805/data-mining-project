Abstract
This paper presents a new vision Transformer, called
Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting
Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efﬁciency by lim-iting self-attention computation to non-overlapping local windows while also allowing for cross-window connection.
This hierarchical architecture has the ﬂexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Trans-former make it compatible with a broad range of vision tasks, including image classiﬁcation (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on
COCO, and +3.2 mIoU on ADE20K, demonstrating the po-tential of Transformer-based models as vision backbones.
The hierarchical design and the shifted window approach also prove beneﬁcial for all-MLP architectures. The code and models are publicly available at https://github. com/microsoft/Swin-Transformer. 1.

Introduction
Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Beginning with
AlexNet [35] and its revolutionary performance on the
ImageNet image classiﬁcation challenge, CNN architec-*Equal contribution. †Interns at MSRA. ‡Contact person.
Figure 1. (a) The proposed Swin Transformer builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local win-dow (shown in red). It can thus serve as a general-purpose back-bone for both image classiﬁcation and dense recognition tasks. (b) In contrast, previous vision Transformers [19] produce fea-ture maps of a single low resolution and have quadratic compu-tation complexity to input image size due to computation of self-attention globally. tures have evolved to become increasingly powerful through greater scale [27, 69], more extensive connections [31], and more sophisticated forms of convolution [64, 17, 75]. With
CNNs serving as backbone networks for a variety of vision tasks, these architectural advances have led to performance improvements that have broadly lifted the entire ﬁeld.
On the other hand, the evolution of network architectures in natural language processing (NLP) has taken a different path, where the prevalent architecture today is instead the
Transformer [58]. Designed for sequence modeling and transduction tasks, the Transformer is notable for its use of attention to model long-range dependencies in the data.
Its tremendous success in the language domain has led re-searchers to investigate its adaptation to computer vision, where it has recently demonstrated promising results on cer-tain tasks, speciﬁcally image classiﬁcation [19] and joint vision-language modeling [43].
In this paper, we seek to expand the applicability of
Transformer such that it can serve as a general-purpose backbone for computer vision, as it does for NLP and as CNNs do in vision. We observe that signiﬁcant chal-lenges in transferring its high performance in the language domain to the visual domain can be explained by differ-ences between the two modalities. One of these differ-ences involves scale. Unlike the word tokens that serve as the basic elements of processing in language Trans-formers, visual elements can vary substantially in scale, a problem that receives attention in tasks such as object de-tection [38, 49, 50].
In existing Transformer-based mod-els [58, 19], tokens are all of a ﬁxed scale, a property un-suitable for these vision applications. Another difference is the much higher resolution of pixels in images com-pared to words in passages of text. There exist many vi-sion tasks such as semantic segmentation that require dense prediction at the pixel level, and this would be intractable for Transformer on high-resolution images, as the compu-tational complexity of its self-attention is quadratic to im-age size. To overcome these issues, we propose a general-purpose Transformer backbone, called Swin Transformer, which constructs hierarchical feature maps and has linear computational complexity to image size. As illustrated in
Figure 1(a), Swin Transformer constructs a hierarchical rep-resentation by starting from small-sized patches (outlined in gray) and gradually merging neighboring patches in deeper
Transformer layers. With these hierarchical feature maps, the Swin Transformer model can conveniently leverage ad-vanced techniques for dense prediction such as feature pyra-mid networks (FPN) [38] or U-Net [47]. The linear compu-tational complexity is achieved by computing self-attention locally within non-overlapping windows that partition an image (outlined in red). The number of patches in each window is ﬁxed, and thus the complexity becomes linear to image size. These merits make Swin Transformer suit-able as a general-purpose backbone for various vision tasks, in contrast to previous Transformer based architectures [19] which produce feature maps of a single resolution and have quadratic complexity.
A key design element of Swin Transformer is its shift of the window partition between consecutive self-attention layers, as illustrated in Figure 2. The shifted windows bridge the windows of the preceding layer, providing con-nections among them that signiﬁcantly enhance modeling power (see Table 4). This strategy is also efﬁcient in re-gards to real-world latency: all query patches within a win-dow share the same key set1, which facilitates memory ac-cess in hardware. In contrast, earlier sliding window based self-attention approaches [30, 46] suffer from low latency on general hardware due to different key sets for different 1The query and key are projection vectors in a self-attention layer.
Figure 2. An illustration of the shifted window approach for com-puting self-attention in the proposed Swin Transformer architec-In layer l (left), a regular window partitioning scheme is ture. adopted, and self-attention is computed within each window. In the next layer l + 1 (right), the window partitioning is shifted, re-sulting in new windows. The self-attention computation in the new windows crosses the boundaries of the previous windows in layer l, providing connections among them. query pixels2. Our experiments show that the proposed shifted window approach has much lower latency than the sliding window method, yet is similar in modeling power (see Tables 5 and 6). The shifted window approach also proves beneﬁcial for all-MLP architectures [56].
The proposed Swin Transformer achieves strong perfor-mance on the recognition tasks of image classiﬁcation, ob-ject detection and semantic segmentation.
It outperforms the ViT / DeiT [19, 57] and ResNe(X)t models [27, 64] sig-niﬁcantly with similar latency on the three tasks. Its 58.7 box AP and 51.1 mask AP on the COCO test-dev set sur-pass the previous state-of-the-art results by +2.7 box AP (Copy-paste [23] without external data) and +2.6 mask AP (DetectoRS [42]). On ADE20K semantic segmentation, it obtains 53.5 mIoU on the val set, an improvement of +3.2 mIoU over the previous state-of-the-art (SETR [73]). It also achieves a top-1 accuracy of 87.3% on ImageNet-1K image classiﬁcation.
It is our belief that a uniﬁed architecture across com-puter vision and natural language processing could beneﬁt both ﬁelds, since it would facilitate joint modeling of vi-sual and textual signals and the modeling knowledge from both domains can be more deeply shared. We hope that
Swin Transformer’s strong performance on various vision problems can drive this belief deeper in the community and encourage uniﬁed modeling of vision and language signals. 2.