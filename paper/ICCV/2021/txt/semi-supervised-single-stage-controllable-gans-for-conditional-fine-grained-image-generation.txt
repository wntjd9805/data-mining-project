Abstract
Previous state-of-the-art deep generative models im-prove fine-grained image generation quality by design-ing hierarchical model structures and synthesizing images across multiple stages. The learning process is typical-ly performed without any supervision in object categories.
To address this issue, while at the same time to alleviate the level of complexity of both model design and train-ing, we propose a Single-Stage Controllable GAN (SSC-GAN) for conditional fine-grained image synthesis in a semi-supervised setting. Considering the fact that fine-grained object categories may have subtle distinctions and shared attributes, we take into account three factors of vari-ation for generative modeling: class-independent content, cross-class attributes and class semantics, and associate them with different variables. To ensure disentanglement among the variables, we maximize mutual information be-tween the class-independent variable and synthesized im-ages, map real data to the latent space of a generator to perform consistency regularization of cross-class attributes, and incorporate class semantic-based regularization into a discriminator’s feature space. We show that the proposed approach delivers a single-stage controllable generator and high-fidelity synthesized images of fine-grained categories.
SSC-GAN establishes state-of-the-art semi-supervised im-age synthesis results across multiple fine-grained datasets. 1.

Introduction
Deep generative learning [6, 21, 22, 23, 26, 7] has gained a wide range of research interests due to the high capaci-ty of the generative models in learning complex data dis-tributions. Most of them are based on generative adver-∗Corresponding author.
Figure 1. The representative images are synthesized by SSC-GAN in semi-supervised (top row) and fully supervised (middle row) settings on the CUB dataset [42]. Although only half of training data are labeled in the semi-supervised case, the synthesis quality of SSC-GAN is comparable to that of the model with full supervi-sion, and can be close to the quality of real data (bottom row). sarial networks (GANs) [17] and variational autoencoders (VAEs) [27]. Unsupervised or supervised training patterns are typically adopted to achieve remarkable success in im-age synthesis [16, 43, 29, 44, 11, 12, 1, 2]. However, the resulting generators are either unable to control class se-mantics or require massive labeled samples. To address this issue, semi-supervised generative learning has been studied
[13, 25, 31, 33, 40]. Generic semi-supervised generative modeling is based on the assumption that the amount of un-labeled data is adequate. This does not hold when learning on fine-grained data, due to the reason that both data acqui-sition and annotation may be expensive and require exten-sive expertise.
Training high-fidelity generators for fine-grained objec-t categories is inherently challenging [48, 47, 49], due to the difficulties in the following aspects: On the one hand, both training samples and labels are insufficient; on the oth-er hand, the distinctions among different categories can be
Figure 2. The model structure of SSC-GAN for fine-grained image generation. Generative modeling is performed based on a class-independent variable b, a cross-class variable z and a class variable yz. An encoder M is incorporated to map images into the latent space of a generator G via adversarial training with a discriminator Dz. On the other hand, the code M (x) is used to synthesize a new image
˜x, and z is associated with cross-class attributes by requiring x and ˜x to have similar content independent of b and yz. In order to make the most of the unlabeled data, an additional discriminator Dx b is incorporated to distinguish real images from fake ones without the condition of class labels, while at the same time to maximize the mutual information between b and the synthesized images. As a result, b is associated with class-independent content. Further, we impose regularization on the feature space of a class-conditional discriminator
Dx y to enhance class separability, which is beneficial for class-conditional distribution matching between real and synthesized data. subtle. To induce a generator to capture the underlying fac-tors which give rise to fine-grained data, previous works
[5, 32, 37, 18, 30, 20, 10] adopt hierarchical model struc-tures, and the image generation process consists of multiple stages. To make the generation controllable, different vari-ables are incorporated in different stages to associate with the discovered factors. The model and training complexity can be extremely high. In addition, object-level annotation-s are usually needed for background and mask generation.
More importantly, there is no attempt so far to explicitly control class semantics in fine-grained image synthesis. In this work, we explore an effective way to model the fac-tors of variation without any object-level annotations, while performing class-conditional image generation with limited supervision as shown in Figure 1.
More specifically, we aim to perform semi-supervised class-conditional generative modeling for fine-grained ob-ject categories, while at the same time the factors of vari-ations are encoded for generation controllability. We pro-pose a Single-Stage Controllable GAN (SSC-GAN), which learns to synthesize high-fidelity fine-grained images in semi-supervised scenarios. To achieve this goal, fine-grained images are synthesized conditioned on a class-independent variable, a cross-class variable and a class vari-able. Considering the inadequate amount of training data and labels, the disentanglement of these variables is im-portant for a generator to capture class semantics, and we thus improve a generic semi-supervised GAN-based model in the following three aspects. First, we incorporate an addi-tional discriminator to impose marginal distribution match-ing between real and synthesized data, while at the same time to maximize the mutual information between the class-independent variable and synthesized images. Second, we leverage an encoder to map images into the latent space of a generator, and generate new images by changing the val-ues of class-independent and class variables. The gener-ator is induced to learn cross-class attributes by minimiz-ing the differences of the latent codes of the original and resulting images. Third, a class label-embedded discrimi-nator is often used for class-conditional distribution align-ment. However, the discriminator’s features are not neces-sarily effective for reflecting the distinctions between fine-grained categories. To guide the generator to capture what the class variable essentially represents, we further regular-ize the discriminator’s feature space. The model structure of SSC-GAN is illustrated in Figure 2.
We summarize the main contributions of this work as follows: (1) We propose a semi-supervised GAN-based generative model, SSC-GAN, which is single-stage and controllable for conditional fine-grained image generation. (2) Generative modeling is performed based on a class-independent variable, a cross-class variable and a class vari-able, which are disentangled by incorporating effective reg-ularizers accordingly without requiring any object-level an-notations. (3) An effective solution can be obtained for the optimization problem without heavy tuning. 2.