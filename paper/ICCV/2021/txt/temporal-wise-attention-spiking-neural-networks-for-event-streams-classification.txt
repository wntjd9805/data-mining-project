Abstract
How to effectively and efficiently deal with spatio-temporal event streams, where the events are generally sparse and non-uniform and have the µs temporal reso-lution, is of great value and has various real-life appli-cations.
Spiking neural network (SNN), as one of the brain-inspired event-triggered computing models, has the potential to extract effective spatio-temporal features from the event streams. However, when aggregating individual events into frames with a new higher temporal resolution, existing SNN models do not attach importance to that the se-rial frames have different signal-to-noise ratios since event streams are sparse and non-uniform. This situation inter-feres with the performance of existing SNNs. In this work, we propose a temporal-wise attention SNN (TA-SNN) model to learn frame-based representation for processing event streams. Concretely, we extend the attention concept to temporal-wise input to judge the significance of frames for the final decision at the training stage, and discard the ir-relevant frames at the inference stage. We demonstrate that
TA-SNN models improve the accuracy of event streams clas-sification tasks. We also study the impact of multiple-scale temporal resolutions for frame-based representation. Our approach is tested on three different classification tasks: gesture recognition, image classification, and spoken digit recognition. We report the state-of-the-art results on these tasks, and get the essential improvement of accuracy (al-most 19%) for gesture recognition with only 60 ms. 1.

Introduction
Dynamic vision sensors (DVS)[20, 28] pose a new paradigm shift by using sparse and asynchronous events to represent visual information. Unlike the conventional cameras, which produce fixed low-rate synchronized frames (typically less than 60 frames per second), DVS cameras
∗ Equal contribution.
† Corresponding authors.
Figure 1. Our proposed model use the TA to judge the significance of frames at the training stage and discard the irrelevant frames at the inference stage. The sample is from the DVS128 Gesture dataset. The green and cyan colors denote the On and Off channels which correspond to brightness increase and decrease, respectively (more details of event streams in section 3.1). encode the time, location, and polarity of the brightness changes for each pixel at an extremely high event rate (1M to 1G events per second ), and exhibit advantages mainly in three aspects[11, 30]. Firstly, DVS cameras require much less resource, as the events are sparse and only triggered when the intensity changes. Secondly, the µs temporal res-olution (TR) of DVS can avoid motion blur by produc-ing high-rate events. Thirdly, DVS cameras have a high dynamic range (140dB vs. 60dB of conventional cam-eras), which makes them able to acquire information from challenging illumination conditions. These characteristics bring superiorities over conventional cameras when orient-ing to visual tasks which need low latency, low power con-sumption, and stability for variant illumination, which have been used in high-speed object tracking[30], autonomous driving[4], SLAM[7], low-latency interaction[1], etc.
However, we have observed that the event streams
recorded by DVS cameras are usually redundant in the temporal dimension, which is caused by high TR and ir-regular dynamic scene changes. This characteristic makes event streams almost impossible to process directly through deep neural networks (DNNs), which are based on dense computation.
Compromising on this, additional data preprocessing[2, 33, 11] is required and inevitably dilutes the advantages of low-latency and power-saving of events.
Inspired by the working pattern of the mammalian visual cortex[11], spiking neural networks (SNNs) have a unique event-triggered computation characteristic that can respond to the events in a nearly latency-free and power-saving way[31, 25, 11], and it is naturally fit for processing event.
However, due to the lack of training technology, the per-formance of deep SNNs has become the biggest obstacle to their application. During experiments, we find that there is still a lot of optimizing room for SNNs to process the event data more efficiently and effectively. That’s why we intro-duce the attention mechanism into SNNs.
In this work, we propose the temporal-wise attention
SNNs (TA-SNNs) by extending the attention concept to temporal-wise input to automatically filter out the irrele-vant frames for the final decision. For TA-SNNs, how to implement the attention mechanism while retaining the event-triggered characteristic is the primary consideration.
Classic attention methods, such as self-attention[36], are hard to use because the change of network connection de-stroys the event-triggered characteristic in SNNs. Inspired by squeeze-and-excitation (SE) block[13], we design the
TA module to obtain the statistical features of events at dif-ferent times, generate the attention scores and then weigh the events by the scores. At the same time, we propose a data augmentation method called random consecutive slice (RCS) to utilize the event data. In order to keep the event-encoded data characteristics, we then use binary attention scores at the inference stage with a threshold in the TA mod-ule, which is termed as input attention pruning (IAP) and obtains an unchanged or even higher accuracy with RCS.
Without losing generality, we test our approach with two kinds of SNN models, i.e., leaky integrate-and-fire (LIF) and leaky integrate-and-analog-fire (LIAF), on three kinds of tasks: gesture recognition, image classification, and spo-ken digit recognition. We report the state-of-the-art results on these tasks in long-term TRs, and get the essential im-provement of accuracy (almost 19%) for gesture recogni-tion with low-latency and power-saving property.
We summarize our contributions as follows: 1) We propose the TA-SNNs for event streams that can undertake the end-to-end training and inference tasks with low latency, low power consumption, and high performance. To the best of our knowledge, this is the first work to introduce temporal-wise attention into
SNNs. 2) We propose the IAP method for SNNs and get similar or even better performance compared with those us-ing full inputs (see Fig.4). The IAP brings a crucial power-saving significance for SNNs and other event-based algorithms. 3) Inspired by the data augmentation method in video recognition[35] and overlap method for event stream process[21], we introduce the RCS method to make full use of the sampled data. 2.