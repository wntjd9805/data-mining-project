Abstract
Solving geometric tasks involving point clouds by us-ing machine learning is a challenging problem. Standard feed-forward neural networks combine linear or, if the bias parameter is included, affine layers and activation func-tions. Their geometric modeling is limited, which motivated the prior work introducing the multilayer hypersphere per-ceptron (MLHP). Its constituent part, i.e., the hypersphere neuron, is obtained by applying a conformal embedding of Euclidean space. By virtue of Clifford algebra, it can be implemented as the Cartesian dot product of inputs and weights. If the embedding is applied in a manner consistent with the dimensionality of the input space geometry, the de-cision surfaces of the model units become combinations of hyperspheres and make the decision-making process geomet-rically interpretable for humans. Our extension of the MLHP model, the multilayer geometric perceptron (MLGP), and its respective layer units, i.e., geometric neurons, are consistent with the 3D geometry and provide a geometric handle of the learned coefficients. In particular, the geometric neuron ac-tivations are isometric in 3D, which is necessary for rotation and translation equivariance. When classifying the 3D Tetris shapes, we quantitatively show that our model requires no activation function in the hidden layers other than the em-bedding to outperform the vanilla multilayer perceptron. In the presence of noise in the data, our model is also superior to the MLHP. 1.

Introduction
Understanding the geometry of a neuron is a crucial pre-requisite to successfully performing geometric deep learn-ing [4]. Owing to their inherent connection to the notion of distance, circles (or spheres) are fundamental atomic struc-tures for defining geometric constraints. Note, e.g., how the third corner of a triangle is constrained by specifying the radii of two circles centered in each of the other two corners.
This motivates us to consider geometries of decision sur-faces beyond hyperplanes, such as hyperspheres, in order to properly represent isometries.
Going beyond planar decision surfaces to non-planar ones increases the complexity of the model, but this has proved to be beneficial performance-wise [5, 3, 14]. To construct and represent such surfaces, one needs to consider more general spaces, in which case, Klein geometries [20] provide a useful theoretical framework.
In this paper, we employ Clifford algebra as a tool to perform computations in conformal geometry. Building on top of the previous works on Clifford neurons [6] and spherical decision surfaces [18], we explore the multilayer hypersphere perceptron (MLHP) [2] model applied to the problem of classifying the 3D Tetris shapes, essentially a collection of point clouds, see Section 5.1 and Fig. 3. To the best of our knowledge, MLHP has not been previously investigated in this context. We focus specifically on 3D ge-ometry, which is important for tasks such as pose estimation, which in turn is a prerequisite for grasping, 3D inpainting, and augmented reality.
Striving to make the decision-making process more intu-itive and be consistent with the dimensionality of the input space geometry, we perform the conformal embedding in a
Minkowski space. Consequently, we observe that the deci-sion surfaces of the MLHP units become combinations of hyperspheres. We call such an extension of the MLHP model the multilayer geometric perceptron (MLGP) and refer to its respective layer units as geometric neurons. Owing to the homogeneous representation [8], we provide an inter-pretation of our model parameters directly in the Euclidean space, which is only intuitive if the embedding agrees with the input geometry.
We summarize our contributions as follows: (a) We demonstrate how spherical decision surfaces im-prove the understanding of the decision-making process of neural networks, provided that their construction is done adhering to the input space geometry. (b) We propose an extension to the MLHP, the MLGP model1 with geometric neurons, and show that, apart from being more geometrically explainable, it achieves favorable quantitative results when classifying the 3D
Tetris shapes, and is superior when they are perturbed. 1The code is available at github.com/pavlo-melnyk/mlgp-embedme.
Figure 1. The proposed (feed-forward) MLGP model — our modification of the baseline MLHP. The embedding of the input array is performed point-wise and at each subsequent layer vector-wise: the first embedding term (yellow) is always set to 1, the second (blue) is 2. Since the embedding of the model input is point-wise, the hidden layer the scaled magnitude of the vector being embedded, i.e., x
|| consists of geometric neurons that represent combinations of hyperspheres. The output layer consists of hypersphere neurons [3]. 1 2 ||
−
− (c) By introducing a matrix operator, we derive the iso-morphism between the sandwich product of the motor (rotation followed by translation) with a general geomet-ric object in the conformal ME3
R3+1,1 space (see
Section 3.1 for notation) and the corresponding matrix– vector product in the Euclidean R5 space. Using this result, we prove that the geometric neuron activations are isometric in R3 by construction, which is necessary for rotation and translation equivariance. We further demonstrate it experimentally.
≡ 2.