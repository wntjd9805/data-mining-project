Abstract other generative methods.
Recently, some works found an interesting phenomenon that adversarially robust classiﬁers can generate good im-ages comparable to generative models. We investigate this phenomenon from an energy perspective and provide a novel explanation. We reformulate adversarial example generation, adversarial training, and image generation in terms of an energy function. We ﬁnd that adversarial train-ing contributes to obtaining an energy function that is ﬂat and has low energy around the real data, which is the key for generative capability. Based on our new understanding, we further propose a better adversarial training method,
Joint Energy Adversarial Training (JEAT), which can gen-erate high-quality images and achieve new state-of-the-art robustness under a wide range of attacks. The Inception
Score of the images (CIFAR-10) generated by JEAT is 8.80, much better than original robust classiﬁers (7.50). In par-ticular, we ﬁnd that the robustness of JEAT is better than other hybrid models. 1.

Introduction
Adversarial training can improve the robustness of a classiﬁer against adversarial perturbations imperceptible to humans. Unlike a normal classiﬁer, an adversarially robust classiﬁer can generate good images by gradient descending the cross-entropy loss from random noises. Recently, some works discovered this phenomenon, and the quality of gen-erated images is even comparable to GANs [21, 7]. The generative capability of an adversarially robust model from a classiﬁcation task is interesting and surprising. However, it is unclear why an adversarially trained classiﬁer can gen-erate natural images. As image generation is a crucial topic, understanding the generative capability of adversarially ro-bust classiﬁers could be inspiring and can give hints to many
*This work was done when he was a research intern in Huawei Noah’s
Ark Lab. Email: ee zhuy@zju.edu.cn.
†Corresponding to: Jiacheng Sun (sunjiacheng1@huawei.com)
In this paper, we aim to understand the generative capa-bility of an adversarially trained classiﬁer and further im-prove the quality of generated images from the energy per-spective. For Energy-Based Model (EBM) [10], it ﬁrst gen-erates low energy samples from random noises, then in-creases the energy of generated samples by updating model
In this way, EBM can obtain a good energy parameters. function that is smooth and has low energy near the real data, as illustrates in Fig. 1(a). Thus, EBM can generate good images by sampling with Langevin Dynamics with the good energy function [5].
We show that adversarially trained classiﬁers can also obtain such a good energy function, which is ﬂat and has low energy near the real data. This implies the generative capability of the adversarially robust classiﬁer. For a clas-siﬁer, we can deﬁne energy functions on the output logits.
We reformulate the original adversarial training and image generation in terms of energy functions. In fact, the adver-sarial examples are high-energy samples near the real data, and adversarial training is trying to decrease the energy of these examples by updating model parameters. This proce-dure can also help us to learn a ﬂat energy function with low energy near the real data, as illustrated in Fig. 1(b). More-over, based on our understanding, we ﬁnd another interest-ing phenomenon that a normal classiﬁer is able to generate images if we add random noise to images during training.
Though injecting noises to training data can generate high-energy samples, the perturbation direction may not be ef-ﬁcient compared with the adversarial attack. Thus, larger noise is needed in training for generating high-energy sam-ples.
The cross-entropy loss can be expressed as the differ-ence between Eθ(x, y) and Eθ(x). We show that Eθ(x, y), which is ﬂat and has lower energy around the real data, plays a key role in the conditional generation task. How-ever, making the energy Eθ(x, y) ﬂat and low near the real data is just a by-product of original adversarial training.
Thus we propose Joint Energy Adversarial Training (JEAT)
Figure 1. EBM training and adversarial training make the energy functions smooth near the real data in different ways. (a) EBM starts from random samples, then moves along the direction of energy descent for ﬁxed steps to obtain low energy samples. During optimization, the energy of these points is increased. Thus EBM obtains a smooth energy region around real data. (b) Adversarial examples are high-energy samples around real data. Adversarial training decreases the energy of adversarial examples by updating model parameters, thus obtains a
ﬂat region around real data. to directly optimize the Eθ(x, y). We generate adversarial examples by directly increasing the Eθ(x, y) as Eq. (18) and update model parameters by maximizing the likelihood of joint distribution pθ(x, y) as Eq. (19). We show that
JEAT further improves the quality of the generated images.
The main contributions of our paper are summarized be-low: where sign is the sign function. PGD attack is a kind of iterative variant of FGSM which generates an adversarial sample starting from a random position in the neighborhood of the clean image. PGD can be formulated as: (cid:26) xn = xn−1 + η · sign(∇xn−1 L(xn−1, y; θ)), xn = clip(xn, x0 − (cid:15), x0 + (cid:15)), (3)
• We propose a novel explanation of the image gener-ation capability of a robust classiﬁer from an energy perspective.
• We ﬁnd the generative capability of normal classiﬁers with injecting noise in the training process and explain it from the energy point of view.
• We propose a training algorithm JEAT from an energy perspective that improves image generative capability. 2. Preliminaries 2.1. Adversarial Training
Adversarial training, ﬁrst proposed in [8], can effectively defend against adversarial attacks by solving a bi-level min-max optimization problem [18]. It can be formulated as: (cid:26) minθ E(x,y)∼D[L(x + δ∗, y; θ)],
δ∗ = arg max(cid:107)δ(cid:107)p≤(cid:15) L(x + δ, y; θ), (1) where y is the ground-truth label of the input x, δ∗ denotes the adversarial perturbation added to x, L denotes the loss function, (cid:107)·(cid:107)p denotes the (cid:96)p-norm that constrains the per-turbated sample in an (cid:96)p-ball with radius (cid:15) centered at x.
The (cid:96)∞ adversarial perturbation is usually approxi-mately solved by the fast gradient sign method (FGSM) [8] and projected gradient descent (PGD) [18]. For FGSM at-tacks δx takes the form:
δx = (cid:15) sign(∇xL(x, y; θ)), (2) where x0 is a clean image and η is the perturbation step. 2.2. Image Generation of Robust Model
Generating images with an adversarially trained classi-ﬁer is ﬁrst raised by Santurkar, Madry, etc.
[20] where they demonstrate that a robust classiﬁer alone sufﬁces to tackle various image synthesis tasks such as generation. For a given label y, image generation minimizes the loss L of label y by: x(cid:48) = x − η 2 · ∇xL(x, y; θ) +
√
η(cid:15). (4)
Starting from sample x0 ∼ N (µy, Σy) where µy =
Ex∼Px|y (x) and Σy = Ex∼Px|y ((x − µy)T (x − µy)), we can obtain a better image by minimizing the loss L. Some simple improvements such as choosing a more diverse dis-tribution to start with could further improve the quality of generated images [20]. However, it is not the focus of this paper. 2.3. Energy Based Model
Energy Based Models [16, 10] show that any probability density p(x) for x can be expressed as pθ(x) = exp(−Eθ(x))
Zθ
, (5) where Eθ(x) represents the energy of x and is modeled by neural network, Zθ = (cid:82) exp(−Eθ(x))dx is the normal-izing factor parameterized by θ also known as the parti-tion function. The optimization of the energy-based model
Figure 2. Energy contour of different models. (a) The energy Eθ(x, y) contour of a naturally trained model; (b) The energy contour of an adversarially trained model; (c) The energy contour of PreJEAT (ours); (d) The energy contour of JEAT (ours). Darker colors in the plots represent lower energy values. The blue star point in the center of each ﬁgure is a chosen real image from CIFAR-10. We perturb the image in two random directions to get the energy landscape. is through maximum likelihood learning by minimizing
LM L(θ) = Ex∼PD [− log pθ(x)]. The gradient is [5]:
∇θLM L(θ) = Ex+∼pD [∇θEθ(x+)]−Ex−∼pθ [∇θEθ(x−)]. (6)
Because sampling from pθ is not feasible for an EBM,
Langevin Dynamics is commonly used to approximately
ﬁnd samples from pθ [11]. Thus, EBM training usually con-tains two stages: approximately generating samples from pθ by Langevin Dynamics along the direction of energy de-scent and optimizing model parameters to increase the en-ergy of these samples and decrease the energy of real sam-ples by SGD. In this way, as illustrated in Fig. 1 (a), EBM could obtain a smooth energy function around real data and generate samples through Langevin Dynamics.
From energy perspective, we can also deﬁne pθ(x, y) as 3. Energy Perspective on Robust Classiﬁer 3.1. Energy Perspective on Adversarial Training
We denote f (·; θ) as a classiﬁcation neural network pa-rameterized by θ. Let x be a sample. Then f (x; θ)[k] repre-sents the kth output of the last layer and we deﬁne pθ(y|x) as: pθ(y|x) = exp(f (x; θ)[y]) k=1 exp(f (x; θ)[k]) (cid:80)n
, (11) which resembles Boltzmann distribution, and n represents total possible classes. From Eq. (8) and (11), we deﬁne two energy functions as follows: (cid:26) Eθ(x, y) = − log(exp(f (x; θ)[y])),
Eθ(x) = − log((cid:80)n k=1 exp(f (x; θ)[k])). (12) (7)
From Eq. (12), we have ˜Zθ = Zθ . Furthermore, if the clas-siﬁcation loss function is cross-entropy loss, it could also be expressed as: follows: pθ(x, y) = exp(−Eθ(x, y))
˜Zθ
, where ˜Zθ = (cid:82) (cid:80) y exp(−Eθ(x, y))dx. Thus we also get pθ(y|x) expressed by Eθ(x) and Eθ(x, y): pθ(y|x) = pθ(x, y) pθ(x)
= exp(−Eθ(x, y)) · Zθ exp(−Eθ(x)) · ˜Zθ
. (8) 2.4. Sampling with Langevin Dynamics
Using ∇x log(p(x)), Langevin Dynamics could generate samples from density distribution p(x). The process starts from an initial point ˜x0 from a prior distribution π and re-cursively updates ˜x by:
η 2
∇˜xt−1 log(p(˜xt−1)) +
˜xt = ˜xt−1 +
η(cid:15), (9)
√ where (cid:15) ∼ N (0, I) and η is a ﬁxed step size. When η → 0 and T → ∞, ˜xT is exactly an sample from p(x) under some condition [31]. If we want to sample from pθ(x, y) where y is a certain label, we could use Eq. (9) as well. Based on the energy framework as Eq. (7), we have ∇x log(pθ(x, y)) =
−∇xEθ(x, y). Hence the sampling process becomes:
˜xt = ˜xt−1 −
η 2
∇˜xt−1Eθ(˜xt−1, y) +
√
η(cid:15). (10)
L(x, y; θ) = Eθ(x, y) − Eθ(x). (13)
In original adversarial training as in [18, 32, 24], by Fast
Gradient Sign Method, adversarial example could be found by xadv = x + η · sign(∇x(Eθ(x, y) − Eθ(x))), (14) which is the direction of gradient ascent of loss deﬁned in
Eq. (13). The direction of adversarial perturbation relates to not only Eθ(x, y) but also Eθ(x). And in original adversar-ial training, the optimization process aims to decrease the loss in Eq. (13) by updating model parameters.
As we mentioned in Sec. 2.3, a good energy function
Eθ(x, y) which is smooth and has low energy around real data is the key factor for generating good images for a given label y. We deﬁne the change of energy Eθ(x, y) after ad-versarial attack and after optimization as: (cid:26) ∆xEθ(x, y) = Eθ(xadv, y) − Eθ(xori, y),
∆θEθ(x, y) = Eθupdated (xadv, y) − Eθ(xadv, y), (15)
Figure 3. We illustrate the changes of energy in original adversarial training [18] on CIFAR-10 in 50 epochs (model has converged). The center points of the tags represent the mean value and the lengths represent the variance. (a) Adversarial examples increase the energy
Eθ(x, y) as ∆xEθ(x, y)>0 during the training. The energy Eθ(x, y) of adversarial examples decrease after updating parameters as
∆θEθ(x, y)<0 during the training. (b) The value of ∆θEθ(x) ﬂuctuates around zero, sometimes positive and sometimes negative. Thus
Eθ(x) has not been well optimized in the classiﬁcation task. where Eθ(xadv, y) is the energy of xadv given label y before updating parameters, Eθ(xori, y) is the energy of xori given label y before updating parameters, and Eθupdated(xadv, y) is the energy of xadv given label y after updating parame-ters. The deﬁnition of ∆xEθ(x) and ∆θEθ(x) are similar in (15) with removing y.
As illustrated in Fig. 3 (a), adversarial attacks generate high-energy adversarial examples, and then the energy of adversarial examples is decreased by updating model pa-rameters. Compared to EBM, adversarial training ﬂattens the energy region around real data in a way different from
EBM training. Adversarial training ﬁnds adversarial exam-ples with high energy Eθ(x, y) near the data and then de-creases the energy of those samples by updating model pa-rameters. Noted that, in EBM, sampling from pθ(x, y) by
Langevin Dynamics follows the direction of energy descent, which is the opposite direction of adversarial examples. As illustrated in Fig. 1 (b), adversarial training can obtain a ﬂat energy function around real data.
We illustrate energy Eθ(x, y) contours in Fig. 2 for a nor-mal classiﬁer and an adversarially trained classiﬁer. From
Fig. 2 (a), the energy function around the center is sharp for a normal classiﬁer, and the energy of the real data is high. Because the low-energy region deviates from the cen-ter in a normal classiﬁer, the generated images along the direction of energy descent are likely to fall into a region far away from the real data, which would not have good quality. For a robust classiﬁer, the energy function near the center is smoother and lower, as shown in Fig. 2 (b).
Image generation by a robust classiﬁer is introduced in
Sec. 2.2, if we transform the loss function to energy ex-pression as Eq. (13), the generating procedure of images becomes: x(cid:48) = x − η 2 · ∇x(Eθ(x, y) − Eθ(x)) +
√
η(cid:15), (16) where (cid:15) ∼ N (0, I) and η is step size. The term Eθ(x, y) −
Eθ(x) in Eq. (16) implies that the generation iterations are related to both the energy of Eθ(x, y) and the energy of Eθ(x). By exploring low energy region of Eθ(x, y), which corresponds to high probability pθ(x, y) region given (7), we could ob-constant normalizing factor from Eq. tain a good sample from high probability pθ(x, y) region.
Minimizing Eθ(x, y) contributes to maximizing pθ(x, y) to generate images corresponding to the label y as shown in
Eq. (10), but the energy of Eθ(x) is irrelevant to label y.
As illustrated in Fig. 3 (b), Eθ(x) has not been well opti-mized in the classiﬁcation task, which may introduce label-independent noise. Thus, Eθ(x) may be a factor restrict-ing the generative capability of the robust classiﬁer, and we could drop Eθ(x) while only using Eθ(x, y) as: x(cid:48) = x −
η 2
· ∇x(Eθ(x, y)) +
√
η(cid:15). (17)
Eq. (17) is closely relate to sampling from Langevin Dy-namics as Eq. (10). Approaching low energy region of
Eθ(x, y) is equivalent to approaching the high probability region of pθ(x, y). As shown in Fig. 4(a) and (b), using
Eq. (17) gives better generated images than using Eq. (16).
Figure 4. Given the label ship, the images generated by different methods. (a) shows ten images generated by original robust clas-siﬁer using Eq. (16). (b) shows ten images generated by original robust classiﬁer using Eq. (17). (c) shows ten images generated by robust classiﬁer trained with adversarial examples got by Eq. (18) using Eq. (17). 3.2. Endowing Generative Capability to Normal
Classiﬁer
As analyzed before, we can generate high-energy sam-ples from real data and reduce the energy of these samples during the optimization process to obtain a classiﬁer with generative capability. The process of optimizing the energy
Eθ(x, y) to be ﬂat and low around real data contributes to generating good images. Adversarial training follows the
Table 1. Normal attack and energy attack which just increases
Eθ(x, y) on normal classiﬁer (WideResNet-28-10). The pertur-bation radius is 8/255. They perform similarly on the CIFAR-10 and CIFAR-100 datasets and they can successfully attack the clas-siﬁer.
Attack
No Attack
Normal Attack
Energy Attack
CIFAR-10 CIFAR-100 94.39% 0.12% 0.11% 78.54% 0.08% 0.10%
Eq. (17) after training as Preliminary Joint Energy Adver-sarial Training (PreJEAT).
As we illustrate in Fig. 2, in a PreJEAT trained classiﬁer, the energy around the natural image (center) is ﬂatter and lower than a normal classiﬁer and original robust classiﬁer.
In a robust classiﬁer, the low energy region deviates from the center. But in the PreJEAT trained classiﬁer, the natu-ral image has the lowest energy. Thus, directly generating adversarial examples by Eθ(x, y) contributes to obtaining a
ﬂat and low energy function near the real data. By adver-sarial training with Eq. (18) we can obtain a robust classiﬁer with better generative capability, as shown in Fig. 4(c). 3.4. Joint Energy Adversarial Training
We veriﬁed that a PreJEAT trained model has good gen-erative capability in the previous section. However, there is still a discrepancy between training loss objective as Eq. (13) and adversarial training as Eq. (18) in PreJEAT. And we also ﬁnd that the images generated by PreJEAT are not smooth enough in Fig. 4(c). We hope that the optimiza-tion process also optimizes Eθ(x, y) more directly to re-duce the energy around real data. Hence we propose a new algorithm, Joint Energy Adversarial Training (JEAT), in Al-gorithm 1 to improve the generative capability of a classi-ﬁer. In JEAT, we replace cross-entropy loss − log pθ(y|x) in PreJEAT with :
− log pθ(x, y) = − log pθ(y|x) − log pθ(x), (19) where pθ(x) is deﬁned in Eq. (5). Optimizing pθ(x, y) helps to get better Eθ(x, y) as shown in Eq. (7). The gradi-ent of log pθ(x) is
∇θlog(pθ(x)) = −∇θEθ(x) − Epθ(x)[∇θEθ(x)]. (20)
We use Stochastic Gradient Langevin Dynamics (SGLD) to approximate Epθ(x) [6].
JEAT uses Eq. (18) to ﬁnd adversarial example and
Eq. (17) to generate image like PreJEAT. With our proposed
JEAT, adversarial examples, adversarial training, and im-age generation are connected to the energy Eθ(x, y) in a clear way. We also plot the energy contour of Eθ(x, y) in
Fig. 2(d). Compared to the other three models, the energy function of a JEAT trained classiﬁer near the center is the
Figure 5. The images generated by the models injected different strengths of noise in the training process. The clean accuracy of these models is almost the same. We use δ to denote the noise added to training data. (a) δ ∼ N (0, 24/255). Obviously, this (b) δ ∼ N (0, 16/255), model produces recognizable images. the shape of ship is visible. (c) δ ∼ N (0, 8/255), it is hard to recognize the shape of objects. procedure and provides an effective way to ﬁnd high-energy samples with small perturbations. Meanwhile, a simple ran-dom noise could also ﬁnd high-energy samples but less ef-fectively.
To verify our claim, we train normal classiﬁers by in-jecting different strengths of random noise into training data. As shown in Fig. 5, classiﬁers trained with differ-ent strengths of noise generate images of different qualities through the procedure in Sec. 3.1. Though the quality is not good enough, cars, birds and ships can be recognized with high conﬁdence in Fig. 5 (a). From the comparison of images generated by adding different strengths of noise, we
ﬁnd that stronger random noise is needed to train a normal classiﬁer in order to generate recognizable images. This is consistent with our explanation above that random noise is a “blind” adversarial direction to ﬁnd high-energy samples.
Because the direction is uncertain and very likely not the di-rection to efﬁciently increase energy, stronger perturbations may be needed. This experiment also shows that the process which generates high-energy samples from real data and re-duces the energy of these samples during the optimization contributes to obtaining a classiﬁer with generative capabil-ity. 3.3. Generating Adversarial Examples by Energy
As mentioned above, Eθ(x, y) determines the generative capability of a robust classiﬁer. In original adversarial train-ing, it does not increase the energy Eθ(x, y) of original data directly. In fact, only using Eθ(x, y) can also make x to be an adversarial sample xadv as shown in Tab. 1.
Using Eθ(x, y) only in Eq. (14), we have: xadv = x + η · sign(∇x(Eθ(x, y))). (18)
Adversarial examples generated by energy attack in
Eq. (18) have almost the same attack effect with the orig-inal normal attack by Eq. (14). The difference between x and xadv is unrecognizable, but their energy are quite dif-ferent. We name the method which trained with adversar-ial examples found by Eq. (18) and generates images by
Algorithm 1 Training and Generating of JEAT: Given net-work f , E(x, y) = − log(exp(f (x)[y])) represent energy of (x, y), E(x) = − log((cid:80) y exp(f (x)[y])) represent energy of x, (cid:96)∞ adversarial perturbation radius (cid:15), SGLD step-size α,
SGLD steps K, replay buffer B, reinitialization probility ρ, epochs T , dataset of size M , learning rate η. ation.
In terms of image generation, we compare our generated pictures with JEM [10] and robust classiﬁer [20]. We show the images generated by different algorithms and the scores of these methods based on Inception Score and Frechet In-ception Distance metrics.
Training: for i = 1, 2..., T do for j = 1, 2..., M do (cid:73) Generating energy-based adversarial samples:
δ = U(−(cid:15), (cid:15))
δ = δ + (cid:15) · sign(∇x(Eθ(x, y)))
δ = max(min(δ, (cid:15)), −(cid:15)) xadv = xj + δ (cid:73) Generating samples by SGLD:
˜x0 ∼ U(0, 1) with probability ρ, else ˜x0 ∼ B for t = 0, 1, 2, ..., K − 1 do
˜xt+1 = ˜xt − α 2 · ∇xtEθ(˜xt) +
α · N (0, I)
√ end for
Add ˜xK to B (cid:73) Updating model parameters:
∇θLp(y|xadv) = ∇θ(Eθ(xadv, y) − Eθ(xadv))
∇θLp(xadv) = ∇θ(Eθ(xadv) − Eθ(˜xK))
θ = θ − η · (∇θLp(y|xadv) + ∇θLp(xadv)) end for end for
Generating: x0 ∼ random sample for t = 0, 1, 2, ..., K − 1 do xt+1 = xt − α 2 · ∇xtEθ(xt, y) + end for
Output: xgen = xK
√
α · N (0, I)
ﬂattest among the four classiﬁers. Moreover, the energy contour is smooth across different energy levels. Hence images generated from a JEAT trained classiﬁer are more likely a natural image and exhibits many natural details.
Moreover, a ﬂat energy function is less sensitive to noise perturbation. We will verify in experiments that JEAT im-proves both quality of generated images and adversarial ro-bustness systematically. 4. Experiments 4.1. Experimental settings
To verify the validity of our energy-based explana-tion and the effectiveness of JEAT, we conducted experi-ments on CIFAR-10, CIFAR-100 and compare them with
For network architecture, we use the other methods.
WideResNet-28-10 for all algorithms. We repeat our ex-periments ﬁve times and report the mean and standard devi-4.2. Image Generation
We use the procedure described in Sec. 3.1 to generate images by using different classiﬁers. The images generated by the adversarially trained classiﬁer are shown in Fig. 6(a).
As analyzed in Sec. 3.1, Eθ(x) can be the factor that re-stricts the generative capability of a robust model. We show the images generated by PreJEAT which uses Eθ(x, y) in-stead of Eθ(x, y) − Eθ(x) both in adversarial training and generating images in Fig. 6(b). This approach greatly im-proves the quality of generated images. The energy plots of classiﬁers are presented in Sec. 3. By using PreJEAT, the energy is smoother and has a larger low energy region around real data, and it generates better quality images, es-pecially in ﬁne details and overall shape.
We also show the images generated by JEM [10] which is an energy-based model utilizing a classiﬁer in Fig. 6(c).
Compared to Fig. 6(b), JEM generates a more diverse back-ground and is smooth in pixel. However, the features of objects stand out and are more perceptible to human in the images generated by PreJEAT.
As is shown in Fig. 6(d), our JEAT algorithm generates the best images compared to the other three. The reason behind is that JEAT is adversarially trained with energy
Eθ(x, y) as in Eq. (18), and directly optimize Eθ(x, y) in the training contributes to getting better Eθ(x, y). The clas-siﬁer trained by JEAT generates natural images with abun-dant features. Moreover, JEAT has the best performance based on the metrics of the Inception Score and Frechet In-ception Distance.
We also validate the generalizability of the JEAT trained classiﬁer in the sense that the images generated differ from any images in the training set, as shown in Fig. 7. Hence,
JEAT is not just memorizing training images it has seen but generalizes as well. 4.3. Score Matching Metric
“Score” is deﬁned here as sθ(x, y) = ∇x log pθ(x, y)
[25]. Score matching tries to match the vector ﬁeld of the gradient of log pθ(x, y) with respect to x to the real vector
ﬁeld given in data distribution [25] by minimizing Fisher
Divergence: 1 2 · Epdata(x|y)[(cid:107)∇x log pθ(x, y) − ∇x log pdata(x, y)(cid:107)2 2]. (21)
With the score deﬁned in the distribution of (x, y) as sθ(x, y), an equivalent optimization problem to (21) is min-Figure 6. The generated images of different models. (a) The images generated by standard adversarially trained classiﬁer; (b) The images generated by PreJEAT trained classiﬁer; (c) The images generated by JEM [10] trained classiﬁer; (d) The images generated by JEAT trained classiﬁer. Images in each line are generated for a class in CIFAR-10.
It is a simple and effective metric that a lower value in-dicates ∇x log pθ(x, y) is closer to ∇x log pdata(x, y). So smaller value of Fisher Score corresponds to better gener-ative capability (not considering failure cases here). Fisher
Score could be used as a metric to assess the generative ca-pability of models. Similarly, we denote Marginal Fisher
Score (MFS) as:
Epdata(x)[ 1 2 · (cid:107)∇xEθ(x)(cid:107)2 2 − tr(∇2 xEθ(x))]. (24)
Figure 7. JEAT generates different images different from train-ing data. The left column is generated by JEAT, and the right ten columns are images in the training dataset with the minimum distance and highest similarity to the image generated by JEAT.
The image we generated is different from the original ones in the training dataset. We use SSIM [30] to measure similarity.
As shown in Tab. 2, both the Joint Fisher Score and the
Marginal Fisher Score for our JEAT are the smallest in all the four models, which implies that the score of our model matches well with the ground truth distribution. This also means that JEAT can generate better images using Langevin
Dynamics. The score matching metric is also consistent with the results of IS and FID, which are commonly used. imizing
Epdata(x|y)[ 1 2 · (cid:107)sθ(x, y)(cid:107)2 2 + tr(∇xsθ(x, y))], (22)
Table 2. Metrics for different models. “JFS” and “MFS” are the scores to evaluate the Joint and marginal Fisher Scores. “Normal” denotes normal training, “AT” denotes adversarial training. where ∇xsθ(x, y) denotes the Jacobian of sθ(x, y) [13].
From energy perspective, score sθ(x, y) = −∇xEθ(x, y).
Hence we denote objective in Eq. (22) as Joint Fisher Score (FS):
Epdata(x|y)[ 1 2 · (cid:107)∇xEθ(x, y)(cid:107)2 2 − tr(∇2 xEθ(x, y))]. (23)
Method Normal AT [20] 6.97 23.08
--JFS↓
MFS↓
IS↑
FID↑ 2.83 3.12 7.50 60.90
JEM [10] 0.330 1.44 8.66 38.40
JEAT 0.023 0.15 8.80 38.24
4.4. Robustness of JEAT
In this section, we show that the JEAT models not only can generate good images but also has comparable robust-ness with other hybrid models. We compare our model among hybrid models, including Glow [14], IGEBM [4], and JEM [10]. As shown in Tab. 3, we compare standard classiﬁcation accuracy, and robust accuracy under (cid:96)∞ PGD-20 attack with (cid:15) = 8/255. The experimental results show that JEAT can both improve the generative capability and adversarial robustness of JEM.
JEM [10] is a well-written paper, which proposes that classiﬁers can also be trained as generative models and such models have the robustness comparable to adversarial train-ing. However, when we use standard method [3] for eval-uation, JEM’s robustness is only 6.11% (under (cid:96)∞ PGD-20, (cid:15)=8/255). We follow the same standard evaluation [3] method to test our JEAT’s robustness and show it can im-prove from 6.11% to 30.55% (under (cid:96)∞ PGD-20, (cid:15)=8/255).
Table 3. The performance comparison among four hybrid models on CIFAR-10. We use (cid:96)∞ PGD-20 attack with (cid:15) = 8/255.
Model
Glow [14]
IGEBM [4]
JEM [10]
JEAT
Standard Accuracy Robustness 67.6% 45.06% 92.90% 85.16%
-32.19% 6.11%1 30.55% 5.