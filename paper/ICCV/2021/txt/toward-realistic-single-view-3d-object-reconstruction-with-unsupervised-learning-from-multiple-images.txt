Abstract 1.

Introduction
Recovering the 3D structure of an object from a single image is a challenging task due to its ill-posed nature. One approach is to utilize the plentiful photos of the same object category to learn a strong 3D shape prior for the object.
This approach has successfully been demonstrated by a re-cent work of Wu et al. (2020), which obtained impressive 3D reconstruction networks with unsupervised learning.
However, their algorithm is only applicable to symmetric objects. In this paper, we eliminate the symmetry require-ment with a novel unsupervised algorithm that can learn a 3D reconstruction network from a multi-image dataset.
Our algorithm is more general and covers the symmetry-required scenario as a special case. Besides, we employ a novel albedo loss that improves the reconstructed details and realisticity. Our method surpasses the previous work in both quality and robustness, as shown in experiments on datasets of various structures, including single-view, multi-view, image-collection, and video sets. Code is available at: https://github.com/VinAIResearch/LeMul.
Images are 2D projections of real-world 3D objects, and recovering the 3D structure from a 2D image is an impor-tant computer vision task with many applications. Most image-based 3D modeling methods rely on multi-view in-puts [42, 43, 16, 17, 11, 57, 48, 21], requiring multiple images of the target object captured from different views.
However, these methods are not applicable to the scenar-ios where only a single input image is available, which is the focus of our work in this paper. This problem is called single-view 3D reconstruction, and it is ill-posed since an image can be a projection of inﬁnitely many 3D shapes.
Interestingly, humans are very good at estimating the 3D structure of any known class object from a single image; we can even predict how it looks in unseen views. This is perhaps because humans have strong prior knowledge about the 3D shape and texture of the object class in considera-tion. Inspired by this observation, many category-speciﬁc 3D modeling methods have been proposed for speciﬁc ob-ject categories such as faces [3, 40, 59, 46, 39, 44, 47, 13], hands [60, 30, 4, 18], and bodies [34, 24].
In this paper, instead of focusing on any individual cate-gory, we aim to develop a general framework that can work for any object category, as long as there are many images from that category to train a single-view 3D reconstruction network. Furthermore, given the difﬁculty of acquiring 3D ground-truth annotation, we also aim to develop an unsuper-vised learning method which does not require the ground-truth 3D structures for the objects in the training images.
However, this is a challenging problem due to the huge vari-ation of the training images, regarding their viewpoint, ap-pearance, illumination, and background.
A recent study [52] made a break-though in solving this problem with a novel end-to-end trainable deep network.
Their network consisted of several modules to regress the image formation’s components, including the object’s 3D shape, texture, viewpoint, and lighting parameters, so that the rendered image was similar to the input. The modules were trained in an unsupervised manner on image datasets.
They assumed a single image per training example, so it was still highly under-constrained. To make this training pro-cedure converge, the authors proposed using the symmetry constraint. Their system successfully recovered 3D shape of human faces, cat faces, and synthetic cars after training on respective datasets. For convenience, from now on we will call this Learning from Symmetry method as LeSym.
While showing good initial results, LeSym has several limitations. First, it requires the target object to be almost symmetric, severely restricting its applicability to certain object classes. For highly asymmetric objects, this method does not work, and for nearly symmetric objects, it would not preserve the asymmetric details. Second, with a strong symmetry constraint, an incorrect mirror line estimation would lead to unrealistic 3D reconstruction. Some exam-ples and detailed discussions on these issues can be found in Sec. 4. Third, when multiple images of the same object in the training dataset are available, LeSym cannot corre-late and leverage these images to improve the reconstruc-tion accuracy and stability. This is a drawback because there are many imagery datasets that contain multiple images for each object. For example, multiview stereo datasets have photos of each object captured at different views. Some datasets instead have multiple pictures of the same view but with different lighting conditions or focal lengths. Facial datasets often have multiple images for each person, and video datasets have a large number of frames covering the same object in each video.
In this paper, we propose a more general framework, called LeMul, that effectively Learns from Multi-image datasets for more ﬂexible and reliable unsupervised train-ing of 3D reconstruction networks. It employs loose shape and texture consistency losses based on component swap-ping across views. This is an “unsupervised” method since it does not require any 3D ground-truth data in training.
Although it exploits multiple images per training instance, these images are so diverse and cannot be combined in tradi-tional approaches to form any 3D supervision. LeMul can cover the symmetric object addressed in LeSym by using the original and the ﬂipped image with less regularized re-sults. More importantly, it handles a wider range of training datasets and object classes.
Besides, we employ an albedo loss in LeMul, which ac-curately recovers ﬁne details of the 3D shape. This loss is inspired by a well-known Shape-from-Shading (SfS) lit-erature [32]. It greatly improves the realisticity of the re-constructed 3D model, sometimes approaching laser-scan quality, from a low-res single image input.
In short, our contributions are: (1) we introduce a gen-eral framework, called LeMul, that can exploit multi-image datasets in learning 3D object reconstruction from a sin-gle image without the symmetry constraint; (2) we employ shape and texture consistency losses to make that unsuper-vised learning converge; (3) we apply an albedo loss to im-prove realisticity of the reconstruction results; (4) LeMul shows state-of-the-art performance, qualitatively and quan-titively, on a wide range of datasets. 2.