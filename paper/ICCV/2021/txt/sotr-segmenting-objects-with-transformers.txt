Abstract
Most recent transformer-based models show impressive performance on vision tasks, even better than Convolu-tion Neural Networks (CNN). In this work, we present a novel, ﬂexible, and effective transformer-based model for high-quality instance segmentation. The proposed method,
Segmenting Objects with TRansformers (SOTR), simpliﬁes the segmentation pipeline, building on an alternative CNN backbone appended with two parallel subtasks: (1) predict-ing per-instance category via transformer and (2) dynami-cally generating segmentation mask with the multi-level up-sampling module. SOTR can effectively extract lower-level feature representations and capture long-range context de-pendencies by Feature Pyramid Network (FPN) and twin transformer, respectively. Meanwhile, compared with the original transformer, the proposed twin transformer is time-and resource-efﬁcient since only a row and a column at-tention are involved to encode pixels. Moreover, SOTR is easy to be incorporated with various CNN backbones and transformer model variants to make considerable improve-ments for the segmentation accuracy and training conver-gence. Extensive experiments show that our SOTR performs well on the MS COCO dataset and surpasses state-of-the-art instance segmentation approaches. We hope our sim-ple but strong framework could serve as a preferment base-line for instance-level recognition. Our code is available at https://github.com/easton-cau/SOTR. 1.

Introduction
Instance segmentation, a fundamental task in computer vision, requires the correct prediction of each object in-stance and its per-pixel segmentation mask in an image.
*Equal contribution.
†Corresponding author. E-mail: lizb@cau.edu.cn
Figure 1: Selected output of SOTR. We combine CNN with transformer and obtain competitive qualitative results.
Notice that not only are larger objects well delineated, tar-gets with elaborate shapes can also get nice segmentation.
It becomes more challenging because of the contiguously increasing demands for precise separation of instances in complicated scenes with dense objects and accurate predic-tion of their masks at the pixel level. Modern instance seg-mentation approaches [24, 15] are typically built on CNN and follow the detect-then-segment paradigm, which con-sists of a detector used to identify and locate all objects, and a mask branch to generate segmentation masks. The success of this segmentation philosophy is attributed to the following favorable merits, i.e. translation equivariance and location, but faces the following obstacles: 1) CNN rela-tively lacks features’ coherence in high-level visual seman-tic information to associate instances due to the limited re-ceptive ﬁeld, leading to the sub-optimal results on large ob-jects; 2) Both the segmentation quality and inference speed rely heavily on the object detector, incurring inferior perfor-mance in complex scenarios.
To overcome these drawbacks, many recent studies tend to escape from the detect-then-segment manner toward bottom-up strategy [29, 31], which learns per-pixel em-bedding and instance-aware features, and then uses post-processing techniques to successively group them into in-stances based on the embedding characteristics. Therefore, these methods can well retain position and local-coherence information. However, the main shortcomings of bottom-up models are unstable clustering (e.g., fragmented and joint masks) and poor generalization ability on the dataset with different scenes. Our SOTR (Figure 1 and 2) effectively learns position-sensitive features and dynamically generates instance masks following the basic principle of [37], with-out the post-processing grouping and the bound of bounding box’s locations and scales.
Furthermore, inspired by the power of transformer in natural language processing (NLP) [4, 10, 35], dozens of works try to entirely substitute the convolution operation or combine the CNN-like architectures with transformers for feature extraction in vision tasks [11, 8, 5], which can eas-ily capture global-range characteristics and naturally mod-els long-distance semantic dependencies. In particular, self-attention, the key mechanism of transformers, broadly ag-gregates both feature and positional information from the whole input domain. Thus transformer-based models can better distinguish overlapped instances with the same se-mantic category, which makes them more suitable than
CNN on high-level vision tasks. Nevertheless, insufﬁcien-cies still exist in these transformer-based approaches. On the one hand, the typical transformer does not behave well in extracting low-level features, leading to erroneous pre-dictions on small objects. On the other hand, due to the extensive feature map, a large amount of memory and time are required, especially during the training stage.
To cope with these weaknesses, we propose an innova-tive bottom-up model called SOTR that ingeniously com-bines the advantages of CNN and transformer. More specif-ically, we adopt a new transformer model inspired by [20] to acquire global dependencies and extract high-level fea-tures for predictions in subsequent functional heads. Fig-ure 2 shows the overall pipeline of our SOTR. It is com-posed of three parts, a CNN backbone, a transformer, and a multi-level upsampling module. An image is ﬁrst fed to
FPN to generate feature maps in multi-scale. After patch recombination and positional embedding, the transformer takes the clip-level feature sequences or blocks as inputs and further grasps the global-level semantic features as the powerful complement of the backbone. Then, part of the output feature is input to functional heads for the category and convolution kernel prediction. Finally, the multi-level upsampling module fuses the multi-scale features to a uni-ﬁed one to generate instance masks with the assistance of the dynamic convolution operation.
The focus of SOTR is to investigate ways to better utilize the semantic information extracted by the transformer. With the aim to reduce the memory and computational complex-ity of the conventional self-attention mechanism, we put forward twin attention, which adopts a sparse representa-tion of the traditional attention matrix. We carry out a great deal of ablation experiments to explore the optimal architec-ture and hyper-parameters. In summary, not only does our
SOTR provide a new framework for instance segmentation, but also it outperforms most of the CNN approaches on the
MS COCO [27] dataset. Speciﬁcally, the overall contribu-tions of our work are listed as follows:
• We introduce an innovative CNN-transformer-hybrid instance segmentation framework, termed SOTR. It can effectively model local connectivity and long-range dependencies leveraging CNN backbone and transformer encoder in the input domain to make them highly expressive. What’s more, SOTR considerably streamlines the overall pipeline by directly segment-ing object instances without relying on box detection.
• We devise the twin attention, a new position-sensitive self-attention mechanism, which is tailored for our transformer. This well-designed architecture enjoys a signiﬁcant saving in computation and memory com-pared with original transformer, especially on large in-puts for a dense prediction like instance segmentation.
• Apart from pure transformer based models, the pro-posed SOTR does not need to be pre-trained on large datasets to generalize inductive biases well. Thus,
SOTR is easier applied to insufﬁcient amounts of data.
• The performance of SOTR achieves 40.2% of AP with the ResNet-101-FPN backbone on the MS COCO benchmark, outperforming most of state-of-the-art ap-proaches in accuracy. Furthermore, SOTR demon-strates signiﬁcantly better performance on medium (59.0%) and large objects (73.0%), thanks to the ex-traction of global information by twin transformer. 2.