Abstract
Current state-of-the-art object detection and segmenta-tion methods work well under the closed-world assump-tion. This closed-world setting assumes that the list of object categories is available during training and deploy-ment. However, many real-world applications require de-tecting or segmenting novel objects, i.e., object categories never seen during training. In this paper, we present, UVO (Unidentified Video Objects), a new benchmark for open-world class-agnostic object segmentation in videos. Be-sides shifting the focus to the open-world setup, UVO is significantly larger, providing approximately 6 times more videos compared with DAVIS, and 7 times more mask (in-stance) annotations per video compared with YouTube-VO(I)S. UVO is also more challenging as it includes many videos with crowded scenes and complex background mo-tions. We also demonstrated that UVO can be used for other applications, such as object tracking and super-voxel segmentation. We believe that UVO is a versatile testbed for researchers to develop novel approaches for open-world class-agnostic object segmentation, and inspires new re-search directions towards a more comprehensive video un-derstanding beyond classification and detection. 1.

Introduction
During daily activities, humans routinely encounter novel objects, e.g., unfamiliar birds or unknown flowers; despite this unfamiliarity, people have no problem perceiv-ing them as distinct object instances. Even cinematic ex-amples like UFOs will be identified as independent things.
Many real-world applications such as object search [27, 30], instance registration [50], human-object interaction model-ing [14] and human activity understanding [6] require such open-world prediction abilities, e.g., exhaustively detecting or segmenting objects (both known and unknown), to ac-complish their tasks. Open-world is also the natural set-ting for applications like embodied AI (e.g., robotics, au-tonomous driving) and augmented-reality assistants, which will encounter novel situations regularly. (a) (c) (b)
Figure 1: State-of-the-art object detection/segmentation methods do not work well in open-world settings. We evaluated (a) Mask R-CNN trained on COCO and (b) Google AI cloud API on UVO videos and found both methods fail to segment many objects that have not been seen in train-ing. (c) Real-world applications require segmenting all objects that appear in the videos, even unseen objects. Mask R-CNN works well only on pre-defined categories and fails to recognize objects (e.g., barbell) or confuses non-object with objects in the taxonomy (refrigerator). Google cloud ob-ject detector offers stronger detection results but still misses all gym equip-ment in the background. (c) UVO is designed to detect/segment all objects regardless of the categories and beyond.
In contrast, the current state-of-the-art methods are de-signed for closed-world detection and segmentation. Mask
R-CNN [17], when trained on COCO [24] with a closed-world taxonomy of 80 object categories, cannot segment novel objects (see Figure 1a); the model can only be ex-pected to segment the classes it is trained for. Similarly, an industry publicly-available object detector (capable of detecting 550+ objects) [1] still cannot detect many ob-jects (see Figure 1b). From a methodology perspective, open-world object segmentation is a challenging problem due to its open-taxonomy nature. Though extensive re-search has been done to develop either supervised top-down approaches [17, 34, 33] or unsupervised bottom-up approaches [39, 21, 15] for object detection/segmentation in the closed-world setting, we find that simple modifica-tions of existing approaches do not work well for the open-world segmentation problem. On one hand, adopting a top-down approach, e.g., Mask R-CNN, to class-agnostic ob-ject segmentation by replacing its multi-class loss with a
binary foreground versus background loss performs poorly on unseen classes (as shown in Table 6). This is because the top-down approaches are strongly biased toward contextual cues from seen classes [35]. On the other hand, unsuper-vised approaches, e.g., GBH [15], rely on pixel grouping using local cues, e.g., colors and/or motions, which have no notion about semantic object boundaries, thus also perform poorly (see Figure 8). We believe that open-world object segmentation is a challenging yet important problem that needs to be addressed by approaches that are conceptually different from existing methods.
Open-world object segmentation also provides opportu-nities for long video modeling and more complex predic-tion tasks. Current video understanding approaches [36, 5, 38, 12, 37] cannot scale well to long videos due to GPU memory constraints and are not designed for complex pre-diction tasks beyond classification and detection. Grouping pixels into semantic entities (including unknown classes) can provide plausible alternatives for long-term video mod-eling and flexible prediction tasks, e.g., reasoning about objects and their interactions, even when their types are unknown, possibly by applying graph convolutional net-works [42, 43], knowledge graphs [49, 13], or attention-based models [41, 9] on top of entities instead of pixel-based CNNs which are memory-intensive.
Due to its broad applications, open-world object segmen-tation is an important problem to study. Current datasets are often constructed with predefined taxonomy in a closed-world manner. Removing object labels in existing datasets will not make them suitable for open-world segmentation: to evaluate detectors in open-world, a dataset needs to con-tain exhaustive annotations on all objects; otherwise, a model detecting un-annotated objects is not rewarded and can even be penalized. Ideally, the dataset should be anno-tated in a bottom-up fashion: annotators watch videos, spot and mask all objects. To our knowledge, no existing dataset provides such exhaustive annotations at scale for videos or images 1. Bottom-up annotation pipelines are absent in videos and are rare in images 2. In this paper, we make the first step forward in addressing this problem by constructing a new benchmark for open-world object segmentation, and providing a comprehensive set of baselines with in-depth analysis to understand the benchmark and problem. Our contributions are:
• A method for constructing open-world object segmen-tation datasets using object interpolation and tracking, which is 4x more efficient than the baseline.
• We introduce, Unidentified Video Objects (UVO), a new benchmark for open-world class-agnostic ob-1LVIS [16] is a federated dataset of multiple small shards; each contains exhaustive annotations w.r.t. a subset of classes within the shard but not inter-shard: e.g., “person” is not annotated in many images. 2LVIS is bottom-up in federated setting; ADE20k [51] is smaller scale. ject segmentation. UVO focuses on open-world and has approximately 6x more videos compared with
DAVIS [29], and 7x more mask (instance) annotations per video compared with YouTube-VO(I)S [47, 48].
• We provide a comprehensive set of baselines to under-stand our proposed tasks and benchmark. We believe that UVO is a versatile testbed for open-world object segmentation and will inspire new research directions toward more complex video understanding tasks be-yond classification and detection. 2.