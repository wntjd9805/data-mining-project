Abstract
We propose Rank & Sort (RS) Loss, a ranking-based loss function to train deep object detection and instance seg-mentation methods (i.e. visual detectors). RS Loss super-vises the classiﬁer, a sub-network of these methods, to rank each positive above all negatives as well as to sort positives among themselves with respect to (wrt.) their localisation qualities (e.g. Intersection-over-Union - IoU). To tackle the non-differentiable nature of ranking and sorting, we refor-mulate the incorporation of error-driven update with back-propagation as Identity Update, which enables us to model our novel sorting error among positives. With RS Loss, we signiﬁcantly simplify training: (i) Thanks to our sorting ob-jective, the positives are prioritized by the classiﬁer with-out an additional auxiliary head (e.g. for centerness, IoU, mask-IoU), (ii) due to its ranking-based nature, RS Loss is robust to class imbalance, and thus, no sampling heuris-tic is required, and (iii) we address the multi-task nature of visual detectors using tuning-free task-balancing coefﬁ-cients. Using RS Loss, we train seven diverse visual detec-tors only by tuning the learning rate, and show that it con-sistently outperforms baselines: e.g. our RS Loss improves (i) Faster R-CNN by ∼ 3 box AP and aLRP Loss (ranking-based baseline) by ∼ 2 box AP on COCO dataset, (ii) Mask
R-CNN with repeat factor sampling (RFS) by 3.5 mask AP (∼ 7 AP for rare classes) on LVIS dataset; and also out-performs all counterparts. Code is available at: https:
//github.com/kemaloksuz/RankSortLoss. 1.

Introduction
Owing to their multi-task (e.g. classiﬁcation, box regres-sion, mask prediction) nature, object detection and instance segmentation methods rely on loss functions of the form: (cid:88) (cid:88)
LV D = t Lk
λk t , (1) k∈K t∈T which combines Lk t , the loss function for task t on stage k (e.g. |K| = 2 for Faster R-CNN [32] with RPN and R-*Equal contribution for senior authorship.
Figure 1. A ranking-based classiﬁcation loss vs RS Loss. (a) En-forcing to rank positives above negatives provides a useful objec-tive for training, however, it ignores ordering among positives. (b)
Our RS Loss, in addition to raking positives above negatives, aims to sort positives wrt. their continuous IoUs (positives: a green tone based on its label, negatives: orange). We propose Identity Update (Section 3), a reformulation of error-driven update with backprop-agation, to tackle these ranking and sorting operations which are difﬁcult to optimize due to their non-differentiable nature.
CNN), weighted by a hyper-parameter λk t . In such formu-lations, the number of hyper-parameters can easily exceed 10 [27], with additional hyper-parameters arising from task-speciﬁc imbalance problems [28], e.g. the positive-negative imbalance in the classiﬁcation task, and if a cascaded ar-chitecture is used (e.g. HTC [7] employs 3 R-CNNs with different λk t ). Thus, although such loss functions have led to unprecedented successes, they require tuning, which is time consuming, leads to sub-optimal solutions and makes fair comparison of methods challenging.
Recently proposed ranking-based loss functions, namely
“Average Precision (AP) Loss” [6] and “average Locali-sation Recall Precision (aLRP) Loss” [27], offer two im-portant advantages over the classical score-based functions (e.g. Cross-entropy Loss and Focal Loss [22]): (1) They di-rectly optimize the performance measure (e.g. AP), thereby providing consistency between training and evaluation ob-jectives. This also reduces the number of hyper-parameters as the performance measure (e.g. AP) does not typically (2) They are robust to class-have any hyper-parameters.
imbalance due to their ranking-based error deﬁnition. Al-though these losses have yielded state-of-the-art (SOTA) performances, they need longer training and more augmen-tation.
Broadly speaking, the ranking-based losses (AP Loss and aLRP Loss) focus on ranking positive examples over negatives, but they do not explicitly model positive-to-positive interactions. However, there is evidence that it is helpful to prioritize predictions wrt. their localisation qual-ities by using an auxiliary (aux. - e.g. IoU, centerness) head
[15, 17, 38, 44] or by supervising the classiﬁer to directly regress IoUs of the predictions without an aux. head (as shown by Li et al. [18] in Quality Focal Loss - QFL).
In this paper, we propose Rank & Sort (RS) Loss as a ranking-based loss function to train visual detection (VD – i.e. object detection and instance segmentation) meth-ods. RS Loss not only ranks positives above negatives (Fig. 1(a)) but also sorts positives among themselves with respect to their continuous IoU values (Fig. 1(b)). This approach brings in several crucial beneﬁts. Due to the prioritization of positives during training, detectors trained with RS Loss do not need an aux. head, and due to its ranking-based na-ture, RS Loss can handle extremely imbalanced data (e.g. object detection [28]) without any sampling heuristics. Be-sides, except for the learning rate, RS Loss does not need any hyper-parameter tuning thanks to our tuning-free task-balancing coefﬁcients. Owing to this signiﬁcant simpliﬁca-tion of training, we can apply RS Loss to different methods (i.e. multi-stage, one-stage, anchor-based, anchor-free) eas-ily (i.e. only by tuning the learning rate) and demonstrate that RS Loss consistently outperforms baselines.
Our contributions can be summarized as follows: (1) We reformulate the incorporation of error-driven optimization into backpropagation to optimize non-differentiable ranking-based losses as Identity Update, which uniquely provides interpretable loss values during training and allows deﬁnition of intra-class errors (e.g. the sorting error among positives). (2) We propose Rank & Sort Loss that deﬁnes a ranking ob-jective between positives and negatives as well as a sorting objective to prioritize positives wrt. their continuous IoUs.
Due to this ranking-based nature, RS Loss can train models in the presence of highly imbalanced data. (3) We present the effectiveness of RS Loss on a diverse set of four object detectors and three instance segmenta-tion methods only by tuning the learning rate and without any aux. heads or sampling heuristics on the widely-used
COCO and long-tailed LVIS benchmarks: E.g. (i) Our RS-R-CNN improves Faster-CNN by ∼ 3 box AP on COCO, (ii) our RS-Mask R-CNN improves repeat factor sampling by ∼ 3.5 mask AP (∼ 7 AP for rare classes) on LVIS. 2.