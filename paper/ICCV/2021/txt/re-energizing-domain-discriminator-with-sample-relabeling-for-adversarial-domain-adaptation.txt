Abstract
Many unsupervised domain adaptation (UDA) methods exploit domain adversarial training to align the features to reduce domain gap, where a feature extractor is trained to fool a domain discriminator in order to have aligned feature distributions. The discrimination capability of the domain classifier w.r.t. the increasingly aligned feature distributions deteriorates as training goes on, thus cannot effectively fur-ther drive the training of feature extractor.
In this work, we propose an efficient optimization strategy named Re-enforceable Adversarial Domain Adaptation (RADA) which aims to re-energize the domain discriminator during the training by using dynamic domain labels. Particularly, we relabel the well aligned target domain samples as source domain samples on the fly. Such relabeling makes the less separable distributions more separable, and thus leads to a more powerful domain classifier w.r.t. the new data distribu-tions, which in turn further drives feature alignment. Exten-sive experiments on multiple UDA benchmarks demonstrate the effectiveness and superiority of our RADA. 1.

Introduction
The rapid development of deep learning has helped achieve significant improvements for many computer vision tasks. Those learned models usually have high performance on the test data that share similar characteristics as the train-ing data, but suffer from significant performance degrada-tion when deployed in new environments [11, 13, 26, 25].
There are domain gaps between the training source data (source domain) and the testing target data (target domain).
Besides, annotating the data for the target domain is expen-sive and time-consuming. To mitigate the notorious domain gap issue without manual annotation efforts, unsupervised domain adaptation (UDA) [11, 13, 19, 22, 64, 27, 9] is re-cently extensively explored, which aims to learn from la-beled source domain and unlabeled target domain.
*Corresponding Author.
Figure 1: Motivation and our idea. (a) Previous methods align domain distributions by adversarially training the do-main classifier with static domain labels of the samples.
The discrimination capability of the domain classifier w.r.t. the increasingly aligned feature distributions deteriorates as training goes on (i.e., more samples are inseparable), which in turn provides less driving power to the feature extractor for alignment and prevents effective optimization. (b) We propose a new solution which allows dynamic domain la-bels. We relabel the “well aligned” target samples as source domain, which makes the two less separable distributions more separable and thus leads to a more powerful domain classifier, which in turn further drives feature alignment.
For domain adaptation, theoretical analysis by Ben-David et al. [1] shows that reducing the feature differences between the source and target domains can reduce the up-per bound of the target domain error. Many domain adap-tation methods tend to learn domain invariant/transferable
Inspired by the feature representations [15, 17, 14, 35].
Generative Adversarial Networks (GANs) [16], adversar-ial learning has been successfully applied for UDA [13, 46, 7, 43, 49, 31, 62, 10]. The core idea of adversarial do-main adaptation approaches is to train a domain discrimi-nator/classifier to distinguish between the source and target domains and train the generator/feature extractor to mini-mize the feature discrepancy between the source and tar-get domain in order to fool the discriminator in a minmax two-player game. High discrimination capability for the do-main discriminator is desired in order to be able to drive the feature alignment. Typically, a domain adversarial neural
network (DANN) [13] introduces a gradient reversal layer (GRL) for adversarial training, where the ordinary gradient descent is applied for optimizing the domain classifier and the sign of the gradient is reversed when passing through the
GRL to optimize the feature extractor. Conditional domain adversarial network (CDAN) [31] improves DANN by con-ditioning the domain discriminator on both the object clas-sification predictions/likelihoods and the extracted features.
All these methods train the domain discriminator with the static domain labels of the source and target samples.
However, as shown in Figure 1 (a), as the adversarial train-ing goes on, the feature distributions for the source domain and target domain are increasingly aligned. The discrimi-nation capability of the domain discriminator/classifier w.r.t. the more aligned distributions is weaker than that w.r.t. the earlier less aligned distributions. Such degra-dation of discrimination capability in turn provides less driving power to the feature extractor for alignment, hindering effective optimization, even though there are still not aligned samples in the feature space.
To better understand what is happening, on top of a rep-resentative baseline scheme CDAN [31], we observe the variation of the discrimination capability of the domain dis-criminator by calculating the average entropy of domain classification for all the training samples at each training epoch (see Figure 2 (a)), where an epoch is a single pass through the full training set. As we know, a larger entropy of domain classification (i.e., the discriminator has larger ambiguity on the domain classes of the samples) indicates a poorer discrimination capability of the domain classifier.
We also observe how well the alignment is by calculating a domain discrepancy measurement Maximum Mean Dis-crepancy (MMD) [30, 2, 55] (see Figure 2 (b)).
Figure 2 (a) reveals that the average entropy of the base-line scheme goes through a process of fast decreasing and then slowly rises with fluctuation. 1) In the early epochs, the successive training of domain classifier increases its discrimination power and thus the entropy decreases. 2)
Meanwhile, as the training goes on, the feature distributions for the source domain and target domain are increasingly aligned, i.e., the domain discrepancy decreases as shown in Figure 2 (b). This could adversarially increase the en-tropy. 3) The optimization progresses/paces of the feature extractor and the domain discriminator are usually not the same. Around 5 to 15 epochs, the domain discrepancy con-tinues to decrease even when the discrimination capabil-ity reduces (i.e., entropy increases). This may be because the optimization of feature extractor lags behind that of the domain discriminator and thus the feature extractor can be further optimized. 4) The discrimination capability of the domain classifier w.r.t. the more aligned distributions is be-coming weaker than that w.r.t. the earlier less aligned distri-butions (the entropy increases). The persistent degradation
Figure 2: The variation of (a) the discrimination capability of the domain discriminator (measured by entropy of do-main classification) and (b) alignment state (measured by domain discrepancy measure of MMD) in the training. For the baseline scheme CDAN [31] (marked by green), the dis-crimination capability of the domain discriminator deterio-rates w.r.t. the gradually aligned distributions after the initial dip of the entropy, which in turn provides less driving power to the feature extractor for alignment. In contrast, thanks to our strategy of re-labeling the aligned target samples as source samples, our scheme (marked by red) could improve the discrimination capability of the domain discriminator (i.e., preventing the increasing of the entropy) and thus in turn further drives feature alignment. These experiments are conducted on Office-31 of the setting W→A. Note that more analysis can be found in our Supplementary. in turn provides less driving power to the feature extrac-tor for alignment, hindering effective optimization (where the alignment state cannot be effectively improved after 15 epochs, please refer to the green curve in Figure 2 (b)).
Thus, a mechanism that could enhance/improve on the fly the discrimination capability of the domain classifier to re-energize the adversarial training is highly desired.
Motivated by this, in this paper, we propose a new op-timization strategy named Re-enforceable Adversarial Do-main Adaptation (RADA) which aims to re-energize the do-main discriminator/classifier during the training. Particu-larly, instead of training the domain classifier using static domain labels for the source and target samples (see Fig-ure 1 (a)), we propose to exploit dynamic domain labels, where we relabel the “well aligned” target samples, i.e., those close to source domain, as source domain samples, in each mini-batch to be optimized/trained. As illustrated in
Figure 1 (b), this makes the two less separable distributions more separable and thus leads to a more powerful domain discriminator. Such re-energized domain discriminator in turn further drives the feature alignment for the feature ex-tractor. From Figure 2, we can observe that after using our proposed data relabeling strategy, the domain classification entropy is much lower than that of the baseline scheme and the domain discrepancy is also smaller.
We summarize our main contributions as follows:
• We pinpoint that the popular adversarial domain adap-tation approaches in general face optimization difficulty, which is caused by the deteriorated discrimination capa-bility of the domain classifier as the feature distributions become increasingly aligned in training.
• To alleviate the deterioration of the discrimination ca-pability of the domain discriminator, we propose an ef-ficient optimization strategy named Re-enforceable Ad-versarial Domain Adaptation (RADA), which is capable of re-energizing the domain discriminator during train-ing which in turn further drives feature alignment. We achieve this by relabeling the well aligned target samples as source domain samples for online training of the do-main discriminator.
We will demonstrate the effectiveness of the proposed
RADA on top of multiple widely-used adversarial learn-ing based domain adaptation baselines. RADA significantly outperforms the state-of-the-art UDA approaches. RADA is simple yet effective and can be used as a plug-and-play op-timization strategy for existing adversarial learning based
UDA approaches. Note that we do not make any change to the network architecture of the domain discriminator, which makes the RADA friendly to many adversarial do-main adaptation methods. 2.