Abstract
We extend the task of composed image retrieval, where an input query consists of an image and short textual de-scription of how to modify the image. Existing methods have only been applied to non-complex images within narrow do-mains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and lan-guage contexts. To address this issue, we collect the Com-pose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text.
To extend current methods to the open-domain, we pro-pose CIRPLANT, a transformer based model that leverages rich pre-trained vision-and-language (V&L) knowledge for modifying visual features conditioned on natural language.
Retrieval is then done by nearest neighbor lookup on the modified features. We demonstrate that with a relatively simple architecture, CIRPLANT outperforms existing meth-ods on open-domain images, while matching state-of-the-art accuracy on the existing narrow datasets, such as fash-ion. Together with the release of CIRR, we believe this work will inspire further research on composed image retrieval.
Our dataset, code and pre-trained models are available at https://cuberick-orion.github.io/CIRR/. 1.

Introduction
We study the task of composed image retrieval, that is, finding an image from a large corpus that best matches a user query provided as an image-language pair. Unlike tra-ditional content-based [38] or text-based [24, 42] image re-trieval where a single modality is used to describe the target image, composed image retrieval involves both visual and textual modalities to specify the user’s intent. For humans the advantage of a bi-modal query is clear: some concepts and attributes are more succinctly described visually, others
Figure 1. Example of composed image retrieval from the proposed
CIRR dataset. The input is composed of a reference image and a modifying text, to which the model must find a close match. A major challenge is the inherent ambiguity and underspecification of visual aspects to be preserved or modified. Our dataset includes open-domain images with rich contexts to facilitate the study of such challenge. through language. By cross-referencing the two modalities, a reference image can capture the general gist of a scene, while the text can specify finer details. The challenge is the inherent ambiguity in knowing what information is impor-tant (typically one object of interest in the scene) and what can be ignored (e.g., the background and other irrelevant objects). However, existing datasets for this task fall short of allowing us to adequately study this problem.
Consider the example in Fig. 1. Real-life images usu-ally contain rich object interactions on various scales. In each case, to readily identify the relevant aspects to keep or change and pay less attention elsewhere (e.g., the color of the dog’s fur and background objects), a model must de-velop in-depth visual reasoning ability and infer implicit human agreements within both the visual and language con-texts. However, existing datasets are constrained to domains such as fashion products [4, 12, 13] or synthetic objects [40] with relatively simple image contents. We argue that the current datasets are insufficient for exploring the unique re-search opportunity mentioned above.
Motivated by this problem, we collect the Compose Im-It is age Retrieval on Real-life images (CIRR) dataset. based on the open-domain collection of real images from
NLVR2 [35], for which we collected rich, high-quality an-notations that aim to tease out the important aspects of the reference image and textual description for a given query.
Compared with existing datasets, CIRR places more em-phasis on distinguishing between visually similar images, which provides a greater challenge, as well as a chance for studying fine-grained vision-and-language (V&L) reason-ing in composed image retrieval. Our dataset also allows for evaluation on fully labeled subsets, which addresses a shortcoming of existing datasets that are not fully labeled and therefore contain multiple false-negatives (as unlabeled images are considered negative).
Meanwhile, we propose Composed Image Retrieval using Pretrained LANguage Transformers (CIRPLANT), which extends current methods into open-domain images by leveraging the knowledge of large-scale V&L pre-trained (VLP) model [25]. Although the advantages of such pre-trained models have been validated in many visiolinguis-tic tasks [6, 25, 28], to the best of our knowledge, none have been applied to composed image retrieval. We conjec-ture one of the reasons being the existing domain-specific datasets cannot greatly benefit from the pre-training, which uses more complex, open-world images. Moreover, to adopt the VLP models for fine-tuning, most of the down-stream tasks are formulated as classification tasks [6, 25].
For composed image retrieval, it requires taking as input both the reference and target images. However, this greatly raises the computational overhead for retrieval, as the model needs to exhaustively assess each input query paired with each candidate target before yielding the one with the highest prediction score. Instead, we propose to preserve the conventional metric learning pipeline, where the input queries are jointly embedded using the VLP model and later compared with features of candidate images through ℓ2-norm distance. Specifically, our design maintains the same objective of “language-conditioned image feature modifica-tion” as previous work [5, 8, 40], while manages to utilize the pre-trained V&L knowledge in large-scale models. We demonstrate that our proposed model reaches state-of-the-art on the existing fashion dataset while outperforming cur-rent methods on CIRR. 2.