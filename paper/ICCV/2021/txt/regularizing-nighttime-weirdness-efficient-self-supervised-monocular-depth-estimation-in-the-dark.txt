Abstract
Monocular depth estimation aims at predicting depth from a single image or video. Recently, self-supervised methods draw much attention since they are free of depth annotations and achieve impressive performance on sev-eral daytime benchmarks. However, they produce weird outputs in more challenging nighttime scenarios because of low visibility and varying illuminations, which bring weak textures and break brightness-consistency assumption, re-spectively. To address these problems, in this paper we pro-pose a novel framework with several improvements: (1) we introduce Priors-Based Regularization to learn distribution knowledge from unpaired depth maps and prevent model from being incorrectly trained; (2) we leverage Mapping-Consistent Image Enhancement module to enhance image visibility and contrast while maintaining brightness con-sistency; and (3) we present Statistics-Based Mask strat-egy to tune the number of removed pixels within texture-less regions, using dynamic statistics. Experimental results demonstrate the effectiveness of each component. Mean-while, our framework achieves remarkable improvements and state-of-the-art results on two nighttime datasets. Code is available at https://github.com/w2kun/RNW . 1.

Introduction
Monocular depth estimation is a fundamental topic in computer vision as it has wide range of applications in aug-mented reality [35], robotics [11] and autonomous driving
[34], etc. It often needs dense depth maps to learn the map-*Contributes equally
†Corresponding authors
‡PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Com-puter Science and Engineering, Nanjing University of Sci & Tech.
Figure 1. Depth from nuScenes (left) and RobotCar (right). (a) In-put images: cyan dashed box indicates an textureless patch caused by the low visibility (e.g., dark), and two red borders illustrate the varying lights between t and t + 1 frames. (b) shows that low visi-bility and varying lights result in big holes and non-smoothness in the depth maps using MonoDepth2 [15], respectively. (c) demon-strates depth predictions of our framework. ping from color images to the depth maps in supervised set-tings [12, 47, 50]. However, high-quality depth data are costly collected in a broad range of environments by us-ing expensive depth sensors (e.g. LiDAR and TOF). Hence, many efforts have been made to develop self-supervised ap-proaches [14, 54, 24, 52], which train a depth network to estimate depth maps by exploring geometry cues in videos, i.e., reconstructing a target view (or frame) from another view, instead of utilizing high-quality depth data. Further-more, their performances are comparable to the supervised methods in well-lit environments, such as KITTI [13] and
Cityscapes [10]. Unfortunately, there are a very few works to handle with more challenging nighttime scenarios. Thus we focus on nighttime self-supervised depth estimation.
Actually, the nighttime scenario includes two important problems, low visibility and varying illuminations, result-ing in that most of the existing self-supervised methods (e.g., MonoDepth2 [15]) produce a weird depth output (see
Fig.1(b)). 1) Low visibility usually creates textureless areas.
For example, the cyan dashed box in the left of Fig.1(a) shows a dark region with indistinguishable visual texture.
This textureless aggravates depth maps with big holes in the left of Fig. 1(b), though they may be used to correctly reconstruct the target view by sampling nearby pixels with similar brightness. 2) Varying illuminations from flickering streetlights or moving cars, undermine the brightness con-sistency assumption in the right of Fig.1(a), where the two image patches with different brightness are cropped from the same place of two temporally adjacent frames. This in-consistency brings an imperfect reconstruction of the tar-get view, that is, a high training loss, which also produces non-smooth depth map in the right of Fig. 1(b). Clearly, the incorrect depth prediction (e.g., non-smoothness and big holes) indicates a failure of training the depth network.
To address these two problems, in this paper we propose an efficient nighttime self-supervised framework for depth estimation with three improvements. Firstly, we introduce
Priors-Based Regularization (PBR) module to constrain the incorrect depth in neighborhoods of depth references, and prevent the depth network from being incorrectly trained.
This constraint is implemented by learning prior depth dis-tribution from unpaired references in an adversarial man-ner. Furthermore, 2D coordinates are encoded as an addi-tional input of PBR to find useful depth distribution, which is related with its pixel location. Secondly, we leverage
Mapping-Consistent Image Enhancement (MCIE) module to deal with the low visibility. Although image enhance-ment methods, e.g., Contrast Limited Histogram Equaliza-tion (CLHE) [37], can be used to achieve remarkable re-sults on low-light images [9, 23], they are difficult to handle the correspondence among video frames, which is essen-tial to self-supervised depth estimation. Thus, we extend the CLHE method to keep brightness consistency while enhancing low-visible video frames. Finally, we present
Statistics-Based Mask (SBM) to tackle textureless regions.
Though Auto-Mask [15] is a widely used strategy to effi-ciently choose textureless regions, its dependence on pho-tometric loss makes it unable to adjust the numbers of re-moved pixels. To compensate this weakness, we introduce
SBM to better handle nighttime scenarios by flexibly tun-ing masked pixels using dynamic statistics.
In short, our contributions can be summarized as three-fold:
• We propose Priors-Based Regularization module to learn distribution knowledge from unpaired references and pre-vent model from being incorrectly trained.
• We leverage Mapping-Consistent Image Enhancement module to deal with low visibility in the dark and main-tain brightness consistency.
• We present Statistics-Based Mask to better handle tex-tureless regions, by using dynamic information. Together, these contributions yield state-of-the-art performance in nighttime depth estimation task and efficiently reduce the weirdness in depth outputs. 2.