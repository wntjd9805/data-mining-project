Abstract
Supervised deep learning-based hash and vector quan-tization are enabling fast and large-scale image retrieval systems. By fully exploiting label annotations, they are achieving outstanding retrieval performances compared to the conventional methods. However, it is painstak-ing to assign labels precisely for a vast amount of train-ing data, and also, the annotation process is error-prone.
To tackle these issues, we propose the first deep unsu-pervised image retrieval method dubbed Self-supervised
Product Quantization (SPQ) network, which is label-free and trained in a self-supervised manner. We design a Cross
Quantized Contrastive learning strategy that jointly learns codewords and deep visual descriptors by comparing in-dividually transformed images (views). Our method ana-lyzes the image contents to extract descriptive features, al-lowing us to understand image representations for accurate retrieval. By conducting extensive experiments on bench-marks, we demonstrate that the proposed method yields state-of-the-art results even without supervised pretraining. 1.

Introduction
Approximate Nearest Neighbor (ANN) search has re-ceived much attention in image retrieval research due to its low storage cost and fast search speed. There are two mainstream approaches in the ANN research, one is Hash-ing [42], and the other is Vector Quantization (VQ) [16].
Both methods aim to transform high-dimensional image data into compact binary codes while preserving the seman-tic similarity, where the difference lies in measuring the dis-tance between the binary codes.
In the case of hashing methods [7, 43, 17, 15, 32], the distance between binary codes is calculated using the Ham-ming distance, i.e., a simple XOR operation. However, this approach has a limitation that the distance can be repre-sented with only a few distinct values, where the complex distance representation is incapable. To alleviate this issue, (a) Contrastive Learning (b) Cross Quantized Contrastive Learning (Ours)
Figure 1. Comparison between (a) contrastive learning and (b) cross quantized contrastive learning. The separately sampled two transformations (t, t′ ∼ T ) are applied on an image x to gen-erate two different views ˜xi and ˜xj, and corresponding deep de-scriptor ˆxi and ˆxj are obtained from the feature extractor F(·), respectively. The feature representations in contrastive learning are achieved by comparing the similarity between the projection head outputs ˆzi and ˆzj. Instead of projection, we introduce the quantization head, which collects codebooks of product quantiza-tion. By maximizing cross-similarity between the deep descriptor of one view and the product quantized descriptor of the other, both codewords and deep descriptors are jointly trained to contain dis-criminative image content representations.
VQ-based methods [23, 13, 24, 2, 48, 3, 49] have been pro-posed, exploiting quantized real-valued vectors in distance measurement instead. Among these, Product Quantization (PQ) [23] is one of the best methods, delivering the retrieval results very fast and accurately.
The essence of PQ is to decompose a high-dimensional space of feature vectors (image descriptors) into a Carte-sian product of several subspaces. Then, each of the im-age descriptors is divided into several subvectors according to the subspaces, and the subvectors are clustered to form centroids. As a result, Codebook of each subspace is con-figured with corresponding centroids (codewords), which are regarded as quantized representations of the images.
The distance between two different binary codes in the PQ scheme is asymmetrically approximated by utilizing real-valued codewords with look-up table, resulting in richer dis-tance representations than the hashing.
Recently, supervised deep hashing methods [44, 5, 22, 29, 47] show promising results for large-scale image re-trieval systems. However, since binary hash codes can-not be directly applied to learn deep continuous represen-tations, performance degradation is inevitable compared to the retrieval using real vectors. To address this problem, quantization-based deep image retrieval approaches have been proposed in [20, 4, 46, 31, 26, 21]. By introducing dif-ferentiable quantization methods on continuous deep image feature vectors (deep descriptors), direct learning of deep representations is allowed in the real-valued space.
Although deep supervised image retrieval systems pro-vide outstanding performances, they need expensive train-ing data annotations. Hence, deep unsupervised hashing methods have also been proposed [30, 11, 39, 50, 41, 14, 36, 45, 37], which investigate the image similarity to discover semantically distinguishable binary codes without annota-tions. However, while quantization-based methods have ad-vantages over hashing-based ones, only limited studies exist that adopt quantization for deep unsupervised retrieval. For example, [35] employed pre-extracted visual descriptors in-stead of images for the unsupervised quantization.
In this paper, we propose the first unsupervised end-to-end deep quantization-based image retrieval method; Self-supervised Product Quantization (SPQ) network, which jointly learns the feature extractor and the codewords. As shown in Figure 1, the main idea of SPQ is based on self-supervised contrastive learning [8, 40, 6]. We regard that two different “views” (individually transformed out-puts) of a single image are correlated, and conversely, the views generated from other images are uncorrelated. To train PQ codewords, we introduce Cross Quantized Con-trastive learning, which maximizes the cross-similarity be-tween the correlated deep descriptor and the product quan-tized descriptor. This strategy leads both deep descriptors and PQ codewords to become discriminative, allowing the
SPQ framework to achieve high retrieval accuracy.
To demonstrate the efficiency of our proposal, we con-duct experiments under various training conditions. Specif-ically, unlike previous methods that utilize pretrained model weights learned from a large labeled dataset, we conduct ex-periments with “truly” unsupervised settings where human supervision is excluded. Despite the absence of label infor-mation, SPQ achieves state-of-the-art performance.
The contributions of our work are summarized as:
• To the best of our knowledge, SPQ is the first deep un-supervised quantization-based image retrieval scheme, where both feature extraction and quantization are in-cluded in a single framework and trained in a self-supervised fashion.
• By introducing cross quantized contrastive learning strategy, the deep descriptors and the PQ codewords are jointly learned from two different views, delivering discriminative representations to obtain high retrieval scores.
• Extensive experiments on fast image retrieval proto-col datasets verify that our SPQ shows state-of-the-art retrieval performance even for the truly unsupervised settings. 2.