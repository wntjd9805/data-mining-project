Abstract
Contrastive learning allows us to ﬂexibly deﬁne power-ful losses by contrasting positive pairs from sets of negative samples. Recently, the principle has also been used to learn cross-modal embeddings for video and text, yet without ex-ploiting its full potential. In particular, previous losses do not take the intra-modality similarities into account, which leads to inefﬁcient embeddings, as the same content is mapped to multiple points in the embedding space. With
CrossCLR, we present a contrastive loss that ﬁxes this is-sue. Moreover, we deﬁne sets of highly related samples in terms of their input embeddings and exclude them from the negative samples to avoid issues with false negatives. We show that these principles consistently improve the quality of the learned embeddings. The joint embeddings learned with CrossCLR extend the state of the art in video-text retrieval on Youcook2 and LSMDC datasets and in video captioning on Youcook2 dataset by a large margin. We also demonstrate the generality of the concept by learning improved joint embeddings for other pairs of modalities. 1.

Introduction
Cross-modal tasks, especially tasks connecting video and text, expand the inﬂuence and applicability of computer vision. It enables, for example, video retrieval based on text queries [11, 10, 24], image and video captioning [12], and exploitation of text-based meta-data for visual feature learning [26, 28, 57, 41]. Linking the different, not directly comparable sources of data creates new challenges that do not appear in visual-only learning.
In this paper, we consider cross-modal contrastive learn-ing and introduce a loss that relates the data in a more efﬁcient manner than direct adoption of a loss designed for vision-only data. Contrastive learning is based on the deﬁnition of positive and negative samples relative to an ankor point, which yields a ﬂexible principle: pull together an anchor and a positive sample in the embedding space, and push apart the anchor from many negative samples.
Many implementations of this principle have been pro-posed: max-margin loss [14], triplet loss [46, 47, 18],
∗ Work done during an internship at Amazon Tübingen.
Figure 1. When learning a joint embedding between two modali-ties A and B, existing contrastive learning losses, such as Max-Margin [11, 24] and CLIP [34], ignore the possibility of false negative samples and, thus, push semantically related concepts apart. The proposed CrossCLR loss indentiﬁes inﬂuential sam-ples (large circles/boxes), removes them from the negative set and increases their weight in the minibatch. Moreover, CrossCLR adds intra-modal links to the loss. and InfoNCE [44]. Usually positive pairs are deﬁned as synthetic, spatial [3, 33], or temporal variations of an in-stance [33]. Instance discrimination has been applied also to cross-modal tasks, where positive pairs (or a set of pos-itives) are sampled from the same time window (MIL-NCE [26], AVSA [30], CM-ACC [25]).
In this paper, we investigate two issues of existing cross-modal contrastive learning techniques. 1. The cross-modal loss only ensures that the features from the two modali-ties map to proximate points in the joint embedding, but they lack an explicit measure that also ensures that similar features from the same modality stay close-by in the joint embedding. In previous works, it is implicitly presumed that the similarity between modalities, via transitivity, will also preserve similarity within the modality. However, it has not been shown that this is the case1. If similar features from the same modality map to far-apart points in the joint embedding, the embedding lacks semantic meaning and, thus, will generalize badly. 2. The focus of previous cross-1Since many previous works focus on unsupervised feature learning rather than learning joint embeddings, they do not assume that the input embeddings are meaningful, in the sense that semantically related con-cepts are initially close-by in the feature space. Therefore, preservation of input similarities is meaningless to them. In this work, we do assume that the input embeddings for each modality (e.g. ImageNet pretrained features) already cover some semantics, and we target a joint embedding that leverages these semantics across modalities.
modal contrastive losses is on the deﬁnition of positive pairs, whereas negative samples are randomly drawn from the entire distribution. This does not reﬂect the effect of what we call inﬂuential samples – samples that are similar to many other samples and, thus, have a large inﬂuence on the shape of the embedding. Marking inﬂuential samples as negatives will likely push apart samples that are actually strongly related.
As a remedy to the ﬁrst problem, we propose a con-trastive loss that enforces the joint embedding to respect the similarity of samples in the original features spaces.
Moreover, as a remedy to the second problem, we deﬁne inﬂuential samples as samples with a high connectivity within the dataset and remove these samples from the set of negatives. We also introduce a loss weighting based on the connectivity. We show that all three measures lead to an improved cross-modal embedding as tested in terms of video-text retrieval and video captioning. While this paper focuses on video and text as modalities, we show that the positive effects of the proposed cross-modal loss generalizes to other pairs of modalities. 2.