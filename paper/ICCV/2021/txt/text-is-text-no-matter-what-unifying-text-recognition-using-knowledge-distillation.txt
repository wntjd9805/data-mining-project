Abstract
Text recognition remains a fundamental and extensively researched topic in computer vision, largely owing to its wide array of commercial applications. The challenging nature of the very problem however dictated a fragmenta-tion of research efforts: Scene Text Recognition (STR) that deals with text in everyday scenes, and Handwriting Text
Recognition (HTR) that tackles hand-written text.
In this paper, for the ﬁrst time, we argue for their uniﬁcation – we aim for a single model that can compete favourably with two separate state-of-the-art STR and HTR models. We ﬁrst show that cross-utilisation of STR and HTR models trigger signiﬁcant performance drops due to differences in their in-herent challenges. We then tackle their union by introduc-ing a knowledge distillation (KD) based framework. This however is non-trivial, largely due to the variable-length and sequential nature of text sequences, which renders off-the-shelf KD techniques that mostly work with global ﬁxed length data, inadequate. For that, we propose four dis-tillation losses, all of which are speciﬁcally designed to cope with the aforementioned unique characteristics of text recognition. Empirical evidence suggests that our proposed uniﬁed model performs at par with individual models, even surpassing them in certain cases. Ablative studies demon-strate that naive baselines such as a two-stage framework, multi-task and domain adaption/generalisation alternatives do not work that well, further authenticating our design. 1.

Introduction
Text recognition has been studied extensively in the past two decades [37], mostly due to its potential in commercial applications. Following the advent of deep learning, great progress [4, 35, 57, 63, 5, 8, 7] has been made in recognition accuracy on different publicly available benchmark datasets
[41, 58, 30, 39]. Beyond supervised text recognition, very recent attempts have been made that utilise synthetic train-ing data via domain adaptation [67], learn optimal augmen-Figure 1. Despite performing well for scene images (IAM [39]), a model trained on HTR datasets (a), performs poorly in STR sce-narios (ICDAR-2015 [30]) and vice-versa (b). Although jointly training a model (c) using both STR and HTR datasets helps im-prove the disparity between the datasets, the gap still remains far behind the specialist models. Our KD based proposed method leads to performance at par or even better than individual models. tation strategy [38, 6], couple with visual question answer-ing [10], and withhold adversarial attacks [60].
Albeit with great strides made, the ﬁeld of text recogni-tion remains fragmented, with one side focusing on Scene
Text Recognition (STR) [30], and the other on Handwriting
Text Recognition (HTR) [39]. This however is not surpris-ing given the differences in the inherent challenges found in each respective problem: STR studies text in scene images posing challenges like complex backgrounds, blur, arte-facts, uncontrolled illumination [63], whereas HTR tackles handwritten texts where the main challenge lies with the free-ﬂow nature of writing [6] of different individuals. As a result, utilising models trained for STR on HTR (and vice versa) straightforwardly would trigger a signiﬁcant perfor-mance drop (see Figure 1). This leads to our motivation – how to design a uniﬁed text recognition model that works ubiquitously across both scenarios.
While there is no existing work addressing this issue, one might naively think of training a single text recogni-tion network using training data from both STR and HTR datasets. However, for the apparent issues of large domain gap and model capacity limitation [54], while the jointly trained model reduces the performance gap between HTR and STR datasets, it still lags signiﬁcantly behind individual
specialised models. Another solution is to include a classi-ﬁcation network prior to specialised STR and HTR models (i.e., a two-stage network). During evaluation, the classi-ﬁer decides if an input belongs to scene or handwritten text, followed by choosing an appropriate model for downstream recognition. Yet, this solution has two downsides: a) clas-siﬁcation network will incur additional computational cost and extra memory consumption to store all three neural net-works. b) cascaded connection of the classiﬁer and text recognition models will compound cumulative errors.
In this work, we introduce a knowledge distillation (KD)
[22, 49] based framework to unify individual STR and HTR models into a single multi-scenario model. Our design at a high-level, does not deviate much from a conventional KD setting where a learnable student model tries to mimic the behaviour of a pre-trained teacher. We ﬁrst train both STR and HTR models separately using their respective training data. Next, each individual model takes turns to act as a teacher in the distillation process, to train a single uniﬁed student model. It is this transfer of knowledge captured by specialised teachers into a single model, that leads to our superior performance in contrast to training a single model using joint STR and HTR datasets (see Figure 1).
Making such a design (KD) to work with text recognition is however non-trivial. The difﬁculty mainly arises from the variable-length and sequential natures of text images – each consists of a sequence of different number of individ-ual characters. Hence, employing off-the-shelf KD methods
[49] that aim at matching output probabilities and/or hidden representations between pre-trained teacher and learnable student model, which are used for global ﬁxed length data, may not be sufﬁcient to transfer knowledge at local charac-ter level. We thus propose three additional distillation losses to tackle the unique characteristics of text recognition.
More speciﬁcally, we ﬁrst impose a character aligned hint loss. This encourages the student to mimic character-speciﬁc hidden representations of specialised teacher over the varying sequence of characters in a text image. Next, an attention distillation loss is further imposed over the atten-tion map obtained at every step of character decoding pro-cess by an attentional decoder. This compliments the char-acter localised hint-loss, as attention-maps capture rich and diverse contextual information emphasising on localised re-gions [23]. Besides localised character level information, capturing long-range non-local dependencies among the se-quential characters is of critical importance, especially for an auto-regressive attentional decoder framework [34]. Ac-cordingly we propose an afﬁnity distillation loss as our third loss, to capture the interactions between every pair of posi-tions of the variable character length sequence, and guide the uniﬁed student model to emulate the afﬁnity matrix of the specialised teachers. Finally, we also make use of state-of-the-art logit distillation loss to work with our three pro-It aims at matching output probabilities of posed losses. student network over the character vocabulary, with that of pre-trained teachers.
Our main contributions can be summarised as follows: (a) We design a practically feasible uniﬁed text recognition setting that asks a single model to perform equally well across both HTR and STR scenarios. (b) We introduce a novel knowledge distillation paradigm where an uniﬁed student model learns from two pre-trained teacher models specialised for STR and HTR. (c) We design three addi-tional distillation losses to speciﬁcally tackle the variable-length and sequential nature of text data. (d) Extensive ex-periments coupled with ablative studies on public datasets, demonstrate the superiority of our framework. 2.