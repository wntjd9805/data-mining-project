Abstract
Unsupervised domain adaptation (DA) has gained sub-stantial interest in semantic segmentation. However, almost all prior arts assume concurrent access to both labeled source and unlabeled target, making them unsuitable for scenarios demanding source-free adaptation. In this work1, we enable source-free DA by partitioning the task into two: a) source-only domain generalization and b) source-free target adaptation. Towards the former, we provide theoreti-cal insights to develop a multi-head framework trained with a virtually extended multi-source dataset, aiming to balance generalization and specificity. Towards the latter, we utilize the multi-head framework to extract reliable target pseudo-labels for self-training. Additionally, we introduce a novel conditional prior-enforcing auto-encoder that discourages spatial irregularities, thereby enhancing the pseudo-label quality. Experiments on the standard GTA5→Cityscapes and SYNTHIA→Cityscapes benchmarks show our superi-ority even against the non-source-free prior-arts. Further, we show our compatibility with online adaptation enabling deployment in a sequentially changing environment. 1.

Introduction
Almost all supervised learning systems assume that the training and testing data follow the same input distribution.
However, this assumption is impractical as target scenarios often exhibit a distribution shift. For example, self-driving cars often fail to generalize when deployed in conditions different from training, such as cross-city [10] or cross-weather [59] deployment. This is because the model fails to apprehend the generic, causal factors of variations and instead, holds on to domain-specific spurious correlations
[24]. Over-reliance on training data from a particular dis-tribution can cause the model to fail even for mild domain-shifts like changes in illumination, texture, background, etc.
Unsupervised domain adaptation (DA) is one of the pri-mary ways to address such problems. Here, the goal is to transfer the knowledge from a labeled source domain to an
*Equal contribution. 1Project page: https://sites.google.com/view/sfdaseg
Figure 1. In source-free DA, the vendor accesses source-data to prepare a foresighted source-model. Following this, the client re-ceives only the source-model to perform unsupervised target adap-tation while prevented access to the proprietary source-data. unlabeled target domain. The major limitation of typical
DA approaches [58] is the requirement of concurrent access to both source and target domain samples. While concurrent access better characterizes the distribution shift, it is a major bottleneck for real-world deployment scenarios. Consider a modern corporate dealing where the vendor organization has access to a large-scale labeled dataset (i.e. source-data) which is used to train a source-model. The vendor finds multiple clients interested in deploying the source-model in their specific target environments. However, both parties are restrained from data sharing due to proprietary, privacy, or profit related concerns. This motivates us to seek learn-ing frameworks where the vendor can trade only the source-model and the client can perform target adaptation without the source-data. This special case of domain adaptation
[40, 34, 43] is Source-Free Domain Adaptation (SFDA).
In this work, we aim to develop an SFDA framework for semantic segmentation of urban road scenes. In a co-operative setup, both vendor and the client must adopt spe-cialized learning strategies to benefit the end goal. a) Vendor-side strategies. These strategies can be dis-cussed under two broad aspects viz. source dataset and training strategy. The vendor must acquire a substantially diverse large-scale dataset aiming to subsume unknown tar-In literature, Multi-Source DA (MSDA) get scenarios.
[89, 71, 1] and domain generalization (DG) [38] works use multiple labeled source domains to improve target gener-alization. However, gathering annotation for more than one domain is costly and time-consuming [12]. Thus, we focus on developing a strategy to simulate multiple novel domains from samples from a single labeled domain.
Carefully crafted augmentations randomly perturb the task-irrelevant factors (such as stylization, texture modulation, etc.), facilitating the learning of domain-invariant represen-tations. Hence, we devise multiple augmentation-groups (AGs), where each group modulates the image by vary-ing certain statistics thereby constructing virtual, labeled source-domains, to be treated as a multi-source dataset.
Next, we focus on developing an effective training strat-egy. The naive solution would be to train a single model on the entire multi-source dataset to learn domain-generic features. However, this can lead to sub-optimal perfor-mance if a certain AG alters the task-relevant causal factors
[24]. Further, the target domain may be similar to one or a combination of AGs. In such cases, domain-specific (AG-specific) learning is more helpful. This motivates us to seek a domain-specific framework to complement the domain-generic model. Thus, we give theoretical insights to ana-lyze domain-specific hypotheses and propose Source-only
Multi-Augmentation Network (SoMAN) as shown in Fig. 1.
Going forward, we recognize that SoMAN may lack the ability to capture inductive bias, which would prevent the model from manifesting structurally consistent predictions.
This is particularly important for dense prediction tasks
[32, 31]. Modeling general context dependent priors en-courages the prediction of plausible scene segments while discouraging common irregularities (e.g. merged-region or split-region issues [33]). To this end, we introduce a separate model namely, conditional Prior-enforcing Auto-Encoder (cPAE). cPAE is trained on segmentation maps available with the vendor, and used at the client-side to im-prove the source-free adaptation performance. b) Client-side strategies. We draw motivation from pseudo-label based self-training approaches [17, 95]. The target samples are passed through the source-model to se-lect a set of pseudo-labels which are later used to finetune the network. In the absence of source-data, effectiveness of such self-training depends on the following two aspects.
First, the training must be regularized to retain the vendor-side, task-specific knowledge. We address this by allow-ing only a handful of weights to be updated while others are kept frozen from the vendor-side training. Second, the pseudo-label selection criteria must overcome issues related to label-noise and information redundancy. We address this by selecting the optimal prediction from the SoMAN-heads and using the pruned output after forwarding through cPAE.
In summary, we make the following main contributions:
• We propose to address source-free DA by casting the vendor-side training as multi-source learning. To this end, we provide theoretical insights to analyze differ-ent ways to aggregate the domain-specific hypotheses.
It turns out that a combination of domain-generic and leave-one-out configuration performs the best.
• While accessing a single source domain, we propose a systematic way to select a minimal set of effective augmentations to resemble a multi-source scenario.
The vendor uses this to develop a multi-head network,
SoMAN subscribing to the leave-one-out configuration.
• Aiming to have a strong support for the spatially-structured segmentation task, we develop a conditional prior-enforcing auto-encoder. This encourages plausi-ble dense predictions thereby enhancing the quality of pseudo-labels to aid the client-side self-training.
• Our source-free framework achieves state-of-the-art results for both GTA5 → Cityscapes and SYNTHIA →
Cityscapes adaptation benchmarks, even when com-pared against the non-source-free prior arts. 2.