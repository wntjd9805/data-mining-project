Abstract
Multi-label zero-shot learning (ZSL) is a more realistic counter-part of standard single-label ZSL since several ob-jects can co-exist in a natural image. However, the occur-rence of multiple objects complicates the reasoning and re-quires region-speciﬁc processing of visual features to pre-serve their contextual cues. We note that the best existing multi-label ZSL method takes a shared approach towards attending to region features with a common set of atten-tion maps for all the classes. Such shared maps lead to diffused attention, which does not discriminatively focus on relevant locations when the number of classes are large.
Moreover, mapping spatially-pooled visual features to the class semantics leads to inter-class feature entanglement, thus hampering the classiﬁcation. Here, we propose an alternate approach towards region-based discriminability-preserving multi-label zero-shot classiﬁcation. Our ap-proach maintains the spatial resolution to preserve region-level characteristics and utilizes a bi-level attention module (BiAM) to enrich the features by incorporating both region and scene context information. The enriched region-level features are then mapped to the class semantics and only their class predictions are spatially pooled to obtain image-level predictions, thereby keeping the multi-class features disentangled. Our approach sets a new state of the art on two large-scale multi-label zero-shot benchmarks: NUS-WIDE and Open Images. On NUS-WIDE, our approach achieves an absolute gain of 6.9% mAP for ZSL, compared to the best published results. Source code is available at https://github.com/akshitac8/BiAM. 1.

Introduction
Multi-label classiﬁcation strives to recognize all the cat-egories (labels) present in an image. In the standard multi-label classiﬁcation [32, 39, 15, 4, 21, 40, 41] setting, the category labels in both the train and test sets are identical.
*Equal contribution
In contrast, the task of multi-label zero-shot learning (ZSL) is to recognize multiple new unseen categories in images at test time, without having seen the corresponding visual ex-amples during training. In the generalized ZSL (GZSL) set-ting, test images can simultaneously contain multiple seen and unseen classes. GZSL is particularly challenging in the large-scale multi-label setting, where several diverse cate-gories occur in an image (e.g., maximum of 117 labels per image in NUS-WIDE [5]) along with a large number of un-seen categories at test time (e.g., 400 unseen classes in Open
Images [16]). Here, we investigate this challenging problem of multi-label (generalized) zero-shot classiﬁcation.
Existing multi-label (G)ZSL methods tackle the problem by using global image features [20, 46], structured knowl-edge graph [17] and attention schemes [13]. Among these, the recently introduced LESA [13] proposes a shared atten-tion scheme based on region-based feature representations and achieves state-of-the-art results. LESA learns multiple attention maps that are shared across all categories. The region-based image features are weighted by these shared attentions and then spatially aggregated. Subsequently, the aggregated features are projected to the label space via a joint visual-semantic embedding space.
While achieving promising results, LESA suffers from two key limitations. Firstly, classiﬁcation is performed on features obtained using a set of attention maps that are shared across all the classes.
In such a shared attention framework, many categories are observed to be inferred from only a few dominant attention maps, which tend to be diffused across an image rather than discriminatively fo-cusing on regions likely belonging to a speciﬁc class (see
Fig. 1). This is problematic for large-scale benchmarks comprising several hundred categories, e.g., more than 7k seen classes in Open Images [16] with signiﬁcant inter and intra-class variations. Secondly, attended features are spa-tially pooled before projection to the label space, thus en-tangling the multi-label information in the collapsed image-level feature vectors. Since multiple diverse labels can ap-pear in an image, the class-speciﬁc discriminability within such a collapsed representation is severely hampered.
Figure 1. Comparison, in terms of attention visualization, between shared attention-based LESA [13] and our approach on example
NUS-WIDE test images. For each image, the visualization of attentions of positive labels within that image are shown for LESA (top row) and our approach (bottom row). In the case of LESA, all classes in these examples are inferred from the eighth shared attention module except for dog class in (b), which is inferred from the ninth module. As seen in these examples, these dominant attention maps struggle to discriminatively focus on relevant (class-speciﬁc) regions. In contrast, our proposed approach based on a bi-level attention module (BiAM) produces attention maps by preserving class-speciﬁc discriminability, leading to an enriched feature representation. Our BiAM effectively captures region-level semantics as well as global scene-level context, thereby enabling it to accurately attend to object class (e.g., window class in (a)) and abstract concepts (e.g., reﬂection class in (a)). Best viewed zoomed in. 1.1. Contributions 2. Proposed Method
To address the aforementioned problems, we pose large-scale multi-label ZSL as a region-level classiﬁcation prob-lem. We introduce a simple yet effective region-level clas-siﬁcation framework that maintains the spatial resolution of features to keep the multi-class information disentangled for dealing with large number of co-existing classes in an im-age. Our framework comprises a bi-level attention module (BiAM) to contextualize and obtain highly discriminative region-level feature representations. Our BiAM contains re-gion and global (scene) contextualized blocks and enables reasoning about all the regions together using pair-wise re-lations between them, in addition to utilizing the holistic scene context. The region contextualized block enriches each region feature by attending to all regions within the image whereas the scene contextualized block enhances the region features based on their congruence to the scene fea-ture representation. The resulting discriminative features, obtained through our BiAM, are then utilized to perform region-based classiﬁcation through a compatibility func-tion. Afterwards, a spatial top-k pooling is performed over each class to obtain the ﬁnal predictions.
Experiments are performed on two challenging large-scale multi-label zero-shot benchmarks: NUS-WIDE [5] and Open Images [16]. Our approach performs favorably against existing methods, setting a new state of the art on both benchmarks. Particularly, on NUS-WIDE, our ap-proach achieves an absolute gain of 6.9% in terms of mAP for the ZSL task, over the best published results [13].
Here, we introduce a region-based discriminability-preserving multi-label zero-shot classiﬁcation framework aided by learning rich features that explicitly encodes both region as well as global scene contexts in an image.
Problem Formulation: Let x ∈ X denote the feature in-stances of a multi-label image i ∈ I and y ∈ {0, 1}S the cor-responding multi-hot labels from the set of S seen class la-bels Cs. Further, let AS∈RS×da denote the da-dimensional attribute embeddings, which encode the semantic relation-ships between S seen classes. With np as the number of positive labels in an image, we denote the set of attribute embeddings for the image as ay={Aj, ∀j : y[j]=1}, where
|ay|=np. The goal in (generalized) zero-shot learning is to learn a mapping f (x) : X →{0, 1}S aided by the attribute embeddings ay, such that the mapping can be adapted to in-clude the U unseen classes (with embeddings AU ∈RU ×da ) at test time, i.e., f (x) : X →{0, 1}U for ZSL and f (x) :
X →{0, 1}C for the GZSL setting. Here, C=S + U repre-sents the total number of seen and unseen classes. 2.1. Region-level Multi-label ZSL
As discussed earlier, recognizing diverse and wide range of category labels in images under the (generalized) zero-shot setting is challenging. The problem arises, primarily, due to the entanglement of features of the various differ-ent classes present in an image. Fig. 2(a) illustrates this feature entanglement in the shared attention-based classi-ﬁcation pipeline [13] that integrates multi-label features by
of seen classes (AS). The response maps are then top-k pooled along the spatial dimensions to obtain image-level per-class scores s ∈ RS, which are then utilized for train-ing the network (in Sec. 2.3). Such a region-level classiﬁ-cation, followed by a score-level pooling, helps to preserve the discriminability of the features in each of the h · w re-gions by minimizing the feature entanglement of different positive classes occurring in the image.
The aforementioned region-level multi-label ZSL frame-work relies on discriminative region-based features. Stan-dard region-based features x only encode local region-speciﬁc information and do not explicitly reason about all the regions together. Moreover, region-based features do not possess image-level holistic scene information. Next, we introduce a bi-level attention module (BiAM) to enhance feature discriminability and generate enriched features ef . 2.2. Bi-level Attention Module
Here, we present a bi-level attention module (BiAM) that enhances region-based features by incorporating both re-gion and scene context information, without sacriﬁcing the spatial resolution. Our BiAM comprises region and scene contextualized blocks, which are described next. 2.2.1 Region Contextualized Block
The region-contextualized block (RCB) enriches the region-based latent features hr by capturing the contexts from dif-ferent regions in the image. We observe encoding the indi-vidual contexts of different regions in an image to improve the discriminability of standard region-based features, e.g., the context of a region with window can aid in identifying other possibly texture-less regions in the image as house or building. Thus, inspired by the multi-headed self-attention
[30], our RCB allows the features in different regions to in-teract with each other and identify the regions to be paid more attention to for enriching themselves (see Fig. 3(b)).
To this end, the input features xr ∈ Rh×w×dr are ﬁrst pro-cessed by a 3×3 convolution layer to obtain latent features hr ∈ Rh×w×dr . These latent features are then projected to a low-dimensional space (d(cid:48) r = dr/H) to create query-key-value triplets using a total of H projection heads, h = hrWQ qr h , kr h = hrWK h , vr h = hrWV h , (2) h , WV h , WK where h∈{1, 2, .., H} and WQ h are learnable weights of 1×1 convolution layers with input and output channels as dr and d(cid:48) r, respectively. The query vector (of length d(cid:48) r) derived from each region feature1 is used to ﬁnd its correlation with the keys obtained from all the region fea-tures, while the value embedding holds the status of the cur-rent form of each region feature. h ∈ Rh×w×d(cid:48) 1Query qr r features each. Similar observation holds for keys, values, etc. r can be considered as h·w queries represented by d(cid:48)
Figure 2. Comparison of our region-level classiﬁcation frame-work (b) with the shared attention-based classiﬁcation pipeline (a) in [13]. The shared attention-based pipeline performs an attention-weighted spatial averaging of the region-based features to generate a feature vector per shared attention. These (spatially pooled) features are then classiﬁed to obtain S class scores per shared attention, which are max-pooled to obtain image-level class predictions. In contrast, our framework minimizes inter-class fea-ture entanglement by enhancing the region-based features through a feature enrichment mechanism, which preserves the spatial res-olution of the features. Each region-based enriched feature repre-sentation is then classiﬁed to S seen classes. Afterwards, per class top-k activations are aggregated to obtain image-level predictions. performing a weighted spatial averaging of the region-based features based on the shared-attention maps. In this work, we argue that entangled feature representations are sub-optimal for multi-label classiﬁcation and instead propose to alleviate this issue by posing large-scale multi-label ZSL as a region-level classiﬁcation problem. To this end, we introduce a simple but effective region-level classiﬁcation framework that ﬁrst enriches the region-based features by the proposed feature enrichment mechanism. It then classi-ﬁes the enriched region-based features followed by spatially pooling the per-class region-based scores to obtain the ﬁnal image-level class predictions (see Fig. 2(b)). Consequently, our framework minimizes inter-class feature entanglement and enhances the classiﬁcation performance.
Fig. 3 shows our overall proposed framework. Let ef ∈
Rh×w×dr be the output region-based features, which are to be classiﬁed, from our proposed enrichment mechanism (i.e., BiAM). Here, h, w denote the spatial extent of the region-based features with h · w regions. These features ef are ﬁrst aligned with the class-speciﬁc attribute embed-dings of the seen classes. This alignment is performed, i.e., a joint visual-semantic space is learned, so that the classi-ﬁer can be adapted to the unseen classes at test time. The aligned region-based features are classiﬁed to obtain class-speciﬁc response maps m ∈ Rh×w×S given by, m = ef WaA(cid:62)
S , s.t., AS ∈ RS×da , (1) where Wa ∈ Rdr×da is a learnable weight matrix that is used to reshape the visual features to attribute embeddings
Figure 3. Our region-level multi-label (G)ZSL framework: The top row shows an overview of our network architecture. Given an image, the region-level features xr are ﬁrst obtained using a backbone. The region features are enriched using a Bi-level Attention Module (BiAM). This module incorporates region (b) and scene (c) contextualized blocks which learn to aggregate region-level and scene-speciﬁc context, respectively, which is in turn used to enhance the region features. The enriched features ef are mapped to the joint visual-semantic space to relate them with class semantics, obtaining m. Per-class region-based prediction scores are then spatially pooled to generate ﬁnal image-level predictions. Notably, our design ensures region-level feature enrichment while preserving the spatial resolution uptil class predictions are made, which minimizes inter-class feature entanglement, a key requisite for large-scale multi-label (G)ZSL.
Given these triplets for each head, ﬁrst, an intra-head processing is performed by relating each query vector with
‘keys’ derived from the h · w region features. The resulting normalized relation scores (rh ∈ Rhw×hw) from the soft-max function (σ) are used to reweight the corresponding
‘value’ vectors. Without loss of generality1, the attended features αh ∈ Rh×w×d(cid:48) r are given by,
αh = rhvr h, where rh = σ( qr hkr(cid:62) h (cid:112)d(cid:48) r
). (3)
Next, these low-dimensional self-attended features from each head are channel-wise concatenated and processed by a convolution layer Wo to generate output or ∈ Rh×w×dr , or = [α1; α2; . . . αH ]Wo. (4)
To encourage the network to selectively focus on adding complimentary information to the ‘source’ latent feature hr, a residual branch is added to the attended features or and further processed with a small residual sub-network cr(·), comprising two 1×1 convolution layers, to help the net-work ﬁrst focus on the local neighbourhood and then pro-gressively pay attention to the other-level features. The en-riched region-based features er ∈ Rh×w×dr from the RCB are given by, er = cr(hr + or) + (hr + or). (5)
Consequently, the discriminability of the latent features hr is enhanced by self-attending to the context of different re-gions in the image, resulting in enriched features er. 2.2.2 Scene Contextualized Block
As discussed earlier, the RCB captures the regional con-text in the image, enabling reasoning about all regions to-gether using pair-wise relations between them. In this way,
RCB enriches the latent feature inputs hr. However, such a region-based contextual attention does not effectively en-code the global scene-level context of the image, which is necessary for understanding abstract scene concepts like night-time, protest, clouds, etc. Understanding such labels from local regional contexts is challenging due to their ab-stract nature. Thus, in order to better capture the holis-tic scene-level context, we introduce a scene contextual-ized block (SCB) within our BiAM. Our SCB attends to the region-based latent features hr, based on their congru-ence with the global image feature xg (see Fig. 3(c)). To this end, the learnable weights Wg project the features xg to a dr-dimensional space to obtain the global ‘key’ vectors kg ∈ Rdr , while the latent features hr are spatially average pooled to create the ‘query’ vectors qg ∈ Rdr , (6) qg = GAP(hr), kg = xgWg, vg = hr.
The region-based latent features hr are retained as ‘value’ features vg. Given these query-key-value triplets, ﬁrst, the query qg is used to ﬁnd its correlation with the key kg. The resulting relation score vectors rg ∈ Rdr are then used to reweight the corresponding channels in value features to ob-tain the attended features αg ∈ Rh×w×dr , given by,
αg = vg ⊗ rg, where rg = sigmoid(qg ∗ kg), (7)
compute the image-level per-class scores s ∈ RS. The net-work is trained using a simple, yet effective ranking loss
Lrank on the predicted scores s, given by, (cid:88) (cid:88) max(si[n] − si[p] + 1, 0). (10)
Lrank =
Figure 4. Effect of enhancing the region-based features through our feature enrichment mechanism: BiAM. The two comple-mentary RCB and SCB blocks in BiAM integrate region-level se-mantics and global scene-level context, leading to a more discrim-inative feature representation. While RCB alone (on the left) is able to capture the region-level semantics of person class, it con-fuses those related to protest label. However, encoding the global scene-level context from the SCB in BiAM (on the right) improves the semantic recognition of scene-level concepts like protest. where ⊗ and ∗ denote channel-wise and element-wise mul-tiplications. The channel-wise operation is chosen here since we want to use the global contextualized features to dictate kernel-wise importance of the feature channels for aggregating relevant contextual cues without disrupting the local ﬁlter signature. Similar to RCB, to encourage the network to selectively focus on adding complimentary in-formation to the ‘source’ hr, a residual branch is added after processing the attended features through a 3×3 con-volution layer cg(·). The scene-context enriched features eg ∈ Rh×w×dr from the SCB are given by, eg = cg(αg) + hr. (8)
In order to ensure the enrichment due to both region and global contexts are well captured, the enriched fea-tures (er and eg) from both region and scene contextual-ized blocks are channel-wise concatenated and processed through a 1 × 1 channel-reducing convolution layer cf (·) to obtain the ﬁnal enriched features ef ∈ Rh×w×dr , given by, ef = cf ([er; eg]). (9)
Fig. 4 shows that encoding scene context into the region-based features improves the attention maps of scene level labels (e.g., protest), which were hard to attend to using only the region context. Consequently, our bi-level attention module effectively reasons about all the image regions to-gether using pair-wise relations between them, while being able to utilize the whole image (holistic) scene as context. 2.3. Training and Inference
As discussed earlier, discriminative region-based fea-tures ef are learned and region-wise classiﬁed to obtain class-speciﬁc response maps m ∈ Rh×w×S (using Eq. 1).
The response maps m are further top-k pooled spatially to i p∈yp,n /∈yp
Here, yp = {j : y[j]=1} denotes the positive labels in image i. The ranking loss ensures that the predicted scores of the positive labels present in the image rank ahead, by a margin of at least 1, of the negative label scores.
At test time, for the multi-label ZSL task, the unseen class attribute embeddings AU ∈ RU ×da of the respective unseen classes are used (in place of AS) for computing the class-speciﬁc response maps m ∈ Rh×w×U in Eq. 1. As in training, these response maps are then top-k pooled spa-tially to compute the image-level per-class scores s ∈ RU .
Similarly, for the multi-label GZSL task, the concatenated embeddings (AC∈RC×da ) of all the classes C = S + U are used to classify the multi-label images. 3. Experiments
Datasets: We evaluate our approach on two benchmarks:
NUS-WIDE [5] and Open Images [16]. The NUS-WIDE dataset comprises nearly 270K images with 81 human-annotated categories, in addition to the 925 labels obtained from Flickr user tags. As in [13, 46], the 925 and 81 labels are used as seen and unseen classes, respectively. The Open
Images (v4) is a large-scale dataset comprising nearly 9 million training images along with 41,620 and 125,456 im-ages in validation and test sets. It has annotations with hu-man and machine-generated labels. Here, 7,186 labels, with at least 100 training images, are selected as seen classes.
The most frequent 400 test labels that are absent in the train-ing data are selected as unseen classes, as in [13].
Evaluation Metrics: We use F1 score at top-K predictions and mean Average Precision (mAP) as evaluation metrics, as in [31, 13]. The model’s ability to correctly rank labels in each image is measured by the F1, while the its image ranking accuracy for each label is captured by the mAP.
Implementation Details: Pretrained VGG-19 [28] is used to extract features from multi-label images, as in [46, 13].
The region-based features (of size h, w=14 and dr=512) from Conv5 are extracted along with the global features of size dg=4,096 from FC7. As in [13], (cid:96)2-normalized 300-dimensional GloVe [25] vectors of the class names are used as the attribute embeddings AC. The two 3×3 convolu-tions (input and output channels are set to 512) are followed by ReLU and batch normalization layers. The k for top-k pooling is set to 10, while the heads H=8. For training, we use the ADAM optimizer with (β1, β2) as (0.5, 0.999) and a gradual warm-up learning rate scheduler with an initial lr of 1e−3. Our model is trained with a mini-batch size of 32 for 40 epochs on NUS-WIDE and 2 epochs on Open Images.
Table 1. State-of-the-art comparison for multi-label ZSL and
GZSL tasks on NUS-WIDE. We report the results in terms of mAP and F1 score at K∈{3, 5}. Our approach outperforms the state-of-the-art for both ZSL and GZSL tasks, in terms of mAP and F1 score. Best results are in bold.
Table 2. State-of-the-art comparison for multi-label ZSL and
GZSL tasks on Open Images. Results are reported in terms of mAP and F1 score at K∈{10, 20}. Our approach sets a new state of the art for both tasks, in terms of mAP and F1 score. Best results are in bold.
Method
Task mAP
F1 (K = 3) F1 (K = 5)
Method
Task mAP
F1 (K = 10) F1 (K = 20)
CONSE [23]
LabelEM [1]
Fast0Tag [46]
Attention per Label [14]
Attention per Cluster [13]
LESA [13]
Our Approach
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL 9.4 2.1 7.1 2.2 15.1 3.7 10.4 3.7 12.9 2.6 19.4 5.6 26.3 9.3 21.6 7.0 19.2 9.5 27.8 11.5 25.8 10.9 24.6 6.4 31.6 14.4 33.1 16.1 20.2 8.1 19.5 11.3 26.4 13.5 23.6 13.2 22.9 7.7 28.7 16.8 30.7 19.0 3.1. State-of-the-art Comparison
NUS-WIDE: The state-of-the-art comparison for zero-shot (ZSL) and generalized zero-shot (GZSL) classiﬁcation is presented in Tab. 1. The results are reported in terms of mAP and F1 score at top-K predictions (K∈{3, 5}). The approach of Fast0Tag [46], which ﬁnds principal directions in the attribute embedding space for ranking the positive tags ahead of negative tags, achieves 15.1 mAP on the ZSL task. The recently introduced LESA [13], which employs a shared multi-attention mechanism to recognize labels in an image, improves the performance over Fast0Tag, achieving 19.4 mAP. Our approach outperforms LESA with an abso-lute gain of 6.9% mAP. Furthermore, our approach achieves consistent improvement over the state-of-the-art in terms of
F1 (K∈{3, 5}), achieving gains as high as 2.0% at K=5.
Similarly, on the GZSL task, our approach achieves an mAP score of 9.3, outperforming LESA with an absolute gain of 3.7%. Moreover, consistent performance improve-ment in terms of F1 is achieved over LESA by our approach, with absolute gains of 1.5% and 2.2% at K=3 and K=5.
Open Images: Tab. 2 shows the state-of-the-art compari-son for multi-label ZSL and GZSL tasks. The results are reported in terms of mAP and F1 score at top-K predic-tions (K∈{10, 20}). We follow the same evaluation pro-tocol as in the concurrent work of SDL [2]. Since Open
Images has signiﬁcantly larger number of labels, in compar-ison to NUS-WIDE, ranking them within an image is more challenging. This is reﬂected by the lower F1 scores in the table. Among existing methods, LESA obtains an mAP of 41.7% for the ZSL task. In comparison, our approach out-performs LESA by achieving 73.6% mAP with an absolute gain of 31.9%. Furthermore, our approach performs favor-ably against the best existing approach with F1 scores of 8.3
CONSE [23]
LabelEM [1]
Fast0Tag [46]
Attention per Cluster [13]
LESA [13]
Our Approach
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL 40.4 43.5 40.5 45.2 41.2 45.2 40.7 44.9 41.7 45.4 73.6 84.5 0.4 2.6 0.5 5.2 0.7 16.0 1.2 16.9 1.4 17.4 8.3 19.1 0.3 2.4 0.4 5.1 0.6 12.9 0.9 13.5 1.0 14.3 5.5 15.9
Figure 5. Impact of region-based classiﬁcation for the ZSL task on NUS-WIDE, in terms of mAP and F1 at K∈{3, 5}. Classi-fying spatially pooled features (blue bars) entangles the features of the different classes resulting in sub-optimal performance. In contrast, our proposed approach, which classiﬁes each region in-dividually and then spatially pools the per region class scores (red bars), minimizes the inter-class feature entanglement and achieves superior classiﬁcation performance. and 5.5 at K=10 and K=20. It is worth noting that the ZSL task is challenging due to the high number of unseen labels (400). As in ZSL, our approach obtains a signiﬁcant gain of 39.1% mAP over the best published results for GZSL and also achieves favorable performance in F1. Additional details and results are presented in the supplementary. 3.2. Ablation Study
Impact of region-based classiﬁcation: To analyse this impact, we train our proposed framework without region-based classiﬁcation, where the enriched features ef are spa-tially average-pooled to a single feature representation (of size dr) per image and then classiﬁed. Fig. 5 shows the per-formance comparison between our frameworks trained with and without region-based classiﬁcation in terms of mAP and F1. Since images have large and diverse set of positive labels, spatially aggregating features without the region-based classiﬁcation (blue bars), leads to inter-class feature entanglement, as discussed in Sec. 2.1. Instead, preserving the spatial dimension by classifying the region-based fea-tures, as in the proposed framework (red bars), mitigates
Table 3. Impact of the proposed BiAM comprising RCB and
SCB blocks. Note that all results here are reported with the same region-level classiﬁcation framework and only the features utilized within the classiﬁcation framework differs. Both RCB alone and
SCB alone achieve consistently improved performance over stan-dard region features. For both ZSL and GZSL tasks, the best per-formance is obtained when utilizing the discriminative features ob-tained from the proposed BiAM. Best results are in bold.
Method
Task mAP
F1 (K = 3) F1 (K = 5)
Standard region features
RCB alone
SCB alone
BiAM (RCB + SCB)
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL
ZSL
GZSL 21.1 6.8 23.7 7.6 23.2 8.6 26.3 9.3 28.0 12.0 31.9 14.7 29.4 14.0 33.1 16.1 26.9 14.5 29.0 17.6 27.8 16.7 30.7 19.0
Table 4. ZSL comparison on NUS-WIDE with attention vari-ants: our attention (left) and other attentions [33, 12] (right).
Method mAP
BiAM: RCB w/ LayerNorm 25.0 24.6 24.3 26.3
BiAM: RCB w/ sigmoid
BiAM: SCB w/ softmax
BiAM: Final
Method
Non-Local [33]
Criss-Cross Atn [12]
BiAM (Ours) mAP 23.1 23.9 26.3 the inter-class feature entanglement to a large extent. This leads to a superior performance for the region-based clas-siﬁcation on both multi-label ZSL and GZSL tasks. These results suggest the importance of region-based classiﬁca-tion for learning discriminative features in large-scale multi-label (G)ZSL tasks. Furthermore, Fig. 6 presents a t-SNE visualization showing the impact of our region-level classi-ﬁcation framework on 10 unseen classes from NUS-WIDE.
Impact of the proposed BiAM: Here, we analyse the im-pact of our feature enrichment mechanism (BiAM) to obtain discriminative feature representations. Tab. 3 presents the comparison between region-based classiﬁcation pipelines based on standard features hr and discriminative features ef obtained from our BiAM on NUS-WIDE. We also present results of our RCB and SCB blocks alone. Both
RCB alone and SCB alone consistently improve the (G)ZSL performance over the standard region-based features. This shows that our region-based classiﬁcation pipeline bene-ﬁts from the discriminative features obtained through the two complementary attention blocks. Furthermore, best re-sults are obtained with our BiAM that comprises both RCB and SCB blocks, demonstrating the importance of encoding both region and scene context information. Fig. 8 shows a comparison between the standard features-based classiﬁ-cation and the proposed classiﬁcation framework utilizing
BiAM on example unseen class images.
Varying the attention modules: Tab. 4 (left) shows the comparison on NUS-WIDE when ablating RCB and SCB modules in our BiAM. Including LayerNorm in RCB or re-Figure 6. t-SNE visualization showing the impact of the pro-posed region-level classiﬁcation framework on the inter-class feature entanglement. We present the comparison on 10 unseen classes of NUS-WIDE. On left: the single feature representation-based classiﬁcation pipeline, where the enriched features are spa-tially aggregated to obtain a feature vector (of length dr) and then classiﬁed. On right: the proposed region-level classiﬁcation framework, which classiﬁes the region-level features ﬁrst and then spatially pools the class scores to obtain image-level predictions.
Our classiﬁcation framework maintains the spatial resolution to preserve the region-level characteristics, thereby effectively mini-mizing the inter-class feature entanglement.
Figure 7. ZSL comparison on NUS-WIDE when varying H, top-k and h·w regions. Results improve slightly as heads H in-creases till 8 and drops beyond 8, likely due to overﬁtting to seen classes. A similar trend is observed when top-k increases. De-creasing h·w regions from 14x14 to 9x9 does not affect much. placing its softmax with sigmoid or replacing sigmoid with softmax in SCB result in sub-optimal performance com-pared to our ﬁnal BiAM. Similarly, replacing our BiAM with existing Non-Local [33] and Criss-cross [12] atten-tion blocks also results in reduced performance (see Tab. 4 (right)). This shows the efﬁcacy of BiAM, which integrates both region and holistic scene context.
Varying the hyperparameters: Fig. 7 shows the ZSL per-formance of our framework when varying heads H, k in top-k and number of regions (h·w). Performance improves as H is increased till 8 and drops beyond 8, likely due to overﬁtting to seen classes. Similarly, as top-k increases be-yond 10, features of spatially-small classes entangle and re-duce the discriminability. Furthermore, decreasing the re-gions leads to multiple classes overlapping in the same re-gions causing feature entanglement and performance drop.
Compute and run-time complexity: Tab. 5 shows that our approach achieves signiﬁcant performance gains of 6.7% and 31.3% over LESA with comparable FLOPs, memory cost, training and inference run-times, on NUS-WIDE and
Open Images, respectively. For a fair comparison, both methods are run on the same Tesla V100.
Additional examples w.r.t. failure cases of our model such as confusing abstract classes (e.g., sunset vs. sunrise) and ﬁne-grained classes are provided in the supplementary.
Table 5. Comparison of our BiAM with LESA in terms of ZSL performance (mAP), train and inference time, FLOPs and memory cost on NUS-WIDE (NUS) and Open Images (OI).
Our BiAM achieves signiﬁcant gain in performance with compa-rable compute and run-time complexity, over LESA. mAP (NUS / OI) Train (NUS / OI)
Inference FLOPs Memory
Method
LESA [10]
BiAM (Ours) 19.4 / 41.7 26.1 / 73.0 9.1 hrs / 35 hrs 7.5 hrs / 26 hrs 1.4 ms 2.3 ms 0.46 G 0.59 G 2.6 GB 2.8 GB
Figure 8. Qualitative comparison on four test examples from
NUS-WIDE, between the standard region features and our dis-criminative features. Top-3 predictions per image for both ap-proaches are shown with true positives and false positives. Com-pared to the standard region-based features, our approach learns discriminative region-based features and performs favorably. 3.3. Standard Multi-label Classiﬁcation
In addition to multi-label (generalized) zero-shot clas-siﬁcation, we evaluate our proposed region-based classiﬁ-cation framework on the standard multi-label classiﬁcation task. Here, image instances for all the labels are present in training. The state-of-the-art comparison for the standard multi-label classiﬁcation on NUS-WIDE with 81 human an-notated labels is shown in Tab. 6. Among existing methods, the work of [14] and LESA [13] achieve mAP scores of 32.6 and 31.5, respectively. Our approach outperforms all published methods and achieves a signiﬁcant gain of 15.2% mAP over the state of the art. Furthermore, our approach performs favorably against existing methods in terms of F1. 4.