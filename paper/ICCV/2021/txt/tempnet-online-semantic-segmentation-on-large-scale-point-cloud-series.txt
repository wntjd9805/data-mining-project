Abstract
Online semantic segmentation on a time series of point cloud frames is an essential task in autonomous driving.
Existing models focus on single-frame segmentation, which cannot achieve satisfactory segmentation accuracy and of-fer unstably ﬂicker among frames. In this paper, we propose a light-weight semantic segmentation framework for large-scale point cloud series, called TempNet, which can improve both the accuracy and the stability of existing semantic seg-mentation models by combining a novel frame aggregation scheme. To be computational cost-efﬁcient, feature extrac-tion and aggregation are only conducted on a small portion of key frames via a temporal feature aggregation (TFA) net-work using an attentional pooling mechanism, and such en-hanced features are propagated to the intermediate non-key frames. To avoid information loss from non-key frames, a partial feature update (PFU) network is designed to par-tially update the propagated features with the local features extracted on a non-key frame if a large disparity between the two is quickly assessed. As a result, consistent and information-rich features can be obtained for each frame.
We implement TempNet on ﬁve state-of-the-art (SOTA) point cloud segmentation models and conduct extensive experi-ments on the SemanticKITTI dataset. Results demonstrate that TempNet outperforms SOTA competitors by wide mar-gins with little extra computational cost. 1.

Introduction
In order to better perceive the driving context, most au-tomated vehicles are equipped with LiDAR sensors to con-tinuously acquire point cloud data. The performance of an online semantic segmentation algorithm for a time se-ries of point cloud frames referred to as point cloud series, is essential for an automated vehicle to make correct de-cisions in real-time. For example, a commercial off-the-shelf (COTS) LiDAR with 128 channels [7] could produce 10 frames per second with each frame containing a large number of around 480,000 points. Due to the discrete and
*Co-corresponding authors d n u o r
G h t u r t e z e e u q
S 2
V g e
S d n u o r
G h t u r t e z e e u q
S 2
V g e
S
Frame 1
Frame 2
Frame 3
Figure 1. Segmentation examples of using SequeezeSegV2, a
SOTA single-frame segmentation method, on two sequences of three consecutive point cloud frames. It can be seen that applying such schemes to point cloud series leads to unstable and inaccurate results, e.g., errors denoted by circles suddenly appear. sparse spatial distribution nature of point cloud data, se-mantic segmentation on large-scale point cloud series is a more difﬁcult task compared with semantic segmentation of videos. Given the restricted computational power of an au-tomated vehicle, a practical semantic segmentation method for point cloud series should meet the following two re-quirements. First, the segmentation results should be ac-curate so that automated vehicles can make correct driving decisions based on the results. Second, the method should be real-time, which means that any frame of point cloud in the series should be correctly segmented within a certain bounded time. Otherwise, the returned results may become invalid or useless.
In the literature, a number of point cloud semantic seg-mentation methods [27, 22, 16] have been proposed, which mainly focus on one single and static point cloud frame and may generate inconsistent segmentation results when deal-ing with consecutive frames of point clouds. For instance, as shown in Figure 1, when using SequeezeSegV2 [31], a    
state-of-the-art (SOTA) single-frame point cloud segmenta-tion scheme, segmentation errors (illustrated by dashed cir-cles in the ﬁgure) may suddenly appear and ﬂicker among a series of consecutive frames. Moreover, these schemes are too computationally expensive to process point cloud series.
As a result, there is no existing successful point cloud se-mantic segmentation scheme, to the best of our knowledge, that can handle the online point-cloud-series semantic seg-mentation problem.
In this paper, we propose a light-weight point-cloud-series semantic segmentation framework, called TempNet, which can improve both the accuracy and the efﬁciency of existing semantic segmentation models by combining a novel frame aggregation scheme.
Inspired by the work
[11, 37] in video segmentation tasks, which selectively con-ducts feature fusion to eliminate ﬂickers, features of previ-ous point cloud frames and those of the current frame are
ﬁrst efﬁciently obtained and then effectively aggregated to achieve reliable and accurate segmentation results.
There are two main challenges in designing TempNet.
First, to obtain features of each point cloud frame for aggre-gation is of prohibitive computational cost for online point-cloud-series segmentation tasks. To tackle this challenge, full feature extraction and aggregation are only conducted a small portion of frames, referred to as key frames, and such enhanced features are directly propagated to intermediate non-key frames. Furthermore, to avoid possible information loss from non-key frames, a partial feature update (PFU) network is designed to partially update the propagated fea-tures with the features extracted on a non-key frame if a large disparity between the two is quickly assessed. Due to the complexity of real-world scenarios, it is hard to obtain an optimal key frame selection strategy. We take an adap-tive frame scheduling (AFS) method to dynamically deter-mine the number of key frames according to the disparities assessed in recent non-key frames.
Second, given previous point cloud frames and their fea-tures, how to augment the features of the current key frame to improve the stability of segmentation, however, is non-trivial. As to point clouds, when objects are far or sur-rounded by interfering objects, it is hard to guarantee the robustness of single-frame segmentation schemes. We have the observation, referred to as local spatial consistency of point clouds, that the local spatial structure of an object in the point cloud should be consistent between frames al-though the object might be moving. Leveraging the local spatial consistency of point clouds, we design a temporal feature aggregation (TFA) network based on graph atten-tion convolution to effectively aggregate features of consec-utive frames. Speciﬁcally, TFA prefers to search for neigh-boring keypoints with similar geometric characteristics and semantic features in the previous frame. Moreover, an at-tention mechanism is adopted in TFA so that spatially con-sistent features would be more impactful in the aggregation.
We implement TempNet on ﬁve state-of-the-art (SOTA) point cloud segmentation models, i.e., PointNet++ [20],
GACNet [29], SequeezeSegV2 [31], DarkNet53Seg [2] and
RandLA-Net [5]. We conduct extensive experiments on the
SemanticKITTI dataset. Results demonstrate that TempNet outperforms SOTA competitors by wide margins with lit-tle extra computational cost. We highlight the main contri-butions made in this paper as follows: 1) an online point cloud series semantic segmentation framework TempNet is proposed, which is light-weight and easy to implement on existing single-frame segmentation schemes; 2) a temporal feature aggregation network is designed, which utilizes the continuity of motions and attentional pooling to effectively aggregate two point cloud frames in motion; 3) extensive experiments on the real-world SemanticKITTI dataset are conducted and results demonstrate the efﬁcacy of TempNet.
The whole suite of codebase will be released. 2.