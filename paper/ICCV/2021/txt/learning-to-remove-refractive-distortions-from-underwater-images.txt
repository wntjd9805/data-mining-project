Abstract
The fluctuation of the water surface causes refractive distortions that severely downgrade the image of an under-water scene. Here, we present the distortion-guided net-work (DG-Net) for restoring distortion-free underwater im-ages. The key idea is to use a distortion map to guide net-work training. The distortion map models the pixel dis-placement caused by water refraction. We first use a physi-cally constrained convolutional network to estimate the dis-tortion map from the refracted image. We then use a gen-erative adversarial network guided by the distortion map to restore the sharp distortion-free image. Since the distortion map indicates correspondences between the distorted image and the distortion-free one, it guides the network to make better predictions. We evaluate our network on several real and synthetic underwater image datasets and show that it out-performs the state-of-the-art algorithms, especially in presence of large distortions. We also show results of com-plex scenarios, including outdoor swimming pool images captured by drone and indoor aquarium images taken by cellphone camera. 1.

Introduction
Underwater scenes, when observed in air, suffer from strong distortion artifacts due to refraction caused by the wavy water surface. Restoring the true underwater images by removing the refractive distortions can benefit numerous tasks in underwater exploration and outer-space expedition (by extending to remove the atmospheric distortions).
However, it is non-trivial to remove the refractive distor-tions because 1) the geometric deformations are highly non-rigid and discontinuous due to the non-linear light trans-port through the wavy water surface, and 2) fast-evolving waves also cause blurriness in the image. Classical ap-proaches usually take a long sequence of images (or video) of a static underwater scene, and rely on the mean/median images [31, 30] or the “lucky patch” [16, 14], which hap-pens to be free from distortion in a certain frame, to restore the latent distortion-free image. As these methods require
Figure 1. We design a physics-based distortion-guided network for underwater image correction. Our method predicts the distortion-free image, given three distorted underwater images. video input of a static scene, they cannot be used for images captured on a moving platform (for example, an underwater vehicle). The seminal work of [34] presents a model-based tracking method to undistort underwater images. But their parametric model cannot be easily tuned and applied to arbi-trary waves. Most recently, Li et al. [28] propose a learning-based method to correct refractive distortions using a sin-gle image. This work demonstrates great potential of using deep neural networks to tackle the challenging problem of refractive distortion removal. But this network does not ac-count for physical constraints and requires a large training set (over 300k images from the ImageNet [10]).
In this paper, we present the distortion-guided network (DG-Net) for restoring distortion-free underwater images.
The key idea is to use a distortion map to guide network training. The distortion map models the pixel displace-ment caused water refraction. As the distortion map reveals correspondences between the distorted and distortion-free images, we can use it to guide the network to make bet-ter predictions. We first use convolutional neural network (CNN) to estimate the distortion map from the refracted im-age. Specifically, we design training losses that follow the physical model of refractive distortions. We also exploit the temporal consistency of the distortion map by taking three sequential images as input. We use three parallel CNNs
Figure 2. The overall architecture of DG-Net. It consists of two subnets: a convolutional network for estimating the refractive distortions (Dis-Net) and a distortion-guided generative adversarial network for restoring the distortion-free image (DG-GAN). Note that we have three CNN networks in Dis-Net which takes each of three inputs. The generator and discriminator of DG-GAN are represented by G and
D, respectively. F and B denotes forward and backward mapping of images. to generalize features from each input, and then use recur-rent layers to refine the CNN-predicted distortion maps by enforcing the temporal consistency among them. We can use the estimated distortion map to correct slight refrac-tive distortions. Since large distortion are non-invertible (due to many-to-one mapping), we then use a distortion-guided generative adversarial network (GAN) to recover sharp distortion-free image. The distortion map is used to guide the training of both generator and discriminator of the
GAN. Our network is trained on a synthetic refracted image dataset, with patterns that resemble the underwater scenes.
We evaluate the DG-Net on our own synthetic dataset and several real captured underwater image datasets [24, 33, 34]. The results show that our method out-performs the state-of-the-arts [23, 24, 28, 30, 34], especially in presence of large distortions. Compared with the model-based meth-ods [24, 34, 30], we do not need long video sequence of a static underwater scene to achieve accurate reconstruction.
Although we still take three images to exploit the temporal constraints, the images can be captured with the burst mode in a very short time interval. Our method can therefore be used for dynamic scenes such as videos from a flying drone and videos of aquatic scenes with moving objects. Com-pared with the learning-based methods [23, 28], our net-work requires fewer training data (around one tenth in size), but achieves better accuracy in presence of large distortions and generalizes well on real scenes. 2.