Abstract
We introduce the active audio-visual source separation problem, where an agent must move intelligently in order to better isolate the sounds coming from an object of in-terest in its environment. The agent hears multiple audio sources simultaneously (e.g., a person speaking down the hall in a noisy household) and it must use its eyes and ears to automatically separate out the sounds originating from a target object within a limited time budget. Towards this goal, we introduce a reinforcement learning approach that trains movement policies controlling the agent’s camera and microphone placement over time, guided by the improvement in predicted audio separation quality. We demonstrate our approach in scenarios motivated by both augmented real-ity (system is already co-located with the target object) and mobile robotics (agent begins arbitrarily far from the target object). Using state-of-the-art realistic audio-visual simula-tions in 3D environments, we demonstrate our model’s ability to ﬁnd minimal movement sequences with maximal payoff for audio source separation. Project: http://vision. cs.utexas.edu/projects/move2hear. 1.

Introduction
Audio-visual events play an important role in our daily lives. However, in real-world scenarios, physical factors can either restrict or facilitate our ability to perceive them. For example, a father working upstairs might move to the top of the staircase to better hear what his child is calling out to him from below; a traveler in a busy airport may shift closer to the gate agent to catch the ﬂight delay announcements amidst the din, without moving too far in order to keep her suitcase in sight; a friend across the table in a noisy restaurant may tilt her head to hear the dinner conversation more clearly, or scooch her chair to better catch music from the band onstage.
Such examples show how controlled sensor movements can be critical for audio-visual understanding. In terms of audio sensing, a person’s nearness and orientation relative to a sound source affects the clarity with which it is heard, especially when there are other competing sounds in the
Figure 1: Active audio-visual source separation. Given multiple mixed audio sources Si in a 3D environment, the agent is tasked to separate a target source (shown in green) by intelligently moving around using cues from its egocentric audio-visual input to improve the quality of the predicted target audio signal. See text. environment. In terms of visual sensing, one must see obsta-cles to circumvent them, spot desired and distracting sound sources, use visual context to hypothesize an out-of-view sound source’s location, and actively look for “sweet spots" in the visible 3D scene that may permit better listening.
In this work, we explore how autonomous multi-modal systems might learn to exhibit such intelligent behaviors.
In particular, we introduce the task of active audio-visual source separation: given a stream of egocentric audio-visual observations, an agent must decide how to move in order to recover the sounds being emitted by some target object, and it must do so within bounded time. See Figure 1. Unlike traditional audio-visual source separation, where the goal is to isolate sounds in passive, pre-recorded video [23, 27, 1, 20, 43, 28, 17, 29], the proposed task calls for actively controlling the camera and microphones’ positions over time.
Unlike recent embodied AI work on audio-visual navigation, where the goal is to travel to the location of a sounding object [15, 25, 14, 16], the proposed task calls for returning a separated soundtrack for an object of interest in limited time, without necessarily traveling all the way to it.
We consider two variants of the new task. In the ﬁrst, the system begins exactly at the location of the desired sounding object and must ﬁne-tune its positioning to hear better; this variant is motivated by augmented reality (AR) applications where the object of interest is known and visible (e.g., the person seated across from someone wearing an assistive
audio-visual AR device) yet local movements of the device sensors are still beneﬁcial to improve the audio separation. In the second variant, the system begins at an arbitrary position away from the object of interest; this variant is motivated by mobile robotics applications, where an agent detects a sound from afar (e.g., the child calling from downstairs) but it is entangled with distractors and requires larger movements within the environment to hear correctly. We refer to these scenarios as near-target and far-target, respectively.
Towards addressing the active audio-visual source sep-aration problem, we propose Move2Hear, a reinforcement learning (RL) framework where an agent learns a policy for how to move to hear better. The agent receives an egocentric audio-visual observation sequence (RGB and binaural au-dio) along with the target category of interest1, and at each time step decides its next motion (a rotation or translation of the camera and microphones). During training, as it aggre-gates these observations over time with an explicit memory module and recurrent network, the agent is rewarded for improving its estimate of the target object’s latent (monau-ral) sound. In particular, the reward promotes movements that better resolve the target sound from the other distractor sounds in the environment. Our approach handles both near-and far-target scenarios.
Importantly, optimal positioning for audio source separa-tion is not the same as navigation to the source, both because the agent faces a time budget—it may be impossible to reach the target in that time—and also because the geometry of the 3D environment and relative positions of distractor sounds make certain positions relative to the target more amenable to separation. For example, in Fig. 1 the agent is tasked with separating the audio from object S3. Here, going directly to
S3 is not ideal, as that position will have high interference from the other audio sources in the scene, S1 and S2. By moving around the kitchen bar, the agent manages to dampen the signal from S1 signiﬁcantly due to the intermediate ob-stacles (walls) and, simultaneously, emphasize the signal from S3 compared to S2, thus leading to better separation.
We test our approach with realistic audio-visual
SoundSpaces [15] simulations on the Habitat platform [54] comprising 47 real-world Matterport3D environment scans [10], along with an array of sounds from diverse hu-man speakers, music, and other background distractors. Our model successfully learns how to move to hear its target more clearly in unseen scenes, surpassing baselines that sys-tematically survey their surroundings, smartly or randomly explore, or navigate directly to the source. We explore the synergy of both vision and audio for solving this task.
Our main contributions are 1) we deﬁne the active audio-visual separation task, a new direction for embodied AI research; 2) we present the ﬁrst approach to begin tackling this task, namely a new RL-based framework that integrates 1e.g., a human speaker, a musical instrument, or some other sound type sound separation and visual navigation motion policies; and 3) we thoroughly experiment with a variety of sounds, visual environments, and use cases. While just the ﬁrst step in this area, we believe our work lays groundwork to explore new problems for multi-modal agents that move to hear better. 2.