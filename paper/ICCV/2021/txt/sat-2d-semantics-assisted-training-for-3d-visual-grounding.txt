Abstract 3D visual grounding aims at grounding a natural lan-guage description about a 3D scene, usually represented in the form of 3D point clouds, to the targeted object re-gion. Point clouds are sparse, noisy, and contain limited semantic information compared with 2D images. These in-herent limitations make the 3D visual grounding problem more challenging. In this study, we propose 2D Semantics
Assisted Training (SAT) that utilizes 2D image semantics in the training stage to ease point-cloud-language joint rep-resentation learning and assist 3D visual grounding. The main idea is to learn auxiliary alignments between rich, clean 2D object representations and the corresponding ob-jects or mentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object label, image feature, and 2D geometric feature, as the extra input in training but does not require such inputs during inference. By effectively uti-lizing 2D semantics in training, our approach boosts the accuracy on the Nr3D dataset from 37.7% to 49.2%, which signiﬁcantly surpasses the non-SAT baseline with the identi-cal network architecture and inference input. Our approach outperforms the state of the art by large margins on multiple 3D visual grounding datasets, i.e., +10.4% absolute accu-racy on Nr3D, +9.9% on Sr3D, and +5.6% on ScanRef. 1.

Introduction
Visual grounding provides machines the ability to ground a language description to the targeted visual region.
The task has received wide attention in both datasets [54, 31, 19] and methods [16, 46, 53, 50]. However, most previ-ous visual grounding studies remain on images [54, 31, 19] and videos [57, 38, 51], which contain 2D projections of inherently 3D visual scenes. The recently proposed 3D vi-sual grounding task [1, 4] aims to ground a natural language description about a 3D scene to the region referred to by a language query (in the form of a 3D bounding box). The 3D visual grounding task has various applications, includ-ing autonomous agents [40, 47], human-machine interac-Figure 1. 3D visual grounding aims to ground a language query to a targeted 3D object region, as shown by the green 3D bound-ing box. (a) Previous 3D visual grounding studies are trained with a sole 3D grounding loss that maximizes the similarity between positive object-query pairs. However, the sole objective is less ef-fective as point clouds are sparse and noisy. (b) 2D semantics con-tain rich and clean object representations and can be used as extra visual inputs to assist 3D grounding. However, requiring extra 2D inputs in inference limits potential application scenarios. (c) Our proposed 2D Semantics Assisted Training (SAT) uses 2D seman-tics only in training and does not require extra inputs in inference.
The green and red boxes are the targeted and distracting objects. tion in augmented/mixed reality [20, 22], intelligent vehi-cles [29, 12], and so on. i.e.,
Visual grounding tries to learn a good joint represen-the 3D tation between visual and text modalities, point cloud and language query in 3D visual grounding.
As shown in Figure 1 (a), previous 3D grounding stud-ies [1, 4, 17, 55] directly learn the joint representation with a sole 3D grounding objective of maximizing the posi-tive object-query pairs’ similarity scores. Speciﬁcally, the model ﬁrst generates a set of 3D object proposals and then fuses each proposal with the language query to predict a similarity score. The framework is trained with a sole objec-tive that maximizes the paired object-query scores and min-imizes the unpaired ones’ scores. However, direct joint rep-resentation learning is challenging and less effective since 3D point clouds are inherently sparse, noisy, and contain limited semantic information. Given that the 2D object rep-resentation provides rich and clean semantics, we explore using 2D image semantics to help 3D visual grounding.
How to assist 3D tasks with 2D image semantics remains an open problem. Previous studies on 3D object detection and segmentation have proposed a series of methods that take 2D semantics as extra visual inputs to assist 3D tasks.
Representative approaches include aligning the 2D object detection results with 3D bounding boxes [34, 48, 25] and concatenating the image visual feature with 3D points [24, 14, 44, 41, 32]. However, these methods require extra 2D inputs in both training and inference. The necessity of extra input 2D data during inference limits potential application scenarios since 2D inputs might not exist in inference or re-quire extra pre-processing, such as 2D-3D matching and 2D detection. Instead of as extra visual inputs in both training and inference (as shown in Figure 1 (b)), we explore using 2D semantics only in training to assist 3D visual grounding.
In this study, we propose 2D Semantics Assisted Train-ing (SAT), which utilizes 2D image semantics (in the form of object label, image feature, and 2D geometric feature) to ease joint representation learning between the 3D scene and language query. As shown in Figure 1 (c), in addition to the main 3D visual grounding loss [1, 4] that maximizes the score between the paired 3D object and language query,
SAT introduces auxiliary loss functions that align objects in 2D images with the corresponding ones in 3D point clouds or language queries. The learned auxiliary alignments ef-fectively distill the rich and clean 2D object representation to assist 3D visual grounding. Speciﬁcally, in SAT, we study the training loss design for auxiliary alignments and the encoding method for 2D semantics features. For the for-mer, we propose an object correspondence loss based on the triplet loss [18, 10, 45, 26] for 3D and 2D object alignment.
For the latter, we propose a transformer attention mask that generates good 2D semantics features and prevents leaking 2D inputs to the output module.
We experiment with the SAT approach on a transformer-based model [42] we propose and name as 3D grounding transformer. We benchmark SAT on the Nr3D [1], Sr3D [1], and ScanRef [4] datasets. The extra 2D semantics, together with SAT’s specially designed way of using them, effec-tively help the model learn a better 3D object point cloud representation and ease joint representation learning. With the same network architecture and inference inputs, SAT improves the grounding accuracy on Nr3D from the non-SAT baseline’s 37.7% to 49.2%.
In summary, our main contributions are:
• We propose 2D Semantics Assisted Training (SAT) that assists 3D visual grounding with 2D semantics.
To the best of our knowledge, SAT is the ﬁrst method that helps 3D tasks with 2D semantics in training but does not require 2D inputs during inference.
• With the proposed object correspondence loss and the 2D semantics encoding method, SAT effectively uti-lizes 2D semantics to learn a better 3D object repre-sentation, which leads to signiﬁcant accuracy improve-ments on the Nr3D [1] (+10.4% in absolute accuracy),
Sr3D [1] (+9.9%), and ScanRef [4] (+5.6%) datasets. 2.