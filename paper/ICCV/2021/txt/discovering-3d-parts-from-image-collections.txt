Abstract
Reasoning 3D shapes from 2D images is an essential yet challenging task, especially when only single-view images are at our disposal. While an object can have a compli-cated shape, individual parts are usually close to geomet-ric primitives and thus are easier to model. Furthermore, parts provide a mid-level representation that is robust to appearance variations across objects in a particular cat-egory. In this work, we tackle the problem of 3D part dis-covery from only 2D image collections. Instead of relying on manually annotated parts for supervision, we propose a self-supervised approach, latent part discovery (LPD).
Our key insight is to learn a novel part shape prior that allows each part to ﬁt an object shape faithfully while con-strained to have simple geometry. Extensive experiments on the synthetic ShapeNet, PartNet, and real-world Pas-cal 3D+ datasets show that our method discovers consis-tent object parts and achieves favorable reconstruction ac-curacy compared to the existing methods with the same level of supervision. Our project page with code is at https://chhankyao.github.io/lpd/. 1.

Introduction
Recognizing and reasoning about objects surrounding us is essential for many computer vision systems. While deep learning models have been shown to perform well at rec-ognizing [27, 46, 17] and localizing [13, 44, 35] objects in a 2D image, reasoning 3D attributes of objects from a single image remains a challenging task. Single-view 3D reasoning is fundamentally ill-posed due to several factors that cause ambiguous object appearance in 2D images, e.g., camera pose, self-occlusions, lighting, and material proper-ties. Although objects in general have complicated shapes, they can usually be decomposed into parts that have simpler geometry and are relatively easy to model. Furthermore, most object instances of a particular category share similar part conﬁgurations, e.g., the wings, body, and tail of air-planes. In this work, we propose to tackle the problem by discovering faithful and consistent 3D parts from 2D image collections. Compared to existing single-view 3D recon-struction approaches that directly predict an object shape, we aim to learn rich and dense part conﬁgurations which form an entire object when combined.
Although several recent methods [48, 37, 3, 10, 33, 36, 41, 24] leverage part-based representations for 3D object reasoning, they rely on either 3D object shapes or explicit part annotations as supervision. Moreover, the learned parts only serve as additional information and are not exploited to improve 3D reconstruction. Considering that collect-ing ground-truth 3D shapes and the corresponding part la-bels is labor intensive, we follow a practical scenario where only single-view images, 2D object silhouettes, and camera viewpoints are available for model training. In contrast to existing techniques, our method automatically discovers 3D parts from image collections in a self-supervised manner.
A common practice to represent 3D parts is to use geo-metric primitives such as ellipsoids or cuboids [48]. They
provide a strong regularization of part shapes but are usu-ally too coarse to faithfully represent object parts. As an alternative, several approaches adopt meshes [23, 50, 14, 22, 34, 18, 15, 32] or point-cloud [8, 20, 28, 40, 6] repre-sentations. Although these representations are more expres-sive and can faithfully describe part shapes, they lack part shape regularization that is particularly needed in a weakly-supervised or unsupervised setting. The key insight of this work is to represent 3D parts with deep latent embeddings.
Speciﬁcally, we propose to learn a prior distribution of part shapes with a variational auto-encoder (VAE) [26] that en-codes part shapes as latent embeddings. We call this net-work Part-VAE and pre-train it with a set of geometric prim-itives like cones, cylinders, cuboids, and ellipsoids. We then learn a reconstruction network that takes an input image and predicts part embeddings to obtain a 3D mesh by passing through the decoder of Part-VAE. To further improve the quality of part discovery and reconstruction, we propose a novel part adversarial loss which involves re-assembling parts from different objects in the same category. We name the proposed method latent part discovery (LPD). Figure 1 shows several reconstruction results with and without part prior, which demonstrate that LPD can discover consistent parts and produce faithful reconstruction to the input image.
We evaluate LPD on the synthetic ShapeNet [1], Part-Net [39] and the real-world Pascal 3D+ [51] datasets. Both quantitative and qualitative results demonstrate that our ap-proach achieves favorable performance against the state-of-the-art methods using the same level of supervision.
In addition to part discovery, our part representation enables object manipulation like selective part swapping, interpola-tion, and random shape generation from the latent space. In this work, we make the following contributions:
• We propose a part-based single-view 3D reasoning net-work which can automatically discover object parts. To the best of our knowledge, this is the ﬁrst work that dis-covers 3D parts in a self-supervised manner without using any 3D shape or multi-view supervision.
• We develop Part-VAE to learn a latent prior over part shapes. We show that training with geometric primi-tives can learn useful part embeddings, allowing each part to faithfully represent object shape while constrained to have simple geometry.
• We conduct extensive experiments on both synthetic and natural images. Qualitatively, our method produces more faithful and consistent object parts compared to other part-based methods. Quantitatively, the discovered parts improve whole-object reconstruction and achieve favor-able accuracy against the state-of-the-art techniques. In addition, our Part-VAE allows us to manipulate object parts for various applications. 2.