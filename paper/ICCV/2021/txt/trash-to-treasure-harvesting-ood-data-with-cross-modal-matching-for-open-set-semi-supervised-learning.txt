Abstract
Open-set semi-supervised learning (open-set SSL) inves-tigates a challenging but practical scenario where out-of-distribution (OOD) samples are contained in the unlabeled data. While the mainstream technique seeks to completely filter out the OOD samples for semi-supervised learning (SSL), we propose a novel training mechanism that could effectively exploit the presence of OOD data for enhanced feature learning while avoiding its adverse impact on the
SSL. We achieve this goal by first introducing a warm-up training that leverages all the unlabeled data, including both the in-distribution (ID) and OOD samples. Specifi-cally, we perform a pretext task that enforces our feature extractor to obtain a high-level semantic understanding of the training images, leading to more discriminative fea-tures that can benefit the downstream tasks. Since the OOD samples are inevitably detrimental to SSL, we propose a novel cross-modal matching strategy to detect OOD sam-ples. Instead of directly applying binary classification [39], we train the network to predict whether the data sample is matched to an assigned one-hot class label. The appeal of the proposed cross-modal matching over binary classifica-tion is the ability to generate a compatible feature space that aligns with the core classification task. Extensive ex-periments show that our approach substantially lifts the performance on open-set SSL and outperforms the state-of-the-art by a large margin. 1.

Introduction
“One man’s trash is another man’s treasure.” – Hector Urquhart
Semi-supervised learning (SSL) provides an effective way of leveraging massive unlabeled data to improve the performance of deep neural network when only limited la-beled samples are available. Most existing SSL methods
*Equal contribution.
†Corresponding author.
Figure 1: t-SNE [26] visualization of image features ex-tracted from CIFAR100 [20]. Images of the same category are shown in the same color. (a) Features learned by our method are more compact and cleaner than those learned by MTCF [39]. (b) By leveraging OOD samples in the pro-posed pretext training, we have achieved more discrimina-tive feature space than using ID samples only. assume that labeled and unlabeled data share the same cat-egory space. However, this assumption is difficult to satisfy since it still requires tedious efforts to confirm the purity of the unlabeled data. Very recently, Yu et al. [39] proposed a more realistic setting called open-set semi-supervised learn-ing (open-set SSL). Open-set SSL considers a more chal-lenging but practical scenario where outliers, that do not belong to the categories of the labeled data, may exist in the unlabeled data. Resolving the open-set SSL problem has crucial practical meanings as it can significantly reduce the workload of data preparation in the actual applications.
A straightforward approach to cope with the out-of-distribution (OOD) samples is to completely remove them
from the SSL training, as the prior works [28] have shown that including OOD unlabeled data can severely impact the performance of SSL. While there exist a diverse collection of approaches for OOD sample detection, they typically re-quire a large corpus of in-distribution (ID) data with class labels. However, due to the scarcity of the labeled data in SSL, the existing OOD detection methods would fail to achieve satisfactory performance and are hence not suitable to be deployed in the open-set SSL.
In order to eliminate the influence of OOD samples, [39] devises a multi-task curriculum framework (MTCF) with a binary OOD classification head that strives to filter out all the OOD samples. The classification of in-distribution (ID) samples and the detection of OOD samples are unified into a joint optimization framework, where unlabeled samples with lower OOD scores will be gradually added for semi-supervised training. However, the proposed binary OOD classification task and the ID classification have conflicting goals in terms of feature learning. Specifically, the training of OOD detection aims to cluster all ID samples (regardless of their categories) into one category (i.e., ID data) while the task of ID classification tends to enhance the category discrimination between ID samples. Unifying the contra-dictory optimization goals into one framework that shares a backbone network could compromise the final performance and increase the difficulty of training.
In this paper, we present a novel training framework for open-set SSL that can effectively exploit the presence of
OOD data for enhanced feature learning while avoiding its adverse impact to the SSL. First, instead of completely dis-carding the OOD data, we introduce a warm-up training that makes full use of all the unlabeled data, including the OOD samples, to enhance the representation learning of our back-bone network. Unlike the conventional pre-training, our warm-up training performs a pretext task that deviates from the target application. In particular, we ask the network to predict the rotation of the rotationally augmented data in a self-supervised manner. This enforces our backbone model to obtain high-level semantic understanding of the images and hence leads to more discriminative features that could benefit the downstream applications, e.g. the classification task. Particularly, as shown in Figure 1(b), OOD samples, which are outliers in SSL algorithms, turn out to be trea-sures that can enhance feature learning when fully utilized in the proposed self-supervised pretext training. The idea of leveraging self-supervised techniques for boosting the per-formance of semi-supervised learning has been shown ef-fective in the previous work [41]. However, it is only ver-ified in the traditional SSL, where the unlabeled data share the same category space with the labeled ones. We are the first to investigate this idea in the open-set setting and show that the self-supervised auxiliary task could be beneficial to open-set SSL with properly designed training strategy.
Second, we propose a more effective approach for de-tecting and filtering OOD samples based on a novel cross-modal matching mechanism. First of all, each unlabeled sample is assigned the category with the highest predicted probability of the model as a pseudo label. We then propose a cross-modal matching head to infer whether the embed-ding of the image and its pseudo-label are matched. Once trained, OOD samples can be screened out due to its low confidence with all the ID categories. Unlike the binary classification based OOD detection [39], the feature learn-ing of cross-modal matching aligns well with that of the target ID classification task, as both strive to achieve bet-ter discrimination between the image features of different categories. We show in Figure 1(a) that our method can ob-tain features with much more compact and purified clusters than that of [39]. Furthermore, we can effectively detect
ID samples with incorrect pseudo labels, also coded “hard” samples, via cross-modal matching. This helps to further improve the performance of the trained model since hard samples could harm the model training especially at the early stage when the prediction accuracy of pseudo labels is relatively low. We propose an adaptive training mech-anism which gradually involves more hard samples as the model proceeds to achieve better performance.
Our proposed approach is a general training framework that can be easily implemented into existing SSL methods.
We show that our method greatly improves the state-of-the-art performance in extensive open-set semi-supervised image recognition benchmarks including CIFAR-10 [20],
Animals-10, CIFAR-100, and TinyImageNet [22]. We sum-marize our contributions as follows:
• A novel training pipeline for open-set SSL that lever-ages the presence of OOD samples for enhanced fea-ture learning while avoiding their adverse impact.
• A specially tailored warm-up training method that uses self-supervised learning to boost the performance of open-set SSL.
• A novel OOD and hard sample detection algorithm based on cross-modal matching, which achieves com-patible feature space with the target classification task.
• New state-of-the-art performance on open-set SSL including CIFAR-10, over extensive benchmarks
CIFAR-100, TinyImageNet, and Animals-10. 2.