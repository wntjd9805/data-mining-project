Abstract
In this paper, we present a novel Dynamic DETR (De-tection with Transformers) approach by introducing dy-namic attentions into both the encoder and decoder stages of DETR to break its two limitations on small feature res-olution and slow training convergence. To address the
ﬁrst limitation, which is due to the quadratic computa-tional complexity of the self-attention module in Trans-former encoders, we propose a dynamic encoder to ap-proximate the Transformer encoder’s attention mechanism using a convolution-based dynamic encoder with various attention types. Such an encoder can dynamically adjust attentions based on multiple factors such as scale impor-tance, spatial importance, and representation (i.e., feature dimension) importance. To mitigate the second limitation of learning difﬁculty, we introduce a dynamic decoder by replacing the cross-attention module with a ROI-based dy-namic attention in the Transformer decoder. Such a decoder effectively assists Transformers to focus on region of inter-ests from a coarse-to-ﬁne manner and dramatically lowers the learning difﬁculty, leading to a much faster convergence with fewer training epochs. We conduct a series of experi-ments to demonstrate our advantages. Our Dynamic DETR signiﬁcantly reduces the training epochs (by 14×), yet re-sults in a much better performance (by 3.6 on mAP). Mean-while, in the standard 1× setup with ResNet-50 backbone, we archive a new state-of-the-art performance that further proves the learning effectiveness of the proposed approach. 1.

Introduction
Object detection aims at predicting a set of bounding boxes and category labels for each object of interest. Mod-ern object detectors are based on convolutional neural net-works, and share the same paradigm – a backbone for fea-ture extraction and a head for localization and classiﬁca-tion tasks [22, 10]. Until recently, Detection Transformer (DETR) has been proposed as an alternative solution to the
Figure 1. Convergence curve comparison between our proposed approach and state-of-the-art methods. Our Dynamic DETR largely reduces the training epochs (by 14×), yet results in a sig-niﬁcantly better performance (by 3.6). object detection problem.
It views object detection as a set-based matching problem. By leveraging Transformers
[25] originally developed for language tasks, it is able to model the relations of objects and their global image con-text from a set of learned object queries. It then performs a global optimization that forces unique predictions from ob-ject queries via bipartite matching, effectively removing the need of hand-designed components such as non-maximum suppression (NMS) and anchor generation in traditional ob-ject detection methods.
However, DETR suffers from several problems that pre-vent it from wide adoption in the community. On one hand, the input resolution of features maps is limited in the native
Transformer as feature encoder, since the complexity of the self-attention module grows quadratically with the increase of the input resolution. It results in incompatibility to the typical feature pyramid that is widely used in modern ob-ject detectors, and relatively low performance at detecting small objects. On the other hand, it requires much longer
training epochs to converge than the existing object detec-tors since the cross-attention module struggles to learn on a large global feature map from an initial dense attention to a ﬁnal sparse attention. Thus, it is the high demand of an effective solution for improving DETR.
Recent work Deformable DETR [29], which combines the sparse spatial sampling of deformable convolution, and the relation modeling capability of Transformers, to miti-gate the slow convergence and high complexity issues of
DETR. It has achieved noticeable improvements on perfor-mance and efﬁciency in training. It is interesting to exploit if the efﬁciency and performance of DETR can be further improved.
In this paper, we propose an alternative solution to ad-dress the above two problems of DETR by a dynamic at-tention framework, called Dynamic DETR, which consists of a dynamic encoder and a dynamic decoder. We replace the Transformer encoder in DETR with a new convolution-based dynamic encoder, which apply dynamic attention on full scales of feature pyramid based on scale importance, spatial importance, and representation (i.e., feature dimen-sion) importance. Since it makes self-attentions feasible on full scale of representations from low to high resolutions, the performance of DETR can be signiﬁcantly boosted. In addition, the dynamic decoder replaces the cross-attention module in the DETR decoder with a ROI-based dynamic at-tention, which can effectively assists Transformers to focus on regions of interest in a coarse-to-ﬁne manner and dra-matically lowers the learning difﬁculty, leading to a much faster convergence with fewer training epochs.
Our contribution can be summarized in three-folds:
• We propose a novel Dynamic DETR approach, which coherently combines a dynamic convolution-based en-coder and a dynamic Transformer-based decoder. The proposed approach signiﬁcantly improves the repre-sentation ability of object detection head and the learn-ing efﬁciency without any computational overhead.
• Compared to the original DETR, Our Dynamic DETR largely reduces the training epochs (by 14×), yet re-sults in a signiﬁcantly better performance (by 3.6), shown in Figure 1.
• To our best knowledge, we are the ﬁrst end-to-end method that achieves a better than traditional perfor-mance in the standard 1x setup with a ResNet-50 back-bone, at 42.9 mAP. 2.