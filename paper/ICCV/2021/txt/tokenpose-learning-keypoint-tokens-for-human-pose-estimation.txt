Abstract
Human pose estimation deeply relies on visual clues and anatomical constraints between parts to locate keypoints.
Most existing CNN-based methods do well in visual repre-sentation, however, lacking in the ability to explicitly learn the constraint relationships between keypoints. In this pa-per, we propose a novel approach based on Token repre-In de-sentation for human Pose estimation (TokenPose). tail, each keypoint is explicitly embedded as a token to simultaneously learn constraint relationships and appear-ance cues from images. Extensive experiments show that the small and large TokenPose models are on par with state-of-the-art CNN-based counterparts while being more lightweight. Specifically, our TokenPose-S and TokenPose-L achieve 72.5 AP and 75.8 AP on COCO validation dataset respectively, with significant reduction in parameters (↓ 80.6% ; ↓ 56.8%) and GFLOPs (↓ 75.3%; ↓ 24.7%). Code is publicly available1. 1.

Introduction 2D human pose estimation aims to localize human anatomical keypoints which deeply relies on both visual cue and keypoints constraint relationships. It is a fundamental task in computer vision, which has attracted extensive at-tention from academia and industry.
Over the past decade, deep convolutional neural net-works have achieved impressive performances on human pose estimation due to their powerful capacity in visual rep-resentation and recognition [8, 29, 22, 21, 38, 12, 37, 24].
Since heatmap representation has become the standard la-*This work was done when Yanjie and Sen Yang were interns at
MEGVII Tech.
†Corresponding author. 1https://github.com/leeyegy/TokenPose
Figure 1. The process of predicting the location of the left ankle.
For visual cue learning, the proposed TokenPose focuses on the global context in the first few layers, and then gradually converges to some local regions as the network goes deeper. In the last few layers, TokenPose has considered hip and knee in turn which are close to the target keypoint, and finally localizes the position of the left ankle. For constraint cue learning, TokenPose shows that localizing the left ankle mostly relies on the left knee and right ankle, corresponding to adjacency constraint and symmetric con-straint respectively. bel representation to encode the positions of keypoints, most existing models tend to use fully convolutional lay-ers to maintain the 2D-structure of feature maps until the network output. Nevertheless, there are usually no concrete variables abstracted by such CNN models to directly rep-resent the keypoint entities, which limits the ability of the model to explicitly capture constraint relationships between parts.
Recently, Transformer [35] and its variants that origi-nated from natural language processing (NLP) have merged as new choices for various vision tasks. Its ability of model-ing global dependencies is more powerful than CNN, which points out a promising way to efficiently capture relation-ships between visual entities/elements. And in the field of
NLP, all language elements such as words or characters are usually symbolized by embeddings or token vectors with fixed dimensions, so as to better measure their similarities in a vector space, like the way of word2vec [20].
We borrow such a concept of “token” and present a novel token-based representation for human pose estima-tion, namely TokenPose. Specifically, we conduct two dif-ferent types of tokenizations: keypoint tokens and visual tokens. Visual tokens are yielded by uniformly splitting an image into patches and mapping the flattened patches into embeddings with fixed dimensions. Meanwhile, keypoint tokens are randomly initialized embeddings, each of which represents a specific type of keypoint (e.g., left knee, left ankle, right eye, etc.). The resulting keypoint tokens can learn both visual clues and constraint relations from inter-actions with visual tokens and the other keypoint tokens re-spectively. An example of how the proposed model predicts the location of left ankle is shown in Figure 1. The positions of keypoints are finally estimated over the token-based rep-resentation outputted by our network. The architecture of
TokenPose is illustrated in Figure 2.
It is worth noting that TokenPose learns the statistic con-straint relationships between keypoints from large amounts of data. Such information is encoded into keypoint to-kens that can record their relationships by vector similari-ties. During inference, TokenPose associates keypoint to-kens with those visual tokens whose corresponding patches possibly contain the target keypoints. By visualizing the attentions, we can observe how they interact and how the model exploits cues to localize keypoints.
The contributions are summarized as follows:
• We propose to use token to represent each keypoint en-tity. In this way, visual cue and constraint cue learning are explicitly incorporated into a unified framework.
• Both hybrid and pure Transformer-based architectures are explored in this work. As far as we know, proposed
TokenPose-T is the first pure Transformer-based model for 2D human pose estimation.
• We conduct experiments over two widely-used bench-mark datasets: COCO keypoint detection dataset [19]
TokenPose and MPII Human Pose dataset [1]. achieves competitive state-of-the-art performance with much fewer parameters and computation cost com-pared with existing CNN-based counterparts. 2.