Abstract pooling and strided convolution).
Extracting robust feature representation is one of the key challenges in object re-identiﬁcation (ReID). Although convolution neural network (CNN)-based methods have they only process one local achieved great success, neighborhood at a time and suffer from information loss on details caused by convolution and downsampling operators (e.g.
To overcome these limitations, we propose a pure transformer-based object ReID framework named TransReID. Speciﬁcally, we ﬁrst encode an image as a sequence of patches and build a transformer-based strong baseline with a improvements, which achieves competitive few critical results on several ReID benchmarks with CNN-based methods. To further enhance the robust feature learning in the context of two novel modules transformers, are carefully designed. (i) The jigsaw patch module (JPM) is proposed to rearrange the patch embeddings via shift and patch shufﬂe operations which generates robust features with improved discrimination ability and more diversiﬁed coverage. (ii) The side information embeddings (SIE) is introduced to mitigate feature bias towards camera/view variations by plugging in learnable embeddings to incorporate these non-visual clues. To the best of our knowledge, this is the ﬁrst work to adopt a pure transformer for ReID research. Experimental results of TransReID are superior promising, which achieve state-of-the-art performance on both person and vehicle ReID benchmarks. Code is available at https://github. com/heshuting555/TransReID. 1.

Introduction
Object re-identiﬁcation (ReID) aims to associate a particular object across different scenes and camera views, such as in the applications of person ReID and vehicle
ReID. Extracting robust and discriminative features is a crucial component of ReID, and has been dominated by
*This work was done when Shuting He was intern at Alibaba supervised by Hao Luo and Pichao Wang.
†Corresponding author
Figure 1: Grad-CAM [34] visualization of attention maps: (a)
Original images, (b) CNN-based methods, (c) CNN+attention methods, (d) Transformer-based methods which captures global context information and more discriminative parts.
Figure 2: Visualization of output feature maps for 2 hard samples with similar appearances.
Transformer-based methods retain backpack details on output feature maps in contrast to CNN-based methods, as noted in red boxes. For better visualization, input images are scaled to size 1024 × 512.
CNN-based methods for a long time [19, 37, 36, 44, 42, 5, 12, 13, 53, 15].
By reviewing CNN-based methods, we ﬁnd two important issues which are not well addressed in the ﬁeld of object ReID. (1) Exploiting the rich structural patterns in a global scope is crucial for object ReID [54]. However,
CNN-based methods mainly focus on small discriminative regions due to a Gaussian distribution of effective receptive
ﬁelds [29]. Recently, attention modules [54, 6, 3, 21, 1] have been introduced to explore long-range dependencies
[45], but most of them are embedded in the deep layers and do not solve the principle problem of CNN. Thus, attention-based methods still prefer large continuous areas and are hard to extract multiple diversiﬁed discriminative parts (see
Figure 1). (2) Fine-grained features with detail information are also important. However, the downsampling operators (e.g. pooling and strided convolution) of CNN reduce spatial resolution of output feature maps, which greatly affect the discrimination ability to distinguish objects with similar appearances [37, 27]. As shown in Figure 2, the details of the backpack are lost in CNN-based feature maps, making it difﬁcult to differentiate the two people.
(ViT)
Recently, Vision Transformer
[8] and Data-efﬁcient image Transformers (DeiT) [40] have shown that pure transformers can be as effective as CNN-based methods on feature extraction for image recognition.
With the introduction of multi-head attention modules and the removal of convolution and downsampling operators, transformer-based models are suitable to solve the aforementioned problems in CNN-based ReID for the following reasons. (1) The multi-head self-attention captures long range dependencies and drives the model to attend diverse human-body parts than CNN models (e.g. thighs, shoulders, waist in Figure 1). (2) Without downsampling operators, transformer can keep more detailed information. For example, one can observe that the difference on feature maps around backpacks (marked by red boxes in Figure 2) can help the model easily differentiate the two people. These advantages motivate us to introduce pure transformers in the object ReID.
Despite its great advantages as discussed above, transformers still need to be designed speciﬁcally for object ReID to tackle the unique challenges, such as the large variations (e.g. occlusions, diversity of poses, camera perspective) in images. Substantial efforts have been devoted to alleviating this challenge in CNN-based methods. Among them, local part features [37, 44, 20, 48, 28] and side information (such as cameras and viewpoints)
[7, 61, 35, 30], have been proven to be essential and effective to enhance the feature robustness.
Learning part/stripe aggregated features makes it robust against occlusions and misalignments [49]. However, extending the rigid stripe part methods from CNN-based methods to pure transformer-based methods may damage long-range dependencies due to global sequences splitting into several isolated subsequences. In addition, taking side information into consideration, such as camera and viewpoint-speciﬁc information, an invariant feature space can be constructed to diminish bias brought by side information variations.
However, the complex designs for side information built on
CNN, if directly applied to transformers, cannot make full use of the inherent encoding capabilities of transformers.
As a result, speciﬁc designed modules are inevitable and essential for a pure transformer to successfully handle these challenges.
Therefore, we propose a new object ReID framework dubbed TransReID to learn robust feature representations.
Firstly, by making several critical adaptations, we construct a strong baseline framework based on a pure transformer.
Secondly, in order to expand long-range dependencies and enhance feature robustness, we propose a jigsaw patches module (JPM) by rearranging the patch embeddings via shift and shufﬂe operations and re-grouping them for further feature learning. The JPM is employed on the last layer of the model to extract robust features in parallel with the global branch which does not include this special operation. Hence, the network tends to extract perturbation-invariant and robust features with global context. Thirdly, to further enhance the learning of robust features, a side information embedding (SIE) is introduced.
Instead of the special and complex designs in CNN-based methods for utilizing these non-visual clues, we propose a uniﬁed framework that effectively incorporates non-visual clues through learnable embeddings to alleviate the data bias brought by cameras or viewpoints. Taking cameras for example, the proposed SIE helps address the vast pairwise similarity discrepancy between inter-camera and intra-camera matching (see Figure 6). SIE can also be easily extended to include any non-visual clues other than the ones we have demonstrated.
To our best knowledge, we are the ﬁrst to investigate the application of pure transformers in the ﬁeld of object ReID.
The contributions of the paper are summarised:
• We propose a strong baseline that exploits the pure transformer for ReID tasks for the ﬁrst time and achieve comparable performance with CNN-based frameworks. design (JPM), consisting of shift and patch shufﬂe operation, which facilitates perturbation-invariant and robust feature representation of objects. jigsaw patches module
• We a
• We introduce a side information embeddings (SIE) that encodes side information by learnable embeddings, and is shown to effectively mitigate the feature bias.
• The ﬁnal framework TransReID achieves state-of-the-art performance on both person and vehicle
ReID including MSMT17[46],
Market-1501[55], DukeMTMC-reID[33], Occluded-Duke[31], VeRi-776[25] and VehicleID[24]. benchmarks 2.