Abstract
Human motion synthesis is an important problem with applications in graphics, gaming and simulation environ-ments for robotics. Existing methods require accurate mo-tion capture data for training, which is costly to obtain.
Instead, we propose a framework for training generative models of physically plausible human motion directly from monocular RGB videos, which are much more widely avail-able. At the core of our method is a novel optimization formulation that corrects imperfect image-based pose esti-mations by enforcing physics constraints and reasons about contacts in a differentiable way. This optimization yields corrected 3D poses and motions, as well as their corre-sponding contact forces. Results show that our physically-corrected motions signiﬁcantly outperform prior work on pose estimation. We can then use these to train a gener-ative model to synthesize future motion. We demonstrate both qualitatively and quantitatively signiﬁcantly improved motion estimation, synthesis quality and physical plausibil-ity achieved by our method on the large scale Human3.6m dataset [12] as compared to prior kinematic and physics-based methods. By enabling learning of motion synthesis from video, our method paves the way for large-scale, real-istic and diverse motion synthesis. 1.

Introduction
Given videos of human motion, how can we infer the 3D trajectory of the body’s structure and use it to generate new, plausible movements that obey physics constraints? Ad-dressing the intricacies of this question opens up an array of possibilities for high-ﬁdelity character animation and mo-tion synthesis, informed by real-world motion. This would beneﬁt games, pedestrian simulation [55] in testing environ-ments for self-driving cars, realistic long-horizon predic-tions for model-based control and reinforcement learning, as well as physics-based visual tracking.
The vast majority of existing approaches in learning-based human motion synthesis
[1, 59, 26, 21, 5] rely on large-scale motion capture observations, such as AMASS [32], which are typically costly and time-consuming to acquire, logistically challenging, and most often limited to recordings in indoor environments. These factors form a bottleneck that hinders the collection of high-quality human motion data, particularly in settings where there is interaction among multiple people or interaction with a number of stationary and moving objects in the scene. The recorded motions typically also lack realism and diversity as they are acquired by acting out a set of pre-deﬁned motions. In addition to this issue, many time-series models trained on motion capture data make predic-tions that are oblivious to the physics constraints of motion and contact, often leading to inaccurate, jerky, and implau-sible motion.
In this paper we entirely forego reliance on motion cap-ture and aim to train physically plausible human motion synthesis directly from monocular RGB videos. We pro-pose a framework that reﬁnes noisy image-based pose esti-mates by enforcing physics constraints through contact in-variant optimization [37, 36], including computation of con-1
tact forces. We then use the results of the reﬁnement to train a time-series generative model that synthesizes both future motion and contact forces. Our contributions are:
• We introduce a smooth contact loss function to per-form physics-based reﬁnement of pose estimates, es-chewing the need for separately trained contact detec-tors or nonlinear programming solvers.
• We demonstrate that when visual pose estimation is combined with our physics-based optimization, even without access to motion capture datasets, it is sufﬁ-cient to train motion synthesis models that approach the quality of motion capture prediction models.
We validate our method on the Human3.6m dataset [12], and demonstrate both qualitatively and quantitatively the improved motion synthesis quality and physical plausi-bility achieved by our method, compared to prior work on learning-based motion prediction models, such as
PhysCap [44], HMR [16], HMMR [58], and VIBE [18]. 2.