Abstract 1.

Introduction
Nowadays, customer’s demands for E-commerce are more diversiﬁed, which introduces more complications to the product retrieval industry. Previous methods are either subject to single-modal input or perform supervised image-level product retrieval, thus fail to accommodate real-life scenarios where enormous weakly annotated multi-modal data are present. In this paper, we investigate a more real-istic setting that aims to perform weakly-supervised multi-modal instance-level product retrieval among ﬁne-grained product categories. To promote the study of this challeng-ing task, we contribute Product1M, one of the largest multi-modal cosmetic datasets for real-world instance-level re-trieval. Notably, Product1M contains over 1 million image-caption pairs and consists of two sample types, i.e., single-product and multi-product samples, which encompass a wide variety of cosmetics brands. In addition to the great diversity, Product1M enjoys several appealing character-istics including ﬁne-grained categories, complex combina-tions, and fuzzy correspondence that well mimic the real-world scenes. Moreover, we propose a novel model named
Cross-modal contrAstive Product Transformer for instance-level prodUct REtrieval (CAPTURE), that excels in captur-ing the potential synergy between multi-modal inputs via a hybrid-stream transformer in a self-supervised manner.
CAPTURE generates discriminative instance features via masked multi-modal learning as well as cross-modal con-trastive pretraining and it outperforms several SOTA cross-modal baselines. Extensive ablation studies well demon-strate the effectiveness and the generalization capacity of our model. Dataset and codes are available at https:
//github.com/zhanxlin/Product1M .
Figure 1. Our proposed task performs instance-level retrieval among multi-modal data.
The past two decades have witnessed the high enrich-ment of the commodity types and the diversiﬁcation of on-line customer’s demand in E-commerce. On the one hand, online merchandise has increasingly diversiﬁed categories and a large proportion of them are exhibited as a product portfolio where multiple instances of different products ex-ist in one image. On the other hand, online customers or merchants may want to retrieve the single product in a port-folio for price comparison [42] or online commodity rec-ommendation [34]. Furthermore, with the ever-accelerating accumulation of heterogeneous data generated by multi-media, it remains a problem how an algorithm can han-dle large-scale and weakly annotated data [45] to perform multi-modal retrieval.
In this paper, we explore a realistic problem: how to per-form instance-level1 ﬁne-grained product retrieval given the 1Instance-level product retrieval refers to the retrieval of all single prod-† Equal contribution. (cid:63) Corresponding Author. ucts existed in a product portfolio image.
Dataset
#samples
#categories
#instances
#obj/img weak supervision multi-modal instance-level retrieval
RPC checkout [47]
Twitter100k [17]
INRIA-Websearch [22]
Dress Retrieval [7]
Product1M(Ours) 30,000 100,000 71,478 20,200 1,182,083 200
-353
-458 367,935
--∼20,200 92,200 12.26
--∼1.0 2.83 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Table 1. Comparisons between different datasets. ‘-’ indicates inapplicable. The #instances and #obj/img of Product1M are in italics since there are no instance labels for the train set and we only count the instances in the val and test set. Product1M is one of the largest multi-modal datasets as well as the ﬁrst dataset speciﬁcally tailored for real-world instance-level retrieval scenarios. large-scale weakly annotated multi-modal data? We com-pare different paradigms of retrieval in Figure 1. As can be seen, image-level retrieval tends to return trivial results since it does not distinguish different instances, while multi-modal instance-level retrieval is more favorable for search-ing for various kinds of products among multi-modal data.
Despite the generality and the practical value of this prob-lem, it is not well studied due to the lack of real-world datasets and a clear problem deﬁnition.
In the literature of product retrieval, intra-modal [32, 1, 31, 30] and cross-modal retrieval [43, 12, 48, 4, 44, 8] take as input single-modal information, e.g., an image or a piece of text, and per-forms matching search between separate data points. Unfor-tunately, such retrieval schemes signiﬁcantly restrict their use in many scenarios where multi-modal information ex-ists in both the queries and targets. More importantly, pre-vious works focus on the relatively simple case, i.e., image-level 2 retrieval for single-product images [24, 13] and the instance-level nature of retrieval is unexplored.
To bridge this gap and advance the related research, we collect a large-scale dataset, named Product1M, proposed for multi-modal instance-level retrieval. Product1M con-tains over 1 million image-caption pairs and consists of two types of samples, i.e., single-product and multi-product samples. Each single-product sample belongs to a ﬁne-grained category and the inter-category difference is sub-tle. The multi-product samples are of great diversity, re-sulting in complex combinations and fuzzy correspondence that well mimic the real-world scenarios. To the best of our knowledge, Product1M is one of the largest multi-modal datasets as well as the ﬁrst dataset speciﬁcally tailored for real-world multi-modal instance-level retrieval scenarios.
In addition to the constructed dataset, we also propose a novel self-supervised training framework that extracts rep-resentative instance-level features from large-scale weakly annotated data. Speciﬁcally, we ﬁrst train a multi-product detector from pseudo-labels by incorporating a simple yet effective data augmentation scheme. Then, CAPTURE is proposed to capture the potential synergy of images and texts via several pretext tasks. We showcase that some prevailing cross-modal pretraining methods [27, 25, 6, 38] 2Image-level product retrieval refers to recognizing a speciﬁc product instance in a single-product image. might be ﬂawed under the multi-instance setting due to the design defects in the network architecture or the inappro-priate pretext task. In contrast, CAPTURE utilizes a hybrid-stream architecture that encodes data of different modalities separately and fuses them in a uniﬁed way, which is ex-perimentally shown to be beneﬁcial for our proposed task.
Moreover, we introduce the cross-modal contrastive loss to enforce CAPTURE to reach alignment between image and texts, which avoids the mismatch issue incurred by the in-appropriate pretext task.
Crucially, CAPTURE surpasses the SOTA cross-modal baselines in terms of all main metrics by a large margin. We further conduct extensive ablation experiments to demon-strate the generalization capacity of CAPTURE and explore several critical factors of our proposed task. We hope the proposed Product1M, CAPTURE, and solid baselines can help advance future research on real-world retrieval. 2.