Abstract
Effectively tackling the problem of temporal action lo-calization (TAL) necessitates a visual representation that jointly pursues two confounding goals, i.e., fine-grained dis-crimination for temporal localization and sufficient visual invariance for action classification. We address this chal-lenge by enriching both the local and global contexts in the popular two-stage temporal localization framework, where action proposals are first generated followed by action clas-sification and temporal boundary regression. Our proposed model, dubbed ContextLoc, can be divided into three sub-networks: L-Net, G-Net and P-Net. L-Net enriches the lo-cal context via fine-grained modeling of snippet-level fea-tures, which is formulated as a query-and-retrieval process.
G-Net enriches the global context via higher-level model-ing of the video-level representation. In addition, we intro-duce a novel context adaptation module to adapt the global context to different proposals. P-Net further models the context-aware inter-proposal relations. We explore two ex-isting models to be the P-Net in our experiments. The effi-cacy of our proposed method is validated by experimental results on the THUMOS14 (54.3% at tIoU@0.5) and Activ-ityNet v1.3 (56.01% at tIoU@0.5) datasets, which outper-forms recent states of the art. Code is available at https:
//github.com/buxiangzhiren/ContextLoc. 1.

Introduction
Temporal action localization (TAL) is a fundamental task in video understanding.
It aims at classifying action in-stances in an untrimmed video and locating their temporal boundaries. Recently, TAL has drawn increasing attention from the research community because of its wide range of applications such as action retrieval, video summarization, and intelligent security [32, 41, 31, 18].
Prior TAL methods can be divided into two categories.
[14, 47, 2, 30] classify and lo-One-stage approaches
*Corresponding author. (a) Snippet-level local context (b) Video-level global context
Figure 1. (a) Each proposal consists of a number of video snippets.
A snippet is a small number of consecutive frames and serves as the basic unit of feature extraction. It is the snippets capturing the start and end times of the action that play an important role in ac-tion localization. (b) The video-level global context is important as it involves background and high-level activity information that can be critical to distinguish action categories of similar appear-ance and motion patterns. cate action instances from an input video in a single shot.
Two-stage approaches first generate category-agnostic ac-tion proposals
[20, 21, 10, 25, 11, 50, 1] and then per-form action classification and temporal boundary refine-ment [48, 43, 45, 4, 7] for each proposal. They have their respective advantages. One-stage approaches can be easily trained in an end-to-end fashion while two-stage approaches usually obtain superior performance.
Effectively tackling the task of TAL necessitates a vi-sual representation that jointly pursues two confounding goals, i.e., fine-grained discrimination for temporal local-ization and sufficient visual invariance for action classifi-cation. This paper addresses this challenge by leveraging rich local and global contexts in a video, in our proposed two-stage method.
The local context refers to snippets within a proposal.
They contain fine-grained temporal information that is crit-ical to localization. As shown in Figure 1(a), we locate the boundary of the action “cricket bowling” by the moments of bowling and catching the cricket. Therefore, it is the snip-pets capturing these special moments that facilitate localiza-tion in the temporal domain. However, prior approaches ob-tain features of a proposal by applying temporal max pool-ing to features of snippets within it, which unavoidably dis-cards the fine-grained temporal information.
The global context refers to the entire video.
It pro-vides discriminative information complementary to features of a proposal for action classification. As shown in Fig-ure 1(b), to distinguish “long jump” and “high jump”, we need to check not only the last a few frames of the action duration but also background frames outside the duration.
In addition, the global context provides high-level activity information enforcing strong prior on the categories of ac-tions that should appear in it. For example, it is unlikely to see sports actions in a video of household activity. Un-fortunately, the video-level global context has been largely ignored by existing TAL models.
We introduce a novel network architecture, termed Con-textLoc, to model local and global contexts in a unified framework for TAL. It consists of three sub-networks: L-Net, G-Net and P-Net. Inspired by the self-attention [40], L-Net performs a query-and-retrieval procedure. But different with the self-attention, the queries, keys and values in our
L-Net correspond to different semantic entities, and they are specially designed to enrich the local context. Specifically, the feature vector of a proposal is taken as a query to match the key feature vectors of snippets within this proposal so that the relevant fine-grained values in the local context can be retrieved and aggregated to this proposal.
G-Net models the global context by integrating the video-level representation and features of each proposal.
However, a naive concatenation of these two would be in-sufficient because the former contains not only relevant cues but also irrelevant noise. In addition, the portions of con-text needed to enhance different proposals are different. To effectively integrate video-level information with proposal-level features, we propose global context adaptation.
It attends the video-level representation to the local context within each proposal so that the global context is adapted to them respectively.
P-Net models context-aware inter-proposal relations.
This includes interactions between proposal-level features enhanced by the local context and interactions between global contexts adapted to different proposals. We use an existing model as P-Net and investigate two candidates: P-GCN [48] and the non-local network [44].
It is worth noting our ContextLoc is different from P-GCN [48]. P-GCN only considers inter-proposal relations, and the features of a proposal are obtained via applying temporal max pooling to the features of snippets within it.
In contrast, ContextLoc enriches the local context via fine-grained modeling of snippet-level features and enriches the global context via higher-level modeling of the video-level representation. We consider P-GCN or its relative as a use-ful component of our framework, i.e., P-Net, and show that the local and global contexts we advocate are complemen-tary to the inter-proposal relations.
We evaluate our proposed method on two popular bench-marks for TAL. On THUMOS14 [16], it achieves 54.3% mAP at tIoU 0.5, which outperforms the previous best method PBRNet [23]. On ActivityNet v1.3 [13], it achieves 56.01% mAP at tIoU 0.5, which outperforms the state-of-the-art method PBRNet.
The contributions of this paper are summarized below.
• To our knowledge, this is the first work attempting to exploit the snippet-level local context and the video-level global context to enhance proposal-level features within a two-stage TAL framework.
• We introduce a novel network architecture, termed
ContextLoc, consisting of three sub-networks, i.e., L-Net, G-Net, and P-Net. L-Net is the first of its kind to use a proposal to query the snippets within it and retrieve the local context to supplement it with fine-grained temporal information. G-Net augments fea-tures of each proposal by integrating the video-level representation. We introduce a novel context adap-tation process to adapt the global context to different proposals. While P-Net is built on existing networks, we show that P-Net, regardless of its instantiation, is complementary to our L-Net and G-Net. Our Con-textLoc unifies the respective advantages of these three sub-networks and achieves more effective TAL.
• ContextLoc is superior or comparable to the state-of-the-art performance on two popular TAL benchmarks, i.e., THUMOS14 and ActivityNet v1.3. 2.