Abstract
Boundary Representations (B-Reps) are the industry standard in 3D Computer Aided Design/Manufacturing (CAD/CAM) and industrial design due to their ﬁdelity in representing stylistic details. However, they have been ig-nored in the 3D style research. Existing 3D style metrics typically operate on meshes or point clouds, and fail to ac-count for end-user subjectivity by adopting ﬁxed deﬁnitions of style, either through crowd-sourcing for style labels or hand-crafted features. We propose UVStyle-Net, a style sim-ilarity measure for B-Reps that leverages the style signals in the second order statistics of the activations in a pre-trained (unsupervised) 3D encoder, and learns their relative impor-tance to a subjective end-user through few-shot learning.
Our approach differs from all existing data-driven 3D style methods since it may be used in completely unsupervised settings, which is desirable given the lack of publicly avail-able labeled B-Rep datasets. More importantly, the few-shot learning accounts for the inherent subjectivity asso-ciated with style. We show quantitatively that our proposed method with B-Reps is able to capture stronger style signals than alternative methods on meshes and point clouds de-spite its signiﬁcantly greater computational efﬁciency. We also show it is able to generate meaningful style gradients with respect to the input shape, and that few-shot learning with as few as two positive examples selected by an end-user is sufﬁcient to signiﬁcantly improve the style measure.
Finally, we demonstrate its efﬁcacy on a large unlabeled public dataset of CAD models. Source code and data are available at github.com/AutodeskAILab/UVStyle-Net. 1.

Introduction
B-Reps are the de facto standard for industrial design, and the representation most widely used in the consumer product and automotive industries where style is of great importance. B-Reps offer unparalleled editability in a com-they are not dis-pact, memory efﬁcient representation, crete/sampled (as per mesh/point cloud) offering precise boundaries with continuous smooth surfaces/edge curves.
Figure 1: Overview of UV-StyleNet: Grams of activations are normalized and extracted for each layer. The weights applied to each layer deﬁne the meaning of style. (a) Top-10 query results using uniform layer weights w (b) Top-10 query results using w(cid:63) based on the user-selected examples (positive in green, negative in red). In this example, w(cid:63) ≈ [0, 0, 0, 1, 0, 0, 0](cid:62). Zoom to see ﬁllets/stylistic details.
Figure 2: Lower case examples from font ‘Viaoda Libre’. While
‘j’ and ‘r’ share some stylistic features, they are not obviously sim-ilar to ‘c’, ‘s’ or ‘z’, i.e. font classes provide a ground truth for style compatibility (as perceived by their designers) yet only a weak la-bel for style itself.
See Appendix A for a brief introduction to B-Reps. A B-Rep style similarity measure has many use cases, i.e. ﬁnd-ing architectural parts that are in-keeping with the style of a building, or selecting parts for a car that ﬁt with the manu-facturer’s existing range. Moreover, the gradient of a style similarity measure can be used to generate helpful visual-izations or modify the input 3D shape `a la Gatys et al. [11].
Geometric style is inherently subjective and may have a different meaning in different object class domains, i.e. the boundary between style and content is unclear. For exam-ple, in the context of chair designs, number of legs could be considered either style or content depending on the partic-ular use case. Thus, an effective geometric style measure must cater for these different interpretations of the end user.
While existing methods use hand-crafted features [24, 23] or crowd-sourcing [21, 25, 28, 26] to pre-deﬁne and measure geometric style, we propose a user-deﬁned few-shot style metric learning method that leverages the range of style signals available in the activations of a pre-trained 3D object encoder through second order statistics (Gram matri-ces). The relative importance of each layer’s Gram matrix is then learnt through selection of just a few examples of what style means to an end user (see Figure 1).
Despite the abundant use of B-Reps in industrial settings, there is a fundamental lack of publicly available B-Rep data for training machine learning models — in particular, there are no existing B-Rep datasets that include a reliable ground truth for style. To overcome this challenge, we provide an adaptation to SolidLetters [16], which improves the style consistency within font classes for the evaluation test set.
The font classes, however, still provide only a weak label for style (see Figure 2), and as such we propose an unsu-pervised method and use the font labels purely for quan-titative evaluation to justify design choices of our method.
For comparison against existing SOTA on real-world data we also provide evaluation with the unlabeled ABC dataset
[20] of CAD models and a manually labeled subset of it.
The main contributions of this work are as follows:
• We demonstrate that the second order statistics (Gram matrix) approach used in 2D image style literature can be generalised to (B-Rep) 3D shapes
• We introduce a general few-shot learning method for capturing a subjective end-user’s deﬁnition of 3D style and demonstrate its effectiveness on B-Reps
• We show quantitative efﬁciency and performance ad-vantages of using UVStyle-Net architecture with B-Reps over similar approaches on meshes and point clouds using a new synthetic public dataset (SolidLet-ters) and a small subset of ABC labeled for style
• We verify our method on the ABC dataset with no style or content labels for pre-training, and demonstrate the effectiveness of our few-shot learning process to cap-ture subjective user-deﬁned style similarity measures
In summary, we introduce a geometric style similarity measure for 3D solids that may be used in completely unla-beled settings for arbitrary object classes, with user subjec-tivity handled by few-shot learning given only a few exam-ples. While our method is adaptable for all 3D input types, we demonstrate the beneﬁts of our approach with B-Reps both quantitatively and qualitatively. 2.