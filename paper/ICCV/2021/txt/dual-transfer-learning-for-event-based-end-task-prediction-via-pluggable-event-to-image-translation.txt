Abstract
Event cameras are novel sensors that perceive the per-pixel intensity changes and output asynchronous event streams with high dynamic range and less motion blur. It has been shown that events alone can be used for end-task learning, e.g., semantic segmentation, based on encoder-decoder-like networks. However, as events are sparse and mostly reﬂect edge information, it is difﬁcult to recover orig-inal details merely relying on the decoder. Moreover, most methods resort to the pixel-wise loss alone for supervision, which might be insufﬁcient to fully exploit the visual de-tails from sparse events, thus leading to less optimal perfor-mance. In this paper, we propose a simple yet ﬂexible two-stream framework named Dual Transfer Learning (DTL) to effectively enhance the performance on the end-tasks with-out adding extra inference cost. The proposed approach consists of three parts: event to end-task learning (EEL) branch, event to image translation (EIT) branch, and trans-fer learning (TL) module that simultaneously explores the feature-level afﬁnity information and pixel-level knowledge from the EIT branch to improve the EEL branch. This sim-ple yet novel method leads to strong representation learning from events and is evidenced by the signiﬁcant performance boost on the end-tasks such as semantic segmentation and depth estimation. 1.

Introduction
Event cameras have recently received much attention in the computer vision and robotics community for their dis-tinctive advantages, e.g., high dynamic range (HDR) and less motion blur [11]. These sensors perceive the intensity changes at each pixel asynchronously and produce event streams encoding time, pixel location, and polarity (sign) of intensity changes. Although events are sparse and mostly respond to the edges in the scene, it has been shown that it is possible to use events alone for learning the end-tasks, e.g., semantic segmentation [1, 14], optical ﬂow and depth esti-mation [15, 22, 55, 80], via deep neural networks (DNNs).
These methods usually follow the encoder-decoder-like net-Figure 1: (a) There is no direct relation between EIT and EEL in the prior-arts. (b) The proposed DTL framework by using EIT branch as a pluggable unit and transferring both feature-level and prediction-level information to enhance the performance of EEL. work structures, e.g., [1, 14, 22], as shown in Fig. 1(a), and are trained in a fully supervised manner. However, as events mostly reﬂect edge information, unlike the canonical im-ages, it is difﬁcult to recover the original structural details from events merely relying on the decoder (see Fig. 2(b)).
Importantly, learning from sparse events with the pixel-wise loss (e.g., cross-entropy loss) alone for supervision often fails to fully exploit visual details from events, thus leading to less optimal performance.
The other line of research has shown the possibility of generating images from events [24, 37, 48, 54, 58, 61, 66].
The generated images have been successfully applied to the end-tasks, e.g., object recognition [48]; however, there ex-ist two crucial problems. First, using these images as the intermediate representations of events leads to considerable inference latency. Second, there is no direct connection re-garding the optimization process between two tasks.
In-deed, the feature representations learned from image gener-ation contain more structural details (see Fig. 2(c)), which can be a good guide for learning end-tasks. However, these crucial clues have been rarely considered and explored to date. Moreover, some event cameras provide active pixel sensor (APS) frames, which contain very crucial visual in-formation; nonetheless, the potential has been scarcely ex-plored to assist learning end-tasks from events.
Therefore, in this paper, we design a concise yet ﬂexi-ble framework to alleviate the above dilemmas. Motivated by recent attempts for event-to-image translation [42, 48, 58, 61], transfer learning [31, 81], and multi-task learning
[56], we propose a novel Dual Transfer Learning (DTL) paradigm to efﬁciently enhance the performance of the end-task learning (see Fig. 3). Such a learning method is uniﬁed in a two-stream framework, which consists of three compo-nents: Event to End-task Learning (EEL) branch, Event to
Image Translation (EIT) branch and Transfer Learning (TL) module, as shown in Fig. 1(b). Speciﬁcally, we integrate the idea of image translation to the process of end-task learning, thus formulating the EIT branch. The EEL branch is then signiﬁcantly enhanced by the TL module, which exploits to transfer the feature-level and prediction-level information from EIT branch. In particular, a novel afﬁnity graph trans-fer loss is proposed to maximize the feature-level instance similarity along the spatial locations between EEL and EIT branches. The prediction-level information is transferred from the EIT branch to the EEL branch using the APS and translated images based on a teacher network trained using the canonical images on the end-task. Moreover, we pro-pose to share the same feature encoder between EEL and
EIT branches and optimize them in an end-to-end manner.
We minimize the prediction gap between the APS and trans-lated images based on the teacher network and subtly lever-age the supervision signal of the EEL branch to enforce se-mantic consistency for the EIT branch, which surprisingly helps to recover more semantic details for image transla-tion. Once training is done, the EIT branch and TL module can be freely removed, adding no extra inference cost.
We conduct extensive experiments on two end-tasks, semantic segmentation (Sec. 4.1) and depth estimation (Sec. 4.2). The results show that this simple yet novel method brings signiﬁcant performance gains for both tasks.
As a potential, our method can also learn the end-tasks via the teacher network without using ground truth labels. Al-though the EIT branch is regarded as an auxiliary task, the results demonstrate that our DTL framework contributes to recover better semantic details for image translation.
In summary, our contributions are three folds. (I) We propose a novel yet simple DTL framework for the end-task learning. (II) We propose a TL module where we trans-fer both feature-level and prediction-level information to the end-tasks. (III) We conduct extensive experiments on two typical end-tasks, showing that DTL signiﬁcantly improves the performances while adding no extra inference cost. We also demonstrate that DTL recovers better semantic de-tails for the EIT branch. Our project code is available at https://github.com/addisonwang2013/DTL. 2.