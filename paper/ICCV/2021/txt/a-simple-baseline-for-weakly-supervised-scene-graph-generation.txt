Abstract
We investigate the weakly-supervised scene graph gener-ation, which is a challenging task since no correspondence of label and object is provided. The previous work regards such correspondence as a latent variable which is itera-tively updated via nested optimization of the scene graph generation objective. However, we further reduce the com-plexity by decoupling it into an efficient first-order graph matching module optimized via contrastive learning to ob-tain such correspondence, which is used to train a standard scene graph generation model. The extensive experiments show that such a simple pipeline can significantly surpass the previous state-of-the-art by more than 30% on the Visual
Genome dataset, both in terms of graph matching accuracy and scene graph quality. We believe this work serves as a strong baseline for future research. Code is available at https://github.com/jshi31/WS-SGG. 1.

Introduction
Given an image, scene graph generation (SGG) is to generate a scene graph [17, 44], consisting of the de-tected objects and the possible relationships between the objects. Such abstraction mimics the structured representa-tions of language [33, 42], facilitating various downstream visual reasoning tasks, e.g. VQA [38, 12, 13], image cap-tion [46, 59], image generation [15]. However, most of the current SGG models are supervised trained with scene graph annotations, which suffered from two limitations.
First, they rely on the expensive annotations of object lo-cations and relations. Second, they are hard to generalize to out-of-domain objects or relations that are required by downstream tasks, e.g., the VQA dataset, where the ques-tions query novel objects and relations that are out of the scene graph dataset domain.
To overcome the above limitations, we investigate the weakly-supervised scene graph generation (WS-SGG) problem, demonstrated in Fig. 1. We relax the annota-tion requirements of SGG only to consider the ungrounded scene graph, composed of solely image-level object and re-lation labels without knowing the exact object locations,
Figure 1. Demonstration the task of WS-SGG. During training, only the pair of image and the ungrounded scene graph are pro-vided. For test stage, given an image, the model should output the full scene graph. i.e., bounding boxes. Such a weakly-supervised learning setting effectively assuages the difficulties associated with data annotation. For the generalization issue, one can obtain the ungrounded scene graph label from the caption of the image through a language parser [42], and the ungrounded scene graph can be paired with the image that is described by the caption. As the community has collected extremely large image caption datasets [34], it provides WS-SGG task with a considerable amount of training data and alleviates the generalization problem. Hence, WS-SGG is of great significance.
We are not the first one to deal with this task. Typically one can deem the region proposals extracted from an image as the nodes of a visual graph, and the ungrounded scene graph as the label graph. The node alignment between these two graphs has to be discerned during the optimization pro-cess of SGG. Therefore, one major challenge for WS-SGG is graph matching, as we should consider the similarity of both the nodes and their relationships. One previous work
VSPNet [52] converts the standard scene graph, where ob-jects are nodes and relations are edges, to a bipartite graph with one part object nodes and another part relations nodes, where the role (subject, object) becomes the edge. It iter-atively match each part of nodes, which fails to apply to
standard scene graph structure, which has already been ex-tensively researched [21, 53, 14, 45, 11, 29, 56, 3, 22, 37, 41, 50, 36] Therefore directly addressing WS-SGG on the standard scene graph structure can easily leverage the pre-vious research outcomes.
Therefore, we tackle the WS-SGG task on the standard scene graph structure and propose a simple baseline that de-couples the problem into two parts: a weakly-supervised graph matching (WSGM) module that learns to align the vi-sual and label graph; and a standard supervised SGG model to generate the scene graph. We select the efficient first-order graph matching (FOGM) algorithm (only match by node similarity) and employ the Multi-Instance Learning (MIL) mechanism with the contrastive learning objective to train the graph matching module, and use the matched scene graph as pseudo ground truth to train the standard
SGG model. The decoupling allows the baseline to adapt to any standard SGG model, resulting in a versatile model.
To our surprise, we find the simple baseline can already achieve much higher performance than VSPNet, the current state-of-the-art, in terms of both graph matching and SGG.
Furthermore, we try to answer the following core ques-tions related to our model. (1) Is the graph matching idea better than a grounding model [16]? (2) What is the good practice of selecting the negative sample and loss to enable such contrastive learning for WSGM? (3) Is higher-order graph matching (considering relation similarity) better?
In a word, our contribution is that we propose a very sim-ple baseline that versatilely works for standard scene graphs but can significantly outperform the complex state-of-the-art method.
The rest of this paper is organized as follows. We review the related works about SGG, WS-SGG, and graph match-ing in Sec. 2. We formulate the problem and introduce our model pipeline in Sec. 3. Experimental settings and result analysis are presented in Sec. 4. Finally, we conclude the paper in Sec. 5. 2.