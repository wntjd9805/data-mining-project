Abstract
Many existing approaches for unsupervised domain adap-tation (UDA) focus on adapting under only data distribu-tion shift and offer limited success under additional cross-domain label distribution shift. Recent work based on self-training using target pseudolabels has shown promise, but on challenging shifts pseudolabels may be highly unreli-able and using them for self-training may lead to error accumulation and domain misalignment. We propose Se-lective Entropy Optimization via Committee Consistency (SENTRY), a UDA algorithm that judges the reliability of a target instance based on its predictive consistency un-der a committee of random image transformations. Our algorithm then selectively minimizes predictive entropy to increase confidence on highly consistent target instances, while maximizing predictive entropy to reduce confidence on highly inconsistent ones. In combination with pseudolabel-based approximate target class balancing, our approach leads to significant improvements over the state-of-the-art on 27/31 domain shifts from standard UDA benchmarks as well as benchmarks designed to stress-test adaptation under label distribution shift. Our code is available at https://github.com/virajprabhu/SENTRY. 1.

Introduction
Unsupervised domain adaptation (UDA) learns to transfer a predictive model from a labeled source domain to an unla-beled target domain. The particular instantiation of learning under covariate shift has been extensively studied within the computer vision community [13, 18, 25, 34, 44, 45].
However, many modern UDA methods, such as distribu-tion matching based techniques, implicitly assume that the task label distribution does not change across domains, i.e (y). When such an assumption is violated,
P distribution matching is not expected to succeed [22, 49]. (y) = P
S
T
In many real-world adaptation scenarios, one may en-counter data distribution (i.e. covariate) shift across domains together with label distribution shift (LDS). For instance, a source dataset can be curated to have a balanced label distri-bution while a naturally arising target dataset may follow a
Figure 1: Top: Conventional entropy-minimization based ap-proaches for unsupervised domain adaptation (UDA) operate by increasing model confidence on unlabeled target instances. Under strong distribution shifts, some instances may initially be mis-aligned and entropy minimization can lead to error accumulation.
Bottom: We propose Selective Entropy Optimization via Com-mittee Consistency (SENTRY), a UDA algorithm that i) identifies reliable target instances based on their predictive consistency under a set of random image transformations, and ii) selectively optimizes model entropy on these instances to induce domain alignment. power law label distribution, as some categories naturally oc-cur more often than others (e.g. DomainNet [31], LVIS [16], and MSCOCO [23]). In order to make domain adaptation broadly applicable, it is critical to develop UDA algorithms that can operate under joint data and label distribution shift.
Recent works have attempted to address the problem of joint data and label distribution shift [22, 43], but these ap-proaches can be unstable as they rely on self-training using often noisy pseudo-labels or conditional entropy minimiza-tion [22] over potentially miscalibrated predictions [15, 39].
Thus, when learning with unconstrained self-training, early mistakes can result in error accumulation [6] and significant domain misalignment (see Figure 1, top).
To address the problem of error accumulation arising from unconstrained self-training, we propose Selective En-tropy Optimization via Committee Consistency (SENTRY), a novel selective self-training algorithm for UDA. First, rather than using model confidence which can be miscalibrated un-der a domain shift [39], we identify reliable target instances for self-training based on their predictive consistency under a committee of random, label-preserving image transforma-tions. Such consistency checks have been found to be a reliable way to detect model errors [2]. Having identified reliable and unreliable target instances, we then perform selective entropy optimization: we consider a highly con-sistent target instance as likely correctly aligned, and in-crease model confidence by minimizing predictive entropy for such an instance. Similarly, we consider an instance with high predictive inconsistency over transformations as likely misaligned, and reduce model confidence by maximizing predictive entropy. See Figure 1 (bottom).
Contributions. We propose SENTRY, an algorithm for un-supervised adaptation under simultaneous data and label distribution shift. We make the following contributions: 1. A novel selection criterion that identifies reliable target instances for self-training based on predictive consis-tency over a committee of random, label-preserving image transformations. 2. A selective entropy optimization objective that min-imizes predictive entropy (increasing confidence) on highly consistent target instances, and maximizes it (reducing confidence) on highly inconsistent ones. 3. We propose using class-balanced sampling on the source (using labels) and target (using pseudolabels), and find it to complement adaptation under LDS. 4. SENTRY sets a new state-of-the-art on 27/31 domain shifts belonging to both standard and LDS versions of several DA benchmarks for classification, including
DomainNet [31], OfficeHome [46], and VisDA [32]. 2.