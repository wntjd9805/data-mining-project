Abstract
We propose a new approach to human clothing model-ing based on point clouds. Within this approach, we learn a deep model that can predict point clouds of various out-fits, for various human poses, and for various human body shapes. Notably, outfits of various types and topologies can be handled by the same model. Using the learned model, we can infer the geometry of new outfits from as little as a single image, and perform outfit retargeting to new bodies in new poses. We complement our geometric model with appearance modeling that uses the point cloud geometry as a geometric scaffolding and employs neural point-based graphics to capture outfit appearance from videos and to re-render the captured outfits. We validate both geometric modeling and appearance modeling aspects of the proposed approach against recently proposed methods and establish the viability of point-based clothing modeling. 1.

Introduction
Modeling realistic clothing is a big part of the overarch-ing task of realistic modeling of humans in 3D. Its immedi-ate practical applications include virtual clothing try-on as well as enhancing the realism of human avatars for telep-resence systems. Modeling clothing is difficult since out-fits have wide variations in geometry (including topological changes) and in appearance (including wide variability of textile patterns, prints, as well as complex cloth reflectance).
Modeling interaction between clothing outfits and human bodies is an especially daunting task.
In this work, we propose a new approach to modeling clothing (Figure 1) based on point clouds. Using a recently introduced synthetic dataset [7] of simulated clothing, we learn a joint geometric model of diverse human clothing outfits. The model describes a particular outfit with a la-tent code vector (the outfit code). For a given outfit code and a given human body geometry (for which we use the most popular SMPL format [34]), a deep neural network (the draping network) then predicts the point cloud that ap-proximates the outfit geometry draped over the body.
The key advantage of our model is its ability to reproduce diverse outfits with varying topology using a single latent space of outfit codes and a single draping network. This is made possible because of the choice of the point cloud representation and the use of topology-independent, point cloud-specific losses during the learning of the joint model.
After learning, the model is capable of generalizing to new outfits, capturing their geometry from data, and to drape the acquired outfits over bodies of varying shapes and in new poses. With our model, acquiring the outfit geometry can be done from as little as a single image.
We extend our approach beyond geometry acquisition to include the appearance modeling. Here, we use the ideas of differentiable rendering [36, 51, 31] and neural point-based graphics [2, 40, 60]. Given a video sequence of an outfit worn by a person, we capture the photometric properties of the outfit using neural descriptors attached to points in the point cloud, and the parameters of a rendering (decoder) network. The fitting of the neural point descriptors and the rendering network (which capture the photometric proper-ties) is performed jointly with the estimation of the outfit code (which captures the outfit geometry) within the same optimization process. After fitting, the outfit can be trans-ferred and re-rendered in a realistic way over new bodies and in new poses.
In the experiments, we evaluate the ability of our ge-ometric model to capture the deformable geometry of new outfits using point clouds. We further test the ca-pability of our full approach to capture both outfit ge-ometry and appearance from videos and to re-render the learned outfits to new targets.
The experimental comparisons show the viability of the point-based ap-proach to clothing modeling. We will publish our code and model at https://saic-violet.github.io/ point-based-clothing/. 2.