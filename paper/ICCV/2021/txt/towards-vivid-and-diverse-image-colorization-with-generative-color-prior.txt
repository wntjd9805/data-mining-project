Abstract
Colorization has attracted increasing interest in recent years. Classic reference-based methods usually rely on external color images for plausible results. A large im-age database or online search engine is inevitably required for retrieving such exemplars. Recent deep-learning-based methods could automatically colorize images at a low cost.
However, unsatisfactory artifacts and incoherent colors are always accompanied. In this work, we aim at recovering vivid colors by leveraging the rich and diverse color priors encapsulated in a pretrained Generative Adversarial Net-works (GAN). Specifically, we first “retrieve” matched fea-tures (similar to exemplars) via a GAN encoder and then in-corporate these features into the colorization process with feature modulations. Thanks to the powerful generative color prior and delicate designs, our method could pro-duce vivid colors with a single forward pass. Moreover, it is highly convenient to obtain diverse results by modify-ing GAN latent codes. Our method also inherits the merit of interpretable controls of GANs and could attain control-lable and smooth transitions by walking through GAN latent space. Extensive experiments and user studies demonstrate that our method achieves superior performance than previ-ous works. 1.

Introduction
Colorization, the task of restoring colors from black-and-white photos, has wide applications in various fields, such as photography technologies, advertising or film in-dustry [21, 48]. As colorization requires estimating missing color channels from only one grayscale value, it is inher-ently an ill-posed problem. Moreover, the plausible solu-tions of colorization are not unique (e.g., cars in black, blue, or red are all feasible results). Due to the uncertainty and di-versity nature of colorization, it remains a challenging task.
Classic reference-based methods (e.g. [17, 35, 53]) re-quire additional example color images as guidance. These methods attempt to match the relevant contents between ex-emplars and input gray images, and then transfer the color statistics from the reference to the gray one. The quality of generated colors (e.g., realness and vividness) is strongly dependent on the reference images. However, retrieving de-sirable reference images requires significant user efforts. A recommendation system for this can be a solution, but it is still challenging to design such a retrieval process. A large-scale color image database or online search engine is in-evitably required in the system.
Recently, convolutional neural network (CNN) based colorization methods [7, 11, 23] have been proposed to fa-cilitate the colorization task in an automatic fashion. These methods learn to discover the semantics [31, 58] and then directly predict the colorization results. Though remarkable achievements in visual qualities are achieved, unsatisfactory artifacts and incoherent colors are always accompanied.
In this work, we attempt to exploit the merits of both reference-based and CNN-based methods, i.e., achieving realistic and vivid colorization results on par with reference-based methods, while keeping them automatic at a low cost.
Inspired by recent success in Generative Adversarial Net-work (GAN) [3, 27], our core idea is to leverage the most relevant image in the learned GAN distribution as the exem-plar image. The rich and diverse color information encapsu-lated in pretrained GAN models, i.e., generative color prior, allows us to circumvent the explicit example retrieval step and integrate it into the colorization pipeline as a unified framework. Specifically, we first ‘retrieve’ matched fea-tures (similar to exemplars) via a GAN encoder and then incorporate these features into the colorization process with feature modulations.
In addition, our method is capable of achieving diverse colorization from different samples in the GAN distribution or by modifying GAN latent codes.
Thanks to the interpretable controls of GANs, our method could also attain controllable and smooth transitions by walking through GAN latent space.
We summarize the contributions as follows. (1) We develop a unified framework to leverage rich and diverse generative color prior for automatic colorization. (2) Our method allows us to obtain diverse outputs. Controllable and smooth transitions can be achieved by manipulating the code in GAN latent space. (3) Experiments show that our method is capable of generating more vivid and diverse col-orization results than previous works. 2.