Abstract
Although recent research has witnessed a significant progress on the video deblurring task, these methods strug-gle to reconcile inference efficiency and visual quality si-multaneously, especially on ultra-high-definition (UHD) videos (e.g., 4K resolution). To address the problem, we propose a novel deep model for fast and accurate UHD
Video Deblurring (UHDVD). The proposed UHDVD is achieved by a separable-patch architecture, which collab-orates with a multi-scale integration scheme to achieve a large receptive field without adding the number of generic convolutional layers and kernels. Additionally, we design a residual channel-spatial attention (RCSA) module to im-prove accuracy and reduce the depth of the network appro-priately. The proposed UHDVD is the first real-time de-blurring model for 4K videos at 35 fps. To train the pro-posed model, we build a new dataset comprised of 4K blurry videos and corresponding sharp frames using three differ-ent smartphones. Comprehensive experimental results show that our network performs favorably against the state-of-the-art methods on both the 4K dataset and public bench-marks in terms of accuracy, speed, and model size. 1.

Introduction
Ultra-High-Definition (UHD, i.e., 12 megapixels or 4K) becomes a trend during the last several years. Many device manufacturers have released new devices (e.g., smartphones and DSLR cameras) with 4K support. Unfortunately, irreg-ular camera shakes and high-speed movements often gener-ate undesirable blurs in captured UHD videos. The blurred video leads to visually low quality and hampers high-level vision tasks [27].
Numerous image and video deblurring methods have been proposed to recover the sharp frames from a cap-tured blurry video. Conventional methods usually make as-sumptions on motion blurs and latent frames. Among these methods, motion blurs are usually modeled as uniform ker-Figure 1. PSNR(dB) vs. runtime(ms) of several deblurring meth-ods and our method on different datasets. The green region indi-cates real-time inference at 30 fps. The blue PSNR and icons are methods on the HD dataset and the red ones with the same shape are on the 4K dataset. Clearly, our method is better not only in efficiency but also in accuracy. Furthermore, we try to handle the 4K resolution and achieve considerable results. nels [35, 57, 54] or non-uniform kernels (e.g., region-wise
[11, 1, 9, 45, 4] and pixel-wise [10, 33]). While the sharp frames are usually constrained by hand-crafted image priors
[19, 39, 24, 8] to regularize the solution space. However, these assumptions do not usually hold for real cases, which leads to an inaccurate estimation of the blur kernel and the quality of the deblurred image is not desirable.
To address these issues, deep learning deblurring algo-rithms have been proposed recently. These methods use convolutional neural networks (CNNs) to explicitly learn features from blurry input and regress the blur kernel [34, 7, 38] or directly recover the clean image [26, 52, 53, 56, 50].
These algorithms can remove blur effects caused by cam-era shakes and object motions, and achieve state-of-the-art results on image deblurring task. However, existing CNN-based methods have two major problems. The first one is that the computation and memory usage are too large for practical applications, especially when the resolution of in-put images is high. For example, the recent video deblurring method of CDVD-TSP [30] needs about four seconds and one minute to deblur a single frame from HD (720p) and
UHD (4K) videos, respectively. The second one is that most existing CNN-based video deblurring methods lack flexibil-ity in dealing with different types of information due to less discrimination ability learning between blur and sharp pairs.
Therefore, generating detailed textures from blurred videos is still a non-trivial problem.
To overcome the above limitations, we present a new
UHDVD network which has advantages of high efficiency, low memory overhead, and high quality deblurring perfor-mance. Our method is partially motivated by the patch-hierarchical image deblurring methods [50, 37] where multi-patch hierarchy is fed into the network. This scheme achieves great improvements on 720p image deblurring at very high efficiency. However, the multi-patch hierarchy
[50, 37] has the same spatial resolution at different levels and require slow algorithms to layout the patches and stitch them together, which hinders the reconstruction ability of the deep network and reduces the feature extraction speed.
We note that a low-resolution image is easier to recover than high-resolution since there are less class information and fewer modes (i.e., edges and textures) [13]. Therefore, we propose a novel separable-patch architecture combined with a multi-scale integration scheme, which allows to capture the global structure on the coarse scale and process multi-ple patches of each scale in parallel within an iteration.
In addition, most existing deblurring algorithms employ a cascaded network to help the latent frame restoration
[30, 50]. However, to the best of our knowledge, simply stacking the same network to construct deeper networks can hardly obtain better improvements [37]. To achieve more expressive and intelligent video deblurring capability, we further propose a cascaded residual channel and spatial at-tention (RCSA) module to improve deblurring performance without sacrificing speed. The proposed RCSA is able to adaptively learn more useful channel-wise features and em-phasize the most informative region on the feature map for video deblurring.
The main contributions of this paper are summarized as:
• We propose a novel UHDVD network by using a separable-patch architecture combined with a multi-scale integration scheme. To the best of our knowl-edge, our proposed model is the first video deblurring model that can deblur 4K videos in real-time by paral-lelizing multiple patches.
• We design a cascaded RCSA module to improve fea-ture representation power and discriminative ability, ensuring high deblurring performance.
• We establish a 4K video deblurring dataset including both synthesized and real captured videos. We evaluate the proposed model on the proposed benchmark and public datasets [25, 26, 36] and show that the proposed method performs favorably against state-of-the-arts. 2.