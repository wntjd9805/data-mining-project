Abstract
We propose a new approach for high resolution seman-tic image synthesis. It consists of one base image generator and multiple class-specific generators. The base genera-tor generates high quality images based on a segmentation map. To further improve the quality of different objects, we create a bank of Generative Adversarial Networks (GANs) by separately training class-specific models. This has sev-eral benefits including – dedicated weights for each class; centrally aligned data for each model; additional training data from other sources, potential of higher resolution and quality; and easy manipulation of a specific object in the scene. Experiments show that our approach can generate high quality images in high resolution while having flexibil-ity of object-level control by using class-specific generators. 1.

Introduction
In the unconditional setting, an 33, 17, 3, 24] settings. image is generated by randomly sampling a latent code.
With the advent of StyleGAN2 [32], for some classes such as faces, the generated images are almost indistinguish-In the conditional setting, an im-able from real images. age is generated based on some input conditioning signal such as another image, a segmentation map, or other pri-ors. Most notably, SPADE [24] (i.e., spatially-adaptive nor-malization) significantly boosted image quality for seman-tic image generation. This normalization and its variants have been used as standard building blocks in many follow-ing works [35, 25, 31, 30]. However, the image quality of conditional GANs is still inferior to that of unconditional
GANs, especially StyleGAN2. Also, most approaches out-put 256 × 256 resolution images, which is not high enough for many real world applications, and their generation qual-ity is inferior for higher resolutions.
Image generation has been explored extensively in both the unconditional [8, 18, 19, 32] and conditional [15, 37,
In this work, we target high resolution image generation conditioned on input segmentation maps (yellow insets in 1
Fig. 1, first column). Contrary to most prior work which typically follow the SPADE architecture design, we explore a new direction. First, we develop a conditional version of StyleGAN2. Our generator backbone builds upon the original StyleGAN2 architecture, which can generate better quality images and is more easily scaled to high resolutions.
Specifically, we use an encoder to extract hierarchical fea-ture representations from the input segmentation map and use it to modulate the StyleGAN2 generator. The resulting generator is less memory intensive and faster to train com-pared to SPADE based approaches.
Second, instead of using a single generator, we use multi-ple class-specific generators to improve the quality of small foreground objects. There are several advantages of using different models for different classes: (i) Class-specific gen-erators with dedicated weights learned for each foreground class have the capability to generate more details. Imagine a bedroom filled with objects of different scales such as bed, lamp, table, chair, chest and others (Fig. 1, first row). A single GAN might allocate most of its capacity toward gen-erating larger content such as floor, walls and bed since they contribute most to the overall realism, and therefore neglect the details of smaller objects such as lamps and chests. Us-ing separate generators for smaller objects result in more textural details and better shape integrity in those regions (Fig. 1, columns 2-3). (ii) Class-specific generators can be trained with image crops that always align the objects to the center. As shown in [19, 32], spatial alignment is often cru-cial for high quality generation. On the other hand, a single
GAN generating an entire scene at once needs to deal with objects appearing in arbitrary locations which is more diffi-cult to learn. (iii) Class-specific generators can benefit from more training data, which is especially important for more rare classes. Other than the scene training set from which all foreground objects are extracted, one can leverage separate class-specific datasets for training each foreground genera-tor. (iv) Class-specific generators enable more applications.
For example, we can generate out-of-distribution scene im-ages such as a car on a sidewalk (Fig. 8), and we can also easily change the appearance of one object instance in the scene without modifying the rest of it (Fig. 7).
However, na¨ıvely training separate generators and com-bining their results does not generate satisfactory results.
It can lead to appearance inconsistency among generated objects in the same scene; e.g., a generated foreground ob-ject might not have compatible colors and perspective with its background or in the context of other objects. Training all models together based on shared global information will address the inconsistency issue, but it is extremely compu-tationally demanding and cannot scale. Instead, we provide the result of our single GAN base model as input to each class-specific generator since it captures the global knowl-edge of the scene and constrains the foreground object ap-pearance to be harmonious with each other. Although our focus is on generating complex scenes, our approach can also be used to generate a single object class with multi-ple complex parts like humans (Fig. 1, second row). We demonstrate this by training separate class-specific genera-tors for faces and shoes.
Contributions. (1) A general framework for high quality conditional generation of complex scenes by training mul-tiple class-specific generators; (2) A powerful StyleGAN2-based conditional base generator; (3) We demonstrate state-of-the-art image quality synthesized by our model against existing methods and show some potential applications. 2.