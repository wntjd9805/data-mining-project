Abstract
The core of deep metric learning (DML) involves learning visual similarities in high-dimensional embedding space. One of the main challenges is to generalize from seen classes of training data to unseen classes of test data. Re-cent works have focused on exploiting past embeddings to increase the number of instances for the seen classes. Such methods achieve performance improvement via augmenta-tion, while the strong focus on seen classes still remains.
This can be undesirable for DML, where training and test data exhibit entirely different classes.
In this work, we present a novel training strategy for DML called MemVir.
Unlike previous works, MemVir memorizes both embedding features and class weights to utilize them as additional vir-tual classes. The exploitation of virtual classes not only utilizes augmented information for training but also allevi-ates a strong focus on seen classes for better generaliza-tion. Moreover, we embed the idea of curriculum learning by slowly adding virtual classes for a gradual increase in learning difﬁculty, which improves the learning stability as well as the ﬁnal performance. MemVir can be easily ap-plied to many existing loss functions without any modiﬁca-tion. Extensive experimental results on famous benchmarks demonstrate the superiority of MemVir over state-of-the-art competitors. Code of MemVir is publicly available1. 1.

Introduction
Deep metric learning (DML) is of great importance for learning visual similarities in a wide range of vision tasks, such as image clustering [17], unsupervised learn-ing [4, 15, 5], and image retrieval [43, 10, 24, 12]. Learning visual similarity aims to build a well-generalized embed-ding space that reﬂects visual similarities of images using a deﬁned distance metric. Typically, training and test data exhibit entirely different classes in DML. Thus, the main challenge is to maximize generalization performance from a training distribution to a shifted test distribution, which differs from classic classiﬁcation tasks that deal with i.i.d.
*Authors contributed equally. 1https://github.com/navervision/MemVir
Figure 1. In conventional training, the loss function is com-puted with actual classes. On the other hand, in MemVir, classes from previous steps (virtual classes) are used to compute the loss function along with the actual classes.
Moreover, the number of classes and embeddings are gradu-ally increased by adding virtual classes, where C and B de-note number of classes and batch size, N and M are hyper-parameters for MemVir. training and test distributions [31, 37].
Current DML approaches focus on learning visual sim-ilarities with objective functions, which considers pair-wise similarity (pair-based losses) [6, 38, 46] or similar-ity between samples and class representatives (proxy-based losses) [41, 40, 28, 7, 42]. Recent studies propose exploit-ing additional embeddings from past training steps, which are saved and controlled in the memory queue, to increase the number of samples in a mini-batch and that of hard neg-ative pairs [15, 5, 45, 22]. And yet, these methods of utiliz-ing past embeddings is still constrained to the seen classes of the training data. Thus, the trained model might result to over-ﬁt to the seen classes while under-perform on the un-seen classes in test data. Therefore, to learn an embedding space that generalizes, we need to alleviate the strong focus on seen classes during the training phase [37, 31, 30].
In this paper, we propose a novel training strategy, which trains a model with Memory-based Virtual classes (MemVir), for DML. In MemVir, we maintain memory queues for both class weights and embedding features. In-stead of using them to increase the number of instances of seen classes, they are treated as virtual classes to com-pute the loss function along with the actual classes, as il-lustrated in Figure 1. Moreover, we incorporate the idea of curriculum learning (CL) to gradually increase the learn-ing difﬁculty by slowly adding virtual classes. The pro-posed MemVir has the following advantages: 1) MemVir trains a model with augmented information, which includes increased number of classes (C → (N + 1)C) and in-stances (B → (N + 1)B) without additional feature ex-traction. 2) CL-like gradually increasing the learning dif-ﬁculty improves the optimization stability and ﬁnal perfor-mance. 3) Exploiting virtual classes help achieve more gen-eralized embedding space by alleviating excessively strong focus on seen classes of training data. 4) MemVir can be easily applied to many existing loss functions to obtain a signiﬁcant performance boost without any modiﬁcation of the loss function.
Contributions. 1) We propose a novel training strategy for DML that exploits past embeddings and class weights as virtual classes to improve generalization. We further im-prove the training process and performance by incorporat-ing the idea of CL. 2) We exhaustively analyze our proposed method and demonstrate that employing virtual classes im-proves generalization by alleviating a strong focus on seen classes theoretically and empirically. 3) MemVir achieves state-of-the-art performance on three popular benchmarks of DML in both conventional and Metric Learning Reality
Check (MLRC) [33] evaluation protocol. 2.