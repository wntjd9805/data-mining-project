Abstract
Cross-modality person re-identiﬁcation is a challeng-ing task due to large cross-modality discrepancy and intra-modality variations. Currently, most existing methods fo-cus on learning modality-speciﬁc or modality-shareable features by using the identity supervision or modality la-bel. Different from existing methods, this paper presents a novel Modality Confusion Learning Network (MCLNet).
Its basic idea is to confuse two modalities, ensuring that the optimization is explicitly concentrated on the modality-irrelevant perspective. Speciﬁcally, MCLNet is designed to learn modality-invariant features by simultaneously mini-mizing inter-modality discrepancy while maximizing cross-modality similarity among instances in a single framework.
Furthermore, an identity-aware marginal center aggrega-tion strategy is introduced to extract the centralization fea-tures, while keeping diversity with a marginal constraint.
Finally, we design a camera-aware learning scheme to en-rich the discriminability. Extensive experiments on SYSU-MM01 and RegDB datasets show that MCLNet outperforms the state-of-the-art by a large margin. On the large-scale
SYSU-MM01 dataset, our model can achieve 65.40 % and 61.98 % in terms of Rank-1 accuracy and mAP value. 1.

Introduction
Person re-identiﬁcation (ReID) is a technique that uses computer vision technology to determine whether there is a speciﬁc person from a gallery set captured by surveil-lance cameras [17].
It has gained increasing attention in computer vision area for both research and application.
However, there are relatively few works paying attention to the ReID between visible images and infrared images.
*Corresponding Author: Mang Ye
Figure 1. Idea illustration. (a) is the modality confusion learning process. After that, the two modalities are difﬁcult to be correctly classiﬁed, narrowing the modality discrepancy. (b) and (c) are the designed identity-aware and camera-aware marginal constrained center aggregation for person ID and camera ID prediction.
This cross-modality visible-infrared person re-identiﬁcation (cm-ReID) [40] problem is also an important issue in night-time surveillance application. Compared to the widely stud-ied single-modality ReID [5, 52], the cm-ReID is much more challenging due to large visual differences between the two modalities and different camera environments.
To narrow the gap between two modalities, existing methods mainly focus on learning shareable common fea-ture representations, via either one [37, 40] or two-stream networks [45, 9]. [15] designs a spectrum dispelling branch to eliminate the inﬂuence of the spectrum. Besides, some methods generate a common intermediate modality [16] to eliminate the inﬂuences caused by modality discrepancy. A similar approach adopts GAN technique [10] to generate cross-modality images for person matching. However, gen-erating common modality or generating cross-modality im-ages is unavoidably accompanied by noises. Worse still, the generated images greatly increase the computational burden and add more uncertainty for the cross-modality learning, limiting the applicability for practical model deployment.
To address the above limitations, we propose a novel end-to-end Modality Confusion Learning network (MCLNet), which aims to learn modality-invariant features.
Our network neither needs the prior information of input nor generates additional subspace features, which ensures the maximization of input information without additional noise. The basic idea is to confuse the modality discrimi-nation in the feature learning process, making the optimiza-tion explicitly focusing on modality-irrelevant perspective (Fig. 1 (a)). MCLNet minimizes inter-modality discrep-ancy while maximizing cross-modality similarity among in-stances through the min-max game [27]. Incorporated with a partially shared two stream network, our MCLNet can si-multaneously learn the modality-speciﬁc features and ex-tract the modality-invariant features. In a confusion learn-ing manner, it achieves a balance between the modality con-fusion and the general cross-modality feature learning.
Furthermore, we introduce an identity-aware marginal center aggregation strategy to reinforce the representation invariance against modality discrepancy (Fig. 1 (b)). The basic idea is to constrain that samples belonging to the same identity across two modalities are invariant. While encour-aging the extraction of centralized features, a marginal con-straint is incorporated to make sure that the samples are not too concentrated. This keeps the feature diverse, but it greatly enhances the generalization ability. In addition, based on the observation that person images are captured in totally different camera environments, we further inte-grate a camera-aware marginal center aggregation scheme (Fig. 1 (c)). This component fully utilizes the camera labels, capturing the camera-speciﬁc information for the learned representation. This constraint enhances the robustness against camera variations. The proposed components might be easily integrated into other advanced learning models.
Our main contributions can be summarized as follows:
We propose a novel Modality Confusion Learning Network (MCLNet) for cm-ReID. It is an effective learning structure to extract modality irrelevant representation, reinforcing the robustness of the learned representation against modality variations. We introduce an identity-aware marginal con-strained center aggregation strategy. It extracts the central-ization features, while keeping the diversity for better gen-eralization ability with a marginal constraint. We design a camera-aware learning scheme that applies the camera la-bel supervision, enriching the discriminability via camera-aware representations. Extensive experimental results show that our novel framework outperforms the state-of-the-art methods on two cm-ReID datasets. 2.