Abstract
Currently, the state-of-the-art methods treat few-shot semantic segmentation task as a conditional foreground-background segmentation problem, assuming each class is independent.
In this paper, we introduce the concept of certain meta-class, which is the meta information (e.g. middle-level features) shareable among all classes. To ex-plicitly learn meta-class representations in few-shot seg-mentation task, we propose a novel Meta-class Memory based few-shot segmentation method (MM-Net), where we introduce a set of learnable memory embeddings to memo-rize the meta-class information during the base class train-ing and transfer to novel classes during the inference stage.
Moreover, for the k-shot scenario, we propose a novel im-age quality measurement module to select images from the set of support images. A high-quality class prototype could be obtained with the weighted sum of support image fea-tures based on the quality measure. Experiments on both
PASCAL-5i and COCO datasets show that our proposed method is able to achieve state-of-the-art results in both 1-shot and 5-shot settings. Particularly, our proposed MM-Net achieves 37.5% mIoU on the COCO dataset in 1-shot setting, which is 5.1% higher than the previous state-of-the-art. 1.

Introduction
With the development of convolution neural networks (CNNs), fully supervised image semantic segmentation
[12, 3] has achieved great success in both speed and ac-curacy. However, the state-of-the-art image segmentation methods usually require abundant pixel-level annotations which requires huge human labeling efforts.
If we want to segment a new class that has not been seen in the train-ing set, we usually need to label thousands of images for
*Corresponding author: G. Lin (e-mail: gslin@ntu.edu.sg)
Figure 1. Comparison between the typical pipeline of state-of-the-art (SOTA) methods (top part) and that of our proposed meta-class memory based network (MM-Net) (bottom part) for few-shot seg-mentation. The main difference is that SOTA treats the task as a class-agnostic conditional foreground-background segmentation problem, while we propose to learn a set of meta-class middle-level representations shareable between base and novel classes. the new class.
In order to reduce human labeling efforts on novel classes, the few-shot image segmentation task
[35, 17] has been introduced, which aims to predict the segmentation mask of a query image of a novel class with only one or a few labelled support images in testing, while with abundant images of base classes with full annotations in training.
The top part of Fig. 1 shows the typical pipeline of the state-of-the-art (SOTA) few-shot image segmentation meth-ods [35, 34]. Firstly, a pre-trained CNN network is used to extract the features of both support and query images.
Then, the two features are typically processed by convo-lutional layers and compared for similarity so as to gener-ate the segmentation map for the query image. Essentially, these methods treat the few-shot segmentation task as a con-ditional binary foreground-background segmentation prob-lem, i.e. to find and segment the most relevant regions in the query image based on the given support images and their masks, regardless of the class information.
The class-agnostic design in SOTA is understandable.
This is because by class-agnostic design the interaction / comparison between query and support features in base classes can be transferred to novel classes. However, we argue that although different classes of objects are quite dif-ferent, there are still some common attributes or middle-level knowledge shareable among them, which we call meta-class information. Similar observations have been made in [38, 10, 32] for classification and detection tasks, where some low-level information (e.g. circle, dot) and middle-level information (e.g. wings, limbs) are shared among different classes.
Motivated by this, in this paper, we propose a novel
Meta-class Memory Module (MMM) to learn middle-level meta-class embeddings shareable between base and novel classes for few-shot segmentation. As shown in the lower part of Fig. 1, a set of meta-class memory embeddings is introduced into the SOTA pipeline, which can be learned through the back-propagation during the base class training.
The meta-class memory embeddings are then used to attend the middle-level features of query and support images to obtain meta-class activation maps. This can be considered as aligning both query and support middle-level features to the meta-class embeddings. Based on the obtained meta-class activation maps of query and support images, we then perform interaction / comparison between them to propa-gate the support mask information from support activation maps to query activation maps, and the fused query activa-tion maps are finally used for the query mask prediction.
In addition, when it comes to the k-shot scenarios, which means more than one support images are given, previous methods usually apply an average operation [34] on the few support image features to obtain the class prototype feature.
However, we observe that some support images are in low quality which is hard to represent the support class. Thus, we further propose a Quality Measurement Module (QMM) to obtain the quality measure for each support image. Based on the quality measure, the features from all support images are fused via weighted sum to get a better class prototype feature.
In our experiments, we follow the design of [26] to per-form training and testing for a fair comparison. We evalu-ate our proposed method on four different splits with 1-shot and 5-shot settings on PASCAL-5i [24] and COCO [13] datasets. Our method is able to achieve state-of-the-art re-sults on both datasets under both 1-shot and 5-shot settings.
Our main contributions can be summarized as follows:
• For few-shot semantic image segmentation, to our knowledge, we are the first one to introduce a set of learnable embeddings to memorize the meta-class in-formation during base class training that can be trans-ferred to novel classes during testing. Specifically, a
Meta-class Memory Module (MMM) is proposed to generate the meta-class activation maps for both sup-port and query images, which is helpful for the final query mask prediction.
• For k-shot scenarios, a Quality Measurement Module (QMM) is proposed to measure the quality of all the support images so as to effectively fuse all the support features. With QMM, our model is able to pay more attention to the high quality support samples for better query image segmentation.
• Extensive experiments on PASCAL-5i and COCO datasets show that our proposed method performs the best in all settings. Specifically, our method signif-icantly outperforms SOTA on the large scale dataset
COCO, with 5.1% mean mIoU gain, as our memory embeddings are able to learn a universal meta-class representation. 2.