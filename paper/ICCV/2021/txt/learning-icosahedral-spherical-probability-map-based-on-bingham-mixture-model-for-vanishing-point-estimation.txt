Abstract
Existing vanishing point (VP) estimation methods rely on pre-extracted image lines and/or prior knowledge of the number of VPs. However, in practice, this information may be insufﬁcient or unavailable. To solve this problem, we propose a network that treats a perspective image as input and predicts a spherical probability map of VP. Based on this map, we can detect all the VPs. Our method is reliable thanks to four technical novelties. First, we leverage the icosahedral spherical representation to express our proba-bility map. This representation provides uniform pixel dis-tribution, and thus facilitates estimating arbitrary positions of VPs. Second, we design a loss function that enforces the antipodal symmetry and sparsity of our spherical prob-ability map to prevent over-ﬁtting. Third, we generate the ground truth probability map that reasonably expresses the locations and uncertainties of VPs. This map unnecessar-ily peaks at noisy annotated VPs, and also exhibits various anisotropic dispersions. Fourth, given a predicted probabil-ity map, we detect VPs by ﬁtting a Bingham mixture model.
This strategy can robustly handle close VPs and provide the conﬁdence level of VP useful for practical applications. Ex-periments showed that our method achieves the best com-promise between generality, accuracy, and efﬁciency, com-pared with state-of-the-art approaches. 1.

Introduction
Vanishing point (VP) is the intersection of two image lines whose corresponding 3D lines are parallel. VP has various applications such as scene understanding [12], cam-era orientation estimation [14] and 3D reconstruction [13].
While VP estimation has been widely studied, existing ap-proaches have two main limitations. First, numerous meth-ods [25, 24, 22, 33, 1, 18] rely on pre-extracted image lines, but they are sensitive to the number and quality of lines. For example, given a small number of lines, a method may ne-*Haoang Li and Kai Chen contributed equally to this work.
†Kyungdon Joo and Yun-Hui Liu are corresponding authors. (a) Ground Truth (b) Result of [22] (c) Result of [33] (d) Result of [25] (e) Our Input Image (f) Our Output Map and DDs
Figure 1. (a) Four ground truth VPs are associated with red, green, blue and yellow line clusters, respectively (we have ﬁltered out short lines). (b) [22] neglects a VP associated with dotted cyan lines due to insufﬁcient lines. (c) [33] mistakenly detects a VP as-sociated with solid cyan outliers. (d) [25] neglects a VP associated with dotted cyan lines due to the assumption of three orthogonal
VPs. (e, f) We reformulate VP estimation as DD computation.
Given a perspective image, our network predicts a spherical prob-ability map of DD. Based on this map, we can detect all the DDs. glect some VPs (see Figs. 1(a) and 1(b)). Moreover, given several lines corrupted by outliers, e.g., shadow boundaries, a method may mistakenly detect VPs (see Figs. 1(a) and 1(c)). Second, many methods [4, 39, 25, 24, 41] rely on prior knowledge of the number of VPs. They typically as-sume three orthogonal VPs in Manhattan world [9], and thus neglect partial VPs or result in redundant detection in non-Manhattan scenes [30, 38] (see Figs. 1(a) and 1(d)).
While recent approaches [15, 22, 23, 18] can automatically determine the number of VPs, they rely on image lines.
To overcome the above limitations, we propose the ﬁrst
VP estimation method that is independent of image lines and also can automatically determine the number of VPs.
Speciﬁcally, as shown in Fig. 1(e), the connection between a VP and camera center is aligned to a dominant direction (DD). Compared with VP that may be extremely far from
the image center, unit DD starting at the camera center is enclosed by a unit sphere. We thus follow [41, 22] to re-formulate VP estimation as DD computation, i.e., we aim to determine which positions on the sphere correspond to
DDs. To achieve this goal, we propose a network that treats a perspective image as input and predicts a spherical proba-bility map of DD. Based on this map, we can detect all the
DDs, regardless of the number of DDs.
Our method is reliable thanks to four technical novel-ties. First, as shown in Fig. 1(f), we leverage the icosahe-dral spherical representation [2] to express our probability map.1 This representation provides a more uniform pixel distribution than the widely-used equi-angular discretiza-tion on the sphere (see Fig. 2(a)). Accordingly, it facilitates estimating arbitrary orientations of DDs. Second, we de-sign a loss function that not only is effective in ﬁtting data, but also enforces the antipodal symmetry and sparsity of our spherical probability map for regularization. Third, we generate the ground truth map that reasonably expresses the locations and uncertainties of DDs. This map unnecessar-ily peaks at noisy annotated DDs and also exhibits various anisotropic dispersions (see Fig. 6(d)). We train our net-work by minimizing the difference between the predicted and ground truth probability maps. Fourth, given a pre-dicted probability map, we detect DDs by ﬁtting a Bing-ham mixture model [6] (see Fig. 1(f)). This strategy is free of threshold, and thus can handle close DDs more robustly than non-maximum suppression [27]. Moreover, it can pro-vide the conﬁdence level of DD useful for practical appli-cations. Our main contributions are:
• Our method is independent of image lines and also can automatically determine the number of DDs.
• We leverage the icosahedral spherical representation to express our probability map. This representation facil-itates estimating arbitrary orientations of DDs.
• We design a loss function that enforces the antipo-dal symmetry and sparsity of our spherical probability map to prevent over-ﬁtting.
• We introduce a strategy to generate the ground truth probability map that reasonably expresses the loca-tions and uncertainties of DDs.
• We detect DDs by ﬁtting a Bingham mixture model on a predicted map. This strategy is free of threshold and can provide the conﬁdence level of DD. 2.