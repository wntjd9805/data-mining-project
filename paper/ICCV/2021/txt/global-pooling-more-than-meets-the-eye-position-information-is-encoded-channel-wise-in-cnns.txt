Abstract
In this paper, we challenge the common assumption that collapsing the spatial dimensions of a 3D (spatial-channel) tensor in a convolutional neural network (CNN) into a vector via global pooling removes all spatial information. Specifi-cally, we demonstrate that positional information is encoded based on the ordering of the channel dimensions, while semantic information is largely not. Following this demon-stration, we show the real world impact of these findings by applying them to two applications. First, we propose a simple yet effective data augmentation strategy and loss func-tion which improves the translation invariance of a CNN’s output. Second, we propose a method to efficiently deter-mine which channels in the latent representation are re-sponsible for (i) encoding overall position information or (ii) region-specific positions. We first show that semantic segmentation has a significant reliance on the overall po-sition channels to make predictions. We then show for the first time that it is possible to perform a ‘region-specific’ attack, and degrade a network’s performance in a particular part of the input. We believe our findings and demonstrated applications will benefit research areas concerned with un-derstanding the characteristics of CNNs. Code is available at: https://github.com/islamamirul/PermuteNet. 1.

Introduction
One of the fundamental ideas behind different neural net-work architectures [10, 23, 24, 2, 9] is the idea of invariance.
Given an input signal, an X-invariant operation is one that produces the same output regardless of any change (of some type X) to the input. This property is desirable in a multitude of applications in computer vision, the most obvious being object recognition [17, 10, 23]; the goal being to assign the corresponding image-level label (e.g., dog) regardless of where the object is located in the image. This is referred to
*Equal Contribution as translation invariance. Another property of operations that is intimately related to translation invariance is that of translation equivariance: shifting the input and then passing it through the operation is equivalent to passing the input through the operation, and then shifting the signal.
To achieve invariant neural networks, a common strat-egy is to use equivariant operations on a per-layer basis [5], which then culminates in an invariant output. One of the best examples of this can be found in convolutional neural networks (CNNs) for the task of image classification. Fol-lowing a hierarchy of translation equivariant convolutional layers, CNNs use a global pooling layer to transform the 3D (spatial-channel) tensor into a 1D vector, which is then fed into a fully connected layer to produce the classification log-its. One would therefore (intuitively) assume that after the spatial dimensions are collapsed due to the global pooling operation [18, 24], spatial information should be removed while translation invariance is produced. However, previous work has shown that absolute position information exists both in the latent representations [11] as well as the output of the network [14, 12].
None of these previous works have answered the critical question: How can a CNN contain positional information in the representations if there exists a global pooling layer in the forward pass? In this paper, we provide an answer to this question, and demonstrate through rigorous quantitative experiments that CNNs do this by encoding the position information along the channel dimension even though the spatial dimensions are collapsed. Moreover, we show that the position information is encoded based on the ordering of the channel dimensions, while the semantic information is largely invariant to this ordering. We argue that these findings are important to better understand the properties of
CNNs and to guide their future design.
To demonstrate the real-world impact of these findings, we leverage the fact that position information is encoded channel-wise in a number of domains and applications. First, we tackle the lack of translation invariance of CNNs. We propose a simple yet effective loss function which minimizes
the distance between the encodings of translated images to achieve higher translation invariance. Second, we propose an efficient way to identify which channels in the latent representation are responsible for encoding both (i) posi-tion information in the entire image and (ii) ‘region-specific’ position information (e.g., channels which activate for the left part of the image). We show quantitative and qualita-tive evidence that networks have a significant reliance on these channels when making predictions compared with ran-domly sampled channels. Finally, we show it is possible to target region-specific neurons, and impair the performance in a particular part of the image. To summarize, our main contributions are as follows:
• We reveal how global pooling layers admit spatial infor-mation via an ordered coding along the channel dimen-sion. We then demonstrate the real-world applicability of this finding by applying it to the problem domains that follow in this list.
• We propose a simple data-augmentation strategy to improve the translation invariance of CNNs by mini-mizing the distance between the encodings of translated images.
• We propose a simple and intuitive technique to iden-tify the position-specific neurons in a network’s latent representation. We show that multiple complex net-works contain a significant reliance on these position-encoding neurons to make correct predictions.
• We show it is possible to attack a network’s predic-tions in a region-specific manner, and demonstrate the efficacy of this approach on a standard self-driving se-mantic segmentation dataset. 2.