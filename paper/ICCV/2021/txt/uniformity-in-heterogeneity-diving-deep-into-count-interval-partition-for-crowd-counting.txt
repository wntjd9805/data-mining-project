Abstract
Recently, the problem of inaccurate learning targets in crowd counting draws increasing attention. Inspired by a few pioneering work, we solve this problem by trying to predict the indices of pre-deﬁned interval bins of counts in-stead of the count values themselves. However, an inappro-priate interval setting might make the count error contribu-tions from different intervals extremely imbalanced, lead-ing to inferior counting performance. Therefore, we pro-pose a novel count interval partition criterion called Uni-form Error Partition (UEP), which always keeps the ex-pected counting error contributions equal for all intervals to minimize the prediction risk. Then to mitigate the in-evitably introduced discretization errors in the count quan-tization process, we propose another criterion called Mean
Count Proxies (MCP). The MCP criterion selects the best count proxy for each interval to represent its count value during inference, making the overall expected discretiza-tion error of an image nearly negligible. As far as we are aware, this work is the ﬁrst to delve into such a classiﬁca-tion task and ends up with a promising solution for count interval partition. Following the above two theoretically demonstrated criterions, we propose a simple yet effec-tive model termed Uniform Error Partition Network (UEP-Net), which achieves state-of-the-art performance on sev-eral challenging datasets. The codes will be available at:
TencentYoutuResearch/CrowdCounting-UEPNet. 1.

Introduction
The task of crowd counting estimates the number of peo-ple in given images or videos. It has drawn remarkable at-∗Equal contribution. †Corresponding author.
Figure 1: The illustration of our counting pipeline. For image patches with count values between ti and ti+1, our
UEPNet learns to classify them into one (i.e., ci+1) of the m intervals. The count interval borders (t0, ..., tm−2) are de-termined by the proposed UEP criterion to ensure the mini-mum expected prediction error. We assign the ground truth class of a patch by the count interval it falling into. During inference, for a patch classiﬁed to the interval class of ci, we set its predicted count to be the count proxy value σi of ci. And the count proxy is a speciﬁc value selected from that interval with our proposed MCP criterion. tention in recent years, since its wide range of practical ap-plications in public safety. Currently, most existing state-of-the-art methods adopt density maps [1, 18, 19] as the targets and learn with Convolutional Neural Networks (CNNs).
However, the widely used learning targets, i.e., the den-sity maps, are actually inaccurate and tend to be noisy, as discussed in [1, 12, 17, 20, 23]. These imperfect density maps are caused by several factors, such as empirically se-lected Gaussian kernels, large density variations and label-ing deviations. As shown in the supplementary materials, the inaccurate density map introduces severe inconsistency between the semantic content and the ground truth target, acting as a kind of “noises” in the learning target. There-fore, when learning the exact count values with an outlier-sensitive loss, such as the commonly used Mean Squared
Error (MSE), the model may tend to overﬁt these inaccurate and ambiguous “noises”, leading to inferior performance and poor generalization ability [12].
Inspired by a few pioneering works [3, 12, 21, 25], we solve the above problem with the paradigm of local count (the count value of a local patch) classiﬁcation, as shown in
Figure 1. In such a paradigm, the local count is quantiﬁed to one of several pre-deﬁned count interval bins, depending on the interval it falls into. Then we resorts to count interval classiﬁcation by taking each interval as a single class. How-ever, the number of samples with increasing local counts follows a long-tailed distribution, leading to imbalanced training data and extreme counting errors for some count intervals. What makes the problem worser is that sam-ples from different intervals have various learning difﬁculty,
In this pa-thus also contribute different counting errors. per, ﬁrstly, we solve the above problem from the perspective of designing an optimal interval partition strategy. Specif-ically, we derive an Uniform Error Partition (UEP) crite-rion following the principle of maximum entropy. Without any prior over the density distribution of an unseen image, the UEP criterion keeps the expected counting errors nearly equal for all intervals to minimize the prediction risk. Such a novel strategy provides a comprehensive consideration of misclassiﬁcation errors and sample imbalance problem.
Secondly, another crucial yet unsolved problem of this paradigm raises from inference stage. Speciﬁcally, a ﬁxed value (i.e., count proxy) should be selected from each count interval, using as the predicted counts for those patches classiﬁed to this interval. Obviously, mapping a range of local counts to a single count value would inevitably in-troduce perceivable discretization errors. To mitigate the above problem, we propose a Mean Count Proxies (MCP) criterion for the count proxy selection. The MCP criterion uses average count value of samples in an interval as the optimal count proxy, with which the expected discretization error is theoretically demonstrated to be nearly negligible.
Thirdly, since the distribution of count values is essen-tially continuous, it is tricky to unambiguously classify sam-ples whose count values are located around the interval bor-ders. Thus, we design two parallel prediction heads with overlapping count intervals, named Interleaved Prediction
Heads (IPH). In this reciprocal prediction structure, the samples whose count values fall near the interval borders of one head are more likely to be correctly classiﬁed in an-other head, reducing the misclassiﬁcation errors due to the learning ambiguity around interval borders. We conduct ex-tensive experiments on several public datasets with various crowd densities to show the consistent improvements. The contributions of this paper are summarized as follows: 1. We propose a novel criterion termed UEP to tackle with the crucial yet unsolved count interval partition prob-lem, making the local count classiﬁcation based counting models achieve the minimum prediction risk. 2. We propose the MCP criterion to minimize the dis-cretization errors inevitably introduced by the count quan-tiﬁcation, which is orthogonal to the UEP criterion. 3. We propose a simple yet effective model termed UEP-Net, following the above two key criterions. Combining with the IPH, the UEPNet achieves state-of-the-art perfor-mance on several benchmarks. 2.