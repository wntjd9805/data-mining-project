Abstract
Egocentric 3D human pose estimation using a single fisheye camera has become popular recently as it allows capturing a wide range of daily activities in unconstrained environments, which is difficult for traditional outside-in motion capture with external cameras. However, existing methods have several limitations. A prominent problem is that the estimated poses lie in the local coordinate sys-tem of the fisheye camera, rather than in the world coor-dinate system, which is restrictive for many applications.
Furthermore, these methods suffer from limited accuracy and temporal instability due to ambiguities caused by the monocular setup and the severe occlusion in a strongly distorted egocentric perspective. To tackle these limita-tions, we present a new method for egocentric global 3D body pose estimation using a single head-mounted fish-eye camera. To achieve accurate and temporally stable global poses, a spatio-temporal optimization is performed over a sequence of frames by minimizing heatmap repro-jection errors and enforcing local and global body motion priors learned from a mocap dataset. Experimental results show that our approach outperforms state-of-the-art meth-ods both quantitatively and qualitatively. 1.

Introduction
Traditional optical motion capture system with external, outside-in facing cameras is restrictive for many pose es-timation applications that require the person to be able to roam around in a larger space, beyond a fixed recording volume. Examples are mobile interaction applications, pose estimation in large-scale workplace environments, or many
AR/VR applications. To enable this, methods for egocentric 3D human pose estimation using head- or body-mounted cameras were researched. These methods are mobile, flexi-ble, and have the potential to capture a wide range of daily human activities even in large-scale cluttered environments.
Some egocentric capture methods study the estimation of face [9, 8, 21] and hand motions [38, 40, 27, 39], while the estimation of the global full body pose has been less
Figure 1. Given challenging egocentric videos, our method pro-duces realistic and accurate 3D global pose sequence. explored. Mo2Cap2 [45] and xR-egopose [43] use a sin-gle head-mounted fisheye camera to capture the 3D skele-tal body pose in a marker-less way. Both methods have demonstrated compelling 3D pose estimation results while still suffering from an important limitation: They estimate the local 3D body pose in egocentric camera space, while not being able to obtain the body pose with global position and orientation in the world coordinate system. Henceforth, we will refer to the former as “local pose”, in order to dis-tinguish it from the “global pose” defined in the world co-ordinate system. Local pose capture alone is insufficient for many applications. For example, captured local body poses are not enough to animate the locomotion of a virtual avatar in xR environments, which requires global poses.
A straightforward solution is to simply project the lo-cal pose into the world coordinate system with the egocen-tric camera pose estimated by the SLAM. However, the ob-tained global poses exhibit significant inaccuracies. First, they show notable temporal jitters as the video frames are processed independently without taking temporal frame co-herence. Second, they often show tracking failure due to
the self-occlusion in the distorted view of the fisheye cam-era. Third, the obtained global poses often show unrealis-tic motions (such as foot sliding and global jitters) due to the inconsistency between the local pose and the estimated camera pose, which are independent of each other.
To tackle these challenges, we propose a novel approach for accurate and temporally stable egocentric global 3D pose estimation with a single head-mounted fisheye cam-era, as illustrated in Fig. 1. In order to obtain temporally smooth pose sequences, we resort to a spatio-temporal op-timization framework where we leverage the 2D and 3D keypoints from CNN detection as well as VAE-based mo-tion priors learned from a large mocap dataset. The VAE-based motion priors have been proven effective to produce realistic and smooth motions in pose estimation methods like VIBE [19] and MEVA[25]. However, the RNN-based
VAEs in these works are less efficient and unstable due to the vanishing and exploding gradients during our optimiza-tion process. Therefore, we propose a new convolutional
VAE-based motion prior, which enables faster optimization speed and higher accuracy. Furthermore, to reduce the error due to strong occlusion, we proposed a novel uncertainty-aware reprojection energy term by summing up the proba-bility values at the pixels on the heatmap occupied by the projection of the 3D estimated joints rather than comparing the projection of 3D estimated joints against the predicted 2D joint position. Finally, in order to make the local body poses consistent with the camera poses estimated by SLAM, we introduce a global pose optimizer with a separate VAE.
We evaluate our method on the dataset provided by
Mo2Cap2 [45] and also a new benchmark we collected with 2 subjects performing various motions. Our method outper-forms the state-of-the-art methods both quantitatively and qualitatively. Our ablative analysis confirms the efficacy of our proposed optimization algorithm with learned motion prior and uncertainty-aware reprojection loss for improved local and global accuracy and temporal stability. To sum-marize, our technical contributions are as follows:
• A novel framework for accurate and temporally stable global 3D human pose estimation from a monocular ego-centric video.
• A new optimization algorithm with the assistance of local and global motion prior captured by an efficient convolu-tional network based VAE.
• An uncertainty-aware reprojection loss to alleviate the in-fluence of self-occlusions in the egocentric settings.
• Our method outperforms various baselines in terms of the accuracy of the estimated global and local pose.
Our method works for a wide range of motions
We recommend watch-in various environments. ing the video in http://gvv.mpi-inf.mpg.de/ projects/globalegomocap for better visualization. 2.