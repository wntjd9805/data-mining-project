Abstract
Active learning (AL) is successful based on the assump-tion that labeled and unlabeled data are obtained from the same class distribution. However, its performance deteri-orates under class distribution mismatch, wherein the un-labeled data contain many samples out of the class distri-bution of labeled data. To effectively handle the problems under class distribution mismatch, we propose a contrastive coding based AL framework named CCAL. Unlike the exist-ing AL methods that focus on selecting the most informative samples for annotating, CCAL extracts both semantic and distinctive features by contrastive learning and combines them in a query strategy to choose the most informative un-labeled samples with matched categories. Theoretically, we prove that the AL error of CCAL has a tight upper bound.
Experimentally, we evaluate its performance on CIFAR10,
CIFAR100, and an artificial cross-dataset that consists of five datasets; consequently, CCAL achieves state-of-the-art performance by a large margin with remarkably lower an-notation cost. To the best of our knowledge, CCAL is the first work related to AL for class distribution mismatch. 1.

Introduction
Deep Learning, which largely depends on sufficient la-beled data, has achieved unprecedented breakthroughs in supervised learning [23]. Nevertheless, it is impractical to obtain abundant labeled data because labeling requires enormous human and financial costs [36].
Active learning (AL) selects the most informative sam-ples to query their labels, delivering a competitive target model while saving annotation costs relative to the super-In traditional AL methods, it’s gen-vised learning [36]. erally assumed that labeled and unlabeled data are drawn from the same class distribution, i.e., the categories of un-labeled data are the same as those of labeled ones. Un-fortunately, this assumption cannot be maintained in many
*Corresponding Author
†Equal Contribution
Figure 1: An instance of class distribution mismatch. Un-labeled data contains some samples that are out of the class distribution of labeled data. real-world scenarios since that unlabeled data always con-tain lots of samples out of the class distribution of labeled data, i.e., some categories of unlabeled data are not pre-sented in labeled ones. For example, when crawling a large set of images (as shown in Figure 1) [49] by keyword fil-tering (“dog”, “cat”) from the Internet for binary image classification, many unlabeled images not belonging to tar-get classes ( such as “deer”, “horse”, “airplane”, “ship”,
“car”, “flower”) are collected. The same problem has al-ready been found in medical diagnoses containing unseen lesions [11, 48] and the house annotation of remote sens-ing images containing numerous natural sceneries. These scenarios have been formalized as the learning framework, called class distribution mismatch [11] [7].
Under class distribution mismatch, an AL algorithm will suffer a sharp drop in performance if only focusing on querying the “most informative” samples. One main reason for this phenomenon is that a large set of samples with mis-matched categories will be queried, which are invalid for the target model, thereby wasting the annotation budget. Thus, it is essential to reduce the cost for invalid queries while im-proving the information of samples queried (valid queries) in class distribution mismatch. Heuristically, we introduce both invalid query error and valid query error to combat the problem, as described in Eq.1. Specifically, an invalid query error is attributed to those queried samples invalid for im-proving the target model, and a valid query error is due to
3) Dividing the AL error into invalid query error and valid query error, and proving that the AL error of CCAL has a tight upper bound under class distribution mismatch.
The remainder of this paper is organized as follows. In
Section 2, we review some related work. In Section 3, the proposed approach CCAL and theoretical studies are intro-duced. Section 4 presents the experiments, followed by the conclusion in Section 5.
Figure 2: CCAL is combining the semantic score Ssem and distinctive score Sdis to select samples for annotating. 2.