Abstract 1.

Introduction 3D hand pose estimation from monocular videos is a long-standing and challenging problem, which is now see-In this work, we address it for ing a strong upturn. the first time using a single event camera, i.e., an asyn-chronous vision sensor reacting on brightness changes. Our
EventHands approach has characteristics previously not demonstrated with a single RGB or depth camera such as high temporal resolution at low data throughputs and real-time performance at 1000 Hz. Due to the different data modality of event cameras compared to classical cam-eras, existing methods cannot be directly applied to and re-trained for event streams. We thus design a new neu-ral approach which accepts a new event stream represen-tation suitable for learning, which is trained on newly-generated synthetic event streams and can generalise to real data. Experiments show that EventHands outperforms re-cent monocular methods using a colour (or depth) camera in terms of accuracy and its ability to capture hand motions of unprecedented speed. Our method, the event stream sim-ulator and the dataset are publicly available (see https:
//4dqv.mpi-inf.mpg.de/EventHands/).
Event cameras are vision sensors which respond to events, i.e., local changes in the incoming brightness sig-nals. In contrast to conventional RGB cameras which record images at a pre-defined frequency (e.g., 30−60 fps), event cameras operate asynchronously, which enables high clock speeds and temporal resolution of up to 1µs [29]. Due to their unique properties and high-dynamic range, event cameras have already found applications in low-level vision
[7, 40, 45, 67], low-latency robotics [54, 15], visual simul-taneous localisation and mapping (SLAM) [63, 24], fea-ture and object tracking [2, 34], gesture recognition [3, 60], experimental physics (particle tracking and velocimetry)
[9, 61] and astronomy [13, 74], among other fields.
In this work, we are interested in 3D hand pose regres-sion using a single event camera. The high dynamic range, lower latency, and lower throughput of event data are better suited than conventional images for tracking hands, which are often in rapid motion. However, due to the drastically different and less regular data from event cameras, exist-ing RGB- or depth-based methods [10, 68, 31, 36] cannot be directly applied to event streams. A na¨ıve way would be to reconstruct greyscale images from an event stream at arbitrary temporal resolutions first, and then run the same
monocular methods [6, 10, 68, 71, 41] on those. This policy would, unfortunately, nullify most advantages of event cam-eras such as low data bandwidth, abstraction from a large variety in textures and illumination conditions and suppos-edly better generalisation ability. It is also not clear how ex-isting methods would perform on low-resolution greyscale images reconstructed from event streams, as the latter often contain artefacts, and cannot reproduce the exact occurred brightness, due to non-deterministic event thresholds and noise [7, 14, 47]. The primary research question is thus how hands can be reconstructed and tracked in 3D directly from event streams.
We pursue in this paper a learning-based approach and propose the first—to the best of our knowledge—method for 3D reconstruction of a human hand from a single event stream (see Fig. 1). Our neural EventHands approach learns to regress 3D hand poses, represented as global rotation and translation as well as pose parameters of a paramet-ric hand model [46], from locally-normalised event sur-faces (LNES), which is a new way of accumulating events over temporal windows for learning. We generate training data for our neural network using a new high-throughput event stream simulator relying on a parametric hand model
[46, 41]. The training data includes variations in hand shape and texture, lighting, and scene background, and ac-curately mimics the characteristics of a real event camera.
Hence, EventHands generalises well to real data despite be-ing trained with synthetic data only. Next, EventHands runs at 1 KHz, which is significantly faster than any image-based prior works. In summary, our contributions are:
• EventHands, i.e., the first approach for 3D hand pose estimation, including rotation and translation in 3D, from a single event stream, running at 1 KHz.
• A new high-throughput event stream simulator sup-porting a parametric 3D hand model for diverse poses, shapes, and textures, multiple light sources, adjustable event stream properties (e.g., event threshold distribu-tions, noise patterns) and further scene augmentation.
• A live real-time demonstrator of our method running orders of magnitude faster than previous image-based work on a workstation with a single GPU. See our sup-plementary video for recordings thereof.
We evaluate the proposed approach on a wide variety of motions with real and synthetic data, and provide numeri-cal evidence for our design choices as well as comparisons to prior work. We show that EventHands yields accurate estimates even when existing RGB- and depth-based tech-niques fail due to fast motion. 2.