Abstract 1.

Introduction
CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the orig-inal NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embed-dings. At test time, given a single unposed image of an un-seen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF gen-eralises well to unseen objects and achieves on-par perfor-mance with methods that require known camera pose at test time. Our results on real-world images demonstrate that
CodeNeRF can bridge the sim-to-real gap. Project page: https://github.com/wayne1123/code-nerf
Synthesizing novel views of unseen objects given a sparse set of input views or even a single image is a long-standing problem in the fields of computer vision and graph-ics. Synthesis methods require both an accurate representa-tion of the 3D geometry and appearance of objects, and the ability to offer control over changes in viewpoint, shape or texture to render different objects of the same category.
Traditionally, discrete scene representations have been used, such as meshes or voxel grids that store geometry and appearance information either explicitly or via learnt neural features [23]. However, their discrete nature limits their representation power and resolution. With the recent introduction of scene representation networks (SRN), Sitz-mann et al. [24] propose to learn a continuous function that map 3D locations to features of scene properties. Crucially,
SRN does not require 3D supervision and can be trained end-to-end with a differentiable ray marcher that renders the feature-based representation into a set of 2D images. While
SRN allows generalization to unseen objects, accurate cam-set of posed input images and simultaneously learns differ-ent latent embeddings for shape and texture, and the weights of a multi-layer perceptron to predict volume density and view-dependent radiance for each 3D point by enforcing multi-view photometric consistency. At inference, given a single unposed reference image of an unseen object, Co-deNeRF optimizes shape and texture codes as well as cam-era pose. Our disentangled representation provides full con-trol over the synthesis task, enabling explicit editing of ob-ject shape and texture simply by modifying the respective latent codes (see Fig 1). We demonstrate single view recon-struction, novel view synthesis and shape/texture editing on the SRN benchmark and real-world images.
While our work takes inspiration from other continuous neural scene representations [15, 24], it addresses many of their limitations. Unlike NeRF [15], CodeNeRF is not scene specific and can model the variations of shape and appear-ance across an object class. In contrast to SRN [24], Co-deNeRF disentangles geometry and appearance offering ex-plicit control over shape and texture for synthesis tasks. Un-like both, CodeNeRF does not require knowledge of camera pose at test time, estimating it via optimization. Inspired by
DeepSDF [17], we adopt an auto-decoder architecture but depart significantly as we only require 2D supervision and can disentangle shape and texture variations across an ob-ject category. See Fig 2 for an overview of CodeNeRF. 2.