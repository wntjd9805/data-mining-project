Abstract
Recent work has made signiﬁcant progress on using im-plicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Com-pared to rigid objects, articulated objects have higher de-grees of freedom, which makes it hard to generalize to un-seen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. With this disentangled continuous rep-resentation, we demonstrate that we can control the ar-ticulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adap-tation inference algorithm to adjust our model during in-ference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images. Project page with code: https://jitengmu.github.io/A-SDF/. 1.

Introduction
Modeling articulated objects has wide applications in multiple ﬁelds including virtual and augmented reality, ob-ject functional understanding, and robotic manipulation. To understand articulated objects, recent works propose to train deep networks for estimating per-part poses and the joint angle parameters of an object instance in a known cate-gory [40, 82]. However, if we want to interact with the articulated object (e.g., open a laptop), estimating its static state is not sufﬁcient. For example, an autonomous agent needs to predict what the articulated object shape will be like after interactions for planning its action.
In this paper, we introduce Articulated Signed Distance
Functions (A-SDF), a differentiable category-level articu-lated object representation, which can reconstruct and pre-dict the object 3D shape under different articulations. A dif-ferentiable model is useful in applications requiring back-propagation through the model to adjust inputs, such as ren-dering in graphics and model-based control in robotics.
We build our articulated object model based on the deep implicit Signed Distance Functions [58]. While implicit functions have recently been widely applied in modeling static object shape with ﬁne details [64, 65, 72], much less effort has been devoted to modeling general articulated ob-jects. We observe that models with a single shape code in-put, such as DeepSDF [58], cannot encode the articulation variation reliably. It is even harder for the models to gener-alize to unseen instances with unseen joint angles.
To improve the generalization ability, we propose to 1
model the joint angles explicitly for articulated objects. In-stead of using a single code to encode all the variance, we propose to use one shape code to model the shape of object parts and a separate articulation code for the joint angles. To achieve this, we design two separate networks in our model: (i) a shape embedding network to produce a shape embed-ding given a shape code input; (ii) an articulation network which takes input both the shape embedding and an artic-ulation code to deform the object shape. During training, we use the ground-truth joint angles as inputs and learn the shape code jointly with both model parameters. To encour-age the disentanglement, we enforce the same instance with different joint angles to share the same shape code.
During inference, given an unseen instance with un-known articulation, we ﬁrst infer the shape code and articu-lation code via back-propagation. Given the inferred shape code, we can simply adjust the articulation code to gener-ate the instance at different articulations. We visualize the generation process and results for a few objects in Figure 1.
Note the part geometry remains the same as we ﬁx the in-ferred shape code during generation.
To generalize our model to out-of-distribution and un-seen data, e.g., partial point clouds and real-world depth images, we further propose a Test-Time Adaptation (TTA) approach to adjust our model during inference. Note that our unique model architecture with separate shape embed-ding and articulation network provides the opportunity to do so: the separation of shape embedding and articulation net-work ensures the disentanglement is maintained when the shape embedding network is adapted. We adapt the shape embedding network to the current test instance by updating its parameters, while ﬁxing the parameters of the articula-tion network. This procedure allows A-SDF to reconstruct and generate better shapes aligning with the inputs.
To our knowledge, our work is the ﬁrst paper tackling the problem of generic articulated object synthesis in the implicit representation context. We summarize the contri-butions of our paper as follows. First, we propose Articu-lated Signed Distance Functions (A-SDF) and a Test-Time
Adaptation inference algorithm to model daily articulated objects. Second, the disentangled continuous representa-tion allows us to control the articulation code and generate corresponding shapes as output on unseen instances with unseen joint angles. Third, the proposed representation shows signiﬁcant improvement on interpolation, extrapola-tion, and shape synthesis. More interestingly, our model can generalize to real-world depth images from the RBO dataset [45] and we quantitatively demonstrate superior per-formance over the baselines. 2.