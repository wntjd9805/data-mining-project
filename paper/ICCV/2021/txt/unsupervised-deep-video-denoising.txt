Abstract
Deep convolutional neural networks (CNNs) for video denoising are typically trained with supervision, assuming the availability of clean videos. However, in many appli-cations, such as microscopy, noiseless videos are not avail-able. To address this, we propose an Unsupervised Deep
Video Denoiser (UDVD1), a CNN architecture designed to be trained exclusively with noisy data. The performance of UDVD is comparable to the supervised state-of-the-art, even when trained only on a single short noisy video. We demonstrate the promise of our approach in real-world imaging applications by denoising raw video, ﬂuorescence-microscopy and electron-microscopy data.
In contrast to many current approaches to video denoising, UDVD does not require explicit motion compensation. This is advan-tageous because motion compensation is computationally expensive, and can be unreliable when the input data are noisy. A gradient-based analysis reveals that UDVD auto-matically adapts to local motion in the input noisy videos.
Thus, the network learns to perform implicit motion com-pensation, even though it is only trained for denoising. 1.

Introduction
Video denoising is a fundamental problem in image pro-cessing, as well as an important preprocessing step for com-puter vision tasks. Convolutional neural networks (CNNs)
[21] provide current state-of-the-art solutions for this prob-lem [34, 35, 41, 43, 11, 9, 8, 6]. These networks are typ-ically trained using a database of clean videos, which are corrupted with simulated noise. However, in applications such as microscopy, noiseless ground truth videos are often not available. To address this issue, we propose a method to train a video denoising CNN without access to super-*equal contribution. 1See https://sreyas-mohan.github.io/udvd/ for code and more results. vised data, which we call Unsupervised Deep Video De-noising (UDVD). UDVD is inspired by the “blind-spot” technique, recently introduced for unsupervised still image denoising [22, 17, 2, 19], in which a CNN is trained to es-timate each noisy pixel from the surrounding spatial neigh-borhood without including the pixel itself. Here, we pro-pose a blind-spot architecture that processes the surround-ing spatio-temporal neighborhood to denoise videos.
We show that UDVD is competitive with the current su-pervised state-of-the-art on standard benchmarks, despite not having access to ground-truth clean videos during train-ing (see Figure 1). Moreover, when combined with ag-gressive data augmentation and early stopping, it can pro-duce high-quality denoising even when trained exclusively on a single brief noisy video sequence (as few as 30 frames), outperforming unsupervised video denoising tech-niques (e.g. F2F[11] and MF2F [9]) which are pre-trained with supervision. Finally, methods based on pre-training are not suitable for imaging applications where clean data is unavailable. In contrast, we demonstrate that UDVD can effectively denoise three different real-world datasets: raw videos from surveillance cameras, ﬂuorescence-microscopy videos of cells, and electron-microscopy videos of catalytic nanoparticles.
The state-of-the-art performance of UDVD is unex-pected. Nearly all existing approaches to video denois-ing [24, 1, 3, 25], including those based on deep CNNs [34, 41, 11, 13, 42], use estimates of optical ﬂow to adaptively compensate for the motion of objects in the video. Con-ventional wisdom suggest that ignoring such motion should lead to denoising results in which moving content is blurred.
Contrary to this intuition, UDVD and some recent state-of-the-art supervised methods for video denoising [35, 8, 6] yield excellent empirical performance without explicit es-timation of optical ﬂow. How can is this achieved? We use a gradient-based analysis to show that both UDVD and supervised CNNs perform spatio-temporal adaptive ﬁlter-(a) Clean frame, PSNR / SSIM (b) Noisy Input, 19.06 / 0.279 (c) Supervised (FastDVDnet), 31.73 / 0.873 (d) Unsupervised (MF2F) , 30.35 / 0.825 (e) UDVD, 31.62 / 0.869 (f) UDVD-S, 31.39 / 0.865
Figure 1. Unsupervised denoising matches the performance of supervised denoising. Frame from a video in the Set8 dataset denoised using different approaches. (a) Clean frame. (b) Frame corrupted with Gaussian noise of standard deviation 30 (relative to intensity range
[0-255]). (c) FastDVDnet [35], a supervised method trained on the DAVIS dataset. (d) MF2F [9], an unsupervised method which ﬁne-tunes a pre-trained FastDVDnet on the noisy video (e) Our proposed unsupervised method (UDVD), which uses ﬁve frames to denoise each frame, trained on the DAVIS dataset. (f) UDVD trained only on the noisy video itself. Performance is quantiﬁed using PSNR / SSIM
[38], respectively. The corresponding videos, as well as additional examples, are included in Section C of the supplementary material. ing, which is aligned with underlying motion. Thus, these
CNNs are automatically performing implicit motion com-pensation. To quantify this, we demonstrate that it is pos-sible to estimate optical ﬂow accurately from the network gradients, even though the network architectures are not de-signed to account for optical ﬂow, and the models receive no optical-ﬂow information during training.
Our Contributions:
• A novel blind-spot architecture/objective for unsuper-vised video denoising, which achieves performance com-petitive with state-of-the-art supervised methods.
• A training paradigm using aggressive data augmentation (time and space reversal) and early stopping to achieve state-of-the-art performance from training on a single brief noisy video.
• A demonstration of our method’s effectiveness in denois-ing real-world electron and ﬂuorescence microscopy data, as well as raw videos. Unlike most existing methods for unsupervised video denoising, our proposed method does not require pre-training, which is key in real-world imag-ing applications.
• An analysis of the denoising mechanism learned by
UDVD, demonstrating that it performs implicit motion compensation even though it is only trained for denois-ing. We apply the analysis to supervised networks, show-ing that the same conclusion holds. 2.