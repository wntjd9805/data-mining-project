Abstract
Editing an image automatically via a linguistic re-quest can signiﬁcantly save laborious manual work and is friendly to photography novice. In this paper, we focus on the task of language-guided global image editing. Existing works suffer from imbalanced and insufﬁcient data distri-bution of real-world datasets and thus fail to understand language requests well. To handle this issue, we propose to create a cycle with our image generator by creating a novel model called Editing Description Network (EDNet) which predicts an editing embedding given a pair of im-ages. Given the cycle, we propose several free augmenta-tion strategies to help our model understand various edit-ing requests given the imbalanced dataset. In addition, two other novel ideas are proposed: an Image-Request Atten-tion (IRA) module which allows our method to edit an image spatial-adaptively when the image requires different editing degree at different regions, as well as a new evaluation met-ric for this task which is more semantic and reasonable than conventional pixel losses (e.g. L1). Extensive experiments on two benchmark datasets demonstrate the effectiveness of our method over existing approaches. 1.

Introduction
Image editing has a wide range of applications in many scenarios. With the growth of social media such as Insta-gram and Facebook, more and more users like to edit their photos before they post them. People would like to use spe-ciﬁc photo editing software like Photoshop, but using this kind of professional software is not easy. It may cost novice users a lot of time to learn, the process of editing is also time-consuming. Moreover, as smartphones have become the main user terminal, a way that can automatically edit the images using the voice of users (like Siri or Cortana) will be more user-friendly.
In this paper, we focus on the task of global image edit-ing via linguistic requests: given an input image and a lin-∗Corresponding author
Figure 1. The example of language-guided global image editing, compared to existing tasks. Our task work on real-world scenarios with editing requests. guistic editing request, the model is required to produce a target image that matches the request, as shown in the last row of Figure 1. Global image editing is to retouch the in-put image by adjusting the brightness, hue, saturation, con-trast, tint, etc. Among the vision and language area, text-guided image manipulation [11, 16, 5] may seem similar to our task, but they are actually different. First, current text-guided image manipulation methods [11, 16] are designed for domain-speciﬁc datasets, which are either constrained to images of a single salient object (e.g., bird) or virtual dataset with simple objects, as shown in the ﬁrst two rows of Figure 1. While in our task, we edit the images from the real-world environment that contains various objects and scenes. Sec-ond, text-guided image manipulation methods are designed for template text inputs, which summarize the attributes of the target image (e.g., “A bird with black eye rings and a black bill”) instead of describing the editing request. The linguistic requests on virtual datasets are also generated au-tomatically using the templates, which is hard to generalize to the requests from users. In our task, we receive images from the real-world environment and various linguistic re-quests from users that describe the editing process.
To tackle the language-guided global image editing, some methods [14, 22] try to map the language request into a sequence of executable editing operations. In this way, they require predeﬁned editing operations and need addi-[26] is a GAN-tional annotations of editing operations. based method, which models each global operation as a convolutional kernel and uses a neural generator that di-rectly outputs the edited image. However, existing meth-ods mainly have two limitations. First, current methods suf-fer from the problem of insufﬁcient and imbalanced data of real-world datasets. For example, The GIER dataset pro-posed by [22] has 7, 000 input-target-request triplets col-lected from online image editing websites through the time period from 2009 to 2020. Among the data, editing requests related to hue adjustment only account for less than 10% of the entire dataset, which indicates the imbalanced distribu-tion of different editing operations. In addition, more than 80% of the editing requests related to brightness operation is to increase the brightness, indicating that the imbalance even exists inside a speciﬁc operation. Such severe data im-balance issue will make methods fail to understand the lan-guage input well, resulting in simply brightening the input images when the received requests are related to brightness operation but actually to decrease the brightness, and also failing to respond to requests related to hue operation.
Second, even for global editing requests, it is often de-sirable to have different editing degrees on different image regions. For example, for an input image with a dark back-ground and a bright foreground, given a vague user request like “brighten the image”, it is more reasonable to brighten the background a lot but the foreground a little instead of brightening the whole image uniformly. However, previous methods can only apply editing operations globally, which results in unsatisfactory results.
To tackle the aforementioned limitations, we propose the Cycle Augmentation GAN (CAGAN) for language-guided global image editing. First, we propose a new cross-modal cyclic mechanism and data augmentation strat-egy to address the problem of insufﬁcient and imbalanced data. Since directly collecting the training triplet exam-ples (input-target-request) is expensive and laborious, we design a cross-modal cyclic mechanism to augment the data. Speciﬁcally, we devise an Editing Description Net-work (EDNet) to take in the input image and the edited image obtained from the generator, then produce the edit-ing embedding that speciﬁes the image transformation ap-plied on the input image. With EDNet and generator, we can apply swapping augmentation (i.e., swap the input and target image) and random augmentations (i.e., adjusting the brightness, hue of the image) and then reconstruct the in-put image without leveraging linguistic requests. As we learn a better EDNet, we can use EDNet to boost the perfor-mance of the generator by maximizing the cosine similarity between the editing embeddings obtained from EDNet and the condition embedding obtained requests.
Second, we propose an Image-Request Attention (IRA) to adaptively edit the input image in different spatial loca-tions. The IRA calculates the attention between embed-dings of linguistic requests and patches on visual feature maps. By leveraging ground truth target images that are spatial-adaptive retouched as supervision, IRA learns to as-sign an appropriate degree for each location. For example, a very light place will receive a low attention degree when the request is “brighten the image” since the real intention is likely to only brighten the dark places. Finally, we propose a new evaluation metric for language-guided image editing called Redescription Similarity Score (RSS). To calculate the RSS, we leverage a pre-trained speaker model [24] to generate requests given the input image and the generated image, then calculate commonly used sentence similarity metrics between generated requests and ground truth re-quests. Higher similarity indicates better performance.
To sum up, we make the following contributions:
• We propose the CAGAN with a newly designed cross-modal cyclic mechanism and augmentation strategy for language-guided global image editing, which miti-gates the problem of insufﬁcient and unbalanced data.
• We propose IRA that calculates the degree of editing in the spatial dimension, which produces reasonable and interpretable editing results.
• We propose a new metric (RSS) to evaluate the per-formance of editing, which uses a speaker model to redescribe the input-output image pair.
• Experiments on both GIER [22] dataset and MA5k-Req [23] dataset demonstrate the effectiveness of our method. 2.