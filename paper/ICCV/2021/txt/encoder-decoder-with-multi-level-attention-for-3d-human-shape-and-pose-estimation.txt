Abstract 3D human shape and pose estimation is the essential task for human motion analysis, which is widely used in many 3D applications. However, existing methods can-not simultaneously capture the relations at multiple lev-els, including spatial-temporal level and human joint level.
Therefore they fail to make accurate predictions in some hard scenarios when there is cluttered background, occlu-sion, or extreme pose. To this end, we propose Multi-level
Attention Encoder-Decoder Network (MAED), including a
Spatial-Temporal Encoder (STE) and a Kinematic Topol-ogy Decoder (KTD) to model multi-level attentions in a unified framework. STE consists of a series of cascaded blocks based on Multi-Head Self-Attention, and each block uses two parallel branches to learn spatial and temporal attention respectively. Meanwhile, KTD aims at model-ing the joint level attention.
It regards pose estimation as a top-down hierarchical process similar to SMPL kine-matic tree. With the training set of 3DPW, MAED outper-forms previous state-of-the-art methods by 6.2, 7.2, and 2.4 mm of PA-MPJPE on the three widely used bench-marks 3DPW, MPI-INF-3DHP, and Human3.6M respec-tively. Our code is available at https://github.com/ ziniuwan/maed. 1.

Introduction 3D human shape and pose estimation from a single im-age or video is a fundamental topic in computer vision. It is difficult to directly estimate the 3D human shape and pose from monocular images without any 3D information. To tackle this problem, massive 3D labeled data and 3D para-metric human body models [26, 30, 3] with prior knowledge are necessary. Tremendous works [16, 18, 20, 27, 21, 29]
*Equal Contribution. based on Deep Neural Network (DNN) have been made to increase the accuracy and robustness of this task.
However, existing DNN-based methods often fail in some challenging scenarios, including cluttered back-ground, occlusion and extreme pose. To overcome these challenges, three intrinsic relations should be jointly mod-eled for the video-based 3D human shape and pose estima-tion: a). Spatial relation: For the pose estimation task, the human joints areas and the spatial correlations among
It body parts are directly related to the pose prediction. is critical to carefully utilize the spatial relation, especially in the scene of cluttered background. b). Temporal rela-tion: Everyone has particular temporal trajectory in a given video. In occlusion cases, this temporal relation should be exploited to infer the pose of current occluded frame from surrounding frames. c). Human joint relation: In the para-metric 3D body model SMPL [26], human joints are orga-nized as a kinematic tree. Once pose changes, the parent joint rotates first, and then rotates the children. When the pose amplitude is large, we argue that the prior of the depen-dence among joints is especially helpful for accurate pose estimation. However, none of the existing methods fully utilizes the above three relations in a unified framework.
Motivated by the above observations, we propose Multi-level Attention Encoder-Decoder Network (MAED) for video-based 3D human shape and pose estimation. MAED is the first work to explore the above three relations by ex-ploiting corresponding multi-level attentions in a unified framework.
It includes Spatial-Temporal Encoder (STE) for spatial-temporal attention and Kinematic Topology De-coder (KTD) for human joint attention.
Specifically, the STE consists of several cascaded blocks, and each block uses two parallel branches to learn spa-tial and temporal attention respectively. We call the two branches Multi-Head Self-Attention Spatial (MSA-S) and
Multi-Head Self-Attention Temporal (MSA-T), which are inspired by Multi-Head Self-Attention (MSA) mechanism
Figure 1: (a) Spatial-temporal attention: In current frame, the color of each pixel represents the spatial attention score, visual-izing the importance of the spatial position. The color on the time axis represents the temporal attention score, visualizing the similarity between the corresponding frame and current one. Warmer color indicates higher attention score. (b) Kinematic tree-based hierarchical regression: Our model pays more attention to joints denoted by dots with warmer color. in Transformer related works [38, 10, 11, 7, 6]. Derived from MSA, MSA-S and MSA-T have Transformer-like structures, but are different in the order of input features dimensions. As illustrated in Figure 1(a), MSA-S focuses on the critical spatial positions in image, highlighting sig-nificant features for pose estimation. Meantime, MSA-T concentrates on improving the prediction of current frame by exploiting frames that are informative to current one ac-cording to the calculated temporal attention scores.
On the other hand, existing methods usually use an iter-ative feedback regressor [16, 18] to regress the SMPL [26] parameters, in which the pose parameters of all joints are generated simultaneously. However, they ignore the human joint relation. To exploit the dependence among joints, we further propose KTD to simulate the SMPL kinematic tree for joint level attention modeling.
In KTD, each joint is assigned a unique linear regressor to regress its pose pa-rameters. As shown in Figure 1(b), these parameters are generated through a top-down hierarchical regression pro-cess. To estimate a joint, besides image feature, we also take the predicted pose parameters of its ancestors as the input of linear regressor. In this manner, the bias of the par-ent joint’s estimation incurs substantial negative impact on the estimation of all its children, which forces the KTD to predict more accurate results for ancestor joints. In other words, although KTD does not explicitly allocate an atten-tion score to each joint, the top-down regression process im-plicitly encourages the model to pay more attention to the parent joints with more children. As a result, the proposed
KTD captures the inherent relation of joints and effectively reduce the prediction error.
We summarize the contributions of our method below:
• We propose Multi-level Attention Encoder-Decoder
Network (MAED) for video-based 3D human shape and pose estimation. Our proposed MAED con-tains Spatial-Temporal Encoder (STE) and Kinematic
Topology Decoder (KTD). It learns different attentions at spatial level, temporal level and human joint level in a unified framework.
• Our proposed STE leverages the MSA to construct
MSA-S and MSA-T to encode the spatial and temporal attention respectively in the given video.
• Our proposed KTD considers hierarchical dependence among human joints and implicitly captures human joint level attention. 2.