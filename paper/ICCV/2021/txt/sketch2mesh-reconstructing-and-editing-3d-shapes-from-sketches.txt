Abstract
Reconstructing 3D shape from 2D sketches has long been an open problem because the sketches only provide very sparse and ambiguous information. In this paper, we use an encoder/decoder architecture for the sketch to mesh translation. When integrated into a user interface that pro-vides camera parameters for the sketches, this enables us to leverage its latent parametrization to represent and reﬁne a 3D mesh so that its projections match the external contours outlined in the sketch. We will show that this approach is easy to deploy, robust to style changes, and effective. Fur-thermore, it can be used for shape reﬁnement given only single pen strokes.
We compare our approach to state-of-the-art methods on sketches—both hand-drawn and synthesized—and demon-strate that we outperform them. 1.

Introduction
Reconstructing 3D shapes from hand-drawn sketches has the potential to revolutionize the way designers, industrial engineers, and artists interact with Computer Aided Design (CAD) systems. Not only would it address the industrial need to digitize vast amounts of legacy models, an insur-mountable task, but it would allow practitioners to interact with shapes by drawing in 2D, which is natural to them, instead of having to sculpt 3D shapes produced by cumber-some 3D scanners.
Current deep learning approaches [26, 6, 45, 46] that regress 3D point clouds and volumetric grids from 2D sketches have shown promise despite being trained on syn-thetic data, but yield coarse 3D surface representations that are cumbersome to edit. Furthermore, they require multi-view sketches for effective reconstruction [6] or are re-stricted to a ﬁxed set of views [26].
Meanwhile Single View Reconstruction (SVR) ap-proaches have progressed rapidly thanks to the introduc-*Equal contribution (a) Reconstructing (b) Editing
Figure 1. Sketch2Mesh. We propose a pipeline for reconstruct-ing and editing 3D shapes from line drawings. We train an en-coder/decoder architecture to regress surface meshes from syn-thetic sketches. Our network learns a compact representation of 3D shapes that is suitable for downstream optimization: (a) When presented with sketches drawn in a style different from that of the training ones– for example a real drawing – aligning the projected external contours to the input sketch bridges the domain gap. (b)
The same formulation can be used to enable unexperienced users to edit reconstructed shapes via simple 2D pen strokes. Best seen in Supplemental video. tion of new shape representations [11, 33, 30, 36] along with novel architectures [43, 10, 13, 44] that exploit image-plane feature pooling to align reconstructions to input im-ages. Hence, it can seem like a natural idea to also use them for reconstruction from sketches. Unfortunately, as we will show, the sparse nature of sketch images makes it difﬁcult for state-of-the-art SVR networks relying on local feature pooling from the image plane to perform well. This difﬁ-culty is compounded by the fact that different people sketch differently, which introduces a great deal of variability in the training process and makes generalization problematic.
Furthermore, these architectures do not learn a compact rep-resentation of 3D shapes, which makes the learned models
unsuitable for down stream applications requiring a strong shape prior, such as shape editing.
To overcome these challenges, we train an en-coder/decoder architecture [36] to produce a 3D mesh es-timate given an input line drawing. This yields a com-pact latent representation that acts as an information bottle-neck. At inference time, given a previously unseen camera-calibrated sketch, we compute the corresponding latent vec-tor and reﬁne its components to make the projected 3D shape it parameterizes match the sketch as well as possible.
In effect, this compensates for the style difference between the input sketch and those that were used for training pur-poses. We propose and investigate two different ways to do this: 1. Sketch2Mesh/Render. We use a state-of-the-art image translation technique [17] trained to synthesize fore-ground/background images from sketches and then use the resulting images as targets for differentiable raster-ization [38, 36, 34]. 2. Sketch2Mesh/Chamfer. We directly optimize the posi-tion of the 3D shape’s projected contours to make them coincide with those of the input sketch by minimizing a 2D Chamfer distance.
Remarkably, Sketch2Mesh/Chamfer, even though simpler, works as well or better than Sketch2Mesh/Render. The for-mer exploits only external object contours for reﬁnement purposes, which helps with generalization because most graphics designers draw these external contours in a sim-ilar way. It also makes it unnecessary the auxiliary network that turns sketches into foreground/background images.
A further strength of Sketch2Mesh/Chamfer is that it does not require backpropagation from a full rasterized im-age but only from sparse contours. Hence, it is naturally ap-plicable for local reﬁnement given a camera-calibrated par-tial sketch. And, unlike earlier work [32, 20, 21] on shape editing from local pen strokes it allows us to take into ac-count a strong shape prior by relying on the latent vector, ensuring that shapes can be edited robustly with sparse 2D pen strokes. 2.