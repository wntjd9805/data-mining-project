Abstract
We propose to address the problem of few-shot classi-fication by meta-learning “what to observe” and “where to attend” in a relational perspective. Our method lever-ages relational patterns within and between images via self-correlational representation (SCR) and cross-correlational attention (CCA). Within each image, the SCR module trans-forms a base feature map into a self-correlation tensor and learns to extract structural patterns from the tensor.
Between the images, the CCA module computes cross-correlation between two image representations and learns to produce co-attention between them. Our Relational Em-bedding Network (RENet) combines the two relational mod-ules to learn relational embedding in an end-to-end man-ner. In experimental evaluation, it achieves consistent im-provements over state-of-the-art methods on four widely used few-shot classification benchmarks of miniImageNet, tieredImageNet, CUB-200-2011, and CIFAR-FS. 1.

Introduction
Few-shot image classification [11, 27, 53, 72] aims to learn new visual concepts from a small number of exam-ples. The task is defined to classify a given query image into target classes, each of which is unseen during train-ing and represented by only a few support images. Recent methods [1,5,19,34,48,50,63,72,80,84] tackle the problem by meta-learning a deep embedding function such that the distance between images on the embedding space conforms to their semantic distance. The learned embedding func-tion, however, often overfits to irrelevant features [4, 9, 13] and thus fails to transfer to new classes not yet observed in training. While deep neural features provide rich semantic information, it remains challenging to learn a generalizable embedding without being distracted by spurious features.
The central tenet of our approach is that relational pat-terns, i.e., meta-patterns, may generalize better than individ-ual patterns; an item obtains a meaning in comparison with other items in a system, and thus relevant information can
Figure 1: Relational embedding process and its attentional ef-fects. The base image features are transformed via self-correlation to capture structural patterns within each image and then co-attended via cross-correlation to focus on semantically relevant contents between the images. (a), (b), and (c) visualize the acti-vation maps of base features, self-correlational representation, and cross-correlational attention, respectively. See Sec. 5.6 for details. be extracted from the relational structure of items. On this basis, we propose to learn “what to observe” and “where to attend” in a relational perspective and combine them to produce relational embeddings for few-shot learning.
We achieve this goal by leveraging relational patterns within and between images via (1) self-correlational rep-resentation (SCR) and (2) cross-correlational attention (CCA). The SCR module transforms a base representation into its self-correlation tensor and learns to extract structural patterns from it. Self-correlation of a deep feature map en-codes rich semantic structures by correlating each activation of the feature map to its neighborhood. We perform repre-sentation learning on top of it to make relevant structural patterns of the image stand out (Fig. 1 (a)→(b)). On the other hand, the CCA module computes cross-correlation be-tween two image representations and learns to produce co-attention from it. Cross-correlation encodes semantic cor-respondence relations between the two images. We learn high-dimensional convolutions on the cross-correlation ten-sor to refine it via convolutional matching and produce adaptive co-attention based on semantic relations between the query and the support (Fig. 1 (b)→(c)).
The proposed method combines the two modules to learn relational embeddings in an end-to-end manner; it extracts
relational patterns within each image (via SCR), generates relational attention between the images (via CCA), and ag-gregates the cross-attended self-correlation representations to produce the embeddings for few-shot classification. Ex-periments on four standard benchmark datasets demonstrate that the proposed SCR and CCA modules are effective at highlighting the target object regions and significantly im-prove few-shot image classification accuracy. 2.