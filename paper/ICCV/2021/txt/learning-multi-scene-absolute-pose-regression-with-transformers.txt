Abstract
Absolute camera pose regressors estimate the position and orientation of a camera from the captured image alone.
Typically, a convolutional backbone with a multi-layer per-ceptron head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended for learning multiple scenes by re-placing the MLP head with a set of fully connected lay-ers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encod-ing into candidate pose predictions. This mechanism allows our model to focus on general features that are informative for localization while embedding multiple scenes in paral-lel. We evaluate our method on commonly benchmarked indoor and outdoor datasets and show that it surpasses both multi-scene and state-of-the-art single-scene absolute pose regressors. We make our code publicly available from https://github.com/yolish/multi-scene-pose-transformer. 1.

Introduction
Localizing a camera using a query image is a key task in many computer vision applications, such as indoor navi-gation, augmented reality and autonomous driving, to name a few. Contemporary approaches for estimating the posi-tion and orientation of a camera offer different trade-offs between accuracy, runtime, and memory. For example, hi-erarchical localization pipelines [26, 30, 25] achieve state-of-the-art (SOTA) pose accuracy, but are relatively slow (re-sponse time of hundreds of milliseconds) and require a large memory footprint and client-server connectivity. These ap-proaches employ image retrieval (IR) to fetch images that are similar to the query image, followed by local features extraction and matching. The extracted 2D-2D matches are mapped to 2D-3D correspondences via depth or a 3D point cloud, and are then used to estimate the camera pose with Perspective-n-Point (PnP) and RANSAC [13]. Abso-lute pose regressors (APRs), on the other hand, estimate
Figure 1: Multi-scene absolute pose regression with Trans-formers. Two Transformers separately attend to position-and orientation- informative features from a convolutional backbone. Scene-specific queries (0−3) are further encoded with aggregated activation maps into latent representations, from which a single output is selected. The strongest re-sponse, shown as an overlaid color-coded heatmap of at-tention weights, is obtained with the output associated with the input image’s scene. The selected outputs are used to regress the position x and the orientation q. the camera pose with a single forward pass, using only the query image. They are an order of magnitude faster and can be deployed as a standalone application on a thin client due to their small memory footprint. Unfortunately, APRs are also an order of magnitude less accurate compared to hierar-chical localization pipelines and to other methods utilizing 3D data at inference time [27]. Moreover, most APRs are designed to embed a single scene at a time, implying that for a dataset with N scenes (for example, a hospital with many wards and rooms), N models are required to be trained, deployed and chosen from during inference. In this work, we focus on improving the accuracy of APRs while extend-ing the current single-scene paradigm for learning multiple scenes in parallel.
The formulation of absolute camera pose regression was first suggested by Kendall et al. [17]. Following the success of convolutional neural networks (CNNs) in learning differ-ent computer vision tasks, the authors suggested to adapt
a GoogLeNet architecture to camera pose regression by at-taching a multi-layer perceptron (MLP) head to regress the camera position and orientation. The proposed architecture denoted PoseNet, offered a novel, fast and lightweight solu-tion for camera localization. However, it also suffered from low accuracy and limited generalization. Various absolute pose regression methods were suggested to address these issues, proposing modifications to the backbone and MLP architecture [20, 21, 35, 37, 29, 36, 8], as well as different loss formulations and optimization strategies [15, 16, 28].
Despite their variety, such APRs share two common traits: (1) employing a CNN backbone to output a single global la-tent vector which is used for regressing the pose (2) training a model per scene (scene-specific APRs).
Recently, Blanton et al. [3] suggested a method for extend-ing single-scene absolute pose regression to a multi-scene paradigm. Similarly to existing APRs, this method applies a CNN backbone for generating a latent global descriptor of the image. However, instead of using a single scene-specific MLP, it trains a set of Fully Connected (FC) layers, with a layer per scene, which is indexed based on the pre-dicted scene identifier. While offering a new general frame-work for optimizing a single model for multiple scenes, this method was unable to match the accuracy of contemporary
SOTA APRs.
In this work, we propose a novel formulation of multi-scene absolute pose regression, inspired by recent success-ful applications of Transformers to computer vision tasks such as object detection [9] and image recognition [11].
These works demonstrated the effectivity of encoders in focusing on latent features (in image patches or activation maps) that are informative for particular tasks, through self-In addition, decoders were shown attention aggregation. to successfully generate multiple independent predictions, corresponding to queries, based on the input embedding
[9]. Similarly, we propose to apply Transformers to multi-scene absolute pose regression, using encoders to focus on pose-informative features and decoders to transform en-coded scene identifiers to latent pose representations (Fig. 1). As pose estimation involves two different tasks (posi-tion and orientation estimation), related to different visual cues, we apply a shared convolutional backbone at two dif-ferent resolutions and use two different Transformers, one per task. The decoders’ outputs are used to classify the scene and select the respective position and orientation em-beddings, from which the position and orientation vectors are regressed.
We evaluate our approach on two commonly benchmarked datasets, consisting of multiple outdoor and indoor localiza-tion challenges. We show that our method not only provides a new SOTA accuracy for multi-scene APR localization, but importantly provides a new SOTA for single-scene APRs.
Moreover, we show that our approach achieves competitive results even when trained across multiple datasets of signifi-cantly different characteristics. We further conduct multiple ablations to evaluate the sensitivity of our model to differ-ent design choices and analyze its scalability in terms of runtime and memory. In summary, our main contributions are as follows:
• We propose a novel formulation for multi-scene abso-lute pose regression using Transformers.
• We experimentally demonstrate that self-attention al-lows aggregation of positional and rotational image cues.
• Our approach is shown to achieve new SOTA accuracy for both multi-scene and single-scene APRs across contemporary outdoor and indoor localization bench-marks. 2.