Abstract
In this work we investigate the problem of uncertainty estimation for image-guided depth completion. We extend
Deep Basis Fitting (DBF) [54] for depth completion within a Bayesian evidence framework to provide calibrated per-pixel variance. The DBF approach frames the depth com-pletion problem in terms of a network that produces a set of low-dimensional depth bases and a differentiable least squares ﬁtting module that computes the basis weights using the sparse depths. By adopting a Bayesian treatment, our
Bayesian Deep Basis Fitting (BDBF) approach is able to 1) predict high-quality uncertainty estimates and 2) enable depth completion with few or no sparse measurements. We conduct controlled experiments to compare BDBF against commonly used techniques for uncertainty estimation under various scenarios. Results show that our method produces better uncertainty estimates with accurate depth prediction. 1.

Introduction
As we seek to incorporate learned modules in safety crit-ical applications such as autonomous driving, reliable un-certainty estimation becomes as critical as prediction accu-racy [59]. Depth completion is one such task where well-calibrated uncertainty estimates can help to enable robust machine perception. Deep Convolutional Neural Networks (CNNs) are commonly used to solve structured regression problems like depth prediction due to their strong expres-sive power and inductive bias [12]. However, in its native form, a CNN only produces a point estimate, which of-fers few insights into whether or where its output should be trusted. Many probabilistic deep learning methods have been proposed to address this issue [44, 17], but they often fail to output calibrated uncertainty [23] or become suscep-tible to distributional shift [50]. Moreover, these methods can be expensive to compute due to the need for test time sampling [18] or inference over multiple models [36].
In this work, we propose a method for depth completion with uncertainty estimation that avoids the above limita-Figure 1: Qualitative results of our method, Bayesian Deep
Basis Fitting (BDBF), which outputs uncertainty estimates with depth completion. tions. Our approach builds on the idea of Deep Basis Fitting (DBF) [54]. DBF replaces the last layer of a depth comple-tion network with a set of data-dependent weights. These weights are computed by a differentiable least squares ﬁt-ting module between the penultimate features and the sparse depths. The network can also be seen as an adaptive basis function which explicitly models scene structure on a low-dimensional manifold [4, 60]. It can be used as a replace-ment to the ﬁnal layer (with no change to the rest of the network or training scheme), which greatly improves depth completion performance.
We extend DBF by formulating it within a Bayesian evi-dence framework [3]. This is done by placing a prior distri-bution on the DBF weights and marginalizing it out during inference. Such last-layer probabilistic approach have been shown to be reasonable approximations to full Bayesian
Neural Networks [34], while providing the advantage of tractable inference [48]. This is conceptually similar to
Neural Linear Models (NLMs) [58] with the notable dis-tinction that we perform Bayesian linear regression on each image as opposed to the entire dataset.
A Bayesian treatment also enables depth completion with highly sparse data. In DBF, when the number of sparse depths falls below the dimension of the bases, the underly-ing linear system becomes under-determined. We show that by learning a shared prior across images, our method is able to handle any number of sparse depth measurements.
We name our approach Bayesian Deep Basis Fitting (BDBF) and summarize its advantages: 1) It can be used as a drop-in replacement to the ﬁnal layer of many depth completion networks and outputs uncertainty estimates (in the form of per-pixel variance). 2) Compared to other un-certainty estimation techniques, it produces higher quality uncertainty with one training session, one saved model and one forward pass, without needing extra parameters or mod-iﬁcations to the loss function. 3) It can handle any spar-sity level, with performance degrading gracefully towards a pure monocular method when the number of depth mea-surements goes to zero. 2.