Abstract
Deep learning models for semantic segmentation rely on expensive, large-scale, manually annotated datasets. La-belling is a tedious process that can take hours per image.
Automatically annotating video sequences by propagating sparsely labeled frames through time is a more scalable alter-native. In this work, we propose a novel label propagation method, termed Warp-Reﬁne Propagation, that combines semantic cues with geometric cues to efﬁciently auto-label videos. Our method learns to reﬁne geometrically-warped la-bels and infuse them with learned semantic priors in a semi-supervised setting by leveraging cycle-consistency across time. We quantitatively show that our method improves label-propagation by a noteworthy margin of 13.1 mIoU on the ApolloScape dataset. Furthermore, by training with the auto-labelled frames, we achieve competitive results on three semantic-segmentation benchmarks, improving the state-of-the-art by a large margin of 1.8 and 3.61 mIoU on
NYU-V2 and KITTI, while matching the current best results on Cityscapes. 1.

Introduction
Semantic segmentation, i.e. assigning a semantic class to each pixel in an input image, is an integral task in understand-ing shapes, geometry, and interaction of components from images. The ﬁeld has enjoyed revolutionary improvements thanks to deep learning [20, 51, 34]. However, obtaining a large-scale dataset with pixel-level annotations is particularly expensive: for example, labeling takes 1.5 hours on average per image in the Cityscapes dataset [11]. Despite the re-cent introduction of datasets that are signiﬁcantly larger than their predecessors [11, 41, 10, 28], scarcity of labeled data remains a bottleneck when compared to other recognition tasks in computer vision [33, 40, 22].
In the common scenario where data is provided as videos with labels for sparsely subsampled frames, a prominent way to tackle data scarcity is label propagation (LP), which auto-matically annotates additional video frames by propagating labels through time [53, 3]. This intuitive idea to leverage
Figure 1: Quantitative comparison of different auto-labelling meth-ods for multiple propagation lengths using ApolloScape [41] frame-wise ground-truth. We compare our propagation methods, warp-reﬁne and warp-inpaint, to prior-arts semantic-only and motion-only propagation, and show that warp-reﬁne is vastly superior to other auto-labeling approaches, especially for large time-steps. motion-cues via temporal consistency in videos has been widely explored, using estimated motion [2, 12, 26], patch matching [3, 4], or predicting video frames [54]. However, as discussed in Zhu et al. [54], estimating dense motion ﬁelds across long periods of time remains notoriously difﬁcult. Fur-ther, these methods are often sensitive to hyperparameters (e.g. patch size), cannot handle de-occlusion, or require highly accurate optical ﬂow, thus limiting their applicability.
Another promising approach for obtaining large-scale an-notation in semi-supervised settings is self-training (ST), in which a teacher model, trained to capture semantic cues, is used to generate additional annotations on unlabeled images
[18, 19, 56, 55]. While there have been signiﬁcant improve-ments in ST, various challenges still remain in controlling noise in pseudo-labels, such as heuristic decisions on conﬁ-dence thresholds [35], class imbalance in pseudo-labels [13], inaccurate predictions for small segments, and misalignment of category deﬁnition between source and target domain.
*Work done while A. Ganeshan was at Preferred Networks, Inc.
To mitigate the drawbacks of LP and ST, we propose i
Figure 2: Accuracy of propagated labels. We visually compare the proposed warp-reﬁne propagation (top-center) with the motion-only model (bottom-left), the semantic-only model (bottom-right), and the ground-truth annotation (bottom-center). The motion-only model (i) fails to correctly classify the new regions introduced in the target frame , and (ii) often suffers from drifting . In contrast, the semantic-only model (iii) tends to fail for far-away segments, and (iv) cannot handle misaligned class deﬁnitions between the teacher and the student model (e.g. ignore label) . Our method effectively combines the strengths of both of these approaches to overcome their respective limitations. For details of motion-only and semantic-only models, see Section 4.4.
Warp-Reﬁne Propagation (referred to as warp-reﬁne), a novel method to automatically generate dense pixel-level labels for raw video frames. Our method is built on two key insights: (i) By combining motion cues with semantic cues, we can overcome the respective limitations of LP and ST, and (ii) By leveraging cycle-consistency across time, we can learn to combine these two complementary cues in a semi-supervised setting without sequentially-annotated videos.
Speciﬁcally, our method ﬁrst constructs an initial esti-mate by directly combining labels generated via motion cues and semantic cues. This initial estimate, containing erro-neous conﬂict resolution and faulty merges, is then rectiﬁed by a separate reﬁnement network. The reﬁnement network is trained in a semi-supervised setting via a novel cycle-consistency loss. This loss compares the ground-truth labels with their cyclically propagated version created by propagat-ing the labels forward-and-backward through time in a cyclic loop (t → t + k → t). Our loss is built on the observation that as our auto-labeling method is bi-directional, it can be used to generate different versions of each annotated frame.
Once this network is trained, it is used to correct errors caused by propagation of variable length. In Fig. 2 we show a qualitative comparison of our method against prior-arts, demonstrating drastic improvements in label quality.
With quantitative analysis on a large scale autonomous driving datasets (ApolloScape [41]), we concretely establish the superior accuracy of our method against previous state-of-the-art auto-labeling methods. Such an analysis of different methods has been starkly missing from prior works [54, 35, 26]. As shown in Fig. 1, we observe that warp-reﬁne accurately propagates labels for signiﬁcantly longer time intervals, with a notable average 13.1 mIoU improvement on ApolloScape compared to previous work. Further, it accurately labels rare-classes such as ‘Bicycle’ and thin-structures such as ‘Poles’ (cf. Section 4.3). As a result, by training single-frame semantic segmentation models with the additional data labeled by our method, we achieve state-of-the-art performance on KITTI [1], NYU-V2 [27] and
Cityscapes [11] benchmarks (cf. Section 4.5).
In summary, our main contributions are: 1) A novel algo-rithm, termed Warp-Reﬁne Propagation, that produces sig-niﬁcantly more accurate pseudo-labels, especially for frames distant in time; 2) A novel loss function, based on the cycle-consistency of learned transformations, to train our method in a semi-supervised setting; and 3) A quantitative analysis on the quality and utility of different auto-labeling methods on multiple diverse datasets. To the best of our knowledge, our work is the ﬁrst to utilize both semantic and geometric understanding for the task of video auto-labeling. 2.