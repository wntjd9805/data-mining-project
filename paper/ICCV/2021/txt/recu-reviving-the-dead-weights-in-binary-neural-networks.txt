Abstract
Binary neural networks (BNNs) have received increas-ing attention due to their superior reductions of compu-tation and memory. Most existing works focus on either lessening the quantization error by minimizing the gap be-tween the full-precision weights and their binarization or designing a gradient approximation to mitigate the gradi-ent mismatch, while leaving the “dead weights” untouched.
This leads to slow convergence when training BNNs. In this paper, for the ﬁrst time, we explore the inﬂuence of “dead weights” which refer to a group of weights that are barely updated during the training of BNNs, and then introduce rectiﬁed clamp unit (ReCU) to revive the “dead weights” for updating. We prove that reviving the “dead weights” by ReCU can result in a smaller quantization error. Be-sides, we also take into account the information entropy of the weights, and then mathematically analyze why the weight standardization can beneﬁt BNNs. We demonstrate the inherent contradiction between minimizing the quanti-zation error and maximizing the information entropy, and then propose an adaptive exponential scheduler to identify the range of the “dead weights”. By considering the “dead weights”, our method offers not only faster BNN training, but also state-of-the-art performance on CIFAR-10 and Im-ageNet, compared with recent methods. Code can be avail-able at https://github.com/z-hXu/ReCU . 1.

Introduction
Deep Neural Networks (DNNs) have shown tremendous success and advanced many visual tasks [29, 44, 16, 47].
Nevertheless, this comes at the price of massive memory
*Corresponding author: rrji@xmu.edu.cn
Figure 1: Illustration of the quantization error (a) and the gradient mismatch (b). usage and computational burden, which poses a great chal-lenge to the resource-constrained cutting-edge devices such as mobile phones and embedded devices. The commu-nity has proposed various approaches to solve this prob-lem. Typical techniques include, but are not limited to, ef-ﬁcient architecture design [20, 25, 40], knowledge distilla-tion [24, 27, 45], network pruning [15, 32, 36], and network quantization [53, 54, 50, 1].
Among them, by converting the full-precision parame-ters and activations into low-bit forms, network quantiza-tion has offered a promising solution to yield a light and ef-ﬁcient version of DNNs [26, 17, 51, 49]. In the extreme case of a 1-bit representation, a binary neural network (BNN) restricts the weights and activations to only two possible values, i.e., -1 and +1. In comparison with the original net-works, BNNs show overwhelming superiority in reducing the model complexity by around 32× parameter compres-sion, and 58× speedup, using the efﬁcient XNOR and bit-count operations [8].
Despite the superiority of BNNs in memory saving and computation reduction, they suffer a drastic drop in accu-racy compared with their real-valued counterparts [43, 13, 12], which greatly limits the practical deployment. There are two main reasons for the performance degradation: large quantization error in the forward propagation and gra-dient mismatch during backpropagation.
stead of simply minimizing the quantization error, we con-sider the information entropy of the weights to increase the weight diversity of BNNs. For the ﬁrst time, a systemati-cal analysis is derived to explain why the weight standard-ization [41] can boost the performance of BNNs, and then a generalized weight standardization is proposed to further increase the information entropy. Combining the informa-tion entropy and the quantization error, we reveal the inher-ent contradiction between maximizing the former and min-imizing the latter, and then propose an adaptive exponen-tial scheduler to identify the range of the “dead weights” and balance the information entropy of the weights and the quantization error.
We conduct extensive experiments for binarizing net-works including ResNet-18/20 [21] and VGG-small [51] on
CIFAR-10 [28], and ResNet-18/34 [21] on ImageNet [46].
The experimental results show that ReCU achieves state-of-the-art performance, as well as faster training convergence even with the simple STE [4] as our weight gradient approx-imation.
To sum up, this paper makes the following contributions:
• We explore the inﬂuence of “dead weights” show-ing that they can adversely affect the optimization of
BNNs. To the best of our knowledge, this is the ﬁrst work to analyze the “dead weights” in BNNs.
• We introduce a rectiﬁed clamp unit (ReCU) to revive the “dead weights” and then provide a rigorous math-ematical proof that a smaller quantization error can be derived using our ReCU.
• A mathematical analysis on why the weight standard-ization helps boost BNNs is provided, and the inherent contradiction between minimizing the quantization er-ror and maximizing the information entropy in BNNs is revealed.
• Extensive experiments demonstrate that ReCU not only leads to better performance over many state-of-the-arts [14, 41, 33, 53, 17, 50, 43, 11, 48, 38, 18, 19, 10, 34], but also results in faster training convergence. 2.