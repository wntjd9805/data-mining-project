Abstract
While most neural video codecs address P-frame coding (predicting each frame from past ones), in this paper we address B-frame compression (predicting frames using both past and future reference frames). Our B-frame solution is based on the existing P-frame methods. As a result, B-frame coding capability can easily be added to an existing neural codec. The basic idea of our B-frame coding method is to interpolate the two reference frames to generate a single reference frame and then use it together with an existing P-frame codec to encode the input B-frame. Our studies show that the interpolated frame is a much better reference for the P-frame codec compared to using the previous frame as is usually done. Our results show that using the proposed method with an existing P-frame codec can lead to 28.5% saving in bit-rate on the UVG dataset compared to the P-frame codec while generating the same video quality. 1.

Introduction
There are two types of frames in the video cod-ing domain, Intra-frames and Inter-frames.
Intra-frames (I-frames) are encoded/decoded independently of other frames.
I-frame coding is equivalent of image compres-sion. Inter-frames are encoded using motion compensation followed by residuals i.e. a prediction of an input frame is initially devised by moving pixels or blocks of one or mul-tiple reference frames and then the prediction is corrected using residuals. Prediction is an essential task in inter-coding, for it is the primary way in which temporal redun-dancy is exploited. In the traditional paradigm of video cod-ing [34, 41], motion vectors are used to model the motion of blocks of pixels between a reference and an input im-age [34]. In the neural video coding domain, dense optical flow is usually used to model individual pixels movements.
In both cases, a warping is performed on references using motion vectors or optical flow to generate the prediction.
*Qualcomm AI Research is an initiative of Qualcomm Technologies,
Inc.
Figure 1. (a) the general idea of this work i.e. extending an ex-isting P-frame codec to a B-frame codec by adding an interpo-lation block, (b) the rate-distortion improvements on the UVG dataset [38] where the P-frame [2] and the B-frame codecs are trained on the Vimeo-90k dataset [46] for the same number of iter-ations. The improvement is equivalent of 28.5% saving in bit-rate measured by BD-rate gain [6].
Inter-frames are further divided into Predicted (P) frames and Bi-directional predicted (B) frames. P-frame coding, which is suitable for low-latency applications such as video conferencing, uses only past decoded frames as references to generate a prediction. Most of the available literature on neural inter coding falls under this category and the meth-ods often use a single past decoded frame as reference [23] (see Fig. 2.b). On the other hand, B-frame coding, which is suitable for applications such as on-demand video stream-ing, uses both past and future decoded frames as references.
Future references provide rich motion information that fa-cilitate frame prediction and eventually lead to better cod-ing efficiency. The number of neural video codecs that ad-dress B-frame coding is limited [11, 13, 16, 43]. They use two references and generate a prediction either by bidirec-tional optical flow estimation and warping or by performing frame interpolation. The reported results show that these approaches, despite relative success in video coding, do not fully exploit the motion information provided by two refer-ences as the results are not competitive with state-of-the-art
P-frame codecs [2].
For a given input frame, when references from both past and future are available, under a linear motion assump-tion, one can come up with a rough prediction of the in-put frame by linearly interpolating the two references. This
Figure 2. Prediction in inter-frame coding. xt, (cid:101)xt, and (cid:98)xref denote an input frame, the corresponding prediction and the reference, respec-tively. (a) Actual object location. (b) P-Frame prediction, a motion vector with respect to a single reference is transmitted. (c) B-frame prediction based on bidirectional flow/warp, two motion vectors with respect to two references are transmitted. (d) B-frame prediction based on frame interpolation, the interpolation result is treated as the prediction. No motion information is transmitted. (e) Our B-frame prediction approach, the interpolation result is corrected using a unidirectional motion vector similar to P-frame. prediction does not need to be coded since the two ref-erences are already available to the receiver. The neu-ral B-frame coding methods that work based on bidirec-tional flow/warping [13], do not use this useful informa-tion and send the optical flows with respect to both refer-ences (see Fig. 2.c). On the other hand, the interpolation outcome is only accurate under linear motion assumption.
So in the neural B-frame models that rely on frame inter-polation [11, 43], the prediction is likely to not exactly be aligned with the input frame (see Fig. 2.d). Even when a non-linear frame interpolator is employed [45], misalign-ment could still occur. In these situations, the codec solely relies on residuals to compensate for the misalignment. As a result, coding efficiency could be significantly lower com-pared to a scenario where the misalignment is mitigated via some inexpensive side-information first before applying residual coding.
In this work, we address this issue by introducing a new approach for neural B-frame coding, which despite its sim-plicity, is proven very effective. The method involves inter-polating two reference frames to obtain a single reference frame, which is then used by a P-frame model to predict the current frame (see Fig. 1 and Fig. 2.e). A residual is applied to this prediction.
Our method takes advantage of the rich motion informa-tion available to the receiver by performing frame interpo-lation and does not suffer from the residual penalty due to misalignment. Since our B-frame coding solution operates based on a P-frame codec, an existing P-frame codec can be used to code B-frames. In fact, the same network can learn to do both P-frame coding as well as contributing to
B-frame compression. In other words, by adding a frame in-terpolator to a P-frame codec, the codec is able to code both
P-frames and B-frames. One can freely choose an existing interpolation and P-frame method when implementing our technique.
In video coding, videos are split into groups of pictures (GoP) for coding. The neural video codec that we develop in this work B-EPIC (B-Frame compression through Ex-tended P-frame & Interpolation Codec) supports all frame types. Given that different frame types yield different cod-ing efficiencies, it is crucial to choose the right frame type for the individual frames in a GoP. In this work, we look closely into GoP structure.
Our main contributions and findings are as follows:
• We introduce a novel B-frame coding approach based on existing P-frame codecs and frame interpolation,
• A single P-frame network is used for both P-frame and
B-frame coding through weight-sharing,
• A thorough analysis of the effect of GoP structure on per-formance is provided,
• The proposed solution outperforms existing neural video codecs by a significant margin and achieves new state-of-the-art results. 2.