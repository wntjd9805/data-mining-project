Abstract
Event cameras can report scene movements as an asyn-chronous stream of data called the events. Unlike tradi-tional cameras, event cameras have very low latency (mi-croseconds vs milliseconds) very high dynamic range (140 dB vs 60 dB), and low power consumption, as they report changes of a scene and not a complete frame. As they re-port per pixel feature-like events and not the whole inten-sity frame they are immune to motion blur. However, event cameras require movement between the scene and camera to ﬁre events, i.e., they have no output when the scene is relatively static. Traditional cameras, however, report the whole frame of pixels at once in ﬁxed intervals but have lower dynamic range and are prone to motion blur in case of rapid movements. We get the best from both worlds and use events and intensity images together in our complemen-tary design and estimate dense disparity from this combina-tion. The proposed end-to-end design combines events and images in a sequential manner and correlates them to esti-mate dense depth values. Our various experimental settings in real-world and simulated scenarios exploit the superior-ity of our method in predicting accurate depth values with
ﬁne details. We further extend our method to extreme cases of missing the left or right event or stereo pair and also in-vestigate stereo depth estimation with inconsistent dynamic ranges or event thresholds on the left and right pairs. 1.

Introduction
Stereo depth estimation is inspired from the human binocular vision. Estimating the depth from two or more views is one of the long-persisting topics that is tackled in many ways [24]. Early-stage stereo depth estimation meth-ods consider matching all the pixels in a pair of stereo im-ages to estimate the underlying 3D geometry of the scene.
The camera parameters and the stereo setup are mainly available through calibration, and the task is to triangulate the matched pairs to recover the disparity or depth [33].
Stereo matching is still challenging because of the ill-posed
§: now at Lunit Inc. (lunit.io). †: corresponding author.
Figure 1. Estimating dense depth using our event-intensity stereo depth estimation framework. Our end-to-end network can esti-mate depth from the combination of Event-Intensity Stereo (b),
Intensity-only stereo (c), or Event-only stereo (d) pairs. Using event-intensity stereo, we can reach higher quality depth in com-parison to event-only or intensity-only inputs, as it can surpass the shortcomings of each source while gathering the best from them. nature of the problem, occlusions, imperfect imaging set-tings, blurred or low dynamic range images, repetitive pat-terns, and texture-less regions [24]. Recent methods es-timate depth using learning-based frameworks without re-lying on hand-crafted parameters, and can also estimate metric depth based on the prior knowledge of the network
[4, 5, 20, 41], thanks to the modern GPUs, creative archi-tectures, and public-available large scale datasets.
In spite of the signiﬁcant progress, poor lighting condi-tions and complex materials properties are issues that have been less studied [24]. Infusing new sensing devices to en-rich the input media is a direction worth researching. To this end we investigate in event-intensity cameras as a comple-mentary source to enrich details captured from the scene.
Event cameras are new vision sensors that report changes of intensity individually per pixel, and asynchronous to other pixels, at the time of such changes. The output of an event camera, the events, are reported as a stream that varies based on the movement speed and direction of the
camera and the scene. Event cameras are intrinsically im-mune to motion blur, thus an ideal candidate for tasks in-volving rapid movements such as driving scenes. These cameras cover a much higher dynamic range (HDR) when compared to traditional intensity cameras, making them ap-plicable for extreme lighting conditions.
Events are mainly ﬁred at object edges, as intensity changes usually happen on edges, making event cameras an ideal tool for estimating sharp depth values on such bound-aries. However, event cameras do not directly report in-tensity values, as they only sense the changes in intensity.
Event cameras remain silent when the scene is static, e.g., when we stop behind a trafﬁc junction in which parts of the scene become invisible to an event camera. Other sample scenarios that contrast the pros and cons of using event or intensity cameras are presented in Fig. 2.
Considering the pros and cons of traditional and event cameras, we investigate in ﬁnding a way to go beyond the trade-off between events and images and utilize the bene-ﬁts of these widely differing sensors at the same time. In our settings, we use a stereo pair of event-intensity sensors to estimate depth in different movement and lighting condi-tions. We propose a network that uniﬁes events and inten-sity images through a recycling unit and applies deformable aggregations and multiscale reﬁnements to estimate precise depth. Our system can work with all combination of avail-able sensors so it is robust to failures of either modality.
To our knowledge, we are the ﬁrst to investigate in the combination of events and intensity images to estimate dense stereo depth as in Fig. 1. We show the practical ad-vantages of this combination in estimating depth by con-trasting with event-only and image-only stereo depth esti-mation methods on synthetic and real-world datasets. 2. Preliminary
An event camera reports the scene as a tuple of: x and y locations, timestamp of the change in intensity (t) and sign of the change (σ) which indicates whether the sensed inten-sity is higher (positive event) or lower (negative event) than a predeﬁned intensity threshold (τ ). This stream of asyn-chronous events can reach near microseconds resolution of latency making it suitable for fast movement scenes.
As each pixel location of the event stream only holds the timestamp and sign information, an edge-like representa-tion is produced when visualizing the event stream in short periods of time. Unlike ordinary cameras, event cameras report changes of a scene instead of the whole frame, thus, they require lower storage, bandwidth, and power. Recent event sensors report the event stream and intensity image, the active pixel sensor (APS), on a single device. They share a common grid-line of pixels for the events and intensity images, making them free from requiring further transfor-mations when matching events to the image locations. (a) Abs. movement (b) No movement (c) Motion blur (d) LDR
Figure 2. Expressing the pros (green) and cons (red) of intensity cameras (top) and event cameras (bot) on different scenes. Unlike the intensity camera, the event camera cannot capture scenes when absolute changes are zero such as two cars moving with the same speed ((a) Abs. movement), or a car stopped at a junction ((b) No movement). Unlike the event camera, the intensity camera creates blurred edges when the camera or objects are moving ((c) Motion blur), and has a lower dynamic range ((d) LDR). The data belong to the MVSEC dataset [49] as explained in sec. 5.1. 3.