Abstract
Multi-view projection methods have demonstrated their ability to reach state-of-the-art performance on 3D shape recognition. Those methods learn different ways to aggre-gate information from multiple views. However, the camera view-points for those views tend to be heuristically set and
ﬁxed for all shapes. To circumvent the lack of dynamism of current multi-view methods, we propose to learn those view-points. In particular, we introduce the Multi-View Transfor-mation Network (MVTN) that regresses optimal view-points for 3D shape recognition, building upon advances in dif-ferentiable rendering. As a result, MVTN can be trained end-to-end along with any multi-view network for 3D shape classiﬁcation. We integrate MVTN in a novel adaptive multi-view pipeline that can render either 3D meshes or point clouds. MVTN exhibits clear performance gains in the tasks of 3D shape classiﬁcation and 3D shape retrieval with-out the need for extra training supervision. In these tasks,
MVTN achieves state-of-the-art performance on ModelNet40,
ShapeNet Core55, and the most recent and realistic ScanOb-jectNN dataset (up to 6% improvement).
Interestingly, we also show that MVTN can provide network robustness against rotation and occlusion in the 3D domain. The code is available at https://github.com/ajhamdi/MVTN . 1.

Introduction
Given its success in the 2D realm, deep learning naturally expanded to the 3D vision domain. In 3D, deep networks achieve impressive results in classiﬁcation, segmentation, and detection. 3D deep learning pipelines operate directly on 3D data, commonly represented as point clouds [55, 57, 66], meshes [18, 29], or voxels [52, 13, 24]. However, other methods choose to represent 3D information by rendering multiple 2D views of objects or scenes [61]. Such multi-view methods are more similar to a human approach, where the human visual system is fed with streams of rendered images instead of more elaborate 3D representations.
Recent developments in multi-view methods show im-Figure 1. Multi-View Transformation Network (MVTN). We propose a differentiable module that predicts the best view-points for a task-speciﬁc multi-view network. MVTN is trained jointly with this network without any extra training supervision, while improving the performance on 3D classiﬁcation and shape retrieval. pressive performance, and in many instances, achieve state-of-the-art results in 3D shape classiﬁcation and segmentation
[38, 67, 41, 37, 15]. Multi-view approaches bridge the gap between 2D and 3D learning by solving a 3D task using 2D convolutional architectures. These methods render several views for a given 3D shape and leverage the rendered im-ages to solve the end task. As a result, they build upon the recent advances in 2D grid-based deep learning and leverage larger image datasets for pre-training (e.g. ImageNet [59]) to compensate for the general scarcity of labeled 3D datasets.
However, the manner of choosing the rendering view-points for such methods remains mostly unexplored. Current meth-ods rely on heuristics like random sampling in scenes [41] or predeﬁned canonical view-points in oriented datasets [67].
There is no evidence suggesting that such heuristics are em-pirically the best choice. To address this shortcoming, we propose to learn better view-points by introducing a Multi-View Transformation Network (MVTN). As shown in Fig. 1,
MVTN learns to regress view-points, renders those views with a differentiable renderer, and trains the downstream task-speciﬁc network in an end-to-end fashion, thus leading to the most suitable views for the task. MVTN is inspired
by the Spatial Transformer Network (STN) [32], which was developed for the 2D image domain. Both MVTN and STN learn spatial transformations for the input without leveraging any extra supervision nor adjusting the learning process.
The paradigm of perception by predicting the best envi-ronment parameters that generated the image is called Vision as Inverse Graphics (VIG) [25, 40, 70, 36, 77]. One approach to VIG is to make the rendering process invertible or dif-ferentiable [51, 39, 47, 12, 45]. In this paper, MVTN takes advantage of differentiable rendering [39, 47, 58]. With such a renderer, models can be trained end-to-end for a spe-ciﬁc target 3D vision task, with the view-points (i.e. camera poses) being inferred by MVTN in the same forward pass.
To the best of our knowledge, we are the ﬁrst to integrate a learnable approach to view-point prediction in multi-view methods by using a differentiable renderer and establishing an end-to-end pipeline that works for both mesh and 3D point cloud classiﬁcation and retrieval.
Contributions: (i) We propose a Multi-View Transforma-tion Network (MVTN) that regresses better view-points for multi-view methods. Our MVTN leverages a differentiable renderer that enables end-to-end training for 3D shape recog-nition tasks. (ii) Combining MVTN with multi-view ap-proaches leads to state-of-the-art results in 3D classiﬁcation and shape retrieval on standard benchmarks ModelNet40
[71], ShapeNet Core55 [7, 60], and ScanObjectNN [64]. (iii) Additional analysis shows that MVTN improves the ro-bustness of multi-view approaches to rotation and occlusion, making MVTN more practical for realistic scenarios, where 3D models are not perfectly aligned or partially cropped. 2.