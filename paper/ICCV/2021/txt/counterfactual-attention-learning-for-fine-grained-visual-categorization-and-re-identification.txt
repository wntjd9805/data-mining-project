Abstract
Attention mechanism has demonstrated great potential in fine-grained visual recognition tasks. In this paper, we present a counterfactual attention learning method to learn more effective attention based on causal inference. Unlike most existing methods that learn visual attention based on conventional likelihood, we propose to learn the attention with counterfactual causality, which provides a tool to mea-sure the attention quality and a powerful supervisory signal to guide the learning process. Specifically, we analyze the effect of the learned visual attention on network prediction through counterfactual intervention and maximize the effect to encourage the network to learn more useful attention for fine-grained image recognition. Empirically, we evaluate our method on a wide range of fine-grained recognition tasks where attention plays a crucial role, including fine-grained image categorization, person re-identification, and vehicle re-identification. The consistent improvement on all bench-marks demonstrates the effectiveness of our method. 1 1.

Introduction
Attention is one of the most fundamental mechanism of human visual perception. When facing a complex scene, humans are able to select regions of interest, and employ at-tention to narrow down the search and speed up recognition.
Many efforts [57, 65, 50, 18, 45, 14, 1, 6, 36] have been made to model the mechanism of human attention in computer vision systems, which aim to facilitate high-performance recognition by discovering discriminative regions and miti-gating the negative effects brought by diverse visual appear-ance, cluttered backgrounds, occlusions, pose variations, etc.
Since subtle differences are key to distinguish subordinate
â€ Corresponding author.
*Equal contribution. 1Code is available at https://github.com/raoyongming/
CAL
Figure 1: Attention visualization on CUB. We respectively show the original images, baseline attention maps, and attention maps with counterfactual learning. In the left part, we observe that our attention maps can better focus on the object. While comparisons in the right part show that our models prefer to look at the whole object instead of some parts. Best in color. visual categories, visual attention mechanism has proven to be especially effective in fine-grained visual recognition tasks and become a core component in many state-of-the-art methods [47, 31, 7, 65, 50, 19, 28, 25, 62, 44].
Despite the widespread use, the problem of how to learn effective attention is still barely studied. Most existing meth-ods learn the visual attention in a weakly-supervised manner, i.e., the attention modules are simply supervised by the fi-nal loss function, without a powerful supervisory signal to guide the training process. This likelihood based approach only explicitly supervises the final prediction (e.g., class probabilities for classification task) but ignores the causality between the prediction and attention. Previous methods also did not teach the machine how to distinguish between the main clues and biased clues. For example, if most training samples of one specific class appear with sky as background, then the attention model may be very likely to treat the sky as a discriminative region. Although these biased clues may also be beneficial to the classification on the current datasets, the attention model should only focus on the discriminative patterns, i.e. the main clues. Besides, directly learning from data may encourage the model to only focus on some cer-tain attributes of the objects instead of all attributes, which may limit the generalization ability on test set. Therefore, we argue that this attention learning scheme is sub-optimal, where the effectiveness of the learned attentions is not always guaranteed, and the attention may lack discriminative power, clear meaning and robustness. As shown in Figure 1, mis-leading and scattered attentions can still be observed from a well-trained attention model and potentially lead to the wrong predictions. To better understand this phenomenon, we analyze the statistics of both intrinsic attributes and exter-nal environments on the CUB dataset (see Figure 2), where we use the attributes provided by the dataset and manually collect the environment statistics. We see there are biases for both attributes and environment, which indicates either background and single part are not reliable clues for clas-sification. Therefore, it is desired to design new attention learning method beyond conventional likelihood maximiza-tion to mitigate the effects of data biases.
Because of the lack of effective tool to evaluate the quality of attentions quantitatively, correcting misleading attentions is a very challenging task. One straightforward solution is to use extra annotations like bounding boxes or segmentation masks to obtain the regions of interest explicitly such as [1].
However, this kind of method requires considerable cost of human labor and is hard to scale up. Considering the critical role that attention plays in fine-grained visual recognition tasks, it is necessary to design a method to measure the quality of attentions without additional human supervision and further optimize the learned visual attentions.
In this paper, we present a counterfactual attention learn-ing (CAL) method to enhance attention learning based on causal inference. Specifically, we design a tool to analyze the effects of learned visual attention with counterfactual causality. The basic idea is to quantitate the quality of at-tentions by comparing the effects of facts (i.e., the learned attentions) and the counterfactuals (i.e., uncorrected atten-tions) on the final prediction (i.e., the classification score).
Then, we propose to maximize the difference (i.e., effect in causal inference literature [41, 54]) to encourage the net-work to learn more effective visual attentions and reduce the effects of biased training set.
The proposed method is model-agonistic and thus can serve as a plug-and-play module to improve a wide range of visual attention models. Our method is also computa-tional efficient, which only introduces a little extra computa-tion cost during training and brings no computation during inference while can significantly improve attention mod-els. We evaluate our method on three fine-grained visual recognition tasks including fine-grained image categoriza-tion (CUB200-2011 [55], Stanford Cars [23] and FGVC
Aircraft [38]), person re-identification (Market1501 [66],
DukeMTMC-ReID [46] and MSMT17 [60]) and vehicle re-identification (Veri-776 [30] and VehicleID [29]). By ap-Figure 2: The biases of both intrinsic attributes and external envi-ronments on CUB. We demonstrate the biases on the training and testing sets by the statistics the frequencies in different attributes and environments, taking the Ringed Kingfisher as an example. plying our method to a multi-head attention baseline model, we demonstrate our method significantly improves the base-line and achieve state-of-the-art results on all benchmarks. 2.