Abstract
To fully leverage the data captured from different scenes with different view angles while reducing the annotation cost, this paper studies a novel crowd counting setting, i.e. only using partial annotations in each image as training data. Inspired by the repetitive patterns in the annotated and unannotated regions as well as the ones between them, we design a network with three components to tackle those unannotated regions: i) in an Unannotated Regions Char-acterization (URC) module, we employ a memory bank to only store the annotated features, which could help the vi-sual features extracted from these annotated regions ﬂow to these unannotated regions; ii) For each image, Feature
Distribution Consistency (FDC) regularizes the feature dis-tributions of annotated head and unannotated head regions to be consistent; iii) a Cross-regressor Consistency Regu-larization (CCR) module is designed to learn the visual fea-tures of unannotated regions in a self-supervised style. The experimental results validate the effectiveness of our pro-posed model under the partial annotation setting for sev-eral datasets, such as ShanghaiTech, UCF-CC-50, UCF-QNRF, NWPU-Crowd and JHU-CROWD++. With only 10% annotated regions in each image, our proposed model achieves better performance than the recent methods and baselines under semi-supervised or active learning settings on all datasets. The code is https://github.com/ svip-lab/CrwodCountingPAL. 1.

Introduction
The crowd counting task aims to estimate the total num-ber of persons in static images or dynamic videos. The re-cent data-driven models have achieved satisfactory results on crowd counting due to the success of CNN [10], but they
∗: Equal Contribution. †: Corresponding author.
Figure 1. Crowd counting with different supervisions. still require large amounts of annotated data. For instance, the annotators need to label the positions of all heads with points to overcome the various challenging scenes, such as the lighting, camera view, occlusion and various head poses in Fig. 1 (a). Such a labeling strategy is an extremely labor-intensive task, e.g., the total annotation cost is 3,000 human hours in labeling the NWPU-Crowd dataset [42].
Naturally, a key question arises. Can the designed model still produce the competitive performance but use as few annotations as possible? One of the potential directions is to use a part of the dataset in full annotations, as shown in Fig. 1 (b), under the semi-supervised learning (SSL) [21, 34] or active learning [47] strategy. Although these strategies could reduce the number of annotated training images, we still need to fully annotate the images.
It might result in limited challenging scenes, limited viewing angles of the camera as well as limited lighting conditions, which might degrade the model’s generalization ability in the test stage.
We notice that in one image, the person’s head poses are usually the same or similar and the lighting conditions and the viewing angles are consistent. It might be redundant to annotate all the person heads in one image. Therefore, to fully leverage the data captured from different scenes with different view angles while reducing the annotation cost, we propose a novel crowd counting setting, named Partial
Figure 2. An Illustration of the repetitive pattern. The region within the red bounding box is the partially annotated region. The ∆Density map shows the density δ between the red point and the rest regions, and the ∆F eature map represents the feature distance δ between the red point and the rest regions. The blue and orange curves in (f) represent the average minimum distance distribution of the features extracted from the CSRNet (fully-annotated data) and our model (partially-annotated data). The red point is the example used in (a). They indicate that the similar or repetitive patterns (density or feature) occur not only in the annotated regions (within the red bounding box) or unannotated regions (without the red bounding box), but also between the annotated and unannotated regions in almost all images.
Annotation Learning. Different from those attempts that fully annotate a few training images, our proposed partial annotation learning only partially (e.g. 10%) annotates a patch in each training image. In particular, each image con-sists of both 10% annotated regions and 90% unannotated regions in Fig. 1 (c). We do it on the whole training images.
One of the main challenges of partial annotation learning is how to leverage the many unannotated regions for learn-ing a good visual representation in each image since the
CNN-based methods could extract useful and efﬁcient fea-tures for the annotated regions. We observe that the image textures are usually consistent or the same, such as the per-son’s head poses, lighting conditions, and the viewing an-gles, just like the Fig. 2 (a). Further, to investigate whether there exist such repetitive patterns on feature space, we cal-culate the distance between the red point and the rest regions in density map space in Fig. 2 (c) and feature space in Fig. 2 (d). At the global level, there is a similar even the same dis-tribution on the deep blue regions, the most crowded region, in both Fig. 2 (c) and (d). Further, we calculate the average minimum distance in each image between each position in the labelled region and the positions in unlabelled region in ground truth density map space in Fig. 2 (e) and feature space in Fig. 2 (f). The blue and orange curves represent the average minimum distance distribution of the features extracted from the CSRNet (fully-annotated data) and our model (partially-annotated data). The red point is the ex-ample used in Fig. 2 (a). The almost ﬂat blue curve shows the repetitive feature patterns occur not only in the unanno-tated or annotated regions but also between the unannotated and annotated regions in almost all images. Thus, it shows the consistency assumption is reliable for almost all images.
Looking back at the main challenge of how to extract the useful features from unannotated regions, we design the fol-lowing modules at the local and global level to make full use of the repetitive patterns. Firstly, we employ the memory bank idea to store the repetitive feature patterns extracted from the annotated regions in the whole dataset in con-trolled storage size. Then the features of the unannotated re-gions could ﬁnd their nearest counterpart in the memory for the image representation. Consequently, the memory bank could help the information of the annotated regions ﬂow to the unannotated regions. If only considering each feature vector at the local level, it might not be similar or repetitive for the features extracted from annotated and unannotated regions. Besides, since the background consists of unlim-ited patterns and objects, such as the building or sky, we only consider the consistency of the feature distribution of the person’s head regions. Thus, we design a Feature Distri-bution Consistency regularizer to regularize the features ex-tracted from unannotated head regions have similar feature distribution with those extracted from the annotated head regions. In particular, we ﬁrstly forward the network with-out backward gradient to get the predicted density map as an attention map, which could roughly distinguish the head regions and background regions.
Motivated by previous work [20], we propose to utilize a Cross-regressor Consistency Regularization to learn the visual representations for both annotated and unannotated regions in a self-supervised style. The proposed model in-cludes two branches to estimate density maps generated by
Gaussian bandwidth with different sigma. It uses the con-sistency of crowd numbers between two different estimated density maps within the same image.
The contributions of this work are summarized as fol-lows: (1) To reduce the annotation cost and produce the
competitive performance, we propose a novel crowd count-ing setting, named partial annotation learning, that only an-notates a patch of each training image. (2) Inspired by the repetitive patterns, we design an Unannotated Regions
Characterization at the local level and Feature Distribution
Consistency regularizer at the global level to leverage the unannotated regions for visual representation. (3) Based on the consistency of crowd numbers, we also design a
Cross-regressor Consistency Regularization to learn the vi-sual representations in a self-supervised style. (4) The ex-perimental results demonstrate the effectiveness of our pro-posed model. With only 10% annotated regions in each im-age, our proposed model achieves better performance than the recent methods and baselines under semi-supervised or active learning settings on all datasets. 2.