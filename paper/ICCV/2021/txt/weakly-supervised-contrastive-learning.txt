Abstract
Unsupervised visual representation learning has gained much attention from the computer vision community be-cause of the recent achievement of contrastive learning.
Most of the existing contrastive learning frameworks adopt the instance discrimination as the pretext task, which treat-ing every single instance as a different class. However, such method will inevitably cause class collision problems, which hurts the quality of the learned representation. Mo-tivated by this observation, we introduced a weakly super-vised contrastive learning framework (WCL) to tackle this issue. Specifically, our proposed framework is based on two projection heads, one of which will perform the regular in-stance discrimination task. The other head will use a graph-based method to explore similar samples and generate a weak label, then perform a supervised contrastive learn-ing task based on the weak label to pull the similar images closer. We further introduced a K-Nearest Neighbor based multi-crop strategy to expand the number of positive sam-ples. Extensive experimental results demonstrate WCL im-proves the quality of self-supervised representations across different datasets. Notably, we get a new state-of-the-art re-sult for semi-supervised learning. With only 1% and 10% labeled examples, WCL achieves 65% and 72% ImageNet
Top-1 Accuracy using ResNet50, which is even higher than
SimCLRv2 with ResNet101. 1.

Introduction
Modern deep convolutional neural networks demon-strate outstanding performance on various computer vision
*Equal contributions.
†Corresponding author.
Figure 1. A example of the class collision problem. A typical in-stance discrimination method will treats the first column and the third column as a negative pair since there are different instance.
However, the semantic information of the first column and the third column are very similar, treat them as positive pair should be much more reasonable. datasets [11, 15, 30] and edge devices [45, 36, 44, 35].
However, most successful methods are trained in the su-pervised fashion; they usually require a large volume of labeled data that is very hard to collect. Meanwhile, the quality of data annotations dramatically affects the per-formance. Recently, self-supervised learning shows its superiority and achieves promising results for unsuper-vised and semi-supervised learning in computer vision (e.g.
[6, 7, 19, 8, 9, 5, 18, 50]). These methods can learn general-purpose visual representations without labels and have a good performance on linear classification and transferabil-ity to different tasks or datasets. Notably, a big part of the recent self-supervised representation learning framework is based on the idea of contrastive learning.
A typical contrastive learning based method adopts the noise contrastive estimation (NCE) [27] to perform the non-parametric instance discrimination [41] as the pretext task, which encourages the two augmented views of the same im-age to be pulled closer on the embedding space but pushes apart all the other images. Most of the recent works mainly improve the performance of contrastive learning from the image augmentation for positive samples and the explo-ration for negative samples. However, instance discrimi-nation based methods will inevitably induce class collision problem, which means even for very similar instances, they still need to be pushed apart, as shown in Figure 1. This in-stance similarities thus tend to hurt the representation qual-ity [1]. In this way, identifying and even leveraging these similar instances plays a key role in the performance of learned representations.
Surprisingly, the class collision problem seems to attract much lesser attention in contrastive learning. As far as we know, there has been little effort to identify similar sam-ples. AdpCLR [49] finds the top-K closest samples on the embedding space and treats these samples as their positives.
However, in the early stage of training, the model cannot ef-fectively extract the semantic information from the images; therefore, this method needs to use SimCLR [6] to pre-train for a period of time, and then switch to AdpCLR to get the best performance. FNCancel [23] proposed a similar idea but adopts a very different way to find the top-K similar in-stances; that is, for each sample, it generates a support set that contains different augmented views from the same im-age, then use mean or max aggregation strategy over the co-sine similarity score between the augmented views in sup-port set and finally identify the top-K similar samples. Nev-ertheless, the optimal support size is 8 in their experiments, requiring 8 additional forwarding passes to generate the em-bedding vectors. Obviously, these methods have two short-comings. Firstly, they are both time-consuming. In the sec-ond place, the result of top-K closest samples might not be reciprocal, i.e. xi is the K closest sample of xj, but xj might not be the K closest sample of xi. In this case, xj will treat xi as a positive sample, but xi will treat xj as a negative sample, which will result in some conflicts.
In this paper, we regard the instance similarities as intrin-sically weak supervision in representation learning and pro-pose a weakly supervised contrastive learning framework (WCL) to address the class collision issue accordingly. In
WCL, similar instances are assumed to share the same weak label comparing to other instances, and instances with the same weak label are expected to be aggregated. To deter-mine the weak label, we model each batch of instances as a nearest neighbor graph; weak labels are thus determined and reciprocal for each connected component of the graph.
Besides, we can further expand the graph by a KNN-based multi-crop strategy to propagate weak labels, such that we can have more positives for each weak label. In this way, similar instances with the same weak label can be pulled closer via the supervised contrastive learning [25] task.
Nevertheless, since the mined instance similarities might be noisy and not completely reliable, in practice, we adopt a two-head framework, one of which handles this weakly su-pervised task while the other is to perform the regular in-stance discrimination task. Extensive experiments demon-strate the effectiveness of our proposed method across dif-ferent settings and various datasets.
Our contribution can be summarized as follows:
• We proposed a two-head based framework to address the class collision problem, with one head focusing on the instance discrimination and the other head for at-tracting similar samples.
• We proposed a simple graph based and parameter-free method to find similar samples adaptively.
• We introduced a K-Nearest Neighbor based multi-crops strategy that can provide much more diverse in-formation than the standard multi-crops strategy.
• The experimental result shows WCL establishes a new state-of-the-art performance for contrastive learning based methods. With only 1% and 10% labeled sam-ples, WCL achieves 65% and 72% Top-1 accuracy on
ImageNet using ResNet50. Notably, this result is even higher than SimCLRv2 with ResNet101. 2.