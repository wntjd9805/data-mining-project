Abstract
Recent works have advanced the performance of self-supervised representation learning by a large margin. The core among these methods is intra-image invariance learn-ing. Two different transformations of one image instance are considered as a positive sample pair, where various tasks are designed to learn invariant representations by comparing the pair. Analogically, for video data, rep-resentations of frames from the same video are trained to be closer than frames from other videos, i.e. intra-video invariance. However, cross-video relation has barely been explored for visual representation learning. Un-like intra-video invariance, ground-truth labels of cross-video relation is usually unavailable without human labors.
In this paper, we propose a novel contrastive learning method which explores the cross-video relation by using cycle-consistency for general image representation learn-ing. This allows to collect positive sample pairs across dif-ferent video instances, which we hypothesize will lead to higher-level semantics. We validate our method by trans-ferring our image representation to multiple downstream tasks including visual object tracking, image classification, and action recognition. We show significant improvement over state-of-the-art contrastive learning methods. Project page is available at https://happywu.github.io/ cycle_contrast_video . 1.

Introduction
There has been a surge of recent interest in contrastive learning of visual representation [66, 29, 3, 28, 57, 9, 26, 41]. We have witnessed that contrastive learning out-performs supervised pre-training with large-scale human annotations in various visual recognition tasks [26, 9]. The key of this self-supervised task is to construct different views and transformations of the same instance, and learn the deep representation to be invariant to the view changes.
To construct different views for forming positive image pairs in contrastive learning, the most common way is to use different data augmentations on the same instance (e.g.
Figure 1: Cross-video cycle-consistency for image representation learning. Starting from one frame in a video, we find its soft near-est neighbor from other videos as a forward step, then the cycle-consistency is achieved when the soft nearest neighbor finds its closest frame within the same video as the start frame in a back-ward step. random cropping, image rotation, colorization).
However, simply performing artificial augmentation on single instance has shown its limitation in multiple applica-tions [58, 67]. For example, Tian et al. [58] have performed detailed analysis on how different augmentations can affect different downstream visual recognition tasks. Going be-yond single image, researchers have also looked into videos as the source for obtaining positive pairs of training im-ages [49, 21, 61]. That is, two nearby frames in the same video can be taken as a natural augmentation in time for the same object instance. By training using temporal augmen-tation, the representation can learn viewpoint and deforma-tion invariance. However, these approaches are still limited to find positive pairs and learning their similarity within a single instance.
In this paper, we propose to perform contrastive learning with positive image pairs sampled across different videos instead of the same video. We hypothesize this can po-tentially capture higher-level semantics and categorical in-formation beyond low-level intra-instance invariances mod-eled by previous approaches. Specifically, given two image frames Ii and Ij from a video, instead of directly using them as a positive pair for training [49, 21], we will first “explain”
frame Ii by composing frames from other videos that are similar to Ii, then compare the composed frames to Ij for contrastive learning.
Assuming we have a neural network feature extractor to learn, we extract the feature representations for the image frame Ii as qi, and representations for frames from other videos as U = {u1, u2, ..., um}. Given these representa-tions, we compute the similarities between qi and U , and normalize them to a probability distribution. We use this probability distribution to re-weight and compose the fea-tures U as a new feature representation for frame Ii (frames that are more similar to Ii will have larger weight). We call this new feature as a soft nearest neighbor to Ii. We then form a positive pair of training data with this new represen-tation and the feature of Ij (a different frame from the same video as Ii). As shown in Figure 1, this procedure goes through a cycle of starting from one frame Ii in a video, searching forward by matching frames from other videos, and retrieving frame Ij backward in the first video. We call this process Cycle-Consistent Contrastive Learning. In-tuitively, enforcing such a cycle-consistency can explicitly push video frames with similar structure closer, thus leads to a natural clustering of semantics.
We perform the proposed self-supervised representa-tion learning on unlabeled video dataset Random Related
Video Views (R2V2) [21] and transfer the learned repre-sentation to various downstream tasks including visual ob-ject tracking, image classification and action recognition.
We stress our goal is to use the temporal signal to learn a general image-level representation for multiple appli-cations beyond videos-level recognition tasks. We show significant improvements over multiple state-of-the-art ap-proaches. We also conduct extensive ablation studies of dif-ferent components and design choices of our method.
Our contributions include: (i) A novel cross-video cycle-consistent contrastive learning objective that explores cross-video relations, going beyond previous intra-image and intra-video invariant learning; (ii) The proposed loss en-forces image representations from the same category (of similar visual structures) closer without explicitly generat-ing pseudo labels; (iii) The learned image representation achieves significant improvement in multiple downstream tasks including object tracking, image classification and ac-tion recognition. 2.