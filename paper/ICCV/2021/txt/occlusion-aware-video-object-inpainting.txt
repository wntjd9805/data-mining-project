Abstract
Conventional video inpainting is neither object-oriented nor occlusion-aware, making it liable to obvious artifacts when large occluded object regions are inpainted. This paper presents occlusion-aware video object inpainting, which recovers both the complete shape and appearance for occluded objects in videos given their visible mask segmen-tation.
To facilitate this new research, we construct the ﬁrst large-scale video object inpainting benchmark YouTube-VOI to provide realistic occlusion scenarios with both oc-cluded and visible object masks available. Our techni-cal contribution VOIN jointly performs video object shape
In particu-completion and occluded texture generation. lar, the shape completion module models long-range ob-ject coherence while the ﬂow completion module recovers accurate ﬂow with sharp motion boundary, for propagat-ing temporally-consistent texture to the same moving ob-ject across frames. For more realistic results, VOIN is op-timized using both T-PatchGAN and a new spatio-temporal attention-based multi-class discriminator.
Finally, we compare VOIN and strong baselines on
YouTube-VOI. Experimental results clearly demonstrate the efﬁcacy of our method including inpainting complex and dy-namic objects. VOIN degrades gracefully with inaccurate input visible mask. 1.

Introduction
Conventional video inpainting infers missing pixel re-gions by distilling information from remaining unmasked video regions. However, as shown in Figure 1, these mod-els [1, 2, 3, 4] often fail to recover moving objects with large occlusion by wrongly inpainting the occluded region with irrelevant background colors and produce obvious artifacts.
This is due to their lack of object and occlusion awareness.
On the contrary, our human visual system possesses power-ful amodal perception ability to reason the complete struc-1Project page is at https://lkeab.github.io/voin. 2This research is supported in part by the Research Grant Council of the Hong Kong SAR under grant no. 16201420 and Kuaishou Technology. ture of moving objects under occlusion, including the ap-pearance of the invisible regions in high ﬁdelity [5, 6].
To overcome the above limitations, we make the ﬁrst sig-niﬁcant attempt on occlusion-aware video object inpainting, which completes occluded video objects by recovering their shape and appearance in motion. While there exist object completion models, they are applicable only to single im-ages [7] in highly limited scenarios such as car and indoor furniture [8, 9]. These single-image models do not utilize temporal coherence during mask and content generation, thus leading to temporal artifacts and unsmooth transition when directly applied to videos.
Training models for occlusion reasoning requires a large number and variety of occluded video objects with amodal mask annotations. One difﬁculty arises from the existing amodal datasets, which come mostly in single images and are small [11, 12], or cover very limited object classes [13].
Inspired by [7], our approach is trained in a self-supervised manner with only modal annotations. To create realistic data in large quantity, we contribute the ﬁrst large-scale video object inpainting benchmark with diverse occlusion patterns and object classes for both training and evalua-tion. Speciﬁcally, we generate occlusion masks for video objects by high-ﬁdelity simulation of overlapping objects in motion, thus taking into consideration object-like occlu-sion patterns, motion and deformation under various de-grees of occlusion. Our new YouTube-VOI dataset based on YouTube-VOS [14] contains 5,305 videos, a 65-category label set including common objects such as people, ani-mals and vehicles, with over 2 million occluded and visible masks for moving video objects.
To infer invisible occluded object regions, we propose the VOIN (Video Object Inpainting Network), a uniﬁed multi-task framework for joint video object mask comple-tion and object appearance recovery. Our object shape com-pletion module learns to infer complete object shapes from only visible mask regions and object semantics, while our appearance recovery module inpaints occluded object re-gions with plausible content. To obtain pixel-level temporal coherency, we design a novel occlusion-aware ﬂow com-pletion module to capture moving video object and prop-agate consistent video content across even temporally dis-tant frames, by enforcing ﬂow consistency during the in-Figure 1. Video object inpainting results comparison with state-of-the-art LGTSM [10], FGVC [2] and STTN [3]. Our VOIN takes corrupted video with free-form occlusion masks as input, and faithfully recovers the occluded object region while preserving spatial detail and temporal coherence. visible object region generation. Furthermore, we augment the temporal patch-based GAN training process using a new multi-class discriminator with specially designed spatio-temporal attention module (STAM), which effectively ac-celerates model convergence and further improves inpaint-ing quality.
Finally, we evaluate VOIN and strong adapted baselines on YouTube-VOI benchmark, where quantitative and quali-tative results clearly demonstrate VOIN’s advantages. This paper complements conventional video inpainting and fa-cilitates future development of new algorithms on repairing occluded video objects. 2.