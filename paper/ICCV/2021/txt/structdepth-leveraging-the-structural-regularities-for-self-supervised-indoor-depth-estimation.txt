Abstract depth estimation
Self-supervised monocular has achieved impressive performance on outdoor datasets. Its performance however degrades notably in indoor environ-ments because of the lack of textures. Without rich textures, the photometric consistency is too weak to train a good depth network. Inspired by the early works on indoor mod-eling, we leverage the structural regularities exhibited in indoor scenes, to train a better depth network. Speciﬁcally, we adopt two extra supervisory signals for self-supervised training: 1) the Manhattan normal constraint and 2) the co-planar constraint. The Manhattan normal constraint enforces the major surfaces (the ﬂoor, ceiling, and walls) to be aligned with dominant directions. The co-planar constraint states that the 3D points be well ﬁtted by a plane if they are located within the same planar region. To gen-erate the supervisory signals, we adopt two components to classify the major surface normal into dominant directions and detect the planar regions on the ﬂy during training.
As the predicted depth becomes more accurate after more training epochs, the supervisory signals also improve and in turn feedback to obtain a better depth model. Through extensive experiments on indoor benchmark datasets, the results show that our network outperforms the state-of-the-art methods. The source code is available at https:
//github.com/SJTU-ViSYS/StructDepth. 1.

Introduction
Inferring the dense 3D map from a single image is a challenging problem without satisfactory solutions until the booming of deep neural networks. With the deep convolu-tional neural networks (CNNs), we can predict the accu-rate depth from a single image, via training the network
∗Both are the ﬁrst authors with equal contributions. †Corresponding author: Danping Zou (dpzou@sjtu.edu.cn). This work was sup-ported by NSFC (62073214).
Figure 1. Our self-supervised monocular depth learning leverages the structural regularities of indoor environments for training. The aligned normal (with Manhattan directions) and the planar regions provide extra losses in training and lead to better 3D structures at inference. with a lot of ground-truth depth labels. The recent self-supervised learning paradigm does not require the ground-truth depth, while still obtaining high-quality results on benchmark datasets, using the photometric consistency as the major supervisory signal. Nevertheless, when existing self-supervised methods are trained on indoor images, the quality of depth estimation degrades notably[51][3]. The main reason is the lack of textures in indoor images. Unlike outdoor scenes, the indoor scenes are full of texture-less re-gions, such as white walls, ceilings, and ﬂoors. Without rich textures, the photometric loss becomes too weak to train a good depth model. Seeking stronger or extra supervisory signals is therefore necessary for training a better depth net-work.
There have been a few attempts. An optical-ﬂow
ﬁeld propagated from the sparse SURF[1] ﬂow by a self-supervised network, is used to guide training on texture-less regions [51]. Another attempt [48] is to use an image patch instead of individual pixels to compute the photomet-ric loss and apply extra constraints to the depth within the planar regions extracted from image segmentation. Though those attempts improve the results, they did not fully ex-ploit the structural regularities presented in indoor environ-ments, a valuable source of information for 3D learning.
The structural regularities, known as the Manhattan-world model[6], describe that the scene consists of major planes aligned with dominant directions. This simple yet effec-tive high-level prior leads to a much better performance in many vision tasks, such as indoor modeling[16][17][5], vi-sual SLAM[50][12][43], and visual-inertial odometry[54], but has not been applied to monocular depth learning.
In this work, we propose to apply the high-level prior of indoor structural regularities to self-supervised depth esti-mation as shown in Fig. 1. Speciﬁcally, we adopt two ex-tra supervisory signals for training: 1) the Manhattan nor-mal constraint and 2) the co-planar constraint. The Man-hattan normal constraint enforces the major surfaces (the
ﬂoor, ceiling, and walls) to be aligned with dominant direc-tions. The co-planar constraint states that the 3D points be well ﬁtted by a plane if they are located within the same planar region. We add two extra components into the train-ing process. The ﬁrst one is Manhattan normal detection.
It classiﬁes the major surface normal, computed from the depth predicted by the network, into the directions associ-ated with the vanishing points by an adaptive thresholding scheme. The second one is planar region detection. We fuse the color and the geometric information derived from the depth and apply a classic segmentation algorithm to ex-tract planar regions. During training, the two components incorporate the estimated depth to produce supervisory sig-nals on the ﬂy. Though those signals may be noisy in early epochs because of inaccurate depth, they will gradually im-prove as the depth quality improves, and in turn beneﬁt the depth estimation.
We conduct experiments on the indoor benchmark datasets: NYU-v2 [39], ScanNet[7], and InteriorNet[28].
The results show that our method outperforms the existing state-of-the-art methods. Our main contributions are as fol-lows: 1) A novel learning pipeline for self-supervised depth es-timation leveraging structural regularities of indoor environ-ments. To our best knowledge, this has not been presented in previous work. 2) Two novel components providing extra supervisory signals on the ﬂy during the training process. Our compo-nents can be used to train a multi-task network including depth estimation, normal estimation, and planar region de-tection in a self-supervised manner, although the latter two tasks serve to train a better depth model in our current im-plementation. 3) We set a new state-of-the-art in self-supervised indoor depth estimation. 2.