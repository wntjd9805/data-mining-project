Abstract
There has been a recent surge of interest in cross-modal pre-training. However, existed approaches pre-train a one-stream model to learn joint vision-language repre-sentation, which suffers from calculation explosion when conducting cross-modal retrieval.
In this work, we pro-pose the Contrastive Cross-Modal Knowledge Sharing Pre-training (COOKIE) method to learn universal text-image representations. There are two key designs in it, one is the weight-sharing transformer on top of the visual and textual encoders to align text and image semantically, the other is three kinds of contrastive learning designed for sharing knowledge between different modalities. Cross-modal knowledge sharing greatly promotes the learning of unimodal representation. Experiments on multi-modal matching tasks including cross-modal retrieval, text match-ing, and image retrieval show the effectiveness and effi-ciency of our pre-training framework. Our COOKIE fine-tuned on cross-modal datasets MSCOCO, Flickr30K, and
MSRVTT achieves new state-of-the-art results while using only 3/1000 inference time comparing to one-stream mod-els. There are also 5.7% and 3.9% improvements in the task of image retrieval and text matching. Source code will be available at https://github.com/kywen1119/COOKIE. 1.

Introduction
Cross-modal pre-training has significantly advanced the progress of representation learning in vision-language field.
It aims at narrowing the heterogeneous gap between vision and language [27, 11, 30]. Recent vision-language pre-training(VLP) methods utilize large-scale image-text pairs to learn the unified representation of visual and textual in-puts, which greatly improve the performance of V+L tasks such as cross-modal retrieval [19, 48, 12], image captioning
[46, 15] and visual question answering [2, 1]. In this pa-per we focus on multi-modal retrieval tasks including cross-modal retrieval (image-text matching and video-text match-ing) and single-modal matching (text matching and image
Figure 1: Illustration of cross-modal knowledge sharing.
Images with similar semantics are sometimes different in structure, subject, background, and style, which leads to in-accurate matching. By matching with the semantics of the corresponding texts, the distance of their embeddings in the common space is narrowed. retrieval).
Humans do not perceive the world with just one sense.
It is the same for model pre-training: simply using single-modal supervision seems not enough. As illustrated in
Fig. 1, two images having the same semantic meaning could look totally different.
In this case, we need to resort to cross-modal pre-training. One-stream VLP methods were recently used for cross-modal pre-training. They use multi-layer transformers [45] as the joint encoder. The input is the concatenation of visual tokens and textual tokens. However, there are two obvious shortcomings for such methods: a)
Two-stage visual feature extraction with Faster R-CNN [41] is time-consuming and may lose some global information, as discussed in [48]. b) One-stream methods need to pro-cess the concatenation of image and text tokens. Such cal-culation will lead to inference time explosion for retrieval tasks. Double-stream methods are also commonly used for cross-modal pre-training. They use a visual path and a textual path to encode images and texts separately. This leads to high efficiency but very limited performance for cross-modal retrieval. There are two obvious constraints: a)
The lack of cross-modal interactions weakens the semantic alignment of images and texts. b) Simple supervision from cross-modal contrastive learning(CCL) loses the knowledge
that the single-modal encoders have learned from the origi-nal images or texts.
In this work, we propose COOKIE: Contrastive Cross-Modal Knowledge Sharing Pre-training, a novel framework designed for multi-modal retrieval tasks. Our COOKIE framework is able to leverage the advantages of both one-stream VLP methods and the double-stream methods while avoiding their aforementioned disadvantages. There are mainly two designs in our framework: The double-stream visual semantic embedding structure with weight-sharing transformer encoder(WS-TE) and the cross-modal and single-modal contrastive learning methods.
The former design, the double-stream visual semantic embedding structure with WS-TE, speeds up cross-modal training and testing while strengthening the semantic align-ment of images and texts. More specifically, COOKIE is designed in a double-stream fashion, thus the inference time explosion caused by one-stream methods is avoided. In the visual stream, the feature is extracted by ResNet instead of Faster-RCNN. In this way, we avoid the huge compu-tation cost while keeping the global visual information. To address the absence of cross-modal interactions which pre-vious double-stream methods lack, a weight-sharing trans-former encoder(WS-TE) is designed to force the model to pay more attention to tokens with the same semantic mean-ings, which guarantees refined vision-language alignment.
Secondly, our COOKIE is optimized by three kinds of contrastive learning: cross-modal contrastive learning, single-modal visual contrastive learning(VCL) and textual contrastive learning(TCL). Compared with single-modal methods [51, 17, 49], cross-modal contrastive pre-training shares knowledge of pre-trained image encoders and text encoders, e.g. ResNet and BERT. An explanation of cross-modal knowledge sharing can be seen in Fig. 1. The two pictures have the same semantic meaning “a person is wait-ing with luggage”, but are quite different due to camera angle and background. With the help of cross-modal con-trastive learning, the image embeddings are drawn closer to each other by the lead of text embeddings. Meanwhile, we don’t expect the single-modal encoders to lose too much information learned from large-scale unimodal pretraining.
Thus, VCL and TCL are added to maintain the single-modal knowledge learned from original images and texts.
Our single-modal objectives differ from structure preserv-ing losses [47, 43]. By manually searching for positive within-modal pairs, they promote the alignment of cross-modal semantics. While our design is much simpler and more effective due to automatically generated pairs. Fur-ther, our VCL and TCL also allow the visual and textual encoder to retain the ability to capture within-modal simi-larity, which helps single-modal retrieval tasks.
To summarize, we make the following contributions.
• We propose a new cross-modal pre-training paradigm
COOKIE. With specially designed weight-sharing transformer encoder(WS-TE), COOKIE provides both efficiency from its double-stream structure and compa-rable effectiveness of one-stream methods.
• Three pre-training objectives including cross-modal contrastive learning(CCL) and single-modal con-trastive learning(VCL and TCL) are designed for cross-modal knowledge sharing which promotes multi-modal retrieval.
• The proposed COOKIE outperforms previous methods on multi-modal matching tasks including image-text matching, video-text matching, text matching and im-age retrieval. Specifically, our COOKIE achieves com-parable results with sota method Oscar [30] using only 3/1000 inference time on Flickr30K and MSCOCO.
COOKIE increases R@1 of MSRVTT from 16.0 to 20.0. For single-modal matching tasks, our model has 3.9% and 5.7% performance gains on text matching and image retrieval respectively. 2.