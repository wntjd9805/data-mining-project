Abstract
Given only a few glimpses of an environment, how much can we infer about its entire floorplan? Existing methods can map only what is visible or immediately apparent from context, and thus require substantial movements through a space to fully map it. We explore how both audio and visual sensing together can provide rapid floorplan reconstruction from limited viewpoints. Audio not only helps sense geometry outside the camera’s field of view, but it also reveals the existence of distant freespace (e.g., a dog barking in another room) and suggests the presence of rooms not visible to the camera (e.g., a dishwasher humming in what must be the kitchen to the left). We introduce AV-Map, a novel multi-modal encoder-decoder framework that reasons jointly about audio and vision to reconstruct a floorplan from a short input video sequence. We train our model to predict both the interior structure of the environment and the associated rooms’ semantic labels. Our results on 85 large real-world environments show the impact: with just a few glimpses spanning 26% of an area, we can estimate the whole area with 66% accuracy—substantially better than the state of the art approach for extrapolating visual maps. 1.

Introduction
Floorplans of complex 3D environments—such as homes, offices, shops, churches—are a compact ground-plane repre-sentation of their overall layout, showing the different rooms and their connectivity. Floorplans are useful for visualizing a large space, navigating an unfamiliar building, planning safety routes, and communicating architectural designs.
Traditionally a floorplan is created by distilling a fully observed 3D environment into its footprint—whether manu-ally or with the aid of 3D sensors [42, 31]. Recent research aims to infer room layouts using imagery and/or scans, with impressive results [29, 8, 27, 46]. However, existing meth-ods are limited to mapping the regions they directly observe.
*work done while interning at Facebook AI Research. Project webpage: http://www.cs.cmu.edu/~spurushw/publication/avmap
Figure 1: Audio-visual floorplan reconstruction: A short video walk through the house can reconstruct the visible portions of the floorplan, but is blind to many areas. We introduce audio-visual floorplan reconstruction, where sounds in the environment help infer both the geometric properties of the hidden areas as well as the semantic labels of the unobserved rooms (e.g., cooking sounds behind a wall to the camera’s left suggest the kitchen).
They require a dense walk-through for the camera to capture most of the space. This is often wasteful if not impossible for a human manually photographing a large space or a robot needing to immediately perform tasks in a new environment.
In this work, we address this shortcoming by estimating floorplans beyond the regions captured by a camera.
Our idea is to “see" beyond the visible regions by lis-tening. Audio provides strong spatial and semantic signals that complement the mapping capabilities of visual sens-ing. In particular, the value of audio for floorplan estimation is threefold. First, observed sound is inherently driven by geometry; audio reflections bounce off major surfaces and reveal the shape of a room, beyond the camera’s field of view. Second, sounds heard from afar can suggest the exis-tence of distant freespace where the sounding object could exist. Third, hearing semantically meaningful sounds from different directions naturally reveals the plausible room lay-outs based on the activities or objects those sounds represent.
For example, a shower running suggests the direction of the bathroom, even before we see it; microwave beeps suggest a kitchen; climbing footsteps suggest a staircase. See Figure 1.
To this end, we propose a new research direction: audio-visual floorplan reconstruction. Given a short RGB video complete with multi-channel audio, the goal is to produce a 2D floorplan that shows the freespace and occupied regions and divides them into a discrete set of semantic room labels (family room, kitchen, etc.). Importantly, the floorplan out-put extends significantly beyond the area directly observable in the video frames. This efficiency is critical for navigating robots that need to act without exhaustively touring a space, as well as offline scenarios where a user wants to extract a broad map from an existing short video. For example, imag-ine floorplans inferred from YouTube videos to facilitate real estate viewing or from wearable camera video to benefit spatial reasoning in augmented reality.
Our AV-Map approach works as follows. We devise a deep convolutional neural network architecture that lever-ages sequences of audio and visual data to reason about the structure and semantics of the floorplan. Our model inde-pendently extracts floorplan-aligned features from audio and
RGB data, encodes sequences of features of each modality using self-attention mechanisms, and finally fuses informa-tion from audio and RGB using a decoder architecture.
We consider two settings: device-generated sounds (ac-tive) and environment-generated sounds (passive). In the active setting, the camera emits a known sound while it moves. This corresponds to a use case where a person or robot does a swift walk-through of an environment while their phone/camera emits some sound. In the passive setting, we observe only naturally occurring sounds made by objects and people in the building. This corresponds to a use case where we are simply given a passively recorded video, likely captured for some other purpose.
To our knowledge, ours is the first attempt to infer floor-plans from audio-visual data. Our results on 85 large real-world, multi-room environments show that AV-Map not only consistently outperforms traditional vision-based mapping, but also improves the state-of-the-art approach [34] for ex-trapolating occupancy maps beyond visible regions (with a relative gain of 8% in floorplan accuracy). Though ob-serving only a small fraction of the full homes, our model yields good interior maps covering much of their area. We also show audio and vision are synergistic signals to classify room types, allowing high-level perception of the semantics of the space even before directly visiting each room. 2.