Abstract
We present PrimitiveNet, a novel approach for high-resolution primitive instance segmentation from point clouds on a large scale. Our key idea is to transform the global segmentation problem into easier local tasks. We train a high-resolution primitive embedding network to pre-dict explicit geometry features and implicit latent features for each point. The embedding is jointly trained with an adversarial network as a primitive discriminator to decide whether points are from the same primitive instance in lo-cal neighborhoods. Such local supervision encourages the learned embedding and discriminator to describe local sur-face properties and robustly distinguish different instances.
At inference time, network predictions are followed by a re-gion growing method to ﬁnalize the segmentation. Experi-ments show that our method outperforms existing state-of-the-arts based on mean average precision by a signiﬁcant margin (46.3%) on ABC dataset [31]. We can process ex-tremely large real scenes covering more than 0.1km2. Abla-tion studies highlight the contribution of our core designs.
Finally, our method can improve geometry processing algo-rithms to abstract scans as lightweight models. Code and data will be available based on Pytorch1 and Mindspore2. 1.

Introduction 3D scanning techniques have made rapid advances in re-cent years with 3D sensors. State-of-the-art algorithms [47, 28, 65, 12] or commercial softwares [1] ease the reconstruc-tion and digitization of the real-world environments. How-ever the quality and the complexity of the output model are below the required standards for target applications including gaming and virtual/augmented reality (AR/VR).
For example, a typical indoor scan contains several mil-1https://github.com/hjwdzh/PrimitiveNet 2https://gitee.com/mindspore/mindspore/tree/ master/model_zoo/research/3d/PrimitiveNet
Figure 1. We propose PrimitiveNet to robustly segment primitive instances at the level of (a) objects or (b) chunks of scenes. (c)
We can handle extremely large scenes covering 0.1km2. (d) We improve scene abstraction and deliver lightweight models. lion faces ﬁlled with noises, which is not affordable for a cell phone. Main directions to address these issues include local mesh decimation [18, 37, 52] and primitive instance assembly [5, 32, 23, 2]. Mesh decimation collapses edges iteratively but fails to preserve important structures. Prim-itive assembly requires segmenting points into instances of primitives and thus is limited by the segmentation quality.
We aim to signiﬁcantly improve the segmentation quality and the ﬁnal production of lightweight models from scans.
Primitive instance segmentation has a long history in geometry processing with two standard solutions using
Ransac [53] or region growing [41, 51]. The main chal-lenge is to ﬁnd appropriate parameters to robustly recover shapes from noises and robustly preserve boundaries of sim-ilar primitives. Recently, this problem is partially addressed using deep learning techniques [35, 57, 39] at the object level. However, they require to extract global shape proper-ties and have limited capacity for correctly predicting small
instances or processing point clouds on a large scale.
To address these limitations, we transform the global primitive ﬁtting problem into local tasks that are easier to learn and generalize, which robustly distinguish different instances and derive high-quality segmentation at different scales (Figure 1 (a-c)). We train a primitive embedding net-work that focuses on learning per-point local surface prop-erties including both explicit geometry features and implicit latent features. One popular choice of explicit feature is the object center, which proves to be effective for accurate se-mantic instance segmentation[29, 15, 22] with post cluster-ing. However, it is not suitable for primitive segmentation:
While object scale is relatively local, sizes of primitives like
ﬂoor planes can be large enough to cover the whole scene.
Further, centers can be shared among different primitives and thus not a discriminative feature for clustering. Instead, we design explicit features for a point as its local tangent plane supervised by the location with a normal direction on the ground truth shape nearest to the point. We use latent features to distinguish primitive instances. We supervise features with primitive types if available during training.
Since it is insufﬁcient to distinguish instances with the same type, we additionally train an adversarial metric as a prim-itive discriminator to decide whether two latent features in-dicate different instances. To encourage latent features to capture local properties, we constrain the primitive discrim-inator to evaluate features of closed points. We ﬁnd such local constraint highlights feature differences at boundaries and is robust for supervision.
Our design of primitive embedding network combines
PointNet [49] and sparse convolution [20, 7] to extract high-resolution point features locally and regular volume fea-tures within a larger receptive ﬁeld. They are concatenated and passed through multi-layer perceptrons to derive per-point explicit and implicit surface features, where implicit features are translated as primitive type scores via a linear layer. Explicit features and primitive scores are supervised by ground truth data. To enforce primitive discriminator and implicit features to be local, we sample implicit fea-tures from only pairs of closed points below a certain dis-tance threshold for discriminator input.
Experiments show that we signiﬁcantly outperform ex-isting methods at ABC dataset [31] and self-collected scene dataset under several metrics. Notably, we outperform state-of-the-art methods under mean average precision by 46.3% on the ABC dataset. We handle extremely large scenes by processing chunks and merging them seamlessly.
Ablation studies show that local properties are critical to the performance, and our high-resolution backbone further improves the prediction. Our explicit property supervision helps to increase robustness for different levels of noise. Fi-nally, we integrate our approach into a robust pipeline to abstract scanned point clouds as light-weight models.
In sum, our core research contributions are:
• We design an adversarial primitive embedding network that learns discriminative local surface properties.
• We propose a high-resolution backbone combining point and voxel features.
• Our design signiﬁcantly outperforms the state-of-the-arts and can handle extremely large-scale environment.
• We integrate our algorithm to a pipeline that produces lightweight models from real scans. 2.