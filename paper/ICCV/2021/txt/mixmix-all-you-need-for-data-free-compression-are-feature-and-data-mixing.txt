Abstract
User data confidentiality protection is becoming a rising challenge in the present deep learning research. Without access to data, conventional data-driven model compres-sion faces a higher risk of performance degradation. Re-cently, some works propose to generate images from a spe-cific pretrained model to serve as training data. However, the inversion process only utilizes biased feature statistics stored in one model and is from low-dimension to high-dimension. As a consequence, it inevitably encounters the difficulties of generalizability and inexact inversion, which leads to unsatisfactory performance. To address these prob-lems, we propose MixMix based on two simple yet effective techniques: (1) Feature Mixing: utilizes various models to construct a universal feature space for generalized inver-sion; (2) Data Mixing: mixes the synthesized images and labels to generate exact label information. We prove the effectiveness of MixMix from both theoretical and empiri-cal perspectives. Extensive experiments show that MixMix outperforms existing methods on the mainstream compres-sion tasks, including quantization, knowledge distillation and pruning. Specifically, MixMix achieves up to 4% and 20% accuracy uplift on quantization and pruning, respec-tively, compared to existing data-free compression work. 1.

Introduction
To enable powerful deep learning models on the em-bedded and mobile devices without sacrificing task perfor-mance, various model compression techniques have been discovered. For example, neural network quantization [12, 23, 27, 55] converts 32-bit floating-point models into low-bit fixed point models and benefits from the acceleration of fixed-point computation and less memory consumption.
Network pruning [8, 14, 48] focuses on reducing the redun-dant neural connections and finds a sparse network. Knowl-edge Distillation (KD) [18, 41] transfers the knowledge in the large teacher network to small student networks.
However, one cannot compress the neural networks ag-gressively without the help of data. As an example, most
Figure 1. An overview of the results of post-training data-free quantization. Each color bar denotes a data source. Data inverted from ResNet-50 encounters an accuracy deficit on MobileNetV2. full precision models can be safely quantized to 8-bit by directly rounding the parameters to their nearest inte-gers [26, 39]. However, when the bit-width goes down to 4, we have to perform quantization-aware training using the data collected from users to compensate for the accuracy loss. Unfortunately, due to the increasing privacy protec-tion issue1, one cannot get user data easily. Moreover, the whole ImageNet dataset contains 1.2M images (more than 100 gigabytes), which consumes much more memory space than the model itself. Therefore, the data-free model quan-tization is more demanding now.
Recently, many works [4, 15, 50] propose to invert im-ages from a specific pretrained model. They try to match the activations’ distribution by comparing the recorded run-ning mean and running variance in the Batch Normal-ization (BN) [24] layer. Data-free model compression with the generative adversarial network is also investigated in [5, 11, 49]. All these works put their focus on develop-ing a better criterion for inverting the data from a specific model. We call this type of data as model-specific data.
We identify two problems of model-specific data. First, the synthesized images generated by a specific model are biased and cannot be generalized to another. For example, in DeepInversion [50], synthesizing 215k 224×224 resolu-1https://ec.europa.eu/info/law/law-topic/ data-protection_en
Figure 2. The overall pipeline of the proposed MixMix algorithm. Data Mixing can reduce the incorrect solution space by mixing the pixel and label of two trainable images. Feature Mixing incorporates the universal feature space from various models and generates a one-for-many synthesized dataset, after which the synthesized data can be applied to any models and application. tion images from ResNet-50v1.5 [40] requires 28000 GPU hours. These images cannot be used for another model eas-ily. As we envisioned in Fig. 1, data inverted from Mo-bileNetV2 has 4% higher accuracy than ResNet-50 data on MobileNetV2 quantization and vice versa. Therefore, model-specific data inversion requires additional thousands of GPU hours to adapt to compression on another model.
Second, due to the non-invertibility of the pretrained model, model-specific data results in inexact synthesis. A simple example is not hard to find: given a ReLU layer that has 0 in its output tensor, we cannot predict the corresponding input tensor since the ReLU layer outputs 0 for all negative input. As a result, finding the exact inverse mapping of a neural network remains a challenging task.
In this work, we propose MixMix data synthesis algo-rithm that can generalize across different models and ap-plications, which contributes to an overall improvement for data-free compression. MixMix contains two sub-algorithms, the first one is Feature Mixing which utilizes the universal feature produced by a collection of pre-trained models. We show that Feature Mixing is equal to optimize the maximum mean discrepancy between the real and syn-thesized images. Therefore, the optimized data shares high fidelity and generalizability. The second algorithm is called
Data Mixing that can narrow the inversion solution space to synthesize images with exact label information. To this end, we summarize our core contributions as: 1. Generalizability: We propose Feature Mixing that can absorb the knowledge from widespread pre-trained ar-chitectures. As a consequence, the synthesized data can generalize well to any models and applications. 2. Exact Inversion: We propose Data Mixing which is able to prevent incorrect inversion solutions. Data Mix-ing can preserve correct label information. 3. Effectiveness: We verify our algorithm from both the-oretical and empirical perspectives. Extensive data-free compression applications, such as quantization, pruning and knowledge distillation are studied to demonstrate the effectiveness of MixMix data, achieving up to 20% ab-solute accuracy boost. 2.