Abstract
Rolling shutter (RS) images can be viewed as the result of the row-wise combination of global shutter (GS) images captured by a virtual moving GS camera over the period of camera readout time. The RS effect brings tremendous difﬁculties for the downstream applications.
In this pa-per, we propose to invert the above RS imaging mechanism, i.e., recovering a high framerate GS video from consecutive
RS images to achieve RS temporal super-resolution (RSSR).
This extremely challenging problem, e.g., recovering 1440
GS images from two 720-height RS images, is far from be-ing solved end-to-end. To address this challenge, we ex-ploit the geometric constraint in the RS camera model, thus achieving geometry-aware inversion. Speciﬁcally, we make three contributions in resolving the above difﬁculties: (i) formulating the bidirectional RS undistortion ﬂows under the constant velocity motion model, (ii) building the connec-tion between the RS undistortion ﬂow and optical ﬂow via a scaling operation, and (iii) developing a mutual conversion scheme between varying RS undistortion ﬂows that corre-spond to different scanlines. Building upon these formu-lations, we propose the ﬁrst RS temporal super-resolution network in a cascaded structure to extract high framerate global shutter video. Our method explores the underly-*Corresponding author ing spatio-temporal geometric relationships within a deep learning framework, where no extra supervision besides the middle-scanline ground truth GS image is needed. Essen-tially, our method can be very efﬁcient for explicit propa-gation to generate GS images under any scanline. Experi-mental results on both synthetic and real data show that our method can produce high-quality GS image sequences with rich details, outperforming state-of-the-art methods. 1.

Introduction
Many consumer cameras such as webcams or mobile phones are generally built upon CMOS sensors due to their low cost and simplicity in manufacturing, which commonly adopt the rolling shutter (RS) mechanism. Unlike its global shutter (GS) counterpart, an RS camera generates an image row-by-row sequentially, which gives rise to the so-called
RS effect (e.g., stretch, wobble) in images and videos cap-tured by a moving RS camera. The RS effect is increasingly becoming a nuisance in photography. Simply ignoring the
RS effect in computer vision applications leads to perfor-mance degradation or even failure [2, 6, 19]. However, the
RS image combines information about both the scene ge-ometry and the concealed motion [43], according to which the RS effect can be removed by single frame [18,28,29,43] or multiple frames [20, 30, 36, 41, 42]. As single-view RS
correction is inherently a highly ill-posed problem, follow-ing [20, 41, 42], using at least two consecutive frames can make it tractable. Intuitively, observing a pair of consecu-tive RS images, humans seem to be able to infer a plausible explanation for the underlying geometry (i.e., camera mo-tion and scene structure).
In this paper, we propose to invert the rolling shutter imaging mechanism, i.e., recovering the high framerate global shutter video from consecutive rolling shutter im-ages, as illustrated in Fig. 1. Essentially, this inversion pro-cess mimics the above human ability. This paper aims at recovering a temporal sequence of latent GS image frames from two consecutive RS images, i.e., performing RS tem-poral super-resolution (RSSR). This task involves solving the underlying RS geometry, which is arduous for existing methods (e.g., [41]) to achieve robust and accurate estima-tion due to subtle intra-frame motions, requiring non-trivial camera calibration and iterative optimizations.
To some extent, RSSR relates to two-view RS correction
[20, 41, 42] and GS video interpolation [4, 5, 15, 24]. How-ever, our task contains additional complexity, because be-yond eliminating the geometric RS distortion, we also want to generate a set of high framerate GS images in chrono-logical order. This is particularly challenging as we have to ensure the temporal smoothness of the output sequence.
Unfortunately, the artifacts appear in the pure geometric methods [41, 42] due to inaccurate RS geometric estima-tion. Only one reliable GS image can be recovered by [20].
See supplementary materials for more analyses. Also, dif-ferent from the slight and controllable pixel displacement in the GS video interpolation task, which is located inside its optical ﬂow, the pixel displacement when correcting the RS image may exceed its local neighborhood deﬁned by its op-tical ﬂow, depending on the type of motion, the 3D structure and the scanline time. Speciﬁcally, we identify (for the ﬁrst time) that the RS-aware pixel displacement can be obtained by scaling the corresponding optical ﬂow vector under the constant velocity motion model, but the size and sign of the scaling factor are determined by the intrinsic RS geometry.
To address the above challenges, we formulate the bidi-rectional RS undistortion ﬂows to characterize the pixel-wise RS-aware pixel displacement, and further advance a calculation method for the mutual conversion between vary-ing RS undistortion ﬂows corresponding to different scan-lines. In particular, we prove that the scaling factor is in the interval of (−1, 1) when correcting an RS image to its middle-scanline GS image. As a result of utilizing these parameterizations, we propose a data-driven solution for
RSSR with good interpretability, which intrinsically encap-sulates the complete underlying RS geometry that more so-phisticated methods (e.g., [41, 42]) struggle to learn away.
The proposed geometry-aware RSSR pipeline employs a cascaded architecture to extract a latent high framerate GS video sequence from two consecutive RS images. Firstly, we estimate the bidirectional optical ﬂows by using the clas-sic PWC-Net [34]. Secondly, we design a UNet-like net-work to learn the scaling factor of each pixel (i.e., middle-scanline correlation map in Sec. 4) such that the middle-scanline RS undistortion ﬂows can be inferred. At the same time, RS undistortion ﬂows for any scanline can be associ-ated and propagated explicitly. Finally, the softmax splat-ting [24] is used to produce the high framerate GS video frames at arbitrary scanlines. Our RSSR network can be trained end-to-end and only the middle-scanline GS images are needed for supervision. Since none of the learned net-work parameters are time-dependent, it can synthesize as many GS frames as needed. Extensive experimental results demonstrate that our approach is superior to the state-of-the-art methods in removing the RS artifacts, and it can gen-erate a coherent video sequence as well.
Our main contributions are summarized as follows:
• We identify and establish a detailed proof of the scanline-dependent nature of the bidirectional RS undistortion ﬂows, which is essential for understand-ing the intrinsic geometrical properties of RS correc-tion problem [20, 41–43].
• From the theoretical perspective, we provide a sound motivation for our ﬁrst learning-based RSSR solution for latent GS video sequence extraction from two con-secutive RS images, which brings RS images alive.
• Our approach not only outperforms the state-of-the-art methods in both RS effect removal and inference efﬁ-ciency, but also can produce a smooth and continuous video sequence far beyond the reach of [20]. 2.