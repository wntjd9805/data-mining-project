Abstract
Monocular 3D object detection is of great signiﬁcance for autonomous driving but remains challenging. The core challenge is to predict the distance of objects in the ab-sence of explicit depth information. Unlike regressing the distance as a single variable in most existing methods, we propose a novel geometry-based distance decomposition to recover the distance by its factors. The decomposition fac-tors the distance of objects into the most representative and stable variables, i.e. the physical height and the projected visual height in the image plane. Moreover, the decomposi-tion maintains the self-consistency between the two heights, leading to robust distance prediction when both predicted heights are inaccurate. The decomposition also enables us to trace the causes of the distance uncertainty for different scenarios. Such decomposition makes the distance predic-tion interpretable, accurate, and robust. Our method di-rectly predicts 3D bounding boxes from RGB images with a compact architecture, making the training and inference simple and efﬁcient. The experimental results show that our method achieves the state-of-the-art performance on the monocular 3D Object Detection and Bird’s Eye View tasks of the KITTI dataset, and can generalize to images with dif-ferent camera intrinsics 1. 1.

Introduction
Object detection is a fundamental and challenging prob-lem in computer vision. With the emergence of deep learning [43, 16], 2D object detection has achieved great progress in the past years [13, 12, 36, 29, 26, 25]. However, it is yet insufﬁcient for applications requiring 3D spatial in-formation like autonomous driving. 3D object detection, which detects objects as 3D bounding boxes, has drawn much attention. Comparing with 3D object detection meth-ods relying on expensive LiDAR sensors to provide depth information [47, 38, 51, 22], monocular 3D object detec-1https://github.com/Rock-100/MonoDet
Figure 1. Our distance decomposition is based on the imaging ge-ometry [14] of a pinhole camera. The distance from the center of an object to the camera, denoted as Z, can be calculated by
Z = f H h , where f denotes the focal length of the camera, H de-notes the physical height of the object, and h denotes the length of the projected central line (PCL). The PCL represents the pro-jection of the vertical line at the center of the 3D bounding box.
This equation shows the distance of objects is determined by the physical height and projected visual height in the image plane. tion [5, 37] infers depth from monocular images with low computation and energy cost. The core challenge of monoc-ular 3D object detection is inferring the distance of objects in the absence of explicit depth information. Given the vi-sual appearance of an object, its spatial location can be in-ferred based on the imaging geometry [14] as an inverse problem. Thus, the priors of object physical size, scene lay-out, and the imaging process of cameras are essential to ex-ploit to recover the distance.
On one hand, such geometric priors have been exploited to predict the pose or distance of objects by their factors. In 6D object pose estimation, PVNet [35] and SegDriven [17] regress the 2D keypoints of objects. In category-level 6D object pose and size estimation, NOCS [44] uses the nor-malized object coordinate space map. In stereo 3D object detection, Stereo R-CNN [23] uses sparse 2D keypoints, yaw angle, object physical size, and a region-based pho-tometric alignment using the left and right RoIs. These works [35, 17, 23, 44] recover the pose or distance by sev-eral factors, such as 2D keypoints, 2D bounding boxes, and object physical size, which achieves interpretable and ro-bust pose or distance estimation.
On the other hand, most monocular 3D object detection methods deal with the challenging distance prediction by regressing it as a single variable. The learning-based meth-ods [50, 7, 41, 42] directly learn a mapping from input images to the distance. The pseudo-LiDAR based meth-ods [45, 31, 48] ﬁrst regress the depth map of an input image and then predict the distance of objects with the depth map.
The 3D-anchor-based methods [2, 3, 9] break the distance prediction into the region proposal and the offset regression.
The only exceptions are [33, 19, 24], which recover the distance by minimizing the re-projection error between 3D bounding boxes and 2D bounding boxes or 2D keypoints.
However, [33, 19, 24] still lag behind those methods which regress the distance as a single variable.
Aiming to close this gap, we propose a novel geometry-based distance decomposition to recover the distance by its factors. Different from [33, 19, 24], we abstract ob-jects as vertical lines at the center of 3D bounding boxes, and their visual projection as the projection of these verti-cal lines, then recover the distance by them based on the imaging geometry [14], as shown in Fig. 1. The decompo-sition is designed to be as simple as possible, yet effective and efﬁcient to extract the most representative and stable factors of the distance of objects, i.e., the physical height and the projected visual height. The advantages of the de-composition are four-fold. 1) It makes the distance predic-tion interpretable. The physical height can be interpreted as an intrinsic attribute of objects, and the visual height can be interpreted as the extrinsic position in a scene. 2) The physical height and projected visual height are easy to es-timate as revealed by our observations. 3) The decomposi-tion maintains the self-consistency between the two heights, leading to robust distance prediction when both predicted heights are inaccurate. 4) The decomposition enables us to trace and interpret the causes of the distance uncertainty for different scenarios, by introducing an uncertainty-aware regression loss for the decomposed variables. In addition, our method can generalize to images with different camera intrinsics, because it reasons the distance only by the local information of objects and decouples the focal length from the distance prediction. This generalization ability is crucial to facilitate the deployment of the machine learning models of monocular 3D vision [10].
The contributions of our method are summarized below: 1. A novel geometry-based distance decomposition makes the distance prediction interpretable, accurate and robust. 2. Based on the decomposition, our method originally traces the causes of the distance uncertainty. 3. Our method directly predicts 3D bounding boxes from
RGB images with a compact architecture, making the training and inference simple and efﬁcient. 4. Our method achieves the state-of-the-art (SOTA) per-formance on the monocular 3D Object Detection and
Bird’s Eye View tasks of the KITTI dataset [11], and can adapt to images with different camera intrinsics. 2.