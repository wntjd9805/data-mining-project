Abstract
Can general-purpose neural models learn to navigate?
For PointGoal navigation (‘go to ∆x, ∆y’), the answer is a clear ‘yes’ – mapless neural models composed of task-agnostic components (CNNs and RNNs) trained with large-scale model-free reinforcement learning achieve near-perfect performance [27]. However, for ObjectGoal navi-gation (‘find a TV’), this is an open question; one we tackle in this paper. The current best-known result on ObjectNav with general-purpose models is 6% success rate [25].
First, we show that the key problem is overfitting. Large-scale training results in 94% success rate on training envi-ronments and only 8% in validation. We observe that this stems from agents memorizing environment layouts during training – sidestepping the need for exploration and directly learning shortest paths to nearby goal objects. We show that this is a natural consequence of optimizing for the task metric (which in fact penalizes exploration), is enabled by powerful observation encoders, and is possible due to the finite set of training environment configurations.
Informed by our findings, we introduce Treasure Hunt
Data Augmentation (THDA) to address overfitting in Ob-jectNav. THDA inserts 3D scans of household objects at ar-bitrary scene locations and uses them as ObjectNav goals – augmenting and greatly expanding the set of training lay-outs. Taken together with our other proposed changes, we improve the state of art on the Habitat ObjectGoal Navi-gation benchmark by 90% (from 14% success rate to 27%) and path efficiency by 48% (from 7.5 SPL to 11.1 SPL). 1.

Introduction
Consider an agent given a task such as ‘Bring me a teapot’ in a new environment. In order to successfully per-form the command, it first must navigate around the envi-ronment to find the teapot. This search subtask is referred to as ObjectNav [1, 3] and is illustrated in Fig. 1.
*Correspondence to maksymets@facebook.com
In this work, we examine if general-purpose neural mod-els learn to navigate when given their goal specified as an object name. Specifically, models that are composed of navigation-agnostic general architectural components (CNNs, RNNs, fully-connected layers, etc.) and trained un-der an experimental setup that provides no inductive bias towards how humans believe this problem should be solved e.g. no map-like or spatial structural components in the agent, no mapping supervision, no auxiliary tasks – nothing other than the task of navigation to a goal object. We find this question interesting both from a scientific perspective (what are the fundamental limits of learnability?) and an engineering perspective (general-purpose architectures are widely applicable across tasks and any advances made are likely to have significant ripple effects).
In recent work studying PointGoal Navigation (or navi-gating to a relative waypoint), task-agnostic neural models trained with large-scale model-free reinforcement learning (RL) achieve near-perfect performance [27]. We find how-ever that similar models are unable to achieve even non-trivial performance in unseen environments when applied directly to ObjectNav. We show that the key problem is ex-treme overfitting – even in the presence of standard tricks like early stopping. Large-scale training results in 97% suc-cess rate in training environments and only 8% success un-seen environments from validation.1
We identify three key reasons for this poor generalization and demonstrate techniques to reduce their effect: 1) Overly rich sensors. Given that ObjectNav relies on se-mantic information, agents are often provided observations in the form of RGB-D images and a semantic segmentation of the current frame (via either ground-truth or a separately trained network). However, we argue that this sensor suite is
‘too rich’ for the task, i.e. makes memorizing environments 1We encourage the reader to fight hindsight bias at this point – it is not at all obvious that general-purpose neural networks trained with model-free reinforcement learning should be able to achieve such high perfor-mance on a large number of diverse training environments. We could very well have found ourselves in a world where both training and validation performance was poor, with approximation or optimization error to blame.
Figure 1: In ObjectNav, an agent is spawned (at locations 1,2,3) and asked to find the ‘bed’, shown in (a). We introduce the idea of Treasure
Hunt Data Augmentation (THDA), where we insert common household objects from the YCB dataset (shown in (e)) at random locations in the house (b,c,d) and ask the agent to find them. In the original ObjectNav dataset (left half of figure), the agent’s spawn position varies (1−3), but the goal location does not. In THDA, both the robot spawn position and the goal location changes. This increases the training data significantly by expanding the diversity of the (starting position, target category, target position) tuples in the training data.
We demonstrate empirical that this improve validation performance significantly on unseen scenes. and goals during training easy. We empirically find that limiting the agent to a minimal sensor suite of a Depth sen-sor and a segmentation mask of the goal category reduces overfitting (when combined with our other ideas below). 2) Mismatch between reward and necessary behavior at inference. ObjectGoal navigation is fundamentally about exploration – when an agent is put into an unseen environ-ment, there are few priors it can rely on for finding the ob-ject and thus it must explore. The typical reward used for training navigation agents encourages them to reach their goal as quickly as possible (called ‘slack penalty’). Unfor-tunately, the direct implication is that this reward penalizes exploration. Over a finite set of training environments, we argue that this both encourages memorization and fails to teach the agent how to explore. We propose a reward called
ExploreTillSeen where the agent is initially rewarded for exploring the environment and then for navigating to a tar-get object as quickly as possible after seeing it. 3) Limited training data. Even with reduced sensors and a exploration-promoting reward, the fundamental challenge of data scarcity remains. Datasets for ObjectNav depend on large 3D scenes with high quality semantic annotations which are difficult to collect. We introduce Treasure Hunt
Data Augmentation as a way to combat this. THDA inserts objects into existing 3D environments to generate synthetic
ObjectNav episodes to augment the training set. This idea is illustrated in Fig. 1 with inserted objects mug, pliers, jug.
By addressing these causes of overfitting, we are able to train an agent that generalizes better to novel scenes.
Contributions. We summarize our contributions below: – We significantly improve the performance of a simple mapless model-free RL baseline on the challenging Habi-tat ObjectNav benchmark – from 6.2% [25] to 21.2% suc-cess rate. We also demonstrate significant improvements on path efficiency – from 2.1% SPL to 7.7% SPL. Even with-out THDA, this already sets a new state of art on the task. – We propose an effective ExploreTillSeen reward func-tion for the ObjectNav task that combines exploration and distance to target rewards with the object identified reward. – We introduce Treasure Hunt Data Augmentation (THDA) that is highly effective for pre-training the improved RL baseline, further improving the start of art from 21.2% suc-cess (7.7% SPL) to 27.4% success (11.1% SPL). All three of our ideas together result in a relative improvement in the state of art by +48% SPL and +90% Success. In fact,
THDA shows competitive results with prior state of art even in a zero-shot setting, i.e. without any training on the target dataset used by the challenge. 2.