Abstract
Knowledge distillation (KD) transfers the dark knowl-edge from cumbersome networks (teacher) to lightweight (student) networks and expects the student to achieve more promising performance than training without the teacher’s knowledge. However, a counter-intuitive argument is that better teachers do not make better students due to the ca-pacity mismatch. To this end, we present a novel adaptive knowledge distillation method to complement traditional approaches. The proposed method, named as Student Cus-tomized Knowledge Distillation (SCKD), examines the ca-pacity mismatch between teacher and student from the per-spective of gradient similarity. We formulate the knowl-edge distillation as a multi-task learning problem so that the teacher transfers knowledge to the student only if the student can benefit from learning such knowledge. We vali-date our methods on multiple datasets with various teacher-student configurations on image classification, object detec-tion, and semantic segmentation. 1.

Introduction
Deep neural networks have achieved state-of-the-art re-sults in a variety of applications such as computer vi-sion [20], speech recognition [1], and natural language pro-cessing [6, 30]. Although it is established that introduc-ing more computational costs often improves the perfor-mance of the models, big models are computationally too expensive to be deployed on devices, which only limited computational resources are available such as mobile de-vices and embedded devices. Model compression tech-niques have emerged to address such issues, and knowledge distillation [12] has proven to be a promising way to obtain a small model without significant performance loss among those techniques.
It works by encouraging a lightweight student model to mimic the behavior learns by a cumber-some teacher model.
For the success of knowledge distillation, some re-Figure 1: Best view in color. Top: The gradient similar-ity of knowledge distillation and student loss at different iterations in the gradient space. Middle: Prior approaches.
The knowledge distillation process between two networks is stationary in different iterations. Bottom: Our approach automatically decides to switch on or switch off the knowl-edge distillation loss based on their corresponding relative gradient direction to student loss. searchers have focused on the relation between students and teachers. These works challenge a common intu-ition that better teachers make better students. Mirzadeh et.al. [24] found out that students distilled from a bigger teacher, one with more parameters and higher accuracy,
can perform worse than the same students distilled from a smaller teacher. Cho et.al. [2] also discovered the same phenomenon, and it is even more severe when training on a large-scale, challenging dataset such as ImageNet. Both works conclude that the student and the teacher’s capacity mismatch is the reason for the negative correlation between teachers’ accuracy and students’ performance. Meanwhile, other works also show that the same teacher model [9, 36] or even teacher with lower performance [34, 25, 35] than the student model can be used as the teacher model to perform knowledge distillation, which gives rise to self-distillation methods [5, 38, 39].
Though the ”better teacher, worse student” contradic-tion has been discovered, how to resolve this issue is still rarely explored. TAKD [24] present to use teacher assistant, which is a smaller teacher, as a media to smooth the knowl-edge transfer procedure between large teacher and small student. ESKD [2] propose an early stop strategy during the knowledge distillation process, which reduces the negative effect of KD. However, these methods require manual tun-ing. When the student model changes, these methods need to carefully choose either a teacher assistant model or an appropriate early stopping criterion to balance the trade-off between the positive and negative effects that are brought by the teacher’s knowledge.
In this paper, we tackle this issue from the perspective of gradient similarity between the teacher and the student dur-ing the KD training process. We first analyze that the ca-pacity mismatch does not continuously happen in the train-ing stage by checking the network representation similar-ity. We then formulate the knowledge distillation as multi-task learning and present an adaptive knowledge distilla-tion approach that can be adapted based on target student model, named as Student Customized Knowledge Distil-lation (SCKD). As a result, the SCKD performs different knowledge distillation strategies for different student mod-els and ideally allocates the optimal knowledge transfer pro-cess. The Figure 1 shows a comparison of SCKD and prior approaches. Our framework makes no restrictions on the number of knowledge, knowledge types (i.e., single teacher, multi-teachers, or self-distillation), and place to perform distillations (i.e., on intermediate representation or output space). It can plugin into any existing knowledge distilla-tion framework and improve the student performance im-mediately. Additionally, our approach is applicable to a va-riety of vision tasks, including image recognition, object de-tection, and semantic segmentation. Our contributions are summarized as follows:
• We propose an adaptive knowledge distillation, named as Student Customized Knowledge Distillation (SCKD) method. The SCKD can automatically ad-just the KD process based on the target student model, which is achieved by calculating the gradient similar-ity between the teacher’s distillation loss and student loss during training.
• The proposed SCKD shows evident advantages over conventional knowledge distillation approaches on various visual tasks, and show immediate performance improvement by inserting into existing knowledge dis-tillation framework. 2.