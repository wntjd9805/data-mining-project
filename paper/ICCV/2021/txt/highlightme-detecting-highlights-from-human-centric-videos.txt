Abstract
We present a domain- and user-preference-agnostic ap-proach to detect highlightable excerpts from human-centric videos. Our method works on the graph-based represen-tation of multiple observable human-centric modalities in the videos, such as poses and faces. We use an autoencoder network equipped with spatial-temporal graph convolutions to detect human activities and interactions based on these modalities. We train our network to map the activity- and interaction-based latent structural representations of the different modalities to per-frame highlight scores based on the representativeness of the frames. We use these scores to compute which frames to highlight and stitch contigu-ous frames to produce the excerpts. We train our network on the large-scale AVA-Kinetics action dataset and evalu-ate it on four benchmark video highlight datasets: DSH,
TVSum, PHD2, and SumMe. We observe a 4–12% improve-ment in the mean average precision of matching the human-annotated highlights over state-of-the-art methods in these datasets, without requiring any user-provided preferences or dataset-specific fine-tuning. 1.

Introduction
Human-centric videos focus on human activities, tasks, and emotions [62, 50]. These videos form a major part of the rapidly growing volume of online media [8], coming from multiple domains, such as amateur sports and perfor-mances, lectures, tutorials, video weblogs (vlogs), and indi-vidual or group activities, e.g., cookouts and holiday trips.
However, unedited human-centric videos also tend to con-tain large chunks of irrelevant and uninteresting content, re-quiring them to be edited for efficient browsing [47].
To address this problem, researchers have developed multiple techniques for detecting highlightable excerpts and summarizing videos [11, 53, 42, 63, 44, 67]. Given unedited footage, highlight detection obtains the moments of interest,
*Work done while Uttaran an intern at Adobe Research
Figure 1: Detecting highlight excerpts using human-centric modalities. Our method leverages multiple human-centric modalities, e.g., body poses and faces, observable in videos focusing on human activities, to detect highlights.
We use a 2D or 3D interconnected point representation of each modality to construct a spatial-temporal graph repre-sentation to compute the highlight scores. and summarization computes the most relevant and repre-sentative set of excerpts. Detecting effective highlights not only expedites browsing, but also improves the chances of those highlights being shared and recommended [53]. Cur-rent methods can learn to detect these excerpts given anno-tated highlights [47, 11], or sets of exemplars for different highlight categories, e.g., learning from skiing images to detect skiing excerpts from videos [23, 25]. Other methods obviate the need for supervision by learning the represen-tativeness of each frame or shot with respect to the origi-nal video [36] and exploiting video metadata such as dura-tion [53] and relevance of shots [67, 64]. All these methods either assume or benefit from some domain-specific knowl-edge of the unedited footage, e.g., running and jumping may be more relevant in a parkour video, whereas sliding maneuvers may be more relevant in a skiing video. Alter-native methods do not consider domain-specific knowledge but consider the pre-recorded preferences of multiple users instead to detect personalized highlights [42].
Whether they assume domain-specific knowledge or user-preferences, existing methods work in the 2D image space of the frames or shots constituting the videos. State-of-the-art image-based networks can learn rich semantic features capturing the interrelations between the various detected objects in the images, leading to efficient high-light detection. However, these approaches do not explic-itly model human activities or inter-person interactions that are the primary focus of human-centric videos. Develop-ing methods for human-centric videos, meanwhile, has been essential for a variety of tasks, including expression and emotion recognition [34, 2, 38], activity recognition [56], scene understanding [50, 32], crowd analysis [51], video super-resolution [32], and text-based video grounding [48].
These methods show that human-centric videos need to be treated separately from generic videos, by leveraging human-centric modalities such as poses and faces. There-fore, there is both the scope and the need to bring the ma-chineries of human-centric video understanding to the task of highlight detection as well.
Main contributions. We develop an end-to-end learning system that detects highlights from human-centric videos without requiring domain-specific knowledge, highlight an-notations, or exemplars. Our approach utilizes the hu-man activities and interactions that are expressed through multiple sensory channels or modalities, including faces, eyes, voices, body poses, and hand gestures [1, 38]. We use graph-based representations for all the human-centric modalities to sufficiently represent how the inherent struc-ture of each modality evolves with various activities and interactions over time. Our network learns from these graph-based representations using spatial-temporal graph convolutions and maps the per-frame modalities to highlight scores using an autoencoder architecture. Our highlight scores are based on the representativeness of all the frames in the videos, and we stitch together contiguous frames to produce the final excerpts. Our novel contributions include:
• Highlight detection with human-centric modalities.
Our method identifies the observable modalities, such as poses and faces, in each input video and encodes their inter-relations, across both time and different per-sons, into highlight scores for highlight detection.
• Annotation-free training of highlight scores. We do not require highlight annotations, exemplars, user-preferences, or domain-specific knowledge.
Instead, we only need to detect of one or more human-centric modalities using off-the-shelf modality detection tech-niques to train our highlight scores.
• Domain- and user-agnostic performance.
Our trained network achieves state-of-the-art performance in highlight detection over a diverse range of domains and user preferences, evaluated over multiple bench-mark datasets consisting of human-centric videos.
Our method achieves a mean average precision of 0.64 and 0.20 of matching human-annotated highlight excerpts on the benchmark domain-specific video highlight (DSH) dataset [47] and the personal highlight detection dataset (PHD2) [11] dataset, respectively, and outperform the corre-sponding state-of-the-art methods by 7% and 4% (absolute).
We also achieve state-of-the-art performance on the smaller benchmark datasets of TVSum [46] and SumMe [15], out-performing the current state-of-the-art baselines by 12% and 4% (absolute) on the mean average precision and mean
F-score, respectively. Even for domains that are not fully human-centric (e.g., dog shows) or videos where human-centric modalities are sparsely detected, the performance of our method is comparable to the current state-of-the-art. 2.