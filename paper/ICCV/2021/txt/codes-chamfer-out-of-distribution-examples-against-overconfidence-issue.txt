Abstract
Overconfident predictions on out-of-distribution (OOD) samples is a thorny issue for deep neural networks. The key to resolve the OOD overconfidence issue inherently is to build a subset of OOD samples and then suppress pre-dictions on them. This paper proposes the Chamfer OOD examples (CODEs), whose distribution is close to that of in-distribution samples, and thus could be utilized to allevi-ate the OOD overconfidence issue effectively by suppressing predictions on them. To obtain CODEs, we first generate seed OOD examples via slicing&splicing operations on in-distribution samples from different categories, and then feed them to the Chamfer generative adversarial network for distribution transformation, without accessing to any extra data. Training with suppressing predictions on CODEs is validated to alleviate the OOD overconfidence issue largely without hurting classification accuracy, and outperform the state-of-the-art methods. Besides, we demonstrate CODEs are useful for improving OOD detection and classification. 1.

Introduction
Deep neural networks (DNNs) have obtained state-of-the-art performance in the classification problem [16].
Since those classification systems are generally designed for a static and closed world [3], DNN classifiers will at-tempt to make predictions even with the occurrence of new concepts in real world. Unfortunately, those unexpected predictions are likely to be overconfident. Indeed, a grow-ing body of evidences show that DNN classifiers suffer from the OOD overconfidence issue of being fooled easily to gen-erate overconfident predictions on OOD samples [37, 13].
A widely adopted solution is to calibrate the outputs between in- and out-of-distribution samples to make them
*Joint first authors.
†Corresponding authors: wlpeng@gzhu.edu.cn, zqgu@gzhu.edu.cn.
Figure 1.
For a classification task, (a) OOD samples can be infinitely outnumber ID samples; (b) intuitively, OOD samples, whose distribution is closer to that of ID samples, are more likely to be effective against the OOD overconfidence issue; (c) we aim at generating CODEs, a kind of effective OOD examples. easy to detect [18, 31]. By this way, overconfident predic-tions on OOD samples could be rejected as long as identi-fied by OOD detectors. While those approaches are signifi-cant steps towards reliable classification, the OOD overcon-fidence issue of DNN classifiers remains unsolved. Besides, as challenged by Lee et al. [27], the performance of OOD detection highly depends on DNN classifiers and they fail to work if the classifiers do not separate the predictive dis-tribution well. This motivates us to resolve the OOD over-confidence issue inherently with enforcing DNN classifiers to make low-confident predictions on OOD samples.
Since with infinite amount, the key to resolve the OOD overconfidence issue is to build a subset of OOD samples and then suppress predictions on them. Lee et al. [27] use a generative adversarial network [12] to model the sub-set which, however, requires to be tuned on the testing-distribution. Without synthesizing data from a carefully de-signed distribution, Hendrycks et al. [19] adopt an auxiliary dataset to simulate the subset. However, the optimal choice of such dataset remains an open question, challenges of data imbalance and computational complexity make it less effi-cient and practical [30]. In contrast, Hein et al. [17] sim-ply adopt random noises and permuted images, and report promising results. It thus brings us to the main topic of this paper (see Fig. 1): can we get a subset of OOD samples that is more effective for alleviating the OOD overconfi-dence issue by suppressing predictions on them? Intuitively, suppressing a subset of OOD samples whose distribution is close to that of in-distribution (ID) samples, are expected to bring more benefits, since they are harder to be differen-tiated by DNNs to make low-confident predictions. Under this hypothesis, we propose to generate a subset of OOD samples whose distribution is close to that of ID samples, i.e., effective OOD examples.
In this paper, we propose the novel Chamfer OOD exam-ples (CODEs), which is a kind of effective OOD examples.
Besides, we devise a simple yet effective method to generate
CODEs with training data only. Specifically, we first gen-erate seed examples that is OOD by slicing&splicing oper-ations, and then feed them into the Chamfer generative ad-versarial network (Chamfer GAN) for distribution transfor-mation. Particularly, the Chamfer distance loss is intention-ally imposed on Chamfer GAN to maintain the pixel-level statistic of seed examples, such that CODEs remain to be
OOD. We validate the effectiveness of our approach by sup-pressing predictions on CODEs during training. Extensive experiments show that the OOD overconfidence issue will be largely alleviated by our approach without hurting the original classification accuracy on ID samples, and that our approach outperforms the state-of-the-art methods. We also demonstrate CODEs could have broad applications, e.g., to improve OOD detectors and image classification.
Overall, our contribution is summarized as follows:
• We show distribution distance is the key factor for
OOD examples in alleviating the OOD overconfidence issue, with many other factors excluded.
• We propose a simple yet effective method based on slicing&splicing operations and Chamfer GAN to gen-erate CODEs without accessing to any extra data.
• We validate the superiority of CODEs in alleviating the OOD overconfidence issue of DNN classifiers in-herently without hurting the classification accuracy.
• We demonstrate the effectiveness of CODEs in im-proving OOD detection and image classification. 2.