Abstract
Recent studies imply that deep neural networks are vul-nerable to adversarial examples, i.e., inputs with a slight but intentional perturbation are incorrectly classified by the network. Such vulnerability makes it risky for some security-related applications (e.g., semantic segmentation in autonomous cars) and triggers tremendous concerns on the model reliability. For the first time, we comprehensively eval-uate the robustness of existing UDA methods and propose a robust UDA approach. It is rooted in two observations: i) the robustness of UDA methods in semantic segmentation remains unexplored, which poses a security concern in this field; and ii) although commonly used self-supervision (e.g., rotation and jigsaw) benefits model robustness in classifica-tion and recognition tasks, they fail to provide the critical supervision signals that are essential in semantic segmen-tation. These observations motivate us to propose adver-sarial self-supervision UDA (or ASSUDA) that maximizes the agreement between clean images and their adversarial examples by a contrastive loss in the output space. Extensive empirical studies on commonly used benchmarks demon-strate that ASSUDA is resistant to adversarial attacks. 1.

Introduction
Semantic segmentation aims to predict semantic labels of each pixel in the given images, which plays an important role in autonomous driving [19] and medical diagnosis [28].
However, pixel-wise labeling is extremely time-consuming and labor-intensive. For instance, 90 minutes are required to annotate a single image for the Cityscapes dataset [6].
Although synthetic datasets [29, 30] with freely available labels provide an opportunity for model training, the model trained on synthetic data suffers from dramatic performance degradation when applying it directly to the real data of interest.
Motivated by the success of unsupervised domain adapta-tion (UDA) in image classification, various UDA methods for semantic segmentation are recently proposed. The key idea of these methods is to learn domain-invariant representations by minimizing marginal distribution distance between the source and target domains [15], adapting structured output space [38, 5], or reducing appearance discrepancy through image-to-image translation [1, 51, 18]. Another alternative is to explicitly explore the supervision signals from the target domain through self-training. The key idea is to alterna-tively generate pseudo labels on target data and re-train the model with these labels. Most of the existing state-of-the-art
UDA methods in semantic segmentation rely on this strat-egy and demonstrate significant performance improvement.
[54, 18, 48, 44, 31].
However, one of the critical issues of the aforementioned
UDA methods is that they are possibly vulnerable to adver-sarial attacks. In other words, the performance of a UDA model may dramatically degrade under an unnoticeable per-turbation. Unfortunately, the robustness of UDA methods remains largely unexplored in the literature. With the in-creasing applications of UDA methods in security-related areas, the lack of robustness of these methods leads to mas-sive safety concerns. For instance, even small-magnitude perturbations on traffic signs can potentially cause disas-trous consequences to autonomous cars [9, 33], such as life-threatening accidents.
Self-supervised learning (SSL) aims to learn more trans-ferable and generalizable features for vision tasks (e.g., clas-sification and recognition) [8, 10, 12, 4]. Key to SSL is the design of pretext tasks, such as rotation prediction, selfie, and jigsaw, to obtain self-derived supervisory signals on un-labeled data. Recent studies reveal that SSL is effective in improving model robustness and uncertainty [13]. However, commonly used pretext tasks are designed to capture the global representation of a given image or an image patch.
Such pretext tasks fail to provide critical supervision sig-nals for segmentation tasks where fine-grained or pixel-level representations are required [49].
In this paper, we first perform a comprehensive study to evaluate the robustness of existing UDA methods in seman-tic segmentation. Our results reveal that these methods can be easily fooled by small perturbations and show dramatic performance degradation. To remedy this problem, we in-troduce a new UDA method known as ASSUDA to robustly adapt domain knowledge in urban-scene semantic segmen-tation. The key insight of our method is to leverage the regularization power of adversarial examples. Specifically, we propose the adversarial self-supervision that maximizes the agreement between clean images and their adversarial examples by a contrastive loss in the output space. The ad-versarial examples aim to i) provide fine-grained supervision signals for unlabeled target data, so that more transferable and generalizable features can be learned and ii) improve the robustness of our model against adversarial attacks by taking advantage of both adversarial training and self-supervision.
Our main contributions can be summarized as i) To the best of our knowledge, this paper presents the first systematic study on how existing UDA methods in semantic segmen-tation are vulnerable to adversarial attacks. We believe this investigation provides new insight into this area; ii) We pro-pose a new UDA method that takes advantage of adversarial training and self-supervision to improve the model robust-ness; iii) Comprehensive empirical studies demonstrate the robustness of our method against adversarial attacks on two benchmark settings, i.e., ”GTA5 to Cityscapes” and ”SYN-THIA to Cityscapes”. 2.