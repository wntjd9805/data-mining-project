Abstract
The past year has witnessed the rapid development of applying the Transformer module to vision problems. While some researchers have demonstrated that Transformer-based models enjoy a favorable ability of fitting data, there are still growing number of evidences showing that these models suffer over-fitting especially when the train-ing data is limited. This paper offers an empirical study by performing step-by-step operations to gradually transit a Transformer-based model to a convolution-based model.
The results we obtain during the transition process deliver useful messages for improving visual recognition. Based on these observations, we propose a new architecture named
Visformer, which is abbreviated from the ‘Vision-friendly
Transformer’. With the same computational complexity,
Visformer outperforms both the Transformer-based and convolution-based models in terms of ImageNet classifica-tion accuracy, and the advantage becomes more significant when the model complexity is lower or the training set is smaller. The code is available at https://github. com/danczs/Visformer. 1.

Introduction
In the past decade, convolution used to play a central role in the deep learning models [22, 29, 31, 15] for visual recognition. This situation starts to change when the Trans-former [35], a module that originates from natural language processing [35, 13, 25], is transplanted to the vision scenar-ios. It was shown in the ViT model [14] that an image can be partitioned into a grid of patches and the Transformer is directly applied upon the grid as if each patch is a visual word. ViT requires a large amount of training data (e.g., the ImageNet-21K [12] or the JFT-300M dataset), arguably because the Transformer is equipped with long-range atten-tion and interaction and thus is prone to over-fitting. The
*Corresponding author: Jianwei Niu
Network
FLOPs (G)
Parameters (M) base setting
Full data elite setting
Part of 10% labels 10% classes data
ResNet-50 DeiT-S Visformer-S 4.6 21.8 63.12 80.07 40.41 80.06 4.9 40.2 77.20 82.19 58.74 90.06 4.1 25.6 77.43 78.73 58.37 89.90
Table 1. The comparison among ResNet-50, DeiT-S, and the pro-posed Visformer-S model in ImageNet classification. Although
DeiT-S performs well under the elite setting, its performance drops dramatically when the base setting is used or when fewer data are used for training. In comparison, Visformer-S is friendly to both the base and elite settings, and reports smaller accuracy drops us-ing a limited number of training data. Please refer to the main texts for the detailed settings. follow-up efforts [34] improved ViT to some extent, but these models still perform badly especially under limited training data or moderate data augmentation compared with convolution-based models.
On the other hand, vision Transformers can achieve much better performance than convolution-based mod-els when trained with large amount of data. Namely, vision Transformers have higher
‘upper-bound’ while convolution-based models are better in ‘lower-bound’. Both upper-bound and lower-bound are important properties for neural networks. Upper-bound is the potential to achieve higher performance and lower-bound enables networks to perform better when trained with limited data or scaled to different complexity.
Based on the observation of lower-bound and upper-bound on Transformer-based and convolution-based net-works, the main goal of this paper is to identify the reasons behind the difference, by which we can design networks with higher lower-bound and upper-bound. The gap be-tween Transformer-based and convolution-based networks can be revealed with two different training settings on Ima-geNet. The first one is the base setting. It is the standard set-ting for convolution-based models, i.e., the training sched-ule is shorter and the data augmentation only contains basic operators such as random-size cropping [32] and flipping.
The performance under this setting is called base perfor-mance in this paper. The other one is the training setting used in [34].
It is carefully tuned for Transformer-based models, i.e., the training schedule is longer and the data augmentation is stronger (e.g., RandAugment [11], Cut-Mix [41], etc., have been added). We use the elite per-formance to refer to the accuracy produced by it.
We take DeiT-S [34] and ResNet-50 [15] as the exam-ples of Transformer-based and convolution-based models.
As shown in Table 1, Deit-S and ResNet-50 employ com-parable FLOPs and parameters. However, they behave very differently trained on the full data under these two settings.
Deit-S has higher elite performance, but changing the set-ting from elite to base can cause a 10%+ accuracy drop for DeiT-S. ResNet-50 performs much better under the base setting, yet the improvement for the elite setting is merely 1.3%. This motivates us to study the difference between these models. With these two settings, we can roughly esti-mate the lower-bound and upper-bound of the models. The methodology we use is to perform step-by-step operations to gradually transit one model into another, by which we can identify the properties of modules and designs in these two networks. The entire transition process, taking a total of 8 steps, is illustrated in Figure 1.
Specifically, from DeiT-S to ResNet-50, one should (i) use global average pooling (not the classification token), (ii) introduce step-wise patch embeddings (not large patch flat-tening), (iii) adopt the stage-wise backbone design, (iv) use batch normalization [20] (not layer normalization [1]), (v) leverage 3 × 3 convolutions, (vi) discard the position em-bedding scheme, (vii) replace self-attention with convolu-tion, and finally (viii) adjust the network shape (e.g., depth, width, etc.). After a thorough analysis on the reasons behind the results, we absorb all the factors that are helpful to vi-sual recognition and derive the Visformer, i.e., the Vision-friendly Transformer.
Evaluated on ImageNet classification, Visformer claims better performance than the competitors, DeiT and ResNet, as shown in Table 1. With the elite setting, the Visformer-S model outperforms DeiT-S and ResNet-50 by 2.12% and 3.46%, respectively, under a comparable model complexity.
Different from Deit-S, Visformer-S also survives two extra challenges, namely, when the model is trained with 10% la-bels (images) and 10% classes. Visformer-S even performs better than ResNet-50, which reveals the high lower-bound of Visformer-S. Additionally, for tiny models, Visformer-Ti significantly outperforms Deit-Ti by more than 6%.
The contribution of this paper is three-fold. First, for the first time, we introduce the lower-bound and upper-bound to investigate the performance of Transformer-based vision models. Second, we close the gap between the
Transformer-based and convolution-based models by a gradual transition process and thus identify the properties of the designs in the Transformer-based and convolution-based models. Third, we propose the Visformer as the final model that achieves satisfying lower-bound and upper-bound and enjoys good scalability at the same time. 2.