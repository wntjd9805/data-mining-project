Abstract
We present DepthInSpace, a self-supervised deep-learning method for depth estimation using a structured-light camera. The design of this method is motivated by the commercial use case of embedded depth sensors in nowadays smartphones. We ﬁrst propose to use estimated optical ﬂow from ambient information of multiple video frames as a complementary guide for training a single-frame depth estimation network, helping to preserve edges and reduce over-smoothing issues. Utilizing optical ﬂow, we also propose to fuse the data of multiple video frames
In particular, fused to get a more accurate depth map. depth maps are more robust in occluded areas and incur less in ﬂying pixels artifacts. We ﬁnally demonstrate that these more precise fused depth maps can be used as self-supervision for ﬁne-tuning a single-frame depth estimation network to improve its performance. Our models’ effec-tiveness is evaluated and compared with state-of-the-art models on both synthetic and our newly introduced real datasets. The implementation code, training procedure, and both synthetic and captured real datasets are available at https://www.idiap.ch/paper/depthinspace. 1.

Introduction
With the advent of structured-light cameras, depth-sensing became conceivable with basic algorithms imple-mentable on devices with computational constraints in real-time. For instance, Kinect V1 uses a correlation-based block matching technique [36], and Intel RealSense [22] employs a semi-global matching scheme [16]. However, learning-based approaches in this ﬁeld are relatively lim-ited. Fanello et al. [35] propose a computationally efﬁcient feature matching method. Projecting image patches to com-pact binary representation is proposed in UltraStereo [10] to achieve a low complex matching scheme. HyperDepth [34] casts the problem of depth estimation as a classiﬁcation-regression task, which it solves using an ensemble of cas-caded random forests. However, HyperDepth assumes the availability of ground-truth labels either from high-accuracy sensors or exhaustive stereo-matching search algorithms.
Due to the lack of large-scale, precise ground-truth data, an end-to-end training of a deep neural network in a self-supervised manner has been at the center of attention re-cently. ActiveStereoNet [49] uses Siamese networks for predicting disparity and proposes a novel photometric loss function based on a Local Contrast Normalization (LCN) scheme for training. A separate color sensor is used in [24] to enhance the performance of [49]. Riegler et al. [33] ex-ploit the photometric loss function of [49] and propose an edge-detection network along with an edge-aware smooth-ness loss function to overcome the issue of edge fattening.
They also introduce another loss function that leverages the information of other video frames to supervise the disparity estimation network’s training. To do so, they use the esti-mated disparity and camera pose parameters to transform pixels into a 3D point cloud and apply the consistency of predicted depth of matched pixels across multiple frames.
We take the work in [33] as the baseline, and our contri-butions in this article are as follows:
• We propose a novel training scheme that uses op-tical ﬂow predictions from ambient images to ﬁnd matched pixels independently of the estimated dispar-ities, which stabilizes the training and enhances accu-racy. Our sensor can capture ambient images conve-niently, and we exploit this feature in this regard.
• We extend this model to fuse information from multi-ple video frames to obtain more precise disparity maps with sharper edges and fewer artifacts.
• We ﬁnally propose to exploit the resulting fused dis-parity maps to ﬁne-tune a single-frame disparity esti-mation network. 2.