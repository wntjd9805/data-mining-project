Abstract
Adversarial learning strategy has demonstrated remark-able performance in dealing with single-source Domain
Adaptation (DA) problems, and it has recently been applied to Multi-source DA (MDA) problems. Although most exist-ing MDA strategies rely on a multiple domain discriminator setting, its effect on the latent space representations has been poorly understood. Here we adopt an information-theoretic approach to identify and resolve the potential adverse effect of the multiple domain discriminators on MDA: disintegra-tion of domain-discriminative information, limited compu-tational scalability, and a large variance in the gradient of the loss during training. We examine the above issues by situating adversarial DA in the context of information regu-larization. This also provides a theoretical justification for using a single and unified domain discriminator. Based on this idea, we implement a novel neural architecture called a
Multi-source Information-regularized Adaptation Networks (MIAN). Large-scale experiments demonstrate that MIAN, despite its structural simplicity, reliably and significantly outperforms other state-of-the-art methods. 1.

Introduction
Although a large number of studies have demonstrated the ability of deep learning to solve challenging tasks, the problems are mostly confined to a similar type or a single domain. One remaining challenge is the problem known as domain shift [15], where a direct transfer of informa-tion gleaned from a single source domain to unseen target domains may lead to significant performance impairment.
Domain adaptation (DA) approaches aim to mitigate this problem by learning to map data of both domains onto a common feature space. Whereas several theoretical results
[3, 45] and algorithms for DA [23, 25, 11] have focused on the case in which only a single-source domain dataset is given, we consider a more challenging and generalized problem of knowledge transfer, referred to as Multi-source unsupervised DA (MDA). Following a seminal theoretical result on MDA [2], many deep MDA approaches have been proposed, mainly depend on the adversarial framework.
Most of existing adversarial MDA works [44, 46, 21, 48, 47, 43] have focused on approximating all combinations of pairwise domain discrepancy between each source and the target, which inevitably necessitates training of multiple binary domain discriminators. While substantial technical advances have been made in this regard, the pitfalls of using multiple domain discriminators have not been fully stud-ied. This paper focuses on the potential adverse effects of using multiple domain discriminators on MDA in terms of both quantity and quality. First, the domain-discriminative information is inevitably distributed across multiple discrim-inators. For example, such discriminators primarily focus on domain shift between each source and the target, while the discrepancies between the source domains are neglected.
Moreover, the multiple source-target discriminator setting often makes it difficult to approximate the combined H-divergence between mixture of sources and the target domain because each discriminator is deemed to utilize the samples only from the corresponding source and target domain as inputs. Compared to a bound using combined divergence, a bound based on pairwise divergence is not sufficiently flexible to accommodate domain structures [2]. Second, the computational load of the multiple domain discriminator setting rapidly increases with the number of source domains (O(N )), which significantly limits scalability. Third, it could undermine the stability of training, as earlier works solve multiple adversarial min-max problems.
To overcome such limitations, instead of relying on mul-tiple pairwise domain discrepancy, we constrain the mutual information between latent representations and domain la-bels. The contribution of this study is summarized as fol-lows. First, we show that such mutual information regular-ization is closely related to the explicit optimization of the
H-divergence between the source and target domains. This affords the theoretical insight that the conventional adversar-ial DA can be translated into an information-theoretic regu-larization problem. Second, from these theoretical findings we derive a new optimization problem for MDA: minimiz-ing adversarial loss over multiple domains with a single do-main discriminator. The algorithmic solution to this problem is called Multi-source Information regularized Adaptation
Networks (MIAN). Third, we show that our single domain discriminator setting serves to penalize every pairwise com-bined domain discrepancy between the given domain and the mixture of the others. Moreover, by analyzing exist-ing studies in terms of information regularization, we found another negative effect of the multiple discriminators set-ting: significant increase in the variance of the stochastic gradients.
Despite its structural simplicity, we demonstrated that
MIAN works efficiently across a wide variety of MDA sce-narios, including the DIGITS-Five [30], Office-31 [32], and
Office-Home datasets [41]. Intriguingly, MIAN reliably and significantly outperformed several state-of-the-art methods, including ones that employ a domain discriminator sepa-rately for each source domain [44] and that align the mo-ments of deep feature distribution for every pairwise domain
[30]. 2.