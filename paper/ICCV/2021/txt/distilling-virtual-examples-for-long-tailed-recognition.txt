Abstract
We tackle the long-tailed visual recognition problem from the knowledge distillation perspective by proposing a
Distill the Virtual Examples (DiVE) method. Speciﬁcally, by treating the predictions of a teacher model as virtual exam-ples, we prove that distilling from these virtual examples is equivalent to label distribution learning under certain con-straints. We show that when the virtual example distribu-tion becomes ﬂatter than the original input distribution, the under-represented tail classes will receive signiﬁcant im-provements, which is crucial in long-tailed recognition. The proposed DiVE method can explicitly tune the virtual exam-ple distribution to become ﬂat. Extensive experiments on three benchmark datasets, including the large-scale iNat-uralist ones, justify that the proposed DiVE method can signiﬁcantly outperform state-of-the-art methods. Further-more, additional analyses and experiments verify the virtual example interpretation, and demonstrate the effectiveness of tailored designs in DiVE for long-tailed problems. 1.

Introduction
Deep convolutional neural networks have achieved re-markable success in various ﬁelds of computer vision, part of which should be attributed to rich and representative datasets. Manually-built datasets are often well-designed and roughly balanced, with sufﬁcient samples for every cat-egory, e.g., ImageNet ILSVRC 2012 [22]. In the real world, however, image data are often inherently long-tailed. A few categories (the “head” categories) contain most training im-ages, while most categories (the “tail” ones) have only few samples. Some recently released datasets start to draw our attention to this practical setting, e.g., iNaturalist [4] and
LVIS [8]. These datasets show a naturally long-tailed dis-*J. Wu is the corresponding author and supported by the National Nat-ural Science Foundation of China (No. 61772256 and 61921006). X.-S.
Wei was supported by the Fundamental Research Funds for Central Uni-versities (No. 30920041111) and CAAI-Huawei MindSpore Open Fund (CAAIXSJLJJ-2020-022A). tribution. Models trained on them are easily biased towards the head classes, while the tail categories often have much lower accuracy rates compared to the head ones. This bias is, of course, not welcome by researchers or practitioners.
Many attempts have been made to deal with long-tailed recognition [16, 3, 1, 34, 15, 14].
In particular, resam-pling makes a more balanced distribution through under-sampling [10, 5] the head classes or oversampling the tail classes [2, 9, 23]. Another direction, reweighting, is to assign higher costs for tail categories in novel loss func-tions [3, 25, 16]. Recent methods [34, 15] also decouple the training of the backbone network and the classiﬁer part.
These methods, however, never cross the category bound-ary. That is, resampling, reweighting, and decoupling all happen independently inside each category, and there is no interaction among different categories.
A simple but interesting experiment motivated us to uti-lize cross-category interactions for long-tailed recognition.
The complete CIFAR-100 dataset is balanced, and a highly imbalanced subset of it (i.e., CIFAR-100-LT) is a widely used benchmark for long-tailed recognition [34]. We use the entire CIFAR-100 training set to train a (teacher) net-work, and then use knowledge distillation [12] to distill a student network on the long-tailed CIFAR-100-LT with im-balance factor 100. The student’s test accuracy is 61.58%, which is signiﬁcantly (more than 10 percentage points) higher than existing long-tail recognition methods (c.f . Ta-ble 1)! Then, what makes its accuracy so high aside from the teacher being trained using the entire training set (which is not available in our long-tailed setting for the student)?
Our answer to this question is two-fold: virtual examples
& knowledge distillation, or in short, distilling the virtual examples.
In a dog vs. cat binary recognition problem, if the predic-tion for a dog image is (0.7, 0.3), we interpret this predic-tion as two virtual examples: 0.7 dog virtual example, plus 0.3 cat virtual example. This interpretation extends natu-rally to the multiclass case. If dog is a head category and cat is a tail category, the 0.3 cat virtual example will help recognize cats, even if the input image is in fact a dog.
empirically that the ﬂatter the teacher’s virtual example distribution, the higher the student’s accuracy is.
• To level and to distill the virtual example distribution (DiVE). Noticing that even FULL in Fig. 1 is still long-tailed, we propose methods to make the virtual exam-ple distribution balanced, and then distill from it, which directly and explicitly learns from the balanced virtual example distribution.
As validated by experiments, the proposed DiVE method outperforms existing long-tailed recognition methods by large margins in various long-tailed recognition datasets.
Figure 1. (Virtual) example distribution of different models. 2.