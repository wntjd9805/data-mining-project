Abstract
Recently, deep learning-based image enhancement al-gorithms achieved state-of-the-art (SOTA) performance on several publicly available datasets. However, most existing methods fail to meet practical requirements either for vi-sual perception or for computation efﬁciency, especially for high-resolution images. In this paper, we propose a novel real-time image enhancer via learnable spatial-aware 3-dimentional lookup tables(3D LUTs), which well considers global scenario and local spatial information. Speciﬁcally, we introduce a light weight two-head weight predictor that has two outputs. One is a 1D weight vector used for image-level scenario adaptation, the other is a 3D weight map aimed for pixel-wise category fusion. We learn the spatial-aware 3D LUTs and fuse them according to the aforemen-tioned weights in an end-to-end manner. The fused LUT is then used to transform the source image into the target tone in an efﬁcient way. Extensive results show that our model outperforms SOTA image enhancement methods on public datasets both subjectively and objectively, and that our model only takes about 4ms to process a 4K resolution image on one NVIDIA V100 GPU. 1.

Introduction
Recently, many deep learning-based approaches have been proposed and achieved SOTA results [9, 15, 25, 4, 20, 14, 26, 19, 28] in the ﬁeld of computational imaging. How-ever, complex network architecture and high computation overheads prevent them from real-time processing. Figure 1 shows the comparison of performance and efﬁciency (i.e., execution time) of several network architectures on HDR+
Burst Photography dataset [6]. Most existing methods can-not produce visually pleasant results in real time.
Considering both performance and efﬁciency, it is still a big challenge for image enhancement due to the diver-sity of capture scenarios. Recently, many hybrid meth-ods [8, 25, 30], which combine image prior in traditional
*Authors contributed equally
†Corresponding author
Figure 1: Performance and efﬁciency on HDR+ burst pho-tography dataset of different methods for 480p (640 × 480) and 4K (3840 × 2160) resolution on NVIDIA V100 GPU.
Our method achieves the highest PSNR and the second fastest execution speed. DeepLPF [16] is out of memory on 4K resolution. approaches and multi-level features in deep learning-based approaches, are proposed and achieve SOTA performance.
[25] proposes a new image enhancement method with good image quality, high computation efﬁciency and low mem-ory consumption. However, as the limitations pointed out by authors, it works simply based on pixel values, without considering local information. This may produce less sat-isfactory results in local areas. For example, as shown in
Figure 7, local contrast is limited in some results captured in high dynamic range scenes. In addition, there are also some color distortion and artifacts as shown in Figure 8.
To solve these issues, we present a novel CNN-based im-age enhancement approach, where spatial information is in-troduced to traditional 3D lookup tables to boost its per-formance. Particularly, T spatial-aware 3D LUTs, each of which is a set of M basic learnable 3D LUTs, and a two-head weight predictor are trained simultaneously under a new loss function to balance well between details, col-ors, and visual perception. The weight predictor has two outputs. One is a 1D weight vector with global informa-Figure 2: Overview of our proposed framework. It consists of multiple spatial-aware 3D LUTs (i.e., T spatial-aware 3D
LUTs, each with M basic 3D LUTs selected by M -channel pixel-wise category information.), a self-adaptive two-head weight predictor, and interpolation for spatial-aware 3D LUTs. The weight predictor takes down-sampled images as input and generates two outputs. One is a 1D weight vector used for image-level scenario adaptation, the other is a 3D weight map aimed for pixel-wise category fusion, enabling our LUT-based enhancer with image-adaptive spatial-aware ability. tion used for integration of different LUTs on dimension T, which is called image level scenario adaptation. The other is a 3D weight map with pixel-wise category information aimed for combination of multiple LUTs on dimension M, which is named pixel-wise category fusion. Enhanced im-ages are obtained by fusion of spatial-aware 3D LUTs ac-cording to the aforementioned two kinds of weights. In ad-dition, our approach only takes about 4 ms to process an image of 4K resolution on NVIDIA V100 GPU platform.
The main contributions are summarized as follows:
• We propose a spatial-aware 3D LUTs architecture by constructing multiple basic 3D LUTs and introducing two-head weight predictor. This architecture makes it more robust in local enhancement.
• We design a two-head weight predictor which learns image-level scenario and pixel-wise category informa-tion with low computation overheads. Such weight in-formation combined with spatial-aware 3D LUTs ef-fectively improves the performance of image enhance-ment, and balances well between detail, color and per-ception under the supervision of our loss functions.
• We conduct extensive experiments to compare our ap-proach with existing methods on two public datasets.
Results demonstrate advantages of our approach quan-titatively and qualitatively on performance and efﬁ-ciency. 2.