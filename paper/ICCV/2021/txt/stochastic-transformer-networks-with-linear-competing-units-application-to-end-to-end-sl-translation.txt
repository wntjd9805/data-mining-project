Abstract
Automating sign language translation (SLT) is a chal-lenging real-world application. Despite its societal impor-tance, though, research progress in the field remains rather poor. Crucially, existing methods that yield viable perfor-mance necessitate the availability of laborious to obtain gloss sequence groundtruth.
In this paper, we attenuate this need, by introducing an end-to-end SLT model that does not entail explicit use of glosses; the model only needs text groundtruth. This is in stark contrast to existing end-to-end models that use gloss sequence groundtruth, either in the form of a modality that is recognized at an intermedi-ate model stage, or in the form of a parallel output process, jointly trained with the SLT model. Our approach consti-tutes a Transformer network with a novel type of layers that combines: (i) local winner-takes-all (LWTA) layers with stochastic winner sampling, instead of conventional ReLU layers, (ii) stochastic weights with posterior distributions estimated via variational inference, and (iii) a weight com-pression technique at inference time that exploits estimated posterior variance to perform massive, almost lossless com-pression. We demonstrate that our approach can reach the currently best reported BLEU-4 score on the PHOENIX 2014T benchmark, but without making use of glosses for model training, and with a memory footprint reduced by more than 70%. 1.

Introduction
The Sign Languages (SLs) are the native languages of the Deaf and therefore they are the main communication means within the Deaf communities. The SLs are rich vi-sual languages, that convey information through multiple modalities, which are of complementary nature. Specifi-cally, SLs utilize both manual (hand shape, movement and
*ai.voskou@edu.cut.ac.cy pose), as well as non-manual modalities (e.g., facial ex-pressions, lip movements, head movements, shoulders and torso), to convey salient meanings [30].
Exploiting the latest advances in computer vision and machine learning to facilitate the communication of SL-speakers with SL non-speakers is an endeavor of high po-tential impact to the livelihoods of the Deaf. Automating the process of converting SL video to written language is the goal of SLT (e.g., [3, 5, 4, 37, 25, 27]). This has proven to be a hard task for computer vision algorithms, as a natu-ral consequence of the syntax, of the complex entailed ges-tures, and of the multitude of concurrent modalities that are combined to convey a unique meaning.
Due to these challenges, the computer vision community has traditionally focused on recognizing sequences of sign glosses. These are natural language words that attempt to encode the meaning of SL signs, forming a minimal dic-tionary of indicative lexical items. Thus, the combination of glosses pertaining to some SL video does not constitute translation in natural language; yet, it can help a non-SL speaker get a feeling of what the SL speaker is talking about.
The process of pinpointing glosses in SL videos is usually referred to as sign language recognition (SLR). This dis-tinction is important, as the grammar and the structure of sign and spoken languages are very different. These differ-ences are reflected in the outcome of SLR, whereby there is no simple way of associating recognized glosses to ac-tual words/phrases in natural language. This renders SLR outcomes of limited usefulness in real-world applications.
In an effort to alleviate the limited usefulness of SLR while, at the same time, improving the translation quality of
SLT systems, several researchers have recently considered methods that combine SLR with SLT [3, 5, 37, 4]. Specif-ically, existing methods choose among two alternatives: (i) perform SLR and then translate the sequence of detected glosses into natural language (S2G2T); and (ii) train a mul-titask Deep learning model that jointly performs SLR and
SLT, in a way that the representations learned in the inter-mediate layers are shared among tasks (S2(G+T)). In the most recent works in the field, this is effected by exploiting a state-of-the-art framework for sequential data modeling, namely the Transformer network [34].
Transformer networks [34] currently constitute the state-of-the-art paradigm for sequential data modeling; this in-cludes both sequence-to-sequence modeling tasks and (au-toregressive) density modeling tasks. The main principle of
Transformer networks, which sets them apart from all previ-ous deep learning approaches for sequential data, consists in the use of a neural attention-based mechanism, dubbed self-attention; this captures (long) temporal dynamics within a modeled sequence. Specifically, self-attention is a dot prod-uct attention [21] that draws all queries, keys and values from the same sequence. This way, self-attention is the key mechanism that allows for each position within a sequence to attend all the others; this enables capturing long-range dependencies in the data. In addition, it enables high-scale parallelization of computation, which previous approaches (with recurrent connections) cannot afford.
Existing Transformer network formulations are widely founded upon Dense layers with ReLU activation func-tions. However, several recent works have shown that, by using activation functions employing some sort of stochas-ticity in their operation, one can yield a considerable per-formance improvement, especially in hard machine learn-ing tasks. In this context, [26] yielded a considerable per-formance improvement, without increasing the number of trainable model parameters, by: (i) Replacing ReLU units with blocks of stochastically competing local winner-takes-all (LWTA) linear units. Specifically, each layer is split into blocks of linear units. At each time, only one of the units within a block passes its activation output to the following layer; that is the winner unit. All the rest are zeroed out, thus passing zero values to the following layer. Winner se-lection is performed on the basis of a stochastic sampling procedure, whereby the greater the unit activation value the higher the probability of it being sampled as the winner. (ii) Performing an (approximate) Bayesian treatment of the layer parameters (connection weights), whereby the model infers a full variational posterior over layer weights, instead of simple point-estimates.
In this work, we draw inspiration from these advances, seeking an SLT approach that yields significantly improved
SL translation accuracy. Our most important goal is to de-vise an end-to-end SLT modeling approach that completely obviates the need of using SLR groundtruth information (glosses) as part of the model pipeline; that is, either as an intermediate recognition step (S2G2T paradigm), or as a joint task used to facilitate optimization of the learned in-termediate input representations (S2(G+T)). Achieving this goal may greatly facilitate progress in the field, since con-structing gloss sequences for large training data corpora is an extremely costly and time-consuming process. In addi-tion, our goal is to contribute an SLT method with reduced memory requirements at inference time, as this is important for real-world applications of our technology.
To this end, we devise a novel formulation of Trans-former networks, built of Dense layers that comprise the following innovative arguments: (i) LWTA dense layers with stochastic winner sampling, as opposed to conven-(ii) stochastic connection weights, tional ReLU layers; across the network, with Gaussian posteriors fitted under a variational Bayes rationale; and (iii) a trained network com-pression scheme, which exploits the estimated variance of the fitted variational posteriors of the layer weights. We employ this novel Transformer network paradigm to for-mulate an end-to-end SLT model which does not use gloss sequence groundtruth throughout its modeling pipeline. We demonstrate that the proposed method achieves compara-ble or better results than the state-of-the-art in the most prominent SLT benchmark, namely PHOENIX 2014T. At the same time, our devised model imposes a significantly lower memory footprint compared to the state-of-the-art.
The remainder of this paper is organized as follows: In
Section 2, we briefly present the recent related work in the field of SLT and SLR, putting more emphasis on the lat-In est advances that make use of Transformer networks.
Section 3, we present the proposed SLT method; we first introduce our novel modeling rationale; subsequently, we devise appropriate training and inference algorithms; then, we elaborate on the model compression process, which we eventually use to obtain a scalable, end-to-end trainable SLT model. In Section 4, we perform a thorough experimental evaluation of our proposed approach, combined with a deep ablation study. To this end, we use the PHOENIX 2014T dataset. Finally, in Section 5 we conclude this paper, sum-marizing our results. 2.