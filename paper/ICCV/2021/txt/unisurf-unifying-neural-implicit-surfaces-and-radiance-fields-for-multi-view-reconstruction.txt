Abstract
Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neu-ral radiance fields have revolutionized novel view synthesis.
However, NeRFâ€™s estimated volume density does not admit accurate surface reconstruction. Our key insight is that im-plicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume render-ing using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a syn-thetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks. 1.

Introduction
Capturing the geometry and appearance of 3D scenes from a set of images is one of the cornerstone problems in computer vision. Towards this goal, coordinate-based neu-ral models have emerged as a powerful tool for 3D recon-struction of geometry and appearance within the last years.
Many recent methods employ continuous implicit func-tions parameterized with neural networks as 3D representa-tions of geometry [3, 8, 12, 31, 32, 37, 41, 43, 47, 57] or ap-pearance [34, 38, 39, 40, 47, 52, 61]. These neural 3D repre-sentations have shown impressive performance on geome-try reconstruction and novel view synthesis from multi-view images. Besides the choice of the 3D representation (e.g., occupancy field, unsigned or signed distance field), one key element for neural implicit multi-view reconstruction is the rendering technique. While some of these works represent the implicit surface as level set and hence render the appear-ance from surfaces [38,52,61], others integrate densities by
Figure 1: Illustration. Implicit models based on surface rendering [38, 61] require input masks and radiance fields
[34] do not optimize implicit surfaces directly. UNISURF provides a principled unified formulation, enabling accurate surface reconstruction from images without input masks. drawing samples along the viewing rays [22, 34, 49].
In existing work, surface rendering techniques have shown impressive performance in 3D reconstruction [38, 61]. However, they require per-pixel object masks as input and an appropriate network initialization since surface ren-dering techniques only provide gradient information locally where a surface intersects with a ray. Intuitively speaking, optimizing wrt. local gradients can be seen as an iterative deformation procedure applied to an initial neural surface which is often initialized as a sphere. Additional constraints such as mask supervision are necessary for converging to a valid surface, see Fig. 2 for an illustration. Due to their reliance on masks, surface rendering methods are limited to object-level reconstruction and do not scale to larger scenes.
On contrary, volume rendering methods like NeRF [34] have shown impressive results for novel view synthesis, also for larger scenes. However, surfaces extracted as level sets of the underlying volume density are usually non-smooth
2.