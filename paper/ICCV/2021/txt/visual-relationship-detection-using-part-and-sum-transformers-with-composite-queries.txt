Abstract
Computer vision applications such as visual relationship detection and human object interaction can be formulated as a composite (structured) set detection problem in which both the parts (subject, object, and predicate) and the sum (triplet as a whole) are to be detected in a hierarchical fash-ion. In this paper, we present a new approach, denoted Part-and-Sum detection Transformer (PST), to perform end-to-end visual composite set detection. Different from exist-ing Transformers in which queries are at a single level, we simultaneously model the joint part and sum hypothe-ses/interactions with composite queries and attention mod-ules. We explicitly incorporate sum queries to enable better modeling of the part-and-sum relations that are absent in the standard Transformers. Our approach also uses novel tensor-based part queries and vector-based sum queries, and models their joint interaction. We report experiments on two vision tasks, visual relationship detection and human object interaction and demonstrate that PST achieves state of the art results among single-stage models, while nearly matching the results of custom designed two-stage models. 1.

Introduction
In this paper, we study problems such as visual relation-ship detection (VRD) [29, 21] and human object interac-tion (HOI) [11, 35, 4] where a composite set of a two-level (part-and-sum) hierarchy is to be detected and localized in an image. In both VRD and HOI, the output consists of a set of entities. Each entity, referred to as a “sum”, represents a triplet structure composed of parts: the parts are (subject, object, predicate) in VRD and (human, interaction, object) in HOI. The sum-and-parts structure naturally forms a two-level hierarchical output - with the sum at the root level and the parts at the leaf level. In the general setting for compos-ite set detection, the hierarchy consists of two levels, but the number of parts can be arbitrary.
Many existing approaches in VRD [29, 16, 30, 54, 54]
CNN
Transformer 
Encoder
Part-and-Sum 
Transformer 
Decoder
… (a) Pipeline overview. (b) Part-and-Sum Transformer Decoder.
Figure 1: Overview of visual composite set detection by Part-and-Sum Transformer (PST). and HOI [36, 4, 24, 14, 10] are based on two stage pro-cesses in which some parts (e.g. the subject and the object in VRD) are detected ﬁrst, followed by detection of the as-sociation (sum) and the additional part (the predicate). Sin-gle stage approaches [50, 49, 15, 19] with end-to-end learn-ing for VRD and HOI also exist.
In practice, two-stage approaches produce better performance while single-stage methods are easier to train and use.
The task for an object detector is to detect and local-ize all valid objects in an input image, making the output a set. Though object detectors such as FasterRCNN [33] are considered end-to-end trainable, they perform instance level predictions, and require post-processing using non-maximum suppression to recover the entire set of objects in an image. Recent developments in Transformers [44] and their extensions to object detection [3] enable set-level end-to-end learning by eliminating anchor proposals and non-maximum suppression.
In this paper, we formulate the visual relationship de-tection (VRD) [29, 21] and human object interaction (HOI)
[11, 35, 4] as composite set (two-level hierarchy) detection problems and propose a new approach, part-and-sum Trans-formers (PST) to solve them. PST is different from exist-ing detection transformers where each object is represented by a vector-based query in either a one-level set [3]. We show the importance of establishing an explicit “sum” rep-resentation for the triplet as a whole to be simultaneously
Composite query
Part queries: 
Subject, Object,
Predicate
Sum query: 
A relation
Image memory
CA (I) Part-Sum Interaction (I) Part-Sum Interaction
CA
SA
CA (II) Factorized SA (II) Factorized SA
Inter-relation SA (II) Factorized SA
Sum branch
Part branch
Intra-relation SA
CA
SA (a) Vector based queries (b) Tensor based queries (c) Composite queries
Figure 2: Part-and-Sum Transformer Decoder with Composite queries. We compare various Transformer with various query designs.
Note that CA: Cross-attention layer; SA: Self-attention layer. modeled/engaged with the part queries (e.g., subject, ob-ject, and predicate). Both the global and the part features have also been modeled in the discriminatively trained part-based model (DPM) algorithm [9] though the global and part interactions there are limited to relative spatial loca-tions. To summarize, we develop a new approach, part-and-sum Transformers (PST), to solve the composite set detec-tion problem by creating composite queries and composite attention mechanism to account for both the sum (vector-based query) and parts (tensor-based queries) representa-tions. Using systematic experiments, we study the roles of the sum and part queries and intra- and inter-token attention in Transformers. The effectiveness of the proposed PST al-gorithm is demonstrated in the VRD and HOI tasks. 2.