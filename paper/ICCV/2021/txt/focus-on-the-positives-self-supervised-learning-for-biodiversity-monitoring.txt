Abstract
We address the problem of learning self-supervised rep-resentations from unlabeled image collections. Unlike exist-ing approaches that attempt to learn useful features by max-imizing similarity between augmented versions of each in-put image or by speculatively picking negative samples, we instead also make use of the natural variation that occurs in image collections that are captured using static monitor-ing cameras. To achieve this, we exploit readily available context data that encodes information such as the spatial and temporal relationships between the input images. We are able to learn representations that are surprisingly effec-tive for downstream supervised classification, by first iden-tifying high probability positive pairs at training time, i.e. those images that are likely to depict the same visual con-cept. For the critical task of global biodiversity monitoring, this results in image features that can be adapted to chal-lenging visual species classification tasks with limited hu-man supervision. We present results on four different cam-era trap image collections, across three different families of self-supervised learning methods, and show that careful image selection at training time results in superior perfor-mance compared to existing baselines such as conventional self-supervised training and transfer learning. 1.

Introduction
Learning transferable representations of visual data without requiring explicit semantic supervision at training time is an important and open problem in computer vision.
Recent progress on this front has been impressive, result-ing in self-supervised methods that are capable of learn-ing features that approach, and in some cases even sur-pass, their fully supervised counterparts across a range of downstream tasks [22, 17]. As a result of not having ac-cess to any semantic supervision (e.g. discrete category la-bels in the case of image classification), current best per-forming self-supervised methods typically use aggressive image augmentation strategies to generate different “views” of an input image during training [57, 26, 10, 23, 11]. The
Figure 1. (a) Conventional self-supervised methods are capable of bringing visually similar examples closer in image embedding space. (b) There is often rich context information (e.g. encoding where and when an image was captured) that can also convey sim-ilarity. (c) By combing the complimentary nature of both signals, we can improve the quality of the final embedding space. training time objective then consists of pulling these distinct
“views” of the same image close to one another in feature space. The ability to generate plausible image variation us-ing these manually designed image augmentation strategies would then appear to be a limiting factor in the further ad-vancement of self-supervised learning (SSL).
Current state-of-the-art self-supervised methods have predominantly been designed using image collections origi-nally constructed for supervised learning e.g. [15, 62]. This has necessitated the exploration of different augmentation strategies to introduce appearance variation during train-ing. However, a more natural signal to use is to exploit the fact that image observations that are made close in time and space are very likely to contain the same object in-stance. This form of natural variation has been used in self-supervised learning from video [43, 38, 39] or spatially dis-tributed image collections [28, 3]. More generally, one can think of having access to not only a collection of images at training time, but also potentially rich context information pertaining to when and where each image was captured, in addition to other cues.
The central question that we address in this paper is how to make use of this context information during self-supervised learning to select more useful and varied im-age pairs at training time. The aim is to provide the self-supervised algorithm with “natural” positive image pairs and thus to establish connections in the latent feature space that were not possible with conventional augmentation-based approaches (Figure 1). We evaluate several different approaches and show, perhaps surprisingly, that the choice of images has more of an impact on performance than the underlying self-supervision algorithm. Our analysis is ap-plicable to any self-supervised method that attempts to max-imize the similarity between two “views” of the same vi-sual concept, even those that do not require negative sample pairs [11].
We focus our evaluation on image collections that have been captured using camera traps - also known as “wild-cams” or “trail cameras”. These types of images are com-monly collected for the purpose of biodiversity monitor-ing [58, 35, 6, 48, 7, 21]. Unlike more conventional im-age datasets typically used by the computer vision commu-nity, camera trap images exhibit some interesting proper-ties that make them particularly well suited to evaluating self-supervised learning: (i) Camera trap images are not captured by humans directly, instead the cameras are au-tomatically triggered based on the proximity of nearby an-imals. This overcomes the “iconic” object view bias that tends to be prevalent in datasets like ImageNet [15] or iNat-uralist [52, 51]. They also contain other challenges such as partial depictions of objects due to occlusion, significant scene illumination changes, and strong object and location correlations [6]; (ii) The images are often captured in short temporal bursts and come with rich context data in the form of the time of day, the time of year, and location, and may also include additional information characterizing the local habitat. This information can provide useful clues about what animal species are potentially present at a given loca-tion; (iii) A conservative estimate is that tens of thousands of ever-active cameras are deployed globally [46], necessi-tating large amounts of tedious work on the part of ecolo-gists and conservation biologists to manually annotate the incoming images or to correct errors made by automati-Informative image cally generated classifier predictions. features, derived from self-supervised learning, could sig-nificantly reduce this manual effort and have the potential to be an important tool in aiding the critical task of scalable global biodiversity monitoring.
We make the following three contributions: 1. We explore the benefits of self-supervised learning on four challenging camera trap datasets. We observe that self-supervised features are significantly more ef-fective on average for downstream classification com-pared to widely adopted transfer learning baselines. 2. We show that how images from these datasets are se-lected during self-supervised training has much more impact on the quality of learned features compared to the choice of the underlying self-supervised training loss that is used. 3. While the role of negative image pairs in self-supervised learning has received significant attention, we show that current methods are surprisingly ro-bust to incorrectly selected positive image pairs dur-ing training. This provides an important insight for the design of future self-supervised methods. 2.