Abstract
Deep convolutional networks have recently achieved great success in video recognition, yet their practical re-alization remains a challenge due to the large amount of computational resources required to achieve robust recog-nition. Motivated by the effectiveness of quantization for boosting efficiency, in this paper, we propose a dynamic network quantization framework, that selects optimal pre-cision for each frame conditioned on the input for efficient video recognition. Specifically, given a video clip, we train a very lightweight network in parallel with the recognition network, to produce a dynamic policy indicating which numerical precision to be used per frame in recognizing videos. We train both networks effectively using standard backpropagation with a loss to achieve both competitive per-formance and resource efficiency required for video recog-nition. Extensive experiments on four challenging diverse benchmark datasets demonstrate that our proposed approach provides significant savings in computation and memory us-age while outperforming the existing state-of-the-art meth-ods. Project page: https://cs-people.bu.edu/ sunxm/VideoIQ/project.html. 1.

Introduction
With the availability of large-scale video datasets [5, 36], deep learning models based on 2D/3D convolutional neural networks (CNNs) [6, 52, 48, 28, 17] have dominated the field of video recognition. However, despite impressive performance on standard benchmarks, efficiency remains a great challenge for many resource constrained applications due to the heavy computational burden of deep CNN models.
Motivated by the need of efficiency, existing research efforts mainly focus on either designing compact mod-els [41, 49, 11] or sampling of salient frames for efficient recognition [60, 57, 34]. While these methods have shown promising results, they all use 32-bit precision for process-ing all the frames in a given video, limiting their achievable efficiency. Specifically, orthogonal to the network design, the computational cost of a CNN is directly affected by the
Figure 1: A conceptual overview of our approach. Instead of processing all the video frames with the same 32-bit precision,
VideoIQ learns to dynamically select optimal quantization preci-sion conditioned on input clips for efficient video recognition. It is computationally very efficient to process more informative frames with high precision and less informative ones with lower precision, without sacrificing accuracy. Best viewed in color. bit-width of weights and activations [16, 68, 8], which sur-prisingly as another degree of freedom for efficient video inference, is almost overlooked in previous works. To illus-trate this, let us consider the video in Figure 1, represented by five uniformly sampled frames. A quick glance on the video clearly shows that only the third frame can be processed using 32-bit precision as this is the most informative frame for recognizing the action “Long Jump”, while the rest can be processed at very low precision or even skipped (i.e., pre-cision set to zero) without sacrificing the accuracy (Bottom), resulting in large computational savings compared to pro-cessing all frames with same 32-bit precision, as generally done in mainstream video recognition methods (Top).
Inspired by this observation, we introduce Video
Instance-aware Quantization (VideoIQ), which for the first time advocates a novel input-dependent dynamic network quantization strategy for efficient video recognition. While dynamic network quantization looks trivial and handy at the first glance, we need to address two challenges: (1) how to efficiently determine what quantization precision to use per target instance; and (2) given instance-specific precisions, how can we flexibly quantize the weights and activations
of a single deep recognition network into various precision levels, without additional storage or computation cost.
To address the aforementioned challenges, we propose a simple end-to-end differentiable approach to learn a decision policy that selects optimal precision conditioned on the input, while taking both accuracy and efficiency into account in rec-ognizing complex actions. We achieve this by sampling the policy from a discrete distribution parameterized by the out-put of a lightweight policy network, which decides on-the-fly what precision should be used on a per frame basis. Since these decision functions are discrete and non-differentiable, we train the policy network using standard back-propagation through Gumbel Softmax sampling [24], without resorting to complex reinforcement learning, as in [60, 9, 63]. More-over, instead of storing separate precision-specific models, we train a single deep neural network for action recognition using joint training, which enables us to directly adjust the numerical precision by simply truncating the least signifi-cant bits, without performance degradation. Our proposed approach provides not only high computational efficiency but also significant savings in memory–a practical require-ment of many real-world applications which has been largely ignored by prior works [34, 59, 35, 60].
We conduct extensive experiments on four standard video recognition datasets (ActivityNet-v1.3 [3], FCVID [25],
Mini-Sports1M [28] and Mini-Kinetics [5]) to demonstrate the superiority of our proposed approach over state-of-the-art methods. Our results show that VideoIQ can yield significant savings in computation and memory (e.g., aver-age 26.0% less GFLOPS and 55.8% less memory), while achieving better recognition performance, over the most competitive SOTA baseline [34]. We also discover that the decision policies learned using our method are transferable to unseen classes and videos across different datasets. Fur-thermore, qualitative results suggest that our learned policies correlate with the distinct visual patterns in video frames, i.e., our method utilizes 32-bit full precision only for rele-vant video frames and process non-informative frames at low precision or skip them for computation efficiency. 2.