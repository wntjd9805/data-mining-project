Abstract
This paper presents a generic method for generating full facial 3D animation from speech. Existing approaches to audio-driven facial animation exhibit uncanny or static up-per face animation, fail to produce accurate and plausi-ble co-articulation or rely on person-speciﬁc models that limit their scalability. To improve upon existing models, we propose a generic audio-driven facial animation ap-proach that achieves highly realistic motion synthesis re-sults for the entire face. At the core of our approach is a categorical latent space for facial animation that dis-entangles audio-correlated and audio-uncorrelated infor-mation based on a novel cross-modality loss. Our ap-proach ensures highly accurate lip motion, while also syn-thesizing plausible animation of the parts of the face that are uncorrelated to the audio signal, such as eye blinks and eye brow motion. We demonstrate that our approach outperforms several baselines and obtains state-of-the-art quality both qualitatively and quantitatively. A percep-tual user study demonstrates that our approach is deemed more realistic than the current state-of-the-art in over 75% of cases. We recommend watching the supplemental video before reading the paper: https://github.com/ facebookresearch/meshtalk 1.

Introduction
Speech-driven facial animation is a highly challenging research problem with several applications such as facial animation for computer games, e-commerce, or immersive
VR telepresence. The demands on speech-driven facial an-imation differ depending on the application. Applications such as speech therapy or entertainment (e.g., Animojies or
AR effects) do not require a very presice level of realism in the animation. In the production of ﬁlms, movie dubbing, driven virtual avatars for e-commerce applications or im-mersive telepresence, on the contrary, the quality of speech animation requires a high degree of naturalness, plausibil-ity, and has to provide intelligibility comparable to a natural speaker. The human visual system has been evolutionary
Figure 1. Given a neutral face mesh of a person and a speech signal as input, our approach generates highly realistic face animations with accurate lip shape and realistic upper face motion such as eye blinks and eyebrow raises. adapted to understanding subtle facial motions and expres-sions. Thus, a poorly animated face without realistic co-articulation effects or out of lip-sync is deemed to be dis-turbing for the user.
Psychological literature has observed that there is an im-portant degree of dependency between speech and facial gestures. This dependency has been exploited by audio-driven facial animation methods developed in computer vi-sion and graphics [4, 2]. With the advances in deep learning, recent audio-driven face animation techniques make use of person-speciﬁc approaches [22, 26] that are trained in a su-pervised fashion based on a large corpus of paired audio and mesh data. These approaches are able to obtain high-quality lip animation and synthesize plausible upper face motion from audio alone. To obtain the required training data, high-quality vision-based motion capture of the user is
required, which renders these approaches as highly imprac-tical for consumer-facing applications in real world settings.
Recently, Cudeiro et al. [8] extended this work, by propos-ing a method that is able to generalize across different iden-tities and is thus able to animate arbitrary users based on a given audio stream and a static neutral 3D scan of the user.
While such approaches are more practical in real world set-tings, they normally exhibit uncanny or static upper face animation [8]. The reason for this is that audio does not encode all aspects of the facial expressions, thus the audio-driven facial animation problem tries to learn a one-to-many mapping, i.e., there are multiple plausible outputs for every input. This often leads to over-smoothed results, especially in the regions of the face that are only weakly or even un-correlated to the audio signal.
This paper proposes a novel audio-driven facial anima-tion approach that enables highly realistic motion synthe-sis for the entire face and also generalizes to unseen identi-ties. To this end, we learn a novel categorical latent space of facial animation that disentangles audio-correlated and audio-uncorrelated information, e.g., eye closure should not be bound to a speciﬁc lip shape. The latent space is trained based on a novel cross-modality loss that encourages the model to have an accurate upper face reconstruction inde-pendent of the audio input and accurate mouth area that only depends on the provided audio input. This disentangles the motion of the lower and upper face region and prevents over-smoothed results. Motion synthesis is based on an autoregressive sampling strategy of the audio-conditioned temporal model over the learnt categorical latent space. Our approach ensures highly accurate lip motion, while also be-ing able to sample plausible animations of parts of the face that are uncorrelated to the audio signal, such as eye blinks and eye brow motion. 2.