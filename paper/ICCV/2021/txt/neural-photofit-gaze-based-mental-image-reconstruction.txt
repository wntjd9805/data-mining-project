Abstract
We propose a novel method that leverages human fixa-tions to visually decode the image a person has in mind into a photofit (facial composite). Our method combines three neural networks: An encoder, a scoring network, and a de-coder. The encoder extracts image features and predicts a neural activation map for each face looked at by a human observer. A neural scoring network compares the human and neural attention and predicts a relevance score for each extracted image feature. Finally, image features are aggre-gated into a single feature vector as a linear combination of all features weighted by relevance which a decoder de-codes into the final photofit. We train the neural scoring network on a novel dataset containing gaze data of 19 par-ticipants looking at collages of synthetic faces. We show that our method significantly outperforms a mean baseline predictor and report on a human study that shows that we can decode photofits that are visually plausible and close to the observer’s mental image. 1.

Introduction
Visually decoding images that only exist in peoples’ minds (also known as mental image reconstruction, MIR) has recently started to attract increasing research inter-est across a range of disciplines, including computational neuroscience, computational biology, and computer vision.
MIR is profoundly challenging given that the informa-tion required to succeed in this task is encoded in com-plex neural dynamics in the brain and not easily accessi-ble from the outside. The dominant approach for MIR has been to reconstruct mental images directly from brain ac-tivity recorded using functional magnetic resonance imag-ing [2, 13, 17, 31, 34] or electroencephalography [7, 30].
Another recent line of work has explored other sensing modalities, in particular human eye fixations. Although fix-ations, in comparison, only provide an indirect measure of a person’s mental image and as such complicate the recon-Figure 1. Overview of our method for gaze-based mental image reconstruction. With an image in mind, users search for similar fa-cial features in multiple auxiliary images while their gaze is being recorded. An encoder extracts image features and corresponding neural activation maps from these images. A scoring network pre-dicts the relevance of each image feature by comparing the fixation and neural activation maps. Image features are finally aggregated and decoded into a photofit.
struction task further, they are promising because they are a less obtrusive and more practically useful measure of visual and cognitive processing, e.g., during scene perception [14] or visual search [41]. While several methods have been pro-posed to predict the target of visual search from fixations and image features [1, 3, 26, 33, 40], only two previous works have tried to also visually decode (reconstruct) the search target [24, 25]. These works first predicted the ob-ject class and attributes of the mental image from human eye fixations and then synthesised random samples from the predicted class.
We significantly go beyond this state of the art by proposing a method that – for the first time – reconstructs the specific instance of the mental image from eye fixations and auxilary images, without the system having seen this instance before. We specifically focus on reconstructing fa-cial images given this has high practical value also beyond criminology and is challenging given the large amount of fa-cial appearance details. Our method encodes multiple facial images looked at by an observer into separate feature vec-tors using a Siamese CNN encoder, fuses these vectors into a single feature vector using a novel scoring network, and finally decodes the mental image from this representation.
The scoring network compares neural activation maps for each output feature of the encoder with human fixations to predict how important each feature is for reconstructing the mental image. Our method addresses the scarcity of gaze data at training time by allowing training of the encoder and decoder on large image datasets and only requiring joint im-age and gaze data for training the scoring network.
The specific contributions of our work are threefold:
First, we introduce an annotated dataset of human fixa-tions on synthesised face images during face recognition that lends itself to studying the task of gaze-based mental image reconstruction. Second, we introduce a novel prob-lem formulation and method that, for the first time, allows us to synthesise a photofit – that is, a visual reconstruction of the mental image of a face – from human eye fixations.
Third, using this dataset as well as through a human study, we report on a series of experiments successfully demon-strating gaze-based reconstruction of mental face images1. 2.