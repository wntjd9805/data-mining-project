Abstract
Batch Whitening is a technique that accelerates and sta-bilizes training by transforming input features to have a zero mean (Centering) and a unit variance (Scaling), and by removing linear correlation between channels (Decor-relation). In commonly used structures, which are empir-ically optimized with Batch Normalization, the normaliza-tion layer appears between convolution and activation func-tion. Following Batch Whitening studies have employed the same structure without further analysis; even Batch Whiten-ing was analyzed on the premise that the input of a lin-ear layer is whitened. To bridge the gap, we propose a new Convolutional Unit that in line with the theory, and our method generally improves the performance of Batch
Whitening. Moreover, we show the inefficacy of the original
Convolutional Unit by investigating rank and correlation of features. As our method is employable off-the-shelf whiten-ing modules, we use Iterative Normalization (IterNorm), the state-of-the-art whitening module, and obtain significantly improved performance on five image classification datasets:
CIFAR-10, CIFAR-100, CUB-200-2011, Stanford Dogs, and ImageNet. Notably, we verify that our method improves stability and performance of whitening when using large learning rate, group size, and iteration number. Code is available at https://github.com/YooshinCho/ pytorch_ConvUnitOptimization. 1.

Introduction
Batch Normalization (BN) [11] is considered as a key component of deep neural networks. It significantly stabi-lizes and accelerates training by normalizing input features to have a zero mean (Centering) and a unit variance (Scal-ing), which is followed by a linear transform. Numerous follow-up studies have been proposed following the success of BN, and Batch Whitening [8, 20, 10, 9] studies were pro-posed that not only centering and scaling, but also removing linear correlation between input channels (Decorrelation) to (a) Original (b) Ours (c) Shift (d) Ours+Shift
Figure 1: Illustration of the Convolutional Units. (a) is the
Original Convolutional Unit. (b) is our modified Convolu-tional Unit that whitens the input of convolution. (c) is the
Original Convolutional Unit that employs Grouped Shift. (d) is our modified Convolutional Unit that employs shift to directly whiten the input of point-wise convolution improve BN. Although it is well-known that decorrelation increases network capacity, and stabilizes and accelerates training, it is not used in practice due to its ambiguity and complexity. Unlike centering and scaling, decorrelation is not an one-to-one mapping transform, and obtaining the in-verse square root of the covariance matrix is computation-ally complex.
Naturally, previous whitening studies have focused on introducing computationally efficient whitening methods, and investigating the reasons for the varying performance of each whitening method. Specifically, whitening meth-ods [8, 20, 10] based on ZCA [2, 13], Cholesky Decom-position [5], and Newton’s iterations [3] were proposed, and Iterative Normalization (IterNorm) [10], based on New-ton’s iterations, archeived the state-of-the-art performance owing to its small stochasticity. The superiority of Iter-Norm was investigated in [9] by comparing it with other whitening modules, but the performance gain is yet to be fully explored. To investigate the efficacy of IterNorm, we train ResNet [7] and Wide-Residual Network (WRN) [26]
on CIFAR-10, CIFAR-100 [15], and ImageNet [18]. We apply IterNorm by replacing all BN of ResNet and WRN with IterNorm. From the results shown in Table 1, we can observe that the performance of IterNorm is not satisfac-tory on CIFAR-100, despite the correlation of features be-ing successfully removed.
To identify the reasons for the poor results, we revisit the theory of Batch Whitening [16]. We identify the gap be-tween the theory and practice in terms of block design, and assume that the inefficacy of IterNorm can be attributed to the way in which whitening modules are used. Mechanisms of Batch Whitening were analyzed on the premise that the input of the linear layer is whitened; however, in commonly used block design, normalization layer is followed by a linear transform and the activation function before convo-lution as illustrated in Figure 1a.
In this paper, we call these specific order of the layers as “Convolutional Unit”.
This Convolutional Unit was empirically optimized with
BN without any analysis, but following Batch Whitening studies [8, 20, 10] have employed the same Convolutional
Unit. Thus, irrespective of the efficacy of the whitening modules, the input of convolution is not centered, scaled, and decorrelated. Moreover, there are differences between spatial convolution and the linear layer considered in the-ory. Spatial convolution can be divided into shift opera-tion [24, 12, 4] and point-wise convolution; thus, there is spatial misalignment between the input of spatial convolu-tion and point-wise convolution. We call the gap as input misalignment in this paper. It means the whitening process is affected by the spatial shift operation, even if we directly perform whitening at the input of spatial convolution.
In this paper, we highlight three structural problems that contributes to the gap between the theory and practice: lin-ear transform, position of whitening module, and input mis-alignment. To bridge the gap one step at a time, we modify the Convolutional Unit as illustrated in Figure 1b. Then, we empirically analyze the original and our Convolutional
Unit. We employ IterNorm to empirically confirm the effi-cacy of whitening module is increased when used with our
Convolutional Unit. Series of ablation studies show that the original Convolutional Unit is well-optimized with BN, but not optimized with whitening modules. As we expected,
IterNorm outperforms when using with our Convolutional
Unit, and linear transform makes training unstable and over-fit. To support superiority of our method, we also investi-gate the rank and correlation of features. We empirically confirm that correlation is increased by about five times due to linear transform and activation function in practice. Also, we verify that input feature of normalization layer is not full rank when using the original Convolutional Unit. It makes decorrelated output extremely unstable due to noisy chan-nels, and performance degenerate. By contrast, we observe that the input feature of normalization layer is full rank when using our Convolution Unit.
For further improvements, we close the gap, input mis-alignment, which is caused by spatial shift of convolution.
To directly perform whitening at the input of point-wise convolution, we divide spatial convolution into Grouped
Shift [24] and point-wise convolution, and place whitening modules between them as illustrated at Figure 1d. With modifications, we get much better performance than BN and IterNorm with original Convolutional Unit on CIFAR-10, CIFAR-100, CUB-200-2011 [23], Stanford Dogs [14].
To the best of our knowledge, this is the first paper that shows applicability of whitening modules in transfer learn-ing, which we demonstrate on CUB-200-2011 and Stan-ford Dogs. Furthermore, we empirically confirm that our method enhances training stability of whitening modules.
Our method shows significantly improved results at larger learning rates when the performance of BN, IterNorm using the original Convolution Unit decreases. Also, we compare stability of IterNorm with the original Convolutional Unit and our Convolutional Unit as increasing the iteration num-ber.
IterNorm using our Convolutional Unit shows much more stable behavior and significantly better performance with a iteration number larger than 7. Finally, we addition-ally adopt DBN [8], and demonstrate that our Convolutional
Unit generally improves efficacy of whitening modules. 2.