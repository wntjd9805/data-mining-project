Abstract
Given an imperfect predictor, we exploit additional fea-tures at test time to improve the predictions made, without retraining and without knowledge of the prediction function.
This scenario arises if training labels or data are proprietary, restricted, or no longer available, or if training itself is pro-hibitively expensive. We assume that the additional features are useful if they exhibit strong statistical dependence to the underlying perfect predictor. Then, we empirically estimate and strengthen the statistical dependence between the initial noisy predictor and the additional features via manifold de-noising. As an example, we show that this approach leads to improvement in real-world visual attribute ranking. 1.

Introduction
In supervised learning, sometimes we have additional information at training time but not at testing time. One ex-ample is human action recognition, where training images have additional skeletal features like bone lengths [17]. An-other class of examples is tasks with crowdsourced labels, which have derived confidence values from multiple noisy human labelings [13]. In training, each input and output pair (g, y) has additional ‘privileged’ features h to help learn mapping function f . Vapnik and Vashist proposed to exploit these data with learning using privileged information [21].
Their support vector machine (SVM) training algorithm was later generalized to other learning algorithms.
We consider the complementary scenario where addi-tional features h are only available at testing time. We call this testing using privileged information (TUPI). This is dif-ferent from retraining with new features: Given a pre-trained f and test data g with test time features h, TUPI does not as-sume access to the original large set of labeled training data
{(gi, yi)}. TUPI is useful when the predictor is trained on a single feature but multiple (perhaps heterogeneous) features are available when the predictor is deployed.
For instance, when a predictor was trained on a single lens camera but applied to multi-lens cameras (across smartphone models or robot build variations), or when a predictor was trained on RGB camera data yet run on RGB-D camera data. TUPI is also useful when a predictor is proprietary or built into a closed-source library and the training data are inaccessible, e.g., Microsoft Kinect pose estimator, but new features become available later on such as deep-learning-based features (we discuss more applications in Sec. 5).
Problem description. Suppose we have an estimation prob-lem with an input feature space G and the corresponding output space Y ⊂ R. Traditional algorithms aim to iden-tify the underlying function f ∗ : G → Y based on the input training features Gtr = {gtr l } ⊂ G and the corresponding task-specific labels Y tr ⊂ Y. Once an esti-mate f I of f ∗ is constructed, we can apply it to unseen test data points G = {g1, . . . , gn} to construct the prediction f I := f I |G = [f I (g1), . . . , f I (gn)]⊤. 1 , . . . , gtr k, . . . , hm
In the TUPI scenario, we assume that additional feature sets {H i}m i=1 are provided for the test set G such that each test instance gk ∈ G is accompanied by m additional fea-tures {h1 k }. Our goal is to exploit {H i} to improve prediction f I . However, each new feature set H i may or may not be related to the underlying function f ∗ and so may or may not be useful to improve f I . Further, we do not assume explicit forms for the feature extractors g or hi (by which G and H i are obtained), or for the learned function f (e.g. f could be a deep neural network (DNN) regressor or a rule-based classifier). Lastly, we do not assume access to a large set of labeled data points (Gtr, Y tr) at the testing stage. Otherwise, the problem could be solved by adding
{H i}m i=1 to the feature set and applying traditional super-vised feature selection algorithms [18]. In general, exploiting additional information during testing to improve a prediction is an ill-posed problem, and existing algorithms are not able to accomplish this without having access to the data or labels, or are only applicable to very specific test time features.
Denoising statistical dependence. Our insight is that if the test time feature sets are useful, then they will exhibit strong statistical dependence with the underlying perfect predictor f ∗. For instance, in the extreme case of known perfect statis-tical dependence, knowing H i would immediately identify f ∗. Since we do not know in advance which feature sets (if
any) are actually useful, we must estimate these dependen-cies. We regard the initial predictor f I as a noisy version of f ∗, and we empirically estimate the pairwise dependencies between f I and each feature set H i. Then, we selectively strengthen—or denoise—the pairwise statistical dependen-cies to improve f I . That is, we: 1. Embed f I and {H i}m i=1 into a model manifold M where the Hilbert-Schmidt independence criterion, a consistent measure of statistical dependence, constitutes a similarity measure [6]. 2. Strengthen dependencies via denoising on M [7].
We demonstrate the TUPI scenario with visual attribute ranking [16], in which users provide pair-wise rank compar-isons for attribute-based database ordering (our supplemental material contains more problem details). This problem is a good fit for TUPI because ranking applications commonly have multiple related feature representations and attributes.
For this, we simply use the corresponding rank loss (Eq. 12).
Through experiments across seven real-world datasets, we show that our approach can lead to significantly improved estimation accuracy over the initial predictors.
Semi-supervised adaptation. Given the initial prediction f I and the test time features {H i}, our algorithm improves f I in an unsupervised manner. However, like many unsuper-vised approaches, our method has (two) hyperparameters that must be tuned for best-possible performance. This is a difficult issue that has no good approach in the unsupervised setting. In practical applications, we must rely on users to sample and evaluate hyperparameters assuming that their se-lections would be guided by experience on related problems.
For our method, user sampling is feasible as our method induces smoothness in accuracy over hyperparameters and is fast to execute (≈1 second on 50K items; see supplemental
Fig. 1). This setting makes objective assessment challenging as the results are subject to user experience. Therefore, for comparison to other techniques, we use validation labels to tune these parameters. This renders the validation sce-nario of our adaptation algorithm semi-supervised. In our experiments, we further show that using such a small set of validation labels to retrain a new predictor is not competitive.