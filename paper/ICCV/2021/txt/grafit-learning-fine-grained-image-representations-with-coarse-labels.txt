Abstract
This paper tackles the problem of learning a ﬁner rep-resentation than the one provided by training labels. This enables ﬁne-grained category retrieval of images in a col-lection annotated with coarse labels only.
Our network is learned with a nearest-neighbor clas-siﬁer objective, and an instance loss inspired by self-supervised learning. By jointly leveraging the coarse labels and the underlying ﬁne-grained latent space, it signiﬁcantly improves the accuracy of category-level retrieval methods.
Our strategy outperforms all competing methods for re-trieving or classifying images at a ﬁner granularity than that available at train time. It also improves the accuracy for transfer learning tasks to ﬁne-grained datasets. 1.

Introduction
Image classiﬁcation now achieves a performance that meets many application needs [27, 37, 54]. However, in practice, datasets and labels available at training time do not necessarily correspond to those needed in subsequent applications [17]. The granularity of the training-time con-cepts may not sufﬁce for ﬁne-grained downstream tasks.
This has encouraged the development of specialized clas-siﬁers offering a more precise representation. Fine-grained classiﬁcation datasets [29] have been developed for speciﬁc domains, for instance to distinguish different plants [13] or bird species [59].
Gathering a sufﬁciently large collection with ﬁne-grained labels is difﬁcult by itself, as it requires to ﬁnd enough images of rare classes, and annotating them pre-cisely requires domain specialists with in-domain expertise.
This is evidenced by the Open Images construction anno-tation protocol [38] that states that: “Manually labeling a large number of images with the presence or absence of 19,794 different classes is not feasible”. For this reason they resorted to computer-assisted annotation, at the risk of introducing biases due to the assisting algorithm.
To circumvent this issue, we propose in this paper a strat-egy to get strong classiﬁcation and image retrieval perfor-mance on ﬁne concepts using only coarse labels at training.
Our work leverages two intuitions. First, in order to im-prove the granularity beyond the one provided by image la-bels, we need to exploit another signal than just the labels.
For this purpose, we build upon recent works [3, 62] that exploit two losses to address both image classiﬁcation and instance recognition, leveraging the “free” annotations pro-vided by multiple data augmentations of a same instance, in the spirit of self-supervised learning [6, 9, 10, 25].
The second intuition is that it is better to explicitly infer coarse labels even when classifying at a ﬁner granularity.
For this purpose, we propose a simple method that exploits both a coarse classiﬁer and image embeddings to improve
ﬁne-grained category-level retrieval. This strategy outper-forms existing works that exploit coarse labels at training time but do not explicitly rely on them when retrieving ﬁner-grained concepts [61].
By these ways our method liberates the data collection process from the quirks of a rigid ﬁne-grained taxonomy as previously discussed. To validate our strategy, we investi-gate two challenging use-cases:
On-the-ﬂy classiﬁcation. For this task, the ﬁne-grained labels are available at test time only, and we use a non-parametric kNN classiﬁer [61] for on-the-ﬂy classiﬁcation, i.e. without training on the ﬁne-grained labels.
Category-level Retrieval. Given a collection of images annotated with coarse labels, like a product catalog, we aim at ranking these images according to their ﬁne-grained se-mantic similarity to a new query image outside the collec-tion, as illustrated by Figure 1. We believe that this new task better is more realistic than the on-the-ﬂy classiﬁcation setting.
In summary, in this context of coarse-to-ﬁne representa-tion learning, our paper makes the following contributions:
• We propose Graﬁt, a method to learn image representa-tions at a ﬁner granularity than the one offered by the annotation at training time. Inspired by the recent self-supervised BYOL [25] instance learning approach, we
Figure 1: Category-level retrieval orders images based on their semantic similarity to a query. Our Graﬁt method, although it has used only coarse labels (like ’pyrgus’) at training time, produces a ranking consistent with ﬁne-grained labels. Unsupervised learning is a particular case of this task, in which the set of coarse labels is reduced to a singleton. Image credit: [1]. carefully design a joint learning scheme integrating in-stance and coarse-label based classiﬁcation losses. For the latter one, we exploit a knn strategy but with a ded-icated process to manage the memory both at train-time and for inference at test-time.
• We propose two original use-cases to deeply evaluate coarse-trained ﬁne-grained testing evaluation, for which
Graﬁt exhibits outstanding performance. For instance, we improve by +16.3% the top-1 accuracy for on-the-ﬂy classiﬁcation on ImageNet. This improvement is still
+9.5% w.r.t. our own stronger baseline, everything be-ing equal otherwise. Graﬁt also improves transfer learn-ing: our experiments show that our representation dis-criminates better at a ﬁner granularity.
This paper is organized as follows. After reviewing related works in Section 2, we present our method in Section 3.
Section 4 compares our approach against baselines on var-ious datasets, and presents an extensive ablation. Section 5
In the supplemental material, Ap-concludes the paper. pendix A summarizes two experiments that show how an instance-level loss improves the granularity beyond the one learned by a vanilla cross-entropy loss. Appendix B com-plements our experimental section 4 with more detailed re-sults. Appendix C provides visual results associated with different levels of training/testing granularities. 2.