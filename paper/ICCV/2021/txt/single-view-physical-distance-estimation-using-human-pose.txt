Abstract
We propose a fully automated system that si-multaneously estimates the camera intrinsics, the ground plane, and physical distances between peo-ple from a single RGB image or video captured by a camera viewing a 3-D scene from a ﬁxed van-tage point. To automate camera calibration and distance estimation, we leverage priors about hu-man pose and develop a novel direct formulation for pose-based auto-calibration and distance estimation, which shows state-of-the-art performance on publicly available datasets. The proposed approach enables ex-isting camera systems to measure physical distances without needing a dedicated calibration process or range sensors, and is applicable to a broad range of use cases such as social distancing and workplace safety. Furthermore, to enable evaluation and drive research in this area, we contribute to the publicly available MEVA dataset with additional distance an-notations, resulting in “MEVADA” – an evaluation benchmark for the pose-based auto-calibration and distance estimation problem. 1.

Introduction
Estimating physical distances between objects from a single view is an emerging and challenging problem in computer vision. This task has wide applicability to many real-world situations such as deciding on appropriate movements in autonomous vehicles and robots, determining distances between players in sports, and estimating safe distances be-tween people or dangerous objects in open spaces.
Standard image-based approaches to this task typically require complex factory or on-site camera calibration procedures [19, 30, 54, 57, 11] (e.g., us-ing a checkerboard) or specialized hardware. The former measures distances within an Euclidean re-construction of a 3-D scene observed from multiple
∗Work was done when Xiangyu was at Amazon.
Figure 1: Sample output (best viewed at 5× and in color) of our fully automated system (Sect. 5) with potential applications in social distancing. Left: The plot is gen-erated from the Oxford Town Center dataset [20], where the grids in cyan represent the estimated ground plane (each cell is 6ft. × 6ft.), red indicates people within 6 feet from others, i.e., possibly unsafe regarding social dis-tancing guidelines [13], green means safe, the links show each person’s nearest neighbor in 3-D with estimated distance (in feet) superimposed in pink. Top right: A top-down view of the scene. Bottom right: A heat map of individuals considered unsafe aggregated over time, which may guide safety measures, e.g., workplace re-arrangement, to be taken. vantage points by at least one moving or multiple static pre-calibrated cameras. The latter utilizes
RGB-D cameras or range sensors where metric dis-tance measures are directly available. These cameras are expensive, not widely deployed and are limited in range and operating conditions.
Three challenges limit widespread adoption of objects distance estimation: (i) the majority of avail-able video cameras output only RGB images, (ii) applying standard checkerboard-based calibration on-site for the vast majority of uncalibrated, already-installed cameras is prohibitively expensive, and (iii) in most security installations scenes are only observ-able from one camera view. We present a distance estimation method that can be applied to single-view RGB images that utilizes reference objects of roughly-known dimension present in scenes, such as cars, furniture, windows, etc., to “auto-calibrate” a 1
ﬁxed camera.
In particular, when the presence of people in im-agery is ubiquitous, we use the people as a stand in for a calibration pattern to estimate the cam-era intrinsics and scene geometry. We follow the assumptions commonly used in human pose-based auto-calibration [33, 34, 6, 28, 26, 35, 29, 23, 50]: people are standing upright on a common ground plane and can be approximated as vertical line seg-ments of known constant height. Current published methods require sequential steps of intersecting and
ﬁtting lines ﬁrst to calculate the vertical vanishing point and horizon line [18], followed by extraction of camera parameters. We derive and present a sim-pler and more accurate approach to solve camera parameters directly from keypoint measurements in just three linear equations. Additionally, we jointly estimate the ground plane and the 3-D keypoints from a single view. Thus, our system is able to ad-dress all three aforementioned challenges. In this paper, we demonstrate distance estimation between people, but the formulation can be trivially gener-alized to other object classes with (roughly) known dimensions.
Summary of Contributions (i) We derived a di-rect formulation that simultaneously estimates the camera intrinsics, the ground plane, and reconstructs 3-D points from 2-D keypoints by solving three lin-ear equations (Sect. 3). (ii) As an application of the formulation, we developed a fully automated system (Sect. 5) capable of estimating physical distances between people from one RGB image or video with-out manual calibration. (iii) As there are no proper datasets for the distance estimation task (Sect. 2), we built MEVADA on top of the publicly available Mul-tiview Extended Video with Activities (MEVA) [27] dataset to drive research in this area (Sect. 6). The
MEVADA dataset can be found at https://feixh. github.io/projects/physical_distance/. 2.