Abstract
We propose a novel transformer-based styled handwrit-ten text image generation approach, HWT, that strives to learn both style-content entanglement as well as global and local style patterns. The proposed HWT captures the long and short range relationships within the style exam-ples through a self-attention mechanism, thereby encoding both global and local style patterns. Further, the proposed transformer-based HWT comprises an encoder-decoder at-tention that enables style-content entanglement by gather-ing the style features of each query character. To the best of our knowledge, we are the first to introduce a transformer-based network for styled handwritten text generation.
Our proposed HWT generates realistic styled hand-written text images and outperforms the state-of-the-art demonstrated through extensive qualitative, quantitative and human-based evaluations. The proposed HWT can handle arbitrary length of text and any desired writing style in a few-shot setting. Further, our HWT general-izes well to the challenging scenario where both words and writing style are unseen during training, generating realistic styled handwritten text images. Code is avail-able at: https://github.com/ankanbhunia/Handwriting-Transformers 1.

Introduction
Generating realistic synthetic handwritten text images, from typed text, that is versatile in terms of both writ-ing style and lexicon is a challenging problem. Automatic handwritten text generation can be beneficial for people having disabilities or injuries that prevent them from writ-ing, translating a note or a memo from one language to an-other by adapting an author’s writing style or gathering ad-ditional data for training deep learning-based handwritten text recognition models. Here, we investigate the problem of realistic handwritten text generation of unconstrained text sequences with arbitrary length and diverse calligraphic attributes representing writing styles of a writer.
Figure 1: Comparison of HWT (c) with GANwriting [14] (d) and Davis et al. [5] (e) in imitating the desired unseen writing style (a) for given query text (b). While [14, 5] capture global writing styles (e.g., slant), they struggle to imitate local style patterns (e.g., character style, ligatures).
HWT (c) imitates both global and local styles, leading to a more realistic styled handwritten text image generation.
For instance, style of ‘n’ (red line) appearing in (a) is mim-icked by HWT, for a different word including same char-acter ‘n’. Similarly, a group of characters in ‘thought’ and ‘personalities’ (blue and magenta lines) are styled in a way that matches with words (‘throughout’ and
‘qualities’) sharing some common characters in (a).
Furthermore, HWT preserves cursive patterns and connec-tivity of all characters in word ‘also’ (green line).
Generative Adversarial Networks (GANs) [8] have been investigated for offline handwritten text image generation
[4, 3, 14, 7, 5]. These methods strive to directly synthe-size text images by using offline handwriting images during training, thereby extracting useful features, such as writing appearance (e.g., ink width, writing slant) and line thickness changes. Alonso et al. [3] propose a generative architec-ture that is conditioned on input content strings, thereby not restricted to a particular pre-defined vocabulary. However, their approach is trained on isolated fixed-sized word im-ages and struggles to produce high quality arbitrarily long text along with suffering from style collapse. Fogel et al. [7] introduce a ScrabbleGAN approach, where the generated image width is made proportional to the input text length.
ScrabbleGAN is shown to achieve impressive results with respect to the content. However, both [3, 7] do not adapt to a specific author’s writing style.
Recently, GAN-based approaches [5, 14] have been in-troduced for the problem of styled handwritten text image generation. These methods take into account both content and style, when generating offline handwritten text images.
Davis et al. [5] propose an approach based on StyleGAN
[15] and learn generated handwriting image width based on style and input text. The GANwriting framework [14] con-ditions handwritten text generation process to both textual content and style features in a few-shot setup.
In this work, we distinguish two key issues that impede the quality of styled handwritten text image generation in the existing GAN-based methods [5, 14]. First, both style and content are loosely connected as their representative features are processed separately and later concatenated.
While such a scheme enables entanglement between style and content at the word/line-level, it does not explicitly en-force style-content entanglement at the character-level. Sec-ond, although these approaches capture global writing style (e.g., ink width, slant), they do not explicitly encode local style patterns (e.g., character style, ligatures). As a result of these issues, they struggle to accurately imitate local cal-ligraphic style patterns from reference style examples (see
Fig. 1). Here, we look into an alternative approach that ad-dresses both these issues in a single generative architecture. 1.1. Contributions
We introduce a new styled handwritten text genera-tion approach built upon transformers, termed Handwriting
Transformers (HWT), that comprises an encoder-decoder network. The encoder network utilizes a multi-headed self-attention mechanism to generate a self-attentive style fea-ture sequence of a writer. This feature sequence is then input to the decoder network that consists of multi-headed self- and encoder-decoder attention to generate character-specific style attributes, given a set of query word strings.
Consequently, the resulting output is fed to a convolutional decoder to generate final styled handwritten text image.
Moreover, we improve the style consistency of the gen-erated text by constraining the decoder output through a loss term whose objective is to re-generate style feature se-quence of a writer at the encoder.
Our HWT imitates the style of a writer for a given query content through self- and encoder-decoder attention that emphasizes relevant self-attentive style features with re-spect to each character in that query. This enables us to cap-ture style-content entanglement at the character-level. Fur-thermore, the self-attentive style feature sequence generated by our encoder captures both the global (e.g., ink width, slant ) and local styles (e.g., character style, ligatures) of a writer within the feature sequence.
We validate our proposed HWT by conducting extensive
In qualitative, quantitative and human-based evaluations. the human-based evaluation, our proposed HWT was pre-ferred 81% of the time over recent styled handwritten text generation methods [5, 14], achieving human plausibility in terms of the writing style mimicry. Following GANwrit-ing [14], we evaluate our HWT on all the four settings on the IAM handwriting dataset. On the extreme setting of out-of-vocabulary and unseen styles (OOV-U), where both query words and writing styles are never seen during train-ing, the proposed HWT outperforms GANwriting [14] with an absolute gain of 16.5 in terms of Fr`echet Inception Dis-tance (FID) thereby demonstrating our generalization capa-bilities. Further, our qualitative analysis suggest that HWT performs favorably against existing works, generating real-istic styled handwritten text images (see Fig. 1). 2.