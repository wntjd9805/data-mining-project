Abstract
The success of deep neural networks (DNNs) has promoted the widespread applications of person re-identification (ReID). However, ReID systems inherit the vulnerability of DNNs to malicious attacks of visually in-conspicuous adversarial perturbations. Detection of adver-sarial attacks is, therefore, a fundamental requirement for
In this work, we propose a Multi-robust ReID systems.
Expert Adversarial Attack Detection (MEAAD) approach to achieve this goal by checking context inconsistency, which is suitable for any DNN-based ReID systems. Specifically, three kinds of context inconsistencies caused by adversar-ial attacks are employed to learn a detector for distinguish-ing the perturbed examples, i.e., a) the embedding distances between a perturbed query person image and its top-K re-trievals are generally larger than those between a benign query image and its top-K retrievals, b) the embedding dis-tances among the top-K retrievals of a perturbed query im-age are larger than those of a benign query image, c) the top-K retrievals of a benign query image obtained with mul-tiple expert ReID models tend to be consistent, which is not preserved when attacks are present. Extensive exper-iments on the Market1501 and DukeMTMC-ReID datasets show that, as the first adversarial attack detection approach for ReID, MEAAD effectively detects various adversarial at-tacks and achieves high ROC-AUC (over 97.5%). 1.

Introduction
The success of DNNs has benefited a wide range of com-puter vision tasks, such as image classification [13, 16], ob-ject detection [12, 36], face recognition [34, 20], video clas-sification [21, 47], and person ReID [50, 23, 14, 50, 33].
*Corresponding author: liu_min@hnu.edu.cn.
X. Wang was a visiting student at UCR in 2019-20.
Person ReID is a critical task aiming to retrieve pedestri-ans across multiple non-overlapping cameras. By learn-ing the discriminative feature embedding and adaptive dis-tance metric models, DNNs-based ReID models, in re-cent years, have extensive applications in video surveillance or criminal identification for public safety. However, re-cent research has found that these models inherit the vul-nerability of DNNs to adversarial examples [42, 45, 2, 8] which are slightly perturbed input images but lead DNNs to make wrong predictions [11, 39]. Detection of adversar-ial examples is, therefore, a fundamental requirement for robust ReID systems because the insecurity of ReID sys-tems may cause severe losses. However, ReID is defined as a ranking problem rather than a classification problem and thus existing defense methods for image classification
[6, 18, 30, 26, 28, 41] do not fit the person ReID problem.
In addition, the top-K retrievals output by person ReID systems, compared to the prediction label in classification task, contain richer information and can be potentially em-ployed to detect adversarial attacks. To illustrate, let’s con-sider the top-10 retrievals obtained with five different state-of-the-art person ReID systems (LSRO [53], AlignedReID
[48], PCB [38], HACNN [23] and Mudeep [33]) of a query sample before and after an adversarial attack in Fig. 1(a).
When considering the retrievals returned by a single ReID system, e.g. LSRO, they are visually more similar to the query image before attack than that after attack, and they are visually more similar to each other before attack. When considering the retrievals returned by different expert mod-els, they are consistent before attack but certainly not after attack. We did an empirical study as shown in Fig. 1(b) and found that the embedding distance is able to reflect visual similarity; specifically, the retrievals of the benign query tend to gather together in the embedding space while the retrievals of the perturbed query tend to spread over.
Inspired by these observations, we propose Multi-Expert
Figure 1. (a) shows the top-10 retrievals for a query sample before and after an adversarial attack. Five state-of-the-art person ReID models, i.e., LSRO [53], AlignedReID [48], PCB [38], HACNN [23] and Mudeep [33] are used as the expert models. Deep Mis-Ranking attack
[42] is used to generate the adversarial perturbations and AlignedReID is the attack target model. The top-10 retrievals of a benign query (before attack) are consistent across multiple expert models, while they are messy for a perturbed query sample (after attack). (b) presents (using PCA) the embedding space of an expert model (AlignedReID). The original query sample is marked with red star and its retrievals are marked with red plus marker. The perturbed query sample is marked with blue star and its retrievals are marked with blue plus marker.
We observe that the retrievals of the benign query sample gather tightly around the benign query sample in the embedding space. In comparison, the retrievals of the perturbed query sample spread over the space. We have quantitative and more detailed results in Fig. 2.
Adversarial Attack Detection which detects adversarial at-tacks for person ReID systems by detecting context incon-sistency. To the best of our knowledge, this is the first strat-egy to detect adversarial attacks against person ReID sys-tems. To make use of the heterogeneity of different ReID models, we use multiple ReID networks with different ar-chitectures as expert models in MEAAD. We define support set as the top-K retrievals output by a single expert model.
Context used in MEAAD accounts for three types of rela-tions: 1) the relations between the query and its support samples returned by a single expert (Query-Support Affin-ity); 2) the relations among the support samples returned by a single expert (Support-Support Affinity); 3) the rela-tions between the support samples returned by one expert and those returned by another (Cross-Expert Affinity). We then train a detector with the context features of both benign and perturbed query samples and use it to detect adversarial attacks during testing. The contributions are as below,
• To the best of our knowledge, this is the first adversarial attack detection strategy for the defense of ReID systems.
• We empirically study the side effect brought by adver-sarial attacks, i.e., context inconsistency in the retrieval results. We then propose MEAAD which aims to detect adversarial attacks by checking context inconsistency of a query sample to be detected. and on show that, MEAAD ef-DukeMTMC-ReID datasets fectively detects various adversarial attacks and achieves high ROC-AUC (over 97.5% in all cases). the Market1501 experiments
• Extensive 2.