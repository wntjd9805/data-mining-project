Abstract
Video portraits relighting is critical in user-facing hu-man photography, especially for immersive VR/AR experi-ence. Recent advances still fail to recover consistent relit result under dynamic illuminations from monocular RGB stream, suffering from the lack of video consistency super-vision. In this paper, we propose a neural approach for real-time, high-quality and coherent video portrait relighting, which jointly models the semantic, temporal and lighting consistency using a new dynamic OLAT dataset. We pro-pose a hybrid structure and lighting disentanglement in an encoder-decoder architecture, which combines a multi-task and adversarial training strategy for semantic-aware con-sistency modeling. We adopt a temporal modeling scheme via flow-based supervision to encode the conjugated tempo-ral consistency in a cross manner. We also propose a light-ing sampling strategy to model the illumination consistency and mutation for natural portrait light manipulation in real-world. Extensive experiments demonstrate the effectiveness of our approach for consistent video portrait light-editing and relighting, even using mobile computing. 1.

Introduction
The past ten years have witnessed a rapid development of digital portrait photography with the rise of mobile cam-eras. Relighting evolves as a cutting-edge technique in such portrait photography for immersive visual effects of VR/AR experience. How to further enable consistent relit video results under challenging dynamic illumination conditions conveniently remains unsolved and has received substantive attention in both industry and academia.
For video portrait relighting, early solutions [14, 57] rely on a sophisticated studio setup which is expensive and diffi-cult to be deployed. Modern approaches [41, 44, 42] further
*Equal contribution
Figure 1. Our approach achieves high-quality and consistent video portrait relighting under dynamic illuminations in real-time, using only mobile computing and monocular RGB video input. apply color or style transfer techniques to ease the hard-ware requirements. However, they still require two to four orders of magnitude more time than is available for inter-active video application. The recent learning techniques bring huge potential for human portrait modeling and re-lighting [69, 67, 46, 31, 56] from only monocular RGB in-put.
In particular, the methods [43, 39] perform explicit neural inverse rendering but are limited to the low-quality face and Spherical Harmonics (SH) lighting models. Re-cent methods [67, 46] remove the explicit inverse rendering by learning an efficient end-to-end mapping between the in-put headshots and relit ones, while the method [56] further models reflectance attributes explicitly to handle lighting ef-fects like specular or shadow. However, they still focus on single image input without modeling the temporal consis-tency for video portrait relighting, leading to severe jittery artifacts, especially under the challenging dynamic illumi-nations. Some recent mobile devices [2, 18] enables the
“Portrait Lighting” mode for video editing of lighting con-ditions. Critically, they only modify existing illumination rather than relight the captured video into various scenes.
In this paper, we address the above challenges and present a novel real-time and temporally coherent portrait relighting approach from only a monocular RGB video in-put, as illustrated in Fig. 1. Our approach jointly models the semantic, temporal and lighting consistency to enable realistic video portrait light-editing and relighting into new scenes with dynamic illuminations, whilst maintaining real-time performance even on portable device.
Generating such realistic and consistent video relit result in a real-time and data-driven manner is non-trivial. From the data aspect, existing face datasets [10, 60, 56] lack the video ground-truth supervision for consistency modeling.
Thus, we build up a high-quality dataset for video portrait relighting, consisting of 603,288 temporal OLAT (one light at a time) images at 25 frames per second (fps) of 36 ac-tors and 2,810 environment lighting maps. From the algo-rithm side, we further propose a novel neural scheme for consistent video portrait relighting under dynamic illumi-nations. To maintain the real-time performance, we adopt the encoder-decoder architecture to each input portrait im-age similar to previous methods [46, 56]. Differently, we in-troduce a hybrid and explicit disentanglement for semantic-aware consistency, which self-supervises the portrait struc-ture information and fully supervises the lighting informa-tion simultaneously in the bottleneck of the network. Such disentanglement is further enhanced via multi-task train-ing as well as an adversarial strategy so as to encode the semantic supervision and enable more realistic relighting.
Then, to utilize the rich temporal consistency in our dy-namic OLAT dataset, a novel temporal modeling scheme is adopted between two adjacent input frames. Our tempo-ral scheme encodes the conjugated temporal consistency in a cross manner via flow-based supervision so as to model the dynamic relit effect. Finally, a lighting sampling based on Beta distribution is adopted, which augments the dis-crete environment lighting maps and generates a triplet of lighting conditions for adjacent input frames and the tar-get output. Our sampling scheme models the illumination consistency and mutation simultaneously for natural video portrait light-editing and relighting in the real-world. To summarize, our main contributions include:
• We present a real-time neural video portrait relighting approach, which faithfully models the video consis-tency for dynamic illuminations, achieving significant superiority to the existing state-of-the-art.
• We propose an explicit structure and lighting disen-tanglement, a temporal modeling as well as a light-ing sampling schemes to enable realistic video portrait light-editing and relighting on-the-fly.
• We make available our dataset with 603,288 temporal
OLAT images to stimulate further research of human portrait and lighting analysis. 2.