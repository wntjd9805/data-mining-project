Abstract
Recent Visual Question Answering (VQA) models have shown impressive performance on the VQA benchmark but remain sensitive to small linguistic variations in input ques-tions. Existing approaches address this by augmenting the dataset with question paraphrases from visual question gen-eration models or adversarial perturbations. These ap-proaches use the combined data to learn an answer classi-ﬁer by minimizing the standard cross-entropy loss. To more effectively leverage augmented data, we build on the recent success in contrastive learning. We propose a novel train-ing paradigm (ConClaT) that optimizes both cross-entropy and contrastive losses. The contrastive loss encourages rep-resentations to be robust to linguistic variations in ques-tions while the cross-entropy loss preserves the discrimina-tive power of representations for answer prediction.
We ﬁnd that optimizing both losses – either alternately or jointly – is key to effective training. On the VQA-Rephrasings [44] benchmark, which measures the VQA model’s answer consistency across human paraphrases of a question, ConClaT improves Consensus Score by 1.63% over an improved baseline.
In addition, on the standard
VQA 2.0 benchmark, we improve the VQA accuracy by 0.78% overall. We also show that ConClaT is agnostic to the type of data-augmentation strategy used. 1.

Introduction
Visual Question Answering (VQA) refers to the task of automatically answering free-form natural language ques-tions about an image. For VQA systems to work reliably when deployed in the wild, for applications such as assist-ing visually impaired users, they need to be robust to differ-ent ways a user might ask the same question. For example,
VQA models should produce the same answer for two para-phrased questions – “What is in the basket?” and “What is contained in the basket?” since their semantic meaning is the same. While signiﬁcant progress has been made towards building more accurate VQA systems, these models remain brittle to minor linguistic variations in the input question.
*Correspondence to ysh.kant@gmail.com
Figure 1: We make VQA model robust to question para-phrases using a training paradigm ConClaT that minimizes contrastive and cross-entropy losses together. Contrastive learning step pulls representations of positive samples cor-responding to paraphrased questions closer together while pushing those with different answers farther apart. Cross-entropy step makes these representations discriminative to help model answer visual questions accurately.
To make VQA systems robust, existing approaches [44, 47] have trained VQA systems [24] by augmenting the training data with different variations of the input question.
For instance, VQA-CC [44] use a visual question genera-tion (VQG) model to generate paraphrased question given an image and answer. Generally, these models fuse im-age and question features into a joint vision and language (V+L) representation followed by a standard softmax clas-siﬁer to produce answer probabilities and are optimized by minimizing the cross-entropy loss. Unfortunately, cross-entropy loss treats every image-question pair independently and fails to exploit the information that some questions in the augmented dataset are paraphrases of each other.
We overcome this limitation by using a contrastive loss
InfoNCE [36] that encourages joint V+L (Vision and Lan-guage) representations obtained from samples whose ques-tions are paraphrases of each other to be closer while pulling apart the V+L representations of samples with different an-swers. As we operate in a supervised setting, we choose
Supervised Contrastive Loss (SCL) [26] which extends In-foNCE by utilizing the label information to bring samples from the same class (ground-truth answer) together. We in-troduce a variant of the SCL which emphasizes rephrased image-question pairs over pairs that are entirely different but have the same answer. Our proposed training paradigm,
ConClaT (Contrast and Classify Training), minimizes SCL and cross-entropy loss together to learn better vision and language representations as shown in Fig.1. Minimizing the contrastive loss encourages representations to be robust to linguistic variations in questions while the cross-entropy loss preserves the discriminative power of the representa-tions for answer classiﬁcation. Instead of pretraining with
SCL, then ﬁne-tuning with cross-entropy loss as in [26], we ﬁnd that minimizing the two losses either alternately or jointly by constructing loss-speciﬁc mini-batches helps learn better representations. For contrastive loss, we care-fully curate mini-batches by sampling various types of neg-atives and positives given a reference sample.
We show the efﬁcacy of our training paradigm across two rephrasing (i.e., data-augmentation) strategies. Using rephrasings obtained from a VQG model proposed in [44], our approach outperforms a baseline that simply treats these rephrasings as additional samples and ignores the link be-tween question and its paraphrases. We noticed that the
VQG model fails to produce a diverse set of rephrasings for a question. Hence, we use Back-translation to obtain ques-tion rephrasings. Back-translation [15] involves translating an input sentence from one language to another and then translating it back into the original language using a pair of machine translation models (e.g. en-fr and fr-en). We
ﬁnd that Back-translation preserves the semantic meaning of the question while generating syntactically diverse ques-tion. Utilizing the publicly available collection of neural machine translation models in HuggingFace [52], we gen-erate numerous rephrasings of every question. Then, we
ﬁlter poor/irrelevant rephrasings with a sentence similarity model [41] and store 3 rephrasings per original question of
VQA v2.0 dataset without any manual supervision.
We extensively ablate ConClaT with alternate [8], joint and pretrain-ﬁnetune [26] training schemes, and compare with previously proposed triplet [38] and margin-based losses [58] . We evaluate on the VQA Rephrasings bench-mark [44] which measures the model’s answer consistency across several rephrasings of a question. ConClaT improves
Consensus Score by 1.63% over an improved baseline. In addition, on the standard VQA 2.0 benchmark, we improve
VQA accuracy by 0.78% overall. It is also worth noting that
VQA models trained using ConClaT perform better than existing approaches across both the aforementioned data-augmentation strategies – Back-translation and VQG. 2.