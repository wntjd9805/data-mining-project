Abstract
Partial Domain Adaptation (PDA) is a practical and general domain adaptation scenario, which relaxes the fully shared label space assumption such that the source la-bel space subsumes the target one. The key challenge of
PDA is the issue of negative transfer caused by source-only classes. For videos, such negative transfer could be trig-gered by both spatial and temporal features, which leads to a more challenging Partial Video Domain Adaptation (PVDA) problem. In this paper, we propose a novel Par-tial Adversarial Temporal Attentive Network (PATAN) to address the PVDA problem by utilizing both spatial and temporal features for filtering source-only classes. Besides,
PATAN constructs effective overall temporal features by at-tending to local temporal features that contribute more to-ward the class filtration process. We further introduce new benchmarks to facilitate research on PVDA problems, cov-ering a wide range of PVDA scenarios. Empirical results demonstrate the state-of-the-art performance of our pro-posed PATAN across the multiple PVDA benchmarks. Code will be provided at: https://github.com/xuyu0010/PATAN. 1.

Introduction
Video-based problems have long been studied thanks to their wide applications in various fields. Neural networks have made notable advances in these problems with the availability of large-scale labeled video data. However, suf-ficiently large-scale training video data is sometimes un-available, as annotations of video data are costly. Various
*Equal Contribution.
†Corresponding Author.
‡This research is supported by A*STAR under its AME Programmatic
Funds (Grant No. A20H6b0151).
Figure 1. PVDA is a more general setting where the source la-bel space subsumes the target label space. The key challenge of PVDA is the negative transfer caused by outlier source-only classes (‘walk’ and ‘situp’), with extra probability triggered by the incorrect alignment of target temporal features to the source tem-poral features of the outlier classes, depicted as the left dashed arrow between videos from classes ‘run’ and ‘walk’.
Video-based Unsupervised Domain Adaptation (VUDA) methods have been proposed to enable networks to transfer knowledge from a labeled source domain to an unlabeled target domain by learning domain-invariant feature repre-sentations in the absence of target labels.
Though existing VUDA methods enable the learning of transferable features across domains, they generally assume that the video source and target domains share an iden-tical label space, which may not hold in real-world ap-plications. With the presence of large-scale labeled pub-lic datasets, it is more feasible to transfer representations learned in these datasets to unlabeled small-scale datasets.
Such a scenario is defined as Partial Domain Adaptation (PDA), which relaxes the constraint of identical source and target label spaces by assuming that the target label space is a subspace of the source one. This assumption is more practical since large-scale public video datasets can sub-sume categories of the small-scale target datasets. The PDA
problem is more challenging since source-only classes may negatively influence the distribution alignment of target data causing negative transfer.
Compared to images that only contain spatial features, videos contain additional temporal features. This leads to a novel Partial Video Domain Adaptation (PVDA) problem, with trained networks transferred from video source domain to target domain, with the label space of video target domain being the subspace of the video source domain. Models trained in large-scale video datasets such as Kinetics [17] could therefore be applied to smaller-scale datasets such as ARID [31] without supervision through PVDA. When transferring networks for PVDA, negative transfer would be triggered due to the possible spatial-temporal domain shift as depicted in Figure 1, where the appearances of videos in class ‘walk’ are different from that of videos in class ‘run’, i.e. spatial features are different among videos in the two classes. However, videos from both classes share similar motion patterns where the actor moves further away from the camera in an upright position, indicating similar tem-poral features among the videos. When performing data distribution alignment, the similarities in temporal features would lead to videos in class ‘run’ of the target domain to incorrectly align with videos in class ‘walk’ of the source domain, triggering negative transfer.
A crucial step for tackling negative transfer in PVDA is the filtration of source-only outlier classes. Different from images, temporal features should be leveraged for
PVDA from two perspectives: on one hand, effective tem-poral features should be constructed such that temporal fea-tures in outlier source-only classes discriminate those in target classes, alleviating the possibility of triggering neg-ative transfer by temporal features; on the other hand, the temporal features should also contribute towards the filtra-tion of source-only classes while eliminating possible mis-takes caused by mis-classification of spatial features. To this end, we propose a Partial Adversarial Temporal At-tentive Network (PATAN) to address the two challenges uniformly. PATAN first constructs robust overall temporal features by attentive combination of local temporal features which contain different aspects of the whole motion. The attentive combination builds upon the contribution of the local temporal features towards the class filtration process where source-only classes are filtered. The constructed tem-poral features would therefore have higher discriminability over source-only and target classes. Further, PATAN miti-gates negative transfer in PDA through a class filtration pro-cess by utilizing local and overall temporal features jointly, alleviating possible mistakes during the class filtration pro-cess brought by the spatial features.
To further facilitate PVDA research, we propose three sets of benchmarks, built from widely used pub-lic datasets and a recent video dataset dedicated to low-(a) illumination videos. The benchmarks proposed are:
UCF-HMDBpartial, (b) MiniKinetics-UCF, and (c) HMDB-ARIDpartial. The proposed datasets cover a wide range of
PVDA scenarios, providing adequate baselines with distinct domain shift.
In summary, our contributions are threefold. First, we formulated a novel and challenging Partial Video Domain
Adaptation (PVDA) problem. To the best of our knowl-edge, this is the first research that explores partial transfer in videos. Secondly, we analyze the challenges underly-ing PVDA and introduce PATAN to address the challenges.
PATAN constructs robust temporal features,while utilizing both spatial and temporal features for accurate class fil-tration. Finally, we introduce several PVDA benchmarks, and demonstrate the effectiveness of our proposed method, achieving state-of-the-art performance across the multiple
PVDA benchmarks proposed. 2.