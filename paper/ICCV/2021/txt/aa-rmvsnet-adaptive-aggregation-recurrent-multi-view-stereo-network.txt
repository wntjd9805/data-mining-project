Abstract
In this paper, we present a novel recurrent multi-view stereo network based on long short-term memory (LSTM) with adaptive aggregation, namely AA-RMVSNet. We firstly introduce an intra-view aggregation module to adaptively extract image features by using context-aware convolution and multi-scale aggregation, which efficiently improves the performance on challenging regions, such as thin objects and large low-textured surfaces. To overcome the diffi-culty of varying occlusion in complex scenes, we propose an inter-view cost volume aggregation module for adap-tive pixel-wise view aggregation, which is able to pre-serve better-matched pairs among all views. The two pro-posed adaptive aggregation modules are lightweight, ef-fective and complementary regarding improving the ac-curacy and completeness of 3D reconstruction.
Instead of conventional 3D CNNs, we utilize a hybrid network with recurrent structure for cost volume regularization, which allows high-resolution reconstruction and finer hy-pothetical plane sweep. The proposed network is trained end-to-end and achieves excellent performance on various
It ranks 1st among all submissions on Tanks datasets. and Temples benchmark and achieves competitive results on DTU dataset, which exhibits strong generalizability and robustness. Implementation of our method is available at https://github.com/QT-Zhu/AA-RMVSNet. 1.

Introduction
Multi-view stereo (MVS) aims to obtain 3D dense mod-els of real-world scenes from multiple images, which is one of the core techniques in a variety of applications including virtual reality, autonomous driving and heritage conserva-tion. While traditional MVS methods [5, 9, 10, 26, 4] uti-lize hand-crafted matching metrics to measure multi-view consistency, recent deep-learning-based methods [33, 34, 21, 37, 31] achieve superior accuracy and completeness on many MVS benchmarks [3, 16, 35, 27] compared with
*Corresponding author.
Figure 1. Illustration of multi-view 3D reconstruction of Scan 77 in DTU dataset [3] using the proposed AA-RMVSNet. (a) The reference image; (b) adaptive sampling locations in our intra-view
AA approach; (c) the depth map estimated by AA-RMVSNet after filtering; (d) the recovered dense 3D model. the previous state-of-the-arts, through introducing convolu-tional neural network (CNN) which makes feature extrac-tion and cost volume regularization more powerful. How-ever, some challenging problems still remain to be solved to further improve reconstruction quality.
First, general features extracted by 2D CNN in regular pixel grids with fixed receptive fields often have difficulties in handling thin structures or textureless surfaces, which limits the robustness and completeness of 3D reconstruc-tion. Recent MVSNet-based attempts [36, 31, 32] introduce multi-scale information to improve depth estimation. How-ever, context-aware features have not been leveraged well enough for varying richness of texture on different regions.
Second, few works consider pixel-wise visibility issues during multi-view matching cost aggregation, which in-evitably deteriorates the final reconstruction quality, es-pecially under severe occlusion.
In order to select well-captured views for each pixel, Vis-MVSNet [37] uses
pair-wise matching uncertainties as weighting guidance to attenuate pixels that have difficulties to match.
PVA-MVSNet [36] contains a CNN-based voxel-wise view ag-gregation module to guide multiple cost volume aggrega-tion. However, it is hard to give a perfect solution for the occlusion problem in general case.
Moreover, in order to meet the needs of various real-world applications, memory consumption is also essential for a scalable MVS algorithm. Instead of using 3D CNN, some recent methods [34, 31] apply recurrent convolution structure for cost volume regularization, which is effective and memory efficient to reconstruct scenes with wide ranges of depth.
To tackle the aforementioned problems, we therefore present a novel long short-term memory (LSTM) based recurrent multi-view stereo network with both intra-view and inter-view adaptive aggregation modules, namely AA-RMVSNet. The intra-view scheme is designed for robust feature extraction, where context-aware features are adap-tively aggregated for multiple scales and regions with vary-ing richness of texture; the inter-view scheme is used at multi-view cost volume aggregation step, whose aim is to overcome the difficulty of varying occlusion in complex scenarios by allocating higher weights on the well-matched view pairs. As a result, the proposed network is able to ob-tain accurate and complete depth maps to further generate high quality dense point clouds, as illustrated in Fig. 1.
The main contributions of this work are listed below:
• We introduce an intra-view feature aggregation mod-ule to adaptively extract image features by using de-formable convolution and multi-scale aggregation.
• We propose an inter-view cost volume aggregation module to adaptively aggregate cost volumes of dif-ferent views by yielding pixel-wise attention maps for each view.
• Our method ranks 1st among all submissions on Tanks and Temples online benchmark and obtains competi-tive results on DTU dataset. 2.