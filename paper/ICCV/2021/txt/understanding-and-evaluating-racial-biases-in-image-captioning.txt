Abstract 
Image captioning is an important task for benchmarking  visual reasoning and for enabling accessibility for people  with  vision  impairments.  However,  as  in  many  machine  learning  settings,  social  biases  can  infuence  image  cap-tioning  in  undesirable  ways.  In  this  work,  we  study  bias  propagation  pathways  within  image  captioning,  focusing  specifcally  on  the  COCO  dataset.  Prior  work  has  ana-lyzed gender bias in captions using automatically-derived  gender  labels;  here  we  examine  racial  and  intersectional  biases using manual annotations.  Our frst contribution is  in annotating the perceived gender and skin color of 28,315  of  the  depicted  people  after  obtaining  IRB  approval.  Us-ing these annotations, we compare racial biases present in  both manual and automatically-generated image captions. 
We  demonstrate  differences  in  caption  performance,  sen-timent,  and word choice between images of lighter versus  darker-skinned people.  Further,  we fnd the magnitude of  these  differences  to  be  greater  in  modern  captioning  sys-tems compared to older ones, thus leading to concerns that  without  proper  consideration  and  mitigation  these  differ-ences  will  only  become  increasingly  prevalent.  Code  and  data is available at https://princetonvisualai.  github.io/imagecaptioning-bias/.  1.

Introduction 
Computer vision applications have become ingrained in  numerous aspects of everyday life, and problematically, so  have the societal biases they contain.  For example, gender  and racial biases are prevalent in image tagging [62, 7] and  image search [39, 52]; visual recognition models have dis-parate error rates across demographics and geographic re-gions [16, 24]. The perpetuation and amplifcation of social  biases precipitate the need for a deeper exploration of these  systems and of the bias propagation pathways. 
We focus on the task of image captioning: the process of  generating a textual description of an image [69, 50, 76, 48,  3, 33].  This task serves as an important testbed for visual  reasoning  and  can  improve  accessibility  of  digital  images  for people who are blind or low vision. 
In  this  work,  we  assess  the  pathways  for  bias  propa-gation:  from  the  images,  to  the  manual  captions,  and  f-nally  to  the  automatically  generated  captions.  We  focus  our attention on studying the Common Objects in Context  (COCO)  [47,  19]  dataset;  it  is  a  widely  used  image  cap-tioning benchmark [32], thus making any biases especially  problematic [22].  We collect both skin color and perceived  gender annotations on 28,315 of the people in the COCO  2014 validation dataset after obtaining IRB approval.  This  data allows us (and future researchers) to analyze dispari-ties in image captioning (and other visual recognition tasks)  across different demographics. Concretely, we observe: 
•  The dataset is heavily skewed towards lighter-skinned  (7.5x  more  common  than  darker-skinned)  and  male  (2.0x more than female) individuals.1  Further, darker-skinned  females  are  especially  underrepresented,  ap-pearing 23.1x less than lighter-skinned males. 
•  There  are  racial  terms  (including  racial  slurs)  in  the  manual captions. The racial descriptors are not learned  by  the  older  captioning  systems  [59,  50],  but  are  learned  by  the  newer  transformer-based  models  [67]  – although the slurs do not yet appear to be learned. 
•  Image captioning systems perform slightly better (ac-cording to CIDEr [68] and BLEU [55],  although not 
SPICE [2]) on images of lighter-skinned people.  This  is consistent with disparate accuracies on e.g., pedes-trian detection [73] and facial recognition [16]. 
•  There are visual differences in the depictions of lighter  and darker-skinned individuals.  For example, lighter-skinned people tend to be pictured more with indoor  and  furniture objects,  whereas darker-skinned  people  tend to be more with outdoor and vehicle objects. 
•  Even after controlling for visual appearance, the cap-tions still differ in word choices used to describe im-ages  with  lighter  versus  darker-skinned  individuals. 
This  is  particularly  apparent  in  the  manual  captions  and in modern transformer-based systems. 
Our  work  lays  the  foundation  for  studying  bias  propaga-tion  in  image  captioning  on  the  popular  COCO  dataset.  1The gender disparity was previously observed in [78] although with  automatically-inferred rather than manually-annotated labels. 
Data  and  code  is  freely  available  for  research  pur-poses  at  https://princetonvisualai.github.  io/imagecaptioning-bias/.  2.