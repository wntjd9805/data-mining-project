Abstract
Attention module does not always help deep models learn causal features that are robust in any confounding context, e.g., a foreground object feature is invariant to dif-ferent backgrounds. This is because the confounders trick the attention to capture spurious correlations that benefit the prediction when the training and testing data are IID (identical & independent distribution); while harm the pre-diction when the data are OOD (out-of-distribution). The sole fundamental solution to learn causal attention is by causal intervention, which requires additional annotations of the confounders, e.g., a “dog” model is learned within
“grass+dog” and “road+dog” respectively, so the “grass” and “road” contexts will no longer confound the “dog” recognition. However, such annotation is not only pro-hibitively expensive, but also inherently problematic, as the confounders are elusive in nature. In this paper, we propose a causal attention module (CaaM) that self-annotates the confounders in unsupervised fashion. In particular, multi-ple CaaMs can be stacked and integrated in conventional attention CNN and self-attention Vision Transformer.
In
OOD settings, deep models with CaaM outperform those without it significantly; even in IID settings, the atten-tion localization is also improved by CaaM, showing a great potential in applications that require robust visual saliency. Codes are available at https://github.com/
Wangt-CN/CaaM . 1.

Introduction
Do you think attention [59, 53] would always capture the salient regions in an image? No, as shown in Figure 1 (a, top), due to the lack of region-level labels, “learning to attend” is a de facto weakly-supervised task. Or do you think attention would always improve performance? Prob-ably yes, after all, “Attention is All You Need” [14, 16, 44].
As shown in Figure 1 (a, top), even if the attended re-gion is wrong, the model still makes correct predictions.
In conventional IID settings, where the training and test-ing data are identically and independently distributed, the
Figure 1. (a) The qualitative attention maps of two images in
NICO [21] using ResNet18 with CBAM [57]. (b) The accuracies of three methods: ResNet18, ResNet18+CBAM [57] and + CaaM (Ours) in both IID and OOD settings. model equipped with attention is indeed better (red bar is higher than black bar in Figure 1 (b)).
However, few people realize that attention may do evil in OOD settings, where the testing data are out of the train-ing distribution. For example, as shown in Figure 1 (a, top), the attention considers the “ground” region as the visual cue of the “bird” class, because most training “bird” images are in “ground” context; but, when the test image is “bear in ground” (bottom), the attention misleads the model to still predict “bird”. Figure 1 (b) reports that the attention model is even worse than the non-attention baseline in OOD set-ting (red bar is lower than black bar), where the gap is fur-ther amplified by the rare object and context combination in training. Unfortunately, when we deploy such vision sys-tems in critical domains such as car autopilot, it is often the rare case that causes fatal accidents, e.g., recognizing a
“white” truck as “white” clouds1.
Astute readers who are knowledgeable in causality [27, 42] may point out that the key reason for the bipolar role of attention in IID and OOD is due to the confounding effect [55, 66, 65, 60].
In visual recognition, the causal pursuit between the input image X and the output label
Y is confounded by a common cause: the context S. To see the effect, during data collection, X is usually found 1https://www.youtube.com/watch?v=X3hrKnv0dPQ
split ④ does not only represent “sky” and “grass”, but also
“wing”. We formally formulate this problem in improper causal intervention in Section 3.1.
In this paper, we propose a causal attention module (CaaM \ka:m\) which generates data partition iteratively and self-annotates the confounders progressively to over-come the over-adjustment problem. Compared to the coarser contexts, multiple CaaM partitions are fine-grained and more exact to describe the comprehensive confounder.
As shown in Figure 2 (left bottom), each split of partition
TN has images of “bird” unfolding “wings” (see images in red boxes). This encourages the model to capture the
“wing” feature (see the improved visual attention), because
“wing” no longer co-varies with the ④ “Sky; Grass” con-text. Technically, besides a standard attention that attends to the desired causal features (e.g., foreground), CaaM has a complementary attention that deliberately captures the con-founding effect (e.g., background). The two disentangled attentions are optimized in an adversarial minimax fashion, which progressively constitute the confounder set and miti-gates the confounding bias in unsupervised fashion.
We analyze how CaaM learns better causal features than existing baselines in Section 3.2. In Section 3.3, we show two deployment examples on the popular attention-based deep models: CBAM-based CNN [57] and Transformer-based T2T-ViT [63]. Extensive qualitative and quantitative experimental results in Section 4 demonstrate the consistent gain achieved by CaaM.
Our technical contributions are summarized as:
• A novel yet practical visual attention module CaaM who learns causal features that are robust in OOD settings without sacrificing the performance in IID settings.
• We offer a causality-theoretic analysis to guarantee the superiority of CaaM.
• The design of CaaM is generic to popular deep networks. 2.