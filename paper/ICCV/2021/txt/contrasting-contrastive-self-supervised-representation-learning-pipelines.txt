Abstract
In the past few years, we have witnessed remarkable breakthroughs in self-supervised representation learning.
Despite the success and adoption of representations learned through this paradigm, much is yet to be understood about how different training methods and datasets inﬂuence per-formance on downstream tasks. In this paper, we analyze contrastive approaches as one of the most successful and popular variants of self-supervised representation learning.
We perform this analysis from the perspective of the train-ing algorithms, pre-training datasets and end tasks. We examine over 700 training experiments including 30 en-coders, 4 pre-training datasets and 20 diverse downstream tasks. Our experiments address various questions regard-ing the performance of self-supervised models compared to their supervised counterparts, current benchmarks used for evaluation, and the effect of the pre-training data on end task performance. Our Visual Representation Bench-mark (ViRB) is available at: https://github.com/ allenai/virb. 1.

Introduction
Learning compact and general representations that can be used in a wide range of downstream tasks is one of the holy grails of computer vision. In the past decade, we have witnessed remarkable progress in learning representations from massive amounts of labeled data [34, 55, 25]. More recently, self-supervised representation learning methods that do not rely on any explicit external annotation have also achieved impressive performance [24, 38, 7, 23, 5].
Among the most successful approaches are contrastive self-supervised learning methods that achieve results close to their supervised counterparts. These methods typically learn by contrasting latent representations of different aug-mentations, transformations or cluster assignments of im-ages. With a sufﬁcient amount of transformations and im-ages to contrast, the model is driven to learn powerful rep-resentations.
The most common protocol for comparing representa-tions learned by self-supervised methods is to pre-train
Figure 1. Our goal is to study recently proposed contrastive self-supervised representation learning methods. We examine three main variables in these pipelines: training algorithms, pre-training datasets and end tasks. We consider 4 training algorithms, 4 pre-training datasets and 20 diverse end tasks for this study. models on a large dataset such as ImageNet [15] with-out using class labels and then use the learned represen-tations for training end tasks such as image classiﬁcation, object detection or segmentation. Although this protocol has been widely adopted, it provides an incomplete picture of progress, since the noticeable similarities between com-mon pre-training and end tasks might lead to biased and optimistic estimates of performance.
In this work, we provide a comprehensive study of rep-resentations learned by contrastive self-supervised methods.
We explore various alternatives for algorithms, pre-training datasets and end tasks (Figure 1), covering a total of 735 experiments, using 4 algorithms, 4 pre-training datasets and 20 diverse end tasks. Our goal is to provide answers to the following open questions: (1) Is supervised learning on Im-ageNet a good default encoder choice for end tasks? (2) Is
ImageNet accuracy a good metric for measuring progress
in self-supervised representation learning? (3) How do dif-ferent training algorithms compare for different end tasks? (4) Does self-supervision provide better encoders for certain types of end tasks? (5) Does the distribution of pre-training data affect the end-task performance? (6) Do we learn poor representations when using highly unbalanced datasets?
We perform an extensive set of experiments to systemat-ically analyze contrastive self-supervision and provide an-swers to the above questions. We observe a mixture of un-intuitive and intuitive results, which better demonstrate the characteristics of contrastive self-supervised models. 2.