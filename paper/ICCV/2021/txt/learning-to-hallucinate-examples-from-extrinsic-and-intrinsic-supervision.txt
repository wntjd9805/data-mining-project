Abstract
Learning to hallucinate additional examples has recently been shown as a promising direction to address few-shot learning tasks. This work investigates two important yet overlooked natural supervision signals for guiding the hallu-cination process – (i) extrinsic: classifiers trained on hallu-cinated examples should be close to strong classifiers that would be learned from a large amount of real examples; and (ii) intrinsic: clusters of hallucinated and real exam-ples belonging to the same class should be pulled together, while simultaneously pushing apart clusters of hallucinated and real examples from different classes. We achieve (i) by introducing an additional mentor model on data-abundant base classes for directing the hallucinator, and achieve (ii) by performing contrastive learning between hallucinated and real examples. As a general, model-agnostic framework, our dual mentor- and self-directed (DMAS) hallucinator significantly improves few-shot learning performance on widely-used benchmarks in various scenarios. 1.

Introduction
To alleviate the reliance on large, labeled datasets for learning deep models, few-shot learning has attracted increas-ing attention, with the goal of learning novel concepts from one, or only a few, annotated examples [20, 68, 71, 59, 21].
Existing work tries to solve this problem from the perspec-tive of meta-learning [55, 7, 64], which is motivated by the human ability to leverage prior experiences when tackling a new task. Unlike the standard machine learning paradigm, where a model is trained on a set of examples, meta-learning is performed on a set of “simulated” tasks, each consisting of its own support and query sets [68]. The support set is used as the few-shot training data for the leaner, and the query set is used as the test data to evaluate the leaner’s quality.
By sampling small support and query sets from a large col-*equal contribution
†work done while visiting CMU
Figure 1: Learning a hallucinator to generate useful examples for few-shot learning through extrinsic and intrinsic supervision. Dur-ing meta-training, we sample a few-shot task (e.g., 2-way 2-shot classification) on base classes (Fig. 1a). Extrinsic supervision:
The desired classifier for this task is the (dashed) one that would be learned from a large set of real examples (Fig. 1b). We explicitly introduce this strong classifier as “mentor” (abundant examples are available for base classes). We then learn the hallucinator in a way that minimizes the discrepancy between the (solid) “student” clas-sifier (trained on hallucinated examples together with the few real examples) and the (dashed) mentor classifier (Fig. 1c). Intrinsic supervision: Through contrastive learning, clusters of hallucinated and real examples belonging to the same class are pulled together (→←), while simultaneously pushing apart (↔) clusters of hallu-cinated and real examples from different classes (Fig. 1c). During meta-testing, we use the meta-trained, fixed hallucinator to gen-erate additional examples as augmentation for learning classifiers on novel classes. Real examples as light diamonds, hallucinated examples as dark triangles, and classifiers as solid or dashed lines. lection of labeled examples of base classes, meta-learning based approaches learn to extract task-agnostic knowledge, and apply it to a new few-shot learning task of novel classes.
One notable type of task-agnostic (or meta) knowledge comes from the shared mechanism of data augmentation or hallucination across categories [70, 22, 56, 84]. Since synthesizing raw images is often challenging or sometimes unnecessary, recent work has instead focused on hallucinat-ing examples in a learned feature space [70, 22, 56, 84, 76, 83, 87]. This can be achieved by, for example, integrating a
“hallucinator” module into a meta-learning framework, where it generates hallucinated examples guided by real ones from the support set [70]. The hallucinator captures the intra-class variation shared across categories, which generalizes to un-seen classes. The learner then uses an augmented training set, which includes both the real and the hallucinated examples to
learn classifiers. The hallucinator is meta-trained end-to-end with the learner, through back-propagating a classification loss based on ground-truth labels of query data.
Despite the success of prior approaches, we argue that solely using the classification loss on the small query set as supervision is insufficient to adjust the hallucinator to produce effective samples in the few-shot regime. Therefore, the performance of the classifiers trained on hallucinated examples is still substantially inferior to that of the classi-fiers trained on real examples [16, 58]. To overcome this challenge, our key insight is that there are two important yet under-explored natural signals for guiding the data gen-eration process – extrinsic and intrinsic supervision. This work explores how to leverage such supervision to enable hallucinating examples in a way that helps the classification algorithm learn better classifiers.
The first source of supervision is an extrinsic signal from large-sample learning. As illustrated in Figure 1, to be most helpful as a hallucinator, a classifier trained on the hallucinated examples (which are generated from a small support set of real samples) is expected to be close to a strong classifier that would be trained on a large amount of real examples . This extrinsic signal from large-sample learning is a natural source of supervision for few-shot learning, but it has been largely overlooked in prior work. While we have very little data on novel classes, we do have a large number of real examples on base classes. Therefore, on base classes we introduce a “mentor” model, which is a strong classifier pre-trained on all the available large amount of real examples. Correspondingly, the classifier trained on hallucinated examples along with few real support examples becomes the “student.”
We now minimize the discrepancy between the student and mentor classifiers. A straightforward approach would be minimizing the distance between the two classifiers in the parameter space [71, 72, 11], which tends to be difficult and noisy due to the lack of suitable metrics. Hence, we instead encourage the output predictions from the student classifier (e.g., the distribution of class probabilities) to be similar to those predicted by the mentor on the query set. This way of learning is reminiscent of knowledge distillation [29]. By doing so, the hallucinator explicitly learns how to produce examples that enable the student classifier to mimic the be-havior of the mentor. Note that the student-mentor pairs are only used for meta-training on base classes; there are no mentor classifiers for meta-testing on novel classes.
In practice, the student and mentor classifiers could be quite different from each other at the beginning of the train-ing, if the mentor is produced by a large amount of real examples while the student has access to only few real ex-amples. To address this issue, we propose a progressive guidance scheme inspired by curriculum learning [8], and explore two dual directions – (1) we start with a mentor and a student, both trained on a small number of real examples, and we gradually strengthen the mentor by re-training it with increasing number of real examples; and (2) we start with a mentor and a student, both trained on a large number of real examples, and we gradually weaken the student by removing its real examples. During both of the processes, the hallucinator is also trained progressively.
The second source of supervision is the intrinsic label consistency between hallucinated and real examples. As illustrated in Figure 1, hallucinated and real examples be-longing to the same class should be pulled together, while simultaneously pushing apart clusters of hallucinated and real examples from different classes. However, without ap-propriate constraints, the hallucinated examples might be noisy and spread over across class boundaries (e.g., a halluci-nated dog example resides within the cat cluster). To this end, we formulate the problem as supervised contrastive learn-ing, inspired by recent progress on self-supervised learn-ing [74, 28, 12, 30]. We treat hallucinated and real examples as different views of the data, and generate the positive and negative pairs correspondingly. For example, the positives are drawn from both hallucinated and real samples of the same class. Note that different from conventional contrastive learning that learns an embedding space (where the data augmentation is pre-defined), we use the contrastive loss to self-direct the hallucinated examples in the right class cluster or manifold (where is the feature space is pre-trained).
Our contributions are three-fold. (1) By jointly lever-aging the complementary extrinsic and intrinsic supervi-sion, we develop a general meta-learning with hallucina-tion framework. (2) We not only extract shared knowledge across a collection of few-shot learning tasks, similar to most existing meta-learning methods, but also progressively ex-ploit extrinsic knowledge in large-sample models trained on base classes as mentor to guide hallucination and few-shot learning. (3) Through a contrastive learning process, the hallucinated examples are self-directed to maintain the intrinsic label consistency with real examples. Our dual mentor- and self-directed (DMAS) hallucinator is model-agnostic, which can generate data in different feature spaces and can be combined with different classification models to consistently boost their few-shot learning performance on a variety of benchmarks, including ImageNet1K [27, 70], miniImageNet [68, 49], tieredImageNet [51], and CUB [69]. 2.