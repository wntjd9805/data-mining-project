Abstract
Inferring objects and their relationships from an image in the form of a scene graph is useful in many applications at the intersection of vision and language. We consider a challenging problem of compositional generalization that emerges in this task due to a long tail data distribution. Cur-rent scene graph generation models are trained on a tiny fraction of the distribution corresponding to the most fre-quent compositions, e.g. <cup, on, table>. However, test images might contain zero- and few-shot compositions of objects and relationships, e.g. <cup, on, surfboard>. De-spite each of the object categories and the predicate (e.g.
‘on’) being frequent in the training data, the models often fail to properly understand such unseen or rare compositions.
To improve generalization, it is natural to attempt increas-ing the diversity of the training distribution. However, in the graph domain this is non-trivial. To that end, we pro-pose a method to synthesize rare yet plausible scene graphs by perturbing real ones. We then propose and empirically study a model based on conditional generative adversarial networks (GANs) that allows us to generate visual features of perturbed scene graphs and learn from them in a joint fashion. When evaluated on the Visual Genome dataset, our approach yields marginal, but consistent improvements in zero- and few-shot metrics. We analyze the limitations of our approach indicating promising directions for future research. 1.

Introduction
Reasoning about the world in terms of objects and re-lationships between them is an important aspect of human and machine cognition [21]. In our environment, we can often observe frequent compositions such as “person on a surfboard” or “person next to a dog”. When we are faced with a rare or previously unseen composition such as “dog
*This work was partially done while the author was an intern at Mila.
Correspondence to: bknyazev@uoguelph.ca (a) The triplet distribution in Visual Genome [38] is
Figure 1. extremely long-tailed, with numerous few- and zero-shot composi-tions (highlighted in red and yellow respectively). (b) The training set contains a tiny fraction (3%) of all possible triplets, while many other plausible triplets exist. We aim to “hallucinate” such com-positions using GANs to increase the diversity of training samples and improve generalization. Recall results are from [67]. on a surfboard”, to understand the scene we need to un-derstand the concepts of ‘person’, ‘dog’, ‘surfboard’ and
‘on’. While such unbiased reasoning about concepts is easy for humans, for machines this task has remained extremely challenging [3, 32, 4, 35, 40]. Learning-based models tend to capture spurious statistical correlations in the training data [2, 52], e.g. ‘person’ rather than ‘dog’ has always oc-curred on a surfboard. When the evaluation is explicitly focused on compositional generalization – ability to recog-nize novel or rare combinations of objects and relationships – such models then can fail remarkably [3, 45, 67, 36].
Predicting compositions of objects and the relationships between them from images is part of the scene graph gen-eration (SGG) task. SGG is important, because accurately inferred scene graphs can improve downstream results in tasks, such as VQA [83, 29, 7, 41, 63, 26, 12], image cap-tioning [78, 22, 42, 72, 48], retrieval [33, 5, 67, 69, 62] and others [1, 75]. However, inferring scene graphs accurately is
Figure 2. The distributions of top-25 predicate (left) and object (right) categories in Visual Genome [38] (split of [74]). challenging due to a long tail data distribution and inevitable appearance of zero-shot (ZS) compositions (triplets) of objects and relationships at test time, e.g. “cup on surfboard” (Figure 1). The SGG results using the recent Total Direct
Effect (TDE) method [67] show a severe drop in ZS recall highlighting the extreme challenge of compositional general-ization. This might appear surprising given that the marginal distributions in the entire scene graph dataset (e.g. Visual
Genome [38]) and the ZS subset are very similar (Fig. 2).
More specifically, the predicate and object categories that are frequent in the entire dataset, such as ‘on’, ‘has’ and ‘man’,
‘person’ also dominate among the ZS triplets. For example, both “cup on surfboard” and “bear has helmet” consist of frequent entities, but represent extremely rare compositions (Fig. 1). This strongly suggests that the challenging nature of correctly predicting ZS triplets does not directly stem from the imbalance of predicates (or objects), as commonly viewed in the previous SGG works, where the models attempt to improve mean (or predicate-normalized) recall metrics [9, 18, 68, 85, 67, 10, 80, 44, 81, 76]. Therefore, we focus on compositional generalization and associated zero-and few-shot metrics.
Despite recent improvements in compositional general-ization within the SGG task [67, 36, 65], the state-of-the-art result in zero-shot recall is still 4.5% compared to 41% for all-shot recall (Figure 3). To address compositional general-ization, we consider exposing the model to a large diversity of training examples that can lead to emergent generaliza-tion [27, 58]. To avoid expensive labeling of additional data, we propose a compositional augmentation approach based on conditional generative adversarial networks (GANs) [19, 49].
Our general idea is augmenting the dataset by perturbing scene graphs and corresponding visual features of images, such that together they represent a novel or rare situation.
Overall, we make the following contributions:
•We propose scene graph perturbation methods (§ 3.1.1) as part of a GAN-based model (§ 3.1), to augment the training set with underrepresented compositions;
•We propose natural language- and dataset-based met-rics to evaluate the quality of (perturbed) scene graphs (§ 3.2);
•We extensively evaluate our model and outperform a strong baseline in zero-, few- and all-shot recall (§ 4).
Our code is available at https://github.com/ bknyaz/sgg.
[74]
[82]
[82]
[9]
[67]
[67]
[36]
Figure 3. In this work, the compositional augmentations we propose improve on zero-shot (ZS) as well as all-shot recall. 2.