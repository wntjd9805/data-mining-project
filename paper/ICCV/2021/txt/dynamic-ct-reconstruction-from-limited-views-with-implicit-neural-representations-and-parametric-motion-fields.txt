Abstract
Reconstructing dynamic, time-varying scenes with com-puted tomography (4D-CT) is a challenging and ill-posed problem common to industrial and medical settings. Ex-isting 4D-CT reconstructions are designed for sparse sam-pling schemes that require fast CT scanners to capture multiple, rapid revolutions around the scene in order to generate high quality results. However, if the scene is moving too fast, then the sampling occurs along a lim-ited view and is difﬁcult to reconstruct due to spatiotem-In this work, we design a reconstruc-poral ambiguities. tion pipeline using implicit neural representations coupled with a novel parametric motion ﬁeld warping to perform limited view 4D-CT reconstruction of rapidly deforming scenes. Importantly, we utilize a differentiable analysis-by-synthesis approach to compare with captured x-ray sino-gram data in a self-supervised fashion. Thus, our result-ing optimization method requires no training data to re-construct the scene. We demonstrate that our proposed system robustly reconstructs scenes containing deformable and periodic motion and validate against state-of-the-art baselines. Further, we demonstrate an ability to recon-struct continuous spatiotemporal representations of our scenes and upsample them to arbitrary volumes and frame rates post-optimization. This research opens a new av-enue for implicit neural representations in computed to-mography reconstruction in general. Code is available at https://github.com/awreed/DynamicCTReconstruction. 1.

Introduction
Computed-tomography (CT) is a mature imaging tech-nology with vital industrial and medical applications [20, 13, 8]. CT scanners capture x-ray data or sinograms by scanning a sequence of angles around an object. Recon-struction algorithms then estimate the scene from these measured sinograms. CT imaging in both 2D and 3D of static objects is a well-studied inverse problem with both theoretical and practical algorithms [21, 37, 1].
However, reconstruction of dynamic scenes (i.e. scene features changing over time), known as dynamic 4D-CT, is a severely ill-posed problem because the sinogram aggre-gates measurements over time which yields spatio-temporal ambiguities [12, 24, 51, 52, 30]. Analogous to motion blur, a static or quasi-static scene is only captured for a small angular range of the sinogram (motion blur analogy: short exposure), and this mapping is a function of the amount of scene motion relative to the CT scanner’s rotation speed.
Traditional CT reconstruction algorithms have limited capa-bility to address 4D-CT problems due to difﬁculty account-ing for this motion. Yet solving these 4D-CT problems is critically important for a range of applications from clinical diagnosis to non-destructive evaluation for material charac-teristics and metrology. 4D-CT reconstruction techniques have been proposed in the literature to handle periodic motion [33, 24] as well as more general nonlinear, deformable motion [51, 52, 30].
While the latter methods achieve state-of-the-art for de-formable motion, they typically assume slow motion rela-tive to the scanner rotation speed. Speciﬁcally, these algo-rithms assume sparse measurements of the object that span the full angular range (0 − 360◦) and typically require mul-tiple revolutions around the sample to reconstruct images at each time step. This approach is called sparse view CT in the literature [31]. However, this setting is not always practical such as when the object motion is too fast for the scanner to make multiple revolutions. Instead, an alterna-tive approach is to collect measurements over partial angu-lar ranges in a single revolution – which is the more chal-lenging limited-view reconstruction [43]. Even in the static case, the limited-view problem is challenging due to miss-ing information often leading to signiﬁcant artifacts [18].
Figure 1. Given a sinogram, we jointly estimate a scene template and motion ﬁeld to reconstruct the 4D scene. Here, we warp a 3D
Shepp Logan template to reconstruct its linear translation in time. We simulate sinogram measurements from this 4D scene and compute a loss with the given sinogram. This loss is backpropagated to the implicit neural representation (INR) network weights and motion ﬁeld parameters until convergence.
Key Contributions: In this paper, we propose a novel, training-data-free approach for 4D-CT reconstruction that works especially well in limited-view scenarios. Our method, illustrated in Figure 1, consists of an implicit neu-ral representation (INR) [29] model that acts as the static scene prior coupled with a parametric motion ﬁeld to es-timate an evolving 3D object over time. The reconstruc-tion is then synthesized into sinogram measurements using a differentiable Radon transform to simulate parallel-beam
CT scanners. By minimizing the discrepancy between the synthesized and observed sinograms, we are able to opti-mize both the INR weights and motion parameters in a self-supervised, analysis-by-synthesis fashion to obtain accurate dynamic scene reconstructions without training data.
Validation: We are primarily interested in general, in-situ CT imaging where the scanned object cannot be frozen in time and the motion dynamics are arbitrary (e.g., non-periodic) and cannot be predicted (e.g., [17]). However, ac-quiring 4D-CT data is challenging and one of the primary bottlenecks for research in 4D-CT reconstruction. While this is partly due to the expense and logistics of accessing
CT scanners and data, it is also because acquiring ground truth in the 4D case is exceptionally challenging. While there are examples of real CT data [51, 52], these datasets are speciﬁc to particular scanners and sampling schemes.
In order to evaluate our method on an in-situ imaging task and highlight our method’s ability to resolve 4D scenes from limited angles, we introduce a synthetic dataset for parallel beam CT. This dataset is generated with an accu-rate physics simulator for material deformation and used to benchmark ours and competing state-of-the-art methods.
Further, while not our target application, we also evaluate our method’s performance on a medical imaging task of reconstructing a periodically deforming thoracic cavity us-ing simulated x-ray measurements from real thoracic recon-structions. We observe that our method outperforms com-petitive baselines on both of our datasets. 2.