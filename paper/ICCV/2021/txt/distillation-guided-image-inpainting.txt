Abstract
Image inpainting methods have shown significant im-provements by using deep neural networks recently. How-ever, many of these techniques often create distorted struc-tures or blurry inconsistent textures. The problem is rooted in the encoder layers’ ineffectiveness in building a com-plete and faithful embedding of the missing regions from scratch. Existing solutions like course-to-fine, progressive refinement, structural guidance, etc. suffer from huge com-putational overheads owing to multiple generator networks, limited ability of handcrafted features, and sub-optimal uti-lization of the information present in the ground truth. We propose a distillation-based approach for inpainting, where we provide direct feature level supervision while training.
We deploy cross and self-distillation techniques and de-sign a dedicated completion-block in encoder to produce more accurate encoding of the holes. Next, we demonstrate how an inpainting network’s attention module can improve by leveraging a distillation-based attention transfer tech-nique and further enhance coherence by using a pixel-adaptive global-local feature fusion. We conduct extensive evaluations on multiple datasets to validate our method.
Along with achieving significant improvements over previ-ous SOTA methods, the proposed approach’s effectiveness is also demonstrated through its ability to improve existing inpainting works. 1.

Introduction
Image inpainting is aimed at filling damaged or substi-tuting undesired areas of images with plausible and fine-grained contents.
It has a broad range of applications in fields of restoring damaged photographs, retouching pic-tures, etc. Early conventional works typically use low-level features hand-crafted from the incomplete input image and resort to priors (e.g., image statistics) or auxiliary data (e.g., external image databases). They either propagate low-level features from surroundings to the missing regions following a diffusive process [33, 2] or fill holes by searching and fus-ing similar patches from the same image or external image
*Work done while at Indian Institute of Technology Madras, India. databases [29]. Although these methods have good effects in the completion of repeating structures, they are restricted by the available image statistics and cannot produce novel image contents. In recent years, deep learning based meth-ods have been reported to surmount these limitations by uti-lizing large volumes of training images. However, many of these methods suffer from the severe ill-posedness of the task. The hole regions are entirely empty, and without suffi-cient guidance, neural networks struggle to reconstruct the missing contents from scratch satisfactorily.
Works like [28, 15, 18] deploy a single generative model to solve the task. To handle the inherent ill-posedness, a large majority of works uses some form of guidance at in-ference time. We can broadly divide these methods into (a) Coarse-to-fine: One group of works two categories:
[47, 48, 22] deploys a two-stage architecture to do content formation and texture refinement separately in a step-by-step manner. These methods typically produce an inter-mediate coarse image with recovered structures in the first stage and send it to the second stage for texture generation.
Another group of works tries to inpaint the missing region in a progressive manner using a single network [11, 17, 51]. (b) Structural guidance: Recently, [26] used edge generator within the two-stage architecture. [44] proposed a contour generator instead of edge. Such methods suffer from sev-eral limitations: (a) It takes many more parameters to de-ploy two generators. Methods like [44] require even more parameters for the structure prediction branch. Progressive or recurrent approaches typically suffer from slow inference speed or high computational cost. (b) Although structural knowledge improves performance, it is still limited due to the handcrafted choice of the auxiliary information. For ex-ample, the optimal structure information might vary from one scene to another (edge vs. contour).
The inpainted image quality depends heavily on the coarse network, and [35] experimentally verified that if we remove the coarse network and train end-to-end only the final network of such two-stage methods, it results in a no-table drop in performance. In this work, we argue that the root cause lies in how existing networks are trained. We empirically show that when training a single network, using the ground-truth image as the only supervision at the end
knowledge from one architecture (teacher) to another (stu-dent). We use the AN as the ‘teacher’ network that provides supervisory signals for different layers of the ‘student’ IN.
For encoder, we use feature maps of AN to inform the IN about the ideal embeddings of the holes. To make sure that the IN encoder is able to mimic this ‘ideal’ target as close as possible, we propose a dedicated ‘completion block.’ It solely focuses on filling the holes by adaptively gathering relevant information from the neighborhood. We show that completion block coupled with AN’s intermediate supervi-sion brings significant performance improvement to an en-coder’s performance (Fig. 1, 4th column).
After generating a coarse embedding of the holes, we focus on refining it in the decoder. Attention modules are de facto standard for feature refinement, which inher-ently assumes that the missing regions in the input fea-ture are roughly complete, which is needed to generate a valid pixel/patch-wise similarity score. This assumption can fail for complex holes, hole boundaries, etc. We de-sign a distillation-based attention transfer technique, where we force the attention module of IN to learn the ideal pixel/patch-wise similarities from the same module in the
AN. Further, to generate more refined results, we intro-duce a pixel-adaptive global-local feature fusion technique, which allows each hole pixel to generate content consistent with the immediate local neighborhood and boundaries.
Our main contributions are as follows: (1) We propose a distillation based training strategy for in-painting. For encoder, we demonstrate the utility of deep feature level supervision coupled with dedicated adaptive completion-blocks that generates much better embedding of the hole regions. (2) For decoder, we design an atten-tion transfer technique that enables the attention module to learn an ideal affinity-finding behavior for refining the coarse embeddings from the encoder. Further, we design a pixel-adaptive global-local consistent structure for gen-erating more coherent results. (3) The proposed training strategy directly helps the inpainting network to learn bet-ter and does not require multiple generators or progressive refinement at inference time, increasing efficiency. Our net-work shows superior performance on 3 standard datasets.
We also verify the efficacy of the proposed distillation strat-egy by demonstrating its role in improving existing SOTA methods. 2.