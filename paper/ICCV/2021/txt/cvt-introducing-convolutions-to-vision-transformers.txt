Abstract
We present in this paper a new architecture, named Con-volutional vision Transformer (CvT), that improves Vision
Transformer (ViT) in performance and efficiency by intro-ducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifica-tions: a hierarchy of Transformers containing a new convo-lutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural net-works (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of
Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting ex-tensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transform-ers and ResNets on ImageNet-1k, with fewer parame-ters and lower FLOPs.
In addition, performance gains are maintained when pretrained on larger datasets (e.g.
ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 ac-curacy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial com-ponent in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher res-olution vision tasks. Code will be released at https:
//github.com/microsoft/CvT. 1.

Introduction
Transformers [30, 9] have recently dominated a wide range of tasks in natural language processing (NLP) [31].
The Vision Transformer (ViT) [10] is the first computer vi-sion model to rely exclusively on the Transformer archi-tecture to obtain competitive image classification perfor-mance at large scale. The ViT design adapts Transformer
*This work is done when Haiping Wu was an intern at Microsoft.
†Corresponding author
Figure 1: Top-1 Accuracy on ImageNet validation com-pared to other methods with respect to model parame-ters. (a) Comparison to CNN-based model BiT [17] and
Transformer-based model ViT [10], when pretrained on
ImageNet-22k. Larger marker size indicates larger archi-tectures. (b) Comparison to concurrent works: DeiT [29],
T2T [40], PVT [33], TNT [13] when pretrained on
ImageNet-1k. architectures [9] from language understanding with mini-mal modifications. First, images are split into discrete non-overlapping patches (e.g. 16 × 16). Then, these patches are treated as tokens (analogous to tokens in NLP), summed with a special positional encoding to represent coarse spa-tial information, and input into repeated standard Trans-former layers to model global relations for classification.
Despite the success of vision Transformers at large scale, the performance is still below similarly sized convolutional neural network (CNN) counterparts (e.g., ResNets [14]) when trained on smaller amounts of data. One possible rea-son may be that ViT lacks certain desirable properties in-herently built into the CNN architecture that make CNNs uniquely suited to solve vision tasks. For example, im-ages have a strong 2D local structure: spatially neighbor-ing pixels are usually highly correlated. The CNN archi-tecture forces the capture of this local structure by using 1
Method
ViT [10], DeiT [29]
CPVT [6]
TNT [13]
T2T [40]
PVT [33]
CvT (ours)
Needs Position Encoding (PE) yes no (w/ PE Generator) yes yes yes no
Token Embedding non-overlapping non-overlapping non-overlapping (patch+pixel) overlapping (concatenate) non-overlapping overlapping (convolution)
Projection for Attention Hierarchical Transformers linear linear linear linear spatial reduction convolution no no no partial (tokenization) yes yes
Table 1: Representative works of vision Transformers. local receptive fields, shared weights, and spatial subsam-pling [19], and thus also achieves some degree of shift, scale, and distortion invariance. In addition, the hierarchi-cal structure of convolutional kernels learns visual patterns that take into account local spatial context at varying levels of complexity, from simple low-level edges and textures to higher order semantic patterns.
In this paper, we hypothesize that convolutions can be strategically introduced to the ViT structure to improve performance and robustness, while concurrently maintain-ing a high degree of computational and memory efficiency.
To verify our hypothesises, we present a new architecture, called the Convolutional vision Transformer (CvT), which incorporates convolutions into the Transformer that is in-herently efficient, both in terms of floating point operations (FLOPs) and parameters.
The CvT design introduces convolutions to two core sec-tions of the ViT architecture. First, we partition the Trans-formers into multiple stages that form a hierarchical struc-ture of Transformers. The beginning of each stage consists of a convolutional token embedding that performs an over-lapping convolution operation with stride on a 2D-reshaped token map (i.e., reshaping flattened token sequences back to the spatial grid), followed by layer normalization. This allows the model to not only capture local information, but also progressively decrease the sequence length while si-multaneously increasing the dimension of token features across stages, achieving spatial downsampling while con-currently increasing the number of feature maps, as is per-formed in CNNs [19]. Second, the linear projection prior to every self-attention block in the Transformer module is replaced with our proposed convolutional projection, which employs a s × s depth-wise separable convolution [5] oper-ation on an 2D-reshaped token map. This allows the model to further capture local spatial context and reduce seman-tic ambiguity in the attention mechanism. It also permits management of computational complexity, as the stride of convolution can be used to subsample the key and value ma-trices to improve efficiency by 4× or more, with minimal degradation of performance.
In summary, our proposed Convolutional vision Trans-former (CvT) employs all the benefits of CNNs: local re-ceptive fields, shared weights, and spatial subsampling, while keeping all the advantages of Transformers: dynamic attention, global context fusion, and better generalization.
Our results demonstrate that this approach attains state-of-art performance when CvT is pre-trained with ImageNet-1k, while being lightweight and efficient: CvT improves the performance compared to CNN-based models (e.g. ResNet) and prior Transformer-based models (e.g. ViT, DeiT) while utilizing fewer FLOPS and parameters. In addition, CvT achieves state-of-the-art performance when evaluated at larger scale pretraining (e.g. on the public ImageNet-22k dataset). Finally, we demonstrate that in this new design, we can drop the positional embedding for tokens without any degradation to model performance. This not only simplifies the architecture design, but also makes it readily capable of accommodating variable resolutions of input images that is critical to many vision tasks. 2.