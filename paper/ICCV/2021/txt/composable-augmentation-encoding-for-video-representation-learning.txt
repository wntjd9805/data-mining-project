Abstract
View 2: !!"!#
We focus on contrastive methods for self-supervised video representation learning. A common paradigm in con-trastive learning is to construct positive pairs by sampling different data views for the same instance, with different data instances as negatives. These methods implicitly as-sume a set of representational invariances to the view se-lection mechanism (e.g., sampling frames with temporal shifts), which may lead to poor performance on downstream tasks which violate these invariances (ﬁne-grained video action recognition that would beneﬁt from temporal infor-mation). To overcome this limitation, we propose an ‘aug-mentation aware’ contrastive learning framework, where we explicitly provide a sequence of augmentation param-eterisations (such as the values of the time shifts used to create data views) as composable augmentation encodings (CATE) to our model when projecting the video representa-tions for contrastive learning. We show that representations learned by our method encode valuable information about speciﬁed spatial or temporal augmentation, and in doing so also achieve state-of-the-art performance on a number of video benchmarks. 1.

Introduction
We focus on contrastive learning [16] of self-supervised video representations. The contrastive objective is simple: it pulls the latent representations of positive pairs close to each other, while pushing negative pairs apart. It is a natural
ﬁt for self-supervised learning, where positive and negative pairs can be constructed from the data itself, without the need for additional annotations.
Amongst different positive pair generation techniques, a particularly successful one has been augmentation-invariant contrastive learning [8, 20, 61], which has shown impres-sive results for image representation learning.
In this in-stance discrimination framework, positive pairs are con-structed by applying artiﬁcial aggressive photo-geometric data augmentations to create different versions of the same instances – learned representations are thus encouraged to
View 1: !! e n t t o  ti m a ria v
I n
Encode "′
Representation space
Aware of time
Figure 1. Standard self-supervised contrastive learning methods are augmentation-invariant, they ingest two augmented views of the same instance and encourage their latent representations to be similar. For videos, if the two views were sampled with a temporal shift, this approach would learn representations that are invariant to time changes, losing valuable temporal dynamics information (e.g. the action of slicing oranges) for downstream tasks. We in-stead project latent representations by additionally encoding the relative transformation of the two views (e.g. the time shift by t0) with a composable augmentation encoding (CATE) to make the representation augmentation-aware. be invariant to these data augmentations.
A number of self-supervised works also extend this idea to videos, where instead of artiﬁcial augmentations, they treat temporal shift as a natural data augmentation [46, 57], where two frames from the same video with a temporal shift form a positive pair. While this is useful for capturing high-level semantic information, e.g. knowledge of object cate-gories, it can remove ﬁne-grained information that may be useful depending on the downstream task – as previously studied in images [8, 63]. Consider the case of the oranges in Figure 1 – for object classiﬁcation, invariance to small time shifts in a video that can result in view point changes or shape deformations may be useful and even desirable.
However, for a downstream task involving reasoning about temporal relationships, such as recognising the state tran-sition between a whole orange and orange slices caused by the act of cutting fruit, a representation invariant to temporal shifts may have lost valuable information.
We argue that augmentation-aware information can be
retained if the relative augmentations of the two views are known to the contrastive learning framework. For the or-anges in Figure 1, an encoder could keep the shape infor-mation of the sliced orange view if it is aware that the other view is t0 behind in time (and likely to be a whole orange).
We therefore propose a generalised framework for self-supervised video representation learning as follows. For no-tational convenience, we use the term data augmentation to consider all parameterisable data transformations, includ-ing shifts in space or in time. We then apply these gener-alised data augmentations to create different views of the same data, as is done in previous augmentation-invariant contrastive learning. However, instead of directly applying a contrastive loss on these views, we apply an additional projection head that optionally also encodes the augmenta-tions that were used to create the views in the ﬁrst place.
For example, given two views of an image obtained via cropping, this can be an encoding of the bounding box co-ordinates of the cropping spatial transformation. For videos, this encoding can also include information about the tempo-ral relationship between the views (e.g. a 5-second shift), in addition to spatial transformations. In the case of missing or occluded sequential data, we can also specify the partic-ular locations to be predicted. When no such encodings are provided, our framework becomes standard augmentation-invariant contrastive learning.
We formulate this framework as a prediction task, given sequential data as input. The input sequence in this case contains encoded visual representations to be learned, and optionally a set of encoded data transformations for predic-tion. By using transformers, we can easily compose mul-tiple encoded data transformations in the input sequence.
We refer to this projection as a Composable AugmenTation
Encoding (CATE) model. When data transformations are explicitly encoded in this way, our training objective can motivate a model to utilise such information if it helps with learning (e.g. learning the temporal dynamics that oranges can be cut into slices). We choose to always sample neg-ative pairs across different instances, so the model has the freedom to ignore the transformation encodings if they do not help in reducing the contrastive loss. However, em-pirical results show that they are almost always utilised.
We conduct thorough evaluations to test the efﬁcacy of our framework, and in doing so make the following contribu-tions: (1) we propose Composable AugmenTation Encod-ing (CATE) to learn augmentation-aware representations, and validate that CATE learns representations that preserve useful information (e.g. location, arrow of time) more ef-fectively than a view-invariant baseline without augmen-tation encoding; (2) we perform a number of ablations on augmentation type and parameterisation, and observe that different downstream tasks favour the awareness of dif-ferent augmentations, e.g. temporal awareness is particu-larly helpful for ﬁne-grained action recognition. We also
ﬁnd that encoding both the arrow of time and the abso-lute value of the temporal shift outperforms using just the arrow of time while encoding temporal information; (3) we set a new state-of-the-art for self-supervised learning on Something-Something [15], a dataset designed for ﬁne-grained action recognition, and ﬁnally (4) we also achieve state-of-the-art performance on standard benchmarks such as HMDB51 [31] and UCF101 [52]. 2.