Abstract
We study a crucial problem in video analysis: human-object relationship detection. The majority of previous ap-proaches are developed only for the static image scenario, without incorporating the temporal dynamics so vital to contextualizing human-object relationships. We propose a model with Intra- and Inter-Transformers, enabling joint spatial and temporal reasoning on multiple visual concepts of objects, relationships, and human poses. We find that applying attention mechanisms among features distributed spatio-temporally greatly improves our understanding of human-object relationships. Our method is validated on two datasets, Action Genome and CAD-120-EVAR, and achieves state-of-the-art performance on both of them. 1.

Introduction
As we develop intelligent agents to understand images more comprehensively, the computer vision research prob-lems we are solving have become more and more com-plex. The computer vision community has moved from classifying images and detecting objects, to detecting ob-ject relationships and understanding object interactions. In real-world applications, we often need to infer human be-haviors from videos. In human-centered applications such as human-robot interaction, senior care [36], and health-care [19], understanding the interactions people have with their environment is pivotal. One important problem at the heart of action recognition is detecting human-object rela-tionships in videos: given the frames of a video, we would like to detect which objects a person is interacting with and classify the relationships between the person and objects.
Rather than generating scene graphs on static images
[29, 53, 58, 45], human-object relationship (HOR) detection in videos focuses on the human and the active objects, that is the objects the person is actively interacting with. Unlike human-object interaction (HOI) detection [18, 7, 39, 33],
HOR detection classifies not only the verbs that describe hu-man actions, but also the prepositions between human and objects, such as “behind”, “beneath” and “in”. While many
Figure 1: We tackle the problem of detecting human-object re-lationships in videos. Most prior approaches only model human-object relationships in images and perform static image predic-tions. We propose extra spatio-temporal reasoning on the top of static image predictions by intra- and inter-transformers. verbs are only associated with certain objects, prepositions are often applicable to numerous object categories.
Compared to scene graph generation and HOI detection on images, HOR detection in videos faces several chal-lenges. First, the model needs to find out which objects are the protagonists of the scene. For instance, the clip in Fig-ure 1 contains plenty of background objects that are not of our interest. How can we accurately focus on only the active objects? Second, the object detector, a key component in the detection model, will be confused during model training as video datasets [24, 60] typically only provide annotations on active objects. Without the knowledge of human-object interactions, a simple object detector will mistakenly fire on both the sat-on chair and those stacked chairs not relevant to the action. Third, videos can often be blurred at some
frames, and static image models have difficulty with per-forming inference on blurred frames. Considering these key issues, how can we leverage information from neighboring frames to produce more accurate predictions?
To tackle these issues, we propose a Human-Object Re-lationship Transformer (HORT) model for HOR detection in videos. Our model has two-stages: i) static image pre-diction and ii) using visual concepts from the first stage to recognize active objects and their relationships. HORT first performs static image prediction, where one can plug in an existing model from various choices of scene graph gener-ation or HOI detection. In the second stage, HORT gathers the encoded visual concepts from the first stage (specifically the object, relationship, and human pose embeddings) and feeds them into transformers with intra- and inter-attention mechanisms. The attention mechanisms allow the model to integrate information from spatially and temporally scat-tered visual cues to find out which interactions are happen-ing.
In the transformer modules, we also pass messages from human pose and relationship features to object encod-ing, enabling an object scorer to focus on active objects.
To validate our HORT model, we have benchmarked its detection performance on two video datasets: Action
Genome [24] and CAD-120-EVAR [60]. Our model out-performs state-of-the-art methods in scene graph generation and HOI detection. We have also conducted ablation studies to examine the contribution of each part of our model. 2.