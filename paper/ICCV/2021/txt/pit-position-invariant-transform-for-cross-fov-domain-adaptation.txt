Abstract
Cross-domain object detection and semantic segmenta-tion have witnessed impressive progress recently. Exist-ing approaches mainly consider the domain shift result-ing from external environments including the changes of background, illumination or weather, while distinct cam-era intrinsic parameters appear commonly in different do-mains and their influence for domain adaptation has been
In this paper, we observe that the very rarely explored.
Field of View (FoV) gap induces noticeable instance ap-pearance differences between the source and target do-mains. We further discover that the FoV gap between two domains impairs domain adaptation performance un-der both the FoV-increasing (source FoV < target FoV) and FoV-decreasing cases. Motivated by the observa-tions, we propose the Position-Invariant Transform (PIT) to better align images in different domains. We also intro-duce a reverse PIT for mapping the transformed/aligned images back to the original image space, and design a loss re-weighting strategy to accelerate the training pro-cess. Our method can be easily plugged into existing cross-domain detection/segmentation frameworks, while bringing about negligible computational overhead. Extensive ex-periments demonstrate that our method can soundly boost the performance on both cross-domain object detection and segmentation for state-of-the-art techniques. Our code is available at https://github.com/sheepooo/
PIT-Position-Invariant-Transform. 1.

Introduction
Object detection [20, 41, 40] and semantic segmenta-tion [33, 5, 6, 14] are two fundamental problems in com-puter vision. The former aims at precisely locating and identifying the objects in an image and the latter targets to
Figure 1: Objects (cars) in different positions relative to the camera have different extent of deformation, which remark-ably burdens the alignment of intra-class features. This can be effectively mitigated by our PIT. Top row: images of an object (in different positions) captured by a virtual camera.
Other rows: real photos from the KITTI dataset. classify the semantics of each pixel. Training a general-ized model with high performance for the two tasks calls for massive images with elaborate annotations, while it is laborious to prepare such well-annotated data. Meanwhile, due to the existence of domain shift [1], a model trained on a specific dataset often suffers from significant performance degradation when applied to another domain. A common
*Equal Contribution. † Joint Corresponding author.
solution is to transfer the knowledge acquired from a la-beled source domain to an unlabeled target domain, which is known as Unsupervised Domain Adaptation (UDA) [38].
In general, two typical manners have been explored to adapt models from the source to the target domain. One is pixel-level alignment, target-like images are generated to provide implicit or explicit supervisory signals on target do-main [44, 25, 61]. The other is feature-level alignment, the feature distributions of two domains are aligned through constraining domain discrepancy metrics [34, 48, 62] or performing feature confusion [18, 52, 39].
In the study of cross-domain detection/segmentation, previous works [8, 43, 60, 2, 9, 59, 25, 50, 31, 65] mainly fo-cus on narrowing the domain shift caused by external envi-ronments, e.g. the change of background, illumination and weather, etc. However, very little attention has been paid to the camera’s intrinsic parameters which often bring notice-able domain discrepancy due to the use of various cameras.
We observe that one main camera parameter, the Field of View (FoV)1, induces a distinct dimension of the domain gap. As a matter of fact, the FoV discrepancy frequently occurs among datasets or in real-world scenarios. For in-stance, in autonomous driving, cameras with different FoVs are often used together, because of the inevitable updating of cameras in the long period of data collection. FoV differ-ence derives the variety of instance structural appearances across the source and target domains, leading to the sample diversifying within a category. This obviously increases the burden of domain adaptation models, thus resulting in less desired performance.
Motivated by the above observation, in this paper we at-tempt to alleviate the adverse impact of the diverse FoVs be-tween domains, in order to boost the performance of cross-domain detection/segmentation.
We discuss the influence of the FoV gap in two general cases. (1) In FoV-increasing adaptation (the FoV of the tar-get domain is larger than that of the source domain), the target domain instances with large incident angles cannot be well aligned to the source domain for the lack of similar-appearance counterparts. (2) In FoV-decreasing adaptation (target FoV smaller than source FoV), the sparsity of the source domain instances within a specific range of incident angle also hampers domain alignment. Existing UDA meth-ods usually try to bridge the whole domain gap and opti-mize the model without specifically taking account of the
FoV factor, thus preventing the model from fully learning domain-invariant features.
To address the above problem, we propose the Position-Invariant Transform (PIT) to straightforwardly narrow the
FoV gap between the source and target domains (Fig. 1). 1Field of View (FoV): in photography, the angle between two rays passing through the perspective center (rear nodal point) of a camera lens to the two opposite sides of the format.
Specifically, the pixels lying in the original imaging space are mapped to another two-dimensional space shaped as a spherical surface, such that the appearances of the instances in various positions are aligned to a great extent. Also, we introduce a reverse PIT for mapping the transformed images back to the original image space. In addition, we design an efficient loss re-weighting strategy to speed up the training procedure. Our modules induce little computa-tional overhead while boosting performance, and they can be easily served as plug-and-play modules to any existing cross-domain detection/segmentation frameworks.
Our contributions can be summarized as follows:
• We statistically analyze the negative influence of FoV difference between the source and target domains on
UDA models, in which both the increasing and de-creasing of FoV between domains impair the domain alignment.
• We propose the Position-Invariant Transform (PIT) to align instance structural appearances in different po-sitions in each category, and reverse PIT to map the transformed images to the original image space. We also introduce a loss re-weighting strategy to speed up the training procedure.
• The effectiveness of PIT is verified on both cross-domain detection and segmentation tasks. Equipped with our modules, state-of-the-art UDA methods show soundly better performance than before. 2.