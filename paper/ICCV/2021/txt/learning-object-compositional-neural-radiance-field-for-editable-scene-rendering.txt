Abstract
Implicit neural rendering techniques have shown promising results for novel view synthesis. However, ex-isting methods usually encode the entire scene as a whole, which is generally not aware of the object identity and lim-its the ability to the high-level editing tasks such as mov-ing or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object ac-tivation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing. 1.

Introduction
Virtual tour in a real-world scene is one of the most de-sired experiences for virtual and augmented reality. While early works rely on laborious capturing and reconstruction of the physical world, e.g., geometry, texture, material, etc.,
*Corresponding author
Code is available on the project webpage: https://zju3dv. github.io/object_nerf/ the emerging neural rendering methods open great opportu-nities to ease this task by learning directly from a collection of posed images and achieve promising realistic images. A common follow-up question to ask is: Can we modify the scene, e.g., moving or adding furniture, while still maintain-ing the realistic rendering capability.
Unfortunately, this is not well-supported by existing neu-ral rendering methods. Early approaches tend to encode the entire visible scene into a single neural network, such as
NeRF [17] and SRN [26]. While handling small objects perfectly, these models are hard to scale up for large-scale scenes due to the fixed network capacity. On the other hand, a family of neural rendering approaches utilizes volumet-ric representation [12] to densely encode local information at specific locations, which migrates the scalability burden from network parameters to the scene representation and empirically produces better rendering quality. However, the scene representation and rendering network are in general agnostic to the object identity, which does not support high-level editing tasks such as moving furniture.
In this paper, we propose a neural rendering system that enables scene editing on real-world scenes. Taking a col-lection of posed images captured from the real scene and rough 2D instance masks, our model can render the whole scene as it is in reality, as well as with objects manipulated, such as moving, rotating, or duplicating. Most related to us, OSF [7] enables editable scene rendering in a bottom-up fashion by learning one model per-object and then per-form joint rendering. However, their method does not learn the object arrangements in the real world and requires train-ing images captured for each individual object beforehand,
is non-trivial which is infeasible to obtain on cluttered scene images and thus only verified on synthetic data. In contrast, we aim to design a top-down approach that directly learns a unified neural rendering model for the whole scene which respects the object placement as in the captured scene. To support object manipulation, we design a novel conditional neural rendering architecture that is able to render each object stan-dalone with everything else removed, which can be further rendered from a novel viewpoint, at a new location, or repli-cated. Note that to ensure realistic scene editing, each ob-ject has to be rendered with sharp boundaries without back-ground bleeding, which is infeasible to achieve with only a rough 3D rendering mask or a bounding box (see Fig. 6 for an example). it
Indeed, to learn such an object-compositional neural radiance field for a clustered and real-world scene even with rough 2D instance masks, mainly due to the 3D space ambiguity in the occluded region. In-tuitively, the network could learn only from the rays casting within the instance mask of a particular object when aim-ing to render it. However, without known geometry, it is impossible to identify if a 3D location belongs to the object but occluded, which is common in a cluttered scene, or not even a part of it, since both cases are not marked by the in-stance mask. As a result, the network may overkill part of the object and produce cloudy results. In order to solve this challenge, we learn an extra compact scene branch, without editable capability, to provide biased sampling distribution along the ray and dense depth online during training, which helps to identify the occluded region where the no gradient is applied instead of being supervised as empty space. The scene branch also renders the contents that are not labeled by the instance segmentation to provide a seamless whole scene rendering.
In summary, the contributions of this paper are as fol-lows. Firstly, we propose the first editable neural scene rendering system given a collection of posed images and 2D instance masks, which supports high-quality novel view rendering as well as object manipulation. Secondly, we design a novel two-pathway architecture to learn object-compositional neural radiance field for a clustered and real-world scene resolving occlusion ambiguity. Lastly, the ex-periment and extensive ablation study demonstrate the ef-fectiveness of our system and the design of each compo-nent. Our system performs on-par or even better than the
SoTA methods in terms of standard novel-view synthesis while maintaining the capability of editable scene render-ing with high quality. 2.