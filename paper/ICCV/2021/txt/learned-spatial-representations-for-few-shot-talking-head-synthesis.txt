Abstract
We propose a novel approach for few-shot talking-head synthesis. While recent works in neural talking heads have produced promising results, they can still produce images that do not preserve the identity of the subject in source images. We posit this is a result of the entangled represen-tation of each subject in a single latent code that models 3D shape information, identity cues, colors, lighting and even background details. In contrast, we propose to factor-ize the representation of a subject into its spatial and style components. Our method generates a target frame in two steps. First, it predicts a discrete and dense spatial layout for the target image. Second, an image generator utilizes the predicted layout for spatial denormalization and synthesizes the target frame. We experimentally show that this disentan-gled representation leads to a signiﬁcant improvement over previous methods, both quantitatively and qualitatively. 1.

Introduction
We study the task of learning personalized head avatars in a low-shot setting, also known as “neural talking heads”.
Given a single-shot or few-shot images of a source subject, and a driving sequence of facial landmarks, possibly derived from a different subject, the goal is to synthesize a photo-realistic video of the source subject, under the poses and expressions of the driving sequence. This task has a wide range of applications, including those in AR/VR, video con-ferencing, gaming, animated movie production and video compression in tele-communication.
Traditional graphics-based approaches to this task rely on a 3D face geometry and produce very high quality synthe-sis. However, they tend to focus on modeling the face area without the hair, and they learn a subject-speciﬁc model and cannot generalize to new subjects. In contrast, recent 2D-based approaches [1, 2, 3, 4] learn a subject-agnostic model that can animate unseen subjects given as few as a single im-age. Furthermore, since these works learn an implicit model and do not require an explicit geometric representation, they can synthesize the full head, including the hair, mouth inte-Figure 1: Our framework factorizes the image synthesis process into its spatial and style components. It predicts a discrete latent spatial layout for the target image, which is used to produce per-pixel style modulation parameters for the ﬁnal synthesis. rior, and even wearable accessories like glasses and earrings.
This remarkable generalization ability however comes at the cost of low quality and poor identity preservation when compared to their 3D-based subject-speciﬁc counterparts.
Bridging the quality gap between 2D-based subject-agnostic and 3D-based subject-speciﬁc approaches remains an open problem.
Recent efforts in 2D-based approaches can be divided into two classes; warping-based and direct synthesis. As the name suggests, warping-based methods (e.g., [2]) learn to warp the input image or a recovered canonical pose based on the motion of the driving sequence. While these methods achieve high realism, especially for static and rigid parts of the image, they tend to work well only for a limited range of motion, head rotation and dis-occlusion. On the other hand, direct synthesis approaches (e.g., [1, 3, 4]) encode the source subject into a compressed latent code, and a generator decodes the latent code to synthesize the target pose. These approaches learn a prior over the compressed latent space, and can generate realistic results for a wider range of poses and head motion. However, they exhibit a noticeable identity gap between their output and the source subject.
We posit that the identity gap is caused by the entangled representation of the source subject in a single latent code.
This compressed 1D latent encodes multi-view shape infor-mation, identity cues, as well as color information, lighting and background details. In order to synthesize a target view from a latent code, the generator needs to devise a complex function to decode the uni-dimensional latent into its cor-responding 2D spatial information. We argue this not only consumes a large portion of the network capacity, but also
Figure 2: Overview of our training pipeline. The cross-entropy loss with the oracle segmentation is used during pre-training the layout predictor Gl, and then turned off during the full pipeline training. limits the amount of information that can be encoded in the latent code.
To address this problem, we propose a two-step frame-work that decomposes the synthesis of a talking head into its spatial and style components. Our framework animates a source subject in two steps. First, it predicts a novel spatial layout of the subject under the target pose and expression.
Then, it synthesizes the target frame conditioned on the predicted layout. This factorized representation yields the following key performance advantages.
Better subject-agnostic model performance. The perfor-mance of our subject-agnostic (also called meta-learned) model not only performs better than previous subject-agnostic state-of-the-art, but is also on-par with the subject-ﬁnetuned performance of previous works when there are only few source images available (e.g., less than 10 images).
Better ﬁne-tuned performance with less data. Fine-tuning our model for a speciﬁc subject requires signiﬁcantly less data and fewer iterations than previous works, and yet achieves better performance. For example, we show that ﬁne-tuning our model using 4-shot inputs outperforms previous state-of-the-art models ﬁne-tuned using 32-shot inputs.
Robustness to pose variations. We show that our model is more robust against a wider range of poses and facial expressions, while still producing both realistic and identity-preserving results.
Improved identity preservation. Shape difference be-tween the source and driving identities poses a challenge for identity preservation in reenacted results. The intermedi-ate novel spatial representation learned by our model reduces the sensitivity towards such differences and better preserves the identity.
In summary, we make the following contributions:
• A novel approach that factorizes the talking-head synthesis process into its spatial and style components.
• A novel latent spatial representation that proves effective for few-shot novel view synthesis.
• We achieve state-of-the-art performance in both the single-shot and multi-shot settings, as well as in the meta-learned and subject-ﬁnetuned modes. 2.