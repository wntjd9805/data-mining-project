Abstract
Vision models notoriously flicker when applied to videos: they correctly recognize objects in some frames, but fail on perceptually similar, nearby frames. In this work, we system-atically analyze the robustness of image classifiers to such temporal perturbations in videos. To do so, we construct two new datasets, ImageNet-Vid-Robust and YTBB-Robust, containing a total of 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB, respectively, and thor-oughly re-annotated by human experts for image similarity.
We evaluate a diverse array of classifiers pre-trained on Im-ageNet and show a median classification accuracy drop of 16 and 10 points, respectively, on our two datasets. Addi-tionally, we evaluate three detection models and show that natural perturbations induce both classification as well as lo-calization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and real-istic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions. 1.

Introduction
Applying state-of-the-art image recognition systems to videos reveals a troubling phenomenon: models correctly recognize objects in one frame, but fail to do so in the very next frame (Figure 1). In practice, this flickering of predic-tions is treated as an unfortunate but unavoidable property of image-based models. This issue can be mitigated in of-fline settings by smoothing predictions over time. However, online smoothing isn’t nearly as effective and incurs a delay, resulting in catastrophic mistakes in downstream applica-tions: e.g., flickering object classifications have reportedly led to fatal autonomous vehicle collisions [3].
At its root, prediction flicker is a manifestation of a broader issue: current models lack robustness to small input
∗Equal contribution
Figure 1: Examples of natural perturbations from nearby video frames and resulting classifier predictions from a
ResNet-152 model fine-tuned on ImageNet-Vid. While the images appear almost identical to the human eye, the classi-fier confidence changes substantially. perturbations. In the machine learning community, model ro-bustness has typically been analyzed on images perturbed by an adversary [11, 2], or by hand-designed strategies, such as rotations or blurs [7, 6, 14, 13]. However, these benchmarks rely on synthetically modifying the input image, serving at best as proxies for evaluating robustness to natural perturba-tions, which are common in videos.
In this work, we systematically analyze the prevalence of flicker across vision models. Taking inspiration from the
robustness literature, we evaluate models on perceptually similar images, which we sample from nearby video frames.
However, nearby frames can still exhibit drastic changes (e.g., significant occlusions), which may cause even robust models to fail. We discard such frame pairs by employing human expert labelers to evaluate model robustness only on perceptually similar images, unlike prior work [12]. As a cornerstone of our investigation, we introduce two test sets for evaluating model robustness: ImageNet-Vid-Robust and YTBB-Robust, carefully curated from the ImageNet-Vid and Youtube-BB datasets [27, 24]. To the best of our knowledge these are the first datasets of their kind, contain-ing tens of thousands of images that are human reviewed and grouped into thousands of perceptually similar sets. In total, our datasets contain 3,139 sets of temporally adjacent and visually similar images (57,897 images total).
We use these datasets to measure the robustness of current models to small, naturally occurring perturbations. Although we use videos to sample these images, our datasets allow evaluating the robustness of standard, image-based computer vision models, such as those trained on ImageNet. Our testbed contains over 47 different models, varying model types (CNNs, transformers), architectures (e.g., AlexNet,
ResNet) and training methods (e.g., adversarial training, aug-mentation). To systematically characterize flicker, we also introduce a stringent robustness metric.
Our experiments show that all models in our testbed de-grade significantly in the presence of small, natural perturba-tions in video frames. Under our metric, we find such pertur-bations in ImageNet-Vid-Robust and YTBB-Robust in-duce median accuracy drops of 16% and 10% respectively for classification, and a median 14 point AP drop for detec-tion1. Even for the best-performing classification models trained on public datasets, we observe an accuracy drop of 14% for ImageNet-Vid-Robust and 8% for YTBB-Robust.
Recently introduced, contrastive models trained on weakly supervised web images [23] can reduce this gap, but require over 400 million images, and still exhibit noticeable gaps of 6.1% and 6.7%, respectively.
Our results show that robustness to natural perturbations in videos is problematic for a wide variety of models. Practi-cal deployment of models, especially in safety-critical envi-ronments like autonomous driving, requires predictions that are not only accurate, but also robust over time. Our analysis indicates that ensuring reliable predictions on every frame of a video is an important direction for future work. 2.