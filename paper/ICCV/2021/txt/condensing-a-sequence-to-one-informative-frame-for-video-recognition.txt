Abstract
Video is complex due to large variations in motion and rich content in ﬁne-grained visual details. Abstracting use-ful information from such information-intensive media re-quires exhaustive computing resources. This paper stud-ies a two-step alternative that ﬁrst condenses the video se-quence to an informative “frame” and then exploits off-the-shelf image recognition system on the synthetic frame. A valid question is how to deﬁne “useful information” and then distill it from a video sequence down to one synthetic frame. This paper presents a novel Informative Frame Syn-thesis (IFS) architecture that incorporates three objective tasks, i.e., appearance reconstruction, video categorization, motion estimation, and two regularizers, i.e., adversarial learning, color consistency. Each task equips the synthetic frame with one ability, while each regularizer enhances its visual quality. With these, by jointly learning the frame syn-thesis in an end-to-end manner, the generated frame is ex-pected to encapsulate the required spatio-temporal infor-mation useful for video analysis. Extensive experiments are conducted on the large-scale Kinetics dataset. When com-paring to baseline methods that map video sequence to a single image, IFS shows superior performance. More re-markably, IFS consistently demonstrates evident improve-ments on image-based 2D networks and clip-based 3D networks, and achieves comparable performance with the state-of-the-art methods with less computational cost. 1.

Introduction
Recently, the development of Convolutional Neural Net-works (CNN) convincingly demonstrates high capability of
CNN in image-domain visual recognition. For instance, an ensemble of residual nets [9] achieves 3.5% top-5 error on the ImageNet test set, which is even lower than 5.1% of the reported human-level performance. Nevertheless, it is not trivial to apply a 2D CNN for video recognition. Since video is a temporal sequence with large variations and com-plexities, performing 2D CNN on individual frame cannot (a) Late fusion (b) Slow fusion (c) Early fusion by frame synthesis
Figure 1. Modeling temporal evolution for video recognition by (a) aggregating 2D representations from sampled frames by late fusion, (b) slow fusing the input frames by 3D CNN and (c) con-densing the input video to one frame as early fusion. model temporal evolution across frames.
Extensive progresses have been made to model the tem-poral sequence for video recognition. These works can be grouped into three categories according to which stage temporal information is aggregated, as shown in Figure 1.
The ﬁrst one is late fusion [12, 20, 27, 38, 44, 45], which
ﬁrst extracts the image representation from 2D CNN for each frame and then aggregates the feature sequence for video recognition. Despite being straightforward by em-ploying 2D CNN on video data, pixel-level temporal evolu-tion over frames is overlooked. The second is slow fusion
[2, 6, 10, 12, 21, 22, 23, 24, 31, 33], which feeds entire video clip into a network (e.g., 3D CNN) for spatio-temporal con-volution. This type of networks builds temporal connec-tions among pixels across space and time at the expense of computational cost. For example, the de facto ResNet-101
[9] requires 10G ﬂoating-number operations (FLOPs) for single crop on image data. When transferring this backbone to 3D CNN for 128-frame clip, the number of FLOPs is increased to 234G for SlowFast networks [6].
This paper addresses a new direction that early fuses the information from video sequence to one synthetic frame.
Despite being a 2D image, the frame captures motion dy-namics and visual details of a sequence. In this way, 2D
CNN can be employed to learn both visual appearance as well as temporal evolution from just one frame. To this end, the crucial issue of early fusion becomes what informa-tion deserves to be preserved in a synthetic frame. There-fore, we propose a novel Informative Frame Synthesis (IFS) architecture guiding the generation of synthetic frames by multiple pre-deﬁned tasks and regularizers. This architec-ture mainly consists of three components, i.e., a convolu-tional encoder-decoder network to transfer the input video sequence to a synthetic frame, three objective tasks to en-sure that the frame is reconstructable, semantically and dy-namically consistent with the original sequence, and two regularizers to preserve the ﬁne-grained visual details. By jointly optimizing the transformation network in an end-to-end manner, the generated frame will attempt to encapsulate the required information of each task. In this way, the frame captures temporal evolution for video applications.
The main contribution of this work can be summarized as followings. First, IFS network is novelly proposed to learn the transformation from 3D video clips to 2D image frame. Second, different objectives and regularizers are pro-posed to encapsulate motion dynamics and visual details in a 2D frame. Extensive experiments are conducted on Ki-netics dataset. Ablation studies investigate the impact of each task and regularizer towards frame synthesis. The re-sults demonstrate that IFS with 2D CNN (e.g., ResNet-101) achieves comparable performance to more computationally expensive 3D CNN (e.g., I3D [2]). When the synthetic frames are stacked as a video summary for 3D CNN clas-siﬁcation, higher performance is attained than most of the existing works with less computation. 2.