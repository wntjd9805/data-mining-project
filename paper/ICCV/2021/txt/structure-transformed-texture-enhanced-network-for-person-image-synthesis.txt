Abstract
Pose-guided virtual try-on task aims to modify the fash-ion item based on pose transfer task. These two tasks that belong to person image synthesis have strong corre-lations and similarities. However, existing methods treat them as two individual tasks and do not explore correla-tions between them. Moreover, these two tasks are chal-lenging due to large misalignment and occlusions, thus most of these methods are prone to generate unclear hu-man body structure and blurry ﬁne-grained textures. In this paper, we devise a structure-transformed texture-enhanced network to generate high-quality person images and con-It consists of struct the relationships between two tasks. two modules: structure-transformed renderer and texture-enhanced stylizer. The structure-transformed renderer is introduced to transform the source person structure to the target one, while the texture-enhanced stylizer is served to enhance detailed textures and controllably inject the fash-ion style founded on the structural transformation. With the two modules, our model can generate photorealistic per-son images in diverse poses and even with various fash-ion styles. Extensive experiments demonstrate that our ap-proach achieves state-of-the-art results on two tasks. 1.

Introduction
Person image synthesis has drawn a great deal of at-tention, due to various applications in the movie indus-try, e-commerce, person re-identiﬁcation, etc. There are two critical tasks in person image synthesis: pose trans-fer [17, 22, 15, 40, 20, 23] and pose-guided virtual try-on
[39, 5, 11, 27]. As shown in Figure 1, pose transfer task aims to transfer person images from one pose to other poses, and pose-guided virtual try-on task is to modify the cloth-ing item based on pose transference. These two tasks have strong correlations and similarities, yet existing methods do not explore their correlations. Especially most methods of pose-guided virtual try-on [39, 11, 27] only implicitly model the pose transformation via learning the concatena-tion of the target pose or human parsing map and the source
Figure 1: The correlations between two tasks. Pose transfer corresponds to a person’s structural transformation between the source and the target images, while pose-guided virtual try-on aims to modify the fashion styles based on this trans-formation. image. This implicit transformation is likely to result in blurry and implausible issues when dealing with large mis-alignment among different poses, which affects the quality of generated results.
The key to obtaining high-quality results for these two tasks is to generate sharp human body structure and ﬁne-grained textures (e.g., patterns of clothes and hairs). For the structure generation, the target human body structure can be obtained via a transformation from the source one, such as from the back to the front structure in the Figure 1. Trans-formation simulation is the primary concern of pose transfer task. Previous methods exploit ﬂow-based warping, obtain-ing this transformation with promising results [8, 15, 20].
However, due to the challenge of accurate ﬂow computa-tion and precise warping operation, it may cause artifacts around the structure [28, 24]. In addition to structure gener-ation, most existing methods adopt perceptual-related con-straints (e.g., perceptual loss [14] and adversarial loss [7]) to guide the texture generation and ameliorate the visual qual-ity of the results. However, these losses tend to optimize high-level perception, which does not only focus on textures but also includes other components such as style informa-tion. Therefore, the texture generation cannot be supervised effectively in the optimization process. For example, ﬁne-grained region reconstruction can be further improved [30].
To address the above issues, in this paper, we propose a structure-transformed texture-enhanced network for person image synthesis. The proposed model comprises two key components: structure-transformed renderer and texture-enhanced stylizer. First, structure-transformed renderer aims at deforming the structure, explicitly solving the con-cern of pose alignments for pose transfer and pose-guided virtual try-on tasks. Here, we present a cross-modality deformable convolution to deal with this transformation, which avoids artifacts caused by the ﬂow-based warping and fuses multi-modality information to better capture the structural motions. Then, texture-enhanced stylizer is de-signed to further enhance texture details due to the failure of retaining ﬁne-grained regions. Speciﬁcally, the pose-guided high-frequency attention is introduced to enhance the high-frequency components with spatial contextual information.
We establish a multi-modality long-range dependency to deal well with the invisible regions in the source image.
Besides, this component enables users to manipulate fash-ionable garments controllably, modifying fashion styles for pose-guided virtual try-on task. Beneﬁt from the two novel components, our model combines pose transfer task with pose-guided virtual try-on task, which can model the trans-formation between the source and the target poses explicitly and generate high-quality person images in diverse poses and even with various fashion styles.
The proposed approach exhibits superior performance over existing methods on the DeepFashion [16] and the
FashionTryOn dataset [39]. Furthermore, we perform abla-tion experiments to validate the contribution of key compo-nents in our approach. The main contributions of our paper are summarized as:
• We propose a structure-transformed texture-enhanced network to handle pose-guided virtual try-on task with pose transfer task jointly. Experimental results show the superiority of our model on generating highly pho-torealistic and fashion-diversiﬁed results for person image synthesis.
• We design the structure-transformed renderer based on cross-modality deformable convolutions to process the person’s structural transformation, which fuses multi-modality information to capture the structural motions.
• The texture-enhanced stylizer is proposed to enhance detailed textures and enable users to manipulate the fashion style, avoiding blurry textures and generating various fashion styles. 2.