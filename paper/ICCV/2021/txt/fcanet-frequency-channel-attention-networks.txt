Abstract
Attention mechanism, especially channel attention, has gained great success in the computer vision field. Many works focus on how to design efficient channel attention mechanisms while ignoring a fundamental problem, i.e., channel attention mechanism uses scalar to represent chan-nel, which is difficult due to massive information loss. In this work, we start from a different view and regard the channel representation problem as a compression process using frequency analysis. Based on the frequency analy-sis, we mathematically prove that the conventional global average pooling is a special case of the feature decomposi-tion in the frequency domain. With the proof, we naturally generalize the compression of the channel attention mecha-nism in the frequency domain and propose our method with multi-spectral channel attention, termed as FcaNet. FcaNet is simple but effective. We can change a few lines of code in the calculation to implement our method within existing channel attention methods. Moreover, the proposed method achieves state-of-the-art results compared with other chan-nel attention methods on image classification, object detec-tion, and instance segmentation tasks. Our method could consistently outperform the baseline SENet, with the same number of parameters and the same computational cost.
Our code and models are publicly available at https:
//github.com/cfzd/FcaNet. 1.

Introduction
As an important and challenging problem in feature modeling, attention mechanisms for convolutional neural networks (CNNs) have recently attracted considerable at-tention and are widely used in many fields like computer vi-sion [40] and natural language processing [34]. In principle, they aim at selectively concentrating on some important in-formation and have many types of variants (e.g., spatial at-tention, channel attention, and self-attention) corresponding
*Corresponding author.
Figure 1. Classification accuracy comparison on ImageNet. With the same number of parameters and computational cost, our method consistently outperforms the baseline SENet. to different feature dimensions. Due to the simplicity and effectiveness in feature modeling, channel attention directly learns to attach importance weights with different channels, becoming a popular and powerful tool for the deep learning community.
Typically, a core step of channel attention approaches is to use a scalar for each channel to conduct the calcu-lation due to the constrained computational overhead, and global average pooling (GAP) becomes the de-facto stan-dard choice in the deep learning community because of its simplicity and efficiency. Nevertheless, every rose has its thorn. The simplicity of GAP makes it hard to well capture complex information for various inputs. Some methods like
CBAM [39] and SRM [23] further use global max pooling and global standard deviation pooling to enhance the per-formance of GAP. Different from previous works, we con-sider the scalar representation of a channel as a compression problem. Namely, the information of a channel should be compactly encoded by a scalar while preserving the repre-sentation ability of the whole channel as much as possible.
In this way, how to effectively compress a channel with a
scalar due to the constrained computational overhead is a major difficulty, and it is crucial to channel attention.
With the above motivation, we propose to use the dis-crete cosine transform (DCT) to compress channels in the channel attention mechanism for the following reasons: 1)
DCT is a widely used data compression method in sig-nal processing, especially with digital images and videos.
Many widely used image and video formats like JPEG,
HEIF, MPEG, and H.26x use DCT to realize data compres-sion. DCT has a strong energy compaction property [1, 30], so it could achieve high data compression ratio with high quality [2, 22]. This property meets the demand of the chan-nel attention that representing a channel with a scalar. 2)
DCT can be implemented with an element-wise multiplica-tion, and it is differentiable. In this way, it can be easily integrated into CNNs. 3) Surprisingly, DCT can be viewed as a generalization of GAP. Mathematically, GAP (showing the effectiveness in SENet [21]) is only equivalent to the lowest frequency components of DCT, leaving many other potentially useful frequency components unexplored. This strongly motivates us to tailor DCT for the channel attention mechanism.
In this paper, we further propose a simple, novel, but effective multi-spectral channel attention (MSCA) frame-work based on the above discussions.
In order to better compress channels and explore the components left out by
GAP, we propose to tailor DCT and use multiple but lim-ited frequency components of DCT for the channel atten-tion mechanism. Note that although we use multi-spectral channel attention, each channel is still represented by only one scalar. Along with the MSCA framework, how to se-lect the frequency component of DCT for each channel is important.
In this way, we propose three kinds of fre-quency component selection criteria to fulfill and validate the MSCA framework, which are LF (Low Frequency based selection), TS (Two-Step selection), and NAS (Neural Ar-chitecture Search selection). With these selection criteria, our method achieves state-of-the-art performance against the other channel attention ones.
In a word, the main contribution of this work can be sum-marized as follows.
• We regard the channel attention as a compression prob-lem and introduce DCT in the channel attention. We then prove that conventional GAP is a special case of
DCT. Based on this proof, we generalize the chan-nel attention in the frequency domain and propose our method with the multi-spectral channel attention framework, termed as FcaNet.
• We propose three kinds of frequency component se-lection criteria along with the proposed multi-spectral channel attention framework to fulfill FcaNet.
• Extensive experiments demonstrate the proposed method achieves state-of-the-art results on both Ima-geNet and COCO datasets, with the same computa-tional cost as SENet. The results on ImageNet are shown in Fig. 1. 2.