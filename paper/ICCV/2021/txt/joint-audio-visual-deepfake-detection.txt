Abstract
Deepfakes (”deep learning” + ”fake”) are videos syn-thetically generated with AI algorithms. While they could be entertaining, they could also be misused for falsifying speeches and spreading misinformation. The process to cre-ate deepfakes involves both visual and auditory manipula-tions. Exploration on detecting visual deepfakes has pro-duced a number of detection methods as well as datasets, while audio deepfakes (e.g. synthetic speech from text-to-speech or voice conversion systems) and the relationship between the video and audio modalities have been relatively neglected. In this work, we propose a novel visual / audi-tory deepfake joint detection task and show that exploiting the intrinsic synchronization between the visual and audi-tory modalities could benefit deepfake detection. Experi-ments demonstrate that the proposed joint detection frame-work outperforms independently trained models, and at the same time, yields superior generalization capability on un-seen types of deepfakes. 1.

Introduction
A convincing deepfake intentionally designed for deliv-ering spurious information and fake news, e.g. a politician giving a speech or making a statement1, usually requires meticulous manipulations of both the video and audio chan-nels. In the given example, the video content has been mod-ified with a technique known as lip sync [44, 43] while the voice was from an impersonator. With recent advances in text-to-speech (TTS) and voice conversion (VC) algorithms
[58, 41, 22, 37, 10], synthesizing human speech will be-come even easier, paving a future where audio will play an equally important role as video in deepfake detection. Our work in this paper addresses the interplay between these two modalities, which can be critical towards detecting audiovi-sual deepfakes.
Recent work has mostly focus on identifying visual arti-facts and ‘fingerprints’ from various generative frameworks
[39, 56, 61, 11] or detecting local texture inconsistency 1youtu.be/30NvDC1zcL8
Figure 1: Examples to show that modified video or audio might violate the synchronization patterns. (a) The first row of video frames are unmodified while the second row has been faceswapped, and the words below are spoken by both videos. (b) Same as (a) just the bottom row has been lip-synced instead. Large discrepancies exist between the lip motions in the forged videos and the pronounced words. (c)
The top row are authentic video frames, uttering the word
“Moment”. The corresponding Mel-spectrograms are in the second row, a TTS generated “Moment” Mel-spectrograms are in the third row which sounded more like “wow-mount”.
The audiovisual pair comprising the first and third row breaks the synchronization patterns maintained by the pair of the first and second row, which is what we hope to cap-ture in this work. caused by face swapping [30, 31]. Another branch of work utilizes biometric signals such as detecting specific facial motion patterns inherent in particular individuals [5, 3], but such ID-specific approach is limited by its ability to gen-eralize to new identities. To achieve a more generalized approach, we observe that when humans speak, there is a strong correlation between the lip motions (viseme) and the pronounced syllables (phoneme) [32]. The synchronization breaks at some inconspicuous moments when any one of the modalities is fake, so for instance, in Fig. 1, the lip mo-tions did not fit well with the syllables due to the artifacts introduced by face swapping or lip sync. Moreover, when the phonemes are created from TTS systems, they are often times not pronounced clearly to match mouth shapes, which is a good signal for detecting audiovisual deepfakes.
Based on this intuition, we present a two-plus-one-stream model to jointly discriminate video / audio deep-fakes. Existing multi-modal frameworks take paired inputs from different modalities (e.g. video frames and optical flow; or video and sound for Action Recognition task), and use a shared label on the fused representation, which can be based on a late fusion of two streams [42, 18, 25] or lateral connections [17]. For deepfakes, the labels for the audio and video streams may not necessarily be the same because it could be that either one of the modalities is mod-ified. Learning a shared latent representation like this could thus be sub-optimal.
For this reason, we propose to model the video and audio stream separately with their own labels, as well as tempo-rally aligning the coarse-to-fine representations from both streams. We refer to this as sync-stream, which itself is given a separate label that reflects whether any one of the modalities has been manipulated. By jointly training as we proposed, the network not only learns ‘appearance’ or texture artifacts but also benefits from the sync-steam that discriminates synchronization patterns of authentic audio-visual pairs from that of fake pairs. A limiting factor, how-ever, is the lack of a proper dataset with both visual and au-ditory manipulations. To overcome this, we utilize existing video deepfake datasets containing unmodified audio chan-nels, from which we extract the Mel-spectrograms [51].
By running these spectrograms through different vocoders
[38, 52, 23, 33, 19, 28, 60] that are commonly used in TTS and VC tasks to mimic synthesized speech, we eventually curated a dataset similar in size to existing video deepfake datasets, but with manipulated audio channels.
Our contributions can be summarized as follows: 1. We present a joint audiovisual deepfake detection task that handles the case that either one (or both) of the visual or auditory modalities have been manipulated. 2. Further, we propose a sync-stream that models the syn-chronization patterns of two modalities. We show that with this additional signal, our model generalizes well as a result to unseen deepfakes. 3. Finally, we have built a deepfake dataset that contains both visual and auditory manipulations, with which we hope to encourage further research in the area of joint audiovisual deepfake detection. 2.