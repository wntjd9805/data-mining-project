Abstract
Image translation methods typically aim to manipulate a set of labeled attributes (given as supervision at train-ing time e.g. domain label) while leaving the unlabeled at-tributes intact. Current methods achieve either: (i) disen-tanglement, which exhibits low visual ﬁdelity and can only be satisﬁed where the attributes are perfectly uncorrelated. (ii) visually-plausible translations, which are clearly not disentangled. In this work, we propose OverLORD, a sin-gle framework for disentangling labeled and unlabeled at-tributes as well as synthesizing high-ﬁdelity images, which is composed of two stages; (i) Disentanglement: Learning disentangled representations with latent optimization. Dif-ferently from previous approaches, we do not rely on adver-sarial training or any architectural biases. (ii) Synthesis:
Training feed-forward encoders for inferring the learned attributes and tuning the generator in an adversarial man-ner to increase the perceptual quality. When the labeled and unlabeled attributes are correlated, we model an ad-ditional representation that accounts for the correlated at-tributes and improves disentanglement. We highlight that our ﬂexible framework covers multiple settings as disen-tangling labeled attributes, pose and appearance, localized concepts, and shape and texture. We present signiﬁcantly better disentanglement with higher translation quality and greater output diversity than state-of-the-art methods. 1.

Introduction
Learning disentangled representations for different fac-tors of variation in a set of observations is a fundamen-tal problem in machine learning. Such representations can facilitate generalization to downstream generative and dis-criminative tasks such as novel image synthesis [39] and person re-identiﬁcation [9], as well as improving inter-pretability [14], reasoning [34] and fairness [6]. A popular task which beneﬁts from disentanglement is image trans-lation, in which the goal is to translate a given input im-age from a source domain (e.g. cats) to an analogous im-age in a target domain (e.g. dogs). Although this task is generally poorly speciﬁed, it is often satisﬁed under the as-sumption that images in different domains share common attributes (e.g. head pose) which can remain unchanged during translation.
In this paper, we divide the set of all attributes that deﬁne a target image precisely into two sub-sets; (i) labeled attributes: the attributes that are supervised at training time e.g. whether the image belongs to the “cats” or “dogs” domain; (ii) unlabeled attributes: all the remain-ing attributes that we do not have supervision for e.g. breed of the animal, head pose, background etc. While several methods (e.g. LORD [10], StarGAN [4], Fader Networks
[20]) have been proposed for disentangling labeled and un-labeled attributes, we explain and show why they can not deal with cases where the labeled and unlabeled attributes are correlated. For example, when translating cats to dogs, the unlabeled breed (which speciﬁes fur texture or facial shape) is highly correlated with the domain label (cat or dog), and can not be translated between species. This de-pendency motivates the speciﬁcation of more ﬁne-grained attributes that we wish the translated image to have. Several methods as MUNIT [16], FUNIT [23] and StarGAN-v2 [5] attempt to learn a domain-dependent style representation which ideally should account for the correlated attributes.
Unfortunately, we show that despite their visually pleas-ing results, the translated images still retain many domain-speciﬁc attributes of the source image. As demonstrated in Fig. 2, when translating dogs to wild animals, current methods transfer facial shapes which are unique to dogs and should not be transferred precisely to wild animals, while our model transfers the semantic head pose more reliably.
In this work, we analyze the different settings for dis-entanglement of labeled and unlabeled attributes.
In the case where they are perfectly uncorrelated, we improve over
LORD [10] and scale to higher perceptual quality by a novel synthesis stage. In cases where the labeled and unlabeled attributes are correlated, we rely on the existence of spatial transformations which can retain the correlated attributes
Younger
Original
Older
Pose
Appearance
Translation (a) Labeled Attribute (Age) (b) Pose and Appearance
Identity
Gender+Hair
Translation
Shape
Texture
Translation (c) Labeled Attribute (Gender) With Localized Correlation (Hair) (d) Shape and Texture
Figure 1: A summary of the different attribute forms covered by our disentanglement framework. while exhibiting different or no uncorrelated attributes. We suggest simple forms of transformations for learning pose-independent or localized correlated attributes, by which we achieve better disentanglement both quantitatively and qualitatively than state-of-the-art methods (e.g. FUNIT [23] and StarGAN-v2 [5]). Our approach suggests that adversar-ial optimization, which is typically used for domain transla-tion, is not necessary for disentanglement, and its main util-ity lies in generating perceptually pleasing images. Fig. 1 summarizes the settings covered by our framework.
Our contributions are as follows: (i) Introducing a non-adversarial disentanglement method that carefully extends to cases where the attributes are correlated. (ii) Scaling disentanglement methods to high perceptual quality with a ﬁnal synthesis stage while learning disentangled repre-sentations. (iii) State-of-the-art results in multiple image-translation settings within a uniﬁed framework. 2.