Abstract
This work proposes a weakly-supervised temporal action localization framework, called D2-Net, which strives to temporally localize actions using video-level supervision.
Our main contribution is the introduction of a novel loss formulation, which jointly enhances the discriminability of latent embeddings and robustness of the output temporal class activations with respect to foreground-background noise caused by weak supervision. The proposed formu-lation comprises a discriminative and a denoising loss term for enhancing temporal action localization. The discrimi-native term incorporates a classiﬁcation loss and utilizes a top-down attention mechanism to enhance the separa-bility of latent foreground-background embeddings. The denoising loss term explicitly addresses the foreground-background noise in class activations by simultaneously maximizing intra-video and inter-video mutual information using a bottom-up attention mechanism. As a result, acti-vations in the foreground regions are emphasized whereas those in the background regions are suppressed, thereby leading to more robust predictions. Comprehensive exper-iments are performed on multiple benchmarks, including
THUMOS14 and ActivityNet1.2. Our D2-Net performs favorably in comparison to the existing methods on all datasets, achieving gains as high as 2.3% in terms of mAP at IoU=0.5 on THUMOS14. Source code is available at https://github.com/naraysa/D2-Net. 1.

Introduction
Temporal action localization is a challenging problem, which aims to jointly classify and localize the tempo-ral boundaries of actions in videos. Most existing ap-proaches [37, 5, 36, 30, 43, 32] are based on strong super-vision, requiring manually annotated temporal boundaries of actions during training. In contrast to these strong frame-level supervision based methods, weakly-supervised action localization learns to localize actions in videos, leveraging only video-level supervision. Weakly-supervised action lo-calization is therefore of greater importance since the manual annotation of temporal boundaries in videos is laborious, ex-pensive and prone to large variations [28, 27].
Existing methods [33, 34, 23, 25, 31] for weakly-supervised action localization typically use video-level anno-tations in the form of action classes and learn a sequence of class-speciﬁc scores, called temporal class activation maps (TCAMs). In general, a classiﬁcation loss is used to obtain the discriminative foreground regions in TCAMs. Some ap-proaches [23, 25, 22, 24] learn TCAMs using action labels and obtain temporal boundaries via a post-processing step, while others [31, 15] use a TCAM-generating video classi-ﬁcation branch along with an explicit localization branch to directly regress action boundaries. Nevertheless, the lo-calization performance is heavily dependent on the qual-ity of the TCAMs. The quality of TCAMs is likely to im-prove in fully-supervised settings where frame-level anno-tations are available. Such frame-level information (true foreground and background regions) are unavailable in the weakly-supervised paradigm. In such a paradigm, the pre-dicted foreground regions often overlap with the ground-truth background regions, while predicted background re-gions are likely to overlap with the ground-truth foreground regions. This leads to noisy activations, i.e., false positives and false negatives, in the learned TCAMs. Most existing weakly-supervised action localization methods that learn
TCAMs typically rely on separating foreground and back-ground regions (foreground-background separation) and do not explicitly handle its noisy outputs.
In this work, we address the problem of foreground-background separation along with explicit tackling of noise in TCAMs for weakly-supervised action localization. We propose a uniﬁed loss formulation that is jointly optimized to classify and temporally localize action snippets (group of frames) in videos. Our loss formulation comprises a dis-criminative and a denoising loss term. The discriminative loss seeks to maximally separate backgrounds from actions
addresses noise in TCAMs by maximizing the MI be-tween activations and labels within a video (intra-video) and across videos (inter-video). To the best of our knowledge, we are the ﬁrst to introduce a loss term that simultaneously captures MI across multiple snip-pets within a video and across all videos in a batch for weakly-supervised action localization.
• Experiments are performed on multiple benchmarks, including THUMOS14 [6] and ActivityNet1.2 [3]. Our
D2-Net performs favorably against existing weakly-supervised methods on all datasets, achieving gains as high as 2.3% mAP at IoU=0.5 on THUMOS14. 2.