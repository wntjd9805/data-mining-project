Abstract
In this paper we contribute a simple yet effective ap-proach for estimating 3D poses of multiple people from multi-view images. Our proposed coarse-to-fine pipeline first aggregates noisy 2D observations from multiple cam-era views into 3D space and then associates them into indi-vidual instances based on a confidence-aware majority vot-ing technique. The final pose estimates are attained from a novel optimization scheme which links high-confidence multi-view 2D observations and 3D joint candidates. More-over, a statistical parametric body model such as SMPL is leveraged as a regularizing prior for these 3D joint candi-dates. Specifically, both 3D poses and SMPL parameters are optimized jointly in an alternating fashion. Here the parametric models help in correcting implausible 3D pose estimates and filling in missing joint detections while up-dated 3D poses in turn guide obtaining better SMPL esti-mations. By linking 2D and 3D observations, our method is both accurate and generalizes to different data sources because it better decouples the final 3D pose from the inter-person constellation and is more robust to noisy 2D de-tections. We systematically evaluate our method on pub-lic datasets and achieve state-of-the-art performance. The code and video will be available on the project page: https://ait.ethz.ch/projects/2021/multi-human-pose/. 1.

Introduction
Markerless human motion capture is one of the fun-In recent years damental problems in computer vision. much progress has been made in estimating the configu-ration of the human body in 2D [5, 16, 34, 37, 50] and 3D [4, 25, 31, 32, 52] from a single RGB image as input.
However, if we consider settings in which multiple people are depicted and in particular if these people are interacting with each other at close range, we can expect a multitude of difficulties due to the heavy and complicated occlusions and depth ambiguities. To robustly estimate the poses of such groups, multi-camera setups are indispensable to pro-vide additional observations from different views which can resolve occlusion and provide stereo cues for 3D estimation.
Figure 1. Shape-aware multi-person pose estimation: We pro-pose a novel pipeline for robust recovery of 3D poses and shapes of multiple people from a few camera views. A formulation that links 2D and 3D observations and that is regularized via a para-metric body model is robust to noisy and missing 2D detections.
Articulated poses can even be recovered under heavy occlusion.
Due to the real-world importance of this problem, sev-eral recent approaches have attempted to predict the poses of multiple people, observed from multiple cameras [6, 10, 18, 45, 46, 54]. Such methods can loosely be categorized into two groups. The first group formulates the problem as a cross-view matching and association problem [6, 10, 54].
For example, Zhang et al. [54] introduce an optimization formulation that attempts to jointly solve the per-view pars-ing and cross-view matching problems as an instance of the multicut problem. The formulation is based on an as-sociation graph that links joints within and across multiple views. While the formulation is elegant, in practice it re-quires traversing the dense, cyclic association graph which results in an NP-hard problem. To attain a computationally tractable approach, the authors revert to a greedy heuristic which is sensitive to noisy 2D joint detections and imperfect visual features which limits the accuracy of the method.
Other methods, such as Tu et al. [46] combine the fea-tures from individual camera views into a 3D voxel space.
This volume is then segmented into sub-volumes by a learned person detector. The final 3D human pose configu-rations are regressed from these sub-volumes. Because the
pipeline can be trained end-to-end, high accuracy can be achieved if the training and test distributions are similar.
However, owing to the reliance on the volumetric feature representation – which encapsulates the joint distribution of different individuals, their location in 3D space, the camera setup and even the 2D joint detections – learning such a rep-resentation requires a vast amount of data. In the absence of a large corpus of annotated multi-people, multi-view data, such methods face generalization issues and are sensitive to distribution shifts (Tab. 4).
Embracing this challenging problem, we propose a sim-ple yet effective coarse-to-fine pipeline to estimate 3D multi-person poses from multi-view images. Our method combines concepts from both bottom-up and top-down methods. To avoid having to solve the association problem with partial local evidence, we aggregate initial 3D pose proposals in a 3D feature space. Our first insight is that, in pose estimation, the uncertainty associated with the 2D fea-tures (i.e., joint detections) can be trusted more than in many other computer vision domains due to semantics. Thus we forgo neural-network based classifiers and propose a simple confidence-aware majority voting technique to obtain initial 3D proposals. We experimentally show that it is more ro-bust to distribution differences in terms of human poses, lo-cation of individuals and cameras in space and thus leads to better generalization behavior. This coarse 3D localization step is followed by a refinement step to correct poses and fill in missing joints via an optimization scheme that lever-ages multi-view constraints directly, where high confidence 2D observations are available, and regularizes the 3D pose via a parametric body model.
More precisely, the first part of our pipeline consists of triangulating the 3D coordinates of all pairs of 2D de-tections with the same part label. This is followed by a confidence-aware majority voting technique to cluster the proposals. The technique is based on the insight that if a joint has been seen and predicted accurately (i.e., with high confidence) in several views, then there will be a dense clus-ter of 3D candidates for that joint and low confidence, iso-lated candidates can be discarded. Furthermore, we lever-age the observation that certain joints, for example, the hip, are detected more reliably than end-effectors and can be used as a heuristic to decide the number and location of in-dividual humans. While simple, experiments (Tab. 1) show that our approach outperforms the SOTA learning-based method [46] and matching-based method [54] in terms of the detection performance.
The second part of our pipeline refines the initial 3D estimates based on a novel 2D-3D objective (Eq.(5)).
In our formulation, we optimize the 3D joint locations directly by minimizing the 2D re-projection error if the correspond-ing 2D joint detections are of high confidence (Eq.(1)). To regularize the fitting procedure and to attain complete and kinematically plausible poses we leverage SMPL for low-confidence 3D candidates (Eq.(3)). Importantly, the SMPL parameters are aligned directly to the updated 3D obser-vations (current state of the 3D joint locations). For this we use the learned per-parameter gradient method (Eq.(6)).
This approach is fundamentally different from most exist-ing approaches [4] that fit SMPL parameters directly to 2D observations. We experimentally show that the trian-gulated 3D joints are more accurate than the PCA-based
SMPL skeleton – if they stem from confident 2D observa-tions (Fig. 4). Both initial 3D pose proposal and SMPL pa-rameters are optimized in an alternating manner (Alg. 1).
This is motivated by the insight that a good estimate of 3D poses helps in fitting SMPL, while better SMPL esti-mates make 3D poses more robust. Finally, detailed exper-iments are performed to demonstrate that both components improve the robustness and accuracy of the pose estimation task. In summary, our main contributions are:
• A coarse-to-fine confidence-aware pipeline to aggre-gate noisy 2D observations from all camera views into 3D space and associate them into individual instances.
• A novel refinement pipeline which optimizes 3D poses and their corresponding SMPL models in an alternat-ing fashion. The parametric models help in regulariz-ing low-confidence 3D poses while updated 3D poses in turn guide the SMPL parameter estimation.
• Our method is general since we only leverage off-the-shelf 2D pose detector and body pose priors distilled from motion capture datasets. SOTA performance is achieved on public datasets. 2.