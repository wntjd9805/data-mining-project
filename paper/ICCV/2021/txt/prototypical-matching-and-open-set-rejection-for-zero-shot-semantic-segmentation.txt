Abstract
The DCNN methods in addressing semantic segmenta-tion demand vast amount of pixel-wise annotated training samples. In this work, we present zero-shot semantic seg-mentation, which aims to identify not only the seen classes contained in training but also the novel classes that have never been seen. We adopt a stringent inductive setting in which only the instances of seen classes are accessible during training. We propose an open-aware prototypical matching approach to accomplish the segmentation. The prototypical way extracts the visual representations by a set of prototypes, making it convenient and flexible to add new unseen classes. A prototype projection is trained to map the semantic representations towards prototypes based on seen instances, and will generate prototypes for unseen classes. Moreover, an open-set rejection is utilized to de-tect objects that do not belong to any seen classes, which greatly reduces the misclassification of unseen objects into seen classes due to the lack of seen training instances. We apply the framework on two segmentation datasets, Pascal
VOC 2012 and Pascal Context, and achieve impressively state-of-the-art performance. 1.

Introduction
In semantic segmentation [8, 13, 14, 12] which aims to classify each pixel in a given image, great challenges are induced by the demand for massive training samples with
In the field of image recognition pixel-wise annotations. facing the same dilemma, zero-shot learning (ZSL) [32, 41, 18] is proposed, where the classification model is trained to accommodate unseen objects using knowledge learnt from seen classes. Similarly, zero-shot segmenta-tion (ZSS) [47, 5, 20, 31, 36, 26, 21] is also proposed in semantic segmentation. The goal of ZSS is to generate seg-mentation mask for objects of both seen (with annotated in-stances) and unseen categories (that have never been seen
*Equal contribution.
†Henghui Ding is the corresponding author.
Figure 1. Zero-shot Semantic Segmentation (ZSS). The purpose of ZSS is to transfer knowledge learnt from seen classes to unseen classes (i.e., never-seen in training). In testing, ZSS classifies each pixel to one of the seen classes or newly-added unseen classes. in training), as shown in Figure 1.
The preliminary ZSL setting does not need to distinguish between the seen and unseen classes, which is unrealistic and contradicts the real-world conditions of recognition. A practical generalized zero-shot learning (GZSL) [42] is then proposed since image samples from seen and unseen classes often appear together and it is important to recognize both groups simultaneously. Zero-shot segmentation (ZSS) is naturally an analogue of GZSL, since the given image for segmentation already contains diverse categories. “ZSS” in this paper stands for the generalized case.
In ZSS, an important source of information is the seman-tic representation - semantic information encoded by high-dimensional vectors. The semantic information can include automatically-extracted word vectors, manually-defined at-tribute vectors, context-based embedding, or their combina-tions. Each class (either seen or unseen) has its own seman-tic representation. The ways of utilizing unseen information separates ZSS into two settings: inductive setting and trans-ductive setting (see Figure 1). In inductive training, the vi-sual features and semantic representations of only the seen classes are available; while in transductive training, one can
access the semantic representations (and sometimes images without annotations) of unseen classes, apart from the vi-sual features and semantic representations of seen classes.
Although several methods (e.g., ZS3 [5], CaGNet [20], and
CSRL [33]) are developed under transductive learning, this setting is indeed impractical because it violates the unseen assumption and significantly reduces challenges. Neverthe-less, both settings have reached a consensus that the ground truth of unseen classes should never be present or utilized during training. Therefore, misuse of ground truth of un-seen classes in training the classifier should be prevented.
In this work, we obey a strict inductive setting, in which only the information (i.e., semantic representations, visual features and ground truth) of seen classes are available dur-ing training. In ZSS, to transfer knowledge from seen to un-seen classes, a mapping function from semantic space to vi-sual feature space is expected. For instance, ZS3 [5] trained such a generator on seen classes and used it to produce fake visual features for unseen classes. These fake features are then used to fine-tune the classifier (trained on the seen classes in advance). However, it is controversial that their classifier training uses pairs of fake feature and correspond-ing label, which requires practically inaccessible informa-tion (e.g., the ground-truth of unseen pixels, the number and the attributes of unseen classes). Besides, the trained model can no longer handle newly-added unseen classes, showing a fixed capacity. In this work, to break this limitation, we employ a prototypical way instead of the convolutional clas-sifier way. We extract high-level visual representations by training prototypes that correspond to classes one-to-one.
Segmentation is performed to each pixel by finding the clos-est prototype to its own features. The mapping between se-mantic information and visual features is thereby bridged by the prototype vector. A lightweight projection network is proposed to learn the mapping from semantic information to prototypes. Any new unseen class can be flexibly added during testing, by projecting its semantic description to a prototype and adding the new prototype to existing ones.
Bias problem also imposes a significant challenge to
ZSS. There exists a natural bias towards the seen classes when the model trained merely on seen classes is expected to segment both seen and unseen classes. Misclassifications easily take place when objects in unseen classes are similar to a seen class, to any extent or in any form. Previous ZSS works pay little attention to this issue, resulting in inaccu-rately predicting the unseen class as the seen class. In this work, we propose an open-set rejection (OSR) module as a detector to identify which group (seen or unseen) the test sample belongs to. Concretely, the OSR classifies an ob-ject directly to a certain seen class, or a general “unknown” class. If an object is predicted as the “unknown”, a certain unseen class label that has the closet prototype will be as-signed to it. Therefore, during testing, the possible classes into which unseen objects can be classified is constrained.
The main contributions of this works are concluded from four aspects:
• We clarify the inductive and transductive setting of
ZSS and perform ZSS in challenging inductive setting.
• We employ prototypical matching in ZSS, to bridging the semantic and visual information, and make it flex-ible to add new unseen categories during testing.
• We introduce the open-set rejection into ZSS for the first time, effectively mitigating the bias problem and enhancing the parsing performance.
• We achieve new state-of-the-art performance on ZSS. 2.