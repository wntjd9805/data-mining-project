Abstract Encoding of Stories, Objects, and Pictures
Hareesh Ravi1*
Kushal Kafle2
Scott Cohen2
Jonathan Brandt2
Mubbasir Kapadia1 1Rutgers University, 2Adobe Research 1{hr268, mk1353}@cs.rutgers.edu, 2{kkafle, scohen, jbrandt}@adobe.com
Abstract
Visual storytelling and story comprehension are uniquely human skills that play a central role in how we learn about and experience the world. Despite remarkable progress in recent years in synthesis of visual and textual content in isolation and learning effective joint visual-linguistic rep-resentations, existing systems still operate only at a super-ficial, factual level. With the goal of developing systems that are able to comprehend rich human-generated narra-tives, and co-create new stories, we introduce AESOP: a new dataset that captures the creative process associated with visual storytelling. Visual panels are composed of clip-art objects with specific attributes enabling a broad range of creative expression. Using AESOP, we propose foun-dational storytelling tasks that are generative variants of story cloze tests, to better measure the creative and causal reasoning ability required for visual storytelling. We fur-ther develop a generalized story completion framework that models stories as the co-evolution of visual and textual con-cepts. We benchmark the proposed approach with human baselines and evaluate using comprehensive qualitative and quantitative metrics. Our results highlight key insights re-lated to the dataset, modelling and evaluation of visual sto-rytelling for future research in this promising field of study. 1.

Introduction
“Examples are the best precept” – Aesop, The Two Crabs
Storytelling is integral to human experience. Starting from when we are very young, stories help shape our un-derstanding of the world around us, and the people that in-habit it. Through stories, we encode a wide range of shared knowledge, including common sense physics, cause and ef-fect, human psychology, and morality [52]. Storytelling and story comprehension are closely linked in that both involve the construction of rich mental models, comprising scenes,
*A portion of this work was done during Hareesh Ravi’s internship at
Adobe Research. inanimate objects and their properties, as well as characters and their intentions [36]. Consequently, stories are crucial to mental development in humans. We postulate that ma-chine intelligence requires comparable skills, particularly when interacting with people.
Though there have been some works on understanding and modelling natural language stories [55, 57, 68], there is limited work on aligning stories with the visual world
[33, 34]. When there is no visual information available as part of a story, such as in novels, people still inher-ently visualize the events in real-time to disambiguate de-tails and make inferences about the story [95] with ease.
Humans draw upon deep world knowledge, grounded in visual-linguistic stories and experience that we’ve accumu-lated from a young age. Therefore, it is likely worthwhile to similarly ground machine comprehension and synthesis of stories in the visual world.
Much of the current work on joint understanding of vi-sion and language hinges on learning to describe factual in-formation about objects and scenes in an image. Particu-larly, the text, and benchmarks associated with images in popular datasets such as MSCOCO [47], Flickr [87] and Vi-sual Genome [42] focus on superficial factual descriptions, rather than a narrative. Moreover, the few existing visual story datasets [33, 30] lack coherence and diversity [64] that are key to a good story. Also, these datasets assume visual storytelling as a perceptive process rather than a creative process. For example, in [33], crowd workers wrote natural language stories given a sequence of images from photo al-bums. Such a process leads to superficial and disjoint stories
[3, 34] that focus on connecting text to image rather than on forming a coherent narrative. The limitation of such a pro-cess is evident when people are shown the story panels in random order. For over a third of the stories in [33], hu-man observers are unable to find the “true” order of events, calling into question the value of such datasets for studying stories. Another limitation is that the task is to generate text for a sequence of given images, with story writers having no control over the visual input. Consequently, a trained model is required only to produce a “feasible” text for a given im-Figure 1. Example story from our AESOP dataset with title and genres. The narrative is interesting, coherent and follows a clear causal arc with introduction and a moral at the end. The visual depiction of the story, including the changes in the expression of the characters, shows clear coherence and supports the narrative. age sequence. The converse task, to generate visual input that would match the given text story, is markedly absent in the literature. Though some recent works developed tech-niques for generating visual input from text [41, 73, 61] they still focus on factual information extraction rather than nar-rative understanding.
In this paper, we propose AESOP: a novel dataset that captures the creative process associated with visual story-telling. An example story from our dataset is shown in Fig-ure 1. To ensure stories are diverse and creative, we ask workers to create both the visual and textual parts of the story simultaneously from scratch. Inspired by [94, 73], our dataset employs abstract visual scenes, with a broad set of choices for objects and attributes needed for visual story-telling. Examples of the wide range of stories created from this diverse, yet finite palette, appear in supplementary ma-terials.
Current visual storytelling research has dealt with tasks such as storytelling, generation [30, 33] and illustration [64] or cloze tasks in [34] that primarily focus on cross–modal retrieval or generation. We discuss the limitations of such tasks and propose alternate tasks on AESOP that measure a system’s ability to comprehend and create stories from a true multimodal perspective requiring the perception and creation of both visual and textual modalities, that is absent in existing literature. The objective is for a system to be a creative assistant, by either autonomously or interactively assisting in creative processes like storytelling with visual, linguistic and narrative reasoning abilities.
Our contributions are as follows: (1) AESOP,1 a novel abstract visual storytelling dataset that captures the creative process associated with visual sto-rytelling resulting in diverse, coherent and creative stories compared to existing datasets. (2) We propose novel story comprehension tasks on AE-SOP that demands multimodal, abstract, creative and causal reasoning ability from visual storytelling systems. Further, we propose a novel generalized story comprehension frame-work that models stories in our dataset as the co-evolution of visual and textual concepts. (3) We quantitatively and qualitatively compare the pro-posed method and tasks with existing baselines and mo-tivate our design choices through comprehensive ablation study. To the best of our knowledge, ours is the first work to study stories by aligning abstract visual and textual con-cepts and propose a comprehensive dataset, task and model to study important factors that govern visual storytelling.We will make the dataset publicly available2 to promote future research in this promising and challenging field of study. 2.