Abstract to the target domain.
Domain generalization (DG) aims to generalize a model trained on multiple source (i.e., training) domains to a dis-tributionally different target (i.e., test) domain.
In con-trast to the conventional DG that strictly requires the avail-ability of multiple source domains, this paper considers a more realistic yet challenging scenario, namely Sin-gle Domain Generalization (Single-DG), where only one source domain is available for training. In this scenario, the limited diversity may jeopardize the model general-ization on unseen target domains. To tackle this prob-lem, we propose a style-complement module to enhance the generalization power of the model by synthesizing im-ages from diverse distributions that are complementary to the source ones. More speciﬁcally, we adopt a tractable upper bound of mutual information (MI) between the gen-erated and source samples and perform a two-step op-(1) by minimizing the MI upper timization iteratively: bound approximation for each sample pair, the generated images are forced to be diversiﬁed from the source sam-ples; (2) subsequently, we maximize the MI between the samples from the same semantic category, which assists the network to learn discriminative features from diverse-styled images. Extensive experiments on three bench-mark datasets demonstrate the superiority of our approach, which surpasses the state-of-the-art single-DG methods by up to 25.14%. The code will be publicly available at https://github.com/BUserName/Learning to diversify 1.

Introduction
The remarkable success of modern machine learning al-gorithms is built on the assumption that the source (i.e., training) and target (i.e., test) samples are drawn from sim-ilar distributions. In practice, this assumption is commonly violated by various factors, such as the changes of illumina-tions, object appearance, or background, which are known as the domain shift problem [36, 3]. Due to the discrepancy across domains, the performance of a model trained on the source domain can be signiﬁcantly degraded when applied
To tackle this problem, extensive research has been car-ried out mainly on domain adaptation and domain gener-alization. Domain adaptation aims to transfer the knowl-edge from the labeled source domain(s) to an unlabeled tar-get domain [45, 1, 26, 13], while domain generalization at-tempts to generalize a model to an unseen target domain by learning from multiple source domains [34, 42, 10, 42].
Compared with domain adaptation, domain generalization (DG) is considered as a more challenging task as the tar-get samples are not exposed in the training phase. Re-garding different strategies for transferring knowledge from source domains to the unseen target domain, existing DG techniques can be subsumed under two broad categories, i.e., alignment-based [2, 11, 33] and augmentation-based
[5, 17, 44, 50, 47]. Technically, alignment-based ap-proaches aim to reach the consensus from multiple source domains and learn domain-invariant latent representations for the target domain. Augmentation-based approaches learn to augment the source images with different trans-formations or generate the pseudo-novel samples for each source domain.
In general, the paradigm of DG relies on the availability of multiple source domains. However, it is more plausible to consider a more realistic scenario namely single domain generalization (Single-DG), where only one source domain is at hand during training. Despite DG has been extensively studied, single-DG remains under-explored. It is nontrivial for prior DG methods to handle this new setting, as the sam-ples gathered from multiple sources and the domain identi-ﬁers are no longer accessible. Without the domain informa-tion to rely on, neither alignment-based nor augmentation-based models could well identify the domain-invariant fea-tures or transformations that are robust to unseen target do-main shifts. Very recently, a few works [37, 49] are pro-posed that learn to add adversarial noises on the source im-ages in order to train robust classiﬁers against unforeseen data shifts. While contributing positively to address the model vulnerability, the manipulated images are indistin-guishable from the original source images, which are not sufﬁciently diverse to cover the real target distributions.
To address the issue of insufﬁcient diversity, in this pa-per, we propose a novel approach of Learning-to-diversify (L2D), which aims to improve the model generalization ca-pacity by alternating diverse sample generation and dis-criminative style-invariant representation learning as il-lustrated in Figure 1. Speciﬁcally, we design a style-complement module, which learns to synthesize samples with unseen styles, which are out of original distribu-tions. Different from previous augmentation approaches that quantify diversity with the Euclidean distance in the im-age space, we diversify the generated samples in the latent feature space. By explicitly posing a greater challenge on the trained classiﬁer, the model resilience against the target shift is enhanced. We gradually enlarge the shift between the distributions of the generated samples and the source samples at the training time and perform the two-step op-timization iteratively. By minimizing the tractable upper bound of mutual information for each sample pair, the gen-erated images are forced to diversify from the source sam-ples in subspace. Furthermore, to obtain the style-invariant features, we maximize the mutual information between the images belonging to the same semantic category. Conse-quently, the style-complement module and the task model compete in a min-max game, which improves the general-ization ability of the task model by iteratively generating out-of-domain images and optimizing the style-invariant la-tent space. Note that, under this objective, the images gen-erated by the proposed style-complement module will not only being diversiﬁed from the source ones, but also can be considered as challenging samples to the task model.
The main contributions of our work are summarized as follows.
• We propose a style-complement module that addresses the single domain generalization by learning to gener-ate diverse images.
• A min-max mutual information optimization strategy is designed to gradually enlarge the distribution shift between the generated and the source images, while simultaneously bringing the samples from the same se-mantic category close in the latent space.
• To validate the effectiveness of the proposed method, extensive experiments are conducted on three bench-mark datasets, including digits recognition, corrupted
CIFAR-10, and PACS. We further show the effective-ness of our approach in the standard DG setting under the leave-one-domain-out protocol. The results clearly demonstrate that our method surpasses the state-of-the-art DG and Single-DG methods on all datasets. 2.