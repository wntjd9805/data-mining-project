Abstract
We present a method, Neural Radiance Flow (NeRFlow), to learn a 4D spatial-temporal representation of a dynamic scene from a set of RGB images. Key to our approach is the use of a neural implicit representation that learns to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing consistency across different modalities, our representation enables multi-view rendering in diverse dynamic scenes, including water pouring, robotic interaction, and real images, outperforming state-of-the-art methods for spatial-temporal view synthesis. Our approach works even when being provided only a single monocular real video.
We further demonstrate that the learned representation can serve as an implicit scene prior, enabling video processing tasks such as image super-resolution and de-noising without any additional supervision. 1.

Introduction
We live in a rich and dynamic world, consisting of scenes that rapidly change their appearance across both time and view angle. To accurately model the world around us, we need a scene representation that captures underlying lighting, physics, and 3D structure of the scene. Such representations have diverse applications: they can enable interactive explo-ration in both space and time in virtual reality, the capture of realistic motions for game design, and robot perception and navigation in the environment around them.
Traditional approaches, such as those used in state-of-the-art motion capture systems, typically are specialized to specific phenomena [1, 19] and fail to handle complex occlusions and fine details of motion. A core difficulty is that high resolution coverage of information requires a pro-hibitive amount of memory. Recent work has addressed this by using a neural network as a parametrization for scene de-Code at https://yilundu.github.io/nerflow/.
Figure 1: Given a set of training images captured from different views and timestamps, NeRFlow learns a spatial-temporal repre-sentation that captures the underlying 3D structure and dynamics and, in turn, enables 4D view synthesis. tails [41, 62, 42]. However, these scene representation often require a static scene and a large number of images captured from many cameras, which are not generally available in real-world scenarios.
In this work, we aim to learn a dynamic scene represen-tation which allows photorealistic novel view synthesis in complex dynamics, observed by only a limited number of (as few as one) cameras with known camera parameters. The key challenge is that the observations at each moment are sparse, restricting prior approaches [41, 62] from fitting a complex scene. To address this problem, we present a novel approach, Neural Radiance Flow (NeRFlow), that can effec-tively aggregate partial observations across time to learn a coherent spatio-temporal scene representation. We achieve this by formulating a radiance flow field, which encourages temporal consistency of appearance, density, and motion.
The radiance flow field is represented by two continu-ous implicit neural functions: a 6D (spatial position x, y, z, timestamp t and viewing direction θ, ϕ) radiance function for appearance and density, and a 4D (spatio-temporal position x, y, z, t) flow function for scene dynamics. Our representa-tion enables joint learning of both modules, which is critical given only sparse observations at each moment. Specifically, the flow field provides temporal correspondences for spatial 1
locations, enabling the appearance and density information captured at different moments to propagate across time. On the other hand, the radiance function describes the scene ge-ometry that informs the flow module about how objects are moving. Our model is fully differentiable, and thus can be trained directly using gradient backpropogation. By learning 3D structure and dynamics, our model can accomplish 4D view synthesis (Figure 1).
To evaluate our approach, we consider several challenging setups: a pouring scene which reflects fluid dynamics, an indoor scene in which a robot walks from near to far to exhibit long-range motion with great occlusion, multiple complex real scenes with transparent objects, as well as monocular videos capturing human motions. Our approach yields high-quality 4D view synthesis and outperforms a recent state-of-the-art method [42]. In addition, we show that our method can serve as a type of dynamic scene prior, which allows video denoising and super-resolution without any additional supervision, outperforming both classical and state-of-the-art internal learning methods.
In summary, our work has three contributions. First, we present a novel method, Neural Radiance Flow (NeRFlow), for learning implicit spatial-temporal scene representation.
It enables novel view synthesis across both space and time.
Second, we show that our approach can be effective with very limited observations down to only one camera. We achieve this by introducing a set of temporal consistency constraints over scene appearance, density, and motion. Fi-nally, show that our approach can serve as an implicit scene prior, outperforming classical and internal learning methods in super-resolution and image de-noising. 2.