Abstract
Motion blur caused by the moving of the object or cam-era during the exposure can be a key challenge for visual object tracking, affecting tracking accuracy significantly. In this work, we explore the robustness of visual object track-ers against motion blur from a new angle, i.e., adversarial blur attack (ABA). Our main objective is to online transfer input frames to their natural motion-blurred counterparts while misleading the state-of-the-art trackers during the tracking process. To this end, we first design the motion blur synthesizing method for visual tracking based on the generation principle of motion blur, considering the mo-tion information and the light accumulation process. With this synthetic method, we propose optimization-based ABA (OP-ABA) by iteratively optimizing an adversarial objective function against the tracking w.r.t. the motion and light ac-cumulation parameters. The OP-ABA is able to produce natural adversarial examples but the iteration can cause heavy time cost, making it unsuitable for attacking real-time trackers. To alleviate this issue, we further propose one-step ABA (OS-ABA) where we design and train a joint adversarial motion and accumulation predictive network (JAMANet) with the guidance of OP-ABA, which is able to efficiently estimate the adversarial motion and accumula-tion parameters in a one-step way. The experiments on four popular datasets (e.g., OTB100, VOT2018, UAV123, and
LaSOT) demonstrate that our methods are able to cause significant accuracy drops on four state-of-the-art trackers with high transferability. Please find the source code at https://github.com/tsingqguo/ABA 1.

Introduction
Visual object tracking (VOT) has played an integral part in multifarious computer vision applications nowadays rang-*Qing Guo and Ziyi Cheng are co-first authors and contribute equally.
â€ Lei Ma and Xiaofei Xie are corresponding authors (ma.lei@acm.org, xfxie@ntu.edu.sg).
Figure 1: An example of our adversarial blur attack against a deployed tracker, e.g., SiamRPN++ [30]. Two adjacent frames are fed to our attack and it generates an adversarially blurred frame that misleads the tracker to output an inaccurate response map. ing from augmented reality [1, 46] to video surveillance
[47], from human-computer interaction [36, 32] to traffic control [49], etc. Since the infusion of deep learning, VOT has become more powerful in terms of both algorithmic performance and efficiency [20], leading to the more per-vasive deployment of VOT-enabled on-device applications.
However, the VOT can still exhibit robustness brittleness when faced with less ideal video feed. Among many known degrading factors such as illumination variations, noise vari-ations, etc., motion blur is perhaps one of the most important adverse factors for visual object tracking, which is caused by the moving of the object or camera during the exposure, and can severely jeopardize tracking accuracy [18]. Most of the existing benchmarks [28, 50, 35] only indicate whether a video or a frame contains motion blur or not and this piece of information is still insufficient to analyze the influence from motion blur by means of controlling all the variables, e.g., eliminating other possible interference from other degra-dation modes, which may lead to incomplete conclusions regarding the effects of motion blur in these benchmarks.
Moreover, the currently limited datasets, albeit being large-scale, cannot well cover the diversity of motion blur in the real world because motion blur is caused by camera and object moving in the scene which is both dynamic and unknown. Existing motion blur generation methods cannot 1
thoroughly reveal the malicious or unintentional threat to visual object tracking, i.e., they can only produce natural motion blur which falls short of exposing the adversarial brittleness of the visual object tracker. As a result, it is nec-essary to explore a novel motion blur synthetic method for analyzing the robustness of the visual object trackers, which should not only generate natural motion-blurred frames but also embed maliciously adversarial or unintentional threats.
In this work, we investigate the robustness of visual track-ers against motion blur from a new angle, that is, adversarial blur attack (ABA). Our main objective is to online transfer in-put frames to their natural motion-blurred counterparts while misleading the state-of-the-art trackers during the tracking process. We show an intuitive example in Fig. 1. To this end, we first design the motion blur synthesizing method for visual tracking based on the generation principle of motion blur, considering the motion information and the light accu-mulation process. With this synthetic method, we further propose optimization-based ABA (OP-ABA) by iteratively optimizing an adversarial objective function against the track-ing w.r.t. the motion and light accumulation parameters.
The OP-ABA is able to produce natural adversarial ex-amples but the iteration can lead to a heavy time-consuming process that is not suitable for attacking the real-time tracker.
To alleviate this issue, we further propose one-step ABA (OS-ABA) where we design and train a joint adversarial motion and accumulation predictive network (JAMANet) with the guidance of OP-ABA, which is able to efficiently estimate the adversarial motion and accumulation parameters in a one-step way. The experiments on four popular datasets (e.g., OTB100, VOT2018, UAV123, and LaSOT) demon-strate that our methods are able to cause significant accuracy drops on four state-of-the-art trackers while keeping the high transferability. To the best of our knowledge, this is the very first attempt to study the adversarial robustness of VOT and the findings will facilitate future-generation visual object trackers to perform more robustly in the wild. 2.