Abstract
Successful active speaker detection requires a three-stage pipeline: (i) audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation modeling between a reference speaker and the background speakers within each frame, and (iii) temporal modeling for the reference speaker. Each stage of this pipeline plays an important role for the final performance of the created architecture. Based on a series of controlled experiments, this work presents several practical guidelines for audio-visual active speaker detection. Correspondingly, we present a new architecture called ASDNet, which achieves a new state-of-the-art on the
AVA-ActiveSpeaker dataset with a mAP of 93.5% outper-forming the second best with a large margin of 4.7%. Our code and pretrained models are publicly available 1. 1.

Introduction
Fusion of audio and video modalities has been shown to provide promising solutions to long-standing challenging problems. These include among others, speaker diarization
[16], biometrics [7], and action recognition [15, 38]. Sim-ilar to other tasks, Audiovisual Active Speaker Detection (AV-ASD) has also long been studied in literature [9,10]. A particularly challenging flavor of this problem is AV-ASD in the wild, where speech is to be detected and assigned to one of possibly multiple active speakers at each instant in time.
Clearly, fusing the complementary discriminative informa-tion from audio and video modalities is crucial: visual-only approaches can easily be mistaken by other face/mouth mo-tions such as eating, yawning or emotional expressions.
Audio-only approaches, although able to perform source clustering and separation [18, 46], aren’t sufficiently robust to count the number of speakers and assign speech to the correct source. This is especially challenging with a single microphone input in acoustically adverse conditions, typi-cally encountered in practice. 1https://github.com/okankop/ASDNet
Figure 1. Audio-visual active speaker detection pipeline. The task is to determine if the reference speaker at frame t is speaking or not-speaking. The pipeline starts with audio-visual encoding of each speaker in the clip. Secondly, inter-speaker relation model-ing is applied within each frame. Finally, temporal modeling is used to capture long-term relationships in natural conversations.
Examples are from AVA-ActiveSpeaker dataset [42].
Recently, the AVA-ActiveSpeaker dataset [42] provided the first large-scale standard benchmark for audio-visual ac-tive speaker detection in the wild. Recent research [1, 32] indicates that active speaker detection in the wild requires (i) integration of audio-visual information for each speaker, (ii) contextual information that captures inter-speaker rela-tionships, and (iii) temporal modeling to exploit long term relationships in natural conversation. In this paper, we con-solidate this three-stage pipeline for audio visual speaker detection, illustrated in Fig. 1, and study the importance of each stage in detail.
Contributions. We propose a novel three-stage pipeline for audio-visual active speaker detection in the wild. Our archi-tecture, named ASDNet, sets a new state-of-the-art result on
AVA-ActiveSpeaker dataset with a 93.5% mAP, and outper-forms the second best method [32] with a large margin of 4.7% mAP (Section 4.5). As part of ASDNet, we propose (1) architectures for the audio and video backbones of the audio-visual encoder (Section 3.2), that haven’t been previ-ously explored for active speaker detection; (2) a simple, yet effective inter-speaker relation modeling mechanism (Section 3.3); (3) In addition, we provide detailed ablation study and guidelines for tuning all components of ASDNet. The study includes comparison to the state of the art for the two novel components mentioned above, as well as evaluation of var-ious Recurrent Neural Network (RNN) architectures for temporal modeling (Section 4.2.). 2.