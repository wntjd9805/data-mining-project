Abstract
We study the machine’s understanding of embodied ref-erence: One agent uses both language and gesture to refer to an object to another agent in a shared physical environ-ment. Of note, this new visual task requires understanding multimodal cues with perspective-taking to identify which object is being referred to. To tackle this problem, we intro-duce YouRefIt, a new crowd-sourced dataset of embodied reference collected in various physical scenes; the dataset contains 4,195 unique reference clips in 432 indoor scenes.
To the best of our knowledge, this is the first embodied ref-erence dataset that allows us to study referring expressions in daily physical scenes to understand referential behavior, human communication, and human-robot interaction. We further devise two benchmarks for image-based and video-based embodied reference understanding. Comprehensive baselines and extensive experiments provide the very first result of machine perception on how the referring expres-sions and gestures affect the embodied reference under-standing. Our results provide essential evidence that ges-tural cues are as critical as language cues in understanding the embodied reference. 1.

Introduction
Human communication [51] relies heavily on establish-ing common ground [50, 48] by referring to objects in a shared environment. This process usually takes place in two forms: language (abstract symbolic code) and gesture (unconventionalized and uncoded). In the computer vision community, efforts of understanding reference have been primarily devoted in the first form through an artificial task,
Referring Expression Comprehension (REF) [64, 17, 63, 32, 60, 56, 57], which localizes a particular object in an image with a natural language expression generated by the annotator. Evidently, the second form, gesture, has been left almost untouched. Yet, this nonverbal (gesture) form is more profound in the communication literature compared to the pure verbal (language) form with ample evolutionary evidence [1, 36, 14]; it is deeply rooted in human cognition development [29, 30] and learning process [7], and tightly coupled with the language development [23, 6, 18].
Figure 1: Imagine you walk into a bakery for your favorite. To pre-cisely express your intent, you point to it and produce an utterance
“the white cheese in front of the bread.” This multimodal commu-nicative act is produced by assuming it can be properly understood by the staff, whose embodiment differs in the shared physical en-vironment. Such a daily deictic-interaction scenario illustrates the significance of visual perspective-taking in embodied reference.
Fundamentally, most modern literature deviates from the natural setting of reference understanding in daily scenes, which is embodied: An agent refers to an object to another in a shared physical space [39, 54, 11], as exemplified by
Fig. 1. Embodied reference possesses two distinctive char-acteristics compared to REF. First, it is multimodal. Peo-ple often use both natural language and gestures when re-ferring to an object. The gestural component and language component are semantically coherent and temporally syn-chronous to coordinate with one another, creating a concise and vivid message [22] while elucidating the overloaded meaning if only one modality is presented [20]. Second, rec-ognizing embodied reference requires visual perspective-taking [25, 2, 39], the awareness that others see things from different viewpoints and the ability to imagine what oth-ers see from their perspectives. It requires both the message sender and receiver to comprehend the immediate environ-ments [11], including the relationship between the inter-locutors and the relationships between objects, in the shared perceptual fields for effective communication.
To address the deficiencies in prior work and study ref-erence understanding at a full spectrum, we introduce a new dataset, YouRefIt , for embodied reference understand-ing. The reference instances in YouRefIt are crowd-sourced with diverse physical scenes from Amazon Mechanic Turk (AMT). Participants are instructed to film videos in which they reference objects in a scene to an imagined person (i.e., a mounted camera) using both language and gestures. Min-imum requirements of the scenes, objects, and words are imposed to ensure the naturalness and the variety of col-lected videos. Videos are segmented into short clips, with each clip containing an exact one reference instance. For each clip, we annotate the reference target (object) with a bounding box. We also identify canonical frames in a clip:
They are the “keyframes” of the clip and contain sufficient information of the scene, human gestures, and referenced objects that can truthfully represent the reference instance.
Fine-grained semantic parsing of the transcribed sentences is further annotated to support a detailed understanding of the sentences. In total, the YouRefIt dataset includes 4,195 embodied reference instances from 432 indoor scenes.
To measure the machine’s ability in Embodied Reference
Understanding (ERU), we devise two benchmarks on top of the proposed YouRefIt dataset. (i) Image ERU takes a canonical frame and the transcribed sentence of the refer-ence instance as the inputs and predicts the bounding box of the referenced object. Image ERU adopts the settings from the well-studied REF but is inherently more challenging and holistic due to its requirement on a joint and coherent under-standing of human gestures, natural language, and objects in the context of human communication. (ii) Video ERU takes the video clip and the sentence as the input, identi-fies the canonical frames, and locates the reference target within the clip. Compared to Image ERU, Video ERU takes one step further and manifests the most natural human-robot communication process that requires distinguishing the ini-tiation, the canonical frames, and the ending of a reference act while estimating the reference target in a temporal order.
Incorporating both language and gestural cues, we for-mulate a new multimodal framework to tackle the ERU tasks. In experiments, we provide multiple baselines and ablations. Our results reveal that models with explicit gestu-ral cues yield better performance, validating our hypothesis that gestural cues are as critical as language cues in resolv-ing ambiguities and overloaded semantics with cooperation (perspective-taking) in mind [20, 19, 39, 65, 68], echoing a recent finding in the embodied navigation task [54]. We further verify that temporal cues are essential in canonical frame detection, necessitating understanding embodied ref-erence in dynamic and natural sequences.
This paper makes three major contributions. (i) We col-lect the first video dataset in physical scenes, YouRefIt, to study the reference understanding in an embodied fashion.
We argue this is a more natural setting than prior work and, therefore, further understanding human communica-tions and multimodal behavior. (ii) We devise two bench-marks, Image ERU and Video ERU, as the protocols to study and evaluate the embodied reference understanding. (iii) We propose a multimodal framework for ERU tasks with multiple baselines and model variants. The experimen-tal results confirm the significance of the joint understand-ing of language and gestures in embodied reference. 2.