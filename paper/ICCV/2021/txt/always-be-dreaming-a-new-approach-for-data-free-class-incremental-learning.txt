Abstract
Modern computer vision applications suffer from catas-trophic forgetting when incrementally learning new con-cepts over time. The most successful approaches to al-leviate this forgetting require extensive replay of previ-ously seen data, which is problematic when memory con-straints or data legality concerns exist.
In this work, we consider the high-impact problem of Data-Free Class-Incremental Learning (DFCIL), where an incremental learning agent must learn new concepts over time with-out storing generators or training data from past tasks.
One approach for DFCIL is to replay synthetic images produced by inverting a frozen copy of the learner’s clas-siﬁcation model, but we show this approach fails for common class-incremental benchmarks when using stan-dard distillation strategies. We diagnose the cause of incremental distilla-this failure and propose a novel tion strategy for DFCIL, contributing a modiﬁed cross-entropy training and importance-weighted feature distilla-tion, and show that our method results in up to a 25.1% in-crease in ﬁnal task accuracy (absolute difference) compared to SOTA DFCIL methods for common class-incremental benchmarks. Our method even outperforms several stan-dard replay based methods which store a coreset of im-ages. Our code is available at https://github.com/
GT-RIPL/AlwaysBeDreaming-DFCIL 1.

Introduction
A shortcoming of modern computer vision settings is that they often assume ofﬂine training with a large dataset encompassing all objects to be encountered during deploy-In practice, many applications require a model ment. be continuously updated after new environments/situations are encountered. This is the class-incremental learning paradigm (also known as a subset of continual or lifelong learning), with the loss of knowledge over sequences of learning tasks referred to as catastrophic forgetting. Suc-*Correspondence to: James Smith jamessealesmith@gatech.edu cessful incremental learning approaches have an unfor-they require extensive memory for tunate commonality: replay of previously seen or modeled data to avoid the catastrophic forgetting problem. This is concerning for many computer vision applications because 1) Many com-puter vision applications are on-device and therefore mem-ory constrained [15, 35, 52], and 2) Many computer vi-sion applications learn from data which cannot be legally stored [5, 10, 62]. This leads us to ask: How can computer vision systems incrementally incorporate new information without storing data? We refer to this setting as Data-Free
Class-Incremental Learning (DFCIL) (also known as Data-Free Continual Learning [60]).
An intuitive approach for DFCIL is to simultaneously train a generative model to be sampled for replay [25, 26, 49, 58]. Unfortunately, training a generative model is much more computationally and memory intensive com-pared to a classiﬁcation model. Additionally, it is not clear whether generating images from the data distribution will violate data legality concerns because using a generative model increases the chance of memorizing potentially sen-sitive data [42]. Instead, we explore the concept of model-inversion image synthesis, where we can invert the already provided inference network to obtain images with similar activations in the network to the training data. This idea is inviting because it requires no additional networks to be trained (it only requires the existing inference network) and is less susceptible to data privacy concerns.
The closest existing work for the DFCIL problem is
DeepInversion [60], which optimizes random noise into images for knowledge distillation using a frozen teacher network. DeepInversion is designed for standard student-teacher knowledge distillation and achieves state-of-the-art performance for this task. Unfortunately, the authors report that when trying class-incremental learning for tasks where the old images and new images are similar (such as tasks from the same dataset, a standard benchmarking practice for class-incremental learning), their method performs “sta-tistically equivalent or slightly worse compared to Learning without Forgetting (LwF)”, with LwF [34] being their most competitive existing baseline.
(a) (b) (c)
Figure 1: The distribution of feature embeddings when using synthetic replay data for class-incremental learning. (a) A straight application of synthetic data makes the model learn features more distinguishable between real and fake instead of task 1 and 2. This is the main problem analyzed and addressed in this work. (b) Modifying classiﬁcation loss and adding regularization mitigates the feature drifting between real and fake. (c) This is the desired feature distributions. Our method makes task 1 and 2 more separable.
The goal of this paper (summarized in Figure 1) is to dissect the cause of this failure and propose a solution for
DFCIL. Speciﬁcally, we reason that when training a model with real images from the current task and synthetic images representing the past tasks, the feature extraction model causes the feature distributions of real images from the past tasks (which are not available during training) to be close in the feature space to the real images from the cur-rent task and far in the feature space from the synthetic images. This causes a bias for the model to falsely predict real images from the previous tasks with current task labels.
This phenomena indicates that when training a network with two distributions of data, containing both a semantic shift (past tasks versus current task) and a distribution shift (synthetic data versus real data), the distribution shift has a higher effect on the feature embeddings. Thus, valida-tion/test images from the previous classes will be identiﬁed as new classes due to the model ﬁxating on their domain (i.e., realistic versus synthetic pixel distribution) rather than their semantic content (i.e., past versus current task).
To address this issue, we propose a novel class-incremental learning method which learns features for the new task with a local classiﬁcation loss which excludes the synthetic data and past-task linear heads, instead relying on importance-weighted feature distillation and linear head
ﬁne-tuning to separate feature embeddings of the new and past tasks. We show that our method represents the new state of the art for the DFCIL setting, resulting in up to a 25.1% increase in ﬁnal task accuracy (absolute difference) compared to DeepInversion for common class-incremental benchmarks, and even outperforms popular replay baselines
Naive Rehearsal and LwF with a coreset. In summary, we make the following contributions: 1. We use a classic class-incremental learning benchmark to diagnose and analyze why standard distillation ap-proaches for class-incremental learning (such as Deep-Inversion) fail when using synthetic replay data. 2. We directly address this failure with a modiﬁed cross-entropy minimization, importance-weighted feature dis-tillation, and linear head ﬁne-tuning. 3. We achieve a new state of the art performance for the
DFCIL setting. 2.