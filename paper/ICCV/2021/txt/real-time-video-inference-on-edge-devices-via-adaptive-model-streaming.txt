Abstract
Real-time video inference on edge devices like mobile phones and drones is challenging due to the high compu-tation cost of Deep Neural Networks. We present Adaptive
Model Streaming (AMS), a new approach to improving the performance of efﬁcient lightweight models for video infer-ence on edge devices. AMS uses a remote server to continu-ally train and adapt a small model running on the edge de-vice, boosting its performance on the live video using online knowledge distillation from a large, state-of-the-art model.
We discuss the challenges of over-the-network model adap-tation for video inference and present several techniques to reduce communication the cost of this approach: avoiding excessive overﬁtting, updating a small fraction of important model parameters, and adaptive sampling of training frames at edge devices. On the task of video semantic segmenta-tion, our experimental results show 0.4–17.8 percent mean
Intersection-over-Union improvement compared to a pre-trained model across several video datasets. Our prototype can perform video segmentation at 30 frames-per-second with 40 milliseconds camera-to-label latency on a Samsung
Galaxy S10+ mobile phone, using less than 300 Kbps uplink and downlink bandwidth on the device. 1.

Introduction
Real-time video inference is a core component for many applications, such as augmented reality, drone-based sensing, robotic vision, and autonomous driving. These applications use Deep Neural Networks (DNNs) for inference tasks like object detection [52], semantic segmentation [7], and pose estimation [6]. However, state-of-the-art DNN models are too expensive to run on low-powered edge devices (e.g., mo-bile phones, drones, consumer robots [57, 58]), and cannot run in real-time even on accelerators such as Coral Edge
TPU and NVIDIA Jetson [12, 59, 38].
A promising approach to improve inference efﬁciency is to specialize a lightweight model for a speciﬁc video and task.
The basic idea is to use distillation [30] to transfer knowledge from a large “teacher” model to a small “student” model. For
Figure 1: Semantic segmentation results on real-world outdoor videos: rows from top to bottom represent No Customization,
One-Time, Remote+Tracking, Just-In-Time, and AMS. Uplink and downlink bandwidth usage are reported below each variant.
AMS provides better accuracy with limited bandwidth and reduces artifacts (e.g., see the car/person detected in error by the no/one-time customized models and remote tracking in the second column). example, Noscope [33] trains a student model to detect a few object classes on speciﬁc videos ofﬂine. Just-In-Time [46] extends the idea to live, dynamic videos by training the student model online, specializing it to video frames as they arrive. These approaches provide signiﬁcant speedups for scenarios that perform inference on powerful machines (e.g., server-class GPUs), but they are impractical for on-device inference at the edge. The ofﬂine approach isn’t desirable since videos can vary signiﬁcantly from device to device (e.g., different locations, lighting conditions, etc.), and over time for the same device (e.g., a drone ﬂying over different areas). On the other hand, training the student model online on edge devices is computationally infeasible.
In this paper we propose Adaptive Model Streaming (AMS), a new approach to real-time video inference on edge devices that ofﬂoads knowledge distillation to a remote server communicating with the edge device over the network.
AMS continually adapts a small student model running on the edge device to boost its accuracy for the speciﬁc video in real time. The edge device periodically sends sample video
frames to the remote server, which uses them to ﬁne-tune (a copy of) the edge device’s model to mimic a large teacher model, and sends (or “streams”) the updated student model back to the edge device.
Performing knowledge distillation over the network in-troduces a new challenge: communication overhead. Prior techniques such as Just-In-Time aggressively overﬁt the stu-dent model to the most recent frames, and therefore must frequently update the model to sustain high accuracy. We show, instead, that training the student model over a suitably chosen horizon of recent frames — not too small to over-ﬁt narrowly, but not too large to surpass the generalization capacity of the model — can achieve high accuracy with an order of magnitude fewer model updates compared to
Just-In-Time training.
⇠
Even then, a naïve implementation of over-the-network model training would require signiﬁcant bandwidth. For ex-ample, sending a (small) semantic segmentation model such as DeeplabV3 with MobileNetV2 [54] backbone with 2 million (ﬂoat16) parameters every 10 seconds would require over 3 Mbps of bandwidth. We present techniques to reduce both downlink (server to edge) and uplink (edge to server) bandwidth usage for AMS. For the downlink, we develop a coordinate-descent [61, 47] algorithm to train and send a small fraction of the model parameters in each update. Our method identiﬁes the subset of parameters with the most im-pact on model accuracy, and is compatible with optimizers like Adam [36] that maintain a state (e.g., gradient moments) across training iterations. For the uplink, we present algo-rithms that dynamically adjust the frame sampling rate at the edge device based on how quickly scenes change in the video. Taken together, these techniques reduce downlink and uplink bandwidth to only 181–225 Kbps and 57–296
Kbps respectively (across different videos) for a challeng-ing semantic segmentation task. To put AMS’s bandwidth requirement in perspective, it is less than the YouTube rec-ommended bitrate range of 300–700 Kbps to live stream video at the lowest (240p) resolution [64].
We evaluate our approach for real-time semantic seg-mentation using a lightweight model (DeeplabV3 with Mo-bileNetV2 [54] backbone). This model runs at 30 frames-per-second with 40 ms camera-to-label latency on a Sam-sung Galaxy S10+ phone (with Adreno 640 GPU). Our ex-periments use four datasets with long (10 minutes+) videos spanning a variety of scenarios (e.g., city driving, outdoor scenes, and sporting events). Our results show: 1. Compared to pretraining the same lightweight model without video-speciﬁc customization, AMS provides a 0.4–17.8% boost (8.3% on average) in mean Intersection-over-Union (mIoU), computed relative to the labels from a state-of-the-art DeeplabV3 with Xception65 [11] back-bone model. It also improves mIoU by 4.3% on average (up to 39.1%) compared to customizing the model once using the ﬁrst 60 seconds of each video.
⇥ 2. Compared to a remote inference baseline accompanied by on-device optical ﬂow tracking [67, 1], AMS provides an average improvement of 5.8% (up to 24.4%) in mIoU. less downlink bandwidth on aver-3. AMS requires 15.7 age (up to 44.5
) to achieve similar accuracy compared to Just-In-Time [46] (with similar reductions in uplink bandwidth).
Figure 1 shows three visual examples comparing the accuracy of AMS with these baseline approaches.
Our code and video datasets are available online at https://github.com/modelstreaming/ams.
⇥ 2.