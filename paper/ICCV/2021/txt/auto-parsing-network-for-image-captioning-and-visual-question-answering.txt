Abstract
We propose an Auto-Parsing Network (APN) to discover and exploit the input data’s hidden tree structures for im-proving the effectiveness of the Transformer-based vision-language systems. Specifically, we impose a Probabilis-tic Graphical Model (PGM) parameterized by the atten-tion operations on each self-attention layer to incorporate sparse assumption. We use this PGM to softly segment an input sequence into a few clusters where each cluster can be treated as the parent of the inside entities. By stack-ing these PGM constrained self-attention layers, the clus-ters in a lower layer compose into a new sequence, and the PGM in a higher layer will further segment this se-quence. Iteratively, a sparse tree can be implicitly parsed, and this tree’s hierarchical knowledge is incorporated into the transformed embeddings, which can be used for solving the target vision-language tasks. Specifically, we showcase that our APN can strengthen Transformer based networks in two major vision-language tasks: Captioning and Visual
Question Answering. Also, a PGM probability-based pars-ing algorithm is developed by which we can discover what the hidden structure of input is during the inference. 1.

Introduction
Nowadays, Transformer [57] based frameworks have been prevalently applied into vision-language tasks and im-pressive improvements have been observed in image cap-tioning [16,18,30,44], VQA [78], image grounding [38,75], and visual reasoning [1, 50]. Researchers attribute the progress to the various advantages of Transformer, like the
*Both authors contributed equally to this research.
Figure 1. (a) The different graph priors entailed in the classic self-attention and our Probabilistic Graphical Model (PGM) con-strained self-attention. Left: The classic self-attention pairs ev-ery two nodes in a graph and thus forms a fully connected graph.
Right: Constrained by PGM, five nodes are segmented into three clusters. (b) By stacking our PGM constrained self-attention lay-ers, a hierarchical tree can be automatically constructed. Hence we call our network as Auto-Parsing Network (APN). efficient parallel computing [57], the ability to approximate any sequence-to-sequence function [79], and the exploita-tion of the fully connected graph prior [9] provided by self-attention as shown in the left part of Figure 1(a). Particu-larly, by the graph prior, although vision and language data have quite different superficial forms, their structural com-monalities can be automatically abstracted, embedded, and transferred to narrow the domain gap.
However, the underlying structures of vision and lan-guage data are usually hierarchical and sparse, which are different from fully connected graphs, e.g., a sentence or an image can be parsed as constituent trees of words or ob-jects, respectively [9, 15, 56]. Without sparse and hierar-chical constraints, this system may be overwhelmed by the trivial global dependencies and overlook the critical local context [33, 66, 68]. Taking VQA as an example, the ques-tion in the last row of Figure 4 asks, “What number is the hour hand on?” for an image containing both hour hand and minute hand. A system with the fully connected graph
prior may make an incorrect prediction by directly exploit-ing the global dependency between “number” and “hand” due to their high co-occurrence frequency in the training set, and thus neglects the key local context “hour hand”. A similar problem is also observed in image captioning where noisy image scene graphs constructed by trivial dependen-cies may contribute less to the improvement [40].
To reduce trivial connections of fully connected graphs, researchers usually parse the input into some sparse and hi-erarchical structures, such as the filtered scene graphs [14, 23, 81] or sparse trees [52] and then exploit them into solv-ing various vision and language tasks, e.g., image cap-tioning [70, 72], VQA [4, 32, 55], grounding [6, 37] and
VCR [77]. However, these strategies require a large number of matched graph annotations [27,39,67] for training useful parsers [12, 51, 80, 82]; otherwise the domain shift can be induced, invalidating the parsed graphs.
To relieve the burden of incorporating hierarchical and sparse graph priors, inspired by Tree-Transformer [63], we propose a network that can learn to automatically parse inputs into trees during the end-to-end training without any additional graph annotations, hence named as Auto-Parsing Network (APN). Specifically, we constrain the self-attention operation by a Probabilistic Graphical Model (PGM) [10,29], which is parameterized by differentiable at-tention operations. As shown in the right part of Figure 1(a), the PGM helps to segment the input sequence into a few clusters. After each segmenting iteration, only the entities in the same cluster can attend to each other, and thus, the local context is embedded. Intuitively, each cluster can be considered as the parent of inside entities and these clusters together compose a new sequence. By stacking constrained self-attention layers, the new sequence at a lower layer will be further segmented by the PGM at a higher layer. For example, as in Figure 1 (b), s1 2 in the first level are clustered into a new pseudo-parent node s2 1 in the second level. Then, s2 2 are further clustered. Via this itera-tive way, a tree can be automatically parsed. 1 and s1 1 and s2
By APN, the local and global contexts can be accord-ingly embedded at lower and higher layers. Once we build an encoder-decoder based on APN, both source and tar-get domains’ hierarchical structures can be automatically parsed, embedded, and transferred. We deploy the proposed
APN in two fundamental vision-language tasks: image cap-tioning [60, 65], and visual question answering [5]. Experi-ment results on both tasks show that our APN obtains con-sistent improvements compared to Transformer based mod-els. Furthermore, we develop a parsing algorithm, which can generate constituent trees for vision and language in-puts based on the calculated PGM probabilities. In this way, when the model infers, the hidden structure for each sample can be revealed.
In summary, we have the following contributions:
• Inspired by Tree-Transformer [63], we propose an Auto-Parsing Network (APN) which can unsupervisedly learns to parse trees for the inputs by imposing PGM probabil-ities on self-attention layers and exploiting hierarchical constraints into PGM probabilities.
• We design two different APNs for solving Image Caption-ing and visual Question Answering.
• We show that our APN achieves consistent improvements compared with the classic Transformer on both tasks. 2.