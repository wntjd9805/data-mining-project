Abstract
We present a task and benchmark dataset for person-centric visual grounding, the problem of linking between people named in a caption and people pictured in an im-age. In contrast to prior work in visual grounding, which is predominantly object-based, our new task masks out the names of people in captions in order to encourage meth-ods trained on such image–caption pairs to focus on con-textual cues, such as the rich interactions between mul-tiple people, rather than learning associations between names and appearances. To facilitate this task, we introduce a new dataset, Who’s Waldo, mined automatically from image–caption data on Wikimedia Commons. We propose a
Transformer-based method that outperforms several strong baselines on this task, and release our data to the research community to spur work on contextual models that consider both vision and language. Code and data are available at: https://whoswaldo.github.io 1.

Introduction
The correspondence between people observed in images and their mentions in text is informed by more than sim-ply their identities and our knowledge of their appearances.
Consider the image and caption in Figure 1. We often see such image–caption pairs in newspapers and, as humans, are skilled at recovering associations between the people depicted in images and their references in captions, even if we’re unfamiliar with the specific people mentioned. This ability requires complex visual reasoning skills. For the ex-ample in Figure 1, we must understand an underlying activ-ity (“passing”) and determine who is passing the ball, who is being passed to, and which people in the image are not mentioned at all.
In this paper, we present a person-centric vision-and-language grounding task and benchmark. The general prob-lem of linking between textual descriptions and image re-gions is known as visual grounding, and is a fundamental
∗ Equal contribution
Figure 1. By studying this picture and caption, we can use contex-tual cues to link between the people referred to in the text and their visual counterparts, even if we are unfamiliar with the specific in-dividuals. This capability requires understanding of a broad set of interactions (e.g. “passing”) and expected behaviors (e.g. players pass to their teammates). We propose the task of person-centric vi-sual grounding, where we abstract over identity names (e.g. mask-ing out Sam Schulz and Curtly Hampton with [NAME] tokens) to encourage algorithms to emulate such contextual reasoning. capability in visual semantic tasks with applications includ-ing image captioning [66, 41, 3], visual question answer-ing [19, 20, 26] and instruction following [4, 43, 7]. Our task and data depart from most existing works along two axes. First, our task abstracts over identity information, in-stead focusing specifically on the relations and properties specified in images and text. Second, rather than using data annotated by crowd workers, we leverage captions originat-ing from real-life data sources.
While visual grounding has traditionally centered around localizing objects based on referring expressions, we ob-serve that inferring associations based on expressions in person-centric samples—i.e. people’s names—could lead to problematic biases (e.g. with regards to gender). Hence, we formulate the task to use captions that mask out peo-ple’s names. This allows for an emphasized focus on the context—both in image and text—where the person ap-pears, requiring models to understand complex asymmetric human interactions and expected behaviors. For instance, in the example in Figure 1 we might expect a player to pass to someone on their own team.
To explore this problem, we create Who’s Waldo: a col-lection of nearly 300K images of people paired with textual descriptions and automatically annotated with alignments between mentions of people’s names and their correspond-ing visual regions. Who’s Waldo is constructed from the massive public catalog of freely-licensed images and de-scriptions in Wikimedia Commons. We leverage this unique data source to automatically extract image–text correspon-dences for over 200K people. We also provide evaluation sets that are validated using Amazon Mechanical Turk and demonstrate that our annotation scheme is highly accurate.
To link people across text and images, we propose a
Transformer-based model, building upon recent work on learning joint contextualized image–text representations.
We use similarity measures in the joint embedding space be-tween mentions of people and image regions depicting peo-ple to estimate these links. The contextualized Transformer-based representations are particularly suited to handle the masked names, by shifting the reasoning to surrounding contextual cues such as verbs indicating actions and ad-jectives describing visual qualities. Our results demonstrate that our model effectively distinguishes between different individuals in a wide variety of scenes that capture complex interactions, significantly improving over strong baselines. 2.