Abstract
Few-shot classification aims at classifying categories of a novel task by learning from just a few (typically, 1 to 5) labelled examples. An effective approach to few-shot clas-sification involves a prior model trained on a large-sample base domain, which is then finetuned over the novel few-shot task to yield generalizable representations. However, task-specific finetuning is prone to overfitting due to the lack of enough training examples. To alleviate this issue, we propose a new finetuning approach based on contrastive learning that reuses unlabelled examples from the base do-main in the form of distractors. Unlike the nature of un-labelled data used in prior works, distractors belong to classes that do not overlap with the novel categories. We demonstrate for the first time that inclusion of such distrac-tors can significantly boost few-shot generalization. Our technical novelty includes a stochastic pairing of exam-ples sharing the same category in the few-shot task and a weighting term that controls the relative influence of task-specific negatives and distractors. An important aspect of our finetuning objective is that it is agnostic to distractor la-bels and hence applicable to various base domain settings.
More precisely, compared to state-of-the-art approaches, our method shows accuracy gains of up to 12% in cross-domain and up to 5% in unsupervised prior-learning set-tings. Our code is available at https://github.com/ quantacode/Contrastive-Finetuning.git 1.

Introduction
The ability to learn from very few examples is innate to human intelligence. In contrast, large amounts of labelled examples are required by modern machine learning algo-rithms to learn a new task. This limits their applicability to domains where data is either expensive to annotate and collect or simply inaccessible due to privacy concerns. To overcome this limitation, few-shot classification has been proposed as a generic framework for learning to classify with very limited supervision [12, 31, 34, 59]. Under this paradigm, most approaches leverage prior knowledge from
Figure 1. Classification of Kakapos vs. Honduran Emeralds with just few examples per class and many distractors: The idea is to leverage unlabelled data in the form of distractors that need not be semantically related to the classes in the few-shot task.
The hope is that by pairing distractors and task samples as nega-tives (bottom six red boxes) and encouraging greater dissimilar-ity between such pairs, image representations of the two classes,
Kakapos and Honduran Emeralds, will be pushed farther away.
This would ultimately lead to better classification. a (labelled) base domain to solve a novel task by either finetuning-based transfer [9, 64] or meta-learning [12, 15, 49, 57, 59, 62, 69]. In particular, when the base and novel domains are related, the hope is that representations learnt in the base domain can be generalized to novel tasks, thus facilitating positive knowledge transfer.
While the above paradigm is effective for tasks that can leverage large datasets like ImageNet [52] as the related base domain, for others, such as rare species classifica-tion [68] or medical image classification [70], acquiring necessary prior knowledge can be exceedingly difficult due to the absence of a related base domain with labelled data.
To relax such data requirements, recent techniques explore alternative ways such as unsupervised learning [25, 28] or cross-domain learning [1, 15, 45, 66] to obtain representa-tions useful for novel tasks. In the absence of labelled base data, approaches like [25, 27, 28] seek to benefit from self-supervised representation learning over unlabelled data in a related domain. In a more challenging scenario where re-lated base data is hard to obtain, cross-domain techniques
[10, 64, 66] exploit representations learnt in other domains
that do not have the same task characteristics as the novel tasks.
Although the issue of learning a good prior representa-tion remains a core focus in few-shot classification, it ad-dresses only a part of the problem. In this work, we investi-gate the other important aspect, i.e., effective finetuning spe-cific to the novel task. Our main motivation comes from re-cent findings [1, 9, 16] that demonstrate the outperformance of simple finetuning over more sophisticated prior learning techniques such as meta-learning. Despite its effectiveness, we suspect that finetuning might still suffer from overfitting as a consequence of small training set in a few-shot task. To alleviate this situation, we propose to leverage additional unlabelled data exclusive to the task. Such datapoints are referred to as distractors. For instance, in the case of clas-sifying Honduran Emeralds and Kakapos (rare species of birds), examples of butterflies, cars or ducks can serve as distractors (Fig. 1). By the virtue of its task-exclusivity, distractors can be obtained from various data-abundant do-mains with categories that could be semantically unrelated to novel task categories. However, in this work, we restrict ourselves to just the base data as a source for distractors.
This allows us to efficiently reuse the data under standard settings and directly compare with prior works.
To this end, we pose the imminent question – Can dis-tractors improve few-shot generalization? The answer is, somewhat surprisingly, yes. To elucidate how, we propose
ConFT, a simple finetuning method based on a contrastive loss that contrasts pairs of the same class against those from different classes. We show that with a few simple but cru-cial modifications to the standard contrastive loss, distrac-tors can be incorporated to boost generalization. We hy-pothesize that in the absence of extensive in-domain su-pervision for prior experience, distractor-aware finetuning can yield non-trivial gains. Towards the design of the loss function, we adopt an asymmetric construction of similar-ity pairs to ensure that distractors contribute only through different-class pairs. Our key insight here is two-fold – 1) generalization in contrastive learning can be influenced by not only same-class but also different-class pairs; 2) con-struction of different-class pairs is extremely flexible in that it can include samples from task-specific as well as task-exclusive categories. As a test of generality, we study the effect of our finetuning approach in conjunction with two different prior learning setups, namely, cross-domain and unsupervised prior learning. Our contributions are as fol-lows.
• We propose contrastive finetuning, ConFT, a novel finetuning method for transfer based few-shot classi-fication.
• We show how distractors can be incorporated in a con-trastive objective to improve few-shot generalization.
• The proposed method outperforms state-of-the-art ap-proaches by up to 12 points in the cross-domain few-shot learning and up to 5 points in unsupervised prior learning settings. 2.