Abstract
Humans are able to continuously detect and track sur-rounding objects by constructing a spatial-temporal memory of the objects when looking around. In contrast, 3D ob-ject detectors in existing tracking-by-detection systems often search for objects in every new video frame from scratch, without fully leveraging memory from previous detection results.
In this work, we propose a novel system for in-tegrated 3D object detection and tracking, which uses a dynamic object occupancy map and previous object states as spatial-temporal memory to assist object detection in fu-ture frames. This memory, together with the ego-motion from back-end odometry, guides the detector to achieve more efﬁcient object proposal generation and more accurate ob-ject state estimation. The experiments demonstrate the ef-fectiveness of the proposed system and its performance on the ScanNet and KITTI datasets. Moreover, the proposed system produces stable bounding boxes and pose trajecto-ries over time, while being able to handle occluded and truncated objects. Code is available at the project page: https://zju3dv.github.io/UDOLO. 1.

Introduction
Humans start to develop the spatial working memory in an early age [38, 50], resulting in the awareness of the spatial object arrangement of their surroundings as part of the mental “World Model” [20]. With this memory serving as prior knowledge of 3D object locations, together with an estimation of the ego-motion of the eyes, we would anticipate objects to appear in certain regions in the ﬁeld of view when we look around. This ability enables humans to continuously locate, track and recognize objects in the 3D space, even under severe occlusion or truncation.
However, 3D object detection in most of the existing
∗The ﬁrst two authors contributed equally. The authors are afﬁliated with the State Key Lab of CAD&CG and ZJU-SenseTime Joint Lab of 3D
Vision. †Corresponding author: Xiaowei Zhou.
Figure 1. The spatial-temporal memory in UDOLO is repre-sented by the object occupancy map and the object future states.
The system is able to feedback this memory into the detection pipeline as strong prior to produce better object detection results.
Red arrows represent the information ﬂow of the feedback. tracking-by-detection systems still processes each input im-age from a video stream individually and searches for objects in every new frame from scratch. Then, object tracking is usually performed as a post-processing step to associate the detected bounding boxes with the previously observed track-lets, followed by a recursive ﬁlter or optimization to improve the accuracy and temporal stability of the estimated object states. Although this late integration of temporal information would improve the results, the object detection module is still performed on a per-frame basis without using the mem-ory of the objects in the surrounding scene, which is not only counter-intuitive but also very inefﬁcient.
We argue that the key to making full use of temporal infor-mation is not only to track objects and smooth object states, but more importantly to feed the temporally-accumulated memory of object states back to the detection module, yield-ing an integrated detection and tracking system. To this end, we propose a novel system named UDOLO that en-ables the object detector to take the spatial-temporal memory as a strong prior for more efﬁcient and accurate 3D object detection and tracking, as illustrated in Fig. 1. 1
Speciﬁcally, UDOLO simultaneously detects and tracks 3D objects in a point cloud sequence, either obtained di-rectly from depth sensors or estimated with multiple views of images. The core idea of UDOLO is to maintain a dy-namic object occupancy map (OOM) and object future state predictions as spatial-temporal memory to assist object de-tection in future frames. The OOM is a 2D Bird’s Eye View (BEV) map which shows the likelihood of a location being occupied by an object in the world frame. The OOM is constructed by registering the 3D observations in previous frames to the world frame, given the camera poses from the back-end odometry, and fusing the occupancy states at each location. Next, the UDOLO system integrates the spatial-temporal memory into the modern two-stage object detection pipeline [40, 47] at two different levels: early integration and middle integration. With the early integration, the Region
Proposal Network (RPN) in the front-end object detector extracts object proposals only in the regions that have high object occupancy scores given by the OOM and unobserved regions where new objects may appear. The early integration design essentially reduces the search space during the region proposal stage and saves the effort on evaluating tens of thou-sands of densely arranged predeﬁned anchors in the standard 3D object detector [40, 47], thus leading to more efﬁcient detection. With the middle integration, current-frame ob-ject proposals are fused with back-end object future state predictions by combining the RoI point clouds and passing them through the second stage of the detector to produce the front-end bounding box estimation. The middle integration design enables the detector to leverage the optimized and predicted object future states from back-end, in order to pro-duce more accurate bounding boxes and handle truncation and occlusion.
We evaluate our system on the ScanNet and KITTI datasets and provide ablation analyses on different com-ponents of the system. The experiments show that, with the spatial-temporal memory fed back into the object detection pipeline, 3D object detection performance can be largely im-proved compared to single-frame detection-only baselines in both indoor and outdoor scenes. For the dynamic scenes on
KITTI, 3D multiple object tracking (MOT) is also improved by a large margin. The system is also capable of detecting oc-cluded or truncated objects in cluttered indoor environments and produces more stable object bounding boxes.
In summary, our contributions are as follows:
• A novel framework of integrated detection and tracking that feeds the spatial-temporal memory of objects all the way through the detection pipeline to improve both efﬁ-ciency and accuracy, which has not been explored in the literature to our knowledge.
• An early integration scheme based on a new represen-tation named object occupancy map (OOM) to generate high-quality object proposals and speed up detection and tracking.
• A middle integration design to fuse object state predictions from previous frames with estimations at the current frame to achieve better detection and tracking performance and truncation or occlusion handling. 2.