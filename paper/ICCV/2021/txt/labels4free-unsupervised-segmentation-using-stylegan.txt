Abstract
We propose an unsupervised segmentation framework for StyleGAN generated objects. We build on two main ob-servations. First, the features generated by StyleGAN hold valuable information that can be utilized towards training segmentation networks. Second, the foreground and back-ground can often be treated to be largely independent and be swapped across images to produce plausible composited images. For our solution, we propose to augment the Style-GAN2 generator architecture with a segmentation branch and to split the generator into a foreground and background network. This enables us to generate soft segmentation masks for the foreground object in an unsupervised fashion.
On multiple object classes, we report comparable results against state-of-the-art supervised segmentation networks, while against the best unsupervised segmentation approach we demonstrate a clear improvement, both in qualitative and quantitative metrics.
Project Page : https:/rameenabdal.github.io/Labels4Free 1.

Introduction
Given the high quality and photo-realistic results of current generative adversarial networks (GANs), we are witnessing their widespread adaptation for many applica-tions. Examples include various image and video editing tasks, image inpainting [13, 47, 46, 41], local image edit-ing [42, 2], low bit-rate video conferencing [40], image su-per resolution [28, 12], image colorization [4, 27], and ex-tracting 3D models [30].
While originally conjectured that GANs are merely great at memorizing the training data, recent work in GAN-based image editing [18, 3, 36, 42] demonstrates that GANs learn non-trivial semantic information about a class of objects, e.g., faces or cars. For example, GANs are able to learn the concept of pose and they can show the same, or at least a very similar looking, object with different orientations.
Even though the background changes in subtle ways dur-ing this editing operations, in this paper we explore to what extent the underlying generator network actually learns the distinction between foreground and background, and then
Figure 1: We propose an unsupervised segmentation frame-work that enables foreground/background separation for raw input images. At the core of our framework is an un-supervised network, which segments class-speciﬁc Style-GAN images, and is used to generate segmentation masks for training supervised segmentation networks. to encourage it to disentangle foreground and background, without explicit mask-level supervision. As an important byproduct, we can extract information from an unsuper-vised GAN that is useful for general object segmentation.
For example, we are able to create a large synthetic dataset for training a state-of-the-art segmentation network and then segment arbitrary face (or horse, car, cat) images into foreground and background without requiring any manually assigned labels (see Fig. 1).
Our implementation is based on StyleGAN [24, 23], gen-erally considered the state-of-the-art for GANs trained on individual object classes. Our solution is built on two ideas.
First, based on our analysis of GAN-based image editing, the features generated by StyleGAN hold a lot of informa-tion useful for segmentation, and can be used towards corre-sponding mask synthesis. Second, the foreground and back-ground should be largely independent and be composited in different ways. The exact coupling between foreground and background is highly non-trivial however and there are mul-tiple ways of decoupling foreground and background that we analyze in our work.
For our solution, we propose to augment the StyleGAN generator architecture with a segmentation branch and to
split the generator into a foreground and background net-work. This enables us to generate soft segmentation masks for the foreground object. In order to facilitate easier train-ing, we propose a training strategy that starts from a fully trained network that only has a single generator and utilizes it towards unsupervised segmentation mask generation.
To summarize, our main contributions are:
• A novel architecture modiﬁcation, a loss function, and a training strategy to split StyleGAN into a foreground and background network.
• Generating synthetic datasets for segmentation. Our framework can be used to create a complete dataset of labeled GAN generated images of high quality in an unsupervised manner. This dataset can then be used to train other state-of-the-art segmentation networks to yield compelling results. 2.