Abstract
Attribute-based person search is the task of ﬁnding per-son images that are best matched with a set of text attributes given as query. The main challenge of this task is the large modality gap between attributes and images. To reduce the gap, we present a new loss for learning cross-modal embed-dings in the context of attribute-based person search. We re-gard a set of attributes as a category of people sharing the same traits. In a joint embedding space of the two modal-ities, our loss pulls images close to their person categories for modality alignment. More importantly, it pushes apart a pair of person categories by a margin determined adap-tively by their semantic distance, where the distance metric is learned end-to-end so that the loss considers importance of each attribute when relating person categories. Our loss guided by the adaptive semantic margin leads to more dis-criminative and semantically well-arranged distributions of person images. As a consequence, it enables a simple em-bedding model to achieve state-of-the-art records on public benchmarks without bells and whistles. 1.

Introduction
Person search is the task of ﬁnding people from a large set of images given a query describing their appearances.
It plays critical roles in applications for public safety such as searching for criminals in videos and tracking people using multiple surveillance cameras with non-overlapping
ﬁelds of view. Person search has been formulated as a ﬁne-grained image retrieval problem focusing only on person images, where a solution should discriminate subtle appear-ance variations of different people and at the same time gen-eralize well to people unseen during training.
Most of existing person search techniques need an im-age that exempliﬁes target person as query [3, 4, 5, 17, 18, 20, 24, 27, 30, 33, 38, 48, 50, 52]. However, image query is not always accessible in real world scenarios, e.g., where
*Equal contribution eyewitness memory is the only evidence for ﬁnding crimi-nals. A solution to this issue is to utilize a verbal description as query for person search [22, 23], but it suffers from the inherent ambiguity of natural language and requires com-plicated processes to understand the query.
To address the above issue, we study in this paper per-son search using text attributes as query. Speciﬁcally, a query is given as a set of predeﬁned attributes indicating traits of target person, e.g., gender, age, clothing, and ac-cessory; we consider such a set as a person category, and multiple people sharing the same traits belong to the same person category. This approach is suitable for person search in the wild since attributes are cheap to collect while being less ambiguous and more tractable than natural language descriptions. The use of attributes as query, however, intro-duces additional challenges due to the limited descriptive capability of attributes, which leads to a large modality gap between images and person categories.
Previous work on attribute-based person search attempts to reduce the modality gap by aligning each person cate-gory and corresponding images in a joint embedding space through modality-adversarial training [2, 51] or by enhanc-ing the expressive power of embedding vectors of person categories and images in a hierarchical manner [9]. Al-though these pioneer studies shed light on the important yet less explored approach to person search, there is still large room for further improvement. First, they are unstable and computationally heavy in training due to their adver-sarial learning strategies [2, 51], or expensive in inference due to the large dimensional embedding vectors demand-ing an extra network to be matched [9]. More importantly, these methods treat person categories as independent class labels of person images and ignore their relations, e.g., how many attributes are different between them, although such relations can provide a rich supervisory signal for learning better representations of person categories and images.
We develop a new attribute-based person search method that overcomes these limitations. Our method learns a joint embedding space of the two different modalities through a pair of simple encoder networks, one for images and the
other for person categories; a person category is represented as a binary vector, each of whose dimensions indicates the presence of the corresponding attribute. When conduct-ing person search, a person category is given as query in the form of binary vector and projected onto the joint em-bedding space by the person category encoder, then images whose embedding vectors are closest to that of the query in the space are retrieved.
The main contribution of this work is a new loss func-tion, which enables our model to achieve outstanding per-formance with the simple architecture and retrieval pipeline.
In the joint embedding space, the loss regards each person category as a semantic prototype of associated images, and encourages the images to be close to their prototype so that the two modalities are aligned. The key feature of the loss is that it determines the margin between person categories in the embedding space adaptively by their distance in the bi-nary attribute space. Moreover, the distance is measured by weighted Hamming metric, in which weights multiplied to individual bits (i.e., attributes) are optimized together with parameters of the embedding networks so that the loss fo-cuses on more important attributes when relating person categories. This idea is implemented by Adaptive Semantic
Margin Regularizer (ASMR) as a part of our loss.
The proposed loss function with ASMR allows the distri-butions of person images to be more discriminative and se-mantically well-arranged in the learned embedding space.
Consequently, our method achieves the state of the art on three public benchmark datasets [8, 25, 29] without bells and whistles. Also, compared to the previous work [2, 9, 51], it is efﬁcient since it works on an embedding space of a small dimension with no extra network, and converges very quickly in training since it does not require adversarial training. The main contribution of our work is three-fold:
• We propose a novel cross-modal embedding loss, con-sidering semantic relations between person categories so that the embedding space becomes more discriminative and better generalizes to unseen categories.
• The straightforward architecture and retrieval pipeline of the proposed framework enable fast convergence in training and efﬁcient person search in testing.
• Our method achieves the state of the art on three public benchmarks without bells and whistles. 2.