Abstract
Synthetic data is emerging as a promising solution to the scalability issue of supervised deep learning, espe-cially when real data are diﬃcult to acquire or hard to annotate. Synthetic data generation, however, can itself be prohibitively expensive when domain experts have to manually and painstakingly oversee the process. More-over, neural networks trained on synthetic data often do not perform well on real data because of the domain gap. To solve these challenges, we propose Sim2SG, a self-supervised automatic scene generation technique for matching the distribution of real data. Importantly,
Sim2SG does not require supervision from the real-world dataset, thus making it applicable in situations for which such annotations are diﬃcult to obtain. Sim2SG is de-signed to bridge both the content and appearance gaps, by matching the content of real data, and by matching the features in the source and target domains. We select scene graph (SG) generation as the downstream task, due to the limited availability of labeled datasets. Ex-periments demonstrate signiﬁcant improvements over leading baselines in reducing the domain gap both quali-tatively and quantitatively, on several synthetic datasets as well as the real-world KITTI dataset. 1.

Introduction
Synthetic data, for which annotations can be auto-matically generated, is a promising solution to overcome the well-known supervised learning bottleneck of label-ing real data. For this approach to succeed, such syn-thetic data must look like real data, in both appearance and content. Diﬀerences in appearance and content together comprise the so-called “domain gap” between synthetic and real data [55, 27]. Appearance refers to aspects like color, texture, shape, and lighting, whereas
Correspondence to aayush382.iitkgp@gmail.com, sdeb-nath@nvidia.com, sbirchﬁeld@nvidia.com, marcl@nvidia.com
Figure 1. We present Sim2SG, a self-supervised real-to-sim scene generation technique that matches the distribution of real data, for the purpose of training a network to infer scene graphs. Sim2SG does not require costly supervision from the real-world dataset. (Only one tree is shown in the graph to avoid clutter.) content refers to the number, position, and orientation of objects in the scene, as well as their relationships to one another.
At one extreme, synthetic scenes can be created manually by domain experts to reduce these gaps, but such solutions are expensive and therefore do not scale well [61, 47, 45, 18]. At the opposite extreme, domain randomization intentionally leverages these gaps to facil-itate sim-to-real transfer [54, 41, 56]; these approaches, however, fail to capture the complexity and distribu-tion of real scenes, thus fundamentally limiting their performance.
Recent approaches such as Meta-Sim [27], Meta-Sim2 [14], and SceneGen [52] attempt to reduce the content gap by automatically learning to generate syn-thetic data that matches the distribution of real data.
However, Meta-Sim can only learn the position and orientation of objects, and therefore cannot align the
structure of the synthetic scenes (e.g., number and types of objects in a scene) to real data. Similarly, Meta-Sim2 only learns the distribution of one type of object (cars), and thus cannot match the surrounding context. Both
Meta-Sim and Meta-Sim2 rely on a complex, realistic simulator designed on a set of hand-crafted heuristic rules to ensure that data are generated properly. Scene-Gen addresses these limitations but requires access to a large amount of labeled real data, which undermines the underlying purpose of synthetic data. None of these approaches addresses the appearance gap.
In this paper, we address both the content and ap-pearance gaps, and we do so in a self-supervised manner that requires no real-world labels. Given an unlabeled real dataset, our method aims to automatically generate synthetic data that matches the distribution of the real data. See Figure 1. We propose Sim2SG (Simulation to
Scene Graph), a synthesis-by-analysis framework, that generates scenes via self-learning [72] loop comprising of two alternating stages: synthesis and analysis. During the synthesis stage we leverage a synthetic data genera-tor to create scenes. By ensuring that the number of objects, as well as their type and placement, are similar, the synthesized data resembles the real data, and there-fore the content gap is reduced. During the analysis stage, we use the generated synthetic data for training.
To further reduce both the content and appearance gaps, the corresponding latent and output distributions are aligned via Gradient Reversal Layers [19].
We show the eﬀectiveness of our method in the scene graph generation task [11, 65, 66]. A scene graph (SG) summarizes entities in the scene and plausible relation-ships among them. The diﬃculty of hand-labeling scene graphs has limited the community to a small number of datasets [36, 29]. We experimentally demonstrate our method in three distinct environments: synthetic
CLEVR [25], synthetic Dining-Sim, and real KITTI [20].
We nearly close the domain gap in the CLEVR environ-ment and show signiﬁcant improvements over respective baselines in Dining-Sim and KITTI. Through ablations, we validate our contributions regarding appearance and content gaps.
Contributions: Our contributions are three-fold: (1) To the best of our knowledge, we are ﬁrst to do self-supervised aligned scene generation. (2) We propose a novel synthesis-by-analysis framework that addresses both the content and appearance gaps without using any real labels. (3) Experimentally, we show that Sim2SG obtains signiﬁcant improvements on downstream tasks over baselines in all three scenarios: CLEVR, Dining-Sim, and KITTI. We also present ablations to illustrate the eﬀectiveness of our technique. 2.