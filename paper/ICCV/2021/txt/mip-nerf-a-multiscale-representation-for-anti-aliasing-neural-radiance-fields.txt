Abstract
The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene con-tent at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call “mip-NeRF” (`a la “mipmap”), ex-tends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF’s ability to repre-sent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces aver-age error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22× faster. 1.

Introduction
Neural volumetric representations such as neural radi-ance fields (NeRF) [30] have emerged as a compelling strat-egy for learning to represent 3D objects and scenes from im-ages for the purpose of rendering photorealistic novel views.
Although NeRF and its variants have demonstrated impres-sive results across a range of view synthesis tasks, NeRF’s rendering model is flawed in a manner that can cause ex-cessive blurring and aliasing. NeRF replaces traditional dis-crete sampled geometry with a continuous volumetric func-tion, parameterized as a multilayer perceptron (MLP) that maps from an input 5D coordinate (3D position and 2D viewing direction) to properties of the scene (volume den-sity and view-dependent emitted radiance) at that location.
To render a pixel’s color, NeRF casts a single ray through that pixel and out into its volumetric representation, queries
Figure 1: NeRF (a) samples points x along rays that are traced from the camera center of projection through each pixel, then encodes those points with a positional encoding (PE) γ to produce a feature γ(x). Mip-NeRF (b) instead reasons about the 3D conical frustum defined by a camera pixel. These conical frustums are then featurized with our integrated positional encoding (IPE), which works by ap-proximating the frustum with a multivariate Gaussian and then computing the (closed form) integral E[γ(x)] over the positional encodings of the coordinates within the Gaussian. the MLP for scene properties at samples along that ray, and composites these values into a single color.
While this approach works well when all training and testing images observe scene content from a roughly con-stant distance (as done in NeRF and most follow-ups),
NeRF renderings exhibit significant artifacts in less con-trived scenarios. When the training images observe scene content at multiple resolutions, renderings from the recov-ered NeRF appear excessively blurred in close-up views and contain aliasing artifacts in distant views. A straightfor-ward solution is to adopt the strategy used in offline raytrac-ing: supersampling each pixel by marching multiple rays through its footprint. But this is prohibitively expensive for neural volumetric representations such as NeRF, which re-quire hundreds of MLP evaluations to render a single ray and several hours to reconstruct a single scene.
In this paper, we take inspiration from the mipmapping approach used to prevent aliasing in computer graphics ren-dering pipelines. A mipmap represents a signal (typically an image or a texture map) at a set of different discrete downsampling scales and selects the appropriate scale to use for a ray based on the projection of the pixel footprint
onto the geometry intersected by that ray. This strategy is known as pre-filtering, since the computational burden of anti-aliasing is shifted from render time (as in the brute force supersampling solution) to a precomputation phase— the mipmap need only be created once for a given texture, regardless of how many times that texture is rendered.
Our solution, which we call mip-NeRF (multum in parvo
NeRF, as in “mipmap”), extends NeRF to simultaneously represent the prefiltered radiance field for a continuous space of scales. The input to mip-NeRF is a 3D Gaus-sian that represents the region over which the radiance field should be integrated. As illustrated in Figure 1, we can then render a prefiltered pixel by querying mip-NeRF at intervals along a cone, using Gaussians that approximate the conical frustums corresponding to the pixel. To encode a 3D po-sition and its surrounding Gaussian region, we propose a new feature representation: an integrated positional encod-ing (IPE). This is a generalization of NeRF’s positional en-coding (PE) that allows a region of space to be compactly featurized, as opposed to a single point in space.
Mip-NeRF substantially improves upon the accuracy of
NeRF, and this benefit is even greater in situations where scene content is observed at different resolutions (i.e. se-tups where the camera moves closer and farther from the scene). On a challenging multiresolution benchmark we present, mip-NeRF is able to reduce error rates relative to
NeRF by 60% on average (see Figure 2 for visualisations).
Mip-NeRF’s scale-aware structure also allows us to merge the separate “coarse” and “fine” MLPs used by NeRF for hierarchical sampling [30] into a single MLP. As a conse-quence, mip-NeRF is slightly faster than NeRF (∼ 7%), and has half as many parameters. 2.