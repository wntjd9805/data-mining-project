Abstract
Controllable scene synthesis consists of generating 3D information that satisfy underlying speciﬁcations. Thereby, these speciﬁcations should be abstract, i.e. allowing easy user interaction, whilst providing enough interface for de-tailed control. Scene graphs are representations of a scene, composed of objects (nodes) and inter-object relationships (edges), proven to be particularly suited for this task, as they allow for semantic control on the generated content.
Previous works tackling this task often rely on synthetic data, and retrieve object meshes, which naturally limits the generation capabilities. To circumvent this issue, we in-stead propose the ﬁrst work that directly generates shapes from a scene graph in an end-to-end manner. In addition, we show that the same model supports scene modiﬁcation, using the respective scene graph as interface. Leveraging
Graph Convolutional Networks (GCN) we train a varia-tional Auto-Encoder on top of the object and edge cate-gories, as well as 3D shapes and scene layouts, allowing latter sampling of new scenes and shapes. 1.

Introduction
Scene content generation, including 3D object shapes, images and 3D scenes is of high interest in computer vision.
Applications involve helping the work of designers through automatically generated intermediate results, as well as un-derstanding and modeling scenes, in terms of, e.g., object constellations an co-occurrences. Furthermore, conditional synthesis allows for a more controllable content generation, since users can specify which image or 3D model they want to let appear in the generated scene. Common conditions in-volve text descriptions [39], semantic maps [34] and scene graphs. Thereby, scene graphs have recently shown to offer a suitable interface for controllable synthesis and manipula-tion [11, 4, 20], enabling semantic control on the generated scene, even for complex scenes. Compared to dense seman-tic maps, scene graph structures are more high-level and ex-∗The ﬁrst two authors contributed equally to this work
Figure 1. a) Scene generation: given a scene graph (top, solid lines), Graph-to-3D generates a 3D scene consistent with it. b)
Scene manipulation: given a 3D scene and an edited graph (top, solid+dotted lines), Graph-to-3D is able to generate a varied set of 3D scenes adjusted according to the graph manipulation. plicit, simplifying the interaction with the user. Moreover, they enable controlling the semantic relation between enti-ties, which is often not captured in a semantic map.
While there are a lot of methods for scene graph in-ference from images [36, 23] as well as the reverse prob-lem [11, 2], in the 3D domain, only a few works on scene graph prediction from 3D data have been very recently pre-sented [31, 35]. With this work, we thus attempt to ﬁll this gap by proposing a method for end-to-end generation of 3D scenes from scene graphs. A few recent works inves-tigate the problem of scene layout generation from scene graphs [32, 20], thereby predicting a set of top-view ob-ject occupancy regions or 3D bounding boxes. To construct a 3D scene from this layout, these methods typically rely on retrieval from a database. On the contrary, we employ a fully generative model that is able to synthesize novel context-aware 3D shapes for the scene. Though retrieval leads to good quality results, shape generation is an emerg-ing alternative as it allows further costumizability via inter-polation at the object level [8] and part level [22]. Further, retrieval works can achieve at best (sub-) linear complexity
for time and space w.r.t. database size. Our method essen-tially predicts object-level 3D bounding boxes together with appropriate 3D shapes, which are then combined to create a full 3D scene (Figure 1, left). Leveraging Graph Con-volutional Networks (GCNs) we learn a variational Auto-Encoder on top of scene graphs, 3D shapes and scene lay-outs, enabling latter sampling of novel scenes. Addition-ally, we employ a graph manipulation network to enable changes, such as adding new objects as well as changing object relationships, while maintaining the rest of the scene (Figure 1, right). To model the one-to-many problem of label to object, we introduce a novel relationship discrimi-nator on 3D bounding boxes that does not limit the space of valid outputs to the annotated box.
To avoid inducing any human bias, we want to learn 3D scene prediction from real data. However, these real datasets, such as 3RScan typically present additional limi-tations, such as information holes and, oftentimes, lack of annotations for the canonical object pose. We overcome the former limitation by reﬁning the ground truth 3D boxes based on the semantic relationships from 3DSSG [31]. For the latter, we extract oriented 3D bounding boxes and an-notate the front side of each object, using a combination of class-level rules and manual annotations. We release these annotations as well as the source code on our project page1.
Our contributions can be summarized as: i) We propose the ﬁrst fully learned method for generating a 3D scene from a scene graph. Therefore, we use a novel model for shared layout and shape generation. ii) We also adopt this generative model to simultaneously allow for scene manip-ulation. iii) We introduce a relationship discriminator loss which is better suited than reconstruction losses due to the one-to-many problem of box inference from class labels. iv)
We label 3RScan with canonical object poses.
We evaluate our proposed method on 3DSSG [31], a large-scale real 3D dataset based on 3RScan [30] that con-tains semantic scene graphs. Thereby, we evaluate on com-mon aspects of scene generation and manipulation, such as quality, diversity and fulﬁllment of relational constrains, showing compelling results, as well as an advantage of shar-ing layout and shape features for both tasks. 2.