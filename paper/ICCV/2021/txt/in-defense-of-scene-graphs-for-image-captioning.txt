Abstract
The mainstream image captioning models rely on Con-volutional Neural Network (CNN) image features to gener-ate captions via recurrent models. Recently, image scene graphs have been used to augment captioning models so as to leverage their structural semantics, such as object entities, relationships and attributes. Several studies have noted that the naive use of scene graphs from a black-box scene graph generator harms image captioning per-formance and that scene graph-based captioning models have to incur the overhead of explicit use of image fea-tures to generate decent captions. Addressing these chal-lenges, we propose SG2Caps, a framework that utilizes only the scene graph labels for competitive image caption-ing performance. The basic idea is to close the seman-tic gap between the two scene graphs - one derived from the input image and the other from its caption.
In order to achieve this, we leverage the spatial location of objects and the Human-Object-Interaction (HOI) labels as an ad-ditional HOI graph. SG2Caps outperforms existing scene graph-only captioning models by a large margin, indicat-ing scene graphs as a promising representation for im-age captioning. Direct utilization of scene graph labels avoids expensive graph convolutions over high-dimensional
CNN features resulting in 49% fewer trainable parame-ters. Our code is available at: https://github.com/
Kien085/SG2Caps 1.

Introduction
The mainstream image captioning models rely on con-volutional image features and/or attention to salient re-gions and objects to generate captions via recurrent mod-els [20, 1]. Recently, scene graph representations of im-ages have been used to augment captioning models so as to
*Authors have equal contributions leverage their structural semantics, such as object entities, relationships and attributes [32, 30, 6]. The literature how-ever has mixed opinion about the usefulness of scene graphs in captioning. Few works have reported improvements in caption generation using scene graphs [21, 30], while sev-eral others have highlighted that scene graphs alone yield poor captioning results and can even harm captioning per-formance [10, 14]. In this paper, we identify the challenges in effective utilization of scene graphs for image captioning, and subsequently investigate how to best harness them for this task.
Scene graph representation consisting of nodes and edges can be derived from either (i) images where the nodes correspond to the objects present in the scene, termed as
Visual Scene Graphs (VSG), or (ii) from a caption where nouns and verbs take on the roles of nodes and edges in termed as Textual Scene a rule-based semantic parsing,
Graphs (TSG). The literature of scene graph generation and scene graphs for image captioning primarily refers to the
VSG representation.
To be able to leverage scene graphs for captioning, we need paired VSG-caption annotations. This is currently un-available. Hence, methods requiring explicit scene graphs end up training the VSG generator and the caption generator on disparate datasets [30, 6, 14]. The current practice is to train VSG generators on the Visual Genome (VG) dataset, train TSG to caption generation on COCO-captions dataset, and finally transform the VG-trained VSGs to captions uti-lizing the later. We note two issues with this approach:
• The VG-trained VSGs are highly biased towards cer-tain types of relationships (e.g., has, on); the relationship distribution is significantly long-tailed, and even the top-performing VSG generators fail to learn meaningful rela-tionships accurately [26]. This results in noisy VSGs, which in turn degrades the quality of captions [14].
• There is an assumption in the existing approaches that
TSGs and VSGs are compatible. But, are VSGs and TSGs actually compatible? TSGs, when used as inputs, can gen-1
Figure 1. SG2Caps first creates Visual Scene Graphs (VSG) by combining (1) pseudolabel - output of a black-box VSG generator, and (2) HOI graph from an HOI inference model. Each object node of the VSG has a bounding box label. Object nodes, relations, attributes are color-coded in red, blue, green respectively. The output of VSG encoding is the input for the LSTM-based decoder for the caption generation. translate automatically from TSG models to VSG models.
We argue that this is the main reason why previous efforts to exploit VSGs for captioning did not achieve desired results.
To mitigate the above issues, we explore several novel ways to enhance VSGs in the context of captioning: (i) Human-Object Interaction (HOI) information: Hu-mans tend to describe visual scenes involving humans by focusing on the human-object interactions at the exclusion of other details. If HOI information is extracted from an im-age, it can provide an effective way to highlight the ‘salient’ parts in its VSG, thereby bringing it closer to its correspond-ing TSG. Hence, we propose to harness pre-trained HOI in-ferences as partial VSGs, where all detected objects (not limited to humans) in a scene form the graph nodes and the
HOI information augment a few relevant nodes with appro-priate relationship and attributes. (ii) VSG grounding: A unique aspect of VSG is that each of the node in an VSG is grounded, i.e., has a one-to-one as-sociation with the object bounding boxes in the image. This spatial information can be used to capture the relationship between objects. It is well known from the scene graph gen-eration literature that the inter-object relationship classifica-tion performance greatly benefits from ground-truth bound-ing box locations [18, 17]. Despite this evidence, no VSG-based captioning model has yet used the spatial information of the nodes. We show that such information can signifi-cantly improve captioning performance.
This paper investigates how to best leverage VSGs for caption generation, if at all. To this end, we develop a new image captioning model, termed SG2Caps, that utilizes the
VSGs alone for caption generation (see Fig. 1 for the main idea). In contrast to the existing work, we do not use any image or object-level visual features; yet, we achieve com-petitive caption generation performance by exploiting the (a) An image and the TSG generated from its caption (b) VSG containing all detected objects as nodes
Figure 2. Characterization of TSG and VSG. While TSG only contains salient contents such as man, motorcycle, flag for natu-ral language description, VSG includes unnecessary details such as wheel, tire, window, sign, pole. Objects, attributes, edges are shown in pink,green,blue respectively. (Best viewed in color) erate excellent captions [30]. However, the problem arises when parameters trained for TSGs are used for VSG inputs, assuming direct compatibility. TSGs, being generated from captions, do not include every object seen in the image or all their pairwise relationships - the very information VSGs are designed to extract (see Fig. 2). In other words, VSGs are exhaustive while TSGs focus only on the salient objects and relationships. Thus natural language inductive bias does not
HOI information and VSG grounding. Directly utilizing the scene graph labels avoids expensive graph convolutions over high-dimensional CNN features, we show that it is still effective for caption generation via capturing visual rela-tionships. This also results in 49% reduction in the number of trainable parameters comparing with the methods that re-quire processing of both visual features and scene graphs.
Researchers have shown that image captioning algorithms produce less accurate results due to inherent dataset biases and the unavailability of high quality human annotations.
[2]. Our SG2Caps model leverages additional annotations from other sources beyond image caption datasets thereby reducing NLP bias.
Our contributions are summarized below.
• We show that competitive captioning performance can be achieved on the COCO-captions dataset using VSG alone, without any visual features.
• We experimentally show that VSGs and TSGs are not compatible with each other in the context of caption gen-eration, and we propose to improve the learnable trans-formation directly from VSG to caption.
• We propose a new captioning model, SG2Caps, that uti-lizes spatial locations of VSG nodes and HOI informa-tion to generate better caption from VSGs. While node locations help to identify the meaningful relationships among objects (results in 5 point gain in CIDEr score),
HOI captures the essence of natural language communi-cation (results in 7 point gain in CIDEr score). Thus they help to close semantic gap between TSGs and VSGs for the purpose of image captioning.
• We also devise an extremely light-weight visual feature fusion strategy for the SG2Caps framework. The low-dimensional global visual features added as a summary node in the VSG, boosts image captioning performance atop blackbox scene-graph only model. Our SG2Caps model performs competitively against existing caption-ing models that operate on high-dimensional region level visual features. 2.