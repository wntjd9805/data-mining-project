Abstract
Generative Adversarial Networks (GANs) produce im-pressive results on unconditional image generation when powered with large-scale image datasets. Yet generated im-ages are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we pro-pose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Fr´echet Inception Distance (FID) by at least 17.5% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5% in FID). 1.

Introduction
Photorealistic image generation has increasingly become reality, benefiting from the invention of generative ad-versarial networks (GANs) [24] and its successive break-throughs [67, 3, 25, 60, 5, 41, 42, 43]. The progress is mainly driven by large-scale datasets [18, 57, 91, 38, 54, 42], architectural tuning [10, 98, 42, 43, 69], and loss de-signs [58, 3, 25, 60, 39, 101, 105, 96, 40, 106, 36]. GAN techniques have been popularized into extensive computer vision applications, including but not limited to image translation [35, 107, 108, 54, 33, 82, 64, 20, 63], post-Figure 1. The diagram of our GAN framework using three key components: self-attention in the generator, reference-attention in the discriminator, and a novel dual contrastive loss. Technical diagrams are in Fig. 2 and 4. processing [46, 71, 44, 45, 77, 62, 102], image manipula-tion [13, 14, 70, 1, 4, 80], texture synthesis [94, 53, 59], image inpainting [34, 52, 92, 93], and text-to-image genera-tion [68, 99, 100, 74].
Yet, behind the seemingly saturated performance of the state-of-the-art StyleGAN2 [43], there still persists open issues of GANs that make generated images surprisingly ob-vious to spot [95, 81, 21, 28]. Hence, it is still necessary to revisit the fundamental generation power when other concur-rent deep learning techniques keep advancing and creating space for GAN improvements.
We investigate methods to improve GANs in two dimen-sions. In the first dimension, we work on the loss function.
As the discriminator aims to model the intractable real data distribution via a workaround of real/fake binary classifi-cation, a more effective discriminator can back-propagate more meaningful signals for the generator to compete against.
However, the feature representations of discriminators are often not generalized enough to incentivize the adversari-ally evolving generator and are prone to forgetting previ-ous tasks [11] or previous data modes [72, 49]. This often leads to the generated samples with discontinued semantic structures [51, 98] or the generated distribution with mode collapse [72, 96]. To mitigate this issue, we propose to synergize generative modeling with the advancements in contrastive learning [61, 8]. In this direction, for the first time, we replace the logistic loss of StyleGAN2 with a newly designed dual contrastive loss.
In the second dimension, we revisit the architecture of both generator and discriminator networks. Specifically, many GAN-based image generators rely on convolutional layers to encode features. In such design, long-range de-pendencies across pixels (e.g., large-size semantically cor-related layouts) can only be formulated with a deep stack of convolutional layers. This, however, does not favor the stability of GAN training because of the challenge to co-ordinate multiple layers desirably. The minimax formula-tion and the alternating gradient ascent-descent in the GAN framework further exacerbate such instability. To circum-vent this issue, attention mechanisms that support long-range modeling across image regions are incorporated into GAN models [98, 5]. After that, however, StyleGAN2 claimed the state of the art with a novel architectural design with-out any attention mechanisms. Therefore, it turns not clear whether attention still improves results, which of the popular attention mechanisms [37, 85, 83, 103] improves the most, and in return of how many additional parameters. To answer these questions, we extensively study the role of attention in the current state-of-the-art generator, and during this study improve the results significantly.
In the discriminator, we again explore the role of atten-tion as shown in Fig. 1. We design a novel reference atten-tion mechanism in the discriminator where we allow two irrelevant images as the inputs at the same time: one in-put is sampled from real data as a reference, and the other input is switched between a real sample and a generated sample. The two inputs are encoded through two Siamese branches [6, 15, 73, 97] and fused by a reference-attention module. In this way, we achieve to guide real/fake classifica-tion under the attention of the real world. Contributions are summarized as follow:
• We propose a novel dual contrastive loss in adversarial training that generalizes representation to more effec-tively distinguish between real and fake, and further incentivize the image generation quality.
• We investigate variants of the attention mechanism in
GAN architecture to mitigate the local and stationary issues of convolutions.
• We design a novel reference-attention discriminator architecture that benefits limited-scale datasets.
• We redefine the state of the art by improving FID scores by at least 17.5% on several large-scale benchmark datasets. We also achieve more realistic generation on the CLEVR dataset [38] which poses different chal-lenges from the other datasets: compositional scenes with occlusions, shadows, reflections, and mirror sur-faces. It comes with 47.5% FID improvement. 2.