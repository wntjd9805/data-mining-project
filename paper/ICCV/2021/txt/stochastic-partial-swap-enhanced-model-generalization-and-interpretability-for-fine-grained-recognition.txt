Abstract
Learning mid-level representation for fine-grained recognition is easily dominated by a limited number of highly discriminative patterns, degrading its robustness and generalization capability. To this end, we propose a novel
Stochastic Partial Swap (SPS)1 scheme to address this issue.
Our method performs element-wise swapping for partial features between samples to inject noise during training.
It equips a regularization effect similar to Dropout, which promotes more neurons to represent the concepts. Fur-thermore, it also exhibits other advantages: 1) suppress-ing over-activation to some part patterns to improve feature representativeness, and 2) enriching pattern combination and simulating noisy cases to enhance classifier general-ization. We verify the effectiveness of our approach through comprehensive experiments across four network backbones and three fine-grained datasets. Moreover, we demonstrate its ability to complement high-level representations, allow-ing a simple model to achieve performance comparable to the top-performing technologies in fine-grained recog-nition, indoor scene recognition, and material recognition while improving model interpretability. 1.

Introduction
Fine-grained recognition is more challenging than gen-eral object recognition, as the discriminative differences of categories often reside in subtle parts of objects. Con-ventional methods that succeed in generic object classifica-tion, therefore, often fail to deliver gratifying results in fine-grained classification, since they mainly focus on learning high-level features and overlook subtle variations. Existing works [54, 8, 55, 9, 56, 7, 35, 55, 28, 44, 53, 37, 20] attempt to complement this capacity by exploring various tech-niques. Part-based [54, 8, 55, 9] and sampling-based [56, 7] are the most popular solutions in recent literature. The for-1https://github.com/Shaoli-Huang/SPS.
Figure 1. The left column shows the average number of regions per image contributing to a given predicted value, when using the baseline model trained on the CUB dataset. A more detailed de-scription of this experiment is given in the Experiment section.
The right column illustrates an issue of adopting deep mid-level models for fine-grained classification. In training data, the “Red-yellow patch” pattern may distinguish the “Red-winged blackbird” from most bird species. During training, the neural network tends to associate this part mostly to this label by biasing its weights to the corresponding region. The resulting mid-level model would predict whether an object is a “Red-winged blackbird” mainly based on this pattern while largely ignoring other roles of other patterns. mer primarily localize part regions by strongly-supervised detection pipelines or weakly-supervised learning frame-work, and then extract the discriminative local features as complementary to high-level features. The sampling-based approaches seek to enrich the representation learning by conducting attention sampling over the input images. Al-though these two techniques have succeeded in improving performance, they either require complex training proce-dures or intense computation in inference, limiting their ap-plicability to real-world situations.
Incorporating deep mid-level models into fine-grained recognition has demonstrated its potential in recent en-deavours [35, 22, 52, 23], due to its unique merits. First, mid-level models are easy to obtain and flexible to exploit, thanks to the hierarchical structure of deep neural networks.
Second, they also exhibit a strong capability to capture lo-Figure 2. An illustration of the difference between the Dropout techniques and SPS. Dropout or its variants mainly inject manually-designed noise into features, while SPS exploits samples as a source for noise injection. cal information and serve as a critical complement to the high-level representation approach in fine-grained recogni-tion. The works of [22, 52] showcase that coupling mid-level and high-level classification models indeed leads to enhanced performance.
Despite the promising results, prior approaches have been merely adopting off-the-shelf mid-level model in a plug-and-play fashion rather than enhancing the mid-level model per se. In this paper, we make one step forward along this line, and strive to learn better mid-level representations for fine-grained recognition. We observe that a mid-level model determines the label primarily based on a small num-ber of image regions. As shown in the left column of Fig. 1, for a baseline model on the CUB-200-2011 training dataset, on average the top 35.73 of the total 784 regions per im-age, in fact, contribute to 99% of the final prediction scores.
We speculate that, this is because some subtle object parts exhibit extremely powerful discriminability in the training set, and thus the neural network bias its weight more to-ward these few patterns. For example, as illustrated in the right column of Fig. 1, a “Red-yellow patch” pattern is very distinguishable for Red-winged blackbird. In this case, the neural network tends to learn more neurons highly respon-sive to this pattern, making it dominantly contribute to the prediction. The resulting model will be therefore dominated by a limited number of part patterns, degrading its robust-ness and generalization ability.
To this end, we propose a novel Stochastic Partial
Swap (SPS) strategy to enhance the generalization of mid-level models. The swapping-noise strategy randomly se-lects one sample feature as a noise source during training and swaps some of its feature units into the corresponding locations of another sample. Our proposed method differs from existing injection methods in exploiting sample fea-tures as a noise source (illustrated in Fig.2). This strat-egy delivers several advantages in learning mid-level rep-resentation. First, if most of the swapped-in elements are inactive neurons, our method has a similar regularization effect of DropOut, which encourages more neurons for fea-ture representation. Second, our approach helps suppress some neurons that dominate the predictions. For instance, if some neurons with dominant roles in predicting one cate-gory are swapped into one sample feature of another class, they may cause the sample to be mispredicted. In this case, the cost function will penalize these neurons for their mis-leading influence. Last but not least, this strategy comes with augmentation abilities to enhance the classifier’s ro-bustness. For example, swapping partial features between the intra-class samples will allow the classifier to see more pattern combination of the class. Also, exchanging partial neurons between the inter-class instances produces a sam-ple feature that contains noisy patterns of another category.
We extensively evaluate our method on seven datasets across three different tasks. Experiments show that our ap-proach improves the performance of the baseline by a large margin. Our learned mid-level model obtains an accuracy of 87.29% on the CUB dataset and outperforms other reg-ularization methods and even the high-level model. By in-corporating the high-level representation, our approach with simplicity and efficiency further achieves state-of-the-art or comparable performance on CUB-200-2011, Aircraft, Stan-ford Cars, Food101, MIT indoor, and GTOS datasets. 2.