Abstract
The growing number of action classes has posed a new challenge for video understanding, making Zero-Shot Ac-tion Recognition (ZSAR) a thriving direction. The ZSAR task aims to recognize target (unseen) actions without train-ing examples by leveraging semantic representations to bridge seen and unseen actions. However, due to the com-plexity and diversity of actions, it remains challenging to semantically represent action classes and transfer knowl-edge from seen data.
In this work, we propose an ER-enhanced ZSAR model inspired by an effective human mem-ory technique Elaborative Rehearsal (ER), which involves elaborating a new concept and relating it to known con-cepts. Specifically, we expand each action class as an Elab-orative Description (ED) sentence, which is more discrim-inative than a class name and less costly than manual-defined attributes. Besides directly aligning class seman-tics with videos, we incorporate objects from the video as
Elaborative Concepts (EC) to improve video semantics and generalization from seen actions to unseen actions. Our
ER-enhanced ZSAR model achieves state-of-the-art results on three existing benchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics dataset to overcome limitations of current benchmarks and first com-pare with few-shot learning baselines on this more realis-tic setting. Our codes and collected EDs are released at https://github.com/DeLightCMU/ElaborativeRehearsal. 1.

Introduction
Supervised video action recognition (AR) has made great progress in recent years, benefited from new mod-els such as 3D convolutional neural networks [10, 11, 42] and large-scale video datasets [6, 16]. These supervised models require abundant training data for each action class.
However, desired action classes are continuously increasing with the explosive growth of video applications on smart phones, surveillance cameras and drones. It is prohibitively
*This work was performed when Shizhe Chen was at Carnegie Mellon
University.
Figure 1: Attributes and word embeddings are insufficient to se-mantically represent action classes. Our Elaborative Rehearsal ap-proach defines actions by Elaborative Descriptions (EDs) and as-sociates videos with Elaborative Concepts (ECs, known concepts detected from the video), which improve video semantics and gen-eralization video-action association for ZSAR. (✩for videos, △ for seen actions, ◦ for unseen actions, and □ for ECs) expensive to collect annotated videos for each action class to fuel the training needs of existing supervised models. In order to alleviate such burden, Zero-Short Action Recogni-tion (ZSAR) [50] has become a thriving research direction, which aims at generalizing AR models to unseen actions without using any labeled training data of unseen classes.
A common approach for ZSAR is to embed videos and action classes into a joint semantic space [12, 49], so that the associations between video and seen actions can be trans-ferred to unseen actions. However, how to semantically rep-resent action classes for above associations is a challenging problem due to the complexity and diversity of actions. As shown in Figure 1(a), early works employ manual-defined attributes [29] to represent actions. Despite being a natural methodology, it is hard and expensive to define a complete set of atom attributes that generalizes to arbitrary actions.
To overcome difficulties in attribute definition, recent works adopt word embeddings of action names [50, 4] as class se-mantic representations. Though simple and effective, word embeddings can be ambiguous. Words have different mean-ings in different context and some actions might not even
be interpreted literally according to their names such as the
“dumpster diving” action in Figure 1(b), which are confus-ing to relating different action classes.
In addition to class semantic representations of actions, it has been under-explored in existing ZSAR works on how to learn powerful and generalizable video semantic repre-sentations. Only until recently, deep features [19, 41] have been used to overtake traditional hand-crafted features such as fisher vectors of improved dense trajectory descriptors
[43, 50]. One line of work [15, 21] utilizes objects recog-nized by deep image networks as video descriptors, which assumes that object recognition in image domain is prior knowledge for more advanced action recognition. The pre-dicted objects are naturally embedded in the semantic space and thus can be well generalized to recognize actions even without any video example [21]. However, the video is more than collections of objects, but contains specific re-lationships among objects. Therefore, it is insufficient to represent video contents purely using object semantics. An-other direction of works [4], instead, directly employs state-of-the-art video classification networks in ZSAR. Though powerful enough to capture spatio-temporal information in the video, they are prone to overfit on seen action classes and transfer poorly to unseen ones.
In this work, we take inspiration from a well-established human memory technique, namely Elaborative Rehearsal (ER) [3], for ZSAR. When we learn a new item such as
“dumpster diving”, we first expand the phrase into a readily comprehensible definition, and then relate the definition to known information in our long-term memory, thereby fos-tering retention of the item. In a similar manner, we pro-pose an ER-enhanced model to generalize AR models for new actions. Our approach advances ZSAR in three main aspects under the common paradigm of joint semantic space learning [12, 49]: (1) For the class semantic representa-tion of actions, we construct Elaborative Descriptions (ED) from class names to comprehensively define action classes as shown in Figure 1(c), and embed the ED leveraging prior knowledge from pre-trained language models. (2) For the video semantic representation, we propose two encoding network streams that jointly embed spatio-temporal dynam-ics and objects in videos. We use a pre-trained image object classification model [24] to generate the Elaborative Con-cepts (EC) of objects. Since it is highly likely that some common objects involved in seen and unseen classes, incor-porating EC in video semantics improves the generalization (3) To further improve generalization on unseen classes. of video semantic representations, we propose an ER ob-jective to enforce the model to rehearse video contents with additional semantic knowledge from EC. The embedding of
EC shares the same embedding function as the ED of action classes, which also implicitly makes our ZSAR model more generalizable to diverse class semantic representation. Our
ER-enhanced ZSAR model achieves state-of-the-art perfor-mance on the widely used benchmarks including Olympic
Sports [32], HMDB51 [25] and UCF101 [39] datasets.
Moreover, existing ZSAR benchmarks are relative small and contain overlapped classes with video datasets for fea-ture training. In order to benchmark ZSAR progress on a more realistic scenario, we further propose a new ZSAR evaluation protocol based on a large-scale supervised action dataset Kinetics [5, 6]. In our Kinetics ZSAR benchmark, we demonstrate the first case where ZSAR performance is comparable to a simple but strong few-shot learning base-line under clear split of seen and unseen action classes. 2.