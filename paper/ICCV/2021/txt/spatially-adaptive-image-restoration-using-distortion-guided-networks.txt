Abstract
We present a general learning-based solution for restor-ing images suffering from spatially-varying degradations.
Prior approaches are typically degradation-speciﬁc and employ the same processing across different images and different pixels within. However, we hypothesize that such spatially rigid processing is suboptimal for simultaneously restoring the degraded pixels as well as reconstructing the clean regions of the image. To overcome this lim-itation, we propose SPAIR, a network design that har-nesses distortion-localization information and dynamically adjusts computation to difﬁcult regions in the image. SPAIR comprises of two components, (1) a localization network that identiﬁes degraded pixels, and (2) a restoration net-work that exploits knowledge from the localization net-work in ﬁlter and feature domain to selectively and adap-tively restore degraded pixels. Our key idea is to exploit the non-uniformity of heavy degradations in spatial-domain and suitably embed this knowledge within distortion-guided modules performing sparse normalization, feature extrac-tion and attention. Our architecture is agnostic to physi-cal formation model and generalizes across several types of spatially-varying degradations. We demonstrate the ef-ﬁcacy of SPAIR individually on four restoration tasks- re-moval of rain-streaks, raindrops, shadows and motion blur.
Extensive qualitative and quantitative comparisons with prior art on 11 benchmark datasets demonstrate that our degradation-agnostic network design offers signiﬁcant per-formance gains over state-of-the-art degradation-speciﬁc architectures. Code available at https://github.com/human-analysis/spatially-adaptive-image-restoration. 1.

Introduction
Images are often degraded during the data acquisition process, especially under non-ideal imaging conditions.
Such degradations can be attributed to the medium and dy-namics between the camera, scene elements and the illu-mination. For instance, as shown in Fig. 1 , (1) precipi-tation leads to snow/rain streaks occupying the volume be-tween the scene and the camera, (2) presence of rain-drops
Figure 1. Visualization of degradation masks. The two rows show degraded input images and corresponding predicted masks. on the camera lens causes signiﬁcant degradation in scene visibility, (3) relative motion between the camera or scene elements results in motion blur, and (4) harsh illumination conditions can induce harsh shadows. Despite the disparate source of degradations, they share the same underlying mo-tif that affects the image quality, namely, degradation that is spatially-varying in nature. For example, raindrops and shadows degrade monolithic parts of the image depending on their size and location, motion blur varies with scene depth and degree of motion, and rain streaks effect only sparse regions whose orientation depends on the relative rain direction. Fig. 1 shows representative examples of degraded images and respective distortion-maps. It can be seen that a large number of pixels undergo little or no dis-tortion. Another observation is that the amount of distortion and its spatial distribution is different in every image.
Restoring such images is vital to improve their aesthetic quality as well as the performance of downstream tasks, viz, detection, segmentation, classiﬁcation, and tracking.
Convolutional neural networks (CNNs) currently typify the state-of-the-art for various image restoration tasks. Despite recent progress, existing approaches share several key limi-tations. Firstly, all layers in their networks are generic CNN layers, which apply the same set of spatially-invariant ﬁl-ters to every degraded image. Such layers are limited in their ability to invert degradations that are highly image-dependent and spatially-varying. Secondly, most network architectures are speciﬁcally tailored for individual degra-dation types as they are based on image formation mod-els. Thirdly, the distortion-localization information embed-ded in the labeled datasets remains unused or sub-optimally used in all existing solutions.
Static CNN based models trained to directly regress clean intensities from degraded ones, perform poorly when
input contains unaffected regions as well as severe inten-sity distortions in different spatial regions. Conceptually, a stack of ﬁxed learned ﬁlters, excelling at restoring pix-els degraded with large distortions might not be suitable for reconstructing the texture from unaffected regions. Prac-tically, we observe that such designs often yield poor re-construction performance (introduce unwanted changes or artifacts on pixels that are not degraded in the input to begin with). The image-dependent nature of spatial distribution and magnitude of distortions only exacerbates the problem faced by static CNNs.
Motivated from the understanding that a restoration net-work can beneﬁt from adapting to the degradation present in each test image, we propose a distortion-aware model to simultaneously realize the twin goals of restoration and reconstruction. Our spatially-adaptive image restoration ar-chitecture (referred to as SPAIR) is suited for any type of degradation which selectively affects parts of the image.
It comprises of two components- a distortion-localization network (N etL) and a spatially-guided restoration network (N etR). N etL gathers information from the entire image to estimate a binary mask (localizing high intensity distor-tions) which steers the processing in N etR to selectively improve only degraded regions.
The proposed N etR comprises of 3 distortion-guided blocks- spatial feature modulator (SFM), sparse convolu-tion module (SC) and a custom sparse non-local module (SNL). SFM utilizes the output mask and intermediate fea-tures from N etL to modulate the feature statistics of in-termediate features in N etR. SC and SNL improve fea-tures in the spatially-sparse degraded regions in an image-dependent manner, without affecting the features in clean regions. SNL locally restores features in distorted regions by adaptively gathering global context from all clean re-gions. Our key contributions are: – A two-stage framework to systematically exploit distortion-localization knowledge for directly addressing the challenges associated with diverse spatially-varying degradations in an interpretable manner. It achieves the twin goals of restoration and reconstruction and works across di-verse degradation-types. – Distortion-guided spatially-varying modulation of fea-tures statistics in N etR with the help of distortion-mask and features from a pretrained N etL. – Distortion-guided feature extraction with the help of SC (for local context) and a novel SNL (for global context) modules. These components facilitate spatially-varying restoration while controlling receptive ﬁeld in an image and location-adaptive manner. – We demonstrate the versatility of SPAIR by setting new state-of-the-art on 11 synthetic and real-world datasets for various spatially-varying restoration tasks (removing rain-streaks, rain-drops, shadows, and motion blur), outper-forming existing approaches designed with task-speciﬁc network-engineering. Further, we provide detailed analy-sis, qualitative results, and generalization tests. 2.