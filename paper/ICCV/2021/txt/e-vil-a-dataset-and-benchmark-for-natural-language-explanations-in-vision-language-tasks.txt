Abstract
Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can pro-vide human-friendly and comprehensive explanations. How-ever, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable vision-language tasks that establishes a uniﬁed evaluation framework and provides the ﬁrst comprehensive comparison of existing approaches that generate NLEs for VL tasks. It spans four models and three datasets and both automatic metrics and human evaluation are used to assess model-generated explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs (over 430k instances). We also propose a new model that combines UNITER [15], which learns joint embeddings of images and text, and GPT-2 [38], a pre-trained language model that is well-suited for text gen-eration. It surpasses the previous state of the art by a large margin across all datasets. Code and data are available here: https://github.com/maximek3/e-ViL. 1.

Introduction
Deep learning models achieve promising performance across a variety of tasks but are typically black box in na-ture. There are several arguments for making these models more explainable. For example, explanations are crucial in establishing trust and accountability, which is especially relevant in safety-critical applications such as healthcare or autonomous vehicles. They can also enable us to bet-ter understand and correct the learned biases of models [5].
*Corresponding Author: maxime.kayser@cs.ox.ac.uk
**Now at Universit´e Paris-Dauphine, PSL, and Facebook AI Research.
Explainability efforts in vision tasks largely focus on high-lighting relevant regions in the image, which can be achieved via tools such as saliency maps [1] or attention maps [47].
Our work focuses on natural language explanations (NLEs), which aim to explain the decision-making process of a model via generated sentences. Besides being easy to understand for lay users, NLEs can explain more complex and ﬁne-grained reasoning, which goes beyond highlighting the im-portant image regions. We compare different models that generate NLEs for vision-language (VL) tasks, i.e., tasks where the input consists of visual and textual information, such as visual question-answering (VQA).
NLEs for VL tasks (VL-NLE) is an emerging ﬁeld, and only few datasets exist. Moreover, existing datasets tend to be relatively small and unchallenging (e.g., VQA-X [37]) or noisy (e.g., VQA-E [29]). Another limitation of the VL-NLE ﬁeld is that there is currently no uniﬁed evaluation framework, i.e., there is no consensus on how to evaluate
NLEs. NLEs are difﬁcult to evaluate, as correct explanations can differ both in syntactic form and in semantic meaning.
For example, “Because she has a big smile on her face” and
“Because her team just scored a goal” can both be correct explanations for the answer “Yes” to the question “Is the girl happy?”, but existing automatic natural language generation (NLG) metrics are poor at capturing this. As such, the gold standard for assessing NLEs is human evaluation. Past work have all used different approaches for human evaluations, and therefore no objective comparison exists.
In this work, we propose ﬁve main contributions to ad-dress the lack of comparison between existing work. (1) We propose e-ViL, the ﬁrst benchmark for VL-NLE tasks. e-ViL spans across three datasets of human-written NLEs, and provides a uniﬁed evaluation framework that is designed to be re-usable for future works. (2) Using e-ViL, we com-pare four VL-NLE models. (3) We introduce e-SNLI-VE, a dataset of over 430k instances, the currently largest dataset for VL-NLE. (4) We introduce a novel model, called e-UG,
which surpasses the state of the art by a large (and signiﬁ-cant) margin across all three datasets. (5) We provide the currently largest study on the correlation between automatic
NLG metrics and human evaluation of NLEs. 2.