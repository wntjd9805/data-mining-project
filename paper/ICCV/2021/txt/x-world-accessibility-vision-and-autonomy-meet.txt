Abstract
An important issue facing vision-based intelligent sys-tems today is the lack of accessibility-aware development.
A main reason for this issue is the absence of any large-scale, standardized vision benchmarks that incorporate rel-evant tasks and scenarios related to people with disabili-ties. This lack of representation hinders even preliminary analysis with respect to underlying pose, appearance, and occlusion characteristics of diverse pedestrians. What is the impact of signiﬁcant occlusion from a wheelchair on instance segmentation quality? How can interaction with mobility aids, e.g., a long and narrow walking cane, be recognized robustly? To begin addressing such questions, we introduce X-World, an accessibility-centered develop-ment environment for vision-based autonomous systems. We tackle inherent data scarcity by leveraging a simulation en-vironment to spawn dynamic agents with various mobility aids. The simulation supports generation of ample amounts of ﬁnely annotated, multi-modal data in a safe, cheap, and
*Contributed equally. privacy-preserving manner. Our analysis highlights novel challenges introduced by our benchmark and tasks, as well as numerous opportunities for future developments. We further broaden our analysis using a complementary real-world evaluation benchmark of in-situ navigation by pedes-trians with disabilities. Our contributions provide an initial step towards widespread deployment of vision-based agents that can perceive and model the interaction needs of diverse people with disabilities. 1.

Introduction
As prototypical vision-based machines move from their controlled development labs into the real-world, their im-pact on people with disabilities becomes discernible. Peo-ple with various abilities (e.g., sighted, visually-impaired, mobility-impaired) may each react fairly differently when interacting with an autonomous platform (e.g., ground robot, autonomous vehicle, wearable system), the dynamic context of the surrounding scene (e.g., trafﬁc, crowds), and infrastructure components (e.g., intersection type, potholes, ramps, stairs) [37, 2, 28, 26]. Hence, such factors are related
to increased risk of trafﬁc fatalities among pedestrians with disabilities [36, 1, 38, 42], notably during critical navigation junctions, e.g., intersections [37].
Take, for instance, the case of Starship Technologies which paused operations of its food and package delivery robot within only a matter of days after its deployment at the University of Pittsburgh in October of 2019 [43]. The pause came as a result of an adverse encounter between one of the delivery robots and a mobility-impaired doctoral stu-dent who uses a wheelchair. In the reported scenario, the delivery robot, which routinely occupies the curb’s ramp when waiting to cross at intersections, blocked the student from being able to safely board the sidewalk. As a result, the student reportedly had to wait, in dangerously moving trafﬁc, at the intersection. Clearly, failing to account for the diversity of interaction needs among people with dis-abilities can have dire consequences. Unfortunately, there is a current lack of shared and principled development tools for accessibility-driven, vision-based autonomous systems.
How can we advance the state-of-the-art of systems that understand and seamlessly interact with all people in their environment?
A fundamental barrier to realizing accessibility-aware autonomous systems is the access to data. In particular, peo-ple with disabilities can be both highly diverse, for instance in appearance and mobility characteristics [44], as well as quite rare, even in extensive large-scale data collection ef-forts by instrumented real-world ﬂeets. Thus, common is-sues related to the “long tail” distribution of events can com-pound in our context, i.e., due to the combinatorial rarity of joint safety-critical events (e.g., person crossing an intersec-tion in low visibility at nighttime) with accessibility-related events (e.g., a wheelchair user). To emphasize, due to their rarity, people with disabilities are currently entirely absent from large-scale datasets in computer vision and robotics, even within heavily studied tasks such as pedestrian detec-tion [17, 60, 13] and path prediction [49]. Yet, the pres-ence of a mobility aid can potentially impact the underly-ing perception algorithm, e.g., due to occlusion, as well as be used to infer the intent and future state of a pedestrian, e.g., a visually-impaired pedestrian that may take longer to explore tactile cues when crossing an intersection. In this paper, we take a crucial step forward towards developing perception models that are both robust, i.e., operate at high accuracy under appearance variations of pedestrians with disabilities, and functional, i.e., provide a sufﬁciently ﬁne-grained representation of pedestrians’ states for any subse-quent decision-making modules.
We support the development of vision-based systems with comprehensive understanding of the needs of diverse people with disabilities through four main contributions: (1) We present the X-World platform, which includes an in-teractive photo and behavior-realistic simulation module in-tegrated into CARLA [18] with support for spawning agents that use diverse mobility aids, various sensor and environ-mental conﬁgurations, and extensive ground truth genera-tion for numerous visual-semantic reasoning tasks. (2) To rigorously uncover issues in perception of diverse people, we explore the task of segmenting people and their mobil-ity aids. We leverage the simulation environment to gener-ate the ﬁrst large-scale accessibility-oriented instance seg-mentation dataset. By incorporating images across varying perspectives, towns, environmental conditions, and mobil-ity aids, we use the dataset to highlight new challenges and opportunities in developing robust and broad-impact ma-chine vision models. (3) Although collecting a large-scale dataset for our task in the physical world is difﬁcult, we accompany the simulation-based benchmark with a diverse and challenging real-world dataset obtained from public in-ternet videos of in-situ navigation. The real-world dataset provides complementary analysis and produces generaliza-tion insights related to our ﬁne-grained instance segmenta-tion task. (4) By publicly releasing tools and data for closely integrating computer vision and accessibility research, we contribute towards improving the quality-of-life of individ-uals with disabilities. 2.