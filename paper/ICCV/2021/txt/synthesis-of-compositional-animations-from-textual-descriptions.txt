Abstract
“How can we animate 3D-characters from a movie script or move robots by simply telling them what we would like them to do?” “How unstructured and complex can we make a sentence and still generate plausible movements from it?” These are questions that need to be answered in the long-run, as the field is still in its infancy. Inspired by these problems, we present a new technique for gener-ating compositional actions, which handles complex input sentences. Our output is a 3D pose sequence depicting the actions in the input sentence. We propose a hierarchical two-stream sequential model to explore a finer joint-level mapping between natural language sentences and 3D pose sequences corresponding to the given motion. We learn two manifold representations of the motion, one each for the up-per body and the lower body movements. Our model can generate plausible pose sequences for short sentences de-scribing single actions as well as long complex sentences describing multiple sequential and compositional actions.
We evaluate our proposed model on the publicly avail-able KIT Motion-Language Dataset containing 3D pose data with human-annotated sentences. Experimental results show that our model advances the state-of-the-art on text-based motion synthesis in objective evaluations by a margin of 50%. Qualitative evaluations based on a user study in-dicate that our synthesized motions are perceived to be the closest to the ground-truth motion captures for both short and compositional sentences. 1.

Introduction
Manually creating realistic animation of humans per-forming complex motions is always a challenge. Motion synthesis based on textual descriptions substantially sim-*Corresponding Author: anindita.ghosh@dfki.de.
Figure 1: Overview of our proposed method to generate mo-tion from complex natural language sentences.1 plifies this task and has a wide range of applications, in-cluding language-based task planning for robotics and vir-tual assistants [3], designing instructional videos, creating public safety demonstrations [40], and visualizing movie scripts [27]. However, mapping natural language text de-scriptions to 3D pose sequences for human motions is non-trivial. The input texts may describe single actions with se-quential information, e.g., “a person walks four steps for-ward”, or may not correspond to the discrete time steps of the pose sequences to be generated, such as for com-positional actions, e.g., “a person is spinning around while walking”. This necessitates a machine-level understanding of the syntax and the semantics of the text descriptions to generate the desired motions [4]. While translating a sen-tence to a pose sequence, we need to identify the different parts of speech in the given sentence and how they impact the output motion. A verb in the sentence describes the type of action, whereas an adverb may provide information on the direction, place, frequency, and other circumstances of the denoted action. These need to be mapped into the gen-erated pose sequence in the correct order, laying out addi-tional challenges for motion modeling systems.
Existing text-to-motion mapping methods either gener-ate motions from sentences describing one action only [53] or produce sub-par motions from descriptions of composi-1Code and additional resources available at https://github.com/anindita127/Complextext2animation
tional actions [4]. They fail to translate the long-range de-pendencies and correlations in complex sentences and do not generalize well to motions outside of locomotion [4].
We propose a method to handle complex sentences, meaning sentences that describe a person performing mul-tiple actions either sequentially or simultaneously. For ex-ample, the input sentence “a person is stretching his arms, taking them down, walking forwards for four steps and rais-ing them again” describes multiple sequential actions such as raising the arms, taking down the arms, and walking, as well as the direction and number of steps for the action. To the best of our knowledge, our method is the first to syn-thesize plausible motions from such varieties of complex textual descriptions, which is an essential next step to im-prove the practical applicability of text-based motion syn-thesis systems. To achieve this goal, we propose a hierar-chical, two-stream, sequential network that synthesizes 3D pose sequences of human motions by parsing the long-range dependencies of complex sentences, while preserving the essential details of the described motions. Our output is a sequence of 3D poses corresponding to the motions de-scribed in the sentence (Fig. 1). Our main contributions in this paper are as follows:
Hierarchical joint embedding space.
In contrast to
JL2P [4], we separate our intermediate pose embeddings into two embeddings, one each for the upper body and the lower body. We further separate these embeddings hi-erarchically to limb embeddings. Our model learns the semantic variations in a sentence ascribing speed, direc-tion, frequency of motion, and maps them to temporal pose sequences by decoding the combined embeddings. This results in the synthesis of pose sequences that correlate strongly with the descriptions given in the input sentences.
Sequential two-stream network. We introduce a se-quential two-stream network with an autoencoder architec-ture, with different layers focusing on different parts of the body, and combine them hierarchically to two representa-tions for the pose in the manifold space, one for the upper body and the other for the lower body. This reduces the smoothing of upper body movements (such as wrist move-ments for playing violin) in the generated poses and makes the synthesized motion more robust.
Contextualized BERT embeddings. In contrast to pre-vious approaches [4, 53], which do not use any contextu-alized language model, we use the state-of-the-art BERT model [16] with handpicked word feature embeddings to improve text understanding. The BERT model is pre-trained on a large corpus of unlabelled text including the entire Wikipedia and the Book Corpus [73].
Additional loss terms and pose discriminator. We add a set of loss terms to the network training to better condition the learning of the velocity and the motion manifold [36].
We also add a pose discriminator with an adversarial loss to further improve the plausibility of the synthesized motions.
Experimental results show that our method outperforms the baseline methods of JL2P [4] and Lin et al. [44] sig-nificantly on both the quantitative metrics we discuss in
Sec. 4.3 and on qualitative evaluations. 2.