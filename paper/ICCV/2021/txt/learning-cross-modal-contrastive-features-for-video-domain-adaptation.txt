Abstract
Learning transferable and domain adaptive feature repre-sentations from videos is important for video-relevant tasks such as action recognition. Existing video domain adapta-tion methods mainly rely on adversarial feature alignment, which has been derived from the RGB image space. How-ever, video data is usually associated with multi-modal in-formation, e.g., RGB and optical flow, and thus it remains a challenge to design a better method that considers the cross-modal inputs under the cross-domain adaptation setting. To this end, we propose a unified framework for video domain adaptation, which simultaneously regularizes cross-modal and cross-domain feature representations. Specifically, we treat each modality in a domain as a view and leverage the contrastive learning technique with properly designed sampling strategies. As a result, our objectives regularize feature spaces, which originally lack the connection across modalities or have less alignment across domains. We con-duct experiments on domain adaptive action recognition benchmark datasets, i.e., UCF, HMDB, and EPIC-Kitchens, and demonstrate the effectiveness of our components against state-of-the-art algorithms. 1.

Introduction
Recently, domain adaptation has gained a lot of atten-tion due to its efficiency during training without the need of collecting ground truth labels in the target domain. Ex-isting methods have made significant progress in image-based tasks, such as classification [33, 14, 54, 42], seman-tic segmentation [16, 53, 56, 31, 38] and object detection
[8, 43, 24, 17]. While several works have sought to extend this success to video-based tasks like action recognition by aligning appearance (e.g., RGB) features through adversarial learning [6, 9, 37], challenges persist in video adaptation tasks due to the greater complexity of the video data. More-over, different from the image data, domain shifts in videos for action recognition often involve more complicated envi-ronments, which increases the difficulty for adaptation. For
Figure 1. We propose a cross-modal contrastive learning frame-work for video domain adaptation. Our framework consists of two contrastive learning objectives: (1) cross-modal contrastive learning to align cross-modal representations from the same video, and (2) cross-domain contrastive learning to align representations between the source and target domains in each modality. example, the “fencing” action usually happens in a stadium, but it can happen in other places such as home or outdoors.
Also, different actions can take place under the same back-ground. Therefore, purely relying on aligning RGB features can be biased to the background and affect the performance.
In addition to the appearance cue, other modalities such as motion, audio, and text are considered in (self-)supervised learning methods on the video data [46, 2, 26, 39].
In this work, we focus on appearance and motion as the two most common modalities in the cross-domain action recog-nition task, in which the motion modality (i.e., optical flow) is shown to be more domain-invariant (e.g., background changes) than RGB [36]. As a result, motion can better capture background-irrelevant information, while RGB can identify semantically meaningful information under different camera setups, e.g., camera perspective.
As shown in Figure 1, with two modalities across two domains, adaptation becomes a task of how to explore the relationship of cross-modal and cross-domain features, to fully exploit the multi-modal property for video domain 1
s , F a t , F m adaptation. That is, given either the source video Vs or the target one Vt, they can be associated to either the appear-ance feature F a or the motion feature F m, which results in s , F m four combinations of feature spaces, i.e., F a t .
Thus, the ensuing task is to design an effective adaptation mechanism for dealing with these four feature spaces. Since each modality has its characteristics and benefit (e.g., flow is more domain-invariant and RGB can capture semantic cues), it is of great interest to enable feature learning across the two modalities. Our key contribution stems from the observation that typical adversarial feature alignment schemes used in e.g. [6, 10] may not be directly applied in the cross-modal setting. For example, it is not reasonable to directly align the
RGB feature F a s in the source domain with the flow feature s or F m
F m t
To tackle this issue, motivated by the recent advancements in self-supervised multi-view learning [50] that achieves powerful feature representations, we propose to treat each modality as a view, while introducing the cross-domain video data in our multi-modal learning framework. To this end, we leverage the contrastive learning objectives for performing feature regularization mutually among those four feature spaces (see Figure 1) under the video domain adaptation setting. We note that the prior work [36] also adopts a multi-modal framework, but it focuses on typical adversarial alignment and a self-supervised objective to predict whether the RGB/flow modality comes from the same video clip, without the exploration of jointly regularizing cross-modal and cross-domain features like our work. in either domain.
More specifically, our framework is allowed to contrast features across modalities within a domain (e.g., between
F a s and F m s ) or across domains using one modality (e.g., between F a s and F a t ). Two kinds of loss functions are de-signed accordingly: 1) a cross-modal loss that considers each modality as one view in a video while contrasting views in other videos from the same domain; 2) a cross-domain loss that considers one modality at a time and contrasts features based on the (pseudo) class labels of videos across two do-mains. There are several benefits of the proposed contrastive learning-based feature regularization strategies: 1) it is a unified framework that allows the interplay across features in different modalities and domains, while still enjoying the benefits of each modality; 2) it enables sampling strate-gies of selecting multiple positive and negative samples in the loss terms, coupled with memory banks to record large variations in video clips; 3) our cross-domain loss can be considered as a soft version of pseudo-label self-training with the awareness of class labels, which performs more robustly than typical adaptation methods.
We conduct experiments in video action recognition benchmark datasets, including the UCF [47] ↔ HMDB [27] setting, and the EPIC-Kitchens [11, 36] dataset. We show that including either our cross-modal or cross-domain con-trastive learning objective improves accuracy while com-bining these two strategies in a unified framework obtains the best results. Moreover, our method performs favorably against state-of-the-art domain adaptation techniques, e.g., adversarial feature alignment [6, 36], self-learning scheme
[10], and pseudo-label self-training. The main contributions of this work are summarized as follows.
• We propose a new multi-modal framework for video domain adaptation that leverages the property in four different feature spaces across modalities and domains.
• We leverage the contrastive learning technique with well-designed sampling strategies and demonstrate the application to adaptation for cross-domain action recog-nition by exploiting appearance and flow modalities.
• We show the effectiveness of both the cross-modal and cross-domain contrastive objectives, by achieving state-of-the-art results on UCF-HMDB and EPIC-Kitchens adaptation benchmarks with extensive analysis. 2.