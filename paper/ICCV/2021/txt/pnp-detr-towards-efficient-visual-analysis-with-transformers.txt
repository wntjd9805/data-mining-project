Abstract
Recently, DETR [3] pioneered the solution of vision tasks with transformers, it directly translates the image fea-ture map into the object detection result. Though effective, translating the full feature map can be costly due to redun-In dant computation on some area like the background. this work, we encapsulate the idea of reducing spatial re-dundancy into a novel poll and pool (PnP) sampling mod-ule, with which we build an end-to-end PnP-DETR archi-tecture that adaptively allocates its computation spatially to be more efﬁcient. Concretely, the PnP module abstracts the image feature map into ﬁne foreground object feature vectors and a small number of coarse background contex-tual feature vectors. The transformer models information interaction within the ﬁne-coarse feature space and trans-lates the features into the detection result. Moreover, the
PnP-augmented model can instantly achieve various de-sired trade-offs between performance and computation with a single model by varying the sampled feature length, with-out requiring to train multiple models as existing methods.
Thus it offers greater ﬂexibility for deployment in diverse scenarios with varying computation constraint. We further validate the generalizability of the PnP module on panop-tic segmentation and the recent transformer-based image recognition model ViT [7] and show consistent efﬁciency gain. We believe our method makes a step for efﬁcient visual analysis with transformers, wherein spatial redundancy is commonly observed. Code and models will be available. 1.

Introduction
Object detection is a fundamental computer vision task aiming to recognize object instances in the image and lo-calize them with precise bounding boxes. Modern detectors address this set prediction task mainly with proxy learning
*Work done during an internship at Yitu Tech.
Figure 1. Left: Detection result. Right: Transformer computation density map. Proposed method allows the model to adaptively allocate computation spatially and avoid computation expenditure on less informative background area. objectives, i.e., regressing offset from pre-deﬁned anchor boxes [23, 18] or boundaries from grid locations [27, 34, 9].
Those heuristic designs not only complicate the model de-sign but also require hand-crafted post-processing for du-plicate removal. A recent method DETR [3] eliminates those hand-crafted designs and achieves end-to-end object detection. It builds an effective set prediction framework on top of convolution feature maps with transformers [28] and shows competitive performance to the two-stage Faster
R-CNN [23] detector. The image feature map is ﬂattened in the spatial dimension into one-dimensional feature vec-tors. The transformer then processes them with its strong attention mechanism to generate the ﬁnal detection list.
Albeit simple and effective, applying the transformer networks to a image feature map can be computationally costly, mainly due to the attention operation [28] over the long ﬂattened feature vectors. These features may be redun-dant: natural images often contain enormous background areas apart from the interested objects, which may occupy large part in the corresponding feature representation; also, some discriminative feature vectors may already sufﬁce for detecting the objects. Existing works improving the trans-former efﬁciency mainly focus on accelerating the attention operation [16, 15, 29, 5], and few of them consider the spa-tial redundancy discussed above.
To address the above limitation, we develop a learnable
poll and pool (PnP) sampling module. It aims to compress an image feature map into an abstracted feature set com-posed of ﬁne feature vectors and a small number of coarse feature vectors. The ﬁne feature vectors are deterministi-cally sampled from the input feature map to capture the ﬁne foreground information, which thus are crucial for detect-ing the objects. The coarse feature vectors aggregate in-formation from the background locations, and the result-ing contextual information helps better recognize and lo-calize the objects. A transformer then models the informa-tion interaction within the ﬁne-coarse feature space and ob-tains the ﬁnal result. As the abstracted set is much shorter than the directly ﬂattened image feature map, the trans-former computation is reduced signiﬁcantly and mainly dis-tributed over the foreground locations. Our approach is or-thogonal to the approaches improving the transformer ef-ﬁciency [16, 15, 29, 5] and can be further combined with them to obtain more efﬁcient models.
Concretely, the PnP module is composed of two core sub-modules: a poll sampler and a subsequent pool sam-pler. The poll sampler incorporates a content-aware meta-scoring network that learns to predict the infromativeness score of the feature vector at each spatial location. The fea-ture vectors are then ranked spatially with the informative-ness scores and a subset of most informative feature vectors are selected. The subsequent pool sampler dynamically pre-dicts attention weights on the non-sampled feature vectors and aggregates them into a small number of feature vec-tors that summarize the background information. Similar to the region proposal networks [23], the PnP module also aims to extract object-relevant information, but is end-to-end learned without explicit objective like object bounding box regression. We build a PnP-DETR with the PnP mod-ule, which operates on the ﬁne-coarse feature space and adaptively allocates its transformer computation in the spa-tial domain. Fig. 1 is an example detection with computa-tion density map (refer to Sec. 4.2 for details of the map construction). Existing methods of improving model efﬁ-ciency still need train multiple models of different complex-ities for achieving various trade-offs of computation and performance. Compared with them, the proposed PnP sam-pling allows the transformer to work with a variable number of input feature vectors and achieve instant computation and performance trade-off.
We conduct extensive experiments on the COCO bench-mark, and the results show PnP-DETR effectively reduces the cost and achieves dynamic computation and perfor-mance trade-off. For example, without bells and whistels, a single PnP-DETR-DC5 obtains a 42.7 AP with 72% reduc-tion of transformer computation compared to the 43.3 AP baseline and competitive 43.1 AP with 56% reduction. We further validate the efﬁciency gain with panoptic segmenta-tion and the recent vision transformer model (ViT [7]). For example, PnP-ViT achieves near half of FLOPs reduction with only 0.3 drop of accuracy. To summarize, the contri-butions are:
• We identify the spatial redundancy issue of the image feature map, which causes excessive computation of the transformer network in a DETR model. We there-fore propose to abstract the feature map, so as to sig-niﬁcantly reduce the model computation.
• To realize the feature abstraction, we design a novel two-step poll-and-pool sampling module. It ﬁrst em-ploys a poll sampler to extract the foreground ﬁne fea-ture vectors, and then utilizes a pool sampler to obtain the contextual coarse feature vectors.
• We then build PnP-DETR, wherein the transformer op-erates on the abstract ﬁne-coarse feature space and adaptively distributes the computation in the spatial domain. PnP-DETR is more efﬁcient and achieves instant computation and performance trade-off with a single model, by varying length of the ﬁne feature set.
• The PnP sampling module is general and end-to-end learned without explicit supervision like the re-gion proposal networks [23]. We further validate it on panoptic segmentation and recent ViT model [7] and show consistent efﬁciency gain. We believe our method provides useful insights for future research on efﬁcient solutions of vision tasks with transformers. 2.