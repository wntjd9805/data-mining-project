Abstract
We show for the ﬁrst time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our net-work is trained in live operation without prior data, build-ing a dense, scene-speciﬁc implicit 3D model of occupancy and colour which is also immediately used for tracking.
Achieving real-time SLAM via continual training of a neural network against a live image stream requires sig-niﬁcant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation ﬂow, with dy-namic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efﬁcient geometry representation with automatic detail control and smooth, plausible ﬁlling-in of unobserved regions such as the back surfaces of objects. 1.

Introduction
A real-time Simultaneous Localisation and Mapping (SLAM) system for an intelligent embodied device must in-crementally build a representation of the 3D world, to en-able both localisation and scene understanding. The ideal representation should precisely encode geometry, but also be efﬁcient, with the memory capacity available used adap-tively in response to scene size and complexity; predictive, able to plausibly estimate the shape of regions not directly observed; and ﬂexible, not needing a large amount of train-ing data or manual adjustment to run in a new scenario.
Implicit neural representations are a promising recent advance in off-line reconstruction, using a multilayer per-ceptron (MLP) to map a query 3D point to occupancy or colour, and optimising it from scratch to ﬁt a speciﬁc scene.
An MLP is a general implicit function approximator, able to represent variable detail with few parameters and with-out quantisation artifacts. Even without prior training, the inherent priors present in the network structure allow it to make watertight geometry estimates from partial data, and
Figure 1: Room reconstruction from real-time iMAP with an Azure Kinect RGB-D camera, showing watertight scene model, camera tracking and automatic keyframe set. plausible completion of unobserved regions.
In this paper, we show for the ﬁrst time that an MLP can be used as the only scene representation in a real-time
SLAM system using a hand-held RGB-D camera. Our randomly-initialised network is trained in live operation and we do not require any prior training data. Our iMAP system is designed with a keyframe structure and multi-processing computation ﬂow reminiscent of PTAM [11]. In a tracking process, running at over 10 Hz, we align live RGB-D obser-vations with rendered depth and colour predictions from the
MLP scene map. In parallel, a mapping process selects and maintains a set of historic keyframes whose viewpoints span the scene, and uses these to continually train and improve the MLP, while jointly optimising the keyframe poses.
In both tracking and mapping, we dynamically sample the most informative RGB-D pixels to reduce geometric uncertainty, achieving real-time speed. Our system runs in Python, and all optimisation is via a standard PyTorch framework [20] on a single desktop CPU/GPU system. 1
By casting SLAM as a continual learning problem, we achieve a representation which can represent scenes efﬁ-ciently with continuous and adaptive resolution, and with a remarkable ability to smoothly interpolate to achieve com-plete, watertight reconstruction (Fig. 1). With around 10 -20 keyframes, and an MLP with only 1 MB of parameters, we can accurately map whole rooms. Our scene represen-tation has no ﬁxed resolution; the distribution of keyframes automatically achieves efﬁcient multi-scale mapping.
We demonstrate our system on a wide variety of real-world sequences and do exhaustive evaluation and ablative analysis on 8 scenes from the room-scale Replica Dataset
[29]. We show that iMAP can make a more complete scene reconstruction than standard dense SLAM systems with signiﬁcantly smaller memory footprint. We show com-petitive tracking performance on the TUM RGB-D dataset
[30] against state-of-the-art SLAM systems.
To summarise, the key contributions of the paper are:
• The ﬁrst dense real-time SLAM system that uses an implicit neural scene representation and is capable of jointly optimising a full 3D map and camera poses.
• The ability to incrementally train an implicit scene net-work in real-time, enabled by automated keyframe se-lection and loss guided sparse active sampling.
• A parallel implementation (fully in PyTorch [20] with multi-processing) of our presented SLAM formulation which works online with a hand-held RGB-D camera. 2.