Abstract
A large body of recent work has identiﬁed transforma-tions in the latent spaces of generative adversarial networks (GANs) that consistently and interpretably transform gener-ated images. But existing techniques for identifying these transformations rely on either a ﬁxed vocabulary of pre-speciﬁed visual concepts, or on unsupervised disentangle-ment techniques whose alignment with human judgments about perceptual salience is unknown. This paper intro-duces a new method for building open-ended vocabular-ies of primitive visual concepts represented in a GAN’s la-tent space. Our approach is built from three components: (1) automatic identiﬁcation of perceptually salient direc-tions based on their layer selectivity; (2) human annota-tion of these directions with free-form, compositional natu-ral language descriptions; and (3) decomposition of these annotations into a visual concept vocabulary, consisting of distilled directions labeled with single words. Experi-ments show that concepts learned with our approach are reliable and composable—generalizing across classes, con-texts, and observers, and enabling ﬁne-grained manipula-tion of image style and content. 1.

Introduction
GANs [8] map latent vectors z to images x. Past work has found that directions in this latent space can encode spe-ciﬁc aspects of image semantics: StyleGAN trained on bed-rooms, for example, contains a direction such that moving most z in that direction causes indoor lighting to appear in the associated image [24]. However, current methods for identifying these directions are ad hoc, capturing only a limited set of human-salient dimensions of variation. In this paper, we describe how to construct more expressive and diverse sets of meaningful image transformations—a visual concept vocabulary—by decomposing freeform lan-guage descriptions of GAN transformations.
Consider trying to ﬁnd a direction that makes an outdoor market more festive (Figure 1). The GAN latent space is too large to make random search feasible, while supervised
Figure 1: Building a visual concept vocabulary. First, we gener-ate directions that preserve most of the structure and content in the image. Then we use human annotations to decompose them into directions that correspond to a single salient concept. Finally, we show the decomposed directions generalize across starting rep-resentations and input classes, and can be composed to construct compound directions. approaches cannot verify if the desired direction is present
[11, 7, 24, 19]. Unsupservised approaches [10, 15, 20, 21] may not discover a festive direction, since the model’s prin-cipal components do not necessarily capture changes that are most visually salient to humans.
To improve our understanding of the kinds of inter-pretable semantic transformations encoded in GAN latent space, we propose a new approach for building an open-ended glossary of primitive, perceptually salient directions from the bottom up. Our approach is built from three com-ponents: 1. A new procedure for generating perceptually salient directions based on layer selectivity. The resulting di-rections make meaningful local changes to a scene but are still non-atomic. 2. A data collection paradigm in which human annotators directly label directions with their semantics, which
are complex and compose multiple concepts to de-scribe visual transformations. 3. A new bag-of-directions model which automati-cally decomposes these annotations into a glossary of
“primitive” visual transformations associated with sin-gle words.
Because our method covers the breadth of the GAN la-tent space, it enables reliable image editing with a relatively open-ended vocabulary. We also show how our vocabulary supports generalization to novel compositions and transfer across classes. Code, data, and additional information are available at visualvocab.csail.mit.edu. 2.