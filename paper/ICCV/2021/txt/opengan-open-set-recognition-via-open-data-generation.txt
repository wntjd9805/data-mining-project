Abstract 1.

Introduction
Real-world machine learning systems need to analyze
In novel testing data that differs from the training data.
K-way classiﬁcation, this is crisply formulated as open-set recognition, core to which is the ability to discrimi-nate open-set data outside the K closed-set classes. Two conceptually elegant ideas for open-set discrimination are: 1) discriminatively learning an open-vs-closed binary dis-criminator by exploiting some outlier data as the open-set, and 2) unsupervised learning the closed-set data distribu-tion with a GAN and using its discriminator as the open-set likelihood function. However, the former generalizes poorly to diverse open test data due to overﬁtting to the training outliers, which unlikely exhaustively span the open-world. The latter does not work well, presumably due to the instable training of GANs. Motivated by the above, we propose OpenGAN, which addresses the limitation of each approach by combining them with several technical insights. First, we show that a carefully selected GAN-discriminator on some real outlier data already achieves the state-of-the-art. Second, we augment the available set of real open training examples with adversarially synthesized
“fake” data. Third and most importantly, we build the dis-criminator over the features computed by the closed-world
K-way networks. Extensive experiments show that Open-GAN signiﬁcantly outperforms prior open-set methods.
Machine learning systems that operate in the real open-world invariably encounter test-time data that is unlike training examples, such as anomalies or rare objects that were insufﬁciently or even never observed during train-ing. Fig. 2 illustrates two cases in which a state-of-the-art semantic segmentation network misclassiﬁes a “stroller”/
“street-market” — a rare occurrence in either training or testing — as a “motorcycle”/“building”. This failure could be catastrophic for an autonomous vehicle.
Addressing the open-world has been explored through anomaly detection [59, 25] and out-of-distribution detec-tion [29]. In K-way classiﬁcation, it can be crisply formu-lated as open-set recognition, which requires discriminating open-set data that belongs to a (K+1)th “other” class, out-side the K closed-set classes [45]. Typically, open-set dis-crimination assumes no training examples from the “other” class (i.e., open-training data) [5, 54, 35]. In this setup, an elegant idea is to learn the closed-set data distribution with a GAN and use a GAN-discriminator as the open-set like-lihood function (Fig. 1a) [47, 44, 39, 56, 30]. However, it does not work well due to instable training of GANs. Re-cent work has shown that outlier exposure (Fig. 1b), or the ability to train on some outlier data as open-training exam-ples, can work surprisingly well via the training of a sim-ple open-vs-closed binary discriminator [15, 25]. However, 1
Cityscapes images pixel label prediction other pixels road person motorcycle car building traffic sign sidewalk
Figure 2: We motivate open-set recognition with safety concerns in autonomous vehicles (AVs). Contemporary benchmarks such as Cityscapes [11] ignore a sizeable “other” pixels for evaluation, which are outside K classes of interest. As a result, most state-of-the-art segmentation approaches [52] also ignore them during training, which then become open-set examples. Perhaps surpris-ingly, the ignored “other” pixels include vulnerable objects like wheelchairs and strollers (upper row). Here, a semantic segmen-tation network [52] does not model strollers (upper row) or street-market (lower row), which are outside the K closed-set classes in Cityscapes. The network misclassiﬁes the stroller as a “motor-cycle”, and the street-market as “building”. Such misclassiﬁca-tions can be a critical mistake when fed into AVs because these objects require different plans for obstacle avoidance (e.g., “yield” or “slow-down”). Fig. 4 shows qualitative results by our approach. such discriminators generalize poorly to diverse open-set data [48] due to overﬁtting to the available set of training outliers, which are often biased and fail to exhaustively span the open-world. Motivated by above, we introduce Open-GAN, a simple approach that dramatically improves open-set classiﬁcation accuracy by incorporating several key in-sights. First, we show that using outlier data as a valset to select the “right” GAN-discriminator does achieve the state-of-the-art on open-set discrimination. Second, with outlier exposure, we augment the available set of open-training data by adversarially generating fake open examples that fool the binary discriminator (Fig. 1c). Third and most im-portantly, rather than deﬁning discriminators on pixels, we deﬁne them on off-the-shelf (OTS) features computed by the closed-world K-way classiﬁcation network (Fig. 1d).
We ﬁnd such discriminators generalize much better.
Our formulation differs in three ways from other open-set approaches that employ GANs. (1) Our goal is not to generate realistic pixel images, but rather to learn a ro-bust open-vs-closed discriminator that naturally serves as an open-set likelihood function. Because of this, our ap-proach might be better characterized as a discriminative adversarial network! (2) We train the discriminator with both fake data (synthesized from the generator) and real open-training examples (cf. outlier exposure [25]). (3) We train GANs on OTS features rather than RGB pixels. We show that OpenGAN signiﬁcantly outperforms prior work for open-set recognition across a variety of tasks including image classiﬁcation and pixel segmentation. Moreover, we demonstrate that our technical insights improve the accu-racy of other GAN-based open-set methods: training them on OTS features and selecting their discriminators via vali-dation as the open-set likelihood function. 2.