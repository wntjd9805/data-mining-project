Abstract
Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continu-ous and resolution-independent manner. However, adapt-ing them to articulated shapes is non-trivial. Existing ap-proaches learn a backward warp ﬁeld that maps deformed to canonical points. However, this is problematic since the backward warp ﬁeld is pose dependent and thus requires large amounts of data to learn. To address this, we in-troduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deforma-tion ﬁeld without direct supervision. This deformation ﬁeld is deﬁned in canonical, pose-independent, space, enabling generalization to unseen poses. Learning the deformation
ﬁeld from posed meshes alone is challenging since the cor-respondences of deformed points are deﬁned implicitly and may not be unique under changes of topology. We propose a forward skinning model that ﬁnds all canonical correspon-dences of any deformed point using iterative root ﬁnding.
We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural im-plicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D hu-mans in diverse and unseen poses. 1.

Introduction
Modeling the shape and deformation of articulated 3D objects has traditionally been achieved by deforming a polygonal mesh via linear blend skinning (LBS) with pose-correctives. However, meshes are inherently limited by their resolution-to-memory ratio and their ﬁxed topology.
Therefore, neural implicit surface representations [10, 30, 31, 38] have recently attracted much attention because they provide a resolution-independent, smooth and continuous alternative to discrete meshes. However, updating an im-Figure 1: SNARF: From a sequence of posed meshes (top), we learn a neural implicit 3D shape and a skinning ﬁeld in canonical pose (middle) without supervision of skinning weights or part correspondences. Learned forward skinning enables generalization to unseen poses (bottom) while cap-turing local details via pose conditioning. plicit surface representation as a function of the underlying pose changes is challenging since it requires modifying a continuous function rather than a discrete set of points.
To address this, we propose SNARF (Skinned Neural Ar-ticulated Representations with Forward skinning), a novel approach to learning articulated 3D shapes represented by neural implicit surfaces directly from 3D watertight meshes and corresponding bone transformations with no need for supervision via pre-deﬁned skinning weights. SNARF com-bines the simplicity of skeletal-driven deformation of LBS with the ﬁdelity and topological ﬂexibility of implicit sur-point. Our approach is able to retrieve multiple correspon-dences for any deformed point and therefore naturally han-dles topology changes. We further derive the gradients of our forward skinning module, hence making it differen-tiable and enabling end-to-end learning of the canonical shape and skinning weights jointly from deformed observa-tions. Importantly, and in contrast to prior work, our method does not require any a priori skinning weights or pose cor-rectives deﬁned on the surface and hence can be applied in scenarios where pre-rigged mesh models are not available.
We experimentally demonstrate that our method is able to generate high-quality shapes with arbitrary desired bone transformations, even those far beyond the training distri-bution, where other recent methods like NASA [12] fail.
Since our approach operates in continuous space, it enables reconstruction of ﬁne geometric details. By conditioning the neural implicit function on poses, our method faithfully models local pose-dependent deformations, e.g., the move-ment of clothing or soft tissue. Our code is available at github.com/xuchen-ethz/snarf. 2.