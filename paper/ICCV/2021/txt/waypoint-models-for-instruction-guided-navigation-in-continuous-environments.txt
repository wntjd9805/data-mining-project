Abstract
Little inquiry has explicitly addressed the role of ac-tion spaces in language-guided visual navigation – either in terms of its effect on navigation success or the efﬁciency with which a robotic agent could execute the resulting tra-jectory. Building on the recently released VLN-CE [24] set-ting for instruction following in continuous environments, we develop a class of language-conditioned waypoint pre-diction networks to examine this question. We vary the ex-pressivity of these models to explore a spectrum between low-level actions and continuous waypoint prediction. We measure task performance and estimated execution time on a proﬁled LoCoBot [1] robot. We ﬁnd more expressive models result in simpler, faster to execute trajectories, but lower-level actions can achieve better navigation metrics by approximating shortest paths better. Further, our models outperform prior work in VLN-CE and set a new state-of-the-art on the public leaderboard – increasing success rate by 4% with our best model on this challenging task. 1.

Introduction
A long-term goal of instruction-guided visual navigation research is to develop AI for robotic agents that can reliably follow paths described by natural language navigation in-structions in new environments. Much of the existing work in this domain is robot-agnostic and has focused on highly-abstract simulators where agents navigate by choosing be-tween a small, ﬁxed set of nearby locations that the agent then transitions to deterministically [4, 15, 22, 25] – es-sentially assuming some underlying robot-speciﬁc control system can perform navigation. The Vision-and-Language
Navigation (VLN) [4] task is representative of this class of problem settings.
In sim-to-real experiments, Anderson et al. [3] demon-strate that a major performance bottleneck for transferring
*Work done during an internship at Facebook AI Research.
Correspondence: krantzja@oregonstate.edu
Figure 1. Our approach decomposes the task of following nav-igation instructions in continuous environments into language-conditioned waypoint prediction and low-level navigation.
VLN agents trained in high-level simulators to real robotic systems is producing appropriate sets of nearby locations (or waypoints) to choose from; however, it is infeasible to study waypoint prediction in the discrete, highly-abstract simulator as the agent can only occupy predeﬁned locations.
Recently, Krantz et al. [24] introduced a variant of VLN instantiated in continuous simulated environments (denoted
VLN-CE) such that agents can move to arbitrary positions.
In contrast to the highly-abstract action space in VLN, agents in [24] navigate by executing a sequence of low-level actions such as moving forward 0.25 meters or turn-ing by 15 degree increments. This end-to-end, instruction-to-low-level-control design choice has implications both in simulation and for potential sim-to-real transfer to a robotic platform. During training, these policies must jointly learn
navigation and language grounding over long sequences of actions (∼55 per episode). As a result, [24] shows that mod-els mirroring successful VLN agents perform substantially worse in VLN-CE.
On a real robot, the frequent stop, starts, and turns in-duced by this action space can be slow to execute (requiring frequent changes in velocity and calls to a planner), result in state estimation error, and strain hardware [23, 19]. Further, executing the deep policy network to predict actions at each time step can put extra demand on robot power supplies.
This work explores a spectrum of action spaces between these two extremes – studying instruction-guided naviga-tors that predict relative waypoints with varied expressivity.
At one end, our agents are free to predict relative waypoints as continuous points within some maximum range. On the other, the action space is reduced to taking a ﬁxed step in a direction chosen from a small, ﬁnite set of angles – mim-icking [24] but collapsing consecutive turns. In between, we experiment with mixing discrete and continuous com-ponents to parameterize waypoint predictions.
To do this, we develop an attention-based waypoint pre-diction network for instruction following. Given a naviga-tion instruction and a panoramic RGBD observation at the current position, our agents predict a distribution over rela-tive waypoints in polar coordinates (consisting of a heading angle θ and a distance r). A low-level continuous navigator is then executed to move in a straight line towards the way-point – leaving concerns about obstacle avoidance to the waypoint predictor. We train our agents as model-free con-trol policies using large-scale reinforcement learning [37] on the VLN-CE dataset. We evaluate our agents using stan-dard metrics for VLN-CE as well as the estimated execution time for resulting trajectories on a LoCoBot [1] robot.
We ﬁnd that more expressive waypoint prediction net-works result in simpler paths that are faster to execute; however, more constrained action spaces can achieve better performance by more closely approximating shortest paths.
Our waypoint models paired with continuous low-level nav-igators reduce the average estimated time to execute a tra-jectory by 2.2 times compared to low-level turn/forward models. When paired with discrete low-level navigators to match VLN-CE’s action space, our models improve naviga-tion success rate by 1-4% over prior work on the VLN-CE leaderboard1– a max relative improvement of 14%.
Contributions. We summarize our contributions as: – Developing a class of language-conditioned waypoint prediction networks for the VLN-CE task, – Providing empirical analysis of waypoint prediction ex-pressivity’s effect on navigation success and estimated time to execute trajectories on a representative robot, – Demonstrating that our models paired with low-level navigators set a new state-of-the-art on the VLN-CE test 1eval.ai/web/challenges/challenge-page/719 leaderboard by an absolute 4% success rate.
We provide open-source code and pre-trained models at https://github.com/jacobkrantz/VLN-CE. 2.