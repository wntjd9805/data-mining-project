Abstract
Geometric constraints are shown to enforce scale con-sistency and remedy the scale ambiguity issue in self-supervised monocular depth estimation. Meanwhile, scale-invariant losses focus on learning relative depth, leading to accurate relative depth prediction. To combine the best of both worlds, we learn scale-consistent self-supervised depth in a scale-invariant manner. Towards this goal, we present a scale-aware geometric (SAG) loss, which en-forces scale consistency through point cloud alignment.
Compared to prior arts, SAG loss takes relative scale into consideration during relative motion estimation, enabling more precise alignment and explicit supervision for scale inference.
In addition, a novel two-stream architecture for depth estimation is designed, which disentangles scale from depth estimation and allows depth to be learned in a scale-invariant manner. The integration of SAG loss and two-stream network enables more consistent scale infer-ence and more accurate relative depth estimation. Our method achieves state-of-the-art performance under both scale-invariant and scale-dependent evaluation settings. 1.

Introduction
To alleviate the need of high-quality ground truth depth data, there is a recent surge of interest in self-supervised monocular depth estimation [36, 10]. The basic idea is to jointly learn depth estimation and ego-motion prediction su-pervised by a photometric reconstruction loss. Although, these approaches have achieved remarkable success in pop-ular benchmarks, they are known to suffer from the per-frame scale ambiguity issue [27, 2]. For one thing, the esti-mated depths are not guaranteed to be scale consistent and the ego-motion network also fails to predict globally consis-tent trajectories for long videos. For another, without proper constraints, the depth network has to adapt its scales accord-ing to the ego-motion prediction and vice versa, which con-∗Corresponding author (a) Error (b) Accuracy
Baseline
Input Image
Ours (c) Depth Map
Figure 1. Strength of our scale-consistent depth estimation learned in a scale-invariant manner. (a) Scale-dependent evaluation on
KITTI [9], where predictions are aligned to ground truth using one global scale on a per-sequence basis rather than conventional per-frame scale alignments (See Sec. 4.2.2). (b) Scale-invariant learn-ing allows our method to produce more accurate relative depth. fuses network training and results in performance degener-ation or even training divergence [27]. Meanwhile, recent evidence [7] also indicates that the global scale is a funda-mental source of uncertainty in supervised depth estimation, and that scale-invariant losses focus on relative depth learn-ing and largely beneﬁt depth estimation in terms of accuracy as well as generalization abilities [19].
In light of the above analysis, an interesting question to ask is whether we can achieve scale-consistent depth es-timation but also enjoy the advantages of scale-invariant training under the self-supervised framework? We make the ﬁrst attempt to answer this question by proposing a new paradigm for self-supervised monocular depth estimation.
To this end, we present a scale-aware geometric loss (dubbed SAG loss) that operates in the 3D space. The es-timated depth of adjacent frames are ﬁrst projected to 3D point clouds and then transformed into a common view us-Instead of directly penal-ing the predicted ego-motion.
izing the coordinate differences between point clouds, we estimate their relative motion parameters in a least-square sense, which include not only the rotation and translation, but also the relative scale factor. The SAG loss are com-puted by incorporating the three motion parameters to en-force scale consistent depth estimation of adjacent frames.
With iterative training, depth consistency can ﬁnally be propagated through entire sequences (See Fig. 1 (a)(b)).
In order to enjoy the beneﬁts of scale invariant training, we propose to decompose the depth estimation task into two sub-tasks: normalized depth prediction and scale inference.
Through careful designing, we ensure that the SAG loss is invariant to the scale of the normalized depth prediction and that scale inference can be learned in an explicit manner to guarantee depth scale consistency. We present a concrete implementation of the above idea through a new depth net-work with a two-stream architecture.
Learning scale-consistent depth in a scale-invariant man-ner seems to be contradictory at the ﬁrst glance and be-comes even more intractable under self-supervised frame-works, due to the scale ambiguity and lack of scale super-vision. Thanks to the proposed SAG loss, scale supervision can be explicitly calculated during self-supervised learning to simultaneously ensure scale consistency and allow the disentanglement of depth and scale. As a matter of fact, the idea of taking scale into explicit consideration itself is shown to be beneﬁcial to motion estimation when scale in-consistency indeed exists.
In addition, we also explore a new strategy for ﬁnding corresponding points between point clouds, which further facilitates motion estimation, lead-ing to a more effective SAG loss. By combining the SAG loss with our two-stream depth network, our method is able to take advantages of scale-invariant depth learning, giv-ing rise to geometrically more consistent and quantitatively more accurate depth estimation (See Fig. 1 (c)).
The contribution of this work can be summarized into three folds.
• A new self-supervised depth estimation framework that enjoys the strengths of scale-invariant learning and delivers scale-consistent depth.
• A scale-aware geometric loss to enforce depth consis-tency and to provide supervisions for explicit scale in-ference during self-supervised learning.
• A two-stream depth network to disentangle depth and scale prediction, allowing the normalized depth to be learned irrespective of the global scale.
Experiments on KITTI datasets demonstrate that our method can not only improve depth accuracy but also ben-eﬁt long-term ego-motion estimation. Extensive ablation studies have also been conducted, which further conﬁrm the effectiveness of our contribution. 2.