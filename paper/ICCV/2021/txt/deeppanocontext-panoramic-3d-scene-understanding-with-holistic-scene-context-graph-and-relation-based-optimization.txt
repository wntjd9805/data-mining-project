Abstract
Panorama images have a much larger field-of-view thus naturally encode enriched scene context information com-pared to standard perspective images, which however is not well exploited in the previous scene understanding methods.
In this paper, we propose a novel method for panoramic 3D scene understanding which recovers the 3D room lay-out and the shape, pose, position, and semantic category for each object from a single full-view panorama image. In order to fully utilize the rich context information, we design a novel graph neural network based context model to pre-dict the relationship among objects and room layout, and a differentiable relationship-based optimization module to optimize object arrangement with well-designed objective functions on-the-fly. Realizing the existing data are either with incomplete ground truth or overly-simplified scene, we present a new synthetic dataset with good diversity in room layout and furniture placement, and realistic image quality for total panoramic 3D scene understanding. Experiments demonstrate that our method outperforms existing methods on panoramic scene understanding in terms of both geome-try accuracy and object arrangement. Code is available at https://chengzhag.github.io/publication/dpc. 1.

Introduction
Image-based holistic 3D indoor scene understanding is a long-lasting challenging problem in computer vision, due to scene clutter and 3D ambiguity in perspective geometry.
Over decades, the scene context, which encodes high-order relations across multiple objects following certain design rules, has been widely utilized to improve the scene under-standing [48, 5]. However, it is still arguable and unclear if the top-down context is more or less important than bottom-up local appearance-based approaches for the scene pars-ing task, especially with the rapidly emerging deep learning methods that have achieved great success on object classi-fication and detection. One possible reason could be that
*Corresponding author
Figure 1: From a single panorama image as input, our pro-posed pipeline estimates layout and object poses, then re-constructs the scene with object reconstruction, to achieve total scene understanding. the field of view of a standard camera photo is normally less than 60◦, and thus only limited context can be utilized among a small number of objects co-existing in the image.
Zhang et al. [48] proposed a 3D scene parsing method that takes a 360◦ full-view panorama as the input, where almost all major objects are visible. They showed that the con-text became significantly stronger with more objects in the same image, which enables accurate 3D scene understand-ing even with less engineered local features.
In this paper, we empower the panoramic scene under-standing task with stronger 3D perception capability and aim to predict the objects’ shapes, 3D poses, and seman-tic categories as well as the room layout by taking a single color full-view panorama image as the input. To fulfill this goal, we propose a novel deep learning based framework that leverages both local image information and global con-text for panoramic 3D scene understanding. Specifically, we first extract room layout and object hypothesis from local image regions with the algorithms customized for panorama images, and rely on a global graph-based context model to effectively refine the initial estimations. Overall, our method achieves phenomenal performance on both ge-ometry accuracy and object arrangement for 3D panoramic scene understanding.
Besides renovating the predecessor [48] with a more ad-vanced deep learning algorithm, the key to the significant
performance gain is a novel context model that predicts relations across objects and room layout including sup-porting, attaching, relative orientation, etc., which are then fed into an optimization to adjust the object ar-rangement. This is inspired by the common sense that we, humans, tend to place objects tightly against the wall, e.g., beds, or side-by-side with consistent orientation, e.g., night-stands, and these relations could provide critical informa-tion to fix the object arrangement errors that may be minor in traditional metrics but obviously wrong judged by human perception. To leverage the predicted relations, we propose a novel differentiable optimization with carefully designed objective functions to adjust the initial object arrangement w.r.t. the predicted relations, which further enables joint training of relation prediction and object arrangement. The optimization is fully differentiable, which can be attached with our graph based context model, and conceptually any neural network, for joint training.
Unfortunately, the panoramic scene datasets for holistic 3D scene parsing are still missing in the literature. Existing panorama datasets are either with overly simplified scenes
[48], purely 2D-based [39], or missing important 3D ground truth such as object poses [1, 4]. Since annotating real data with accurate 3D shapes is extremely challenging, we re-sort to synthetic data and create a new dataset for holistic panoramic 3D scene understanding. The dataset provides high-quality ground truth for object location, pose, shape, and pairwise relations, and serves well for training and rig-orous evaluation. Though purely synthetic, we find the learned context model, which relies mainly on indoor scene context but not heavily on the image appearance, can be naturally generalized to real images by retraining bottom-up models that provide the initialization.
In summary, our contributions are as follows. We pro-pose the first deep learning based pipeline for holistic 3D scene understanding that recovers 3D room layout and de-tailed shape, pose, location for objects in the scene from a single color full-view panorama image. To fully exploit context, we design a novel context model that predicts the relationship among objects and room layout, followed by a new differentiable relationship-based optimization module to refine the initial results. To learn and evaluate our model, a new dataset is created for total panoramic 3D scene un-derstanding. Our model achieves the state-of-the-art perfor-mance on both geometry accuracy and 3D object arrange-ment. 2.