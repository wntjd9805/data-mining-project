Abstract
Training temporal action detection in videos requires large amounts of labeled data, yet such annotation is expen-sive to collect. Incorporating unlabeled or weakly-labeled data to train action detection model could help reduce an-In this work, we ﬁrst introduce the Semi-notation cost. supervised Action Detection (SSAD) task with a mixture of labeled and unlabeled data and analyze different types of errors in the proposed SSAD baselines which are di-rectly adapted from the semi-supervised classiﬁcation lit-erature. Identifying that the main source of error is action incompleteness (i.e., missing parts of actions), we allevi-ate it by designing an unsupervised foreground attention (UFA) module utilizing the conditional independence be-tween foreground and background motion. Then we incor-porate weakly-labeled data into SSAD and propose Omni-supervised Action Detection (OSAD) with three levels of su-pervision. To overcome the accompanying action-context confusion problem in OSAD baselines, an information bot-tleneck (IB) is designed to suppress the scene information in non-action frames while preserving the action informa-tion. We extensively benchmark against the baselines for
SSAD and OSAD on our created data splits in THUMOS14 and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and IB methods. Lastly, the beneﬁt of our full
OSAD-IB model under limited annotation budgets is shown by exploring the optimal annotation strategy for labeled, unlabeled and weakly-labeled data. 1 1.

Introduction
Temporal action detection is one of the most fundamen-tal tasks in video understanding, which requires simultane-ously classifying the actions in a video and localizing their start and end times. Recent success of temporal action de-tection models [55, 50, 18, 36, 4] highly relies on large 1https://github.com/bfshi/SSAD_OSAD.
Figure 1: Left: Data type for different tasks. FSAD uses only fully-labeled videos (white area). SSAD uses both fully-labeled and unlabeled videos (blue area). OSAD further uses weakly-labeled videos besides these two data types (green area). Right:
Error analysis on SSAD/OSAD models. We consider three types of errors: 1) Action incompleteness (Miss), 2) Misclassiﬁcation (Cls), and 3) Action-context confusion (Bkgd). Compared to the supervised-only model, the main type of error in SSAD baseline is Miss, while in OSAD baseline it is Bkgd. See Sec. 4.2. amounts of fully-labeled training data with both classiﬁ-cation and localization annotations. However, the annota-tion process, especially for localization, is extremely time-consuming and expensive. To alleviate this problem, one direction is to maximize the usage of unlabeled or weakly-labeled data to bring performance improvement with lower annotation cost. In this work, we study temporal action de-tection using fewer labeled videos together with other levels of supervision, e.g. unlabeled and weakly-labeled videos.
Learning from unlabeled data has been investigated in the task of semi-supervised image classiﬁcation [28, 42, 1, 40] and shows promising results, while such problem set-ting is unexplored in the temporal action detection. We in-troduce the Semi-supervised Action Detection (SSAD) task and establish three SSAD baselines by incorporating three state-of-the-art Semi-Supervised Learning (SSL) models (Mean Teacher [42], MixMatch [1], FixMatch [40]) into a Fully-Supervised Action Detection (FSAD) backbone.
For the purpose of initial evaluation on SSAD benchmark, we choose a straightforward yet effective FSAD method,
SSN [55], as our backbone and leave the development of more complex backbones for future work. However, di-rectly applying SSL algorithms in the SSAD baselines only brings small improvement compared to the supervised-only model. To track down the main source of error, we con-duct error analysis for the SSAD baselines (Fig. 1) and ﬁnd the main problem of the SSAD baseline is action incom-pleteness, namely only detecting parts of the action. To help SSAD baseline better recognize actions, we borrow the idea from object-centric representations [8, 24, 49, 23] to extract more discriminative representation of action which is basically characterized by the foreground objects (hu-mans).2 There have been attempts to endow machines with the ability to detect salient moving objects. However, they either need manual annotation [44, 45, 41, 5], or make as-sumptions improper for action videos [52]. In this work, we propose to detect the foreground without supervision by leveraging the conditional independence between fore-ground and background motions, i.e., the foreground mo-tion is self-contained and not affected by the motion of background. Speciﬁcally, we learn the foreground atten-tion by minimizing the conditional mutual information be-tween foreground and background motion. To this end, our proposed unsupervised foreground attention (UFA) module successfully helps SSAD models recognize relatively com-plete actions without extra annotation cost.
Further, we consider weakly-labeled data with only video-level category labels which is in the middle of the cost-accuracy trade-off between fully-labeled and unlabeled data. It has been shown that weakly-supervised temporal action detection [26, 27, 35, 29, 19, 21] can save anno-tation cost without degrading the performance too much.
Therefore, we further include weakly-labeled data into the
SSAD model and form a uniﬁed framework with three lev-els of supervision, named Omni-supervised Action Detec-tion (OSAD). As a baseline for OSAD, we simply add a video-level classiﬁcation loss for the additional weakly-labeled data. However, training video-level classiﬁcation to realize weak action localization may cause action-context confusion [19, 35], i.e., the model is highly activated at non-action frames because they contain background scene information (e.g. swimming pool) which is highly indica-tive of the action category (e.g. swimming). This phe-nomenon is also veriﬁed in our error analysis (Fig. 1) where the OSAD baseline has higher action-context confusion than supervised-only model. To alleviate the issue, we pro-pose an information bottleneck (IB) method to ﬁlter out the scene information extracted from non-action frames while preserving the action information by training action clas-siﬁcation. Speciﬁcally, we regularize the entropy of non-2In this paper we use the terms “background” and “foreground” to in-dicate the spatial regions in each frame, and “action” and “non-action” denote the temporal frames. action frames which only contain scene information, thus reduce the action-context confusion(Fig. 1).
We conduct extensive experiments on SSAD and OSAD baselines, as well as the proposed UFA and IB. More-over, we show the advantage of multi-level supervision over single-level supervision in a realistic scenario, where we search the best labeling policy under an annotation budget.
To sum up our main contributions, we: (i) propose the
SSAD and OSAD tasks to utilize unlabeled and weakly-labeled data in temporal action detection, and establish sev-eral baseline models for them; (ii) design an unsupervised foreground attention module to alleviate the action incom-pleteness problem in SSAD baselines; (iii) design an infor-mation bottleneck method to solve the action-context confu-sion problem in OSAD baselines; (iv) validate the proposed
SSAD and OSAD methods through extensive experiments, and show the advantage of our full OSAD-IB model under a realistic scenario where an annotation budget is given. 2.