Abstract
In this paper, we present a neat yet effective transformer-based framework for visual grounding, namely TransVG, to address the task of grounding a language query to the corresponding region onto an image. The state-of-the-art methods, including two-stage or one-stage ones, rely on a complex module with manually-designed mechanisms to perform the query reasoning and multi-modal fusion. How-ever, the involvement of certain mechanisms in fusion mod-ule design, such as query decomposition and image scene graph, makes the models easily overﬁt to datasets with spe-ciﬁc scenarios, and limits the plenitudinous interaction be-tween the visual-linguistic context. To avoid this caveat, we propose to establish the multi-modal correspondence by leveraging transformers, and empirically show that the complex fusion modules (e.g., modular attention network, dynamic graph, and multi-modal tree) can be replaced by a simple stack of transformer encoder layers with higher per-formance. Moreover, we re-formulate the visual grounding as a direct coordinates regression problem and avoid mak-ing predictions out of a set of candidates (i.e., region pro-posals or anchor boxes). Extensive experiments are con-ducted on ﬁve widely used datasets, and a series of state-of-the-art records are set by our TransVG. We build the benchmark of transformer-based visual grounding frame-work and make the code available at https://github. com/djiajunustc/TransVG. 1.

Introduction
Visual grounding (also known as referring expression comprehension [31, 60], phrase localization [23, 38], and natural language object retrieval [21, 25]) aims to predict the location of a region referred by the language expression onto an image. The evolution of this technique is of great potential to provide an intelligent interface for the natural language expression of human beings and the visual com-ponents of the physical world. Existing methods addressing
Figure 1. A comparison of (a) two-stage pipeline, (b) one-stage pipeline, and (c) our proposed TransVG framework. TransVG per-forms intra-modality and inter-modality relation reasoning with a stack of transformer layers in a homogeneous way, and grounds the object by directly regressing the box coordinates. this task can be broadly grouped into the two-stage and one-stage pipelines shown in Figure 1. In speciﬁc, the two-stage approaches [31, 34, 46, 60] ﬁrst generate a set of sparse re-gion proposals and then exploit region-expression matching to ﬁnd the best one. The one-stage approaches [9, 27, 56] perform visual-linguistic fusion at intermediate layers of an object detector and output the box with the maximal score over pre-deﬁned dense anchors.
Multi-modal fusion and reasoning is widely studied in the literature [1, 35, 49, 54, 65], and it is the core problem in visual grounding. In general, the early two-stage and one-stage methods address multi-modal fusion in a simple way.
Concretely, the two-stage Similarity Net [46] measures the
similarity between region and expression embedding with an MLP, and the one-stage FAOA [56] encodes the language vector to visual feature by direct concatenation. These sim-ple designs are efﬁcient but lead to sub-optimal results, es-pecially on long and complex language expressions. Fol-lowing studies have proposed diverse architectures to im-prove the performance. Among two-stage methods, mod-ular attention network [59], various graphs [48, 52, 53], and multi-modal tree [28] are designed to better model the multi-modal relationships. The one-stage method [55] has also explored better query modeling by proposing a multi-round fusion module.
Despite the effectiveness, these complicated fusion mod-ules are built on certain pre-deﬁned structures of language queries or image scenes, inspired by the human prior. Typi-cally, the involvement of manually-designed mechanisms in fusion module makes the models overﬁt to speciﬁc scenar-ios, such as certain query lengths and query relationships, and limits the plenitudinous interaction between visual-linguistic contexts. Moreover, even though the ultimate goal of visual grounding is to localize the referred object, most of the previous methods ground the queried object in an indirect fashion. They generally deﬁne surrogate prob-lems of language-guided candidates prediction, selection, and reﬁnement. Typically, the candidates are sparse region proposals [60, 31, 46] or dense anchors [56], from which the best region is selected and reﬁned to get the ﬁnal grounding box. Since these methods’ predictions are made out of can-didates, the performance is easily inﬂuenced by the prior knowledge to generate proposals (or pre-deﬁned anchors) and by the heuristics to assign targets to candidates.
In this study, we explore an alternative approach to avoid the aforementioned problems. Formally, we introduce a neat and novel transformer-based framework, namely
TransVG, to effectively address the task of visual ground-ing. We empirically show that the structurized fusion mod-ules can be replaced by a simple stack of transformer en-coder layers. Particularly, the core component of transform-ers (i.e., attention layer) is ready to establish intra-modality and inter-modality correspondence across visual and lin-guistic inputs, despite that we do not pre-deﬁne any speciﬁc fusion mechanism. Besides, we ﬁnd that directly regressing the box coordinates works better than previous methods to ground the queried object indirectly. Our TransVG directly outputs 4-dim coordinates to ground the object instead of making predictions based on a set of candidate boxes.
The pipeline of our proposed TransVG is illustrated in
Figure 1(c). We ﬁrst feed the RGB image and language ex-pression into two sibling branches. The visual transformer and linguistic transformer are applied in these two branches to model the global cues in vision and language domains, respectively. Then, the abstracted visual tokens and linguis-tic tokens are fused, and the visual-linguistic transformer is exploited to perform cross-modal relation reasoning. Fi-nally, the box coordinates of a referred object are directly regressed to make grounding. We benchmark our frame-work on ﬁve prevalent visual grounding datasets, including
ReferItGame [23], Flickr30K Entities [38], RefCOCO [60],
RefCOCO+ [60], RefCOCOg [31], and our method sets a series of state-of-the-art records. Remarkably, our proposed
TransVG achieves 70.73%, 79.10% and 78.35% on the test set of ReferItGame, Flickr30K and RefCOCO datasets, with 6.13%, 5.80%, 6.05% absolute improvements over the strongest competitors.
In summary, we make three-fold contributions:
• We propose the ﬁrst transformer-based framework for visual grounding, which holds neater architecture yet achieves better performance than the prevalent one-stage and two-stage frameworks.
• We present an elegant view of capturing intra- and inter-modality context homogeneously by transform-ers, and formulating visual grounding as a direct coor-dinates regression problem.
• We conduct extensive experiments to validate the mer-its of our method, and show signiﬁcantly improved re-sults on several prevalent benchmarks. 2.