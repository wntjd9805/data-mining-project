Abstract 1.

Introduction 3D reconstruction of large scenes is a challenging prob-lem due to the high-complexity nature of the solution space, in particular for generative neural networks.
In contrast to traditional generative learned models which encode the full generative process into a neural network and can strug-gle with maintaining local details at the scene level, we introduce a new method that directly leverages scene ge-ometry from the training database. First, we learn to syn-thesize an initial estimate for a 3D scene, constructed by retrieving a top-k set of volumetric chunks from the scene database. These candidates are then refined to a final scene generation with an attention-based refinement that can ef-fectively select the most consistent set of geometry from the candidates and combine them together to create an output scene, facilitating transfer of coherent structures and local detail from train scene geometry. We demonstrate our neu-ral scene reconstruction with a database for the tasks of 3D super resolution and surface reconstruction from sparse point clouds, showing that our approach enables genera-tion of more coherent, accurate 3D scenes, improving on average by over 8% in IoU over state-of-the-art scene re-construction. 3D scene reconstruction has been a long-standing prob-lem in computer vision and graphics, and has recently seen a renewed flurry of developments, driven by successes in gen-erative neural networks [29, 28, 9, 30]. In particular, devel-oping an effective geometric reconstruction is challenging due to the dimensionality of the problem, and the simultane-ous expressibility for local details as well as coherent, com-plex global structures. In recent years, various approaches have been developed for geometric reconstruction, encod-ing the full generative process into a neural network. This can result in difficulty in representing large-scale, complex scenes, as all levels of detail must be fully encoded as part of the generative network.
We thus propose to augment geometric reconstruction with a database which our method learns to leverage at in-ference time, and introduce a generative model that does not need to encode the entire training data as part of the network parameters. Instead, our model learns how to best transfer structures and details from retrieved scene database geometry.
We construct this database as geometric, cropped chunks of 3D scenes from train scene data. Each chunk represents clean, consistent, high-resolution geometry. We leverage
these chunks as a basis for scene reconstruction.
To this end, we develop a neural 3D scene reconstruc-tion approach to generate 3D scenes as volumetric distance fields. This approach consists of two main steps: a top-k nearest neighbor retrieval and combination for initial es-timation, and a refinement stage to produce the higher-quality, final reconstruction. Specifically, to generate a 3D scene from an input condition (e.g., a noisy or sparse ob-servation of a scene), we first learn to construct an initial estimate of the scene as a combination of cropped volu-metric chunks from the database. By providing an initial estimate based on chunks of existing scene geometry, we can more easily encourage consistent, sharp structures al-ready seen in the existing scene geometry. Since these ini-tial scene crop estimates may not be entirely locally consis-tent with each other, we then refine this estimate to produce a final scene reconstruction. The scene refinement is based on patch-based attention which encourages the selection of given scene chunk estimates where they suffice – maintain-ing their clean details – and synthesizing refined geometry otherwise.
By leveraging database retrieval in combination with a generative model, our approach does not need to encode the full train set for effective reconstruction, and facilitates generation of globally coherent, high quality 3D scenes. We demonstrate our approach on the tasks of 3D super resolu-tion and 3D surface reconstruction from sparse point sam-ples on both synthetic and real-world 3D scene data, show-ing significant qualitative and quantitative improvement in comparison to state-of-the-art reconstruction approaches.
Additionally, we show that our approach can also be applied to other generative representations, in particular, to improve implicit-based reconstruction.
In summary, our main contributions are:
• A neural 3D reconstruction technique that leverages details present in a database of cropped scene chunks for improving reconstructed geometry.
• A patch-wise attention-based refinement that robustly fuse together details from the retrieved scene chunks. 2.