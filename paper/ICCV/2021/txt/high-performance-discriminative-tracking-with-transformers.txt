Abstract
End-to-end discriminative trackers improve the state of the art signiﬁcantly, yet the improvement in robustness and efﬁciency is restricted by the conventional discriminative model, i.e., least-squares based regression. In this paper, we present DTT, a novel single-object discriminative tracker, based on an encoder-decoder Transformer architecture. By self- and encoder-decoder attention mechanisms, our ap-proach is able to exploit the rich scene information in an end-to-end manner, effectively removing the need for hand-designed discriminative models. In online tracking, given a new test frame, dense prediction is performed at all spatial positions. Not only location, but also bounding box of the target object is obtained in a robust fashion, streamlining the discriminative tracking pipeline. DTT is conceptual-ly simple and easy to implement. It yields state-of-the-art performance on four popular benchmarks including GOT-10k, LaSOT, NfS, and TrackingNet while running at over 50
FPS, conﬁrming its effectiveness and efﬁciency. We hope
DTT may provide a new perspective for single-object visual tracking. 1.

Introduction
Generic visual tracking is a long-standing topic in the
ﬁeld of computer vision and has attracted increasing atten-tion over the last decades. Despite signiﬁcant progress in recent years [2,6,7,19, 23,26,33, 34,41,42], visual tracking remains challenging due to numerous factors such as very limited online training samples, large appearance variation, and heavy background clutters.
In recent years, Siamese network based trackers [1,5,11– 13, 19, 20, 36, 37] have attracted great attention because of their balanced speed and accuracy. These methods formu-late the visual tracking task as a target matching problem and aim to learn a general similarity metric between the tar-get template and the search region (see Fig. 1(a)). Power-ful backbone networks [19] and effective proposal network-s [11] are proposed to achieve promising results. However, the Siamese learning framework cannot exploit the back-ground information effectively to improve the discrimina-tion.
On the contrary, modern discriminative trackers [2,3,42] are able to exploit the background information and typical-ly learn an adaptive discriminative model by minimizing the regression loss (see Fig. 1(b)). Although they have achieved leading performance on serval benchmarks [15, 16, 35], we point out that such tracking scheme has the following three limitations. 1) The discrimination of the applied regression models (i.e., least-squares based regression) is rough and in-sufﬁcient for robust tracking because the conventional mod-els often fail to reserve the detailed scene information and encode the relationships among the distractors in the back-ground. 2) Modern discriminative models can only con-tribute to localization, and thus have to rely on other meth-ods like ATOM [6] for ﬁnal predicted bounding box, result-ing in a separated tracking pipeline. 3) In online tracking, iterative solution is needed in both the model predictor [2] and the bounding box reﬁnement module [6], which is not friendly to most embedded devices and may negatively af-fect the efﬁciency.
To this end, we present a novel discriminative tracker with Transformers, termed as DTT, which is a conceptual-ly simple, efﬁcient and robust tracking architecture. The core advantage of DTT is that it can effectively and ef-ﬁciently exploit the rich scene information for both clas-siﬁcation and bounding box regression. Speciﬁcally, DT-T is built upon an encoder-decoder Transformer architec-ture [32] where the features of training image obtained by convolutional neural networks (CNNs) are fed to the en-coder, as shown in Fig. 1(c). Due to the self-attention mechanism in the encoder, its outputs, called discrimina-tion. Finally, similar to Siamese trackers [5,13], our predic-tion head consists of a classiﬁcation branch and a bound-ing box regression branch for robust and accurate tracking.
Besides, considering the importance of model update along with its efﬁciency in visual tracking, we employ a simple yet effective update method to ﬁt our DTT to the variation of the scene and the target in online tracking. Without bells and whistles, the overall pipeline is neat, straightforward, and easy to implement. Extensive experiments on four pop-ular benchmarks, GOT-10k [15], LaSOT [10], NfS [16], and
TrackingNet [24], show that DTT achieves the state-of-the-art results on all datasets, while running at over 50 FPS.
Code shall be released.
In summary, our contributions are in four folds. 1. We propose a novel and conceptually simple discrim-inative tracker, called DTT, which is based on an encoder-decoder transformer architecture. 2. DTT is able to exploit rich scene information and gen-erate discriminative feature embeddings in an end-to-end learning pipeline, removing the need of integrating conventional discriminative models. 3. DTT allows dense prediction to obtain both the loca-tion and bounding box of the target object in an robust way, simplifying the pipeline of discriminative track-ing framework. 4. Experimental results show that DTT is comparable with state-of-the-art trackers without bells and whis-tles. We hope this effective and efﬁcient method could provide a new perspective for visual tracking. 2.