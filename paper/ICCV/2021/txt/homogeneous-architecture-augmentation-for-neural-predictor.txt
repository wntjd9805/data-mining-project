Abstract they also suffer a severe limitation:
Neural Architecture Search (NAS) can automatically de-sign well-performed architectures of Deep Neural Networks (DNNs) for the tasks at hand. However, one bottleneck of NAS is the prohibitively computational cost largely due to the expensive performance evaluation. The neural pre-dictors can directly estimate the performance without any training of the DNNs to be evaluated, thus have drawn increasing attention from researchers. Despite their pop-ularity, the short-age of annotated DNN architectures for effectively train-ing the neural predictors. In this paper, we proposed Ho-mogeneous Architecture Augmentation for Neural Predictor (HAAP) of DNN architectures to address the issue afore-mentioned. Specifically, a homogeneous architecture aug-mentation algorithm is proposed in HAAP to generate suf-ficient training data taking the use of homogeneous repre-sentation. Furthermore, the one-hot encoding strategy is introduced into HAAP to make the representation of DNN architectures more effective. The experiments have been conducted on both NAS-Benchmark-101 and NAS-Bench-201 dataset. The experimental results demonstrate that the proposed HAAP algorithm outperforms the state of the arts compared, yet with much less training data. In addi-tion, the ablation studies on both benchmark datasets have also shown the universality of the homogeneous architec-ture augmentation. Our code has been made available at https://github.com/lyq998/HAAP. 1.

Introduction
Deep Neural Networks (DNNs) have been successfully applied to various challenging real-world problems, includ-ing image classification [16], natural language process-ing [8], speech recognition [40], to name a few. Well-designed architectures of DNNs are generally viewed as
*Equal contribution.
â€ Corresponding author. the deciding factor that the DNNs can achieve promising performance. Traditionally, designing architecture is nev-ertheless time-consuming, and is often exerted by experts with rich knowledge in both the DNNs and the task do-main. Neural Architecture Search (NAS) is an automatic way to design the architectures of DNNs without such kind of expertise.
Generally, the NAS algorithm is composed of three dif-ferent parts: search space, search strategy and performance estimation [10]. Particularly, given a search space prede-fined, the search strategy should find a well-performed ar-chitecture when the search terminates. In literature, there are mainly three techniques used as the search strategies: reinforcement learning [32], gradient-based algorithms [21] and Evolutionary Computation (EC) [1]. However, no mat-ter which technique is used, all NAS algorithms need to estimate the performance of the DNNs searched in order for the effective proceeding of search. In most NAS algo-rithms, the search strategy evaluates the solutions individ-ually, which inevitably gives rise to heavy computational overheads. For instance, on the commonly used CIFAR-10 benchmark dataset [19], the Large-Scale evolution NAS algorithm [24] used 250 GPU computers for 11 days. Not only that, the NAS algorithm [41] used 800 GPUs for nearly one month. Unfortunately, this is unaffordable for most aca-demic researchers. To address this issue, various algorithms have been designed to speed up the estimation process with-out numerous computation resource, which can be classified into four different categories: weight inheritance [24], early stopping policy [29, 14], reduced training set [25] and neu-ral predictor [33].
The neural predictor requires a number of well-trained architectures, and then a regression model is trained to map the architectures and the corresponding performance val-ues. When a new DNN is generated, its performance is di-rectly predicted by the regression model. Compared to other speed-up algorithms mentioned above, the neural predictors can provide satisfactory prediction result, thus is popular among the community.
Many previous works focused on designing better re-gression models to improve the performance of predic-tors [28, 34]. They try to achieve a better fit from the per-spective of the regression model. However, the big issue in neural predictor is actually in the training dataset. In prac-tice, it is prohibitively unaffordable to obtain a large train-ing dataset. Even with the ultra high performance hardware, for example, it will take about 32 minutes to train a neural network on the TPU v2 accelerator [38], which means only nearly 45 annotated training data can be obtained one day.
Generally, neural predictor is trained with a small dataset of annotated architectures, e.g., Wen et al. [34] only used 119 annotated architectures to build the neural predictor in practice. The main reason for the poor performance of the neural predictor is not the regression model, but the limited training data. As a result, how to make full use of the exist-ing limited data without increasing the computational cost is an important issue in neural predictor.
In this paper, we propose a novel data augmentation strategy in the space of neural architectures by exploring their Homogeneous properties. The Homogeneous augmen-tations eliminate the influence of layer order, making the neural predictor pay more attention to the overall layer type.
Specifically, the architecture augmentation works by swap-ping the inner orders to generate a group of homogeneous representations. To effectively represent the intrinsic prop-erties of architectures, one-hot encoding strategy is also de-veloped. The flowchart of the proposed Homogeneous Ar-chitecture Augmentation for Neural Predictor (HAAP) is shown in Fig. 1. The experiments on NAS-Bench-101 [38] and NAS-Bench-201 [9] demonstrate the effectiveness of the proposed architecture augmentation method, and show the superiority of HAAP compared with the state of the arts.
Not only that, the Homogeneous augmentations can also be combined with the state-of-the-art neural predictors to im-prove their predictions.
The reminder of this paper is organized as follows. The related works are provided in Sec. 2. Sec. 3 gives the imple-mentation details of the proposed approach. Experiments and extensive experimental results demonstrate the effec-tiveness of HAAP in Sec. 4. Finally, Sec. 5 is for the con-clusion and future works. 2.