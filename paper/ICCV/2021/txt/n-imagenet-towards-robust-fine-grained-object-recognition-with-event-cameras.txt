Abstract
We introduce N-ImageNet, a large-scale dataset targeted for robust, ﬁne-grained object recognition with event cam-eras. The dataset is collected using programmable hard-ware in which an event camera consistently moves around a monitor displaying images from ImageNet. N-ImageNet serves as a challenging benchmark for event-based object recognition, due to its large number of classes and sam-ples. We empirically show that pretraining on N-ImageNet improves the performance of event-based classiﬁers and helps them learn with few labeled data.
In addition, we present several variants of N-ImageNet to test the robust-ness of event-based classiﬁers under diverse camera trajec-tories and severe lighting conditions, and propose a novel event representation to alleviate the performance degrada-tion. To the best of our knowledge, we are the ﬁrst to quanti-tatively investigate the consequences caused by various en-vironmental conditions on event-based object recognition algorithms. N-ImageNet and its variants are expected to guide practical implementations for deploying event-based object recognition algorithms in the real world. 1.

Introduction
Event cameras are neuromorphic vision sensors that en-code visual information as a sequence of events, and have a myriad of beneﬁts such as high dynamic range, low en-ergy consumption, and microsecond-scale temporal reso-lution. However, algorithms for processing event data are still at their nascency. This is primarily due to the lack of a large, ﬁne-grained dataset for training and evaluating different event-based vision algorithms. While the num-ber of event camera datasets surged in the past few years, many ﬁne-grained datasets lack size [39], whereas large-scale real-world datasets lack label diversity [50]. Large amounts of publicly available data are one of the key fac-tors in the recent success of computer vision. For example,
ImageNet [46] triggered the development of accurate, high
Figure 1: Sample events from N-ImageNet displayed along with their RGB counterparts from ImageNet [46]. Positive, negative events are shown in blue and red, respectively. performance object recognition algorithms [15, 23] whereas
MS-COCO [29] led to the advent of eloquent image cap-tioning systems [60].
We provide N-ImageNet, an event camera dataset tar-geted for object recognition that surpasses all existing datasets in both size and label granularity as summarized in Table 1 and Figure 2. Since it is infeasible to manu-ally obtain real-world instances of thousands of object cat-egories, we opt to generate events by moving the sensor in front of an LCD monitor which displays images from Ima-geNet [46] as in [39, 27] using programmable hardware.
N-ImageNet is projected to function as a challenging benchmark for event-based object recognition algorithms.
As shown in Table 1, evaluations of various classiﬁers on
N-ImageNet demonstrate a large room for improvement, in contrast to popular benchmarks such as N-Cars [50] and N-Caltech101 [39]. We also experimentally justify the effec-tiveness of N-ImageNet pretraining. Models pretrained on
N-ImageNet show a large amount of performance gain in various object recognition benchmarks, and are capable of
Dataset
N-Cars [50]
N-Caltech101 [39]
CIFAR10-DVS [27]
ASL-DVS [4]
N-MNIST [39]
MNIST-DVS [49]
N-SOD [44]
DVS128-Gesture [2]
N-ImageNet
# of
Samples 24029 8709 10000 100800 70000 30000 189 1342 1781167
# of
Classes
Top
Accuracy
Robustness
Quantiﬁable? 2 101 10 24 10 10 4 11 1000 95.8 [50] 90.6 [13] 69.2 [47] 94.6 [20] 99.2 [20] 99.1 [20] 97.14 [44] 99.62 [19] 48.93
⨉
⨉
⨉
⨉
⨉
⨉
⨉
⨉
◯
Table 1: Comparison of N-ImageNet with other existing benchmarks for event classiﬁcation. rapidly generalizing to new datasets even with a small num-ber of training samples.
We further analyze the robustness of event-based object recognition algorithms amidst changes in camera trajecto-ries and lighting conditions. Event cameras can operate in highly dynamic scenes and low light environments, but events produced in such conditions tend to have more noise and artifacts from motion blur [9]. We record variants of
N-ImageNet under diverse camera trajectories and light-ing, and quantify the signiﬁcant performance degradation of event-based classiﬁers under environment changes. To the best of our knowledge, our dataset is the ﬁrst event cam-era dataset capable of providing quantitative benchmarks for robust event-based object recognition, as shown in Ta-ble 1. In addition, we propose a simple event representation, called DiST (Discounted Sorted Timestamp Image), that shows improved robustness under the external variations.
DiST penalizes events that are more likely to be noise, and uses sorted indices of event timestamps to ensure durability against speed change.
To summarize, our main contributions are (i) N-ImageNet, the largest ﬁne-grained event camera dataset to date, thus serving as a challenging benchmark, (ii) N-ImageNet pretraining which leads to considerable perfor-mance improvement, (iii) N-ImageNet variants that en-able quantitative robustness evaluation of event-based ob-ject recognition algorithms, and (iv) an event camera rep-resentation exhibiting enhanced robustness in diverse envi-ronment changes. 2.