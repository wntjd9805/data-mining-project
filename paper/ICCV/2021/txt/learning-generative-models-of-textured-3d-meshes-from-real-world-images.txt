Abstract 1.

Introduction
Recent advances in differentiable rendering have sparked an interest in learning generative models of tex-tured 3D meshes from image collections. These models natively disentangle pose and appearance, enable down-stream applications in computer graphics, and improve the ability of generative models to understand the concept of image formation. Although there has been prior work on learning such models from collections of 2D images, these approaches require a delicate pose estimation step that ex-ploits annotated keypoints, thereby restricting their appli-cability to a few speciﬁc datasets. In this work, we propose a GAN framework for generating textured triangle meshes without relying on such annotations. We show that the per-formance of our approach is on par with prior work that relies on ground-truth keypoints, and more importantly, we demonstrate the generality of our method by setting new baselines on a larger set of categories from ImageNet – for which keypoints are not available – without any class-speciﬁc hyperparameter tuning. We release our code at https://github.com/dariopavllo/textured-3d-gan
Most of the recent literature in the ﬁeld of generative models focuses on 2D image generation [36, 54, 22, 3, 23], which largely ignores the fact that real-world images depict 2D projections of 3D objects. Constructing 3D generative models presents multiple advantages, including a fully dis-entangled control over shape, appearance, pose, as well as an explicit representation of spatial phenomena such as oc-clusions. While the controllability aspect of 2D generative models can be improved to some extent by disentangling factors of variation during the generation process [53, 40, 21, 22], the assumptions made by these approaches have been shown to be unrealistic without an inductive bias [33].
It is thus unclear whether 2D architectures can reach the same degree of controllability as a native 3D representation.
As a result, a growing line of research investigates learn-ing textured 3D mesh generators in both GAN [39, 4] and variational settings [15]. These approaches are trained with 2D supervision from a collection of 2D images, but require camera poses to be known in advance as learning a joint dis-tribution over shapes, textures, and cameras is particularly difﬁcult. Usually, the required camera poses are estimated from keypoint annotations using a factorization algorithm
such as structure-from-motion (SfM) [35]. These keypoint annotations are, however, very expensive to obtain and are usually only available on a few datasets.
In this work, we propose a new approach for learning generative models of textured triangle meshes with min-imal data assumptions. Most notably, we do not require keypoint annotations, which are often not available in real-world datasets. Instead, we solely rely on: (i) a single mesh template (optionally, a set of templates) for each image cat-egory, which is used to bootstrap the pose estimation pro-cess, and (ii) a pretrained semi-supervised object detector, which we modify to infer semantic part segmentations on 2D images. These, in turn, are used to augment the initial mesh templates with a 3D semantic layout that allows us to reﬁne pose estimates and resolve potential ambiguities.
First, we evaluate our approach on benchmark datasets for this task (Pascal3D+ [31] and CUB [45]), for which key-points are available, and show that our approach is quantita-tively on par with the state-of-the-art [39] as demonstrated by FID metrics [16], even though we do not use keypoints.
Secondly, we train a 3D generative model on a larger set of categories from ImageNet [6], where we set new base-lines without any class-speciﬁc hyperparameter tuning. To our knowledge, no prior works have so far succeeded in training textured mesh generators on real-world datasets, as they focus either on synthetic data or on simple datasets where poses/keypoints are available. We also show that we can learn a single generator for all classes (as opposed to different models for each class, as done in previous work
[39, 4, 15]) and notice the emergence of interesting disen-tanglement properties (e.g. color, lighting, style), similar to what is observed on large-scale 2D image generators [3].
Finally, we quantitatively evaluate the pose estimation performance of our method under varying assumptions (one or more mesh templates; with or without semantic informa-tion), and showcase a proof-of-concept where 3D meshes are generated from sketches of semantic maps (seman-tic mesh generation), following the paradigm of image-to-image translation. In summary, our main contributions are as follows:
• We introduce a new approach to 3D mesh generation that does not require keypoint annotations, enabling its use on a wider range of datasets as well as new image categories.
• We showcase 3D generative models in novel settings, in-cluding learning a single 3D generator for all categories, and conditional generation from semantic mesh layouts.
In addition, we provide a preliminary analysis of the dis-entanglement properties learned by these models.
• We propose a comprehensive 3D pose estimation frame-work that combines the merits of template-based ap-proaches and semantic-based approaches. We further ex-tend this framework by explicitly resolving pose ambigu-ities and by adding support to multiple templates. 2.