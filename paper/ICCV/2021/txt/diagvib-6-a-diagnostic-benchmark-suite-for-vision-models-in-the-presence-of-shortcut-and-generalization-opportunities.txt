Abstract
Common deep neural networks (DNNs) for image clas-siﬁcation have been shown to rely on shortcut opportunities (SO) in the form of predictive and easy-to-represent visual factors. This is known as shortcut learning and leads to im-paired generalization. In this work, we show that common
DNNs also suffer from shortcut learning when predicting only basic visual object factors of variation (FoV) such as shape, color, or texture. We argue that besides shortcut op-portunities, generalization opportunities (GO) are also an inherent part of real-world vision data and arise from par-tial independence between predicted classes and FoVs. We also argue that it is necessary for DNNs to exploit GO to overcome shortcut learning. Our core contribution is to in-troduce the Diagnostic Vision Benchmark suite DiagViB-6, which includes datasets and metrics to study a network’s shortcut vulnerability and generalization capability for six independent FoV. In particular, DiagViB-6 allows control-ling the type and degree of SO and GO in a dataset. We benchmark a wide range of popular vision architectures and show that they can exploit GO only to a limited extent.
) 2 r o t c a f e p a h s d e t c i d e r p ( 4 3 blue red green hue factor
? test (OOD) shortcut opp. gen. opp. train
Figure 1: Exemplar study in our proposed benchmark. The network is trained to predict factor classes 2, 4, 3 for the shape factor with varying hue. All ﬁve depicted train-ing combinations are uniformly shown during training. The shape 2 co-occurs solely with the blue class of the hue factor, which poses a shortcut opportunity. The shapes 4 and 3 occur uniformly with hue red and green; these combinations pose a generalization opportunity, since they reduce the predictiveness of the hue factor for the shape factor. Test accuracy is computed on examples from OOD factor combinations to evaluate a model’s shortcut vulnera-bility in the context of the given generalization opportunity. 1.

Introduction
Despite their state-of-the-art performance on object clas-siﬁcation tasks, deep neural networks (DNN) are highly prone to shortcut learning [8, 33, 11]. Instead of learning holistic representations and decision rules that can gener-alize beyond the training data, DNNs overly rely on so-called shortcut opportunities (SO), which occur when the target class is highly correlated to one or very few easy-to-represent input factors [12]. This leads to poor generaliza-tion on many out-of-distribution (OOD) settings, e.g. Ima-geNet trained DNNs are biased towards texture and fail to generalize under texture-shape cue conﬂict evaluation [8].
While humans are also prone to shortcut learning in cer-*Corresponding authors. Contact via elias@eeulig.com or volker.fischer@de.bosch.com. tain cases, such as object classiﬁcation under context-based cue conﬂict settings [29], the biological model remains largely unaffected by shortcuts when it comes to predict-ing basic object factors of variation (FoV) such as shape, hue or texture. This “shortcut-immunity” w.r.t. basic
FoV is just as necessary for intelligent systems; thus, ef-forts towards improving model generalization are of utmost importance.
Existing literature studies shortcut behavior in DNNs mainly in the context of object classiﬁcation. In this work, we address a more fundamental variant of shortcut learning, focusing speciﬁcally on the prediction of basic FoV them-selves, similar to [12]. In the context of FoV prediction, we refer to different manifestations of a factor as factor classes.
For example, “red” and “green” are factor classes for the factor hue, whereas “circle” and “elephant shape” are fac-tor classes for the factor shape. Object classes such as
“elephant” or “car” are characterized by the co-occurrence of certain factor classes, e.g. “gray”, “elephant shape”, and
“elephant texture” characterize an elephant. SO arise from this co-occurrence of different factor classes.
As stated in [7], SO are a property of real-world vision data. In this work, we propose to additionally consider gen-eralization opportunities (GO), which are a relaxation of the strict correlation (or co-occurrence) between a target class and an input FoV at training time. For instance, consider the object “car” with factors shape and color; cars ap-pearing in different colors during training would induce a
GO compared to cars appearing only in one color. We refer to such cases as compositional-based GO. Correlations can also be violated in the form of outliers consisting of rare combinations of factor classes, e.g. a “white elephant”. We refer to these cases as frequency-based GO.
A straightforward approach to introduce GO in the train-ing data is data augmentation. For example, [21] applies random color transformations to the training images, re-moving a potential correlation between the target class and the factor color. However, data augmentation results in models that are invariant with respect to the augmented fac-tor. Such models lose important information that may be needed to properly identify and reason about OOD sam-ples. In contrast to being invariant, we argue that a good vi-sion model needs to have an explicit representation of these
FoV. Our work aims at analyzing a model’s capability to exploit the GO already present in the data, as opposed to adding more GO to a dataset, as done in data augmenta-tion. While several synthetic benchmark datasets for com-positional generalization and cue-conﬂict settings already exist [12, 2, 28], none of them enables sufﬁcient and sys-tematic control over the SO and GO present in the dataset for a broad set of different visual object FoV.
Inspired by prior work on shortcut learning and compo-sitional generalization [7, 12], we present a synthetic but diagnostic benchmark suite DiagViB-61that includes dif-ferent studies to evaluate a model’s shortcut vulnerabil-ity under varying degrees of GO. Figure 1 illustrates an exemplar study in our benchmark. The benchmark suite contains an image-generating function that allows direct and independent control over the six basic, visual object
FoV: position, hue, lightness, scale, shape, and texture (Fig. 2). Additionally, our framework pro-vides a dataset-generating function that enables a user to control the nature of SO and GO appearing in a dataset.
This is achieved by introducing different degrees of corre-lation between factors, and inducing co-occurrences of cer-tain factor class combinations. Furthermore, the benchmark suite provides metrics to evaluate a model’s shortcut vulner-ability under different GO for each factor. 1https://github.com/boschresearch/diagvib-6
Figure 2: Image space traversal across all six FoV and four corresponding class labels used in this work. Along each column only the corresponding factor is varied, while all others are ﬁxed. Note that some factors have more than the four classes shown here (refer Tab. 1).
We evaluate a wide range of common deep learning vi-sion models on our benchmark and perform an exhaustive investigation of their shortcut vulnerability w.r.t. the six stated FoV. We show that while they exploit frequency GO, they exploit the more relevant compositional GO only to a limited extent. This holds true also for approaches speciﬁ-cally designed to counteract shortcut learning.
We admit that this benchmark suite does not sufﬁciently and directly prove a vision model’s ability to generalize on real-world data (e.g. object classiﬁcation on ImageNet).
However, it serves as a critical diagnosis that is necessary in order to study a model’s shortcut vulnerability and gen-eralization ability under various controlled tasks and data setups. Ultimately, the design of our benchmark suite al-lows a user to control different degrees of SO and GO in a dataset (not commonly available in real-world data), in or-der to assess a model’s behavior under different conditions.
Our contributions in this work can be summarized as fol-lows: We propose a benchmark suite to create datasets that enable the user to independently combine six visual FoV, al-lowing explicit control over which SO and GO are present in the resulting data. We establish suitable metrics to evalu-ate both a model’s shortcut vulnerability, and its capability to exploit GO in the data. Lastly, we provide empirical evi-dence that common vision architectures exploit GO only to a limited extent, especially compositional-based GO. 2.