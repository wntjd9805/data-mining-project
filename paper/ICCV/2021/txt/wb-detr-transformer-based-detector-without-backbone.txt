Abstract
Transformer-based detector is a new paradigm in ob-ject detection, which aims to achieve pretty-well perfor-mance while eliminates the priori knowledge driven com-ponents, e.g., anchors, proposals and the NMS. DETR, the state-of-the-art model among them, is composed of three sub-modules, i.e., a CNN-based backbone and paired trans-former encoder-decoder. The CNN is applied to extract lo-cal features and the transformer is used to capture global contexts. This pipeline, however, is not concise enough. In this paper, we propose WB-DETR (DETR-based detector
Without Backbone) to prove that the reliance on CNN fea-tures extraction for a transformer-based detector is not nec-essary. Unlike the original DETR, WB-DETR is composed of only an encoder and a decoder without CNN backbone.
For an input image, WB-DETR serializes it directly to en-code the local features into each individual token. To make up the deﬁciency of transformer in modeling local informa-tion, we design an LIE-T2T (local information enhancement tokens to token) module to enhance the internal information of tokens after unfolding. Experimental results demonstrate that WB-DETR, the ﬁrst pure-transformer detector without
CNN to our knowledge, yields on par accuracy and faster inference speed with only half number of parameters com-pared with DETR baseline. 1.

Introduction
CNN-based approaches [18] have dominated object de-tection tasks [20, 32] for years. In these methods, a com-mon component is the backbone network [12, 13, 14, 35],
*Equal contribution.
†Corresponding author is Wenzhe Zhao.
Figure 1. DETR vs. WB-DETR. (a) DETR ﬁrst uses a CNN net-work to extract features, and then utilizes a transformer structure for object detection. (b) WB-DETR serializes the image and uses transformer to detect object directly. acting as extracting image features by a series of convo-lution and pooling layers. Modern CNN-based detectors
[9, 11, 27, 21, 36, 29, 23, 25, 26, 22] regard the detector de-sign as a modules combination process, which always com-posed of a backbone, a neck [21] and multiple detection heads [3]. Among which, the backbone has become a de facto standard to improve the performance and the design of various backbones is also a focus of research in the ﬁeld of object detection. As we all know, the equipment of a backbone is essential for existing CNN-based detectors.
To get out of the paradigm of CNN-based design, Car-ion et al. propose a novel detector named DETR [4]. Un-like previous CNN-based works, DETR is a transformer-based detector [4, 40, 5, 33], which eliminates many hand-crafted operations [4], e.g., anchor generation, rule-based object assignment, non-maximum suppression (NMS) post-processing, and so on. As shown in Figure 1 (a), DETR
In other adjacent tokens in space are not well modeled. words, transformer lacks the ability of local information modeling. Although the T2T [37] module can aggregate the contexts of adjacent tokens, it is unable to model the inter-nal information of the aggregated independent token sep-arately, as illustrated in Figure 2 (a). Accordingly, along with WB-DETR, we present LIE-T2T (Local Information
Enhancement-T2T) module. As shown in Figure 2 (b), LIE-T2T not only reorganizes and unfolds the adjacent tokens, but also calculates the attention on the channel-dimension of each token after unfolding. Because the tokens are ob-tained from feature map through unfold operation, model-ing the relationship between channels of the tokens is equiv-alent to modeling the spatial relationship between the pixels in feature map. That is why channel attention in LIE-T2T can enhance local information.
In a word, we propose WB-DETR (DETR-Based De-tector without Backbone), which is only composed of an encoder and a decoder without the backbone.
Instead of utilizing a CNN to extract features, WB-DETR serializes the image directly, encoding the local features of input into each individual token. Besides, to allow WB-DETR bet-ter make up the deﬁciency of transformer in modeling local information, we design LIE-T2T (Local Information En-hancement Tokens-to Token) module to modulate the inter-nal (local) information of each token after unfolding. Com-pared with the DETR baseline, WB-DETR without back-bone is more unify and neat. We encourage researchers to rethink the modules combination (backbone-neck-head) de-sign paradigm for object detection. 2.