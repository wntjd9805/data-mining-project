Abstract
While convolutional neural networks have shown a tremendous impact on various computer vision tasks, they generally demonstrate limitations in explicitly modeling long-range dependencies due to the intrinsic locality of
Initially designed for natural the convolution operation. language processing tasks, Transformers have emerged as alternative architectures with innate global self-attention mechanisms to capture long-range dependencies.
In this paper, we propose TransDepth, an architecture that bene-ﬁts from both convolutional neural networks and transform-ers. To avoid the network losing its ability to capture local-level details due to the adoption of transformers, we pro-pose a novel decoder that employs attention mechanisms based on gates. Notably, this is the ﬁrst paper that ap-plies transformers to pixel-wise prediction problems involv-ing continuous labels (i.e., monocular depth prediction and surface normal estimation). Extensive experiments demon-strate that the proposed TransDepth achieves state-of-the-art performance on three challenging datasets. Our code is available at: https://github.com/ygjwd12345/
TransDepth. 1.

Introduction
Over the past decade, convolutional neural networks have become the privileged methodology to address fun-damental and challenging computer vision tasks requiring dense pixel-wise prediction, such as semantic segmenta-tion [6, 21], monocular depth prediction [39, 18], and nor-mal surface computation [43]. Since the seminal work of
[27], existing depth prediction models’ have been domi-nated by encoders implemented with architectures such as
ResNet and VGG-Net. The encoder progressively reduces the spatial resolution and learns more concepts with larger receptive ﬁelds. Because context modeling is critical for pixel-level prediction, deep feature representation learning is arguably the most critical model component [5]. How-ever, it is still challenging for depth prediction networks to improve their ability in modeling global contexts. Tra-ditionally, both stacked convolution layers and consecutive down-sampling are used in the encoders to generate sufﬁ-ciently large receptive ﬁelds of deep layers. This problem is typically circumvented rather than resolved to some extent.
Unfortunately, existing strategies bring several drawbacks: (1) the training of very deep nets is affected by the fact that consecutive multiplications wash out low-level features; (2) the local information crucial to dense prediction tasks is dis-carded since the spatial resolution is reduced gradually. To overcome these limitations, several methods have been re-cently proposed. One solution is manipulating the convolu-tional operation directly by using for example large kernel sizes [42], atrous convolutions [5], and image/feature pyra-mids [71]. Another solution is to integrate attention mod-ules into the fully convolutional network architecture. Such a module aims to model global interactions of all pixels in the feature map [60]. When applied to monocular depth prediction [65, 64] a general approach is to combine the attention module with a multi-scale fusion method. More recently, Huynh et al. [31] proposed a depth-attention vol-ume to incorporate a non-local coplanarity constraint to the network. Guizilini et al. [26] rely on a ﬁxed pre-trained se-mantic segmentation network to guide global representation learning. Though these methods’ performance is improved signiﬁcantly, still the above mentioned issues persist.
Transformers were initially used to model sequence-to-sequence predictions in NLP tasks to obtain a larger re-ceptive ﬁeld and have recently attracted tremendous inter-est in the computer vision community. The ﬁrst purely self-attention-based Vision Transformer (ViT) for image recognition was proposed in [16] attaining excellent re-sults on ImageNet compared with the convolutional net-works. Moreover, SETR [72] replaces the encoders with pure Transformers, obtaining competitive results on the
CityScapes dataset. Interestingly, we found that a SETR-like pure Transformer-based segmentation network pro-duces unsatisfactory performance due to the lack of spatial inductive bias in modeling the local information. Mean-while, most previous methods based on deep feature rep-resentation learning fail to solve this problem. Nowadays, only few researchers [3] are considering combining the
CNNs with Transformers to create a hybrid structure to combine their advantages.
In contrast to treating pixel-level prediction tasks as a sequence-to-sequence prediction problem, we ﬁrstly pro-pose to embed Transformers into the ResNet backbone in order to model semantic pixel dependencies. Moreover, we design a new and effective uniﬁed attention gate decoder to address the drawback that the pure linear Transformer’s em-bedding feature lacks spatial inductive bias in capturing the local representation. We show empirically that our method offers a new perspective in model design and achieves state-of-the-art on several challenging benchmarks.
To summarize, our contribution is threefold:
• We are the ﬁrst to propose the use of Transformers for both monocular depth estimation and surface normal pre-diction tasks. Transformers can successfully improve the ability of traditional convolutional neural networks to model long-range dependencies.
• We propose a novel and effective uniﬁed attention gate structure designed to utilize and fuse multi-scale infor-mation in a parallel manner and pass information among different afﬁnities maps in the attention gate decoders for better modeling the multi-scale afﬁnities.
• We conduct extensive experiments on two distinct pixel-wise prediction tasks with three challenging datasets (e.g.,
NYU [47], KITTI [22], and ScanNet [11]), demonstrat-ing that our TransDepth outperforms previous methods on KITTI (0.956 on δ < 1.25), NYU depth (0.900 on
δ < 1.25), and achieves new state-of-the-art results on
NYU surface normal estimation. 2.