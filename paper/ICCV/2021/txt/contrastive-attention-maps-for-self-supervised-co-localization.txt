Abstract
The goal of unsupervised co-localization is to locate the object in a scene under the assumptions that 1) the dataset consists of only one superclass, e.g., birds, and 2) there are no human-annotated labels in the dataset. The most recent method achieves impressive co-localization perfor-mance by employing self-supervised representation learn-ing approaches such as predicting rotation.
In this pa-per, we introduce a new contrastive objective directly on the attention maps to enhance co-localization performance.
Our contrastive loss function exploits rich information of location, which induces the model to activate the extent of the object effectively. In addition, we propose a pixel-wise attention pooling that selectively aggregates the fea-ture map regarding their magnitudes across channels. Our methods are simple and shown effective by extensive qual-itative and quantitative evaluation, achieving state-of-the-art co-localization performances by large margins on four datasets: CUB-200-2011, Stanford Cars, FGVC-Aircraft, and Stanford Dogs. Our code will be publicly available on-line for the research community. 1.

Introduction
Object localization aims to capture the location of the target object in a given image. Over the past decade, deep learning approaches have become mainstream in object lo-calization. These methods typically train a convolutional neural network (CNN) with human-annotated locations in the form of bounding boxes [24, 29, 30]. This has shown great performance but has the downside that the location annotations on all images are too expensive.
To alleviate this, object localization with weaker super-vision, such as image-level class labels [43] or dataset-level superclass label [2, 37], has drawn a lot of attention recently.
*Work done as a research scientist at NAVER AI Lab.
Figure 1: Contrastive learning framework for image co-localization. An encoder embeds two views of one image into feature maps which become attention maps by channel-wise pooling. Then we train the encoder by contrastive ob-jective on the attention maps, which preserve signals from different locations, and by classification objective on the feature maps for the pretext task.
In general, the former is called weakly-supervised object lo-calization (WSOL), whereas we refer to the latter as image co-localization. This paper focuses on the problem of im-age co-localization, which aims to locate common objects in a dataset consisting of only one superclass.
Existing image co-localization methods can be divided into two categories: multiple instance learning (MIL) and self-supervised representation learning (SSL). MIL-based methods [17, 33] firstly generate candidate boxes and then identify if each box contains the target object using hand-crafted features. These approaches require high computa-tional costs for inference, making it difficult to operate in real-time.
On the other hand, the SSL-based methods employ im-age transformation as a pretext task. If the selected trans-form is rotation, the model learns to predict the amount of rotation applied to the image.
Interestingly, the attention map from the learned representation is strongly activated at the location of the target object. The current state-of-the-art method [2] trains the network by cross-entropy for classify-ing artificial labels from the pretext task.
However, Figure 2 illustrates that the activations smear in the backgrounds, which hamper the co-localization per-formance. We suppose one of the reasons be the dis-crepancy between the goals of classification and localiza-tion [9, 32]: the classification loss function trains the model to learn the task-relevant information. We believe that the co-localization performance can be further improved by ad-ditional location-related information.
Contrastive learning [13] became popular in self-supervised representation learning in recent years. How-ever, it is not straightforward to adapt it into image co-localization because current state-of-the-art methods en-code the transformed images into feature vectors by neu-ral networks [3, 14] and the feature vector does not contain spatial information.
To adopt image co-the contrastive learning for localization, we believe that three questions need to be an-swered: (1) how do we encode the input image into the em-beddings that contain spatial information? (2) how do we define positive and negative pairs of embeddings for con-trastive learning? and (3) which image transformation do we use?
To this end, we propose a contrastive learning framework (Figure 1) for image co-localization considering these three questions. First, we aggregate the last convolutional fea-ture map of across the channels to generate an attention map which will serve as an embedding for the contrastive frame-work. It allows the contrastive framework not to lose spatial information. Specifically, we introduce a simple and effec-tive pooling method that chooses the contributed channels separately in each location for computing attention maps.
Second, we maximize the similarity between the attention map of the input original image and the inverse transformed attention map of the transformed image, and maximize the dissimilarity between the former and the attention map on the background. It makes the attention map contain the ex-tent of the object more accurately. Last, we explore vari-ous image transformations for the positive pairs. Then, we suggest the optimal combination for image co-localization.
Overview of our framework is illustrated in Figure 3.
We demonstrate the effectiveness of the proposed method through extensive experiments. Qualitative evalu-ation results show that our method can localize the full ex-tent of the object and ignore the background.
In quanti-tative evaluation, our method achieves new state-of-the-art
Figure 2: Activation maps extracted from baseline and our model. The baseline method trains the network using only the classification loss. Thus, it activates even on back-grounds to predict the amount of transformations. The red boxes are the ground truth, and the green boxes are the pre-dictions. localization performances on CUB-200-2011 [34], Stanford
Cars [20], FGVC-Aircraft [26], and Stanford Dogs [18].
In summary, our main contributions are:
• We propose a novel way to adopt a contrastive learning framework for image co-localization. The proposed framework successfully leads the model to learn the full extent of the target object.
• We extensively study how to adopt contrastive learning for image co-localization: (1) the definition of positive and negative pairs, (2) a simple yet effective attention extraction method, and (3) an optimal combination of image transformations.
• Our method achieves new state-of-the-art localization performances with significant margins on four differ-ent benchmark datasets. Consistent results are ob-served through qualitative evaluation. 2.