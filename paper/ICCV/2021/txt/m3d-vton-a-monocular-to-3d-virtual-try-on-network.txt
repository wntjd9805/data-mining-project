Abstract
Virtual 3D try-on can provide an intuitive and realistic view for online shopping and has a huge potential com-mercial value. However, existing 3D virtual try-on meth-ods mainly rely on annotated 3D human shapes and gar-ment templates, which hinders their applications in prac-tical scenarios. 2D virtual try-on approaches provide a faster alternative to manipulate clothed humans, but lack the rich and realistic 3D representation. In this paper, we propose a novel Monocular-to-3D Virtual Try-On Network (M3D-VTON) that builds on the merits of both 2D and 3D approaches. By integrating 2D information efﬁciently and learning a mapping that lifts the 2D representation to 3D, we make the ﬁrst attempt to reconstruct a 3D try-on mesh only taking the target clothing and a person image as inputs.
The proposed M3D-VTON includes three modules: 1) The
Monocular Prediction Module (MPM) that estimates an initial full-body depth map and accomplishes 2D clothes-person alignment through a novel two-stage warping proce-dure; 2) The Depth Reﬁnement Module (DRM) that reﬁnes the initial body depth to produce more detailed pleat and face characteristics; 3) The Texture Fusion Module (TFM) that fuses the warped clothing with the non-target body part to reﬁne the results. We also construct a high-quality syn-thesized Monocular-to-3D virtual try-on dataset, in which each person image is associated with a front and a back depth map. Extensive experiments demonstrate that the pro-posed M3D-VTON can manipulate and reconstruct the 3D human body wearing the given clothing with compelling de-tails and is more efﬁcient than other 3D approaches. 1 1.

Introduction 3D virtual try-on, the process of ﬁtting a speciﬁc clothing item onto a 3D human shape, has attracted increasing atten-tion due to its promising research and commercial value.
Recently, researchers’ interest has moved from physics-1code will be available at https://github.com/fyviezhao/
M3D-VTON
based [2, 5, 6, 42, 13, 15] or scan-based approaches [37, 27, 44] to learning-based 3D try-on methods [3, 35, 31, 55, 8], dressing a 3D person directly from 2D images and getting rid of costly physics simulation or 3D sensors. However, most of these learning methods [3, 35, 31] build on the parametric SMPL [29] model and depend on some prede-ﬁned digital wardrobe [3], limiting their real-world applica-bility. Moreover, the inference speed of these existing 3D approaches is still insufﬁcient, largely due to the optimiza-tion cost introduced by the parametric 3D representation.
Related to this, research on image-based virtual try-on aims to ﬁt an in-shop clothing onto the target person and has been explored intensively [17, 48, 52, 16, 51, 22, 9]. Most of these works utilize the Thin Plate Spline (TPS) trans-formation [4] to achieve the clothes-person alignment and fusion, obtaining photo-realistic try-on results. These 2D methods are attractive due to their small computation cost and extensive amount of available training data on shopping websites. Nevertheless, their try-on results are in 2D image space and ignore the underlying 3D body information, lead-ing to inferior capability of representing the human body.
To address the above limitation of 2D/3D approaches, we propose a light-weight yet effective Monocular-to-3D
Virtual Try-On Network (M3D-VTON), which integrates both 2D image-based virtual try-on and 3D depth estima-tion to reconstruct the ﬁnal 3D try-on mesh. M3D-VTON consists of three modules as shown in Fig. 2. The ﬁrst part is the Monocular Prediction Module (MPM), which uti-lizes a single network to serve the following three purposes: 1) regressing the parameters for the TPS [4] transforma-tion; 2) predicting the conditional person segmentation that is compatible with the in-shop clothing; 3) estimating the full-body depth map. Different from the warping operation in existing 2D try-on methods, MPM ﬁrst utilizes a novel self-adaptive afﬁne transformation to transform the in-shop clothing to the appropriate size and location before the non-rigid TPS deformation. The second part is the Depth Re-ﬁnement Module (DRM), which jointly uses the estimated depth map, the warped clothing, the non-target body part and the image gradient information to enhance the geomet-ric details in the depth map. In particular, DRM introduces a depth gradient loss to better exploit the high-frequency details in the inputs. Finally, the Texture Fusion Module (TFM) leverages the 2D information (e.g., warped clothing) and the 3D information (e.g., estimated full-body depth) to synthesize the try-on texture. The collaborative use of the 2D information and the body depth map provides instruc-tive information for the synthesizing process. Given the es-timated 2D try-on texture and the reﬁned body depth map,
M3D-VTON obtains a colored point cloud and reconstructs the ﬁnal textured 3D virtual try-on mesh.
We conduct extensive experiments on the new MPV-3D dataset, which is constructed by running PIFuHD [41] on the existing MPV dataset [9]. Compared with other 3D try-on methods, M3D-VTON recovers detailed body shapes and realistic texture color while being more computation-ally efﬁcient. Our main contributions are:
• We are the ﬁrst to exploit the merits of both 2D and 3D approaches to solve the monocular-to-3D try-on prob-lem. Our approach reconstructs realistic 3D clothed humans while being faster than pure 3D methods.
• To facilitate more accurate geometric matching be-tween the clothes and the reference person image, we introduce a self-adaptive pre-alignment strategy.
• We utilize the available shadow information in the im-ages and incorporate a novel depth gradient constraint to guide the network to capture and recover intricate geometric changes.
• We construct a new synthesized 3D virtual try-on dataset, MPV-3D, which may stimulate the develop-ment of the Monocular-to-3D virtual try-on ﬁeld. Ex-tensive experiments show the surprising shape recov-ery and texture generation ability of our M3D-VTON. 2.