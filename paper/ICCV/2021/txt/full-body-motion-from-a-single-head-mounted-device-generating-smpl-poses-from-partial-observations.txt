Abstract
The increased availability and maturity of head-mounted and wearable devices opens up opportunities for remote com-munication and collaboration. However, the signal streams provided by these devices (e.g., head pose, hand pose, and gaze direction) do not represent a whole person. One of the main open problems is therefore how to leverage these sig-nals to build faithful representations of the user. In this paper, we propose a method based on variational autoencoders to generate articulated poses of a human skeleton based on noisy streams of head and hand pose. Our approach relies on a model of pose likelihood that is novel and theoreti-cally well-grounded. We demonstrate on publicly available datasets that our method is effective even from very impov-erished signals and investigate how pose prediction can be made more accurate and realistic. 1.

Introduction
Head-mounted and wearable devices are steadily increas-ing in availability and maturity. These technologies open up opportunities to build tools for remote communication and collaboration that are human-centred, and which allow us to work in the way we naturally interact when we meet in person [27, 50, 62]. Mixed reality devices, such as Mi-crosoft HoloLens, allow 3D content to be displayed and viewed in physical space with local or remote collaborators, all sharing a single coordinate system and spatial context.
However, to communicate effectively with remote collabo-rators, there is a significant and largely unsolved challenge to build faithful representations of the motion of a person wearing such a headset from only head-worn sensors. There is a high perceptual bar to meet if we are to trust such a system to represent ourselves, as we are attuned to motion that does not look human [44].
To sense the motion and actions of a user, devices such as HoloLens provide a variety of signal streams derived
*Work done during an internship at Microsoft Research. Correspon-dence to: adit@dtu.dk using computer vision; these include the location and orien-tation of the head-mounted device (HMD) relative to a world coordinate system, hand pose (location and orientation of the user’s hands relative to the HMD) and even eye track-ing signals [66]. These signal streams provide invaluable information but they do not represent a whole person. Fur-thermore, while each individual stream has its own failure rate due to detection or tracking errors, the combination of all the streams has a much higher compound failure rate, as a failure in any one subsystem can result in non-human behaviour that breaks the trust and understanding required for effective communication.
While the possibility of estimating the full body pose of a person using egocentric views is an attractive prospect on fu-ture devices [64, 52], no currently available consumer device has suitable embedded cameras. On a wearable device, each additional camera is costly in terms of power and thermal dissipation [35], and so it is advantageous for motion pre-diction systems to require as few cameras as possible. Even if future devices provide egocentric body tracking cameras, there will be an ongoing need to allow full-body representa-tions for users of legacy or lower-power devices. Solutions that predict body motion from external cameras mounted on an interacting person are also promising [46], but are limited to cases were there are multiple people interacting and participants are always visible.
To address this challenge, we require a model of human motion that is conditioned on limited low-level inputs and provides plausible inferred body poses while staying respon-sive to the signal streams. In this paper, we address an impor-tant sub-problem: reconstructing the articulated pose of a hu-man skeleton from noisy streams of head and hand pose. We use the variational autoencoder (VAE) framework [30, 51], which allows us to decompose the problem into a generative model of human pose, with an inference model that maps input signals into the learned latent embedding.
Our primary contribution is to show how to make this framework effective even from very impoverished signals: in our case the three orthogonal coordinate frames provided by a head and hand tracker. Our secondary contribution is to formulate a model of pose likelihood which is factorized
into approximately Gaussian models for each of the joints in our skeletal model; this leads to an objective function that is novel and theoretically well-grounded by the mani-fold of poses for each joint in the special Euclidean group
SE(3). Finally, we show that the accuracy of pose predic-tion can be improved in two main ways: first, by using a generative model that is pretrained on full body poses, and second, by providing a temporal history of head and hand poses to the inference model. models, such as conditional restricted Boltzmann machines
[61, 60], variational autoencoders [2, 3, 69, 73], and normal-izing flows [20]. In general, research in this area assumes that body poses observed in the past are complete—with no missing joints. In addition, it ignores the notion of a motion controller. Both of these conditions make our problem dis-tinct, i.e. body pose prediction should be from very sparse signals (i.e. the head and hands only), and these signals provide motion cues to guide prediction. 2.