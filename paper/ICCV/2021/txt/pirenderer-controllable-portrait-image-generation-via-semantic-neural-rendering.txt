Abstract
Generating portrait images by controlling the motions of existing faces is an important task of great consequence to social media industries. For easy use and intuitive con-trol, semantically meaningful and fully disentangled pa-rameters should be used as modifications. However, many existing techniques do not provide such fine-grained con-trols or use indirect editing methods i.e. mimic motions of other individuals.
In this paper, a Portrait Image Neural
Renderer (PIRenderer) is proposed to control the face mo-tions with the parameters of three-dimensional morphable face models (3DMMs). The proposed model can generate photo-realistic portrait images with accurate movements according to intuitive modifications. Experiments on both direct and indirect editing tasks demonstrate the superior-ity of this model. Meanwhile, we further extend this model to tackle the audio-driven facial reenactment task by ex-tracting sequential motions from audio inputs. We show that our model can generate coherent videos with convinc-ing movements from only a single reference image and a driving audio stream. Our source code is available at https://github.com/RenYurui/PIRender.
Figure 1. Example results produced by our PIRenderer. This model can generate photo-realistic portrait images according to the user-specified motions (top), motions of another individual (mid-dle), and motions generated from audios (bottom). 1.

Introduction
Portrait images are one of the most important photo-graphic depictions that are widely used in daily life. Be-ing able to edit portrait images by intuitively controlling the poses and expressions of given faces (see Fig. 1) is an im-portant task with a large variety of applications in the fields of virtual reality, film production, and next-generation com-munication. However, enabling such editing is extremely challenging since it requires the algorithm to perceive reli-able 3D geometric shapes of given faces. Meanwhile, the acuteness of the human visual system towards portrait im-ages requires the algorithm to generate photo-realistic faces and backgrounds, which makes the task even harder.
Recently, advances in Generative Adversarial Networks (GANs) [14] have made tremendous progress in synthe-sizing realistic faces [5, 22, 9]. Some methods [3, 42] driven from GANs tackle this task with image translation techniques, where the goal is to train a model such that the conditional distribution of the generated images given input instructions (e.g. edges) resembles that of real im-ages. Some follow-up algorithms achieve better general-ization by proposing efficient deformation modules [45, 49, 30, 8, 28, 31] or injecting source neural textures to tar-gets [41, 6]. However, the vast majority of existing methods use indirect and subject-specific motion descriptors such as edges [42, 41, 28], semantic segmentation [8], or key-points [30, 31, 28] to describe the target motions. Although these descriptors with 2D spatial information can benefit the generation of target images, they hinder the model ability to edit portraits in an intuitive manner.
To achieve intuitive control, motion descriptors should be semantically meaningful, which requires facial expres-sions, head rotations, and translations to be expressed as fully disentangled variables.
Parametric face modeling methods [4, 26] provide powerful tools for describing 3D faces with semantic parameters. These methods allow con-trolling 3D meshes over parameters such as shape, expres-sions, etc. Incorporating priors of these techniques, one can expect to control the generation of photo-realistic portrait images akin to the processing of graph rendering.
In this paper, a neural rendering model PIRenderer is proposed. Given a source portrait image and target 3DMM parameters, our model generates photo-realistic results with accurate motions. As shown in Fig. 2, the proposed model is divided into three parts: the Mapping Network, the Warp-ing Network, and the Editing Network. The mapping net-work produces latent vectors from motion descriptors. In-structed by the vectors, the warping network estimates de-formations between sources and desired targets and gener-ates coarse results by warping sources with the estimated deformations. The editing network generates the final im-ages from the coarse images. The superiority and versatility of our model are demonstrated in the experiments. We show that our model not only enables intuitive image control by editing target images with user-specified motions but also generates realistic results in the indirect portrait editing task where the goal is to mimic the motions of another individ-ual. Additionally, we show the potential of our model as an efficient neural renderer by further extending it to tackle the audio-driven facial reenactment task. Thanks to the high-level fully disentangled parameterizations, we can extract convincing motions from “weak” control audios. Experi-ments show that our model generates various and vivid mo-tions from an audio stream and transfers the motions into realistic videos of arbitrary target persons. The main contri-butions are summarized as:
• We proposed a portrait image generation model PIRen-derer, which enables intuitive photo-real editing of fa-cial expressions, head rotations, and translations.
• The proposed model can be used to tackle the indirect image editing task which requires imitating other in-dividuals. Thanks to our disentangled modifications and efficient neural renderer, we can extract subject-agnostic motions and generate realistic videos.
• Additional extension on audio-driven facial reenact-ment demonstrates the potential of the proposed model as an efficient face renderer. Our model generates var-ious and vivid videos from only a single portrait image and a driving audio stream. 2.