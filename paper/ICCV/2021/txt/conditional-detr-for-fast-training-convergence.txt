Abstract
The recently-developed DETR approach applies the transformer encoder and decoder architecture to object de-tection and achieves promising performance.
In this pa-per, we handle the critical issue, slow training convergence, and present a conditional cross-attention mechanism for fast DETR training. Our approach is motivated by that the cross-attention in DETR relies highly on the content embed-dings for localizing the four extremities and predicting the box, which increases the need for high-quality content em-beddings and thus the training difficulty.
Our approach, named conditional DETR, learns a con-ditional spatial query from the decoder embedding for decoder multi-head cross-attention. The benefit is that through the conditional spatial query, each cross-attention head is able to attend to a band containing a distinct re-gion, e.g., one object extremity or a region inside the ob-ject box. This narrows down the spatial range for local-izing the distinct regions for object classification and box regression, thus relaxing the dependence on the content em-beddings and easing the training. Empirical results show that conditional DETR converges 6.7× faster for the back-bones R50 and R101 and 10× faster for stronger backbones
DC5-R50 and DC5-R101. Code is available at https:
//github.com/Atten4Vis/ConditionalDETR. 1.

Introduction
The DEtection TRansformer (DETR) method [3] applies the transformer encoder and decoder architecture to object detection and achieves good performance.
It effectively eliminates the need for many hand-crafted components, in-cluding non-maximum suppression and anchor generation.
The DETR approach suffers from slow convergence on training, and needs 500 training epochs to get good per-formance. The very recent work, deformable DETR [53], handles this issue by replacing the global dense attention
Figure 1. Comparison of spatial attention weight maps for our con-ditional DETR-R50 with 50 training epochs (the first row), the original DETR-R50 with 50 training epochs (the second row), and the original DETR-R50 with 500 training epochs (the third row).
The maps for our conditional DETR and DETR trained with 500 epochs are able to highlight the four extremity regions satisfacto-rily. In contrast, the spatial attention weight maps responsible for the left and right edges (the third and fourth images in the sec-ond row) from DETR trained with 50 epochs cannot highlight the extremities satisfactorily. The green box is the ground-truth box. (self-attention and cross-attention) with deformable atten-tion that attends to a small set of key sampling points and using the high-resolution and multi-scale encoder. Instead, we still use the global dense attention and propose an im-proved decoder cross-attention mechanism for accelerating the training process.
Our approach is motivated by high dependence on con-tent embeddings and minor contributions made by the spa-tial embeddings in cross-attention. The empirical results in
DETR [3] show that if removing the positional embeddings in keys and the object queries from the second decoder layer and only using the content embeddings in keys and queries, the detection AP drops slightly1.
Figure 1 (the second row) shows that the spatial atten-tion weight maps from the cross-attention in DETR trained with 50 epochs. One can see that two among the four maps do not correctly highlight the bands for the corresponding extremities, thus weak at shrinking the spatial range for the
*Equal Contribution. This work was done when D. Meng, X. Chen, and Z. Fan were interns at Microsoft Research, Beijing, P.R. China
†Corresponding author. 1The minor AP drop 1.4 is reported on R50 with 300 epochs in Table 3 from [3]. We empirically got the consistent observation: the AP drops to 34.0 from 34.9 for 50 training epochs.
chor box-based methods inherit the ideas from the proposal-based method, Fast R-CNN. Example methods include
Faster R-CNN [9], SSD [26], YOLOv2 [31], YOLOv3 [32],
YOLOv4 [1], RetinaNet [24], Cascade R-CNN [2], Libra
R-CNN [29], TSD [35] and so on.
The anchor-free detectors predict the boxes at points near the object centers. Typical methods include YOLOv1 [30],
CornerNet [21], ExtremeNet [50], CenterNet [49, 6],
FCOS [39] and others [23, 28, 52, 19, 51, 22, 15, 46, 47].
DETR and its variants. DETR successfully applies trans-formers to object detection, effectively removing the need for many hand-designed components like non-maximum suppression or initial guess generation. The high compu-tation complexity issue, caused by the global encoder self-attention, is handled in adaptive clustering transformer [48] and by sparse attentions in deformable DETR [53].
The other critical issue, slow training convergence, has been attracting a lot of recent research attention. The
TSP (transformer-based set prediction) approach [37] elim-inates the cross-attention modules and combines the FCOS and R-CNN-like detection heads. Deformable DETR [53] adopts deformable attention, which attends to sparse posi-tions learned from the content embedding, to replace de-coder cross-attention.
The spatially modulated co-attention (SMCA) ap-proach [7], which is concurrent to our approach, is very close to our approach. It modulates the DETR multi-head global cross-attentions with Gaussian maps around a few (shifted) centers that are learned from the decoder em-beddings, to focus more on a few regions inside the es-timated box. In contrast, the proposed conditional DETR approach learns the conditional spatial queries from the de-coder content embeddings, and predicts the spatial attention weight maps without human-crafting the attention attenua-tion, which highlight four extremities for box regression, and distinct regions inside the object for classification.
Conditional and dynamic convolution. The proposed conditional spatial query scheme is related to condi-tional convolutional kernel generation. Dynamic filter net-work [16] learns the convolutional kernels from the input, which is applied to instance segmentation in CondInst [38] and SOLOv2 [42] for learning instance-dependent convolu-tional kernels. CondConv [44] and dynamic convolution [4] mix convolutional kernels with the weights learned from the input. SENet [14], GENet [13] abd Lite-HRNet [45] learn from the input the channel-wise weights.
These methods learn from the input the convolutional kernel weights and then apply the convolutions to the in-put.
In contrast, the linear projection in our approach is learned from the decoder embeddings for representing the displacement and scaling information.
Transformers. The transformer [40] relies on the at-tention mechanism, self-attention and cross-attention, to
Figure 2. Convergence curves for conditional DETR-DC5-R50 and DETR-DC5-R50 on COCO 2017 val. The conditional
DETR is trained for 50, 75, 108 epochs. Conditional DETR train-ing is converged much faster than DETR. content queries to precisely localize the extremities. The reasons are that (i) the spatial queries, i.e., object queries, only give the general attention weight map without exploit-ing the specific image information; and that (ii) due to short training the content queries are not strong enough to match the spatial keys well as they are also used to match the con-tent keys. This increases the dependence on high-quality content embeddings, thus increasing the training difficulty.
We present a conditional DETR approach, which learns a conditional spatial embedding for each query from the cor-responding previous decoder output embedding, to form a so-called conditional spatial query for decoder multi-head cross-attention. The conditional spatial query is predicted by mapping the information for regressing the object box to the embedding space, the same to the space that the 2D coordinates of the keys are also mapped to.
We empirically observe that using the spatial queries and keys, each cross-attention head spatially attends to a band containing the object extremity or a region inside the object box (Figure 1, the first row). This shrinks the spatial range for the content queries to localize the effective regions for class and box prediction. As a result, the dependence on the content embeddings is relaxed and the training is easier.
The experiments show that conditional DETR converges 6.7× faster for the backbones R50 and R101 and 10× faster for stronger backbones DC5-R50 and DC5-R101. Figure 2 gives the convergence curves for conditional DETR and the original DETR [3]. 2.