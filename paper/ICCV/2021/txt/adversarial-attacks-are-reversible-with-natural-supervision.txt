Abstract
We find that images contain intrinsic structure that en-ables the reversal of many adversarial attacks. Attack vec-tors cause not only image classifiers to fail, but also collat-erally disrupt incidental structure in the image. We demon-strate that modifying the attacked image to restore the nat-ural structure will reverse many types of attacks, provid-ing a defense. Experiments demonstrate significantly im-proved robustness for several state-of-the-art models across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.
Our results show that our defense is still effective even if the attacker is aware of the defense mechanism. Since our de-fense is deployed during inference instead of training, it is compatible with pre-trained networks as well as most other defenses. Our results suggest deep networks are vulnera-ble to adversarial examples partly because their represen-tations do not enforce the natural structure of images. 1.

Introduction
Deep networks achieve strong performance over a num-ber of computer vision tasks, yet they remain brittle un-der adversarial attacks [13, 7, 19, 57]. With crafted per-turbations, attackers can undermine predictions from the state-of-the-art models by changing the features in the rep-resentation [38]. These limitations prevent application of deep networks to sensitive and safety-critical applications
[48, 54, 34, 55], underscoring the gap between current ma-chine learning algorithms and human-level abilities [6].
A large body of work has studied how to train deep networks such that they are robust to adversarial attacks.
Adversarial training and its variants [36, 38, 57, 44, 49], including multitask learning [37, 28] and semi-supervised learning [60], significantly improve robustness. However, while existing methods focus on improving the training al-gorithm, they are burdened because they need to find a sin-gle representation that also works for all possible corrup-tions and attacks. Training-based defenses cannot adapt to the individual characteristics of each attack at testing-time.
In this paper, we introduce an approach for reversing the
Figure 1: Reverse Attacks: Adversarial attacks are small perturbations that cause classification networks to fail [36, 57].
In this paper, we show there are intrinsic signals in natural images to reverse many types of attacks. In the right column, we visualize our reverse attack on an ImageNet im-age. Note that both attack vectors have been multiplied by ten for visualization purposes only. attack process, allowing us to formulate a defense strategy that adapts to each attack during the testing phase. Just as an attacker finds the right additive perturbation to break the input, our approach will find the right additive perturbation to repair the input. Figure 1 shows our reverse attack on a poisoned ImageNet image. However, reverse attacks are more challenging to produce than standard attacks because the category label is unknown to us during testing.
Our key insight is that images contain natural and intrin-sic structure that we can leverage to reverse many types of adversarial attacks. We found that, although adversarial at-tacks aim to fool the image classifier, they also collaterally damage self-supervised objectives. Our approach shows how to capitalize on this incidental signal in order to create adversarial defenses. By using self-supervision for defense at test time, we can guarantee that even the strongest adver-sary cannot manipulate the intrinsic signals that naturally come with the images, providing a more robust defense than training-based methods.
A key advantage of our framework is that it factors out the defense strategy from the visual representation. Since reverse attacks are adaptive, this defense is able to effi-ciently scale to any corruption that violates the natural im-age manifold. Moreover, the modularity of our approach
Figure 2: Defense Overview: We find that adversarial at-tacks on classification networks will also collaterally at-tack self-supervised contrastive networks [10]. Since self-supervision is available during deployment, we exploit this discrepancy to reverse adversarial attacks and provide a de-fense. Our approach modifies the potentially attacked input image such that contrastive distances
Ls are restored. allows it to work with any classifier and complements ex-isting defense models. It can also be integrated into future defense models and defend against novel attacks that cor-rupt natural image structures.
Visualizations, empirical experiments, and theoretical analysis show that our reversal strategy significantly im-proves robust prediction for several established benchmarks and attacks. Our method advances the state-of-the-art de-fense methods by a large margin across four natural im-age datasets including CIFAR-10 (over 7.5% gain), CIFAR-100 (over 5.5% gain), SVHN (over 11.8% gain), and Ima-geNet (over 3.0% gain). Our method is robust against es-tablished attacks, including PGD [36], C&W [7], and Au-In addition, our empirical results demon-toAttack [14]. strate that, even when the attacker is aware of our de-fense mechanism, our approach remains robust. Our mod-els, data, and code are available at https://github.com/cvlab-columbia/SelfSupDefense. 2.