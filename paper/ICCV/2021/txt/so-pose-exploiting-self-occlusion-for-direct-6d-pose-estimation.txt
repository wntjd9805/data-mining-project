Abstract
Directly regressing all 6 degrees-of-freedom (6DoF) for the object pose (i.e. the 3D rotation and translation) in a cluttered environment from a single RGB image is a chal-lenging problem. While end-to-end methods have recently demonstrated promising results at high efﬁciency, they are still inferior when compared with elaborate PnP/RANSAC-based approaches in terms of pose accuracy. In this work, we address this shortcoming by means of a novel reason-ing about self-occlusion, in order to establish a two-layer representation for 3D objects which considerably enhances the accuracy of end-to-end 6D pose estimation. Our frame-work, named SO-Pose, takes a single RGB image as input and respectively generates 2D-3D correspondences as well as self-occlusion information harnessing a shared encoder and two separate decoders. Both outputs are then fused to directly regress the 6DoF pose parameters. Incorporating cross-layer consistencies that align correspondences, self-occlusion and 6D pose, we can further improve accuracy and robustness, surpassing or rivaling all other state-of-the-art approaches on various challenging datasets. 1.

Introduction
Estimating the 6D pose has been widely adopted as an essential cue in high-level computer vision tasks, including robotic grasping and planning [1], augmented reality [39], and autonomous driving [25, 29]. Driven by the recent suc-cess of deep learning, current methods are capable of esti-mating the 6D pose in a cluttered environment at impres-sive accuracy and high efﬁciency [15, 47, 18]. Almost all current top-performing frameworks adopt a two-stage strat-egy that resorts to ﬁrst establishing 2D-3D correspondences and then computing the 6D pose with a RANSAC-based
Perspective-n-Point (PnP) algorithm [47, 20, 9]. Neverthe-*Yan Di is sponsored by China CSC program.       2D-3D Point Matching        
Image            CNN                  2D-3D Point Matching         6D P 6D Pose (a) Basic structure of baseline methods [12, 43] 2D-3D 
Point 
Matching 
Self-occlusion
Information
Image           CNN                    Two-layer Model                               6D Pose (b) Basic structure of our method SO-Pose.
Figure 1. The basic structures of end-to-end 6D pose estimation methods. Compared to single-layer methods [12, 43] that depend on 2D-3D point matching as intermediate results, our method SO-Pose presents a novel two-layer representation that additionally incorporates self-occlusion information about the object. less, while achieving great results, these methods cannot be trained in an end-to-end manner and require extra computa-tion for optimization of pose. Moreover, adopting surrogate training losses instead of directly predicting 6D poses also prevents further differentiable processing/learning (e.g. by means of self-supervised learning [42]) and does not allow to incorporate other down-stream tasks.
Despite two-stage approaches dominating the ﬁeld, a few methods conducting end-to-end 6D pose estimation have been also recently proposed [12, 4, 42, 43]. They typi-cally learn the 6D pose directly from dense correspondence-based intermediate geometric representations, as shown in
Fig. 1(a). Nevertheless, although end-to-end methods keep constantly improving, they are still far inferior to two-stage methods harnessing multi-view consistency check [18], symmetry analysis [9], or disentangled predictions [20].
What limits the accuracy of end-to-end methods? Af-ter in-depth investigation in challenging scenes, we observe
that while the network is approaching the optimum, due to the inherent matching ambiguity of textureless object sur-face, mis-matching error caused by noise is inevitable, re-sulting often in one correspondence ﬁeld corresponding to many 6D poses with similar ﬁtting errors. This leads the training process to converge to a sub-optimum, hindering the overall 6D pose estimation performance. Since elimi-nating errors caused by noise is not trivial, an alternative solution to this problem is to replace the correspondence
ﬁeld with a more precise representation of the 3D object, thus reducing the inﬂuence of noise.
In this work, we attempt at closing the gap between end-to-end and two-stage approaches by leveraging self-occlusion information about the object. For an object in 3D space, we can logically only observe its visible parts due to the nature of the perspective projection. Yet, parts that are invisible due to (self-) occlusion are usually ne-glected during inference.
Inspired by multi-layer models used in 3D reconstruction [34], we focus on self-occlusion information to establish a viewer-centered two-layer rep-resentation of the object pose. While the ﬁrst layer pre-serves the correspondence ﬁeld of visible points on the ob-ject and their projections, the second layer incorporates the self-occlusion information. In essence, instead of directly identifying whether and where each visible point occludes the object, we simplify the procedure by examining the self-occlusion between each pixel and the object coordi-nate planes. As illustrated in Fig. 3, the ray passing through the camera center and each visible point intersects the ob-ject coordinate plane at most three different locations. The coordinates of these intersections are then utilized to form the second layer representation of the object, as shown in
Fig. 1(b). Finally, two cross-layer consistency losses are in-troduced to align self-occlusion, correspondence ﬁeld and 6D pose simultaneously, reducing the inﬂuence of noise.
To summarize, our main contributions are as follows:
• We propose SO-Pose, a novel deep architecture that directly regresses the 6D pose from the two-layer rep-resentation of each 3D object.
• We propose to leverage self-occlusion and 2D-3D cor-respondences to establish a two-layer representation for each object in 3D space, which can be utilized to enforce two cross-layer consistencies.
• SO-Pose consistently surpasses all other end-to-end competitors on various challenging datasets. More-over, SO-Pose also achieves comparable accuracy when compared with other state-of-the-art two-stage methods, whilst being much faster. 2.