Abstract
Measuring similarity between two images often requires performing complex reasoning along different axes (e.g., color, texture, or shape). Insights into what might be im-portant for measuring similarity can can be provided by annotated attributes. Prior work tends to view these an-notations as complete, resulting in them using a simplistic approach of predicting attributes on single images, which are, in turn, used to measure similarity. However, it is im-practical for a dataset to fully annotate every attribute that may be important. Thus, only representing images based on these incomplete annotations may miss out on key in-formation. To address this issue, we propose the Pairwise
Attribute-informed similarity Network (PAN), which breaks similarity learning into capturing similarity conditions and relevance scores from a joint representation of two images.
This enables our model to identify that two images contain the same attribute, but can have it deemed irrelevant (e.g., due to ﬁne-grained differences between them) and ignored for measuring similarity between the two images. Notably, while prior methods of using attribute annotations are often unable to outperform prior art, PAN obtains a 4-9% improve-ment on compatibility prediction between clothing items on
Polyvore Outﬁts, a 5% gain on few shot classiﬁcation of im-ages using Caltech-UCSD Birds (CUB), and over 1% boost to Recall@1 on In-Shop Clothes Retrieval. Implementation available at https://github.com/samarth4149/PAN 1.

Introduction
Learning similarity metrics between images is a cen-tral problem in computer vision with wide-ranging appli-cations such as face recognition [21, 35], image retrieval
[9, 25, 49], prototype based few shot image classiﬁcation
[16, 36, 39, 44], continual learning of image classiﬁca-tion [2, 32, 37], and fashion compatibility or recommen-*Indicates equal contribution
Figure 1: In prior work (e.g. [17, 28, 41, 50]), shown in (a), attributes used for image similarity are predicted for each im-age and then are used as input to the image similarity model.
However, this can result in loss of important information about how attributes are expressed (e.g., different shades of the attribute yellow breast). Thus, in our work, shown in (b), we avoid this loss of information by using a joint represen-tation of the two images to compute multiple disentangled similarity scores, each corresponding to an attribute, and relevances of each similarity score in the ﬁnal similarity pre-diction. This allows for more ﬁne-grained reasoning about different attribute manifestations, boosting performance. dation [6, 40, 41, 42, 43, 52]. There has been a recent trend of learning these metrics by decomposing the prob-Figure 2: PAN overview. Given a pair of images, the goal of PAN is produce its similarity score. We begin by using the image encoder to generate feature vectors for input images. The image features are then fed into the Concept-conditioned
Similarity Module (CSM) that uses these features to generate a set of similarity scores with corresponding relevance weights.
This enables PAN to identify that two images do contain the same attribute, but that they are not relevant to the similarity score since they are different manifestations of the attribute (see Fig 1 for an example). The ﬁnal similarity score p ∈ [0, 1] is produced using a weighted combination of the similarity conditions and their relevance. Note that the different colored lines (blue, pink) represent information ﬂow pertaining to individual images. lem into multiple axes of similarity or similarity condi-tions, which has improved performance on a variety of tasks [12, 20, 26, 27, 28, 40, 41, 42]. Generally speaking, methods that automatically learn what these conditions rep-resent [27, 40] have reported better performance than those that predeﬁne this knowledge using information like labeled image attributes and item categories [20, 42, 41, 28]. We argue this is primarily due to prior work using attributes to predict their presence on single images (e.g. [17, 50, 41, 28]), and subsequently using these predictions for predicting sim-ilarity (Fig 1 (a)). This incurs a loss in information about the different manifestations of an attribute, differences that could affect similarity prediction, but may not be distinguish-able in attribute annotations. While this could be addressed by collecting a complete set of annotations of every possible attribute and their different manifestations that could affect similarity, such a collection would be expensive. In addition, it is often impossible to articulate every ﬁne-grained attribute that may affect similarity.
In this paper, we introduce a Pairwise Attribute-informed similarity Network (PAN) that effectively learns to use su-pervisory information in the form of attribute labels, avoid-ing information loss, to create a powerful image similar-ity model that performs well on a range of diverse tasks.
To illustrate how we do this we refer to the example in
Fig 1(b). The ﬁgure shows two birds (of different cate-gories) from the Caltech-UCSD Birds (CUB) dataset [45], where they are both labeled positively for the binary at-tribute has breast color::yellow, indicating that they have yellow breasts. Prior work (e.g. [17, 28, 41, 50]) directly predicts attributes for each image, which tends to lose information about subtle differences in the manifesta-tions of the attributes, like the shades of the color yellow.
Our PAN model avoids this issue by ﬁrst comparing images in a feature space rather than attribute space, as illustrated in
Fig 1(b). Using the joint image features it then predicts both a similarity score and a relevance for different similarity con-ditions deﬁned by the attributes. Even when the similarity score may coarsely indicate that the two images are similar since they have the same attribute, the model can pick up on ﬁner attribute differences and decide that the mere pres-ence of the same attribute is of low relevance to a positive similarity prediction. As our experiments will show, this difference can make a dramatic impact on the performance of the learned image similarity model.
One major challenge we face is the considerable differ-ence in how attributes relate to the similarity functions that arise in different tasks. For example, in few-shot classiﬁca-tion, where we use the labeled support images in a nearest-neighbors classiﬁer, the goal of a visual similarity classiﬁer would be to simply measure similarity by matching attributes between the test and support images. In contrast, for tasks like fashion compatibility, where two images are deemed similar if they complement each other when worn together
in an outﬁt, image pairs with different attributes (e.g., black and orange) can indicate they are highly compatible. Fur-thermore, simply modeling which attribute pairs indicate compatibility is insufﬁcient, since two attributes which often result in compatible pairs could be deemed incompatible depending on the other attributes that are present. For ex-ample, black and orange items are often compatible, except when some other attributes like red are also present. Thus, visual similarity models must learn a far more complex set of relationships between attributes when learning fashion com-patibility. These differences mean that methods that do well on few-shot classiﬁcation often perform poorly on fashion compatibly and vice-versa. PAN, however, can take this into account via the method we use to convert the incomplete attribute labels for single images into supervisory signals for image pairs and improve performance across diverse tasks.
As we will discuss in Sec 3 (and illustrated in Fig 2), PAN naturally allows for training and automatically learning simi-larity conditions in the absence of any additional data like attributes. We also ﬁnd that PAN can improve performance even in cases where only sparse attribute labels are available.
Summarizing our contributions:
• We propose a Pairwise Attribute-informed similarity
Network (PAN), which incorporates ﬁne-grained at-tribute information during training based on a joint representation of two images enabling us to avoid the loss of information suffered by prior work.
• While prior methods of incorporating attribute informa-tion under-perform prior art, PAN outperforms them on three diverse tasks—by 4-9% on fashion item com-patibility prediction on Polyvore Outﬁts [41], 5% on few shot classiﬁcation on CUB [45] and over 1% Re-call@1 on In-Shop Clothing Retrieval [22], demon-strating PAN’s generality. In comparison to prior ap-proaches of incorporating attribute supervision, PAN is better by a wider margin, e.g., it outperforms them by a sizeable 6-17 % on Polyvore Outﬁts.
• We propose different methods of using attributes for su-pervising predictions along similarity conditions, delv-ing into the interpretations of each, providing insights for their applicability in different tasks.
• Our analysis also outlines the contributions of the train-ing procedures, speciﬁcally training batch-size. This has commonly been overlooked in prior work, but could have signiﬁcant impact on ﬁnal model performance. In doing so, we factor out contribution of the training procedure in demonstrating PAN’s beneﬁts. 2.