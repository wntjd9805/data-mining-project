Abstract
Recent papers have suggested that transfer learning can outperform sophisticated meta-learning methods for few-shot image classification. We take this hypothesis to its logical conclusion, and suggest the use of an ensemble of high-quality, pre-trained feature extractors for few-shot im-age classification. We show experimentally that a library of pre-trained feature extractors combined with a simple feed-forward network learned with an L2-regularizer can be an excellent option for solving cross-domain few-shot image classification. Our experimental results suggest that this simple approach far outperforms several well-established meta-learning algorithms. 1.

Introduction
There has been a lot of recent interest in few-shot image classification [5, 15, 17, 6, 7, 12, 19, 16]. Various papers have explored different formulations of the problem, but in one general formulation, we are given a data set Dtrn of (image, label) pairs sampled from a distribution Ptrn . The goal is to devise a method that uses Dtrn to learn a function f that is itself a few-shot learner. The few-shot learner f takes as input a new labeled data set Dfew consisting of a set of samples from a new distribution Pfew ̸= Ptrn . f then returns a classification function g, which is targeted at classifying samples from the distribution Pfew .
The process of learning f is often referred to as meta-learning in the literature. Learning to classify samples from
Pfew is a “few shot” problem when Dfew is small, perhaps having only one example for each class produced by Pfew .
In the most difficult and generally applicable variant of the few-shot problem—which we consider in this paper—Pfew has no known relationship to Ptrn (this is “cross-domain” few-shot learning) and Dfew is not available while learn-ing f . Thus, the meta-learning process has no access to information about the eventual application. The only in-formation we have about Pfew is the set Dfew , and this is available only when constructing g. We cannot, for exam-ple, choose any hyperparameters controlling the learning of g using information not extracted from Dfew .
Our goal is to devise a learner f that works well, out-of-the-box, on virtually any new distribution Pfew . We ar-gue that in such a scenario, developing novel meta-learning methods to learn f from scratch on given Dtrn may not be the most productive direction of inquiry. Because no information about Pfew is available during meta-learning, it makes sense to choose a Dtrn that has many different types of images, so it is likely to contain some images with features similar to those produced by Pfew , whatever form this distribution takes. Fortunately, in computer im-age classification, the standard benchmark data set is now
ILSVRC2012, a 1000-class version of the full ImageNet
[22]. ILSVRC2012 consists of a wide variety of images, and it has become quite standard for researchers who de-sign and train new image classifiers to publish classifiers trained on ILSVRC2012. Such published artifacts repre-sent thousands of hours of work by researchers who are well-versed in the “black art” of wringing every last per-cent of accuracy out of a deep CNN. Instead of develop-ing new meta-learning methods, it may be more productive to simply fix Dtrn = ILSVRC2012, and attempt to lever-age all of the effort that has gone into learning deep CNNs over ILSVRC2012, by using those CNNs as the basis for the few-shot learner f . As other, even more wide-ranging and difficult benchmark data sets become prevalent (such as the full, 20,000+ class ImageNet), high-quality classifiers trained using that data set may be preferred.
We first show that it is possible to use any of a num-ber of published, high-quality, deep CNNs, learned over
ILSVRC2012, as the basis for a few-shot learner that sig-nificantly outperforms state-of-the art methods. The way to do this is embarrassingly simple: remove the classifier from the top of the deep CNN, fix the weights of the remaining deep feature extractor, and replace the classifier with a sim-ple MLP that is trained using L2 regularization to prevent over-fitting. We call these “library-based” learners because
they are based on standard, published feature extractors.
Next, we ask: if a published deep CNN can be used to produce a state-of-the-art, few-shot learner, can we pro-duce an even higher-quality few-shot learner by combining together many high-quality, deep CNNs? We call such a learner a “full library” learner.
Then, we note that other researchers have suggested the utility of re-using high-quality, pre-trained feature extrac-tors for few-shot image classification. In particular, the au-thors of the “Big Transfer” paper [13] argue that a very large network trained on a huge data set (the JFT-300M data set
[26], with 300 million images) can power an exceptionally accurate transfer-learning-based few-shot learning. Unfor-tunately, the authors have not made their largest JFT-300M-trained network public, and so we cannot experiment with it directly. However, they have made public several versions of their “Big Transfer” network, trained on the full, 20,000+
Interestingly, we find class ImageNet benchmark public. that “big” may not be as important as “diverse”: a single, few-shot classifier comprised of many different high-quality
ILSVRC2012-trained deep CNNs seems to be a better op-tion than a single few-shot classifier built on top of any of the Google-trained CNNs. Finally, we investigate why a full library learner works so well. We postulate two reasons for this. First, having a very large number of features (> 10,000) does not seem to be a problem for few-shot learn-ing. Second, there seems to be strength in diversity, in the sense that different CNNs appear useful for different tasks. 2. High Accuracy of Library-Based Learners 2.1. Designing a Library-Based Learner
We begin by asking: what if we eschew advanced meta-learning methods, and instead simply use a very high-quality deep CNN pulled from a library, trained on the
ILSVRC2012 data set, as the basis for a few-shot classifier?
Specifically, we are given a high-quality, pre-trained deep CNN, from a library of pre-trained networks; we take the CNN as-is, but remove the topmost layers used for clas-sification. This results in a function that takes an image, and returns an embedding. We then use that embedding to build a classifier in an elementary fashion: we feed the em-bedding into a multi-layer perceptron with a single hidden layer; a softmax is used to produce the final classification.
Given a few-shot classification problem, two weight matri-ces W1 and W2 are learned; the first connecting the embed-ding to the hidden layer, the second connecting the hidden layer to the softmax. To prevent over-fitting during training, simple L2 regularization is used on the weight matrices. 2.2. Evaluation
To evaluate this very simple few-shot learner, we first identify nine, high-quality deep CNNs with published models, trained on ILSVRC2012: ResNet18, ResNet34,
ResNet50, ResNet101, ResNet152 (all of the ResNet imple-mentations are the ones from the original ResNet design-ers [8]), DenseNet121, DenseNet161, DenseNet169, and
DenseNet201 (all of the DenseNet implementations are also from the original designers [10]).
Our goal is to produce an “out-of-the-box” few-shot learner that can be used on any (very small) training set
Dfew without additional data or knowledge of the under-lying distribution. We are very careful not to allow vali-dation or parameter tuning on testing data domains, so all parameters and settings need to be chosen apriori. If it is possible to build such a few-shot learner, it would be the most widely applicable: simply produce a few training im-ages for each class, apply the learner. Thus, we perform an extensive hyper-parameter search, solely using the Caltech-UCSD Birds 200 set [32] as a validation data set, and then use the best hyperparameters from that data set in all of our experiments. Hyperparameters considered were learn-ing rate, number of training epochs, regularization penalty weight, the number of neurons in the MLP hidden layer, and whether to drop the hidden layer altogether. A separate hyper-parameter search was used for 5-way, 20-way, and 40-way classification.
We then test the resulting few-shot learner—one learner per deep CNN—on eight different data sets, FGVC-Aircraft
[18], FC100 [21], Omniglot [14], Traffic Sign [9], FGCVx
Fungi [24], Quick Draw [11], and VGG Flower [20]. To evaluate a few-shot learner on a data set, for an “m-way n-shot” classification problem, we randomly select m dif-ferent classes, and then randomly select n images from each class for training; the remainder are used for testing. As this is “out-of-the-box” learning, no validation is allowed.
We performed this evaluation for m in {5, 20, 40} and n in {1, 5}. Due to space constraints, the full results are presented as supplementary material, and we give only a synopsis here (Table 2 and Table 3). In Table 1, we show the best and worst accuracy achieved across the 9 learners, for each of the 8 data sets, for m in {5, 20, 40} and n = 1.
To give the reader an idea of how this accuracy compares to the state-of-the-art, we compare these results with a num-ber of few-shot learners from the literature. We compare against Baseline and Baseline++ [1], MAML [6], Match-ingNet [30], ProtoNet [25], RelationNet [28], Meta-transfer
[27], FEAT [33], and SUR [4]. When a deep CNN clas-sifier must be chosen for any of these methods, we use a
ResNet18. For methods that require a pre-trained CNN (FEAT, Meta-transfer, and SUR), we use the ResNet18 trained by the ResNet designers [8]. Lest the reader be con-cerned that we chose the worst option (ResNet18), we point out that of the library-based few-shot learners, on the 5-way, 5-shot problem ResNet18 gave the best accuracy out of all of the ResNet-based learners for two of the data sets (see Ta-Aircraft
FC100
Omniglot
Texture
Traffic
Fungi
Quick Draw VGG Flower 5-way, 1-shot
Worst
Best 40.9 ± 0.9
RN18 46.2 ± 1.0
DN161 50.8 ± 0.9
DN121 61.2±0.9
RN152 77.2±0.9
RN152 86.5 ± 0.7
DN121 59.1 ± 0.9
DN169 65.1 ± 0.9
RN101 55.5 ± 0.8
RN152 66.6 ± 0.9
DN201 53.0 ± 0.9
DN201 56.6 ± 0.9
DN121 57.3 ± 0.9
RN101 62.8 ± 0.9
RN18 20-way, 1-shot
Worst
Best 20.1 ± 0.3
RN101 24.3 ± 0.3
DN161 27.8 ± 0.4
DN121 36.4 ± 0.4
RN152 56.2 ± 0.5
RN101 69.1 ± 0.5
DN121 38.0 ± 0.4
RN18 42.5 ± 0.4
RN152 29.7 ± 0.3
RN101 38.5 ± 0.4
DN201 31.7 ± 0.4
RN101 33.9 ± 0.5
DN161 33.2 ± 0.5
RN101 39.5± 0.5
DN201 40-way, 1-shot
Worst
Best 14.2 ± 0.2
RN34 17.4 ± 0.2
DN161 19.6 ± 0.2
RN18 27.2 ± 0.3
RN152 47.3 ± 0.3
RN152 61.6 ± 0.3
DN201 28.9 ± 0.2
RN18 33.2 ± 0.3
DN152 22.2 ± 0.2
RN152 29.5 ± 0.2
DN201 23.7 ± 0.3
RN34 26.8 ± 0.3
DN161 26.4 ± 0.3
RN152 31.2 ± 0.3
DN201 79.7 ± 0.8
RN18 83.5 ± 0.8
DN161 62.4 ± 0.5
RN101 70.0 ± 0.5
DN161 53.1 ± 0.3
RN34 62.8 ± 0.3
DN161
Table 1: Accuracy obtained using library deep CNNs for few-shot learning.
Aircraft
FC100
Omniglot
Texture
Traffic
Fungi
Quick Draw VGG Flower
Baseline
Baseline++
MAML
MatchingNet
ProtoNet
RelationNet
Meta-transfer
FEAT
SUR
Worst library-based
Best library-based 47.6 ± 0.7 40.9 ± 0.7 33.1 ± 0.6 33.5 ± 0.6 41.5 ± 0.7 37.5 ± 0.7 46.2 ± 0.7 46.2 ± 0.9 45.2 ± 0.8 61.0 ± 0.9
RN34 66.0 ± 0.9
DN161 66.9 ± 0.7 59.8 ± 0.8 62.0 ± 0.8 59.4 ± 0.8 64.7 ± 0.8 64.8 ± 0.8 75.7 ± 0.8 60.5 ± 0.8 67.2 ± 1.0 71.9 ± 0.8
DN121 80.0 ± 0.6
RN152 96.5 ± 0.2 90.6 ± 0.4 82.6 ± 0.7 89.7 ± 0.5 95.5 ± 0.3 91.2 ± 0.4 93.5 ± 0.4 85.3 ± 0.7 98.7 ± 0.1 94.0 ± 0.4
RN152 96.7 ± 0.2
DN201 62.5 ± 0.7 59.9 ± 0.7 56.9 ± 0.8 54.7 ± 0.7 62.9 ± 0.7 60.0 ± 0.7 70.5 ± 0.7 70.7 ± 0.7 59.6 ± 0.7 79.3 ± 0.6
RN18 83.4 ± 0.6
DN161 82.1 ± 0.7 79.6 ± 0.8 67.4 ± 0.9 73.7 ± 0.8 75.0 ± 0.8 68.6 ± 0.8 80.0 ± 0.8 70.5 ± 0.8 70.6 ± 0.8 78.8 ± 0.7
RN152 85.3 ± 0.7
DN201 57.6 ± 1.7 54.1 ± 1.7 48.3 ± 1.8 55.7 ± 1.7 53.1 ± 1.8 58.7 ± 1.8 66.1 ± 0.8 67.3 ± 1.7 60.0 ± 1.8 77.1 ± 0.8
RN34 79.1 ± 0.7
DN121 75.6 ± 0.7 68.4 ± 0.7 77.5 ± 0.8 70.4 ± 0.8 74.9 ± 0.7 71.9 ± 0.7 77.7 ± 0.7 69.9 ± 0.7 73.5 ± 0.7 77.8 ± 0.7
RN152 81.8 ± 0.6
DN201 90.9 ± 0.5 83.1 ± 0.7 78.0 ± 0.7 74.2 ± 0.8 86.7 ± 0.6 80.6 ± 0.7 90.5 ± 0.6 92.1 ± 0.4 90.8 ± 0.5 95.3 ± 0.4
RN34 96.8 ± 0.3
DN161
Table 2: Comparing competitive methods with the simple library-based learners, on the 5-way, 5-shot problem. ble 4). Further, these competitive methods tend to be quite expensive to train—MAML, for example, requires running gradient descent over a gradient descent—and for such a method, the shallower ResNet18 is a much more reasonable choice than the deeper models (even using a ResNet18, we could not successfully train first-order MAML for 5-shot, 40-way classification, due to memory constraints).
For the competitive methods (other than SUR) we fol-low the same procedure as was used for the library-based few-shot classifiers: any training that is necessary is per-formed on the ILSVRC2012 data set, and hyperparameter validation is performed using the Caltech-UCSD Birds 200 data set. Each method is then used without further tuning on the remaining eight data sets. To evaluate SUR on data set X, we use feature extractors trained on the data sets in
{Omniglot, Aircraft, Birds, Texture, Quickdraw, Flowers,
Fungi,and ILSVRC} − X. Traffic Sign and FC100 datasets are reserved for testing only.
A comparison of each of these competitive methods with the the best and worst-performing library-based learners on the 5-way, 5-shot learning problem is shown in Table 2; a comparison on 20-way, 5-shot learning in Table 3. A more complete set of results is in the supplementary material. 2.3. Discussion
There are a few key takeaways from these results. The best library-based learner always beat every one of the other methods tested, with the only exception of SUR when test-Aircraft
FC100
Omniglot
Texture
Traffic
Fungi
Quick Draw VGG Flower
Baseline
Baseline++
MAML
MatchingNet
ProtoNet
RelationNet
Meta-transfer
FEAT
SUR
Worst library-based
Best library-based 24.2 ± 0.3 18.4 ± 0.3 11.8 ± 0.2 11.9 ± 0.2 22.1 ± 0.3 17.1 ± 0.3 19.1 ± 0.3 23.1 ± 0.5 21.8 ± 0.3 37.5 ± 0.4
RN18 44.6 ± 0.4
DN161 40.0 ± 0.4 33.8 ± 0.3 25.7 ± 0.3 31.6 ± 0.3 38.9 ± 0.4 39.1 ± 0.4 48.0 ± 0.4 34.4 ± 0.5 42.9 ± 0.5 47.1 ± 0.4
RN18 58.8 ± 0.4
RN152 87.5 ± 0.3 76.2 ± 0.2 46.5 ± 0.4 64.6 ± 0.6 88.1 ± 0.2 79.7 ± 0.3 75.3 ± 0.4 66.4 ± 0.6 96.3 ± 0.1 84.3 ± 0.3
RN101 92.0 ± 0.2
DN201 37.0 ± 0.3 34.8 ± 0.3 21.9 ± 0.3 31.6 ± 0.3 38.9 ± 0.3 32.1 ± 0.3 45.5 ± 0.4 47.5 ± 0.6 35.5 ± 0.4 58.7 ± 0.4
RN18 65.1 ± 0.4
DN161 59.9 ± 0.4 55.3 ± 0.4 27.0 ± 0.3 46.5 ± 0.4 46.9 ± 0.4 41.9 ± 0.4 52.0 ± 0.4 43.4 ± 0.6 46.7 ± 0.4 55.9 ± 0.4
RN152 66.0 ± 0.4
DN201 32.5 ± 0.8 28.2 ± 0.8 17.3 ± 0.7 28.1 ± 0.8 33.0 ± 0.9 27.8 ± 0.8 38.6 ± 0.5 43.9 ± 0.8 34.4 ± 0.9 56.1 ± 0.5
RN34 60.8 ± 0.5
DN161 52.8 ± 0.4 45.5 ± 0.4 30.7 ± 0.3 41.2 ± 0.4 33.0 ± 0.9 47.5 ± 0.4 52.6 ± 0.4 47.0 ± 0.6 54.2 ± 0.4 57.2 ± 0.4
RN101 63.9 ± 0.4
DN161 76.7 ± 0.4 64.0 ± 0.4 32.9 ± 0.3 53.7 ± 0.5 70.9 ± 0.4 62.5 ± 0.4 74.0 ± 0.4 80.5 ± 0.5 77.1 ± 0.4 86.8 ± 0.3
RN101 91.6 ± 0.2
DN161
Table 3: Comparing competitive methods with the simple library-based learners, on the 20-way, 5-shot problem. ing on Omniglot data set. For the other data sets, the gap only grows as the number of ways increases. In fact, for the 20-way problem, the worst library-based learner always beat all of the other methods tested (except SUR on Om-niglot). The gap can be quite dramatic, especially on the 20-way problem. The best non-transfer based few-shot learner (MAML, Proto Nets, Relation Nets, and Matching Nets fall in this category) was far worse than even the worst library-based learner: 39% accuracy for Proto Nets vs. 59% ac-curacy for a classifier based on RestNet18 on Texture, 48% accuracy for Relation Nets vs. 57% accuracy for classi-fier based on ResNet101 on Quick Draw. There have been a large number of non-transfer-based methods proposed in the literature (with a lot of focus on improving MAML in particular [6, 7, 12, 19, 16]) but the gap between MAML and the library-based classifiers is very large.
We also note that of the rest non-library methods, Meta-transfer, Baseline, and FEAT were generally the best. We note that Meta-transfer, Baseline, and FEAT use the pre-trained ResNet18 without modification. This tends to sup-port the hypothesis at the heart of this paper: starting with a state-of-the-art feature extractor, trained by experts, may be the most important decision in few-shot learning. 3. A Simple Full Library Classifier 3.1. Extreme Variation in Few-Shot Quality
There is not a clear pattern to which of the library-based classifiers tends to perform best, and which tends to perform worst. Consider the complete set of results, over all of the library-based few-shot learners, for the 5-way, 5-shot prob-lem, shown in Table 4. For “out-of-the-box” use, where no validation data are available, it is very difficult to see any sort of pattern that might help with picking a particu-lar library-based classifier. The DenseNet variations some-times do better than ResNet (on the Aircraft data set, for ex-ample), but sometimes they do worse than the ResNet vari-ations (on the FC100 data set, for example). And within a family, it is unclear which library-based CNN to use.
As mentioned before, ResNet18 provides the best ResNet-based few-shot learner for two of the data sets, but it forms the basis of the worst ResNet-based learner on another two. 3.2. Combining Library-Based Learners
Given the relatively high variance in accuracies obtained using the library deep CNNs across various data sets, it is natural to ask: Is it perhaps possible to use all of these li-brary feature extractors in concert with one another, to de-velop a few-shot learner which consistently does as well as (or even better then) the best library CNN?
Given the lack of training data in few-shot learning, the first idea that one might consider is some simple variant of ensembling: given a few-shot learning problem, simply train a separate neural network on top of each of the deep
CNNs, and then use a majority vote at classification time (hard ensembling) or we can average the class weights at classification time (soft ensembling).
Another option is to take all of the library deep CNNs together, and view them as a single feature extractor. Using the nine models considered thus far in this paper in this way results in 13,984 features. We then train an MLP on top of this, using L2 regularization. Again using the Caltech-UCSD Birds 200 data set for validation, we perform hy-perparameter search to build a few-shot learner for 5-way, 20-way, and 40-way classification problems.
We test both of these options over the eight test sets, and give a synopsis of the results in Table 5. We find that across all 24 test cases (eight data sets, three classification tasks), the best single learner was never able to beat the best method that used all nine deep CNNs. All of the tested
Aircraft
FC100
Omniglot
Texture
Traffic
Fungi
Quick Draw VGG Flower
DenseNet121
DenseNet161
DenseNet169
DenseNet201
ResNet18
ResNet34
ResNet50
ResNet101
ResNet152 64.7 ± 0.9 66.0 ± 0.9 63.6 ± 0.9 62.6 ± 0.9 61.2 ± 0.9 61.0 ± 0.9 62.3 ± 0.9 62.4 ± 0.9 61.7 ± 1.0 71.9 ± 0.8 73.7 ± 0.7 73.5 ± 0.7 75.1 ± 0.7 72.1 ± 0.8 76.2 ± 0.7 73.9 ± 0.8 79.2 ± 0.7 80.0 ± 0.6 96.7 ± 0.3 96.6 ± 0.3 95.0 ± 0.3 96.7 ± 0.6 95.4 ± 0.3 94.9 ± 0.3 94.3 ± 0.4 95.5 ± 0.3 94.0 ± 0.4 82.1 ± 0.6 83.4 ± 0.6 82.3 ± 0.6 83.2 ± 0.6 79.3 ± 0.6 82.5 ± 0.6 83.2 ± 0.6 82.8 ± 0.6 83.0 ± 0.6 85.0 ± 0.7 83.9 ± 0.7 84.6 ± 0.7 85.3 ± 0.7 83.2 ± 0.7 81.3 ± 0.7 79.4 ± 0.8 81.3 ± 0.7 78.8 ± 0.7 79.1 ± 0.7 78.4 ± 0.8 78.4 ± 0.8 78.0 ± 0.7 77.7 ± 0.7 77.1 ± 0.7 77.9 ± 0.8 78.6 ± 0.8 79.0 ± 0.8 81.3 ± 0.7 81.3 ± 0.6 80.6 ± 0.7 81.8 ± 0.6 81.7 ± 0.6 79.7 ± 0.6 78.1 ± 0.8 78.6 ± 0.7 77.8 ± 0.7 96.0 ± 0.4 96.8 ± 0.3 96.1 ± 0.3 96.5 ± 0.3 95.3 ± 0.4 95.3 ± 0.4 95.6 ± 0.4 95.8 ± 0.3 95.6 ± 0.3
Table 4: Complete results for library-based few-shot learners on the 5-way, 5-shot problem. methods had similar performance on the 5-way classifica-tion task, though the best single learner was generally a bit worse than the other methods.
Where the difference becomes more obvious is on the classification tasks with more categories. For the 20-way and 40-way problems, the two ensemble-based methods had a small but consistent drop in accuracy, and building a sin-gle network on top of all nine deep CNNs is clearly the best.
This may be somewhat surprising; for the 40-way prob-lem, hyperparameter search on the Caltech-UCSD Birds 200 data set settled on a single network with 1024 neurons in a hidden layer; this means that more than 14 million pa-rameters must be learned over just 200 images. Clearly, the neural network is massively over-parameterized, and yet the accuracies obtained are remarkable, with over 90% accu-racy on two of the data sets. 4. Data vs. Diversity: Who Wins?
The results from the last section clearly show that an
MLP learned on top of a library of high-quality, deep CNNs makes for an excellent, general-purpose, few-shot learner.
This leaves open the question. When basing a few-shot learner on a fairly simple, transfer-based approach, which is more important, diversity or size, when constructing the few-shot learner? That is, is it better to have a large va-riety of deep CNNs to combine together, each of which is trained on a smaller Dtrn , or is it preferable to base a few-shot learner on a single, deep CNN that has been trained on a larger and more diverse Dtrn ?
To answer this question, we compare the single MLP built upon all nine of the library deep CNNs with an MLP built upon a high-quality deep CNN that was itself con-the full ImageNet structed upon an even larger data set: data set, with more than 20,000 categories. High-quality, publicly available deep CNNs for this data set are rare, but
Google recently released a set of deep CNNs trained on the full ImageNet, specifically for use in transfer learning[13].
We consider three of their deep CNNs. Each is a ResNet:
BiT-ResNet-101-3 (“BiT” stands for “Big Transfer”; “101-3” is a ResNet101 that has 3X the width of a standard
ResNet), BiT-ResNet-152-4, and BiT-ResNet-50-1.
For each of these Big Transfer models, we perform a full hyperparameter search using the Caltech-UCSD Birds data set for validation, as we did for each of the deep CNNs trained on ILSVRC2012.
Interestingly, the Google mod-els tend to do better with a much larger L2 regularization parameter weight (0.5 to 0.7) compared to the other deep
CNNs trained on ILSVRC2012 (which typically performed best on the validation set using a weight of around 0.1). Re-sults are shown in Table 6.
The headline finding is that the single model utilizing a library of nine, ILSVRC2012-trained CNNs, is the best model. It did not not always perform the best on each data set. In fact, on four of the data sets (FC100, Texture, Fungi, and VGG Flower) at least one of the Google Big Transfer models outperformed the single model consisting of nine
ILSVRC2012-trained CNNs.
In each of the those cases, the performance was compa-rable across models, except, perhaps, for the VGG Flower data set, where the best Big Transfer models always ob-tained more than 99% accuracy. However, on the other data sets (Aircraft, Omniglot, Trafic Sign, and QuickDraw) the combined model far outperformed any of the Big Transfer models. The gap was often significant, and the combined model outperformed the best Big Transfer-based model by an average of more than 11% for the 40-way task.
It was also interesting that while the Big Transfer mod-els were generally better than the ILSVRC2012-trained li-brary CNNs, this varied across data sets. On the Aircraft and Omniglot data sets, for example, even the best individ-ual ILSVRC2012-trained library CNNs outperformed the
Big Transfer models.
All of this taken together seems to suggest that, when building a transfer-based few-shot learner, having a large library of deep CNNs is at least as important—and likely more important—than having access to a CNN that was trained on a very large Dtrn .
Aircraft
FC100
Omniglot
Texture
Traffic
Fungi
Quick Draw VGG Flower 5-way, 5-shot
Full Library
Hard Ensemble
Soft Ensemble
Best Single 68.9 ± 0.9 67.8 ± 0.9 68.4 ± 0.9 66.0 ± 0.9 79.1 ± 0.8 79.9 ± 0.7 80.5 ± 0.6 80.0 ± 0.6 97.5 ± 0.3 97.8 ± 0.2 98.0 ± 0.2 96.7 ± 0.2 85.3 ± 0.6 85.4 ± 0.5 85.7 ± 0.6 83.4 ± 0.6 85.8 ± 0.7 85.2 ± 0.7 85.2 ± 0.7 85.2 ± 0.7 81.2 ± 0.8 82.1 ± 0.7 82.0 ± 0.7 79.1 ± 0.7 84.2 ± 0.6 83.5 ± 0.6 84.1 ± 0.5 81.8 ± 0.6 97.4 ± 0.3 97.7 ± 0.2 97.9 ± 0.2 96.8 ± 0.3 20-way, 5-shot
Full Library
Hard Ensemble
Soft Ensemble
Best Single 49.5 ± 0.4 46.6 ± 0.4 47.5 ± 0.4 44.6 ± 0.4 61.6 ± 0.4 60.1 ± 0.4 60.7 ± 0.4 58.8 ± 0.4 95.4 ± 0.2 94.5 ± 0.2 94.9 ± 0.2 92.0 ± 0.2 68.5 ± 0.4 67.8 ± 0.4 68.2 ± 0.4 65.1 ± 0.4 70.4 ± 0.4 67.8 ± 0.4 68.3 ± 0.4 66.0 ± 0.4 65.5 ± 0.5 64.4 ± 0.4 64.4 ± 0.4 60.8 ± 0.5 69.4 ± 0.4 67.9 ± 0.4 68.8 ± 0.4 63.9 ± 0.4 94.3 ± 0.2 93.5 ± 0.2 93.7 ± 0.2 91.6 ± 0.2
Full Library
Hard Ensemble
Soft Ensemble
Best Single 41.2 ± 0.3 38.0 ± 0.3 39.0 ± 0.3 35.9 ± 0.2 51.8 ± 0.2 50.2 ± 0.2 51.2 ± 0.3 48.2 ± 0.3 93.2 ± 0.1 92.1 ± 0.1 92.5 ± 0.1 89.4 ± 0.2 59.3 ± 0.2 58.5 ± 0.2 59.0 ± 0.2 55.4 ± 0.2 62.7 ± 0.2 59.6 ± 0.2 60.2 ± 0.2 57.5 ± 0.2 57.6 ± 0.3 55.6 ± 0.3 56.5 ± 0.3 52.1 ± 0.3 60.8 ± 0.3 59.3 ± 0.3 60.1 ± 0.3 55.5 ± 0.3 91.9 ± 0.2 90.6 ± 0.2 91.1 ± 0.2 88.9 ± 0.2 40-way, 5-shot
Table 5: Accuracy obtained using all nine library CNNs as the basis for a few-shot learner. 5. Why Does This Work? 5.1. Few-Shot Fine-Tuning Is Surprisingly Easy
We turn our attention to asking: why does using a full library of pre-trained feature extractors seem to work so well? One reason is that fine-tuning appears to be very easy with a library of pre-trained features. Consider the follow-ing simple experiment, designed to test whether the number of training points has much effect on the learned model.
We choose a large number of 40-way, problems, over all eight of our benchmark data sets, and train two classi-fiers for each. Both classifiers use all of the 13,984 features provided by the nine library feature extractors. However, the first classifier is trained as a one-shot classifier, and the second classifier is trained using all of the available data in the data set. Our goal is to see whether the sets of learned weights have a strong correspondence across the two learn-ers; if they do, it is evidence that the number of training points has a relatively small effect. Note that in a neu-ral network with a hidden layer consisting of hundreds or thousands of neurons, there are likely to be a large num-ber of learned weights that are of equal quality; in fact, simply permuting the neurons in the hidden layer results in a model that is functionally identical, but that has very different weight matrices. Thus, we do not use a hidden layer in either classifier, and instead use a softmax directly on top of an output layer that linearly combines the input features. Both the one-shot and full-data classifiers were learned without regularization.
Over each of the eight benchmark data sets, for each fea-ture, we take the L1 norm of all of the 40 weights associated with the feature; this serves as an indication of the impor-tance of the feature. We compute the Pearson correlation coefficient for each of the 13,984 norms obtained using the models learned for the 1-shot and full data classifiers. These correlations are shown in Table 7.
What we see is a remarkably high correlation between the two sets of learned weights; above 80% correlation in every case, except for the Traffic Sign data set. However, even in the case of the Traffic Sign data set, there was a weak correlation (the Traffic Sign data is a bit of an outlier in other ways as well, as we will see subsequently).
This would seem to indicate that there is a strong signal with only one data point per class, as the learned weights do not differ much when they are learned using the whole data set. This may be one explanation for the surprising accuracy of the full library few-shot learner. Of course, the strong correlation may gloss over significant differences, and more data does make a significant difference (consider the differ-ence between the one-shot accuracies in Table 1 and the five-shot accuracies in Table 2.). But even one image per class seems to give a lot of information. 5.2. Different Problems Utilize Different Features
Another reason for the accuracy obtained by the full li-brary method may be that different problem domains seem to utilize different sets of features, and different feature ex-tractors. Having a large library ensures that some features relevant to any task are always present.
To investigate this, we construct a large number of 40-way training tasks over each of the various data sets, and for each, we learn a network without a hidden layer on top of all 13,984 features obtained using the library of deep CNNs.
Aircraft
FC100
Omniglot
Texture
Traffic
Fungi
QDraw
Flower 5-way, 5-shot
Full Library
BiT-ResNet-101-3
BiT-ResNet-152-4
BiT-ResNet-50-1 68.9 ± 0.9 54.0 ± 1.1 59.5 ± 1.0 61.9 ± 1.2 79.1 ± 0.8 78.6 ± 0.8 80.9 ± 0.7 79.0 ± 0.8 97.5 ± 0.3 82.5 ± 1.2 94.2 ± 0.5 87.2 ± 1.1 85.3 ± 0.6 82.0 ± 0.9 85.4 ± 0.6 84.2 ± 0.6 85.8 ± 0.7 69.2 ± 0.9 73.3 ± 0.8 75.6 ± 1.0 81.2 ± 0.8 81.2 ± 1.2 82.5 ± 0.9 82.5 ± 0.8 84.2 ± 0.6 63.7 ± 1.1 74.8 ± 0.8 71.5 ± 0.8 97.4 ± 0.3 99.6 ± 0.1 99.7 ± 0.1 99.3 ± 0.2
Full Library
BiT-ResNet-101-3
BiT-ResNet-152-4
BiT-ResNet-50-1 49.5 ± 0.4 35.8 ± 0.4 33.5 ± 0.4 39.6 ± 0.4 61.6 ± 0.4 60.4 ± 0.4 63.4 ± 0.4 60.9 ± 0.4 95.4 ± 0.2 87.8 ± 0.3 85.4 ± 0.4 83.9 ± 0.4 68.5 ± 0.4 69.6 ± 0.4 70.9 ± 0.4 66.4 ± 0.4 70.4 ± 0.4 51.1 ± 0.4 49.2 ± 0.4 53.5 ± 0.4 65.5 ± 0.5 68.4 ± 0.5 68.1 ± 0.5 68.7 ± 0.4 69.4 ± 0.4 57.0 ± 0.4 52.6 ± 0.5 55.0 ± 0.4 94.3 ± 0.2 99.3 ± 0.1 99.5 ± 0.1 99.1 ± 0.1 20-way, 5-shot
Full Library
BiT-ResNet-101-3
BiT-ResNet-152-4
BiT-ResNet-50-1 41.2 ± 0.3 24.6 ± 0.3 25.4 ± 0.2 33.0 ± 0.3 51.8 ± 0.2 49.6 ± 0.2 53.0 ± 0.3 48.8 ± 0.3 93.2 ± 0.1 56.4 ± 0.8 81.0 ± 0.3 84.6 ± 0.2 59.3 ± 0.2 61.5 ± 0.2 58.6 ± 0.2 60.0 ± 0.2 62.7 ± 0.2 40.2 ± 0.2 40.0 ± 0.2 46.9 ± 0.2 57.6 ± 0.3 60.3 ± 0.3 53.9 ± 0.4 59.2 ± 0.3 60.8 ± 0.3 28.9 ± 0.5 44.8 ± 0.3 48.3 ± 0.3 91.9 ± 0.2 99.0 ± 0.1 98.8 ± 0.1 99.0 ± 0.1 40-way, 5-shot
Table 6: Comparing a few-shot learner utilizing the full library of nine ILSVRC2012-trained deep CNNs with the larger
CNNs trained on the full ImageNet.
Aircraft Birds FC100 Fungi Omniglot Quick Draw Texture Traffic VGG Flower
Correlation 0.95 0.88 0.89 0.86 0.93 0.92 0.80 0.18 0.87
Table 7: Correlation between weight learned using one example per class, and the full data.
Again, we compute the L1 norm of the set of weights asso-ciated with each of the features. This time, however, for each problem we then consider the features whose norms are in the top 20%; these could be considered the features most important to solving the classification task.
For each of the (data set, data set) pairs, we compute the average Jaccard similarity of these top-feature sets. Since each set consists of 20% of the features, if each set of fea-tures chosen was completely random, for n features in all, 0.04n
.2n+.2n−0.04n = we would expect a Jaccard similarity of 0.04
.4−0.04 = 0.111. Anything greater indicates the sets of features selected are positively correlated; lower indicates a negative correlation. Results are in Figure 1. For each of the nine CNNs, we also compute the fraction of each CNN’s features that are in the top 20% when a full library classifier is constructed. These percentages are in Figure 2.
The data in these two plots, along with the previous re-sults, tells a consistent story: there appears to be little corre-spondence between data sets in terms of the set of features that are chosen as important across data sets. The largest
Jaccard value in Figure 1 is less than 0.5 (observed between
FC100 and Texture). This shows, in our opinion, a rela-tively weak correspondence. The Traffic Sign data set, had an average Jaccard of 0.108 across the other eight data sets, which is even lower than the 0.111 that would be expected under a purely random feature selection regime. One might speculate that the lack of correspondence across data sets is evidence for the hypothesis that different tasks tend to use different features, which would explain why it is so effective to use an entire library of deep CNNs for few-shot learning.
Also note that in Figure 2, we tend to see that different deep CNNs contribute “important” features at very differ-ent rates, depending on the particular few-shot classification task. This also seems to be evidence that diversity is impor-tant. In general, the DenseNets’ features are preferred over the ResNets’ features, but this is not universal, and there is a lot of variation. It may not be an accident that the three data sets where the selection of features from library CNNs shows the most diversity in Figure 2 (Traffic Sign, Quick
Draw, and Omniglot) are the three data sets where the clas-sifier built on top of all nine library CNNs has the largest advantage compared to the few-shot classifiers built on top of the single, “Big Transfer”-based deep CNN. 6.