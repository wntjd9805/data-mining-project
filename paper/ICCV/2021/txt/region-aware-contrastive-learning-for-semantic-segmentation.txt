Abstract
Recent works have made great success in semantic seg-mentation by exploiting contextual information in a local or global manner within individual image and supervis-ing the model with pixel-wise cross entropy loss. However, from the holistic view of the whole dataset, semantic rela-tions not only exist inside one single image, but also prevail in the whole training data, which makes solely consider-ing intra-image correlations insufﬁcient. Inspired by recent progress in unsupervised contrastive learning, we propose the region-aware contrastive learning (RegionContrast) for semantic segmentation in the supervised manner.
In or-der to enhance the similarity of semantically similar pix-els while keeping the discrimination from others, we em-ploy contrastive learning to realize this objective. With the help of memory bank, we explore to store all the represen-tative features into the memory. Without loss of generality, to efﬁciently incorporate all training data into the memory bank while avoiding taking too much computation resource, we propose to construct region centers to represent features from different categories for every image. Hence, the pro-posed region-aware contrastive learning is performed in a region level for all the training data, which saves much more memory than methods exploring the pixel-level rela-tions. The proposed RegionContrast brings little computa-tion cost during training and requires no extra overhead for testing. Extensive experiments demonstrate that our method achieves state-of-the-art performance on three benchmark datasets including Cityscapes, ADE20K and COCO Stuff. 1.

Introduction
Semantic segmentation, which aims to assign a category label to each pixel in an image, is a fundamental and chal-lenging problem in computer vision.
It has been widely applied to many applications, such as autonomous driving, scene understanding and image editing.
In the past few years, beneﬁting from the availabil-* Corresponding authors. (a) Most Current Methods (b) Proposed RegionContrast
Figure 1. Main difference between our method and previous ones. 1(a) Most existing methods only focus on intra-image relations. 1(b) Our proposed RegionContrast, apart from solely focusing on intra-image information, also considers inter-image correlations in a region level. ity of large-scale datasets such as ImageNet [11] and
Cityscapes [10], semantic segmentation has achieved sig-niﬁcant progress. In particular, based on the fully convo-lutional network (FCN) [30], many state-of-the-art meth-ods emerge, which focus on exploiting contextual informa-tion. DeepLabV3 [5] proposes ASPP module which ag-gregates spatial regularly sampled pixels at different dilated rates while PSPNet [52] proposes pyramid pooling module which partitions the feature maps into multiple regions be-fore pooling. Non-local Network [40] adopts self-attention mechanism to enable every pixel to receive information from all other pixels, resulting in a much more complete pixel-wise representation.
Aforementioned methods, though achieving satisfactory
segmentation results in most occasions, are still faced with critical drawback. Concretely, most current methods fo-cus on mining contextual information in all kinds of ways within the image, neglecting the potential relation informa-tion from other images. As illustrated in Fig. 1, inter-class relations are also worth exploring. For one region of an im-age, dilation convolution or self-attention mechanism can only enable it to receive information from some speciﬁc fea-tures of surrounding categories, while in reality, this kind of region could get in touch with much more kinds of features.
Hence, only exploring intra-image relations is not compre-hensive enough, which results in a demand for feature learn-ing from the holistic view of the whole dataset.
Recently, unsupervised contrastive learning has gained much attention in pre-training a strong feature extractor for downstream tasks such as image classiﬁcation or object de-tection. In brief, most works perform contrastive learning in the image level where all other images from the dataset are considered as negative samples while the very image with random augmentations is treated as the postive sam-ple. Beneﬁting from the utilization of memory bank, large amount of negative samples can be brought in to assist the contrastive learning for better feature representations. Note that most unsupervised contrastive learning methods focus on the classiﬁcation problem, while semantic segmentation, on the other hand, requires much more semantic informa-tion than classiﬁcation. Intuitively, to adapt to segmenta-tion problem, instead of performing image-level contrastive learning, we can adjust to a pixel-level one where pixels in-side and outside the very image get contrasted, as depicted in [41]. However, this kind of formulation suffers from a serious issue: pixels from different images may belong to the same category, which would deteriorate the subsequent feature learning. Hence, instead of sticking to unsupervised settings, we explore contrastive learning in the fully super-vised manner to obtain abundant category information.
In this work, we propose a new contrastive learning paradigm in a fully supervised way, targeting at semantic segmentation problem. With the corresponding categories of pixels as prior knowledge, the contrastive learning can be performed in a much more efﬁcient way. We will ﬁrst describe the most straightforward approach. Speciﬁcally, when the category of each pixel is known which arises from the prediction of the model, different memory banks that conforms to each class are built to store different classes of pixel embeddings. And for each pixel of an image, its corre-sponding positive and negative samples can be retrived from the memory banks, which would complete the contrastive learning process. Though simple and effective, this method would result in heavy memory burden since the number of pixels of an entire dataste is too large, which will also severely slow down the training speed.
To tackle the above issue while restoring enough em-beddings, we propose the region-aware contrastive learn-ing (RegionContrast).
In particular, since the region fea-tures for one class in an image are composed of all the pixel features belonging to this category, we can construct the region centers for different categories within one im-age. In that way, instead of pushing all the pixel embed-dings into memory banks, we just push several region cen-ters from different categories into the banks. Although an image may contain multiple regions that belong to the same category, the features in the embedding space are similar.
Thus for simplicity, we generate one region center for each class of one single image. To facilitate the feature learning for hard-to-classify pixels, we further propose a dynamic sampling method when generating the regions centers to allocate more attention to hard samples. After building the memory banks for different classes, region-aware con-trastive learning can be performed. Speciﬁcally, for one re-gion center of an image, its corresponding positive samples come from the embeddings in the memory bank of the same class, while its negative samples are embeddings from other memory banks. With positive and negative samples pro-vided, contrastive learning procedure can be implemented.
The overall framework of our RegionContrast is shown in Fig. 2, where conventional cross entropy loss works as a pixel-wise supervision and RegionContrast focuses on inter-image relation learning. Most importantly, the pro-posed RegionContrast can be easily applied into any seg-mentation models, and only demands little computation re-sources during training while requiring no extra overhead for testing.
To sum up, our contributions are summarized as follows:
• We propose a new contrastive learning setting in the fully supervised manner and target at the speciﬁc se-mantic segmentation problem.
• To adapt to segmentation scenario in a memory-efﬁcient way, we design an effective region-aware con-trastive learning (RegionContrast) to explore semantic relations from the holistic view of the whole dataset.
• We conduct extensive experiments on several public datasets, and obtain state-of-the-art performance on three semantic segmentation benchmarks, including
Cityscapes, ADE20K and COCO Stuff. 2.