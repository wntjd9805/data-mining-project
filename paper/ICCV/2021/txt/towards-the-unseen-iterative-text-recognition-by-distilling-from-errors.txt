Abstract
Visual text recognition is undoubtedly one of the most extensively researched topics in computer vision. Great progress have been made to date, with the latest models starting to focus on the more practical “in-the-wild” set-ting. However, a salient problem still hinders practical de-ployment – prior state-of-arts mostly struggle with recog-nising unseen (or rarely seen) character sequences. In this paper, we put forward a novel framework to speciﬁcally tackle this “unseen” problem. Our framework is iterative in nature, in that it utilises predicted knowledge of char-acter sequences from a previous iteration, to augment the main network in improving the next prediction. Key to our success is a unique cross-modal variational autoencoder to act as a feedback module, which is trained with the presence of textual error distribution data. This module importantly translates a discrete predicted character space, to a contin-uous afﬁne transformation parameter space used to condi-tion the visual feature map at next iteration. Experiments on common datasets have shown competitive performance over state-of-the-arts under the conventional setting. Most importantly, under the new disjoint setup where train-test labels are mutually exclusive, ours offers the best perfor-mance thus showcasing the capability of generalising onto unseen words (Figure 1 offers a summary). 1.

Introduction
Text recognition being a longstanding problem in com-puter vision plays a pivotal role in a diverse set of applica-tions, ranging from OCR systems [4, 42, 48, 54], navigation and guiding board recognition [10], to more recent ones such as visual question answering [5]. With the advance of deep learning [50, 62, 10, 43], recognition accuracy have notably increased over traditional methods [36]. Research focus has thus shifted to the more practical “in-the-wild” setting in an attempt towards ubiquity. Of these, irregular scene text recognition [50, 10, 58, 60] has gained consider-able attention, yet the focus is placed on irregular-image rectiﬁcation process [62, 58] other than the core recognition
Figure 1. Figure shows how the performance of baseline model
[50] is limited under disjoint setup where testing words are not en-countered during training. Our method performs way better than the baseline in the disjoint setup, reducing the performance gap with standard setup and showcasing its potential onto rarely seen words. Nevertheless, improvement in standard setup can be no-ticed over all datasets as well. problem itself.
In this paper, we continue this push towards practicality, albeit with a different perspective – we importantly focus on the understudied problem of unseen (or rarely seen) word recognition, where no (or limited) word image of a partic-ular character sequence is present during training. Our mo-tivation is straightforward – humans can recognise a word image, even when it falls beyond the scope of known vo-cabulary.
In fact, robustness of a text recognition frame-work largely depends on its performance on rarely or un-seen words [52]. Note that unlike the conventional zero-shot [59] setting where the transfer happens on class-level, here the combination of characters is “unseen” although the characters individually have come up in training. The fact that the sequences not being encountered during training is what makes this task challenging. Our solution for this
“unseen” problem is intuitive: (i) we leverage an iterative framework with a feedback mechanism to give the model a chance to re-visit its false predictions, and (ii) we explicitly ask such feedback to encapsulate useful information that would help the model to correct itself at the following it-eration.
Our ﬁrst contribution is therefore an iterative framework, where characters predicted in the previous iteration provide clues through a feedback loop [17] to enhance performance in the subsequent iterations. This is fundamentally different to current state-of-the-arts [50, 10, 29], most of which adapt
a feed-forward framework consisting of three-components (feature-extracting backbone, bidirectional-RNN encoder, and attention-based recurrent decoder). Despite the atten-tion mechanism, its single-pass nature still dictates wrong predictions, thereby leaving no chance for the model to re-cover. To this end, our iterative design enables the revi-sion of incorrect intermediate predictions in its subsequent steps, via a novel cross-modal (i.e., text prediction to image feature-maps) feedback mechanism. The key to our suc-cess lies with how feedback is progressed at each iteration.
A naive solution might be to apply an independent spelling correction network [13, 56] chained serially to a basic text recognition model. Apart from not being end-to-end train-able, this also ignores the intermediate visual features from the recognition network, ultimately bisecting the feedback loop. We on the other hand advocate that earlier word pre-dictions (text labels) should be fed back cross-modal to the main text recognition network and directly modulate the vi-sual feature maps at the next iteration. That is, the feedback module triggers a mapping from the discrete predicated la-bel space, to a continuous space of afﬁne transformation parameters (akin to [41]) which are consequently used to condition visual features (hence closing the feedback loop).
Simply knowing how feedback works is not enough – we still need to devise what information should be fed back to give a model its best shot at rectifying itself. For this, we re-sort to distilling knowledge from textual error distributions collected from state-of-the-art text recognition models – this is akin to humans who use prior experiences to help them to make corrections. For example, ‘hello’ might be wrongly predicted as ‘nello’ or ‘bello’ due to partial structural sim-ilarity of ‘h’ with ‘n’ or ‘b’. By distilling such error dis-tributions into the feedback module (during training only), the model will gain knowledge of correct character associ-ations. Our second contribution is therefore designing the feedback module via a conditional variational auto-encoder (CVAE) [51] that learns from such error distributions. More speciﬁcally, we augment the vanilla CVAE with an aux-iliary decoder that tries to directly reconstruct the correct word, given any incorrect prediction at each iteration. Note that deterministic alternatives such as typical feedback net-works [47, 17] or spelling correction (prediction reﬁning) networks, [13, 56] would not work well since they do not model the uncertainty among multiple erroneous alterna-tives, dictating a variational formulation like ours.
Our contributions are:
[a] We for the ﬁrst time pro-pose an iterative approach to speciﬁcally tackle the “un-seen” text recognition problem. [b] We design a conditional variational autoencoder to act as a feedback module, which works cross-modal to propagate predicted text labels from an earlier iteration to condition the visual features from the main network. [c] Our novel cross-modal feedback mod-ule is trained by distilling knowledge learned from textual error distributions that model multiple erroneous character sequences to a given candidate word.
Experiments conﬁrm our framework to be capable of adopting unseen words better than state-of-the-art frame-works on various public scene-text recognition and hand-writing recognition datasets.
Further ablative studies demonstrate the superiority of our iterative framework over naive spell checking [56, 13] and language model alterna-tives [22], and that the proposed feedback module can be plug-and-play with more than one base network. 2.