Abstract
Although current face manipulation techniques achieve impressive performance regarding quality and controllabil-ity, they are struggling to generate temporal coherent face videos. In this work, we explore to take full advantage of the temporal coherence for video face forgery detection.
To achieve this, we propose a novel end-to-end framework, which consists of two major stages. The first stage is a fully temporal convolution network (FTCN). The key insight of
FTCN is to reduce the spatial convolution kernel size to 1, while maintaining the temporal convolution kernel size un-changed. We surprisingly find this special design can ben-efit the model for extracting the temporal features as well as improve the generalization capability. The second stage is a Temporal Transformer network, which aims to explore the long-term temporal coherence. The proposed frame-work is general and flexible, which can be directly trained from scratch without any pre-training models or external datasets. Extensive experiments show that our framework outperforms existing methods and remains effective when applied to detect new sorts of face forgery videos. 1.

Introduction
With the development of deep generative models, espe-cially Generative Adversarial Networks (GANs) [29, 40, 7, 38, 31]. Current face manipulation techniques [31, 47, 48, 49, 28, 53, 52] are capable of manipulating the attributes or even the identity of face images. These forged images are even difficult to distinguish by humans, and thus may be abused for spreading political propaganda, damaging our trust in online media. Therefore, detecting face forgery is of paramount importance.
Most previous methods [60, 61, 43, 45] are trained for known face manipulation techniques. But they experience a dramatic drop in performance when the manipulation meth-ods are unseen. Some recent works [34, 57, 18, 59, 21, 42, 36, 11] have noticed this problem and attempted to boost the generalization. However, these methods are vulnerable to
*Corresponding author.
Figure 1. Temporal incoherence in existing datasets: FaceForen-sic++(FF++) [43], DeeperForensics(DFo) [28], Deepfake Detec-tion Challenge Preview(DFDC) [16], and FaceShifter(FSh) [31].
In the top 4 rows, we show four temporal incoherence that hap-pened between the neighborhood frames. In the last row, we show temporal incoherence happened between long-range frames. common perturbations such as image or video compression, noise, and so on. They still show limited generalization ca-pability. A particularly effective work is Face X-ray [32], which proposes to detect blending artifacts instead of gener-ative artifacts. However, the blending artifacts are typically low-level information that is susceptible to post-processing operations. More recent work LipForensics [22] proposes to detect unnatural mouth motion using spatio-temporal neural networks. But they only pay attention to the mouth which may ignore the artifacts in the other region of the face.
In this paper, we propose to leverage temporal coherence for more general face forgery detection. We observe that most face video forgeries are generated in a frame-by-frame manner. Since each altered face is generated independently, it inevitably leads to obvious flickering and discontinuity of
the face area (see Figure 1). So we can leverage the tem-poral incoherence for more general and robust video face forgery detection. Previous works try to leverage spatio-temporal convolution network [21] or the recurrent neural network [5, 36] to learn temporal incoherence. However, we find that they all failed to learn the general temporal in-coherence.
After careful investigation, we find that forged face videos mainly contain two types of artifacts, one is spatially related (e.g. blending boundary, checkboard, blur artifacts), the other is the temporal incoherence. Normally, the spa-tially related artifacts are more significant than the tempo-ral incoherence. Without any specific design, current video face forgery detection methods [21, 5, 36] may rely more on spatial-related artifacts instead of the temporal incoherence for classification.
To encourage the spatio-temporal convolution network to learn the temporal incoherence, we redesign the convo-lution operator and propose a fully temporal convolution network (FTCN). The key idea is to restrict the network’s capability for handling spatial-related artifacts. So we set the kernel size of all spatial (height and width) dimensions to 1 and keep the original kernel size of the time dimension in the 3D convolution operator. Due to the extremely low field for spatial dimension, the network learns to classify by temporal-related artifacts and hardly applies the spatial artifacts for detection. Also, we notice that even if the con-volution operator is only time-dependent, its capability is sufficient to distinguish between real or fake.
Moreover, we find some discontinuity may happen in frames that are not in the neighborhood, for example, the wrinkles or moles of a face may gradually appear or disap-pear. To handle this issue, we propose to leverage Trans-former [51] for capturing long-range dependencies along the time dimension. We add a light-weight Temporal
Transformer after the proposed FTCN. The FTCN and the
Temporal Transformer are trained end-to-end as the whole framework for general video face forgery detection.
Our approach is general and flexible.
It can achieve impressive results without any pre-training knowledge or hand-crafted datasets. In contrast, previous work LipForen-sics [22] relies heavily on pre-training and Face X-ray re-lies on hand-crafted dataset. More importantly, without any manual annotations, our method can locate and visualize the temporal incoherence in the face forgery videos.
We conduct extensive experiments to compare its per-formance with the state-of-the-art in various challenging scenarios. We find that our method significantly outper-forms previous methods in terms of generalization capabil-ity to unseen forgeries, and robust to various perturbations on videos. Furthermore, we perform ablation studies to val-idate the design choices of our framework.
Our contributions are summarized as follows:
• We explore to take full advantage of temporal coher-ence for face forgery detection and propose a frame-work that combined fully temporal convolution net-work (FTCN) and Temporal Transformer to explicitly detect temporal incoherence.
• Equipped with our detector, we can locate and visual-ize the temporal incoherence part of the face forgeries.
• Extensive experiments on various datasets demonstrate the superiority of our proposed methods with respect to generalization capability to unseen forgeries. 2.