Abstract
Advanced self-supervised visual representation learning methods rely on the instance discrimination (ID) pretext task. We point out that the ID task has an implicit semantic consistency (SC) assumption, which may not hold in uncon-strained datasets. In this paper, we propose a novel con-trastive mask prediction (CMP) task for visual representa-tion learning and design a mask contrast (MaskCo) frame-work to implement the idea. MaskCo contrasts region-level features instead of view-level features, which makes it possi-ble to identify the positive sample without any assumptions.
To solve the domain gap between masked and unmasked features, we design a dedicated mask prediction head in
MaskCo. This module is shown to be the key to the success of the CMP. We evaluated MaskCo on training datasets be-yond ImageNet and compare its performance with MoCo V2
[4]. Results show that MaskCo achieves comparable per-formance with MoCo V2 using ImageNet training dataset, but demonstrates a stronger performance across a range of downstream tasks when COCO or Conceptual Captions are used for training. MaskCo provides a promising alternative to the ID-based methods for self-supervised learning in the wild. 1.

Introduction
Self-supervised learning (SSL) of visual representation has been a great success in recent years [11, 3, 10, 2, 1], facilitating a wide range of downstream tasks, including both visual tasks [13, 22, 12, 15, 29], and cross-modality tasks [34, 31]. Recently, several contrastive learning based
SSL methods have demonstrated strong results on down-stream image classiﬁcation task [10], closing the perfor-mance gap between them and supervised learning methods.
As SSL methods do not require human annotation of train-ing datasets, people are optimistic about the day when they surpass supervised feature learning methods when virtually
* Equal contribution.
† Interns at MSRA.
‡ Corresponding author. (a) An example image that satis-ﬁes the SC assumption. (b) An example image that does not satisfy the SC assumption.
Figure 1. Illustration of semantic consistency (SC) assumption by visualizing two cropped views of an image. (a) An image from the
ImageNet dataset satisﬁes the SC assumption, as the two cropped views sample different parts of the same ﬁsh. (b) An image from the COCO dataset does not satisfy the SC assumption, as the two cropped views contain different objects belonging to different se-mantic categories. unlimited data are made use of.
The state-of-the-art (SOTA) SSL methods for visual fea-ture learning are mostly based on a pretext task called in-stance discrimination (ID) [28]. The idea is illustrated in
Figure 2. The intuition behind is that we can learn a good feature representation by merely asking the feature to be discriminative of individual instances. To realize this idea,
SOTA SSL methods [11, 3] optimize a contrastive loss func-tion which enforces the model to output sufﬁciently close representations for different views of a single image. Since semantic information is a principal component in visual features, an implicit assumption here is that a training im-age should have consistent semantic meaning across views, which we call semantic consistency (SC) assumption.
The SC assumption is almost always satisﬁed in the Ima-geNet dataset [5], which exhibits an object-centric bias [21].
An example image from the ImageNet dataset and its two cropped views are shown in Figure 1(a). The two crops sample different parts of the same ﬁsh, so it is quite reason-able to let them have similar feature representations. How-ever, if the ID-based SSL methods are to be extended to datasets beyond ImageNet, they will need to handle images
like the one as shown in Figure 1(b). The two cropped views of this example image contain entirely different objects be-longing to different semantic categories. Is it still reason-able or helpful to enforce similar feature representations for such crops? In other words, are ID-based SSL meth-ods readily applicable to the unconstrained datasets without
SC guarantee?
The initial studies are not encouraging. Purushwalkam et.al. [21] notice that the ID pretext task takes advantage of the object-centric bias in the ImageNet dataset. In addi-tion, some preliminary work [11, 10, 17] tries to directly extend ID-based SSL methods from ImageNet to other datasets, including Instagram-1B [16], Places365 [35], and
YFCC100M [24], but they do not get satisfactory results.
These facts cast a shadow over the future of ID-based SSL methods. After all, being able to make effective use of large amounts of unconstrained data is the most competitive fea-ture of SSL methods.
We are therefore motivated to explore alternative pretext tasks which do not rely on the SC assumption. A task that once achieved great success in natural language pre-training has entered our sight. It is the mask prediction task, also known as the masked language model (MLM) in BERT [6].
In the vision domain, image inpainting is also a mask pre-diction task and has been used as a pretext task for visual representation learning [19]. However, image inpainting is a generative method operating in the pixel space. As Grill et al. pointed out, such method is computationally expensive, and the high level of detail required for image generation may not be necessary for representation learning [10].
In this paper, we propose to use contrastive mask predic-tion as a pretext task for self-supervised visual representa-tion learning. The idea is illustrated in Figure 2 (bottom).
We make the task contrastive so that the extracted features can focus on high-level semantic meanings instead of pixel-level details. In order to realize this idea, we design a novel
SSL method named Mask Contrast (MaskCo). In the de-sign of MaskCo, we answer two basic questions as to what features to contrast and how to bridge the domain gap be-tween features of masked and unmasked regions. First, we propose to use region-level features instead of view-level features to compute contrastive loss. This choice is the key to get rid of the SC assumption because we compare the masked and unmasked versions of the exact region instead of comparing two views. Second, as a domain gap exists between the predicted features from the masked and un-masked view, we insert a mask prediction head (MPH) into the network to bridge the gap. Both quantitative and qualita-tive results have demonstrated that MPH is an indispensable component in MaskCo.
Evaluations of MaskCo on a range of datasets show promising results. While MaskCo achieves comparable per-formance with MoCo V2 [4] when using ImageNet as the
Figure 2. Illustration of pretext tasks for SSL: the commonly used instance discrimination task and the proposed contrastive mask prediction task. pre-training dataset, it achieves much stronger performance than MoCo on multiple downstream tasks when the uncon-strained datasets COCO [14] or Conceptual Captions [23] are used for pre-training. These results verify that MaskCo has relaxed the SC assumption, and has high potential to be used for learning image representations in the wild. 2.