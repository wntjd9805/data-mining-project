Abstract
In this paper, we study the problem of Novel Class Dis-covery (NCD). NCD aims at inferring novel object cate-gories in an unlabeled set by leveraging from prior knowl-edge of a labeled set containing different, but related classes. Existing approaches tackle this problem by con-sidering multiple objective functions, usually involving spe-cialized loss terms for the labeled and the unlabeled sam-ples respectively, and often requiring auxiliary regulariza-tion terms.
In this paper we depart from this traditional scheme and introduce a UNified Objective function (UNO) for discovering novel classes, with the explicit purpose of favoring synergy between supervised and unsupervised learning. Using a multi-view self-labeling strategy, we gen-erate pseudo-labels that can be treated homogeneously with ground truth labels. This leads to a single classification ob-jective operating on both known and unknown classes. De-spite its simplicity, UNO outperforms the state of the art by a significant margin on several benchmarks (≈+10% on
CIFAR-100 and +8% on ImageNet). The project page is available at : https://ncd-uno.github.io. 1.

Introduction
Deep learning has enabled astounding progresses in computer vision. However, the necessity of large annotated training sets for these models is often a limiting factor. For instance, training a deep neural network for classification requires a large amount of labeled data for each class of interest. This constraint is even more severe in scenarios where collecting sufficient data for each class is expensive or even impossible, as for instance in medical applications.
To alleviate these problems, Novel Class Discovery (NCD) [7, 6, 8] has recently emerged as a practical solu-tion. NCD aims at training a network that can simulta-neously classify a set of labeled classes while discovering new ones in an unlabeled image set. The basic motivation
*Corresponding author
Figure 1: A visual comparison of our UNified Objective (UNO) with previous works. Existing approaches tackle
NCD using multiple objective functions such as supervised, clustering and auxiliary objectives. On the contrary, we devise a single classification objective operating on both known and unknown classes. behind this setting is that the network can benefit from the supervision available on the labeled set to learn rich image representations that can be transferred to discover unknown classes in the unlabeled set. At training time, data are split into a set of labeled images and a set of unlabeled images, assuming disjoint sets of classes. These two sets are used to train a single network to classify both the known and the unknown classes. Note that this problem is similar but dif-ferent from semi-supervised learning [22, 24], because, in the latter, the working assumption is that labeled and un-labeled sets share the same classes. Differently, in NCD, the two sets of classes are supposed to be disjoint. More-over, differently from common clustering [1, 25] scenarios, in an NCD framework, labeled data can be utilized at train-ing time, and the challenge consists in transferring the su-pervised knowledge on the known classes to improve clus-tering of the unknown ones.
Most NCD methods usually perform an initial super-vised pretraining step on the labeled set, followed by a clus-tering step on the unlabeled data [8, 10, 11]. This simple pipeline provides an effective means to transfer the repre-sentation capability from the labeled set to the unlabeled one. Generally speaking, these approaches combine two separated objectives. On the one hand, there is direct super-vision through labels on the labeled set. On the other hand, a clustering objective is used to discover the novel cate-gories. Clustering objectives are generally based on pseudo-labels [7, 12, 14, 29, 30, 31] estimated on the unlabeled set.
In practice, these objectives are combined through indepen-dent losses such as cross-entropy (CE) and binary cross-entropy (BCE), respectively. Usually, the BCE loss is com-puted with pseudo pairwise labels often determined by set-ting an ad-hoc threshold which heavily influences the per-formance of these methods.
Additionally, NCD approaches generally require a strong semantic similarity between labeled and unlabeled classes in order to obtain expressive representation for discovering new concepts. In order to decrease the bias of the features toward known classes, Han et al. [7] propose to use an addi-tional phase of self-supervised pretraining on all available images, both labeled and unlabeled, before the supervised pretrain. Moreover, the clustering stage is strengthened with another self-supervised objective (consistency), which enforces the model to output similar predictions for two different data augmentations of the same image. Adding an additional auxiliary objective makes optimization of this model even more cumbersome as it requires to further tune the hyper-parameters for each of these competing objec-tives. Moreover, this method assumes the availability of the unlabeled set in the pretraining stage. This is not suitable when it comes to learning in a sequential fashion as it re-quires to repeat the costly self-supervised pretraining stage every time that the unlabeled set changes.
Motivated by the need of simplifying NCD approaches, and inspired by the recent advancements in self-supervised learning [2, 3], in this paper we propose to eliminate the self-supervised pretraining step and unify all the objectives through a single loss function (see Fig. 1). Specifically, us-ing a multi-view self-labeling strategy, we generate pseudo-labels that can be treated homogeneously with ground truth labels. This makes it possible to use a unified cross-entropy loss on both the labeled and the unlabeled set. In more de-tail, given a batch of images, we generate two views of each image using random transformations. Then, our network predicts a probability distribution over all classes (labeled
+ unlabeled) for each view. This results in two sub-batches that are independently clustered, so the cluster-assignment of each view is simply used as the pseudo-label for the other view. Ground truth and pseudo-labels are then used in com-bination to provide feedback to the network and update its parameters. Importantly, using a unified framework that op-erates on the complete class set enables us to learn a single model that can jointly recognize both labeled and unlabeled classes. We emphasize that this is a key point that is often neglected in the existing solutions for the NCD task.
Contributions. Our contributions can be summarized as (i) we introduce a UNified Objective (UNO) follows: for NCD where cluster pseudo-labels are treated homo-geneously with ground truth labels, allowing a single CE loss to operate on both labeled and unlabeled sets; (ii) us-ing multi-view, multi-head and over-clustering strategies we learn powerful representations while discovering new classes, de facto eliminating the need for self-supervised pretraining in NCD; (iii) experimentally, we show that our method surpasses all previous works on three publicly avail-able benchmarks by a large margin. Notably, we outperform previous methods by 8% in accuracy on ImageNet, and by
≈+10% on CIFAR-100. (iv) Finally, we push NCD to the limit by changing the proportion of labeled and unlabeled samples, and find that our objective outperforms the state-of-the-art even more significantly on complex benchmarks. 2.