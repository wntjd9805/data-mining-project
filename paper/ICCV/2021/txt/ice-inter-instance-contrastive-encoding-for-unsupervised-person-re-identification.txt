Abstract
Unsupervised person re-identification (ReID) aims at learning discriminative identity features without annota-tions. Recently, self-supervised contrastive learning has gained increasing attention for its effectiveness in unsu-pervised representation learning. The main idea of in-stance contrastive learning is to match a same instance in different augmented views. However, the relationship between different instances has not been fully explored in previous contrastive methods, especially for instance-level contrastive loss. To address this issue, we propose Inter-instance Contrastive Encoding (ICE) that leverages inter-instance pairwise similarity scores to boost previous class-level contrastive ReID methods. We first use pairwise sim-ilarity ranking as one-hot hard pseudo labels for hard in-stance contrast, which aims at reducing intra-class vari-ance. Then, we use similarity scores as soft pseudo labels to enhance the consistency between augmented and orig-inal views, which makes our model more robust to aug-mentation perturbations. Experiments on several large-scale person ReID datasets validate the effectiveness of our proposed unsupervised method ICE, which is competitive with even supervised methods. Code is made available at https://github.com/chenhao2345/ICE. 1.

Introduction
Person re-identification (ReID) targets at retrieving an person of interest across non-overlapping cameras by com-paring the similarity of appearance representations. Super-vised ReID methods [28, 2, 22] use human-annotated labels to build discriminative appearance representations which are robust to pose, camera property and view-point varia-tion. However, annotating cross-camera identity labels is a cumbersome task, which makes supervised methods less scalable in real-world deployments. Unsupervised methods
[20, 21, 32] directly train a model on unlabeled data and thus have a better scalability.
Most of previous unsupervised ReID methods [27, 11, 41] are based on unsupervised domain adaptation (UDA).
UDA methods adjust a model from a labeled source domain to an unlabeled target domain. The source domain provides a good starting point that facilitates target domain adapta-tion. With the help of a large-scale source dataset, state-of-the-art UDA methods [11, 41] significantly enhance the per-formance of unsupervised ReID. However, the performance of UDA methods is strongly influenced by source dataset’s scale and quality. Moreover, a large-scale labeled dataset is not always available in the real world. In this case, fully un-supervised methods [20, 21] own more flexibility, as they do not require any identity annotation and directly learn from unlabeled data in a target domain.
Recently, contrastive learning has shown excellent per-formance in unsupervised representation learning. State-of-the-art contrastive methods [38, 5, 14] consider each image instance as a class and learns representations by matching augmented views of a same instance. As a class is usu-ally composed of multiple positive instances, it hurts the performance of fine-grained ReID tasks when different im-ages of a same identity are considered as different classes.
Self-paced Contrastive Learning (SpCL) [13] alleviates this problem by matching an instance with the centroid of the multiple positives, where each positive converges to its cen-troid at a uniform pace. Although SpCL has achieved im-pressive performance, this method does not consider inter-instance affinities, which can be leveraged to reduce intra-class variance and make clusters more compact. In super-vised ReID, state-of-the-art methods [2, 22] usually adopt a hard triplet loss [16] to lay more emphasis on hard sam-ples inside a class, so that hard samples can get closer to normal samples. In this paper, we introduce Inter-instance
Contrastive Encoding (ICE), in which we match an instance with its hardest positive in a mini-batch to make clusters more compact and improve pseudo label quality. Matching the hardest positive refers to using one-hot “hard” pseudo labels.
Since no ground truth is available, mining hardest pos-itives within clusters is likely to introduce false positives into the training process. In addition, the one-hot label does not take the complex inter-instance relationship into consid-eration when multiple pseudo positives and negatives exist
in a mini-batch. Contrastive methods usually use data aug-mentation to mimic real-world distortions, e.g., occlusion, view-point and resolution variance. After data augmenta-tion operations, certain pseudo positives may become less similar to an anchor, while certain pseudo negatives may become more similar. As a robust model should be invari-ant to distortions from data augmentation, we propose to use the inter-instance pairwise similarity as “soft” pseudo labels to enhance the consistency before and after augmentation.
Our proposed ICE incorporates class-level label (cen-troid contrast), instance pairwise hard label (hardest posi-tive contrast) and instance pairwise soft label (augmenta-tion consistency) into one fully unsupervised person ReID framework. Without any identity annotation, ICE signifi-cantly outperforms state-of-the-art UDA and fully unsuper-vised methods on main-stream person ReID datasets.
To summarize, our contributions are: (1) We propose to use pairwise similarity ranking to mine hardest samples as one-hot hard pseudo labels for hard instance contrast, which reduces intra-class variance. (2) We propose to use pairwise similarity scores as soft pseudo labels to enhance the con-sistency between augmented and original instances, which alleviates label noise and makes our model more robust to augmentation perturbation. (3) Extensive experiments highlight the importance of inter-instance pairwise similar-ity in contrastive learning. Our proposed method ICE out-performs state-of-the-art methods by a considerable margin, significantly pushing unsupervised ReID to real-world de-ployment. 2.