Abstract
For all the ways convolutional neural nets have revo-lutionized computer vision in recent years, one important aspect has received surprisingly little attention: the effect of image size on the accuracy of tasks being trained for.
Typically, to be efﬁcient, the input images are resized to a relatively small spatial resolution (e.g. 224 × 224), and both training and inference are carried out at this resolu-tion. The actual mechanism for this re-scaling has been an afterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic are commonly used in most machine learning software frameworks. But do these resizers limit the on-task performance of the trained networks? The an-swer is yes. Indeed, we show that the typical linear resizer can be replaced with learned resizers that can substantially improve performance. Importantly, while the classical re-sizers typically result in better perceptual quality of the downscaled images, our proposed learned resizers do not necessarily give better visual quality, but instead improve task performance.
Our learned image resizer is jointly trained with a base-line vision model. This learned CNN-based resizer creates machine friendly visual manipulations that lead to a con-sistent improvement of the end task metric over the baseline model. Speciﬁcally, here we focus on the classiﬁcation task with the ImageNet dataset [26], and experiment with four different models to learn resizers adapted to each model.
Moreover, we show that the proposed resizer can also be useful for ﬁne-tuning the classiﬁcation baselines for other vision tasks. To this end, we experiment with three different baselines to develop image quality assessment (IQA) mod-els on the AVA dataset [24]. 1.

Introduction
The emergence of deep neural networks along with large scale image datasets has led to major breakthroughs in ma-chine visual recognition. Images in such datasets are typ-ically obtained from the web and as a result have gone through various capture pipelines and post-processing steps.
Besides these generally unknown processing operations, ef-Figure 1. Our proposed framework for joint learning of the image resizer and recognition models.
Task
Model
Bilinear Resizer
Proposed Resizer
Top-1 Error ↓ n o i t a c
ﬁ i s s a l
C
A
Q
I
Inception-v2 [34]
DenseNet-121 [9]
ResNet-50 [8]
MobileNet-v2 [28] 26.7% 33.1% 24.7% 29.5% 24.0% 29.8% 23.0% 28.4%
PLCC ↑
Bicubic Resizer
Proposed Resizer
Inception-v2 [34]
DenseNet-121 [9]
EfﬁcientNet-b0 [38] 0.662 0.662 0.642 0.686 0.683 0.671
Table 1. Summary of our results for image classiﬁcation on Im-ageNet [26], and image quality assessment (IQA) on the AVA dataset [24].
ﬁcient training of visual recognition CNNs requires addi-tional image augmentations such as spatial resizing.
Image down-scaling is the most commonly used pre-processing module in classiﬁcation models. The main (1) mini-batch learning reasons for spatial resizing are: through gradient descent requires the same spatial resolu-tion for all images in a batch, (2) memory limitations pro-hibit training CNNs at high resolutions, and (3) large image sizes lead to slower training and inference. Given a ﬁxed memory budget, there is a trade-off between the memory occupied by the spatial resolution and the batch size. This trade-off can have a signiﬁcant impact on the accuracy of recognition CNNs [27, 39, 10, 12].
Currently rudimentary resizing methods such as nearest neighbor, bilinear, and bicubic are among the top adopted image resizers visual recognition systems. These resizers
are fast, and can be ﬂexibly integrated into the train and test frameworks. However, these methods were developed decades before deep learning became the mainstream so-lution for visual recognition tasks, and hence are not opti-mized or adequate for machine perception.
Recent research on recognition-aware image process-ing has shown promising results on improving accuracy of classiﬁcation models and simultaneously preserving the perceptual quality [20, 29]. This class of methods keep the classiﬁcation model ﬁxed, and only train the enhance-ment module. Meanwhile, there has been some effort on joint learning of both the pre-processor and the recognition model [2, 44, 19, 30, 18, 16, 45]. These algorithms set up training frameworks with hybrid losses that allow for learn-ing better enhancement and recognition, concurrently.
In practice, however, a recognition pre-processing operation such as resizing should not be optimized for better percep-tual quality, because the end goal is for the recognition net-work to produce accurate results, not for the intermediate image to “look good” to a human observer.
In this paper we propose a novel image resizer that is jointly trained with classiﬁcation models (see Figure 1), and is speciﬁcally designed to improve classiﬁcation per-formance (see Table 1). To summarize our contributions:
• We couple our resizer with various classiﬁcation mod-els and show that it effectively adapts to each model and consistently improves over the baseline image classiﬁer.
• The proposed resizer is not constrained by any pixel or perceptual loss, therefore our results present ma-chine adaptive visual effects that differ from conven-tional image processing and super-resolution results.
• The proposed resizer model allows for down-scaling images at arbitrary scaling factors, hence we can con-veniently search for the most optimal resolution for an underlying task.
• We expand the application of the proposed resizer to image quality assessment (IQA) and show that it suc-cessfully adapts to this task.
We purport the proposed method is the ﬁrst pre-processing model that is speciﬁcally developed for vision tasks, and that aims to replace off-the-shelf resizers. Per-haps remote machine learning inference can be mentioned as a distinguishing application of the proposed resizer. In remote inference, transferring full resolution Mega pixel images from clients to servers imposes a signiﬁcant latency on the system. While down-scaling by off-the-shelf resiz-ers on the client side may reduce the latency issue, it may also negatively impact the recognition performance. The
Original (480 × 640)
Learned Resizer (192 × 256)
Figure 2. Example of the proposed resizer trained for image clas-siﬁcation on the ImageNet dataset [26]. The baseline classiﬁcation model is Inception-v2 [34], and it is jointly trained with the resizer model shown in Fig. 3. The resized image ﬁts the classiﬁcation task better than the existing pre-processing resizers such as bilin-ear and bicubic. proposed resizer, therefore, can be an alternative to the off-the-shelf resizers to effectively reduce the expected drop in the recognition performance.
Next, we brieﬂy survey the research works related to this paper. Then, in Section 3 the proposed resizer model is discussed in detail. In Section 4 our results are presented, and ﬁnally we conclude in Section 5. 2.