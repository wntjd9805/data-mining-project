Abstract
The ability to reliably estimate physiological signals from video is a powerful tool in low-cost, pre-clinical health monitoring.
In this work we propose a new approach to remote photoplethysmography (rPPG) – the measurement of blood volume changes from observations of a person’s face or skin. Similar to current state-of-the-art methods for rPPG, we apply neural networks to learn deep repre-sentations with invariance to nuisance image variation. In contrast to such methods, we employ a fully self-supervised training approach, which has no reliance on expensive ground truth physiological training data. Our proposed method uses contrastive learning with a weak prior over the frequency and temporal smoothness of the target signal of interest. We evaluate our approach on four rPPG datasets, showing that comparable or better results can be achieved compared to recent supervised deep learning methods but without using any annotation. In addition, we incorporate a learned saliency resampling module into both our unsuper-vised approach and supervised baseline. We show that by allowing the model to learn where to sample the input im-age, we can reduce the need for hand-engineered features while providing some interpretability into the model’s be-havior and possible failure modes. We release code for our complete training and evaluation pipeline to encourage re-producible progress in this exciting new direction.1 1.

Introduction
Understanding the physiological state of a person is im-portant in many application areas, from health and fitness through to human resource management and human ma-chine interaction. Conventional approaches to estimate such information, such as electrocardiograms (ECG) or photo-*Equal contribution 1https://github.com/ToyotaResearchInstitute/
RemotePPG
Figure 1. From a video of a person’s face alone, our model learns to estimate the person’s cardiac activity in the form of a pho-toplethysmographic (PPG) signal (left) observed through tem-poral patterns in the video, as well as a saliency signal (right) which shows where in the video the model’s estimated activity is strongest (in this case, the center of the forehead). We show through extensive experiments on video datasets with physiologi-cal ground truth that our approach can match and sometimes even improve upon existing end-to-end supervised methods, while pro-viding both interpretability into the model behavior and incurring zero annotation cost to train. The figure above shows a real output from our model trained on the UBFC [3] dataset. We note that the phase offset between predicted and ground truth PPG signals may be due to synchronization issues between the video and ground truth itself - a detail discussed further in Sec. 3.4. plethysmograms (PPG), require interaction with the subject and are troublesome to setup, limiting their usefulness and scalability. In recent years, research utilizing advances from the field of computer vision and machine learning has ex-plored and improved upon methods for passively monitor-ing physiological information from videos of subjects.
In this work we introduce a new method for remote pho-toplethysmography (rPPG), or imaging PPG, a technique in which changes in transmitted or reflected light from the body due to volumetric changes in blood flow are measured at a distance using a standard imaging device. This differs
from the more intrusive contact PPG, in which the same signal is measured at peripheral body tissues such as the fingertips via a contact sensor which projects and measures reflected LED light. Compared to PPG, the signals for re-mote PPG are often too subtle for the human eye to per-ceive, but under certain illumination conditions, can be iso-lated and magnified in digital imagery if one knows where to look [33]. By finding these signals and using them to esti-mate underlying cardiac activity, particularly from webcam-quality video such as shown in the input video of Fig. 1, rPPG can therefore help to meet a need for low-cost, non-contact health monitoring.
In the field of computer vision, many researchers have tackled the problem of rPPG in the past, leaning on a wide variety of techniques from signal processing and machine learning (see e.g. [5, 17, 23, 32, 35]). Recent efforts have tended to favor deep learning, which is known for solv-ing particular tasks well by discovering feature represen-tations that are robust to many forms of nuisance varia-tion.
In rPPG, such variation takes the form of lighting changes, motion, and changes in facial appearance or ges-ture, all of which can easily obscure the underlying PPG signal. Supervised deep learning approaches to rPPG such as [5, 16, 23, 35] have shown that, with annotated data to train on, rPPG can be achieved with higher robustness to such variation. However, the cost of annotated data is not cheap, due to the need to equip subjects with contact PPG or ECG sensors while capturing data. It is therefore hard to scale the capture of such datasets, although, driven by data-hungry algorithms, there have been recent efforts in this direction [22, 23].
In this work we take a contrary approach to applying deep learning to rPPG. We view the problem through the lens of self-supervised learning, and in doing so bridge the data economy of older approaches with the robustness of learned representations. Our contrastive training approach is built around three assumptions about the underlying sig-nal of interest:
A1 We assume the signal of interest lies within a certain range. We set this range for the rest of the paper at 40 to 250 beats per minute, which captures the vast majority of human heart rates [1].
A2 We assume the signal of interest typically does not vary rapidly over short time intervals: the heart rate of a person at time t is similar to their heart rate at t + W , where W is in the order of seconds.
A3 Finally, the signal has some visible manifestation (even if undetectable to the human eye) and is the dominant visual signal within the target frequency range.
Contributions. We show that by setting up a con-trastive learning framework based on these assumptions, it is possible to train a deep neural network to estimate the
PPG signal (and therefore track the heart rate) of a sub-ject from video of their face, entirely without ground truth training data. We introduce novel loss functions for both supervised and contrastive training that are robust to de-synchronization in the ground truth and take advantage of our above assumptions. Moreover, since the behavior of a deep neural network regressing PPG alone may be difficult to comprehend or have confidence in without access to an-notated data, we propose a front-end saliency-based sam-pling module, inspired by [28], to accentuate the parts of the input data which are most relevant to a back-end PPG esti-mator. A by-product of this, as shown in Fig. 1, is that our model can also output interpretable saliency maps. These maps provides some transparency into the spatial location of the model’s discovered signal of interest; in this case, the subject’s forehead and parts of her nose and cheeks, which matches the conventional understanding of where the rPPG signal is strongest [14, 21]. We note that these contribu-tions are independent but complementary. As shown in our experiments, the saliency sampler can be appended to both supervised and contrastive models with similar effect, and our contrastive model can learn to predict PPG without the saliency sampler. Finally, we unify existing freely avail-able PPG video datasets and provide our complete training and evaluation pipeline to encourage further reproducible efforts in what we believe is an exciting direction of re-search in computer vision for human health monitoring. 2.