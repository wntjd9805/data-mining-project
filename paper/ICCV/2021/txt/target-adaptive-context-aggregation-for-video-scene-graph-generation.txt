Abstract
This paper deals with a challenging task of video scene graph generation (VidSGG), which could serve as a struc-tured video representation for high-level understanding tasks. We present a new detect-to-track paradigm for this task by decoupling the context modeling for relation predic-tion from the complicated low-level entity tracking. Speciﬁ-cally, we design an efﬁcient method for frame-level VidSGG, termed as Target Adaptive Context Aggregation Network (TRACE), with a focus on capturing spatio-temporal con-text information for relation recognition. Our TRACE framework streamlines the VidSGG pipeline with a modu-lar design, and presents two unique blocks of Hierarchical
Relation Tree (HRTree) construction and Target-adaptive
Context Aggregation. More speciﬁc, our HRTree ﬁrst pro-vides an adpative structure for organizing possible relation candidates efﬁciently, and guides context aggregation mod-ule to effectively capture spatio-temporal structure informa-tion. Then, we obtain a contextualized feature representa-tion for each relation candidate and build a classiﬁcation head to recognize its relation category. Finally, we pro-vide a simple temporal association strategy to track TRACE detected results to yield the video-level VidSGG. We per-form experiments on two VidSGG benchmarks: ImageNet-VidVRD and Action Genome, and the results demonstrate that our TRACE achieves the state-of-the-art performance.
The code and models are made available at https:// github.com/MCG-NJU/TRACE. 1.

Introduction
Video understanding tasks, such as action recogni-tion [29, 1, 38, 39], temporal action localization [47, 20, 31], spatio-temporal action detection [18, 5], have received lots of research attention in the past few years. Most of these methods simply provide a single label or spatio-temporal extent of each action instance in a long video sequence. (cid:66): Corresponding author.
Figure 1. (a) An example of video-level VidSGG. The sub-ject/object nodes in this graph are entity trajectories and their re-lation is constant for this clip. (b) An example for frame-level
VidSGG. The frame-level VidSGG is determined by short-term information for each frame and could vary along time.
As shown in Figure 1,
However, an ideal video understanding system is expected to not only recognize the action types, but also provide de-tailed and structured interpretation of the whole scene by parsing an event into a sequence of interactions between different visual entities. This structured video representa-tion, known as video scene graph [27], can contribute to more accurate action recognition [10] and allow for our vision models to tackle high-level and complex inference tasks, such as video caption [7, 36], video retrieval [4, 16], and video question answering [33, 15]. Nevertheless, video scene graph generation (VidSGG) [25, 34, 22, 30] has re-ceived much less research efforts in our community, when compared with image scene graph generation [41, 43, 17]. the existing benchmark of
VidSGG can be roughly grouped into two types according to the granularity of its graph representation: (1) video-level scene graph generation, where each graph node represents an object trajectory, and the edge captures the relation be-tween visual entities, which is constant for one clip. (2) frame-level scene graph generation, where the graph is de-ﬁned at frame level and the relation could change over time in this short clip. For video-level VidSGG, it requires to ac-curately trim long videos into short clips (e.g., 30 frames) in advance, according to the precise temporal boundaries of relations. This setting cannot be easily adapted to realistic
VidSGG in untrimmed videos, as trimming is difﬁcult and subjective due to temporal ambiguity. In contrast, frame-level VidSGG provides a more ﬂexible mechanism for rela-tion representation in continuous video streams. In addition, these frame-level VidSGG could easily yield the video-level scene graph by using temporal association to track adjacent results. However, previous works [27, 25, 22] on VidSGG mainly neglect the frame-level scene graphs, and directly recognize video-level relations based on the results of ob-ject tracking. As a result, they all yield a heavy pipeline highly dependent on tracking.
In this paper, we aim to present a new method to address the above two tasks simultaneously. Our basic idea is to ﬁrst generate the video scene graph at each frame by utilizing short-term video information, and then track each frame-level scene graph along time dimension to obtain the video-level result. We argue that this detect-to-track VidSGG paradigm will decouple the tasks of video relation recogni-tion and temporal tracking, making our method focus more on modeling spatio-temporal context in videos. The key of recognizing visual relation is that inferring the interaction between visual entities usually requires comprehensive un-derstanding of spatio-temporal context information in the video. For instance, recognizing whether a person is sitting on a sofa or standing from a sofa is based on the tempo-ral variation of human movement with respect to sofa over time. Thus, we aim to devise a modular framework that can effectively determine and capture such complex spatio-temporal contextual information (e.g., temporal motion, ob-ject relation, person relation etc.) for efﬁcient VidSGG.
Spatio-temporal context information is much more com-plex and diverse in videos than single images. To han-dle this issue, we design an efﬁcient adaptive framework to select and propagate contextual information in videos, coined as TaRget Adaptive Context AggrEgation Network (TRACE). The key to TRACE is to organize relation candi-dates with an adaptive hierarchical relation tree (HRTree), and then perform target-adaptive context information aggre-gation for each relation candidate based on it. HRTree is not only helpful for the information aggregation, but also en-ables efﬁcient processing of numerous relation candidates in a limited memory consumption. As for effective context information aggregation, we present an attentive module to fuse temporal information selectively and a directional propagation module to capture spatial structured informa-tion. Finally, the target-adaptive aggregated representation for each candidate can provide sufﬁcient contextual infor-mation for relation classiﬁcation. Furthermore, we employ a common temporal association algorithm to link frame-level graphs into a video-level result.
Speciﬁcally, we utilize a 3D CNN for extracting tempo-ral features, a 2D CNN for extracting center frame represen-tation, and an object detection network for object candidates with their visual features. Based on these low-level visual representations, TRACE streamlines the VidSGG pipeline with modules of HRTree construction, context aggrega-tion, relation classiﬁcation, and optional temporal associa-tion. We evaluate TRACE on two datasets: Action Genome (AG) [10] and ImageNet-VidVRD (VidVRD) [27]. AG is a brand-new dataset for frame-level VidSGG and only the methods for image SGG are evaluated on it, while VidVRD is a video-level VidSGG dataset. On AG, as a new spe-ciﬁc method for frame-level VidSGG, TRACE achieves the state-of-the-art performance on the standard three evalua-tion modes: scene graph detection (SGDet), scene graph classiﬁcation (SGCls), and predicate classiﬁcation (Pred-Cls). Speciﬁcally, TRACE outperforms the best model on average by 1.5% and 1.2% for mAPrel [46] and mean Re-calls [32] of the three modes, respectively, and get com-parable performance at Recalls. On VidVRD, with a sim-ple temporal linking strategy, our TRACE achieves good performance under the video-level metrics. Concretely,
TRACE outperforms the best model with ground-truth tra-jectories and the same association algorithm by 2.8%, 1.0% and 2.1% at mAP, Recall@50 and Recall@100 respec-tively. Given the same features, TRACE also outperforms the state-of-the-art model by 2.8% at mAP. To sum up, our contributions are as follows: 1) We propose a new detect-to-track paradigm for video-level VidSGG, coined as Target Adaptive Context Ag-gregation Network (TRACE). This new perspective decouples the context modeling for relation prediction from the complicated low-level entity tracking. With this new paradigm, we provide a baseline method for
VidSGG and gains improvement over the state-of-the-art method on ImageNet-VidVRD dataset. 2) As a pure frame-level VidSGG framework, TRACE presents a more modular framework to capture spatio-temporal context information for relation recognition than the previous methods, and obtains the best perfor-mance on Action Genome dataset. 3) In our TRACE, we propose an adaptive structure called as hierarchical relation tree (HRTree). By using
HRTree, the efﬁcient context information aggregation among candidates is enabled. Moreover, our experi-ment demonstrates that this module allows us to save memory for more parameters, thus resulting a better performance than a fully connected graph. 2.