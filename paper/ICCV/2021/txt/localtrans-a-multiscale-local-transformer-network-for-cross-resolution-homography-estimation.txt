Abstract 1.

Introduction
Cross-resolution image alignment is a key problem in multiscale gigapixel photography, which requires to esti-mate homography matrix using images with large resolu-tion gap. Existing deep homography methods concatenate the input images or features, neglecting the explicit formu-lation of correspondences between them, which leads to de-graded accuracy in cross-resolution challenges. In this pa-per, we consider the cross-resolution homography estima-tion as a multimodal problem, and propose a local trans-former network embedded within a multiscale structure to explicitly learn correspondences between the multimodal inputs, namely, input images with different resolutions. The proposed local transformer adopts a local attention map speciﬁcally for each position in the feature. By combin-ing the local transformer with the multiscale structure, the network is able to capture long-short range correspon-dences efﬁciently and accurately. Experiments on both the
MS-COCO dataset and the real-captured cross-resolution dataset show that the proposed network outperforms exist-ing state-of-the-art feature-based and deep-learning-based homography estimation methods, and is able to accurately align images under 10 resolution gap.
×
* Equal contribution
The rapidly development of multiscale gigapixel photog-raphy [5, 42, 45] brings large-scale, long-term and immer-sive visual experience.
It synthesizes a single ultra-high-resolution image through aligning plenty of high-resolution local-views with a low-resolution global-view.
In multi-scale gigapixel photography, the large resolution gap be-tween two views, namely cross-resolution, puts forward a new challenge to traditional homography estimation task.
Homography estimation is deﬁned as the estimation of the projection mapping between two views on the same plane in 3D space, which usually consists of three steps: feature extraction using SIFT [18] or SURF [2], correspondence matching, and homography matrix estimation based on the
RANSAC [9] or a direct linear transform. It relies on dense features with the same resolution to achieve an accurate es-timation, thus, usually fail in solving the cross-resolution problem.
Inspired by the success of deep learning, deep homog-raphy methods based on convolutional neural network are studied to deal with challenging scenes. The pioneer deep homography method proposed by DeTone et al. [8] imple-ments the estimation of homography matrix with a typ-ical VGG-net [30], which extracts correspondences from
the concatenated image pair. Based on this pioneer work,
Le et al. [16] propose a multiscale strategy to progressively estimate the homography via network cascade. However, since the input views are concatenated and downsampled together, simply applying the multiscale strategy cannot solve the cross-resolution problem. A recent approach by
Zhang et al. [44] proposed to extract features from input im-ages separately with shared convolution layers. While the network directly concatenates the features in the following layer, which can be equivalent to concatenating the input images at the very beginning.
In this paper, we present a novel multiscale local trans-former network, which we dubbed LocalTrans, to solve the cross-resolution problem in homography estimation. The transformer structure [33] has made a great success in learn-ing the interaction between multimodal inputs [14, 26, 40] in the ﬁeld of natural language processing and visual ques-tion answering. We therefore take a look at the cross-resolution problem through the lens of “multimodal”, and employ the transformer structure to explicitly capture cor-respondences through the correlation of the cross-resolution images in the feature space.
However, the vanilla transformer structure introduced in [33] brings high GPU memory and computational costs due to the outer product between high-dimensional matri-ces. To achieve a fast and accurate homography estima-tion, we introduce a local transformer and embed it within a multiscale structure. More speciﬁcally, we design a lo-cal convolution-based operation in the proposed local trans-former, which applies a speciﬁc kernel to each position of the high-level feature to efﬁciently capture a local attention.
Then the local transformer is deployed in each level of the multiscale structure, enabling the network to capture corre-spondences with a long-short range attention. The combi-nation of the local transformer and the multiscale structure is signiﬁcantly faster than the global attention mechanism in the vanilla transformer [33]. But most importantly, the pro-posed LocalTrans network shows a superiority to the vanilla transformer with the same backbone in the homography es-timation task.
Beneﬁting from the combination of the local transformer layer and the multiscale structure, the proposed LocalTrans network outperforms the state-of-the-art homography esti-mation methods in terms of PSNR and corner error on the
MS-COCO dataset [17]. Moreover, we demonstrate that the
LocalTrans network highlights a superior performance on challenging real-captured cross-resolution cases under res-olution gap up to 10
, and further apply it to multiscale gigapixel photography, see Fig. 1. The main contributions are summarized as
×
•
We propose to solve the cross-resolution problem in homography estimation using the transformer struc-ture by explicitly capturing the correspondences be-tween the inputs.
•
•
We design a novel local transformer layer embedded within multiscale structure, which is able to capture correspondences with a long-short range attention. Ex-periments demonstrate that the proposed structure out-performs the global attention mechanism.
The proposed local transformer has signiﬁcantly faster speed and lower GPU memory cost compared with the vanilla transformer structure, achieving real-time ho-mography estimation at 60fps (please see Table 1). 2.