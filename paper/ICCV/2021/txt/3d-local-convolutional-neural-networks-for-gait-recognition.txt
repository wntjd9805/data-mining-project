Abstract
The goal of gait recognition is to learn the unique spatio-temporal pattern about the human body shape from its tem-poral changing characteristics. As different body parts be-have differently during walking, it is intuitive to model the spatio-temporal patterns of each part separately. However, existing part-based methods equally divide the feature maps of each frame into ﬁxed horizontal stripes to get local parts.
It is obvious that these stripe partition-based methods can-not accurately locate the body parts. First, different body parts can appear at the same stripe (e.g., arms and the torso), and one part can appear at different stripes in dif-ferent frames (e.g., hands). Second, different body parts possess different scales, and even the same part in different frames can appear at different locations and scales. Third, different parts also exhibit distinct movement patterns (e.g., at which frame the movement starts, the position change frequency, how long it lasts). To overcome these issues, we propose novel 3D local operations as a generic fam-ily of building blocks for 3D gait recognition backbones.
The proposed 3D local operations support the extraction of local 3D volumes of body parts in a sequence with adap-tive spatial and temporal scales, locations and lengths. In this way, the spatio-temporal patterns of the body parts are well learned from the 3D local neighborhood in part-speciﬁc scales, locations, frequencies and lengths. Experi-ments demonstrate that our 3D local convolutional neural networks achieve state-of-the-art performance on popular gait datasets. Code is available at: https://github. com/yellowtownhz/3DLocalCNN . 1.

Introduction
Gait is one of the most important and effective biometric patterns since it can be authenticated at a distance from a
∗This work was done when the author was visiting Alibaba as a re-search intern.
†Corresponding author.
Figure 1. Blocks in backbone CNNs. All these blocks extract features from a local neighborhood. In C2D and C3D, the local neighborhood is a ﬁxed 2D patch (k×k) or 3D volume (k×k×k).
Non-local networks learn adaptive long-range dependency with all positions (H × W × T ). Our 3D local CNN is designed to localize adaptive 3D volumes, instead of a ﬁxed local neighborhood, for multiple local paths and extract corresponding local features. camera without subject’s cooperation. Gait recognition has broad usage in crime prevention, forensic identiﬁcation and social security insurance [2, 14]. In real-world scenarios, beyond the change of body shape caused by walking move-ment, variations such as bag-carrying, coat-wearing, and camera viewpoints switch, also lead to dramatic changes in body appearance, resulting in signiﬁcant challenges to gait recognition.
The essential goal of gait recognition is to learn the unique and invariant representations from the temporal changing characteristics of human body shapes. Early works in gait recognition focused on extracting global fea-tures using convolutional neural networks (CNNs) [35, 20, 29, 19]. GaitNet [41, 40] proposed an auto-encoder frame-work to extract the gait-related features from raw RGB im-ages and then used LSTMs to model the temporal changes of gait sequences. Thomas et al. [33] directly applied 3D-CNNs to extract the sequential information using a model pretrained on natural image classiﬁcation tasks. However, global features do not consider the spatial structure and lo-cal details of the body shape, thus are not discriminative enough when faced with viewpoint variations. A natural choice is to learn the detailed part-based local features com-plementary to the global features or learn features embed-ding for both of them.
Since human body consists of well-deﬁned parts, i.e., head, arms, legs and torso, part-based models have the po-tential to solve the variations in gait recognition. Previous part-based models extracted part features by equally divid-ing the feature maps into ﬁxed horizontal stripes. In Gait-Part [7], 2D appearance features were ﬁrstly extracted by applying pre-deﬁned horizontal partition to the output CNN feature maps of each input frame. Then, the correspond-ing features of the same stripe from all frames were ag-gregated by temporal concatenation of local short-range 2D part features. In GaitSet [3] and GLN [11], frame-level fea-ture maps of the last 2D convolutions were ﬁrstly split into uniform stripes, then max-pooling along the set dimension was applied to them to extract set-level part features.
In
MT3D [18], multiple temporal-scale 3D CNNs were used to explore the temporal relations in sequences. Then, the output feature maps were partitioned into multiple stripes too. However, two issues are neglected by these partition-based gait recognition methods. First, different parts of the human body appear at different scales, and even the same part can appear at different locations and scales in different frames [3]. Second, different parts exhibit distinct move-ment patterns, e.g., at which frame the movement starts, the frequency of position changes, and how long it lasts. Thus, visual appearance and temporal movement changes are mu-tually dependent in a gait period and the characteristics of different natural human body parts are distinct from each other.
It suggests that the gait recognition model should support the extraction and processing of adaptive 3D local volumes for each speciﬁc human body part.
To overcome the aforementioned issues in gait recogni-tion, we propose novel 3D local operations as a generic fam-ily of building blocks for 3D gait recognition backbones.
Our 3D local operations support the extraction of local 3D volumes in a sequence with adaptive spatial and tempo-ral scales, locations and lengths.
In this way, the 3D lo-cal neighborhoods of different body parts are processed in speciﬁc part scales, locations and movement locations, fre-quencies, lengths, as shown in Fig. 1. 2D local operation is already proved to be valid in image recognition [10, 36], where a differentiable 2D attention mechanism is utilized to yield 2D image/feature patches of smoothly varying loca-tions and scales. However, due to the different mechanism of temporal foveation [21], it is very challenging to adapt this idea to 3D local operations. The reason is in two-fold. 1) Spatial sampling of pixels follows the foveation of the human eye, while temporal sampling of frames is different in following the distribution of optical ﬂow. 2) Spatial sam-pling processes 2D patches, temporal sampling deals with 1D sequences, and spatio-temporal sampling processes 3D video volumes. Therefore, a new strategy for 2D and 1D joint sampling is required.
Our local operation consists of four modules: localiza-tion, sampling, feature extraction, and fusion. The local-ization module is designed to learn the adaptive spatial and temporal scales, locations and temporal lengths of six body parts: head, torso, left arm, right arm, left leg and right leg.
The sampling module samples local volumes of smoothly varying locations, scales and temporal lengths. The fea-ture extraction module consists of several convolution and
ReLU [22] layers as in general convolutional blocks. The fusion module is formed as a concatenation layer of global and local outputs followed by a 1 × 1 × 1 convolutional layer. In practice, any building block of existing 3D back-bone CNNs can be viewed as a global path, and the pro-posed local path can be easily inserted into these blocks without any change in the training scheme. Furthermore, the architecture of each component in the local operation is quite ﬂexible for different conﬁgurations.
The main contributions of this work are summarized as follows:
• Compared with C3D [30], P3D [24] and Non-local networks [31], we design a new building block for backbone 3D CNNs that incorporates part-speciﬁc se-quential information, termed 3D local convolutional neural networks.
• We implement a simple but effective form of 3D local
CNNs for gait recognition. This model outperforms state-of-the-art gait recognition methods on two of the most popular datasets, CASIA-B and OU-MVLP.
• To the best of our knowledge, we are the ﬁrst to present a framework that enables the interaction/boosting of global and local 3D volume information in any layer of 3D CNNs. 2.