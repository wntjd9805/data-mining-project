Abstract with the synthesized motions distinctly adapted by the given action labels.
We present a unified and flexible framework to address the generalized problem of 3D motion synthesis that cov-ers the tasks of motion prediction, completion, interpola-tion, and spatial-temporal recovery. Since these tasks have different input constraints and various fidelity and diversity requirements, most existing approaches only cater to a spe-cific task or use different architectures to address various tasks. Here we propose a unified framework based on Con-ditional Variational Auto-Encoder (CVAE), where we treat any arbitrary input as a masked motion series. Notably, by considering this problem as a conditional generation pro-cess, we estimate a parametric distribution of the missing regions based on the input conditions, from which to sample and synthesize the full motion series. To further allow the flexibility of manipulating the motion style of the generated series, we design an Action-Adaptive Modulation (AAM) to propagate the given semantic guidance through the whole sequence. We also introduce a cross-attention mechanism to exploit distant relations among decoder and encoder fea-tures for better realism and global consistency. We con-ducted extensive experiments on Human 3.6M and CMU-Mocap. The results show that our method produces coher-ent and realistic results for various motion synthesis tasks,
∗This research is supported by Institute for Media Innovation, Nanyang
Technological University (IMI-NTU) and the National Research Foun-dation, Singapore under its International Research Centres in Singapore
Funding Initiative. Any opinions, findings and conclusions or recommen-dations expressed in this material are those of the author(s) and do not re-flect the views of National Research Foundation, Singapore. This research is also supported in part by Monash FIT Start-up Grant and SenseTime
Gift Fund, National Science Foundation Grant CNS1951952 and SUTD project PIE-SGP-Al-2020-02. 1.

Introduction
Generating realistic and plausible human body anima-tion with specified actions has been a widely explored but challenging task in computer vision and graphics [29, 4]. To synthesize smooth and natural motions, traditional methods
[30, 32] rely on the availability of complex pose specifica-tions, which are time-consuming and expensive to obtain.
Recent deep learning approaches [5, 60, 56, 55, 17, 20, 61, 58] have investigated generating plausible human mo-tions. However, since different motion synthesis tasks have different goals and expectations (as seen in Figure 1), many approaches are either restricted to one type of motion syn-thesis task or use different methods to address the vari-ous tasks. For example, much work [6, 14, 37, 63] is fo-cused on the motion prediction task, typically adopting re-current neural network (RNN) architectures to predict fu-ture frames sequentially, with new ones dependent only on previously generated frames. Although performing well in motion prediction, these approaches are not directly suited for generalizing to other motion synthesis tasks such as mo-tion completion, interpolation, and spatial-temporal recov-ery, as shown in Figure 1, for which both forward and back-ward dependencies should be exploited. Moreover, many methods [5, 60, 56] are focused on minimizing the recon-struction error between the ground truth and generated mo-tion sequences, while less considering motion diversity and human-likeness, which are also significant for realistic gen-eration. Furthermore, in precise motion animation, it is
troduce a cross-attention mechanism between encoder and decoder features to capitalize on the relationships between input and output poses, irrespective of temporal distances.
In summary, the main contributions of this work are:
• We propose a unified CVAE-based framework to handle various motion synthesis tasks such as mo-tion prediction, completion, interpolation, and spatial-temporal recovery while meeting different input con-straints, different fidelity and diversity requirements.
• We introduce an Action-Adaptive Modulation (AAM) that is able to control semantic motion styles of the generated series.
• We design a cross-attention mechanism that exploits long-term context information to enhance the realism and global consistency of synthesized sequences.
We conducted quantitative and qualitative assessments on the widely-used Human3.6M and CMU-Mocap datasets.
Experiments show our approach outperformed existing pose synthesis methods, generating realistic and plausible mo-tion series conditioned on various input constraints. 2.