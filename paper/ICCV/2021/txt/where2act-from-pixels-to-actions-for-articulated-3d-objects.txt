Abstract
One of the fundamental goals of visual perception is to allow agents to meaningfully interact with their environment.
In this paper, we take a step towards that long-term goal – we extract highly localized actionable information related to elementary actions such as pushing or pulling for articulated objects with movable parts. For example, given a drawer, our network predicts that applying a pulling force on the handle opens the drawer. We propose, discuss, and evaluate novel network architectures that given image and depth data, predict the set of actions possible at each pixel, and the regions over articulated parts that are likely to move under the force. We propose a learning-from-interaction framework with an online data sampling strategy that allows us to train the network in simulation (SAPIEN) and generalizes across categories. Check the website for code and data release. 1.

Introduction
We humans interact with a plethora of objects around us in our daily lives. What makes this possible is our effortless understanding of what can be done with each object, where this interaction may occur, and precisely how our we must move to accomplish it – we can pull on a handle to open a drawer, push anywhere on a door to close it, ﬂip a switch to turn a light on, or push a button to start the microwave. Not only do we understand what actions will be successful, we also intuitively know which ones will not e.g. pulling out a remote’s button is probably not a good idea! In this work, our goal is to build a perception system which also has a similar understanding of general objects i.e. given a novel object, we want a system that can infer the myriad possible interactions1 that one can perform with it.
The task of predicting possible interactions with objects is
∗The majority of the work was done while Kaichun Mo was a research intern at Facebook AI Research. 1Gibson proposed the idea of affordances – opportunities of interaction.
Classical notion of object affordance involves consideration of agent’s morphology. Our interactions are more low-level actions.
Figure 1. The Proposed Where2Act Task. Given as input an ar-ticulated 3D object, we learn to propose the actionable information for different robotic manipulation primitives (e.g. pushing, pulling): (a) the predicted actionability scores over pixels; (b) the proposed interaction trajectories, along with (c) their success likelihoods, for a selected pixel highlighted in red. We show two high-rated proposals (left) and two with lower scores (right) due to interaction orientations and potential robot-object collisions. one of central importance in both, the robotics and the com-puter vision communities. In robotics, the ability to predict feasible and desirable actions (e.g. a drawer can be pulled out) can help in motion planning, efﬁcient exploration and interactive learning (sampling successful trials faster). On the other hand, the computer vision community has largely focused on inferring semantic labels (e.g. part segmentation, keypoint estimation) from visual input, but such passively learned representations provide limited understanding. More speciﬁcally, passive learning falls short on the ability of agents to perform actions, learn prediction models (forward dynamics) or even semantics in many cases (categories are more than often deﬁned on affordances themselves!). Our paper takes a step forward in building a common percep-tion system across diverse objects, while creating its own supervision about what actions maybe successful by actively interacting with the objects.
The ﬁrst question we must tackle is how one can parametrize the predicted action space. We note that any
long-term interaction with an object can be considered as a sequence of short-term ‘atomic’ interactions like pushing and pulling. We therefore limit our work to considering the plausible short-term interactions that an agent can perform given the current state of the object. Each such atomic in-teraction can further be decomposed into where and how e.g. where on the cabinet should the robot pull (e.g. drawer handle or drawer surface) and how should the motion be executed (e.g. pull parallel or perpendicular to handle). This observation allows us to formulate our task as one of dense visual prediction. Given a depth or color image of an object, we learn to infer for each pixel/point, whether a certain prim-itive action can be performed at that location, and if so, how it should be executed.
Concretely, as we illustrate in Figure 1 (a), we learn a prediction network that given an atomic action type, can predict for each pixel: a) an ‘actionability’ score, b) action proposals, and c) success likelihoods. Our approach allows an agent to learn these by simply interacting with various objects, and recording the outcomes of its actions – labeling ones that cause a desirable state change as successful. While randomly interacting can eventually allow an agent to learn, we observe that it is not a very efﬁcient exploration strategy.
We therefore propose an on-policy data sampling strategy to alleviate this issue – by biasing the sampling towards actions the agents thinks are likely to succeed.
We use the SAPIEN [45] simulator for learning and test-ing our approach for six types of primitive interaction, cov-ering 972 shapes over 15 commonly seen indoor object cate-gories. We empirically show that our method successfully learns to predict possible actions for novel objects, and does so even for previously unseen categories.
In summary, our contributions are:
• we formulate the task of inferring affordances for ma-nipulating 3D articulated objects by predicting per-pixel action likelihoods and proposals;
• we propose an approach that can learn from interac-tions while using adaptive sampling to obtain more informative samples;
• we create benchmarking environments in SAPIEN, and show that our network learns actionable visual represen-tations that generalize to novel shapes and even unseen object categories. 2.