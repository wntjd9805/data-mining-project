Abstract
Few-shot learning (FSL) attempts to learn with limited data. In this work, we perform the feature extraction in the
Euclidean space and the geodesic distance metric on the
Oblique Manifold (OM). Specially, for better feature extrac-tion, we propose a non-parametric Region Self-attention with Spatial Pyramid Pooling (RSSPP), which realizes a trade-off between the generalization and the discrimina-tive ability of the single image feature. Then, we embed the feature to OM as a point. Furthermore, we design an Oblique Distance-based Classiﬁer (ODC) that achieves classiﬁcation in the tangent spaces which better approxi-mate OM locally by learnable tangency points. Finally, we introduce a new method for parameters initialization and a novel loss function in the transductive settings. Exten-sive experiments demonstrate the effectiveness of our al-gorithm and it outperforms state-of-the-art methods on the popular benchmarks: mini-ImageNet, tiered-ImageNet, and
Caltech-UCSD Birds-200-2011 (CUB). 1.

Introduction
Convolutional neural networks (CNN) trained with large-scale labeled data have achieved the competitive per-formance as humans in recent years. However, different from these models struggling with a few labeled instances per class, humans learn rapidly by leveraging content and prior knowledge. To address this, few-shot learning (FSL) has drawn increasing attention. FSL aims to learn a prior knowledge that can rapidly generalize to new tasks with limited samples.
The transductive learning methods [66, 38, 33, 4] and metric learning methods [48, 27, 62] have been promising in a recent line of works. Transductive learning has shown superior performance over inductive learning, because the unlabeled test examples are classiﬁed at once, instead of the one sample at a time as in inductive settings. Metric learning methods represent the image in an appropriate fea-ture space and replace the fully connected layer in standard image classiﬁcation [20] with the distance function, e.g. Eu-clidean distance or cosine distance. However, these works may lose the geometric inherent properties in the Euclidean space. Though it can be alleviated by data dimensionality reduction [22, 50, 21], these methods are not generalized well to new tasks or easy to over-ﬁt in the few-shot settings.
Another solution is to make use of Riemannian geome-try [11]. Riemannian geometry studies real smooth differ-ential manifolds and it deﬁnes several geometric notions, e.g. the length of a curve, with a Riemannian metric. Ad-mitting the Riemannian geometry, the Grassmannian man-ifold [26, 55] and the SPD manifold [36] are highly preva-lent in modeling characters of image sets and videos, where intra-class variance, e.g., illumination conditions or other scenarios, are comprised. They are capable of “ﬁlling-in” missing images. However, the advantages are based on modeling sufﬁcient number of images for each class. It is infeasible to apply the aforementioned manifolds to FSL.
Another manifold that admits Riemannian geometry, is the oblique manifold (OM) [52], which is an embedded sub-manifold with all normalized columns and it is used for in-dependent component analysis [1]. We argue that OM is superior in FSL. The reasons are in two-fold: 1) Whiten-ing is not required in OM. The whitening step is to estimate the covariance matrix and remove the scaling indeterminacy of data in Grassmannian manifold or SPD manifold, which is infeasible to perform whitening step for a single image classiﬁcation. Without whitening, OM is free of extrinsic constraints. 2) All the columns of OM have unit Euclidean norm. OM offers an intrinsic property similar to the L2-normalization in Euclidean space.
In this way, the CNN features in Euclidean space can be embedded to OM more easily.
However, the absence of whitening may lead to weak generalization of OM, since the whitening step is to re-move the impact of intra-variance of data. To address this, we resort to the powerful CNN to enhance the gen-eralization, as generalization emerged from the pretrained
CNN [20, 64, 34], ensemble learning [9] or SPP [19].
However, more generalization means less discriminative
a lot when points are too far from anchors [47], the an-chors need to be initialized appropriately. We argue that the anchors should be in the neighborhood of the Karcher mean (KM) [17] among associated points. Considering the potential ill-distribution gap between the train samples and the test samples, we initialize the anchors by selecting sam-ples in the transductive settings, and design a weighted loss function to integrate the tangent spaces. Furthermore, con-sidering the calculation of KM is an NP-hard problem, we propose a pseudo-KM method to utilize the weighted mean (i.e. inner product) operation on the features in Euclidean space, and then embed the mean feature to OM. Empiri-cally we observe the initialization of weights is also criti-cally important. Similar to the anchors, we calculate pro-totypes [46] and embed the prototypes to OM as the initial value of weights.
Finally, we acquire the classiﬁcation scores by perform-ing weighted sum over softmax on the Euclidean distance in tangent space. The effectiveness of our method is demon-strated by extensive experiments on multiple datasets. To conclude, our main contributions are summarized as fol-lows:
• To our best knowledge, it is the ﬁrst time to model FSL on OM, which intrinsically satisﬁes the normality con-straint and without the need for whitening.
• We propose a non-parametric RSSPP. It applies the multi-kernel max-pooling similar to SPP to enhance generalization and the self-attention mechanism to im-prove discriminative ability.
• To perform classiﬁcation in the oblique manifold, we propose the ODC parameterized with weights and an-chors. ODC is initialized with a carefully designed strategy. We also design a weighted sum loss func-tion over the anchors, and utilize the exponential map to update the weights and anchors.
• The experiments on popular datasets demonstrate that our algorithm on FSL signiﬁcantly outperforms the baseline and achieves new state-of-the-art performance on all of them. 2.