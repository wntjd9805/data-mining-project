Abstract
Self-supervised detection and segmentation of fore-ground objects aims for accuracy without annotated train-ing data. However, existing approaches predominantly rely on restrictive assumptions on appearance and motion.
For scenes with dynamic activities and camera motion, we propose a multi-camera framework in which geometric constraints are embedded in the form of multi-view consis-tency during training via coarse 3D localization in a voxel grid and ﬁne-grained offset regression. In this manner, we learn a joint distribution of proposals over multiple views.
At inference time, our method operates on single RGB im-ages. We outperform state-of-the-art techniques both on im-ages that visually depart from those of standard benchmarks and on those of the classical Human3.6M dataset. 1.

Introduction
Robust detection and segmentation of moving people can now be achieved reliably in scenarios for which large
∗Work supported in part by the Swiss National Science Foundation. amounts of annotated data are available. However, for less common activities, such as skiing, it remains challenging, because the required training databases do not exist. Self-supervised approaches [10, 22, 5, 6, 8, 9, 37, 53, 28, 3, 31] promise to address this problem. However, most of them depend on strong constraints, such as the target objects be-ing seen against a static background, or rely on object local-ization and object-boundary detection networks pre-trained with supervision, which limits their applicability.
In this paper, we propose to remove these limitations by using a multi-camera setup for training purposes and explic-itly encoding the 3D geometry of the scene. At inference time, our trained network can then handle single images and outperforms earlier techniques, as shown in Fig. 1. Our al-gorithm can be applied to any object as long as the two as-sumptions from [24] hold: foreground and background are distinguishable by color or texture; every part of the back-ground must be visible more often than not.
Using several cameras complicates data acquisition but only in a limited way because both synchronization and cal-ibration are well understood tasks for which off-the-shelf
solutions exist. In practice, for static cameras, this has to be dealt with only once before a ﬁlming session using well-known techniques [15, 12] and requires far less effort than manually annotating images. For moving cameras, SLAM methods are now robust enough to perform the calibration automatically and fast in the wild [58, 51]. Hence, there are many applications in which training with multiple cameras makes perfect sense, especially those with unusual activities for which large training databases are not available.
To leverage multi-view training data as weak supervi-sion, we introduce the object proposal strategy depicted by
Fig. 2. Candidate 2D bounding boxes are produced by a net-work that can be trained in an unsupervised fashion. They are used to vote into a 3D proposal grid, and multi-view ge-ometry constraints are then imposed to align proposals from different views in a differentiable manner. To train the re-sulting network, we sample a 3D proposal, deconstruct and reconstruct the image in each view using the corresponding 2D bounding box, and compare the resulting resynthesized images to the original ones.
While our self-supervised learning strategy leverages multiple views during training, the resulting model can be used for detection and segmentation in monocular images acquired by moving cameras and featuring unknown back-grounds. Our contributions can be summarized as follows.
• We introduce a self-supervised end-to-end trainable object detection and segmentation approach that ex-plicitly leverages 3D multi-view geometry as weak su-pervision during training.
• It comprises a 3D object proposal framework that en-ables to enforce prediction consistency across views without having to introduce additional loss terms.
To show that our approach can handle unusual activ-ities and fast motion, we demonstrate it on the skiing dataset depicted by Fig. 1, captured by moving cameras, on a small dataset acquired using hand-held cameras, as well as on the more standard H36M dataset [18] acquired using ﬁxed cameras. Note that our multi-view supervi-sion differs from weak supervision in video object seg-mentation as it does not require any segmentation anno-tation. Hence, our method relates to self-supervised ap-proaches. We show that the proposed multi-view train-ing increases single-image accuracy performance at infer-ence time, which allows us to outperform state-of-the-art single-view [22, 53, 9, 31, 20] and multi-view [37] ap-proaches. Our code is publicly available at https:// github.com/isinsukatircioglu/mvc. 2.