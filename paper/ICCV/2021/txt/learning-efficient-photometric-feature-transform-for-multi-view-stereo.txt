Abstract
We present a novel framework to learn to convert the per-pixel photometric information at each view into spatially distinctive and view-invariant low-level features, which can be plugged into existing multi-view stereo pipeline for en-hanced 3D reconstruction. Both the illumination condi-tions during acquisition and the subsequent per-pixel fea-ture transform can be jointly optimized in a differentiable fashion. Our framework automatically adapts to and makes efficient use of the geometric information available in dif-ferent forms of input data. High-quality 3D reconstructions of a variety of challenging objects are demonstrated on the data captured with an illumination multiplexing device, as well as a point light. Our results compare favorably with state-of-the-art techniques. 1.

Introduction
As one central problem in computer vision and graph-ics, shape reconstruction in the presence of complex appear-ance is challenging. At one hand, multi-view stereo meth-ods [8] usually require a Lambertian-dominant reflectance for computing reliable view-invariant features. The appear-ance variation with view or lighting is undesirable, as it may change the native spatial features on the object, or specularly reflect the projected pattern from active illumina-tion [16, 23], leading to difficulties in correspondence com-putation for shape reconstruction.
On the other hand, single-view photometric stereo [34, 5] exploits the lighting variations on each pixel and trans-forms the image measurements into a normal map. While high-quality details can be recovered, it suffers from low-frequency shape distortions [20]. Recently, multi-view pho-tometric stereo [12, 32, 17] accurately integrates the photo-metric cues from different viewpoints, subject to the geo-metric constraints across multiple views.
However, photometric stereo techniques are not scalable to the amount of geometric information in the input data for 3D reconstruction, leading to suboptimal results. When ad-ditional physical cues such as rapid albedo variations are present, they cannot be exploited to improve the recon-struction quality. On the other hand, when the measured photometric information is insufficient to determine a nor-mal field, the quality of results from existing approaches will significantly degrade. Furthermore, related techniques heavily exploit reflectance properties [17, 18], which hin-ders the extension to handle more general appearance such as anisotropic materials.
To tackle the above challenges, we make the key obser-vation that it is not necessary to use normal as the interme-diate representation for 3D reconstruction from photometric measurements. Instead, we propose a novel differentiable framework, to efficiently transform the per-pixel photomet-ric information measured at each view, into automatically learned low-level features, in an end-to-end fashion. The learned per-pixel features essentially exploit the available geometric information in photometric measurements, and can be plugged in existing multi-view stereo pipelines for further processing tasks like spatial aggregation, resulting in enhanced geometric reconstruction. Furthermore, our data-driven framework is highly flexible and can adapt to vari-ous factors, including the physical acquisition capabilities
/ characteristics of different setups, and different types of appearance such as anisotropic reflectance, by feeding cor-responding training data.
The effectiveness of our framework is demonstrated with a high-performance illumination multiplexing device, on geometric reconstruction results of a variety of 3D objects using as few as 16 input photographs per view. Moreover, the framework is generalized to handle the input data of conventional photometric stereo with one point light on at a time (DiLiGenT-MV[17]). Our results compare favorably with state-of-the-art techniques. We make public the code and data of this project 1. 1https://svbrdf.github.io/publications/ptmvs/
*: corresponding author (hwu@acm.org). project.html
2.