Abstract
Multi-objective multi-task learning aims to boost the performance of all tasks by leveraging their correlation and conflict appropriately. Nevertheless, in real practice, users may have preference for certain tasks, and other tasks sim-ply serve as privileged or auxiliary tasks to assist the train-ing of target tasks. The privileged tasks thus possess less or even no priority in the final task assessment by users.
Motivated by this, we propose a privileged multiple descent algorithm to arbitrate the learning of target tasks and priv-ileged tasks. Concretely, we introduce a privileged param-eter so that the optimization direction does not necessarily follow the gradient from the privileged tasks, but concen-trates more on the target tasks. Besides, we also encourage a priority parameter for the target tasks to control the po-tential distraction of optimization direction from the privi-leged tasks. In this way, the optimization direction can be more aggressively determined by weighting the gradients among target and privileged tasks, and thus highlight more the performance of target tasks under the unified multi-task learning context. Extensive experiments on synthetic and real-world datasets indicate that our method can achieve versatile Pareto solutions under varying preference for the target tasks. 1.

Introduction
Besides designing strong model structures [32, 46, 35, 13, 34] and informative task losses [19, 48, 5, 44, 52], multi-task learning (MTL) [50, 38] is apt to enhance performance
*Corresponding authors. and efficiency by seeking more appropriate ways to com-bine multiple tasks, and increasingly gains much research interest. The paradigm of MTL has been shown to out-perform single-task learning (STL) on numerous computer vision problems, such as attribute recognition [51], scene understanding [24] and autonomous driving [9]. To exploit the task correlations, current MTL approaches mainly fol-low a soft-parameter or hard-parameter sharing principle.
In soft-parameter sharing, tasks are aggregated separately and cross-talks [28] among these tasks are usually used to encourage shared knowledge. Nevertheless, the intricacy of designing the cross-talks is specific to particular prob-lem sets, and does not scale well to many tasks. In con-trast, hard-parameter sharing leverages a unique backbone to pursue direct shared representation of tasks [1, 2], along with task-specific sub-networks. Therefore, hard-parameter sharing is able to reduce parameter size in proportion to the task number, and to promote inference speed during testing.
Though parameters among tasks are shared in a hard manner, how to balance all tasks still matters for MTL. Task balancing to circumvent this difficulty goes beyond naive uniform weighting of tasks [51]. Existing heuristics to find appropriate weighting of multiple tasks include grid search-ing, exploring task uncertainties [10], and gradient normal-ization [6]. Recent avant-garde method is to treat the MTL as multi-objective optimization (MOO-MTL) [33]. It pro-poses to find the Pareto front that only allows the common improvement of tasks rather than sacrificing any individual.
Task weights are evaluated dynamically during learning.
Current MTL methods treat all tasks equally, and fo-cus on the average performance of all tasks during infer-ence. In practice, however, users may only need to scruti-nize the performance of some target tasks rather than that of
ploitation of the privileged tasks.
Our approach, privileged task learning (PTL), introduces parameters working on the gradient space, so that we can control them to achieve versatile Pareto critical points to cater user preference as shown in Figure 1. We optimize our proposed P-MGDA by an efficient hybrid-block coor-dinate descent (CD) algorithm. Extensive experiments on both synthetic and real datasets validate the effectiveness of our PTL. Results show that PTL is able to find the solution that not only has overall satisfactory performance across all tasks but especially good for the target task. 2.