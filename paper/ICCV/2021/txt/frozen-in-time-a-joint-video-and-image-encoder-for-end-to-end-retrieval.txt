Abstract
Our objective in this work is video-text retrieval – in partic-ular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute.
We address both these challenges in this paper. We pro-pose an end-to-end trainable model that is designed to take advantage of both large-scale image and video captioning datasets. Our model is an adaptation and extension of the recent ViT and Timesformer architectures, and consists of attention in both space and time. The model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. It is trained with a cur-†Now at Google Research. riculum learning schedule that begins by treating images as ‘frozen’ snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. We also provide a new video-text pretraining dataset WebVid-2M, comprised of over two million videos with weak captions scraped from the internet. Despite train-ing on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including
MSR-VTT, MSVD, DiDeMo and LSMDC. 1.

Introduction
Joint visual-text models have become increasingly popu-lar as they enable a wide suite of downstream tasks, includ-ing text-to-visual retrieval [29, 31, 36, 59], visual caption-ing [24, 58, 66], and visual question and answering [4, 27].
Their rapid development is due to the usual improvements on three fronts: new neural network architectures (e.g. transformers [56] for both text and visual inputs); new
large-scale datasets; and new loss functions that are, for example, able to handle label noise [35]. However, their development mostly proceeds on two independent tracks: one for images, with its own architectures, training datasets and benchmarks [25, 29, 52]; and the other for videos with a similar separation of training datasets and bench-marks [3, 5, 24, 46, 65, 70]. The only common link be-tween the two is that often video networks are initialized by pre-training image networks on image datasets [6, 8]. This separation of effort is suboptimal given the overlap in infor-mation that images and video convey over multiple tasks.
For example, although classifying some human actions re-quires the temporal ordering of video frames, many actions can be classified from just their distribution over frames or even from a single frame [51].
In this paper we take a step towards unifying these two tracks, by proposing a dual encoder architecture which utilises the flexibility of a transformer visual encoder to train from images-with-captions, from video clips-with-captions, or from both (Fig. 1). We do this by treating im-ages as a special case of videos that are ‘frozen in time’. Us-ing a transformer-based architecture allows us to train with variable-length sequences, treating an image as if it was a single frame video, unlike in standard 3D CNNs [8, 18, 64] where to train on images jointly with videos one must incur the cost of actually generating a static video. Furthermore, unlike many recent methods [16, 31, 36] for video-text dual encoding, we do not use a set of ‘expert networks’ that are pre-trained on external image datasets and then fixed, but instead train the model end-to-end.
This end-to-end training is facilitated by scraping the web for a new large-scale video-text captioning dataset of over two million video alt-text pairs (WebVid-2M). We also take advantage of large-scale image captioning datasets such as Conceptual Captions [52].
We make the following contributions: (i) we propose a new end-to-end model for video retrieval that does not rely on ‘expert’ features, but instead, inspired by [6] employs a transformer architecture with a modified divided space-time attention applied directly to pixels; (ii) because our ar-chitecture can gracefully handle inputs of different lengths, it is versatile and can be flexibly trained on both video and image datasets (by treating images as a single-frame video). We build on this flexibility by designing a curricu-lum learning schedule that begins with images and then gradually learns to attend to increasing temporal context when trained on video datasets through temporal embed-ding interpolation. We show that this increases efficiency, allowing us to train models with far less GPU time; (iii) we introduce a new dataset called WebVid-2M, consisting of 2.5M video-text pairs scraped from the web; and finally (iv) we achieve state-of-the-art performance by only using the video modality on MSR-VTT [65], MSVD [9], DiDeMo [3] and LSMDC [46] – outperforming works that use pre-extracted experts from multiple modalities, as well as those that are pretrained on the noisy HowTo100M, which is 20x larger than our dataset in the number of video-text pairs. 2.