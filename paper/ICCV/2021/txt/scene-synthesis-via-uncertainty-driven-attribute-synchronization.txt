Abstract
Developing deep neural networks to generate 3D scenes is a fundamental problem in neural synthesis with immediate applications in architectural CAD, computer graphics, as well as in generating virtual robot training environments.
This task is challenging because 3D scenes exhibit diverse patterns, ranging from continuous ones, such as object sizes and the relative poses between pairs of shapes, to discrete patterns, such as occurrence and co-occurrence of objects with symmetrical relationships. This paper introduces a novel neural scene synthesis approach that can capture diverse feature patterns of 3D scenes. Our method combines the strength of both neural network-based and conventional scene synthesis approaches. We use the parametric prior distributions learned from training data, which provide uncertainties of object attributes and relative attributes, to regularize the outputs of feed-forward neural models. Moreover, instead of merely predicting a scene layout, our approach predicts an over-complete set of attributes. This methodology allows us to utilize the underlying consistency constraints among the predicted attributes to prune infeasible predictions. Experimental results show that our approach outperforms existing methods considerably. The generated 3D scenes interpolate the training data faithfully while preserving both continuous and discrete feature patterns. 1.

Introduction 3D scene synthesis is a fundamental problem in deep generative modeling. This task is challenging because 3D scenes exhibit diverse patterns, ranging from continuous ones, such as the size of each object and the relative poses between pairs of shapes, to discrete patterns, such as occurrence and co-occurrence of objects and symmetric relations. Moreover, there are also generic geometric constraints, e.g., synthesized objects in a 3D scene should not inter-penetrate. Developing neural networks to capture all feature patterns while enforcing geometric constraints remains an open problem. Due to the diversity of feature
Figure 1: Randomly generated scenes (left) and their nearest neighbours (right) in the training set in 3D-FRONT. patterns and constraints, the popular approach of developing a single data representation and training approach proves insufﬁcient.
This paper introduces a novel approach to synthesizing 3D scenes represented as a collection of objects. Each object is encoded by its attributes such as size, pose, existence indicator, and geometric codes (c.f. [42, 56]). The theme of our approach is to look at 3D scene synthesis from hybrid viewpoints. Our goal is to combine the strengths of different approaches and representations that can capture diverse feature patterns and enforce different constraints. We execute this hybrid methodology at two levels.
First, instead of merely synthesizing the absolute at-tributes of each individual object, our approach predicts an over-complete set of attributes which also include relative attributes (e.g., relative poses) between object pairs. Such relative attributes better capture spatial correlations among objects compared to only synthesizing absolute attributes.
From a robust optimization point of view, over-complete attributes possess generic consistency constraints, e.g., the relative attributes should be consistent with object attributes.
These constraints allow us to prune infeasible attributes
in synthesis output (c.f. [17, 10, 19, 2, 21, 54, 15, 48, 39, 55, 51]). This approach is particularly suitable for neural outputs that exhibit weak correlations due to random initial-ization [55, 14, 38, 29]. We can therefore suppress output errors effectively by enforcing the consistency constraints among absolute and relative attributes.
Second, our approach combines the strengths of neural scene synthesis models and conventional scene generation methods. Neural models possess unbounded expressibility and can encode both continuous and discrete patterns.
However, they typically produce single outputs that do not possess useful signals of uncertainties for synchronizing object attributes and relative attributes. For example, suppose
In we know the uncertainty of object attribute is high. such cases, we can replace it with another one based on the attributes of other objects and the corresponding relative attributes. Similarly, we can discard a relative attribute if its uncertainty is high. Our approach addresses this issue by learning parametric prior distributions of absolute and relative attributes. Such distributions provide uncertainties of generated object attributes and relative attributes, offering rich signals to regularize them and prune outliers. Moreover, they also help enforce the penetration-free constraints.
We introduce a Bayesian framework to integrate neural outputs and parametric prior distributions seamlessly. The hyperparameters of this Bayesian framework are optimized to maximize the performance of the ﬁnal output.
We evaluate our approach on 3D-FRONT [12]. We also provide results on SUNCG [40] to provide sufﬁcient comparisons with baseline techniques. Experimental results show that our approach can generate 3D scenes different from the training examples while preserving discrete and con-tinuous feature patterns. Our method outperforms baseline approaches both qualitatively and quantitatively. An ablation study justiﬁes the design choices of our approach. Our code is available at https://github.com/yanghtr/Sync2Gen. 2.