Abstract
Image-to-image (I2I) translation has matured in recent years and is able to generate high-quality realistic im-ages. However, despite current success, it still faces im-portant challenges when applied to small domains. Ex-isting methods use transfer learning for I2I translation, but they still require the learning of millions of parame-ters from scratch. This drawback severely limits its ap-plication on small domains.
In this paper, we propose a new transfer learning for I2I translation (TransferI2I). We decouple our learning process into the image generation step and the I2I translation step. In the first step we pro-pose two novel techniques: source-target initialization and self-initialization of the adaptor layer. The former fine-tunes the pretrained generative model (e.g., StyleGAN) on source and target data. The latter allows to initialize all non-pretrained network parameters without the need of any data. These techniques provide a better initialization for the I2I translation step. In addition, we introduce an auxil-iary GAN that further facilitates the training of deep I2I sys-tems even from small datasets. In extensive experiments on three datasets, (Animal faces, Birds, and Foods), we show that we outperform existing methods and that mFID im-proves on several datasets with over 25 points. Our code is available at: https://github.com/yaxingwang/
TransferI2I. 1.

Introduction
Image-to-image (I2I) translation aims to map an image from a source to a target domain. Several methods ob-tain outstanding results on paired data [21, 61], unpaired data [30, 56, 60], scalable I2I translation [11, 36, 45] and diverse I2I translation [11, 20, 32]. Scalable I2I translation aims to translate images between multiple domains. For ex-ample, a cat face is mapped onto other animal faces (i.e. dog, tiger, bear, etc.). The goal of diverse I2I translation is to synthesize multiple plausible outputs of the target do-main from a single input image (i.e. translating a dog face to various plausible cat faces). Despite impressive leaps for-ward with paired, unpaired, scalable and diverse I2I transla-tion, there are still important challenges. Specifically, to ob-tain good results existing works rely on large labelled data.
When given small datasets (e.g., 10 images per domain) current algorithms suffer from inferior performance. Also, labeling large-scale datasets is costly and time-consuming, making those methods less applicable in practice.
Several works [5, 6, 12, 35, 38] have studied one-shot and few-shot I2I translation. One-shot I2I transla-tion [5, 6, 12, 35] refers to the case where only one source image and one or few target images are available. These works fail to perform multiclass I2I translation. FUNIT [38] conducts few-shot I2I translation, but still requires large datasets at the training stage.
In this paper, we focus on transfer learning for I2I translation with limited data.
Recent work [48, 54] leverages transfer learning for I2I translation. SGP [48] utilizes a pretrained classifier (e.g,
VGG [50]) to initialize the encoder of an I2I model. How-the remaining networks (i.e., decoder, discrimina-ever, tor and adaptor layers1) need to be trained from scratch, which still requires a large dataset to train the I2I trans-lation model. DeepI2I [54] uses a pretrained GAN (e.g.,
StyleGAN [26] and BigGAN [8]) to initialize the I2I model.
However, it still requires to train the adaptor layers from scratch. The adaptor layers contains over 85M parameters (using the pretrained BigGAN) which makes their training on translation between small domains prone to overfitting.
Since both SGP and DeepI2I leverage the adaptor between the encoder and the generator, one potential problem is that the generator easily uses the information from the high-resolution skip connections (connecting to the upper layers 1We follow [54] and call the layers which connect encoder and decoder at several levels adaptor layers.
of the generator), and ignore the deep layers of the gener-ator, which require a more semantic understanding of the data, thus more difficult to train. Inspired by DeepI2I, we use the pretrained GANs to initialize I2I translation model.
Differently, we propose a new method to train I2I transla-tion, overcoming the overfitting and improving the training of I2I model.
In this paper, we decouple our learning process into two steps: image generation and I2I translation. The first step aims to train a better generative model, which is leveraged to initialize the I2I translation system, and contributes to im-prove I2I translation performance. We introduce two contri-butions to improve the efficiency of the transfer, especially important for small domains. (1) we improve source-target initialization by finetuning the pretrained generative model (e.g., StyleGAN) on source and target data. This ensures that networks are already better prepared for their intended task in the I2I system. (2) we propose a self-initialization to pretrain the weights of the adaptor networks (the mod-ule A in Figure 1 (b)) without the need of any data. Here, we exploit the fact that these parameters can be learned by generating the layer activations from both the genera-tor and discriminator (by sampling from the latent variable z). From these activations the adaptor network weights can be learned. For the second step we conduct the actual I2I translation using the learned weights in the first step. Fur-thermore, we propose an auxiliary generator to encourage the usage of the deep layers of the I2I network.
Extensive experiments on a large variety of datasets con-firms the superiority of the proposed transfer learning tech-nique for I2I. It also shows that we can now obtain high-quality image on relatively small domains. This paper shows that transfer learning can reduce the need of data con-siderably; as such this paper opens up application of I2I to domains that suffer from data scarcity. Our main contribu-tions are:
• We explore I2I translation with limited data, reducing the amount of required labeled data.
• We propose several novel techniques (i.e., source-target initialization, self-initialization and auxiliary generator) to facilitate this challenging setting.
• We extensively study the properties of the proposed approaches on two-class and multi-class I2I translation tasks and achieve significant performance improve-ments even for high quality images. 2.