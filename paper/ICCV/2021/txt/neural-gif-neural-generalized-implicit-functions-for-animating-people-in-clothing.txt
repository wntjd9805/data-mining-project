Abstract
We present Neural Generalized Implicit Functions (Neural-GIF), to animate people in clothing as a function of the body pose. Given a sequence of scans of a subject in various poses, we learn to animate the character for new poses. Existing methods have relied on template-based rep-resentations of the human body (or clothing). However such models usually have fixed and limited resolutions, require difficult data pre-processing steps and cannot be used with complex clothing. We draw inspiration from template-based methods, which factorize motion into articulation and non-rigid deformation, but generalize this concept for implicit shape learning to obtain a more flexible model. We learn to map every point in the space to a canonical space, where a learned deformation field is applied to model non-rigid effects, before evaluating the signed distance field. Our for-mulation allows the learning of complex and non-rigid de-formations of clothing and soft tissue, without computing a template registration as it is common with current ap-proaches. Neural-GIF can be trained on raw 3D scans and reconstructs detailed complex surface geometry and defor-mations. Moreover, the model can generalize to new poses.
We evaluate our method on a variety of characters from dif-ferent public datasets in diverse clothing styles and show significant improvements over baseline methods, quantita-tively and qualitatively. We also extend our model to mul-tiple shape setting. To stimulate further research, we will make the model, code and data publicly available at [1]. 1.

Introduction
Human avatars enable numerous applications related to augmented and virtual reality, such as telepresence for en-hanced communication and entertainment, and have been instrumental to reconstruct and perceive people in im-ages [2, 3, 11, 22, 62, 61]. Human shape deforms according to articulation, soft-tissue and non-rigid clothing dynamics, which make realistic animations extremely challenging.
State-of-the-art body models [29, 43, 61] typically learn to deform a fixed topology template, usually using linear blend skinning to model articulation, and blendshapes to model non-rigid effects [29], even including soft-tissue [45] and clothing [31, 42, 52]. The use of a fixed template lim-its the type of clothing and dynamics that can be modeled.
For example, it would be difficult to model the subjects in
Fig. 1 with one or more predefined templates. Furthermore, every type of deformation (soft-tissue or clothing) requires a different model formulation. Additionally for training a model, 3D/4D scans need to be brought into correspon-dence [29, 45], which is a challenging task especially for clothing [44], and not even well defined when garments vary in topology [10]. Recent works have leveraged im-plicit function representation to reconstruct human shape from images [49, 50] or 3D point-clouds [8, 15]. These reconstructions are however static and not animatable.
In this work, we propose a new model, called Neu-ral Generalized Implicit Functions (Neural-GIF) to animate people in clothing as a function of body pose. We demon-strate that we can model complex clothing and body de-formations at a quality not easily achieved with a tradi-tional fixed template-based mesh representations. In con-trast to most prior work, we learn pose-dependent defor-mations without the need of registration of any pre-defined template, which degrades the resolution of the observations, and is a notoriously complex step prone to inaccuracies for complex clothing [44]. Instead we solely require the pose of the input scans as well as the SMPL shape parameter (β).
Another key advantage of our method is that it can repre-sent different topologies using the exact same formulation –we show how to animate jackets, coats, skirts and soft-tissue of undressed humans. Neural-GIF consists of a neu-ral network to approximate the signed distance field (SDF) of the posed surface. Naively learning to predict SDF from pose is hard. Instead, we draw inspiration from template-based methods which factorize motion into articulation and non-rigid deformation, but generalize this concept for im-plicit shape learning. Specifically, we learn to map every point surrounding the surface to a canonical space, where a learned deformation field is applied to model non-rigid ef-fects, before evaluating the SDF. Our model (and its name) is inspired by the seminal paper [53], which shows that a wide variety of shapes can be obtained by simply applying deformation fields to a base implicit shape. The advantage of Neural-GIF is that the network can more easily learn a base shape in canonical space, which can be deformed. In summary, our contributions are:
• Neural-GIF, an implicit based re-posable character, which can be directly learned from 3D scans. Our model can represent complex character/clothing scans of varied topology and geometry.
• We introduce a canonical mapping network, which learns continuous skinning field in 3D space and unpose 3D points surrounding the scan to a canonical T-pose, with-out explicit supervision.
• We introduce a displacement field network, which shifts points in the canonical space before evaluating the SDF, yielding in fine details and deformation.
We test our method on a variety of scans originating from different datasets and provide extensive quantitative and qualitative comparisons. We also extend our formula-tion to multiple shape setting, by adding a shape dependent displacement field network. Experiments demonstrate that our method generalizes to new poses, model complex cloth-ing, and is significantly more robust and more detailed than existing methods [51, 36, 18]. 2.