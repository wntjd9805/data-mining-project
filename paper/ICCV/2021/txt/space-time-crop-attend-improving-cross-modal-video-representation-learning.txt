Abstract
The quality of the image representations obtained from self-supervised learning depends strongly on the type of data augmentations used in the learning formulation. Re-cent papers have ported these methods from still images to videos and found that leveraging both audio and video signals yields strong gains; however, they did not ﬁnd that spatial augmentations such as cropping, which are very im-portant for still images, work as well for videos.
In this paper, we improve these formulations in two ways unique to the spatio-temporal aspect of videos. First, for space, we show that spatial augmentations such as cropping do work well for videos too, but that previous implementations, due to the high processing and memory cost, could not do this at a scale sufﬁcient for it to work well. To address this is-sue, we ﬁrst introduce Feature Crop, a method to simulate such augmentations much more efﬁciently directly in fea-ture space. Second, we show that as opposed to na¨ıve av-erage pooling, the use of transformer-based attention im-proves performance signiﬁcantly, and is well suited for pro-cessing feature crops. Combining both of our discoveries into a new method, Space-Time Crop & Attend (STiCA) we achieve state-of-the-art performance across multiple
In particular, video-representation learning benchmarks. we achieve new state-of-the-art accuracies of 67.0% on
HMDB-51 and 93.1% on UCF-101 when pre-training on
Kinetics-400. Code and pretrained models are available1.
Figure 1: HMDB-51 accuracy vs epoch. Our method,
STiCA, combines space-time crops in feature space with self-attention of time in latent space. This yields signiﬁcant beneﬁts not only in performance but also in speed compared to cropping in input space using two RGB crops, or simply using the default cross-modal only loss. Compared to recent state-of-the-art cross-modal self-supervised learning meth-ods (XDC [6], GDT [106], AVID-CMA [97], SeLaVi [9]) pre-trained on Kinetics-400 [69] STiCA is able to achieve signiﬁcantly better results in fewer epochs. 1.

Introduction
Visual representations have evolved signiﬁcantly in the last two decades. The ﬁrst generation of representations
*Equal contribution. 1https://github.com/facebookresearch/GDT comprises algorithms such as SIFT [87] and HOG [30] that were designed manually. The second generation comprises representations learned from data by using deep neural net-works and manual supervision [31, 59, 76]. We are now transitioning to the third generation, where representations are learned from data without using any manual annotations by means of self-supervision. Current self-supervised rep-resentations, obtained from methods such as MoCo [57],
SimCLR [24] or SwAV [20], convincingly outperform su-pervised ones on downstream tasks such as image classi-ﬁcation, segmentation and object detection. Furthermore, most of these methods are based on noise-contrastive in-stance discrimination, which was proposed in ExemplarC-NNs [39] and put in its current form in [142] and [101].
The idea is to learn representations that are invariant to ir-relevant factors of variations, modelled by strong augmen-tations such as image cropping, while remaining distinctive for the identity of the image.
Noise-contrastive learning is of course not limited to still images. In particular, a number of recent approaches [54, 93, 97, 106] have used noise-contrastive formulations to learn visual or audio-visual representations. However, these methods are not as well developed as their counterparts for still images, with current state-of-the-art methods [54, 106] still lagging behind their supervised counterparts.
In this paper, we identify two areas in which current video representation learning formulations are lacking and improve on them, thus signiﬁcantly improving upon the cur-rent state of the art in this area.
The ﬁrst shortcoming is the lack of a sufﬁcient encoding of spatial invariances. For still images, learning spatial invariances has been shown to be one of the most impor-tant factors for performance [20, 24]. Almost all methods achieve some form of spatial invariance simply by apply-ing different spatial augmentations to the images in different epochs of training. However, learning spatial invariances in this manner requires a slow training process that lasts for many epochs ( 800). Authors have suggested that pack-ing several augmentations of the same image in a single data batch is more effective as it provides a much stronger and more direct incentive for the network to learn invari-ances [20].
⇠
For videos, both strategies are less feasible. Training a model for 200 epochs on Kinetics-400 [69] already requires around 1.5K GPU hours on recent Nvidia V100 architec-tures, and with recent datasets such as IG65M [45] and
HowTo100M [94] only a handful of epochs can realistically be completed. On the other hand, including multiple aug-mentations of the same video in a batch rapidly exhausts the memory of GPUs. Since batch sizes per GPU are already in the single digits due to the size of video data, includ-ing several augmentations is unfeasible. This is particular detrimental for recent contrastive learning approaches such as [24, 58], where reducing the batch size means reducing the pool of negative contrastive samples.
In order to solve this problem, we propose to move spa-tial augmentations to the feature space, in a manner speciﬁ-cally tailored to contrastive learning. Instead of extracting a large number R of different augmentations in the input RGB space, we extract only two of them, apply the trunk of the neural network to extract corresponding features, and then extract R/2 more augmentations directly in feature space.
In this way, one needs to evaluate the slow and memory taxing feature extraction part of the network only twice, re-gardless of the number of augmentations that are produced.
We show that this feature-level augmentation signiﬁcantly improves representation learning performance.
The second challenge that we tackle is how to best en-code temporal information in self-supervised video rep-resentation learning. Currently, most self-supervised video representation learning approaches use 3D-CNNs [21, 132, 133, 144] that compute convolutions across space and time, but the ﬁnal representation is generated by na¨ıve global av-erage pooling over space and time, crucially discarding tem-poral ordering.
In order to address this shortcoming, in this work we propose to use a contextualized pooling function based on the transformer architecture [135] for both self-supervised pretraining and supervised ﬁnetuning. The intuition is that, via multi-head self-attention, the transformer can capture temporal dependencies much better than average pooling, especially for longer inputs. Transformers can also bene-ﬁt from our feature-level crops, as the latter resemble the common approach of randomly masking the inputs to the transformer [62]. Experimental results show that this modi-ﬁcation improves the performance of the learned video rep-resentations substantially, and is cumulative with the beneﬁt of feature crops, at about the same cost of average pooling.
We combine both of our proposed improvements into a new self-supervised learning approach: Space-Time Atten-tion and Cropping (STiCA). To summarize, with STiCA we make the following three main contributions:
• We demonstrate the beneﬁts of stronger spatial invari-ances in self-supervised video representation learning for the ﬁrst time and we propose feature-level augmen-tation to implement the latter efﬁciently.
• We propose to use transformers to model time more effectively in self-supervised video representations, re-placing average as the pooling function.
• We demonstrate strong performance gains by using the two techniques and obtain state-of-the-art performance on two standard benchmarks (67.0% on HMDB-51 and 93.1% on UCF-101). 2.