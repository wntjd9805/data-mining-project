Abstract
Image style transfer aims to transfer the styles of art-works onto arbitrary photographs to create novel artistic images. Although style transfer is inherently an underde-termined problem, existing approaches usually assume a deterministic solution, thus failing to capture the full dis-tribution of possible outputs. To address this limitation, we propose a Diverse Image Style Transfer (DIST) framework which achieves significant diversity by enforcing an invert-ible cross-space mapping. Specifically, the framework con-sists of three branches: disentanglement branch, inverse branch, and stylization branch. Among them, the disen-tanglement branch factorizes artworks into content space and style space; the inverse branch encourages the invert-ible mapping between the latent space of input noise vec-tors and the style space of generated artistic images; the
* Corresponding author stylization branch renders the input content image with the style of an artist. Armed with these three branches, our ap-proach is able to synthesize significantly diverse stylized im-ages without loss of quality. We conduct extensive experi-ments and comparisons to evaluate our approach qualita-tively and quantitatively. The experimental results demon-strate the effectiveness of our method. 1.

Introduction
An exquisite artwork can take a diligent artist days or even months to create, which is labor-intensive and time-consuming. Motivated by this, a series of recent approaches studied the problem of repainting an existing photograph with the style of an artist using either a single artwork or a collection of artworks. These approaches are known as style transfer. Armed with style transfer techniques, anyone could create artistic images.
How to represent the content and style of an image is the key challenge of style transfer. Recently, the seminal work of Gatys et al. [7] firstly proposed to extract content and style features from an image using pre-trained Deep
Convolutional Neural Networks (DCNNs). By separating and recombining contents and styles of arbitrary images, novel artworks can be created. This work showed the enor-mous potential of CNNs in style transfer and created a surge of interest in this field. Based on this work, a se-ries of subsequent methods have been proposed to achieve better performance in many aspects, including efficiency
[13, 21, 34], quality [20, 35, 40, 43, 39, 4], and generaliza-tion [6, 5, 10, 24, 30, 27, 22]. However, diversity, as another important aspect, has received relatively less attention.
As the saying goes, “There are a thousand Hamlets in a thousand people’s eyes”. Similarly, different people have different understanding and interpretation of the style of an artwork. There is no uniform and quantitative definition of the artistic style of an image. Therefore, the stylization re-sults should be diverse rather than unique, so that the prefer-ences of different people can be satisfied. To put it another way, style transfer is an underdetermined problem, where a large number of solutions can be found. Unfortunately, ex-isting style transfer methods usually assume a deterministic solution. As a result, they fail to capture the full distribution of possible outputs.
A straightforward approach to handle diversity in style transfer is to take random noise vectors along with content images as inputs, i.e., utilizing the variability of the input noise vectors to produce diverse stylization results. How-ever, the network tends to pay more attention to the high-dimensional and structured content images and ignores the noise vectors, leading to deterministic output. To ensure that the variability in the latent space can be passed into the image space, Ulyanov et al. [35] enforced the dissimilar-ity among generated images by enlarging their distance in the pixel space. Similarly, Li et al. [23] introduced a di-versity loss that penalized the feature similarities of differ-ent samples in a mini-batch. Although these methods can achieve diversity to some extent, they have obvious limita-tions. First, forcibly enlarging the distance among outputs may cause the results to deviate from the local optimum, resulting in the degradation of image quality. Second, to avoid introducing too many artifacts to the generated im-ages, the weight of the diversity loss is generally set to a small value. Consequently, the diversity of the stylization results is relatively limited. Third, diversity is more than the pixel distance or feature distance among generated im-ages, which contains richer and more complex connotation.
Most recently, Wang et al. [37] achieved better diversity by using an orthogonal noise matrix to perturb the image fea-ture maps while keeping the original style information un-changed. However, this approach is apt to generate distorted results, providing insufficient visual quality. Therefore, the problem of diverse style transfer remains an open challenge.
In this paper, we propose a Diverse Image Style Trans-fer (DIST) framework which achieves significant diversity without loss of quality by enforcing an invertible cross-space mapping. Specifically, the framework takes random noise vectors along with everyday photographs as its inputs, where the former are responsible for style variations and the latter determine the main contents. However, according to above analyses, we can learn that the noise vectors are prone to be ignored in the network. Our proposed DIST frame-work tackles this problem through three branches: disen-tanglement branch, inverse branch, and stylization branch.
The disentanglement branch factorizes artworks into content space and style space. The inverse branch encour-ages the invertible mapping between the latent space of in-put noise vectors and the style space of generated artistic images, which is inspired by [32]. But different from [32], we invert the style information rather than the whole gen-erated image to the input noise vector, since the input noise vector mainly influences the style of the generated image.
The stylization branch renders the input content image with the style of an artist. Equipped with these three branches,
DIST is able to synthesize significantly diverse stylized im-ages without loss of quality, as shown in Figure 1.
Overall, the contributions can be summarized as follows:
• We propose a novel style transfer framework which achieves significant diversity by learning the one-to-one mapping between latent space and style space.
• Different from existing style transfer methods [35, 23, 37] that obtain diversity with serious degradation of quality, our approach can produce both high-quality and diverse stylization results.
• Our approach provides a new way to disentangle the style and content of an image.
• We demonstrate the effectiveness and superiority of our approach by extensive comparison with several state-of-the-art style transfer methods. 2.