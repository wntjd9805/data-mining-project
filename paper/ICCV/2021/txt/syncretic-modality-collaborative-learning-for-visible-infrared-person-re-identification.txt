Abstract
Visible infrared person re-identiﬁcation (VI-REID) aims to match pedestrian images between the daytime visible and nighttime infrared camera views. The large cross-modality discrepancies have become the bottleneck which limits the performance of VI-REID. Existing methods mainly focus on capturing cross-modality sharable representations by learning an identity classiﬁer. However, the heterogeneous pedestrian images taken by different spectrum cameras dif-fer signiﬁcantly in image styles, resulting in inferior dis-criminability of feature representations. To alleviate the above problem, this paper explores the correlation between two modalities and proposes a novel syncretic modality collaborative learning (SMCL) model to bridge the cross-modality gap. A new modality that incorporates features of heterogeneous images is constructed automatically to steer the generation of modality-invariant representations. Chal-lenge enhanced homogeneity learning (CEHL) and aux-iliary distributional similarity learning (ADSL) are inte-grated to project heterogeneous features on a uniﬁed space and enlarge the inter-class disparity, thus strengthening the discriminative power. Extensive experiments on two cross-modality benchmarks demonstrate the effectiveness and su-periority of the proposed method. Especially, on SYSU-MM01 dataset, our SMCL model achieves 67.39% rank-1 accuracy and 61.78% mAP, surpassing the cutting-edge works by a large margin. 1.

Introduction
Person re-identiﬁcation (Re-ID) plays an essential role in video surveillance, which automatically searches person images across multiple non-overlapping cameras [39, 36].
Recently, fast-growing works contribute to visible modality person Re-ID and have achieved remarkable performance
[1, 28]. However, visible cameras cannot capture enough
*Corresponding author identity information in the dark. To ensure the safety of pedestrians at night, infrared cameras are deployed to ac-quire infrared person images, cooperating with visible cam-eras for 24-hour video surveillance. Hence, visible infrared person re-identiﬁcation (VI-REID) [27] has emerged to re-trieve visible (infrared) images according to the given in-frared (visible) images.
VI-REID is challenging due to the considerable visual differences among heterogeneous pedestrian images. Ex-isting studies aim to address this challenge mainly from two aspects, i.e., image-level and feature-level. To achieve modality uniﬁcation, image generation-based methods [11, 22, 25, 3, 23] are proposed to translate heterogeneous im-ages to the same modality for style consistency. However, the introduction of additional noise during the image trans-lation affects the extraction of discriminative features. To ensure feature alignment, dual-path networks are exploited to obtain modality-speciﬁc and modality-invariant repre-sentations [38, 33, 7, 5, 32], but the last few layers are difﬁ-cult to map the speciﬁc representations of each modality to a shared space. Subsequently, one-stream weight-sharing network is introduced in massive works [4, 24, 30, 9] to directly extract modality-sharable features. However, the performance of these methods is far inferior to that of visi-ble modality person Re-ID because of the signiﬁcant color discrepancies between two modalities.
Recently, several researches have built a new modal-ity and combine with two real modalities to conduct tri-modal sharable feature learning, which gains inspiring per-formance. Li et al. [13] introduce an auxiliary X modality as an assistant for modality-invariant feature generation. Ye et al.
[37] propose a homogeneous augmented grayscale modality and enhance the robustness against color varia-tions. However, these methods neglect the distribution of features in the intermediate modality. As shown in Figure 1, since the images of X modality and grayscale modality are directly generated by the visible images without con-sidering the infrared images, there are two main drawbacks encountering in feature distribution of testing set: 1) The
tions of heterogeneous images with the same identity have been assembled, and the feature distances of different iden-tities have been enlarged, thus promoting the performance of VI-REID. The experimental results on SYSU-MM01 and
RegDB datasets validate the effectiveness of our method.
The main contributions of this paper can be summarized as follows:
• We propose a novel syncretic modality collaborative learning model for VI-REID task by constructing a self-generated modality which combines visible and
Joint learning of three infrared image information. modalities induces the network to capture modality-invariant representations with high discriminability.
• We introduce challenge enhanced homogeneity learn-ing to increase the difﬁculty of infrared image clas-siﬁer, thereby urging the network to obtain more dis-criminative features for correct classiﬁcation. Besides, auxiliary distributional similarity learning is employed to shrink the cross-modality gap through tri-directional distance suppression.
• We develop incremental training scheme to handle the distribution of heterogeneous images from coarse to
ﬁne, thus learning more effective modality-shared dis-criminative representations for VI-REID. The perfor-mance of our SMCL model outperforms the state-of-the-art methods by a remarkable margin. 2.