Abstract
In this paper, we abandon the dominant complex lan-guage model and rethink the linguistic learning process in the scene text recognition. Different from previous meth-ods considering the visual and linguistic information in two separate structures, we propose a Visual Language Mod-eling Network (VisionLAN), which views the visual and lin-guistic information as a union by directly enduing the vision model with language capability. Specially, we introduce the text recognition of character-wise occluded feature maps in the training stage. Such operation guides the vision model to use not only the visual texture of characters, but also the linguistic information in visual context for recognition when the visual cues are confused (e.g. occlusion, noise, etc.).
As the linguistic information is acquired along with visual features without the need of extra language model, Vision-LAN significantly improves the speed by 39% and adap-tively considers the linguistic information to enhance the visual features for accurate recognition. Furthermore, an
Occlusion Scene Text (OST) dataset is proposed to eval-uate the performance on the case of missing character-wise visual cues. The state of-the-art results on several benchmarks prove our effectiveness. Code and dataset are available at https://github.com/wangyuxin87/
VisionLAN . 1.

Introduction
As a fundamental and pivotal task, scene text recog-nition (STR) aiming to read the text content from natu-ral images has attracted great interest in computer vision
[15, 31, 32, 42, 46]. By taking the text image as input and textual prediction as output, some early methods regard the text recognition as a symbol classification task [31, 19].
*Corresponding author the architecture of previous methods. Top right:
Figure 1. Comparison between previous methods and ours. Top left: the ex-tra introduced computation cost for capturing linguistic informa-tion when the word length increases. Bottom: the proposed Vi-sionLAN endues the vision model with ability to initiatively cap-ture the linguistic information in visual context during the training stage. In the testing stage, only the vision model is used for pre-diction.
However, it is hard to recognize images with confused vi-sual cues (e.g. occlusion, noise, etc.), which are beyond vi-sual discrimination. As the scene text image contains two-level contents: visual texture and linguistic information, in-spired by the Natural Language Processing (NLP) methods
[23, 5], recent STR works have shifted their research fo-cus to acquiring linguistic information to assist recognition
[47, 46, 28, 45]. Thus, the two-step architecture of vision and language models (top left of Fig. 1) is popular in recent methods. Specifically, the vision model only focuses on vi-sual texture of characters without considering the linguistic information. Then, the language model predicts the rela-tionship between characters through the linguistic learning structure (RNN [32], CNN [7] and Transformer [45]).
Though these methods achieve promising results, there are still two problems: 1) the extra huge computation cost. The computation cost of language model increases significantly with the word length getting longer (linear growth for RNN [32]/ CNN [7] and quadratic growth for
Transformer [45] in Fig. 1). Furthermore, many methods adopt a deep bi-directional reasoning architecture [38, 45, 32] to capture more robust linguistic information, which further doubles the computation burden and greatly limits their efficiency in the real application. 2) The difficulty of aggregating two independent information. It is difficult to comprehensively consider and effectively fuse the visual and linguistic information from two separate structures for accurate recognition [7, 46]. In this paper, we attribute these two problems to the lack of language ability in the vision model, which only focuses on the visual texture of charac-ters without initiatively learning linguistic information [45].
As shown in bottom of Fig. 1, inspired by the human cog-nitive process that the language capability can be acquired
[21, 11], we use vision model as the basic network, and guide it to reason the occluded character during the training stage. Thus, vision model is trained to initiatively learn lin-guistic information in the visual context. In the test stage, vision model adaptively considers the linguistic information in the visual space for feature enhancement when the visual cues are confused (e.g. occlusion, noise, etc.), which ef-fectively supplements the features of occluded characters, and correctly highlights the discriminating visual cues of confused characters (shown in Fig. 5). To the best of our knowledge, this is the first work to give vision model the ability to perceive language in scene text recognition. We call this new simple architecture as Visual Language Mod-eling Network (VisionLAN).
The pipeline of VisionLAN is shown in Fig. 2. Vi-sionLAN contains three parts: backbone network, Masked
Language-aware Module (MLM) and Visual Reasoning
Module (VRM). In the training stage, visual features V are firstly extracted from the backbone network. Then
MLM takes the visual features V and character index P as inputs, and automatically generates the character mask map M askc at corresponding position through a Weakly-supervised Complementary Learning. MLM aims to simu-late the case of missing character-wise visual cues by oc-cluding visual messages in V with M askc.
In order to consider the linguistic information during the visual texture modeling, we propose a VRM with the ability to capture long-range dependencies in the visual space. VRM takes the occluded feature map Vm as input, and is guided to make the word-level prediction. In the test stage, we remove the
MLM and only use VRM for recognition. As the linguistic information is acquired along with visual features without the need of extra language model, VisionLAN introduces
ZERO computation cost for capturing linguistic informa-tion (top right in Fig. 1) and significantly improves the speed by 39% (Sec. 4.4). Compared with previous methods, the proposed VisionLAN obtains more robust performance on the occluded and low-quality images, and achieves new state-of-the-art results on several benchmarks with a con-cise pipeline. In addition, an Occlusion Scene Text (OST) dataset is proposed to evaluate the performance on the case of missing character-wise visual cues.
The main contributions of this paper are as follows: 1) A new simple architecture is proposed for scene text recogni-tion. We further visualize the feature maps to illustrate how
VisionLAN initiatively uses linguistic information to handle the confused visual cues (e.g. occluded, noise, etc.). 2) We propose a Weakly-supervised Complementary Learning to generate accurate character-wise mask map in MLM with only word-level annotations. 3) A new Occlusion Scene
Text (OST) dataset is proposed to evaluate the recognition performance of occluded images. Compared with previ-ous methods, VisionLAN achieves the state-of-the-art per-formance on seven benchmarks (irregular and regular) and
OST with a concise pipeline. 2.