Abstract
We develop a novel self-supervised learning method named Shape Self-Correction for point cloud analysis. Our method is motivated by the principle that a good shape representation should be able to find distorted parts of a shape and correct them. To learn strong shape representa-tions in an unsupervised manner, we first design a shape-disorganizing module to destroy certain local shape parts of an object. Then the destroyed shape and the normal shape are sent into a point cloud network to get represen-tations, which are employed to segment points that belong to distorted parts and further reconstruct them to restore the shape to normal. To perform better in these two asso-ciated pretext tasks, the network is constrained to capture useful shape features from the object, which indicates that the point cloud network encodes rich geometric and con-textual information. The learned feature extractor transfers well to downstream classification and segmentation tasks.
Experimental results on ModelNet, ScanNet and ShapeNet-Part demonstrate that our method achieves state-of-the-art performance among unsupervised methods. Our framework can be applied to a wide range of deep learning networks for point cloud analysis and we show experimentally that pre-training with our framework significantly boosts the performance of supervised models. 1.

Introduction 3D shape understanding is in tremendous demand due to many important tasks like autonomous driving. Point cloud is a simple but effective representation of 3D data, which makes it popular for 3D vision analysis. With the help of extensive manually-labeled supervised information, many ingenious works [21, 22, 26, 40, 17, 42, 20, 14, 18] are pro-posed to directly consume point clouds and achieve remark-†Co-corresponding authors:Bingbing Ni, Ning Liu
∗Equal contributions
Illustration of our main idea. As shown, we
Figure 1: destroy the shape parts with certain heuristic methods and there is a huge mismatch between the distorted parts and the normal parts. We can easily distinguish the distorted parts because we know the geometric characteristics of the ob-ject. Hence we think a strong representation which encodes effective structure information should also have the ability.
We destroy shape parts and train a network to distinguish the destroyed parts and restore them to normal unsupervis-edly in a pretext task. Success in the pretext task indicates the network captures strong shape representations, which can transfer well to downstream tasks. able performance on 3D vision tasks like classification, de-tection and segmentation. However, an enormous amount of 3D point cloud data has not been effectively utilized be-cause of the expensive labeling. Hence utilizing these unla-beled data to perform effective representation learning is an important opportunity for 3D analysis.
Unsupervised learning on point clouds aims to learn useful information and representations from points with-out manually-labeled supervised information, which opens up the possibility to take advantage of unlabeled data.
Several works focus on 3D unsupervised feature learning through using autoencoders and generative adversarial net-works [2, 8, 15, 43, 47, 7]. Certain recently devoted self-supervised works [10, 25, 11, 1, 31] design target related
pretext tasks to encourage the network to capture structural and low-level information. PointGLR [24] effectively cap-tures the underlying high-level semantic knowledge through bidirectional reasoning between the local structures and the global shape and achieves superior performance on classi-fication tasks. Nevertheless, PointGLR relies on the hierar-chical local features and it is not suitable for networks like
PointNet [21] and DGCNN [34]. The goal of our work is to explore a backbone-agnostic self-supervised framework that is capable of fully utilizing local structure of shape parts and boosting the performance of unsupervised learning.
Each 3D shape can be divided into several shape parts/primitives in an unsupervised manner and all the shape parts are closely related through geometric constraints. The geometric constraints reflect robust geometric characteris-tics and imply local structure information and semantic knowledge of the object. Hence, one can easily distinguish distorted parts of a shape if he knows the geometric struc-ture of such shape. Motivated by such principle, we think that a good shape representation which encodes effective structural and semantic information should also have the ability to find distorted parts of a shape and correct them.
Inspired by such observations, we propose a self-supervised framework for learning strong representations of 3D shapes by destroying local parts of a 3D shape and en-couraging the network to distinguish the destroyed shape parts and then restore them to normal. For success in this pretext task, the network is constrained to capture richer ge-ometric and structural information of the 3D point cloud.
The overview of our main idea is shown in Figure 1. Our proposed framework is agnostic of point-based networks like PointNet, KPConv [28], and RSCNN [20]. In this pa-per, we modify PointNet and RSCNN as our feature ex-tractor respectively to evaluate our proposed method. We concatenate the normal shape and disorganized shape as the input of the backbone network during pre-training. With the features of the normal shape, accurate structure informa-tion is obtained so that the network is capable of performing well on the pretext tasks. In addition to the backbone net-work, our proposed framework has three other components , which can be summarized as: 1) Shape-disorganizing mod-ule: we design a cluster of heuristic methods to effectively destroy the geometric structure of normal shape parts; 2)
Distinguishing Branch: we implement a point-wise clas-sifier to segment points that belong to the distorted parts; 3) Restoring Branch: we also design a self-reconstruction module to correct the distorted shape based on the seg-mentation results of the Distinguishing branch. Notably,we propose an approach cluster in Shape-disorganizing module and a wide range of methods that destroy geometric struc-ture of shape parts can be included in.
In this paper, we utilize the ShapeNet [3] dataset as our source set for self-supervised pre-training and evalu-ate the learned features on two important 3D understanding tasks, i.e., shape classification and segmentation. Experi-mental results on several datasets indicate that our method achieves state-of-the-art performance among unsupervised models on both classification and segmentation tasks. Note that our model achieves remarkable performance on a real-world scanned dataset (ScanNet [4]), which demonstrates the transferability and robustness of learned features. We also show experimentally that pre-training with our frame-work significantly boosts the performance of supervised models. On the segmentation task, we also explore the ef-fectiveness of the learned features in a semi-supervised set-ting and our method outperforms previous methods [39, 11]
, especially when labels are most limited. In addition, our per-trained model achieves competitive results on down-stream tasks when only using PointNet as the backbone net-work, which demonstrates the strong feature learning ability of our framework. 2.