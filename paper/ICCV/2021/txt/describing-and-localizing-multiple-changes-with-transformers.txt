Abstract
Change captioning tasks aim to detect changes in im-age pairs observed before and after a scene change and generate a natural language description of the changes.
Existing change captioning studies have mainly focused on a single change. However, detecting and describing multiple changed parts in image pairs is essential for en-hancing adaptability to complex scenarios. We solve the above issues from three aspects: (i) We propose a simula-tion-based multi-change captioning dataset; (ii) We bench-mark existing state-of-the-art methods of single change cap-tioning on multi-change captioning; (iii) We further pro-pose Multi-Change Captioning transformers (MCCForm-ers) that identify change regions by densely correlating dif-ferent regions in image pairs and dynamically determines the related change regions with words in sentences. The proposed method obtained the highest scores on four con-ventional change captioning evaluation metrics for multi-change captioning. Additionally, our proposed method can separate attention maps for each change and performs well with respect to change localization. Moreover, the proposed framework outperformed the previous state-of-the-art meth-ods on an existing change captioning benchmark, CLEVR-Change, by a large margin (+6.1 on BLEU-4 and +9.7 on
CIDEr scores), indicating its general ability in change cap-tioning tasks. The code and dataset are available at the project page 1. 1.

Introduction
Detecting and describing the changed parts in scenes at different times is essential in various scenarios, such as urbanization analysis [1, 2, 3], resource management
[4, 5, 6, 7, 8], updating street-view maps for navigation
[9, 10], damage detection [11, 12], video surveillance [13],
*equal contribution 1https://cvpaperchallenge.github.io/Describing-and-Localizing-Multiple-Change-with-Transformers
Figure 1. Given two images of a scene observed before and after multiple changes (the first column), we generate change caption-ing for each scene change along with attention maps (the second and third columns) indicating the region of changed objects. and robotic applications [14, 15]. Recently, Jhamtani and
Berg-Kirkpatrick [13] proposed the change captioning task to describe changes from image pairs of before and after scene changes. Describing change is useful for extracting semantic contents and conveying information to humans.
Several methods have been proposed for the change cap-tioning [13, 16, 17, 18, 19]. Most existing works focus on describing a single change. As a practical matter, multiple changes could be manifested within an image pair. Jham-tani and Berg-Kirkpatrick [13] studied scene change cap-tioning with multiple changes; however, they addressed the problem with a known number of changes which is not pro-vided in real-world problems. Practically, detecting and describing scene changes without prior information regard-ing changes is more useful in terms of providing informa-tion to users. We address multi-change captioning, where changed regions are localized and language descriptions of scene changes are generated from pair of images with an unknown number of changes, as shown in Figure 1.
Change captioning requires capturing relationships be-tween image pairs, localizing changed regions, and gen-In this work, we intro-erating language descriptions. duce a simple but effective framework Multi-Change Cap-tioning transformers (MCCFormers) based on encoder-decoder transformer [20] which performs well in natural language processing. The encoder transformer captures relationship between local regions in two images to de-tect scene changes. Then, the decoder transformer attends over changed regions and generates language descriptions of the changes. In contrast to existing methods that gener-ate a static attention map [16, 17], the decoder transformer changes the spatial attention for each generated word. Con-sequently, the decoder transformer can distinguish between different changes and avoid confusing them for one another.
To evaluate multi-change captioning and localization ability, we build a novel CLEVR-Multi-Change dataset con-sisting of image pairs containing multiple changes, change captions, and bounding boxes of the changed region. We compare the proposed MCCFormers model with several state-of-the-art methods under the multi-change setup. The experimental results show that the proposed method per-formed well in both change captioning and localization.
The contributions of our work are three-fold: (i) We ad-dress a novel task of multi-change captioning and propose a dataset for this task where multiple changes exist in before-and after-change images and the number of changes is un-known; (ii) We propose MCCFormers, which consists of encoder-decoder transformers that capture relationships be-tween images and densely correlates image regions with words; (iii) The proposed MCCFormers outperforms exist-ing methods in terms of four conventional image captioning evaluation metrics and shows promising ability on localiza-tion for multi-sentence change captioning. 2.