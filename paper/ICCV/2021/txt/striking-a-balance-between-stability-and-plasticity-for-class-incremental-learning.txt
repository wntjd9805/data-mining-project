Abstract
Class-incremental learning (CIL) aims at continuously updating a trained model with new classes (plasticity) with-out forgetting previously learned old ones (stability). Con-temporary studies resort to storing representative exem-plars for rehearsal or preventing consolidated model pa-rameters from drifting, but the former requires an addi-tional space for storing exemplars at every incremental phase while the latter usually shows poor model general-ization. In this paper, we focus on resolving the stability-plasticity dilemma in class-incremental learning where no exemplars from old classes are stored. To make a trade-off between learning new information and maintaining old knowledge, we reformulate a simple yet effective baseline method based on a cosine classiﬁer framework and recip-rocal adaptive weights. With the reformulated baseline, we present two new approaches to CIL by learning class-independent knowledge and multi-perspective knowledge, respectively. The former exploits class-independent knowl-edge to bridge learning new and old classes, while the lat-ter learns knowledge from different perspectives to facili-tate CIL. Extensive experiments on several widely used CIL benchmark datasets show the superiority of our approaches over the state-of-the-art methods. 1.

Introduction
Humans have the ability to incrementally learn unseen new categories without forgetting already learned old cate-gories to realize lifelong learning. Class-Incremental Learn-ing (CIL) resembles this capability and aims at continuously updating a trained model with samples from new classes without forgetting old ones [42, 44, 32], where samples from old classes are not available or only partially avail-able. However, this is not a trivial task for the machine.
If we directly ﬁne-tune a trained model with samples from new classes, it will overﬁt to new classes and forget old ones (see Fig. 1(a)); If we ﬁx the feature embedding space of
Figure 1. The stability-plasticity dilemma in class-incremental learning, illustrated by top-1 accuracy and t-SNE [35] visualiza-tion of embeddings. On CIFAR-100, we randomly use 50 classes for training at the 1-st phase (P1), and then add 5 classes for incre-mental training at the 2-nd phase (P2). (a) Directly ﬁne-tuning a trained model leads to overﬁtting to new classes; (b) Fix a trained model cannot properly incorporate knowledge of new classes into the model; (c) and (d) Our methods strike a balance between sta-bility and plasticity, resulting in better performance. a trained model without further training on new classes, it cannot incorporate knowledge of new classes to improve its generalization capability (see Fig. 1(b)). This is a stability-plasticity dilemma [3, 14] – on the one hand, our model should learn more new knowledge for the sake of plasticity, while on the other hand, our model needs to maintain more old knowledge for the sake of stability (without catastrophic forgetting [27, 31]).
To resolve this problem, many CIL studies [32, 26, 2] re-sort to storing some representative exemplars for rehearsal-based model learning and using a distillation loss [15, 40] for knowledge transfer. However, this approach is imprac-tical in many resource-limited scenarios because it requires to store exemplars of old classes. Besides, training a model with a tiny number of old exemplars would lead to a class-imbalanced learning problem [41, 18]. Instead of storing
old exemplars, some works [20, 1, 25] propose to analyze the importance of model parameters for preventing consol-idated parameters from drifting. But this approach usually suffers from poor model generalization in long-sequence in-cremental learning due to the constraint of model parame-ters. Recently, some studies turn to using better distillation strategies (e.g., attention distillation [7]) or compensating semantic drift in the embedding space [42], but they still fail to make a better trade-off between learning new infor-mation and maintaining old knowledge.
In this paper, we focus on resolving the stability-plasticity dilemma in CIL where no samples from old classes are stored. To this end, we reformulate a sim-ple yet effective baseline method (called SPB) to make a trade-off between learning new information and maintain-ing old knowledge. SPB is built on a cosine classiﬁer frame-work [30] and reciprocal adaptive weights for incremen-tally incorporating knowledge of new classes into a model and effectively aligning feature embedding spaces. Previ-ous studies [30, 38, 18] have shown the effectiveness of cosine classiﬁer based models for simultaneously optimiz-ing an embedding space and class prototypes (i.e., weights of cosine classiﬁers), but they still cannot well resolve the stability-plasticity dilemma in CIL, especially when with-out storing exemplars. In this work, the reformulated SPB baseline addresses the problem by modulating the balance between knowledge from old and new classes in model op-timization, resulting in a trade-off between improving plas-ticity and maintaining stability.
With the reformulated SPB baseline method, we in-troduce two new approaches to further striking a balance between stability and plasticity for CIL. Firstly, conven-tional CIL studies mostly focus on learning knowledge of new and old classes but ignore the fact that new and old classes are typically not overlapping, resulting in sub-optimal performance. Thus, to build a bridge between new and old classes, we propose a SPB-I method that in-corporates a class-independent learner into SPB for learn-ing class-independent knowledge. This class-independent learner provides additional instance-level supervision, so
SPB-I exploits more discriminative information as a bridge for learning new and old classes independent from class la-bels, resulting in better performance (see Fig. 1(c)). Sec-ondly, since samples from old classes are not stored, re-taining richer knowledge of samples from different perspec-tives can help to improve the understanding of both old and new classes. Thus, we propose a SPB-M method to ex-ploit knowledge of samples from multiple perspectives1 by transforming each sample multiple times to generate multi-perspective information and using multiple cosine classi-ﬁers for aggregating knowledge, resulting in better perfor-mance for CIL (see Fig. 1(d)). 1 We term each transformation of a sample as a “perspective”.
Contributions. With the reformulated baseline method (SPB) for resolving the stability-plasticity dilemma in CIL, we introduce two new approaches (SPB-I and SPB-M) to further striking a balance between stability and plasticity.
In SPB-I, we incorporate a class-independent learner into
SPB for learning class-independent knowledge to build a bridge between new and old classes.
In SPB-M, we ex-ploit richer knowledge of samples from different perspec-tives to improve the understanding of both old and new classes. Our experiments show that our approaches (SPB,
SPB-I and SPB-M) outperform the state-of-the-art methods on different CIL tasks. 2.