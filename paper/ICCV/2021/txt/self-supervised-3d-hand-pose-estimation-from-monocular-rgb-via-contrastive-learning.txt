Abstract
Encouraged by the success of contrastive learning on im-age classification tasks, we propose a new self-supervised method for the structured regression task of 3D hand pose estimation. Contrastive learning makes use of unlabeled data for the purpose of representation learning via a loss formulation that encourages the learned feature represen-tations to be invariant under any image transformation. For 3D hand pose estimation, it too is desirable to have in-variance to appearance transformation such as color jit-ter. However, the task requires equivariance under affine transformations, such as rotation and translation. To ad-dress this issue, we propose an equivariant contrastive ob-jective and demonstrate its effectiveness in the context of 3D hand pose estimation. We experimentally investigate the impact of invariant and equivariant contrastive objec-tives and show that learning equivariant features leads to better representations for the task of 3D hand pose esti-mation. Furthermore, we show that standard ResNets with sufficient depth, trained on additional unlabeled data, at-tain improvements of up to 14.5% in PA-EPE on FreiHAND and thus achieves state-of-the-art performance without any task specific, specialized architectures. Code and models are available at https://ait.ethz.ch/projects/2021/PeCLR/ 1.

Introduction
Estimating the 3D pose of human hands from monocular images alone has many important applications in robotics,
Human-Computer Interaction and AR/VR. As such the problem has received significant attention in computer vi-sion literature [12, 14, 15, 26, 31â€“33, 41]. However, esti-mating the location of 3D hand joints within an RGB image is a challenging structured regression problem with difficul-ties that arise from a large diversity in backgrounds, light-ing conditions, hand appearances, as well as self-occlusion caused by the high degrees of freedom of the human hand.
Annotated datasets that cover a larger diversity of en-*Denotes equal contribution
Figure 1: We propose a two-stage framework for 3D hand pose estimation.
I) An encoder is trained in a self-supervised manner on a large set of unlabeled data using a novel equivariant contrastive objective. II) The pre-trained encoder is fine-tuned with little labeled data. The resulting network is more accurate across datasets. vironments and settings are one possibility to alleviate this issue. However, acquiring 3D labeled data is laborious, cost intensive and typically requires multi-view imagery or some form of user instrumentation. Data collected under such cir-cumstances is often difficult to transfer well to in-the-wild imagery [20, 42]. Therefore, much interest is given to ap-proaches that can leverage auxiliary data, which has either no or only 2D joint annotations. For example, such data can be used to outperform many supervised approaches via making use of weak-supervision [3, 4], the integration of
kinematic priors [31], or by exploiting temporal information
[14]. Off-the-shelf joint detectors [5] have been leveraged to automatically generated 2D annotations in large quanti-ties [20]. However, the accuracy of models trained on these labels, or on 3D annotations derived from them, are inher-ently bounded by the label noise. Therefore, the question of how to efficiently leverage unlabeled data for hand pose estimator training remains unanswered.
Recently, self-supervised approaches such as contrastive learning have shown that they can reach parity with super-vised approaches on image classification tasks [6, 8]. These methods leverage unlabeled data to learn powerful feature representations. To do so, positive and negative pairs of im-ages are projected into a latent space via a neural network.
The contrastive objective encourages the latent space sam-ples of the positive pairs to lie close to each other and pushes negative pairs apart. The resulting pre-trained network can then be applied to downstream tasks. Positive pairs are created by sampling an image and applying two sets of distinct augmentations on it, whereas negative pairs corre-spond to separate but similarly augmented images. These augmentations include appearance transformations, such as color drop, and geometric transformations, such as rota-tion. The contrastive objective induces invariance under all of these transformations. However, 3D regressions tasks, such as hand pose estimation, inherently require equivari-ance under geometric transformations. Hence, representa-tions learnt from a standard contrastive objective may not effectively transfer to pose estimation.
To the best of our knowledge, for the first time, we inves-tigate self-supervised representation learning techniques for 3D hand pose estimation in this paper. We derive a method named Pose Equivariant Contrastive Learning (PeCLR).
One of our core contributions is a novel formulation of a contrastive learning objective that induces equivariance to geometric transformations and we show that this allows to effectively leverage the large diversity of existing hand im-ages without any joint labels. These images are used to pre-train a network, which can then be transferred to the final hand pose estimation task via supervised fine-tuning. This provides a promising direction for hand pose estimation and enables an easy transfer of images collected in-the-wild or calibration to a specific domain by fine-tuning a pre-trained network with fewer labels.
Fig. 1 provides an overview of our method. First, we perform self-supervised representation learning. Given an
RGB image of the hand, we apply appearance and geomet-ric transformations to generate positive and negative pairs of derivative images. These are used to train an encoder via our proposed equivariant contrastive loss. By undoing the geometric transformation in latent space, we promote equivariance. However, invertion of these transformations is not straightforward. This is because transformations on images should lead to proportional changes in the latent space. Therefore special care needs to be taken due to dif-ferent magnitudes between latent space and pixel space un-der learned projection. We propose a latent sample normal-ization technique that compensates for this difference and we show that the resulting model yields improved pose es-timation accuracy (cf. Fig. 1, bottom) compared to both supervised and standard contrastive learning.
In the second stage, the pre-trained encoder is fine-tuned on the task of 3D hand pose estimation using labeled data.
The resulting model is evaluated thoroughly in a variety of settings. We demonstrate increased label efficiency for semi-supervision and show that using more unlabeled data is beneficial for the final performance, yielding improve-ments of up to 43% in 3D EPE in the lowest labeled setting (cf. Fig. 6). Next, we show that this improvement also trans-fers to the fully supervised case, where using a standard
ResNet with sufficient depth in combination with unlabeled data and our proposed pre-training scheme outperforms spe-cialized state-of-the-art architectures (cf. Tab. 2). Finally, we demonstrate that self-supervised pre-training leads to an improvement of 5.6% 3D PA-EPE in cross-data evaluation, indicating that pre-training is beneficial for cross-domain generalization (cf. Tab. 3).
In summary, our contributions are as follows: 1. To the best of our knowledge, we perform the first in-vestigation of contrastive learning to efficiently lever-age unlabeled data for 3D hand pose estimation. 2. We propose a contrastive learning objective that en-courages invariance to appearance transformations and equivariance to geometric transformations. 3. We conduct controlled experiments to empirically de-rive the best performing augmentations. 4. We show that the proposed method achieves better label efficiency in semi-supervised settings and that adding more unlabeled data is beneficial. 5. We empirically show that our proposed method outper-forms current, more specialized state-of-the-art meth-ods using standard ResNet models.
Code and models are available for research purposes: https://ait.ethz.ch/projects/2021/PeCLR/. 2.