Abstract
While self-training has advanced semi-supervised se-mantic segmentation, it severely suffers from the long-tailed class distribution on real-world semantic segmenta-tion datasets that make the pseudo-labeled data bias to-ward majority classes.
In this paper, we present a sim-ple and yet effective Distribution Alignment and Random
Sampling (DARS) method to produce unbiased pseudo la-bels that match the true class distribution estimated from the labeled data. Besides, we also contribute a progres-sive data augmentation and labeling strategy to facilitate model training with pseudo-labeled data. Experiments on both Cityscapes and PASCAL VOC 2012 datasets demon-strate the effectiveness of our approach. Albeit simple, our method performs favorably in comparison with state-of-the-art approaches. Code will be available at https:
//github.com/CVMI-Lab/DARS. 1.

Introduction
Recent years have witnessed the great success of deep convolutional neural network (DCNNs) in semantic seg-mentation [46, 37, 8, 36, 57]. The success, however, heavily relies on a large number of training data with accurate pixel-level human annotations, which are prohibitively expensive and time-consuming to collect.
Semi-supervised learning (SSL) provides a promising path [51, 49, 29, 52, 47] to ease human annotation burden by using a small amount of labeled data in conjunction with a large amount of unlabeled data to obtain an accurate model.
In this regard, self-training, alternating between generat-ing pseudo labels for unlabeled data using model predic-tions and training the model with pseudo-labeled data, is a classic and effective approach for semi-supervised learning and has obtained state-of-the-art results [17, 5, 60] in semi-supervised semantic segmentation with DCNNs.
Motivations. Despite the encouraging results, most of the previous self-training approaches [60, 59, 31, 56, 47] as-*equal contribution
†corresponding author
Figure 1. Class distribution mismatch on the Cityscapes dataset [10]. ’Labeled’ and ’ST’ denote the class distribution of true labels in the labeled set and pseudo labels produced by ST.
We line up percentages of each class for better visualization. sume a class-balanced data distribution and hence adopt a single confidence thresholding (ST) scheme to produce the pseudo labels (i.e. pixels with prediction confidence score exceeding a pre-defined threshold are pseudo-labeled) to guarantee pseudo label qualities. However, most real-world semantic segmentation datasets [35, 10, 15, 58] have long tail class distributions with few categories occupying the majority of pixels as illustrated in Fig. 1. And, it is well known that DCNNs trained with such long-tailed data dis-tribution will produce predictions biased toward the dom-inant categories [12]. This can be even more problematic for self-training, since pseudo labels are generated based on these biased model predictions. There exists a severe distri-bution mismatch between true and pseudo labels, especially for tail categories (see Fig. 1), which will harm self-training.
Recently, only very few works [61, 17] attempt to ad-dress the class distribution issue in pseudo labels via sam-pling the same percentage of pixels for each category based on the predicted results instead of using a single confidence threshold. However, as the class distribution for the pre-dictions has already deviated from the true distribution, the
produced pseudo labels will undoubtedly still suffer from the bias. Here, we argue that this distribution mismatch is-sue is a largely overlooked problem, hindering further im-provements in semi-supervised semantic segmentation.
Our Contributions. In this work, we present a simple yet effective baseline method to re-distribute the biased pseudo labels, aligning their distribution with the true distribution for improving semi-supervised semantic segmentation.
First, we highlight the distribution mismatch issue in semi-supervised semantic segmentation, formulate the task as an optimization problem, and further design a Distri-bution Alignment and Random Sampling (DARS) method to obtain unbiased pseudo-labeled data, matching the true class distribution. We point out that many pixels share the same confidence value (confidence overlapping) due to the over-confident issue in DCNNs [20], which makes it not vi-able to achieve distribution matching only by thresholding.
Therefore, we propose distribution alignment with class-wise thresholding and random sampling to achieve perfect distribution alignment.
Second, inspired by [53], during the self-training pro-cess, we contribute a progressive strategy tailored to se-mantic segmentation which gradually increases the strength of data augmentation (e.g. the range of random scaling) and enlarges the labeling ratio. This strategy prevents the model from being overwhelmed by noisy data from an in-accurate model or strongly augmented examples at the ini-tial stage, and avoids overfitting to high-confident pseudo-labeled easy examples through leveraging diversified aug-mented data and an increased number of pseudo-labeled data from an improved model.
Third, our proposed method is generic, simple and ef-ficient, which can be seamlessly incorporated into other self-training pipelines for semi-supervised semantic seg-mentation by adding only a few lines of code. Albeit sim-ple, our approach achieves surprisingly good performance compared with state-of-the-art approaches. For Cityscapes dataset, our model gains a significant amount of perfor-mance boost of 8.89% mIoU in the 1 8 split setting, ap-proaching the fully supervised results. Moreover, we also verify our method on PASCAL VOC 2012, where ours out-performs previous state-of-the-art by 4.49% mIoU.
Finally, we further explore the performance gain in semi-supervised semantic segmentation with the growth of un-labeled data and find that the performance gain gradually saturates in the high-data regime. Further, we analyze the potential bottlenecks for this issue and suggest future direc-tions, hoping to inspire more works in this direction. 2.