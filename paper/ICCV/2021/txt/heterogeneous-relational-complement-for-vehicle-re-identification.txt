Abstract
The crucial problem in vehicle re-identiﬁcation is to ﬁnd the same vehicle identity when reviewing this object from cross-view cameras, which sets a higher demand for learn-ing viewpoint-invariant representations. In this paper, we propose to solve this problem from two aspects: construct-ing robust feature representations and proposing camera-sensitive evaluations. We ﬁrst propose a novel Heteroge-neous Relational Complement Network (HRCN) by incor-porating region-speciﬁc features and cross-level features as complements for the original high-level output. Con-sidering the distributional differences and semantic mis-alignment, we propose graph-based relation modules to embed these heterogeneous features into one uniﬁed high-dimensional space. On the other hand, considering the de-ﬁciencies of cross-camera evaluations in existing measures (i.e., CMC and AP), we then propose a Cross-camera Gen-eralization Measure (CGM) to improve the evaluations by introducing position-sensitivity and cross-camera general-ization penalties. We further construct a new benchmark of existing models with our proposed CGM and experimental results reveal that our proposed HRCN model achieves new state-of-the-art in VeRi-776, VehicleID, and VERI-Wild. 1.

Introduction
Vehicle Re-identiﬁcation (Re-ID) has shown broad ap-plication prospects in urban security surveillance and intel-ligent transportation systems. Given a gallery of images, a vehicle Re-ID algorithm aims to associate images of the same vehicle identity captured by different cameras. With the proposals of large vehicle datasets [17, 19, 22, 33, 6] and deep learning approaches [39, 28, 2, 29, 7, 41], vehicle re-identiﬁcation has made signiﬁcant progresses. Although these methods achieve a performance bottleneck on exist-†Jiajian Zhao and Yifan Zhao contribute equally to this work.
∗Jia Li is the Corresponding author. URL: http://cvteam.net
Figure 1. Motivation of our two relational complements. (a) Cross-region complement: the high-level embedding is reﬁned by re-gional features to focus on discriminative regions. (b) Cross-level complement: features from multiple layers are fused by their rela-tionship to incorporate multi-level cues. ing evaluation measures, recognizing identities with large viewpoint variances, caused by the cross-camera situation, still remains a great challenge.
Existing researches mainly tackle this challenge from two aspects, i.e., data-driven and feature complement. Data-driven methods deem the cross-camera challenges as a nat-ural insufﬁciency of data distributions. It is observed that networks fail to recognize speciﬁc samples due to the lack of similar cases. Thus tens of recent researches tend to syn-thesize more examples using 3D-based models [29, 36] or adversarial learning [42, 23, 1, 4]. In [36], VehicleX dataset is composed of synthetic vehicles rendered by the Unity 3D engine. Lou et al. [23] propose to generate hard negative and multi-view samples as a training data supplement. Due to their instability in synthesizing unrealistic samples, meth-ods of this category do not explicitly regularize the feature
representations for cross-camera generalization.
Methods of the second category propose to utilize dis-criminative regional features as a complement to the global backbone features. Recent researches for localizing these regional features usually resort to additional annotations, in-cluding keypoint localization [31, 44], bounding boxes [7, 37] and part segmentation [25, 20]. For example, He et al. [7] introduce part-regularized local features as a com-plement to the global representation. Methods of this cate-gory show two major beneﬁts: 1) strengthening discrimina-tive regions for distinguishing subtle differences, 2) align-ing parts of cross-view samples for the same identity. How-ever, these methods heavily rely on exhaustive and accurate part annotations. Inaccurate localized part features would lead to a severe misalignment for feature embedding.
Toward these problems, in this paper, we investigate two essential cues for constructing robust complementary features.
Inspired by aforementioned part-guided meth-ods [31, 7, 25], we propose to learn the regional com-plementary embedding without any annotations, which ex-tracted vehicle parts in a circular manner (Fig. 1 (a)). We argue that concentrating on apriori vehicle parts is bene-ﬁcial for various viewpoint transformations and makes the high-level feature be aware of discriminative regions, which is desired for ﬁne-grained identiﬁcation. On the other hand, high-level features in Re-ID usually focus on limited local regions or background noises. Compared with high-level features, though low-level features lack the ability to high-light crucial regions, they contain abundant semantic infor-mation in a whole vehicle. To this end, in Fig. 1 (b), we in-corporate cross-level features from different network stages as a complement for ﬁnal embeddings.
Although it seems meaningful to fuse these cross-region and cross-level features, simple aggregation strategies (e.g., concatenation and summation) on these heterogeneous fea-tures would lead to a severe misalignment, owing to the var-ious semantics and distributions of different features. More-over, unlike the object detection or segmentation tasks, fea-tures from different levels or even regions do not play a static role (e.g., providing clear boundaries) in recognition tasks. The same feature of different object identities shows its characteristics when constructing the ﬁnal representa-tions. Keeping this in mind, we propose a novel Hetero-geneous Relation Complement Network (HRCN) to con-struct dynamic relations for fusing cross-level and cross-region features. To learn these two relationships, we pro-pose a graph-based relation module to learn dynamic pro-jections into a new feature embedding. In the cross-level complementary branch, we construct a hierarchically dy-namic fusion relationship from lower to higher levels, en-couraging the semantic complement for high-level ones. In the regional complementary branch, beyond the projects of part priors, we make a conjunction of the cross-level fea-ture with the regional part features, forming a joint relation-aware representation for ﬁnal classiﬁcation.
Thinking from another view in the cross-camera chal-lenge, existing measures in ReID, i.e., CMC and AP, usually neglect the distribution of retrieved camera IDs. These mea-sures tend to present high scores when a few samples in sim-ilar viewpoints are retrieved. To solve the natural deﬁciency in existing measures, we propose a new measure called
Cross-camera Generalization Measure, namely CGM. This measure introduces two major factors into consideration: 1) position-sensitivity: penalizing the earlier mistakes with higher importance in each camera-independent query, 2) cross-camera generalization: treating the query on each camera as in individual retrieval task.
Contributions of this paper are three-fold: 1) We pro-pose a novel Heterogeneous Relational Complement Net-work to fuse high-level features with heterogeneous com-plementary features, which are multi-level features and re-gional features, based on their relation into a robust repre-sentation feature. 2) We design a new measure named cross-camera generalization measure to evaluate more reasonably the cross-camera generalization capability of models. 3)
We perform extensive experiments to reveal the proposed method outperforms state-of-the-arts on VehicleID [17],
VeRi-776 [19] and VERI-Wild [22], and build a benchmark of existing models with our proposed measure, CGM. 2.