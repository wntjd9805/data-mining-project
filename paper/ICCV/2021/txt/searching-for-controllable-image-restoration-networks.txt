Abstract
We present a novel framework for controllable image restoration that can effectively restore multiple types and levels of degradation of a corrupted image. The proposed model, named TASNet, is automatically determined by our neural architecture search algorithm, which optimizes the efficiency-accuracy trade-off of the candidate model archi-tectures. Specifically, we allow TASNet to share the early layers across different restoration tasks and adaptively ad-just the remaining layers with respect to each task. The shared task-agnostic layers greatly improve the efficiency while the task-specific layers are optimized for restoration quality, and our search algorithm seeks for the best balance between the two. We also propose a new data sampling strat-egy to further improve the overall restoration performance.
As a result, TASNet achieves significantly faster GPU latency and lower FLOPs compared to the existing state-of-the-art models, while also showing visually more pleasing outputs.
The source code and pre-trained models are available at https://github.com/ghimhw/TASNet. 1.

Introduction
Restoration of real-world corrupted images is a challeng-ing problem since the types and the severity (or level) of degradation are unknown. Previous works on blind image super-resolution [4, 26] or blind deblurring [35, 14, 1] tackle this problem by learning to predict the unknown degradation kernel, and then using the predicted kernel to restore clean images. Recently, controllable image restoration has been gaining increased attention as alternative approaches. In this scenario, instead of accepting a single restored image given by the final model, users can control the output restoration to generate multiple images and choose the output image that best fits their preferences.
Early works on controllable image restoration (CIR) [15, 27, 31, 32] mostly consider a single type of degradation and modulate the levels of restoration. For instance, the de-noising model from [15] allows continuous modulation of
*Now at Google Research
†Now at Kookmin University
CResMD [16]
TASNet (Ours)
Figure 1: An example of controllable image restoration. Our model generates visually more pleasing outputs while adjusting restoration levels with 3 times faster GPU latency and 95.7% reduced FLOPs compared to CResMD [16]. denoising a Gaussian noise with σ = 15 ∼ 75. More re-cently, CResMD [16] proposed an extended framework that learns multiple types of degradation (Gaussian blur, Gaus-sian noise, and JPEG compression) jointly with a single network, so that users can interactively adjust not only the level but also the type of degradation. However, as more flexible control is enabled, two new challenges arise for the practical application of CIR models: 1) the high computation cost of generating multiple images to choose from, and 2) the difficulty of finding the true types and the levels of degra-dation, in which failing to do so may lead to significantly deteriorated outputs.
To alleviate these limitations, we present TASNet, a novel deep-learning-based CIR model that is optimized to achieve better visual quality and substantially reduced computa-tional complexity. Figure 1 demonstrates a sample result.
Our TASNet consists of two parts: task-agnostic layers and task-specific layers, where we denote “task” as a restoration problem w.r.t. a combination of degradation types and levels.
The task-agnostic part is composed of the early layers of the baseline supernetwork, where the parameters of the layers are shared across all tasks. Sharing the early layers greatly improves the efficiency of CIR model, since we do not need to re-compute the output of the shared layers each time a
(a) CResMD [16] (b) TASNet (Ours)
Figure 2: The overview of our efficient architecture for controllable image restoration. (a) CResMD [16] has a fixed network across all tasks and requires separate inference through the full model whenever the target restoration task becomes different. (b) Our task-agnostic and task-specific network (TASNet) shares the early layers to facilitate feature reuse. When we perform inference for multiple tasks, the task-agnostic part requires only a single computation, of which the output feature can be reused multiple times as the input for the task-specific network. The architecture of the task-specific network is adaptively adjusted for each given task. The width and the height of boxes represent the number of layers and channels of neural networks, respectively. In this example, two popular restoration tasks of denoising and deblurring are visualized, where different colors represent the corresponding inference path. user changes the task (the type or the level of degradation).
On the other hand, the remaining layers that consist of the task-specific parts are differently adjusted for each task. The main concept is summarized in Figure 2. However, deciding the architectural hyperparameters that balances the efficiency and the performance is still a very challenging problem.
To this end, we propose a new supernetwork-based neural architecture search (NAS) algorithm that can automatically search for the task-agnostic and task-specific architectures on the efficiency-accuracy trade-off curve. Since we need to consider a large number of tasks for continuously vary-ing levels of restoration, the search space of our algorithm should be able to represent a diverse set of architectures.
This is why our algorithm allows channel-level selection for each layer as well as layer-wise decision of whether to share its parameters or not. Specifically, the proposed NAS algo-rithm selects: 1) the number of layers to share (task-agnostic part), 2) the important channels for the shared layers, and 3) the important channels for each task-specific layer, where these task-specific channel selection is adaptive for each task.
We also formulate the overall learning objective to be dif-ferentiable for efficient end-to-end training of our searching framework, which results in the final TASNet. Moreover, we propose a new data sampling strategy to reduce the visual artifacts, which is empirically shown to be effective for cases when the task given by the user is very different from the true degradation of an input image.
Experimental results show that TASNet runs 3.7 times faster than the state-of-the-art CIR model on modern high-end GPUs with 95.7% FLOPs reduction when generating 4K images with 27 modulations. Also, the visual quality of the generated restoration using TASNet is much better than the previous approaches with significantly less artifacts.
Overall, our contributions can be summarized as follows:
• We present a novel neural network, named TASNet, for controllable image restoration (CIR) that remarkably improves the model efficiency and output image quality.
• We propose a supernetwork-based NAS algorithm that finds efficient CIR networks in a differentiable manner.
• We introduce a new data sampling strategy to improve the generated image quality in CIR problems.
• The proposed TASNet outperforms the state-of-the-art models in image quality and computation costs of
FLOPs and CPU/GPU latency. 2.