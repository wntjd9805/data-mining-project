Abstract
The crux of self-supervised video representation learning is to build general features from unlabeled videos. How-ever, most recent works have mainly focused on high-level semantics and neglected lower-level representations and their temporal relationship which are crucial for general video understanding. To address these challenges, this pa-per proposes a multi-level feature optimization framework to improve the generalization and temporal modeling abil-ity of learned video representations. Concretely, high-level features obtained from naive and prototypical contrastive learning are utilized to build distribution graphs, guid-ing the process of low-level and mid-level feature learn-ing. We also devise a simple temporal modeling module from multi-level features to enhance motion pattern learn-ing. Experiments demonstrate that multi-level feature opti-mization with the graph constraint and temporal modeling can greatly improve the representation ability in video un-derstanding. Code is available here. 1.

Introduction
Video representation learning has been a fundamental problem in computer vision to solve a series of video anal-ysis tasks, e.g., action recognition and detection [11, 72, 7, 18, 39, 79], video retrieval [40, 45], video caption [60, 48], and etc. To address this problem, some large-scale hu-man annotated datasets, e.g., Kinetics [11], ActivityNet [7],
YouTube-8M [1], are developed to facilitate video under-standing in speciﬁc downstream tasks. However, human la-beling on videos is expensive, and fully-supervised methods fail to leverage massive unlabeled video data. Therefore, it is signiﬁcant to develop unsupervised video representation learning without resorting to manual labeling. (a) (b) (c) (d)
Figure 1. Graph presentation of four conditions. The nodes de-note different samples, the edges present sample-wise relation-ships, and different colors indicate different characteristics, e.g., appearance, motion and semantic. Fig. 1(a) one hot label in In-foNCE loss, we use self-loop to present only the instance with its augmented view are regarded as positive. Fig. 1(b) instance-wise similarity distribution, measured by cosine similarity in embed-ding space1. We use arrows to show the samples with similarity above threshold. Fig. 1(c) semantic-wise distribution, we connect samples of the same category. Fig. 1(d) comprehensive distribu-tion formulated by the intersection of the former two. Note that we omit self-loops in the last three for concise presentation.
To achieve this goal, early works designed various pre-text tasks to uncover effective supervision from video se-quences
[6, 46, 33, 31, 74, 63]. Recently, contrastive learning has shown to be powerful in image representation learning [28, 47, 55, 12, 26, 77].
It encourages augmen-tation invariant representations by leveraging instance dis-crimination to attract augmented samples of the same in-stance and repel those of different instances. Later, beyond naive instance discrimination, inter-image relationships and semantic structures are proved helpful for learning high-quality representations [36, 67]. To expand this pipeline to video domain, diverse spatiotemporal augmentation tech-niques are proposed to construct contrastive pairs and en-hance motion modeling [17, 50, 64, 75, 14]. Some works used contrastive learning to form temporal cycle or make fu-ture prediction to boost dense spatiotemporal feature mod-1The instance-wise similarity distribution is asymmetric due to the nor-*Corresponding author. Email: wylin@sjtu.edu.cn malization, it is a directed graph.
eling [30, 23].
In brief, our contributions can be summarized as:
However, there are obvious limitations in these works.
Firstly, previous works only explore either instance-wise or semantic-wise distribution [17, 50, 36], lacking a compre-hensive perspective over both sides. Secondly, less effort has been placed on low-level features than high-level repre-sentations, while the former is proven critical for knowledge transfer [80]. Third, directly performing temporal augmen-tations, e.g., shufﬂe and reverse, at input level instead of feature level could impair feature learning [4].
To address these challenges, we propose a novel frame-work that explicitly optimizes features from a uniﬁed multi-level view to achieve more general representations. The representations from different levels of deep neural net-works show different generalization and abstraction proper-ties. Speciﬁcally, it is of common view that high-level fea-tures are more representative towards instances or seman-tics but less feasible towards cross-task transfer. In contrast, low-level features are transfer-friendly but lack structural information over samples, and are particularly sensitive to temporal statistics.
This consideration is particularly meaningful from dif-ferent perspectives. In a high-level sense, we optimize the deep representation from two aspects: 1) instance discrim-ination with conventional InfoNCE loss; 2) semantic struc-In this way, ture modeling with a prototypical branch. the high-level representations can procure structural rela-tionship among samples by formulating both instance- and semantic-wise relationships into distribution graphs as de-In a low-level sense, these distribution picted in Fig. 1. graphs serve as reliable cues to aggregate samples that share similar semantics and instance characteristics (e.g., appear-ance, motion) for better optimization in multiple shallower feature spaces. Through this, low-level features are im-posed with high-level relation knowledge while keeping good cross-task generalization ability.
Since low-level representation is sensitive to input tem-poral sequences, we replace the previous data-level tem-poral augmentation methods with a multi-level solution to enhance the temporal modeling of the pretrained represen-tation.
First, we apply temporal augmentation on multi-level features to construct contrastive pairs that have differ-ent motion patterns with the objective designed to distin-guish the augmented samples and original ones. Second, a retrieval task is proposed to match the features in short and long time spans based on their semantic consistency. Com-pared with previous data-level solutions, our method avoids forcing the backbone model to adapt to unnatural sequences which corrupts spatiotemporal statistics. Experimental re-sults reveal that our proposed simple temporal modeling is more general and suits different network backbones, while the conventional augmentation technique is somewhat lim-ited to two-pathway networks like SlowFast [16].
• We propose a multi-level feature optimization frame-work for unsupervised video representation learning.
Both instance- and semantic-wise knowledge learned from high-level features are leveraged to form a more reliable self-supervisory signal, which is employed to optimize low-level feature distributions thereby en-hancing transferability.
• We develop a simple but effective temporal modeling module with a multi-level augmentation scheme for more robust temporal analysis.
• Our method achieves state-of-the-art performance on two downstream tasks, action recognition and video retrieval, across two datasets, UCF-101 and HMDB-51. Ablation studies demonstrate the efﬁcacy of multi-level feature optimization as well as the new temporal modeling strategy. 2.