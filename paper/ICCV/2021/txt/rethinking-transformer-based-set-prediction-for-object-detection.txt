Abstract
DETR is a recently proposed Transformer-based method which views object detection as a set prediction prob-lem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we inves-tigate the causes of the optimization difﬁculty in the train-ing of DETR. Our examinations reveal several factors con-tributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we pro-pose two solutions, namely, TSP-FCOS (Transformer-based
Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also signiﬁcantly out-perform DETR and other baselines in terms of detection accuracy. Code is released at https://github.com/
Edward-Sun/TSP-Detection. 1.

Introduction
Object detection aims at ﬁnding all objects of interest in an image and predicting their category labels and bounding boxes, which is essentially a set prediction problem, as the ordering of the predicted objects is not required. Most of the state-of-the-art neural detectors [21, 24, 19, 25, 38, 26, 11] are developed in a detect-and-merge fashion that is, instead of directly optimizing the predicted set in an end-to-end fashion, those methods usually ﬁrst make predictions on a set of region proposals or sliding windows, and then per-form a post-processing step (e.g., “non-maximum suppres-sion” or NMS) for merging the the detection results in dif-ferent proposals or windows that might belong to the same object. As the detection model is trained agnostically with respect to the merging step, the model optimization in those object detectors is not end-to-end and arguably sub-optimal.
DEtection TRansformer (DETR) [2] is recently pro-posed as the ﬁrst fully end-to-end object detector. It uses
∗indicates equal contribution.
Transformer [32] to directly output a ﬁnal set of predictions without further post-processing. However, it takes extra-long training time to converge. For example, the popular
Faster RCNN model [26] only requires about 30 epochs to convergence, but DETR needs 500 epochs, which takes at least 10 days on 8 V100 GPUs. Such expensive train-ing cost would be practically prohibitive in large applica-tions. Therefore, in what manner should we accelerate the training process towards fast convergence for DETR-like
Transformer-based detectors is a challenging research ques-tion and is the main focus of this paper.
For analyzing the causes of DETR’s optimization difﬁ-culty we conduct extensive experiments and ﬁnd that the cross-attention module, by which the Transformer decoder obtains object information from images, is mainly respon-sible for the slow convergence.
In pursuit of faster con-vergence, we further examine an encoder-only version of
DETR by removing the cross-attention module. We ﬁnd that the encoder-only DETR yields a substantial improve-ment for the detection of small objects in particular but sub-optimal performance on large objects. In addition, our anal-ysis shows that the instability of the bipartite matching in
DETR’s Hungarian loss also contributes to the slow conver-gence.
Based on the above analysis we propose two mod-els for signiﬁcantly accelerating the training process of
Transformer-based set prediction methods, both of which can be regarded as improved versions of encoder-only
DETR with feature pyramids [18]. Speciﬁcally, we present
TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with
RCNN), which are inspired by a classic one-stage detector
FCOS [30] (Fully Convolutional One-Stage object detector) and a classic two-stage detector Faster RCNN [26], respec-tively. A novel Feature of Interest (FoI) selection mecha-nism is developed in TSP-FCOS to help Transformer en-coder handle multi-level features. To resolve the instability of the bipartite matching in the Hungarian loss, we also de-sign a new bipartite matching scheme for each of our two models for accelerating the convergence in training. In our evaluation on the COCO 2017 detection benchmark [20]
the proposed methods not only converge much faster than the original DETR, but also signiﬁcantly outperform DETR and other baselines in terms of detection accuracy. 2.