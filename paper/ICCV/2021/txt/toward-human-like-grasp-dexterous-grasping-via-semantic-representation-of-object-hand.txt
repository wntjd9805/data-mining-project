Abstract
In recent years, many dexterous robotic hands have been designed to assist or replace human hands in executing var-ious tasks. But how to teach them to perform dexterous op-erations like human hands is still a challenging task.
In this paper, we propose a grasp synthesis framework to make robots grasp and manipulate objects like human beings.
We first build a dataset by accurately segmenting the func-tional areas of the object and annotating semantic touch code for each functional area to guide the dexterous hand to complete the functional grasp and post-grasp manipu-lation. This dataset contains 18 categories of 129 objects selected from four datasets, and 15 people participated in data annotation. Then we carefully design four loss func-tions to constrain the model, which successfully generates the functional grasp of dexterous hand under the guidance of semantic touch code. The thorough experiments in syn-thetic data show our model can robustly generate functional grasp, even for objects that the model has not see before. 1.

Introduction
We have been pursuing to make robots grasp and ma-nipulate objects like human beings, so as to help us com-plete various tasks. Although there are already some dex-terous manipulators like human hands, it is very challeng-ing to control them to operate like a human. Generally, the methods of dexterous grasp synthesis are mainly divided into hand-centered methods and object-centered methods.
Previous work in hand-centered methods has focused on recording grasping activity in the form of hand joint con-figuration or grasp types by landmark tracking [19], model based or data-driven based hand pose estimation [6, 18].
However, due to the high degree of freedom, self-occlusion and self-similarity among fingers, it’s challenging to accu-rately annotate the coordinates or angles of each hand joint.
Recently, researchers begin to shift their focus to the object-centered grasp synthesis. Their methods pay more attention to the pose [10, 34], shape [28, 39, 42, 43, 46–48, 50, 51, 59] and function [5, 16, 17, 23, 36, 44, 45, 60] of the object for grasp and manipulation, observing where contact between the object and the human hand. GraspIt! [37] is used to predict feasible grasps for the object geometry with force closure condition, but it cannot guarantee to gener-ate anthropomorphic grasps. In order to predict human-like grasps, it should be aware of where objects can be grasped by a dexterous hand. ContactDB [4], as the first large-scale dataset that records detailed contact maps for human grasps of household objects, synthesizes the grasp from the contact map and does not need manual specification of per-finger
contact point. However, since each part of the hand does not correspond to the specific part of the contact map, multiple hand configurations may sometimes result in the same con-tact pattern in this method. UniGrasp [52] considers both the object geometry and gripper attributes as inputs to select a set of contact points on the surface of the object such that these contact points satisfy the force closure condition and are reachable by the gripper without collisions, but it’s hard to precisely make contact with these points under noise.
In contrast, our approach uses the real functional area of the object to guide the generation of functional grasp, avoid-ing the “multiple to one” issue caused by the contact map and the difficulty of predicting contact points. Specifically, we segment the functional parts of the object for grasping, and record touch code to describe whether fingers or palm of the hand contact each part of the object.
In this case, the positions of the hand are optimized to touch the object 3D surface. Furthermore, the proposed high-level semantic touch code does not require low-level annotation of joint co-ordinates or angles of the hand which greatly simplifies the hand pose annotation in the grasping tasks. And most im-portantly, the touch code also includes a grasp functional in-tent that guides the fingers towards the task oriented human-like grasp. Given both the segmented functional parts on the object surface and the semantic touch code as input, we train a deep learning model to seek a human-like grasp that gracefully fit the functional parts and the specification of the touch code. The proposed dataset and grasp synthesis results are illustrated in Fig. 1. Moreover, we also train a semantic segmentation network to predict the functional ar-eas and semantic code of objects. At the same time, we connect the segmentation network with the aforementioned grasp synthesis network, and realized the functional grasp synthesis based on the object point cloud on the test set.
Code and data will be provided here. The contributions of this paper can be summarized as follows.
• We propose a grasp synthesis framework that consid-ers both the object functional parts and touch code of the dexterous manipulators for each part of the object.
This joint representation of object-hand interaction is compact and effective which leads to successful task-oriented grasps like human hands.
• We create a dataset to train our network. This dataset contains 129 objects in 18 categories selected from 4 data sets. For each object, we segment the functional areas of the object, and annotate the touch code for each functional area to guide the dexterous hand to complete the functional grasp and post-grasp task.
• We have validated our method in synthetic data and showed that our model can also generate human-like grasps. This proves the importance of the proposed semantic object-hand representation on grasping. 2.