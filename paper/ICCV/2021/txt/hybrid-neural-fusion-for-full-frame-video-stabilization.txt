Abstract
Existing video stabilization methods often generate vis-ible distortion or require aggressive cropping of frame boundaries, resulting in smaller field of views. In this work, we present a frame synthesis algorithm to achieve full-frame video stabilization. We first estimate dense warp fields from neighboring frames and then synthesize the stabilized frame by fusing the warped contents. Our core technical novelty lies in the learning-based hybrid-space fusion that allevi-ates artifacts caused by optical flow inaccuracy and fast-moving objects. We validate the effectiveness of our method on the NUS, selfie, and DeepStab video datasets. Extensive experiment results demonstrate the merits of our approach over prior video stabilization methods. 1.

Introduction
Video stabilization has become increasingly important with the rapid growth of video content on the Internet platforms, such as YouTube, Vimeo, and Instagram. Ca-sually captured cellphone videos without a professional video stabilizer are often shaky and unpleasant to watch.
These videos pose significant challenges for video stabiliza-tion algorithms. For example, videos are often noisy due to small image sensors, particularly in low-light environ-ments. Handheld captured videos may contain large cam-era shake/jitter, resulting in severe motion blur and wobble artifacts from a rolling shutter camera.
Existing video stabilization methods usually consist of three main components: 1) motion estimation, 2) motion smoothing and 3) stable frame generation. First, the motion estimation step involves estimating motion through 2D fea-ture detection/tracking [28, 33, 15, 62], dense flow [69, 70], or recovering camera motion and scene structures [32, 73, 5, 52, 34]. Second, the motion smoothing step then re-moves the high-frequency jittering in the estimated motion and predicts the spatial transformations to stabilize each frame in the form of homography [38], mixture of homog-raphy [35, 17], or per-pixel warp fields [36, 69, 70]. Third, the stable frame generation step uses the predicted spatial 1
(a) Adobe Premiere Pro 2020 warp stabilizer (b) Yu and Ramamoorthi [70] (c) DIFRINT [10]
Figure 2: Limitations of current state-of-the-art video stabilization techniques. (a) Current commercial video stabiliza-tion software (Adobe Premiere Pro 2020) fails to generate smooth videos in challenging scenarios of rapid camera shakes. (b)
Yu and Ramamoorthi’s method [70] produces a temporally smooth video. However, the warped (stabilized) video contains many missing pixels at frame borders and inevitably requires applying aggressive cropping (green checkerboard areas) to generate a rectangular video. (c) The DIFRINT method [10] achieves full-frame video stabilization by iteratively applying frame interpolation to generate in-between, stabilized frames. However, interpolating between frames with large camera motion and moving occlusion is challenging. Their results are thus prone to severe artifacts. transform to synthesize the stabilized video. The stabi-lized frames, however, often contain large missing regions at frame borders, particularly when videos with large cam-era motion. This forces existing methods to apply aggres-sive cropping for maintaining a rectangular frame and there-fore leads to a significantly zoomed-in video with resolution loss (Figure 2(a) and (b)).
Full-frame video stabilization methods aim to address the above-discussed limitation and produce stabilized video with the same field of view (FoV). One approach for full-frame video stabilization is to first compute the stabilized video (with missing pixels at the frame borders) and then apply flow-based video completion methods [38, 21, 12] to fill in missing contents. Such two-stage methods may suffer from the inaccuracies in flow estimation and inpaint-ing (e.g., in poorly textured regions, fluid motion, and mo-tion blur). A recent learning-based method, DIFRINT [10], instead uses iterative frame interpolation to stabilize the video while maintaining the original FoV. However, apply-ing frame interpolation repeatedly leads to severe distortion and blur artifacts in challenging cases (Figure 2(c)).
In this paper, we present a new algorithm that takes a shaky video and the estimated smooth motion fields for sta-bilization as inputs and produces a full-frame stable video.
The core idea of our method lies in fusing information from multiple neighboring frames in a robust manner. Instead of using color frames directly, we use a learned CNN represen-tation to encode rich local appearance for each frame, fuse multiple aligned feature maps, and use a neural decoder net-work to render the final color frame. We first explore multi-ple design choices for fusing and blending multiple aligned frames. We then propose a hybrid fusion mechanism that leverages both feature-level and image-level fusion to al-leviate the sensitivity to flow inaccuracy. We further im-prove the visual quality of the synthesized results by learn-ing to predict spatially varying blending weights, removing blurry input frames for sharp video generation, and transfer-ring high-frequency details residual to the re-rendered, sta-bilized frames. To minimize regions where contents are un-known for all neighboring frames, we propose a path adjust-ment method for balancing the goals of smoothing camera motion and maximizing frame coverage. Our method gen-erates stabilized video with significantly fewer artifacts and distortions while retaining (or even expanding) the original
FoV (Figure 1). We evaluate the proposed algorithm with state-of-the-art methods and commercial video stabilization software (Adobe Premiere Pro 2020 warp stabilizer). Ex-tensive experiments show that our method performs favor-ably against existing methods on three public benchmark datasets [35, 68, 61]. Our main contributions are:
• We apply a neural fusion technique in the context of full-frame video stabilization to alleviate the issues of sensitivity to flow inaccuracy.
• We present a hybrid fusion method for fusing informa-tion from multiple stabilized frames at both feature-and image-level. We systematically validate various design choices through detailed ablation studies.
• We demonstrate favorable performance against repre-sentative video stabilization techniques on three public datasets. 2.