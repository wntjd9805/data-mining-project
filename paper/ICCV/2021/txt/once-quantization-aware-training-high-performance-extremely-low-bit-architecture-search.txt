Abstract
Quantization Neural Networks (QNN) have attracted a lot of attention due to their high efﬁciency. To enhance the quantization accuracy, prior works mainly focus on designing advanced quantization algorithms but still fail to achieve satisfactory results under the extremely low-bit case. In this work, we take an architecture perspective to investigate the potential of high-performance QNN. There-fore, we propose to combine Network Architecture Search methods with quantization to enjoy the merits of the two sides. However, a naive combination inevitably faces un-acceptable time consumption or unstable training problem.
To alleviate these problems, we ﬁrst propose the joint train-ing of architecture and quantization with a shared step size to acquire a large number of quantized models. Then a bit-inheritance scheme is introduced to transfer the quan-tized models to the lower bit, which further reduces the time cost and meanwhile improves the quantization accu-racy. Equipped with this overall framework, dubbed as
Once Quantization-Aware Training (OQAT), our searched model family, OQATNets, achieves a new state-of-the-art compared with various architectures under different bit-widths. In particular, OQAT-2bit-M achieves 61.6% Ima-geNet Top-1 accuracy, outperforming 2-bit counterpart Mo-bileNetV3 by a large margin of 9% with 10% less com-putation cost. A series of quantization-friendly architec-tures are identiﬁed easily and extensive analysis can be made to summarize the interaction between quantization and neural architectures. Codes and models are released at https://github.com/LaVieEnRoseSMZ/OQA 1.

Introduction
Quantization Neural Networks (QNN) is a promising re-search direction to deploy deep neural networks on edge devices. Extensive efforts have been devoted to improving quantization performance with Quantization-Aware Train-(a) 3-bit. (b) 2-bit.
Figure 1. Comparison with the state-of-the-art extremely low-bit neural networks on the ImageNet dataset. Our OQATNets achieve a new state-of-the-art under various bit-widths. ing [4, 41, 10, 20, 9, 2, 15] or Post-Training Quantiza-tion [28, 21]. Recent studies [27, 26] investigate the quanti-zation impact on architecture and thus take the architecture perspective to pursue high-performance quantized models with expert efforts and manual design. Compared with the laborious manual trials, the combination of Network Archi-tecture Search (NAS) and quantization seems to be a more natural solution.
The existing combination of NAS and quantization methods could either be classiﬁed as NAS-then-Quantize or
Quantization-aware NAS as shown in Figure 2. NAS-then-Quantize (Figure 2(a)) usually results in sub-optimal perfor-mance because the ranking order of full precision networks is not identical to that of quantized networks. Thus, this tra-ditional routine may fail to get a good quantized model. Di-rectly searching with quantized models’ performance (Fig-ure 2(b)) seems to be an alternative. However, due to the instability brought by quantization-aware training, simply combining quantization and NAS results in inferior per-formance and sub-optimal quantized models as explained in [3]. Moreover, when quantized into 2-bit, the traditional training process is highly unstable and introduces very large accuracy degradation.
Furthermore, all the existing methods [34, 31, 3, 11, 35] 1
(cid:14)(cid:27)(cid:28)(cid:27)(cid:33)(cid:26) (cid:11)(cid:31)(cid:34)(cid:23)(cid:38)(cid:30)(cid:33)(cid:28)(cid:1) (cid:18)(cid:34)(cid:30)(cid:33)(cid:38) (cid:11)(cid:31)(cid:34)(cid:23)(cid:38)(cid:30)(cid:33)(cid:28)(cid:4)(cid:18)(cid:34)(cid:30)(cid:33)(cid:38) (cid:21)(cid:27)(cid:23)(cid:36)(cid:25)(cid:29)(cid:1)(cid:21)(cid:35)(cid:23)(cid:25)(cid:27) (cid:5)(cid:6) (cid:7)(cid:30)(cid:38)(cid:37) (cid:5)(cid:6)(cid:1)(cid:24)(cid:30)(cid:38)(cid:37)(cid:1) (cid:21)(cid:27)(cid:23)(cid:36)(cid:25)(cid:29)(cid:1)(cid:21)(cid:35)(cid:23)(cid:25)(cid:27) (cid:16)(cid:6)(cid:21) (cid:16)(cid:6)(cid:21)(cid:1)(cid:15)(cid:27)(cid:38)(cid:29)(cid:34)(cid:26) (cid:5)(cid:6) (cid:20)(cid:27)(cid:38)(cid:36)(cid:23)(cid:30)(cid:33) (cid:20)(cid:27)(cid:38)(cid:36)(cid:23)(cid:30)(cid:33)(cid:1)(cid:32)(cid:34)(cid:26)(cid:27)(cid:31)(cid:1) (cid:30)(cid:33)(cid:38)(cid:34)(cid:1)(cid:5)(cid:6)(cid:1) (cid:24)(cid:30)(cid:38)(cid:37)(cid:1) (cid:5)(cid:2) (cid:20)(cid:27)(cid:38)(cid:36)(cid:23)(cid:30)(cid:33) (cid:17)(cid:33)(cid:27)(cid:1) (cid:6)(cid:36)(cid:25)(cid:29) (cid:21)(cid:29)(cid:23)(cid:36)(cid:27)(cid:26)(cid:1) (cid:37)(cid:38)(cid:27)(cid:35)(cid:1)(cid:37)(cid:30)(cid:42)(cid:27) (cid:16)(cid:34)(cid:1)(cid:20)(cid:27)(cid:38)(cid:36)(cid:23)(cid:30)(cid:33) (cid:11)(cid:31)(cid:34)(cid:23)(cid:38)(cid:30)(cid:33)(cid:28)(cid:1) (cid:18)(cid:34)(cid:30)(cid:33)(cid:38) (cid:16)(cid:6)(cid:21) (cid:5)(cid:3) (cid:20)(cid:27)(cid:38)(cid:36)(cid:23)(cid:30)(cid:33) (cid:5)(cid:2) (cid:7)(cid:30)(cid:38)(cid:37) (cid:16)(cid:6)(cid:21) (cid:2)(cid:4)(cid:8)(cid:11) (cid:6)(cid:36)(cid:25)(cid:29)(cid:37) (cid:5) (cid:5) (cid:5) (cid:7)(cid:30)(cid:38)(cid:1)(cid:12)(cid:33)(cid:29)(cid:27)(cid:36)(cid:30)(cid:38)(cid:23)(cid:33)(cid:25)(cid:27) (cid:2)(cid:23)(cid:3)(cid:5)(cid:1)(cid:16)(cid:6)(cid:21)(cid:4)(cid:38)(cid:29)(cid:27)(cid:33)(cid:4)(cid:19)(cid:39)(cid:23)(cid:33)(cid:38)(cid:30)(cid:42)(cid:27) (cid:5)(cid:3) (cid:7)(cid:30)(cid:38)(cid:37) (cid:16)(cid:6)(cid:21) (cid:2)(cid:4)(cid:8)(cid:11) (cid:6)(cid:36)(cid:25)(cid:29)(cid:37) (cid:5)(cid:2) (cid:7)(cid:30)(cid:38)(cid:37) (cid:16)(cid:6)(cid:21) (cid:5)(cid:2) (cid:20)(cid:27)(cid:38)(cid:36)(cid:23)(cid:30)(cid:33) (cid:17)(cid:33)(cid:27)(cid:1) (cid:6)(cid:36)(cid:25)(cid:29) (cid:7)(cid:30)(cid:38)(cid:1)(cid:12)(cid:33)(cid:29)(cid:27)(cid:36)(cid:30)(cid:38)(cid:23)(cid:33)(cid:25)(cid:27) (cid:1)(cid:8)(cid:4)(cid:11)(cid:10)(cid:7)(cid:10) (cid:3)(cid:9)(cid:4)(cid:5)(cid:6) (cid:7)(cid:30)(cid:38)(cid:1)(cid:40)(cid:30)(cid:26)(cid:38)(cid:29) (cid:12)(cid:33)(cid:35)(cid:39)(cid:38)(cid:1)(cid:36)(cid:27)(cid:37)(cid:34)(cid:31)(cid:39)(cid:38)(cid:30)(cid:34)(cid:33) (cid:9)(cid:27)(cid:35)(cid:38)(cid:29) (cid:13)(cid:27)(cid:36)(cid:33)(cid:27)(cid:31)(cid:1)(cid:37)(cid:30)(cid:42)(cid:27) (cid:8)(cid:29)(cid:23)(cid:33)(cid:27)(cid:31)(cid:1)(cid:40)(cid:30)(cid:26)(cid:38)(cid:29) (cid:5)(cid:3) (cid:7)(cid:30)(cid:38)(cid:37) (cid:16)(cid:6)(cid:21) (cid:5)(cid:3) (cid:20)(cid:27)(cid:38)(cid:36)(cid:23)(cid:30)(cid:33) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5)(cid:4) (cid:7)(cid:30)(cid:38)(cid:37) (cid:16)(cid:6)(cid:21) (cid:2)(cid:4)(cid:8)(cid:11) (cid:6)(cid:36)(cid:25)(cid:29)(cid:37) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:5) (cid:2)(cid:24)(cid:3)(cid:5)(cid:1)(cid:10)(cid:41)(cid:30)(cid:37)(cid:38)(cid:30)(cid:33)(cid:28)(cid:1)(cid:19)(cid:39)(cid:23)(cid:33)(cid:38)(cid:30)(cid:42)(cid:23)(cid:38)(cid:30)(cid:34)(cid:33)(cid:4)(cid:6)(cid:40)(cid:23)(cid:36)(cid:27)(cid:1)(cid:16)(cid:6)(cid:21) (cid:2)(cid:25)(cid:3)(cid:5)(cid:1)(cid:17)(cid:33)(cid:25)(cid:27)(cid:1)(cid:19)(cid:39)(cid:23)(cid:33)(cid:38)(cid:30)(cid:42)(cid:23)(cid:38)(cid:30)(cid:34)(cid:33)(cid:4)(cid:6)(cid:40)(cid:23)(cid:36)(cid:27)(cid:1)(cid:22)(cid:36)(cid:23)(cid:30)(cid:33)(cid:30)(cid:33)(cid:28) (cid:2)(cid:17)(cid:39)(cid:36)(cid:37)(cid:3)
Figure 2. The overall framework of existing works on combining quantization and NAS methods. (a) NAS-then-Quanztize denotes directly converting the best searched ﬂoating-point architecture to quantization. (b) Existing Quantization-aware NAS ﬁrst adopts a quantization-aware search algorithm to ﬁnd a single architecture, then retrain the quantized weights and activation. (c) Our OQAT can search for many quantized compact models under various bit widths and deploy their quantized weights directly. adopt a two-stage search-retrain scheme. Speciﬁcally, they
ﬁrst search for one architecture under the full-precision or low-bit setting and then retrain the model given the speciﬁc bit widths and architecture settings. This two-stage pro-cedure undesirably increases the search and retrain cost if we have multiple deployment constraints and hardware bit widths.
To alleviate the aforementioned problems, we present
Once Quantization-aware Training (OQAT), a framework that 1) trains quantized supernet with shared step size and deploys their quantized weights immediately without re-training, 2) progressively produces a series of quantized models under different bit-widths (e.g. 4/3/2 bit). Our ap-proach leverages the recent NAS approaches which do not require retraining [37, 5, 38, 6] and combines it with quan-tization by shared step size. The search space for compact
QNN includes kernel size, depth, width, and resolution. To provide a better initialization and transfer the knowledge of the higher bit-width to the lower bit-width, we propose a bit inheritance mechanism, which reduces the bit-width progressively to enable efﬁcient searching for QNN under different quantization bit-widths. Beneﬁting from the non-retrain property and large search space under different bit widths, we conduct an extensive investigation on the inter-action between neural architecture and model quantization, to shed light on the design principles of the quantized net-work.
Extensive experiments prove the effectiveness of our approach as shown in Figure 1. Our searched quantized model family, OQATNets, achieves state-of-the-art results
In particular, on the ImageNet dataset under 4/3/2 bit. our OQAT-2bit-M far exceeds the accuracy of 2-bit Mo-bileNetV3@LSQ [9] by a large 9% margin while using 10% less computation budget. Compared with the quantization-aware NAS method APQ [35], our OQAT-MBV2-4bit-L uses 43.7% less computation cost while maintaining the same accuracy as APQ-B.
To summarize, the contributions of our paper are three-fold:
• Our OQAT is a quantization-aware NAS framework with the support of share step size to search for quantized compact models and deploy their quantized weights without retraining.
• We present the bit inheritance mechanism to reduce the bit-width progressively so that the higher bit-width models can guide the search and training of lower bit-width models.
• We provide insights into quantization-friendly archi-tecture design. A comprehensive and systematic study reveals the favored quantization-friendly pattern under different bit widths. 2.