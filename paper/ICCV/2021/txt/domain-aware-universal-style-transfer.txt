Abstract
Style transfer aims to reproduce content images with the styles from reference images. Existing universal style trans-fer methods successfully deliver arbitrary styles to origi-nal images either in an artistic or a photo-realistic way.
However, the range of “arbitrary style” defined by exist-ing works is bounded in the particular domain due to their structural limitation. Specifically, the degrees of content preservation and stylization are established according to a predefined target domain. As a result, both photo-realistic and artistic models have difficulty in performing the desired style transfer for the other domain. To overcome this lim-itation, we propose a unified architecture, Domain-aware
Style Transfer Networks (DSTN) that transfer not only the style but also the property of domain (i.e., domainness) from
* Corresponding author a given reference image. To this end, we design a novel domainness indicator that captures the domainness value from the texture and structural features of reference images.
Moreover, we introduce a unified framework with domain-aware skip connection to adaptively transfer the stroke and palette to the input contents guided by the domainness in-dicator. Our extensive experiments validate that our model produces better qualitative results and outperforms pre-vious methods in terms of proxy metrics on both artistic and photo-realistic stylizations. All codes and pre-trained weights are available at Kibeom-Hong/Domain-Aware-Style-Transfer. 1.

Introduction
Recreating an image with the style of another image has been a long-standing research topic. As a seminal work,
Gatys et al. [5] propose the neural style transfer with deep features extracted from pre-trained networks, i.e., VGG-19
Figure 2. Conceptual differences between previous style transfer models and our model (DSTN). Artistic style transfer models (a) adopt encoder-decoder structures to transfer the global style pattern, but struggle to preserve the original contexts of input images. With the help of the skip connection, photo-realistic style transfer models (b) successfully preserve the structural information, but they lack the ability of transferring the delicate style. Different from the previous methods, our networks (c) deliver the structural information of content samples adaptively based on the domainness of a given style image. The dashed line of style transformation and semantic segmentation block indicate that they can be omitted. Note that given the content C and style S, each model outputs the final image (If inal) model [25]. After that, thanks to a series of previous stud-ies, we can meet several stylized pictures of various painting styles of famous painters (e.g., Van Gogh).
Existing universal style transfer methods show the abil-ity to deal with arbitrary reference images on either artis-tic or photo-realistic domain. On one hand, WCT [15] and
AdaIN [10] transform the features of content images to match second-order statistics of reference features. Fol-lowed by these approaches, several artistic transfer stud-ies [24, 33, 3, 28, 9] endeavor to transfer the global style pattern from reference images. Meanwhile, photo-realistic style transfer studies [19, 16, 31, 1] focus on preserving original structures while transferring target styles.
However, we observe the limitation that the meaning of
“arbitrary” is restricted in a specific domain (i.e., either artistic or photo-realistic), and it comes from fundamental structural modifications for predefined target domains, as shown in Figure 2. In specific, artistic style transfer models have difficulty in maintaining clear details in the decoder because there is no clue directly coming from the content image. As a result, structural distortions of the content im-age occur when the artistic style transfer methods confront the photo-realistic reference images (Figure 1 (b)-(f)). On the other hand, photo-realistic style transfer models heavily constrain the transformation of input references with skip connections. Thus, they lack the ability to express delicate patterns (e.g., stroke pattern) of artistic references (Figure 1 (g)-(h)). In summary, existing arbitrary style transfer mod-els generate undesired outputs when they receive samples of the other domain as references.
To overcome this limitation, we focus on capturing the domain characteristics from a given reference image and adjust the degree of the stylization and the structural preser-vation adaptively. For this purpose, we propose Domain-aware Style Transfer Networks (DSTN) which are a uni-fied architecture composed of an auto-encoder with domain-aware skip connections and the domainness indicator. First, we introduce the domain-aware skip connection to balance between content preservation and texture stylization for the domain-aware universal style transfer. Unlike the conven-tional skip connection which conveys intact structural de-tails, the proposed skip connection block adjusts the trans-mission clarity of the high-frequency component from the stylized feature maps according to the domain properties.
To obtain the domain property (i.e., domainness) from a given reference image, we design the domainness indica-tor. Our novel indicator analyzes the characteristics of the domain by utilizing both the texture and structural feature maps extracted from different levels of our encoder. In or-der to predict a continuous domain factor with the range of [0, 1], we propose to augment the intermediate space be-tween art and photo with mixed samples.
With the proposed domain-aware architecture, DSTNs deliver semantic and structural information enabling artis-tic and photo-realistic style transfer, respectively. Conse-quently, DSTNs generate impeccable stylized results for ar-bitrary style, regardless of the target domain (Figure 1 (a)).
In experiments, we train our decoder and the domain-ness indicator with Microsoft COCO [17] and WikiArt [23] datasets. Qualitatively, we show that DSTNs are capable of generating plausible stylized results on both domains. Be-sides, through quantitative proxy metrics and user study, we demonstrate that our method outperforms previous methods on both photo-realistic and artistic style transfer.
Our contributions are three-fold: 1) We propose the novel end-to-end unified architecture, domain-aware style trans-fer networks (DSTN), for multi-domain style transfer. 2)
With the proposed domainness indicator and domain-aware skip connections, we capture the domain characteristics and adaptively balance between the content preservation and the texture transformation. 3) DSTNs achieve the admirable performance given references of both photo-realistic and artistic domains, and outperform previous methods in terms of preservation and stylization.
Figure 3. Overview of the domain-aware style transfer networks (DSTN). (b) shows the process of extracting the high-frequency compo-nents and (c) depicts the detail of feature transformation. 2.