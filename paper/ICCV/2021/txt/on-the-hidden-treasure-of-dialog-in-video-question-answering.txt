Abstract
High-level understanding of stories in video such as movies and TV shows from raw data is extremely chal-lenging. Modern video question answering (VideoQA) sys-tems often use additional human-made sources like plot synopses, scripts, video descriptions or knowledge bases.
In this work, we present a new approach to understand the whole story without such external sources. The secret lies in the dialog: unlike any prior work, we treat dia-log as a noisy source to be converted into text description via dialog summarization, much like recent methods treat video. The input of each modality is encoded by transform-ers independently, and a simple fusion method combines all modalities, using soft temporal attention for localization over long inputs. Our model outperforms the state of the art on the KnowIT VQA dataset by a large margin, with-out using question-specific human annotation or human-made plot summaries.
It even outperforms human evalu-ators who have never watched any whole episode before.
Code is available at https://engindeniz.github. io/dialogsummary-videoqa 1.

Introduction
Deep learning has accelerated progress in vision and lan-guage tasks. Visual-semantic embeddings [18, 9] have al-lowed zero-shot learning, cross-modal retrieval and gener-ating new descriptions from embeddings. Image caption-ing [33] and visual question answering (VQA) [2] have demonstrated generation of realistic natural language de-scription of images and a great extent of multimodal seman-tic understanding. The extension to video captioning [19, 32] and video question answering (VideoQA) [29, 20] has enabled further progress because video requires a higher level of reasoning to understand complex events [37].
VideoQA systems typically have similar architecture fo-cusing on multimodal embeddings/description, temporal at-tention and localization, multimodal fusion and reasoning.
While it is often hard to isolate progress in individual com-ponents, there are some clear trends. For instance, custom self-attention and memory mechanisms for fusion and rea-Figure 1: In VideoQA, a question is associated with Scene
B, but it can only be answered by information from Scene
A. We generate episode dialog summaries from subtitles and give them as input to our VideoQA system, dispensing with the need for external knowledge. soning [24, 17, 7] are gradually being streamlined by using transformer architectures [30, 16, 36]; while visual embed-dings [29] are being replaced by semantic embeddings [20] and text descriptions by captioning [14, 3].
Datasets are essential for progress in the field, but of-ten introduce bias. For instance, questions from text sum-maries are less relevant to visual information [29]; super-vised temporal localization [20] biases system design to-wards two-stage localizationâ†’answering [21, 16]; fixed question structure focusing on temporal localization [20] often results in mere alignment of questions with subti-tles and matching answers with the discovered context [14], providing little progress on the main objective, which is to study the level of understanding.
Bias can be removed by removing localization supervi-sion and balancing questions over different aspects of com-prehension, for instance visual, textual, or semantic [11].
However, the requirement of external knowledge, which can be in the form of hints or even ground truth, does not leave much progress in inferring such knowledge from raw data [11]. Even weakening this requirement to plain text human-generated summaries [10], still leaves a system un-usable in the absence of such data.
In many cases, as illustrated in Figure 1, a question on some part of a story may require knowledge that can be re-covered from dialog in other parts of the story. However, despite being textual, raw dialog is often informal and repet-itive; searching over all available duration of such noisy source is error-prone and impractical. Inspired by the trend of video captioning, we go a step further and apply the same idea to dialog: We summarize raw dialog, converting it into text description for question answering.
Our finding is astounding: our dialog summary is not only a valid replacement for human-generated summary in handling questions that require knowledge on a whole story, but it outperforms them by a large margin.
Our contributions can be summarized as follows: 1. We apply dialog summarization to video question an-swering for the first time (Subsection 5.1). 2. Building on a modern VideoQA system, we convert all input sources into plain text description. 3. We introduce a weakly-supervised soft temporal atten-tion mechanism for localization (Subsection 6.2). 4. We devise a very simple multimodal fusion mechanism that has no hyperparameters (Section 7). 5. We set a new state of the art on KnowIT VQA dataset
[11] and we beat non-expert humans for the first time, working only with raw data (Section 8). 2.