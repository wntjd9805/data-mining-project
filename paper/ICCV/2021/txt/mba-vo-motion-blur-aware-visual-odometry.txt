Abstract
Motion blur is one of the major challenges remaining for visual odometry methods. In low-light conditions where longer exposure times are necessary, motion blur can ap-pear even for relatively slow camera motions. In this paper we present a novel hybrid visual odometry pipeline with di-rect approach that explicitly models and estimates the cam-eraâ€™s local trajectory within the exposure time. This al-lows us to actively compensate for any motion blur that occurs due to the camera motion.
In addition, we also contribute a novel benchmarking dataset for motion blur aware visual odometry. In experiments we show that by di-rectly modeling the image formation process, we are able to improve robustness of the visual odometry, while keep-ing comparable accuracy as that for images without mo-tion blur. Both the code and the datasets can be found from https://github.com/ethliup/MBA-VO. 1.

Introduction
Visual odometry (VO) determines the relative camera motion from captured images. As a fundamental block for many vision applications, such as robotics and vir-tual/augmented/mixed reality, great progress has been made during the last two decades. There have been many algo-rithms proposed in the literature: ranging from classical geometric approaches, deep learning based approaches to hybrid approaches. The geometric approaches recover the motion based on multi-view geometric constraints. Both the reprojection error (e.g. ORB-SLAM [23]) and the pho-tometric consistency (e.g. DSO [7]) are commonly used constraints for the optimization. Deep learning based ap-proaches formulate the problem as an end-to-end regres-sion problem. Current state-of-the-art networks are still not able to achieve comparable performance to the classical ap-proaches for large scale environments. Hybrid approaches usually embed a deep network inside a classical pipeline, to further improve their accuracy and robustness.
While many state-of-the-art algorithms have been pro-posed, motion blur is still a major challenge remaining for
Figure 1. Motion Blur Aware Visual Odometry. We propose a full pipeline for performing motion blur aware visual odometry. By explicitly modelling the image formation process during tracking, we can actively compensate for motion blur in the direct image alignment. visual odometry methods. Motion blur is one of the most common artifacts that degrade images. It usually occurs in low-light conditions where longer exposure times are neces-sary. This affects both feature based approaches (e.g. ORB-SLAM [23]), which struggle to detect keypoints, and direct methods (e.g. DSO [7]) which rely on strong image gra-dients for their alignment. While relocalization strategies can partially mitigate the problem by allowing the VO to recover after losing track, VO would still fail if the camera continues to move in un-explored areas.
In this paper, we thus propose a novel hybrid visual odometry method which is robust to motion blur. As con-ventional algorithms, our method consists of a front-end tracker and a back-end mapper. During tracking, instead of estimating the camera pose at a particular point in time, we estimate the local camera motion trajectory within the expo-sure time for each frame. This allows us to explicitly model the motion blur in the image and leverage it for tracking. We assume that the reference keyframe image is sharp, which is achieved by applying a deep deblurring network on the original motion blurred image. Since keyframes are usually sampled with a frequency much lower than frame-rate (and are less sensitive to latency), we can thus take advantage of a powerful deep network for keyframe deblurring (e.g. [32]).
To estimate the camera motion trajectory during image cap-ture, we locally re-blur the sharp reference keyframe image that is then used for direct image alignment against current tracked frame. The back-end jointly optimizes the camera poses and scene geometry based on the deblurred keyframe images, by maximizing the photometric consistency. We
build our method on the popular DSO [7] framework.
As another contribution, we also propose a novel bench-marking dataset targeting motion blur aware VO. Our dataset contains sequences with varying levels of motion blur. Time synchronized ground truth trajectories are also provided by an accurate indoor motion capturing system.
By making this dataset publicly available to the commu-nity, we hope to encourage further research on making VO robust, which is important for real-world deployments.
We evaluate our approach with both synthetic dataset and real datasets. The experimental results demonstrate that we are able to improve the robustness of the visual odometry, while keeping comparable accuracy as that for images with-out motion blur. Furthermore, our motion blur aware VO (called MBA-VO) is also able to run in real-time on a laptop with a Nvidia GeForce RTX 2080 graphic card. 2.