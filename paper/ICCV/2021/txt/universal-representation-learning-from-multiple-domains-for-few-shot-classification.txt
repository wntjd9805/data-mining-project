Abstract
In this paper, we look at the problem of few-shot image classification that aims to learn a classifier for previously un-seen classes and domains from few labeled samples. Recent methods use various adaptation strategies for aligning their visual representations to new domains or select the relevant ones from multiple domain-specific feature extractors. In this work, we present URL, which learns a single set of universal visual representations by distilling knowledge of multiple domain-specific networks after co-aligning their features with the help of adapters and centered kernel alignment.
We show that the universal representations can be further refined for previously unseen domains by an efficient adapta-tion step in a similar spirit to distance learning methods. We rigorously evaluate our model in the recent Meta-Dataset benchmark and demonstrate that it significantly outperforms the previous methods while being more efficient. 1.

Introduction
As deep neural networks progress to dramatically im-prove results in most of standard computer vision tasks, there is a growing community interest for more ambitious goals. One of them is to improve the data efficiency of the standard supervised methods that rely on large amount of ex-pensive and time-consuming hand-labeled data. Just like the human intelligence is capable of learning concepts from few labeled samples, few-shot learning [24, 33] aims at adapting a classifier to accommodate new classes not seen in training, given a few labeled samples from these classes.
Earlier works in few-shot learning focus on evaluating their methods in homogeneous learning tasks, e.g. Omin-glot [25], miniImageNet [53], tieredImageNet [43], where both the meta-train and meta-test examples are sampled from a single data distribution (or dataset). Recently, the interest of the community has shifted to a more realistic and challenging experimental setting, where the goal is to
*Xialei Liu is the corresponding author.
τ }K
Figure 1. URL – Universal Representation Learning. Unlike the previous methods [13, 29] (illustrated in (a)) that learn K feature extractors {fϕ∗
τ , one for each domain, and retrieve or combine their features for the target task during meta-test stage, our method (illustrated in (b)) learns a single universal feature extractor fϕ that is distilled from from multiple feature extractors {fϕ∗
τ . In meta-test stage, we use a linear transformation Aϑ to further refine the universal representations to unseen domains.
τ }K learn few-shot models that can generalize not only within a single data distribution but also to previously unseen data distributions. To this end, Triantafillou et al. [52] propose a new heterogeneous benchmark, Meta-Dataset that consists of ten datasets from different domains for meta-training and meta-test. While, initially two domains were kept as unseen domains, later three more unseen domains are included to meta-test the generalization ability of learned models.
While the few-shot methods [14, 48, 49, 53], which were proposed before Meta-Dataset was available, can be di-rectly applied to this new benchmark with minor modifi-cations, they fail to cope with domain gap between train and test datasets and thus obtain subpar performance on Meta-Dataset. Recently several few-shot learning methods are
proposed to address this challenge, which can be coarsely grouped into two categories, adaptation [2, 44] and feature selection based methods [13, 29]. CNAPS [44] consists of an adaptation network that modulates the parameters of both a feature extractor and classifier for new categories by encoding the data distribution of few training samples.
Simple CNAPS [2] extends CNAPS by replacing its para-metric classifier with a non-parametric classifier based on
Mahalanobis distance and shows that adapting the classifier from few samples is not necessary for good performance.
SUR [13] and URT [29] further show that adaptation for the feature extractor can also be replaced by a feature selection mechanism. In particular, both [13, 29] learn a separate deep network for each training dataset in an offline stage, employ them to extract multiple features for each image, and then select the optimal set of features either based on a similarity measure [13] or on an attention mechanism [29]. Despite their good performance, SUR and URT are computation-ally expensive and require multiple forward passes through multiple networks during inference time.
In this work, we propose an efficient and high perfor-mance few-shot method, called URL based on Universal
Representation Learning. Like [13, 29], our method builds on multi-domain representations that are learned in an of-fline stage. However, we learn a single set of universal representations (a single feature extractor) over multiple do-mains which has a fixed computational cost regardless of the number of domains at inference unlike them. Similar to the adaptation based techniques [2, 44], our method further employs a simple adaptation strategy to learn the domain specific representations from few samples (see Fig. 1).
In particular, we propose to distill knowledge from multi-ple domains to a single model, which can efficiently leverage useful information from multiple diverse domains. Learn-ing multi-domain representations is a challenging task and requires to leverage commonalities in the domains while minimizing interference (negative transfer [8, 41, 56]) be-tween them. To mitigate this, we align the intermediate representations of our multi-domain network with the ones of the domain-specific networks after carefully aligning each space by using small task-specific adapters and Centered
Kernel Alignment (CKA) [22]. Finally, inspired from the use of Mahalanobis distance in [2], we adapt the learned multi-domain features into the new task by mapping them into a task-specific space. However, unlike [2], we learn the parameters of this mapping via adaptation in a discriminative way. We rigorously evaluate our method in Meta-Dataset benchmark and show that our method outperforms the state-of-the-art methods significantly in both previously seen and unseen domain generalization. 2.