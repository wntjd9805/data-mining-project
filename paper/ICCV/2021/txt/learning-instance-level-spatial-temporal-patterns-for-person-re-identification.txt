Abstract
Person re-identiﬁcation (Re-ID) aims to match pedes-trians under dis-joint cameras. Most Re-ID methods for-mulate it as visual representation learning and image search, and its accuracy is consequently affected greatly by the search space.
Spatial-temporal information has been proven to be efﬁcient to ﬁlter irrelevant negative sam-ples and signiﬁcantly improve Re-ID accuracy. However, existing spatial-temporal person Re-ID methods are still rough and do not exploit spatial-temporal information sufﬁ-ciently. In this paper, we propose a novel Instance-level and
Spatial-Temporal Disentangled Re-ID method (InSTD), to improve Re-ID accuracy. In our proposed framework, per-sonalized information such as moving direction is explic-itly considered to further narrow down the search space.
Besides, the spatial-temporal transferring probability is disentangled from joint distribution to marginal distribu-tion, so that outliers can also be well modeled. Abun-dant experimental analyses are presented, which demon-strates the superiority and provides more insights into our method. The proposed method achieves mAP of 90.8% on Market-1501 and 89.1% on DukeMTMC-reID, improv-ing from the baseline 82.2% and 72.7%, respectively. Be-in order to provide a better benchmark for per-sides, son re-identiﬁcation, we release a cleaned data list of
DukeMTMC-reID with this paper: https://github. com/RenMin1991/cleaned-DukeMTMC-reID/ 1.

Introduction
Person re-identiﬁcation aims to retrieve pedestrians across non-overlapping camera views. Most existing person re-identiﬁcation methods focus on the visual feature repre-sentations of pedestrian images [6, 11, 14, 27, 32, 35, 38, 36, 42], such as appearance, clothes, and textures. The auxiliary information of person images is also adopted recently, such
∗This work is done when Min Ren is an intern at JD AI Research.
Figure 1. For each pair of pedestrian images, instance-level spatial and temporal constraints are provided separately by the proposed framework. Then they are adaptively combined with the visual feature similarity for matching. as parsing information [5, 13, 15, 22, 25], pose of the pedes-trians [3, 19, 20], or human body key points [33]. However, the performances of these methods are still far from the re-quirements of real-world situations. Because it is hard for visual representations to discriminate pedestrian with simi-lar appearance and clothes.
Recent methods model spatial-temporal patterns [8, 18, 21, 34] to ﬁlter out the irrelevant candidates and narrow down the search space. Speciﬁcally, these methods mainly formulate spatial-temporal pattern as a joint distribution
P (Sci,cj , T ), where Sci,cj means moving from camera i to camera j, T means time interval. It has been proven to be efﬁcient to signiﬁcantly improve re-identiﬁcation accuracy.
However, there are two problems of the existing methods.
Firstly, the existing spatial-temporal methods only consider camera-level but neglect instance-level information. The state information of each pedestrian is neglected while it is essential for spatial-temporal patterns of the person. Sec-ondly, existing methods formulate spatial-temporal patterns as a joint distribution, meaning that only those candidates
matching both spatial and temporal priors can be matched.
They are not robust to the outliers.
Firstly,
To solve these problems, we propose a novel method named Instance-level and Spatial-Temporal Disentangled
Re-ID (InSTD) to model the instance-level and spatial-temporal disentangled patterns. the traditional spatial-temporal pattern is updated to be conditional on instance-level state information. Its formulation looks like p(Sci,cj , T |P ), where P is instance-level pedestrian infor-mation. The walking direction of the pedestrian, which is the key instance-level state information, is taken into con-sideration in this paper. The walking direction of a pedes-trian is complimentary information of pedestrian detection and tracking. It is useful because it is highly correlated with spatial-temporal patterns. For example, a pedestrian, who is walking towards the west in the view of a camera, is more probable to appear in the view of the western cameras later, rather than the eastern cameras. Meanwhile, it is economi-cal because pedestrian detection and tracking are necessary steps before person re-identiﬁcation in practice.
Secondly, we disentangle the spatial-temporal pattern by constructing their marginal distribution, i.e. transmis-sion probability P (Sci,cj |P ) and time interval distribution
P (T |P ). They are modeled separately and adaptively com-bined to handle outliers. If the temporal (spatial) pattern of a pedestrian is unusual, the person may be normal in the term of spatial (temporal) patterns. The similarity metric should focus on the spatial (temporal) pattern. For exam-ple, a runner, who is moving faster than most pedestrians, is an outlier from the view of temporal pattern. But the runner can be quite normal in terms of spatial transmission perspective. It is harmful to model this runner by joint dis-tribution of spatial and temporal patterns. To this end, we propose a novel fusion approach to adaptively combine the spatial and temporal patterns. The spatial patterns and tem-poral patterns are complementary, rather than in conﬂict as existing methods, so that outliers can also be well modeled.
The contributions of this paper can be summarized as follows:
• We present a novel instance-level method to model spatial-temporal patterns for person re-identiﬁcation.
The proposed method provides personalized predic-tions by leveraging the instance-level state information of each pedestrian.
• The instance-level spatial-temporal patterns are decou-pled into transmission probabilities and time interval distributions between cameras in the proposed method.
The spatial and temporal patterns become complemen-tary rather than in conﬂict as existing methods.
• Without bells and whistles, the proposed method sur-passes the baseline model based on visual features by 16.9% on DukeMTMC-reID and 8.6% on Market-1501 in the term of mAP, and outperforms the state-of-the-art method based on spatial-temporal patterns by 4.8% on DukeMTMC-reID and 2.2% on Market-1501. 2.