Abstract eral different scene datasets.
We tackle the challenge of learning a distribution over complex, realistic, indoor scenes.
In this paper, we in-troduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radi-ance fields that can be rendered from a free moving cam-era. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D ob-servations. Recent work has shown that generative mod-els of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of sin-gle objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing mod-els lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from view-points that are significantly different from observed view-points. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across sev-1.

Introduction
Spatial understanding entails the ability to infer the ge-ometry and appearance of a scene when observed from any viewpoint or orientation given sparse or incomplete obser-vations. Although a wide range of geometry and learning-based approaches for 3D view synthesis [8, 9, 28, 29, 37, 43, 51] can interpolate between observed views of a scene, they cannot extrapolate to infer unobserved parts of the scene.
The fundamental limitation of these models is their inabil-ity to learn a prior over scenes. As a result, learning-based approaches have limited performance in the extrapolation regime, whether it be inpainting disocclusions or synthesiz-ing views beyond the boundaries of the given observations.
For example, the popular NeRF [29] approach represents a scene via its radiance field, enabling continuous view inter-polation given a densely captured scene. However, since
NeRF does not learn a scene prior, it cannot extrapolate
† Work done during an internship at Apple 1
Model
Multiple Scenes/Objects Generative
Latent
Radiance Field
Scene Level
Camera Placement
NeRF [29]
NSVF [26]
PixelNerf [49]
GRAF [41]
HoloGAN [31]
PlatonicGAN [15]
ENR [10]
GTM-SM [12]
ISS [35]
GSN (ours)
✗
✗
✓
✓
✓
✓
✓
✓
✓
✓
✗
✗
✗
✓
✓
✗
✗
✓
✗
✓
-3D
N×2D 1D 1D 1D 3D 1D 2D 2D
✓
✓
✓
✓
✗
✗
✗
✗
✗
✓
✓
✓
✓
✗
✗
✗
✗
✓
✓
✓
Sphere/Wide-baseline
Sphere/Wide-baseline
Sphere/Wide-baseline
Sphere
Sphere
Sphere
Sphere
Free moving
Free moving
Free moving
Table 1: Summary of contributions and comparison with relevant related work. (Multiple Scene/Objects): Ability to model multiple scenes/objects in the same network. (Generative) Whether the model is generative (e.g. allows for free sampling). (Latent) Latent code spatial dim. (Radiance Field) Whether the model predicts a radiance field. (Scene level) Results demonstrated in scene-level environments. (Camera Placement) What camera motion is permitted? views. On the other hand, conditional auto-encoder models for view synthesis [11, 52, 45, 10, 33, 46] are able to ex-trapolate views of simple objects from multiple viewpoints and orientations. Yet, they overfit to viewpoints seen during training (see [7] for a detailed analysis). Moreover, condi-tional auto-encoders tend to favor point estimates (e.g. the distribution mean) and produce blurry renderings when ex-trapolating far from observed views as a result.
A learned prior for scenes may be used for unconditional or conditional inference. A compelling use case for uncon-ditional inference is to generate realistic scenes and freely move through them in the absence of input observations, relying on the prior distribution over scenes (see Fig. 1 for examples of trajectories of a freely moving camera on scenes sampled from our model). Likewise, conditional in-ference lends itself to different types of problems. For in-stance, plausible scene completions may be sampled by in-verting scene observations back to the learned scene prior
[48]. A generative model for scenes would be a practical tool for tackling a wide range of problems in machine learn-ing and computer vision, including model-based reinforce-ment learning [14], SLAM [5, 6], content creation [20], and adaptation for AR/VR or immersive 3D photography.
In this paper we introduce Generative Scene Networks (GSN), a generative model of scenes that allows view syn-thesis of a freely moving camera in an open environment.
Our contributions are the following. We: (i) introduce the first generative model for unconstrained scene-level radi-ance fields; (ii) demonstrate that decomposing a latent code into a grid of locally conditioned radiance fields results in an expressive and robust scene representation, which out-performing strong baselines; (iii) infer observations from arbitrary cameras given a sparse set of observations by in-verting GSN (i.e., fill in the scene); and (iv) show that GSN can be trained on multiple scenes to learn a rich scene prior, while rendered trajectories are smooth and consistent, main-taining scene coherence. 2.