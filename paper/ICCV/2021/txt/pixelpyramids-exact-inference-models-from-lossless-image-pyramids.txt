Abstract
Autoregressive models are a class of exact inference approaches with highly ﬂexible functional forms, yielding state-of-the-art density estimates for natural images. Yet, the sequential ordering on the dimensions makes these mod-els computationally expensive and limits their applicability to low-resolution imagery. In this work, we propose Pixel-Pyramids,1 a block-autoregressive approach employing a lossless pyramid decomposition with scale-speciﬁc repre-sentations to encode the joint distribution of image pixels.
Crucially, it affords a sparser dependency structure com-pared to fully autoregressive approaches. Our PixelPyra-mids yield state-of-the-art results for density estimation on various image datasets, especially for high-resolution data. 1024, we observe that the density
For CelebA-HQ 1024
× estimates (in terms of bits/dim) are improved to 44 % of the baseline despite sampling speeds superior even to easily parallelizable ﬂow-based models.
∼ 1.

Introduction
Deep generative models have enabled signiﬁcant progress in capturing the probability density of highly struc-tured, complex data, such as of natural images [5, 16, 27, 34, 45] or raw audio [11, 21, 47]. These models ﬁnd ap-plication in a wide range of tasks including image com-pression [8, 44], denoising [2], inpainting [42, 46], and super-resolution [6, 30]. Popular generative models for images include Generative Adversarial Networks (GANs)
[16], which transform a noise distribution to the desired dis-tribution, and Variational Autoencoders (VAEs) [27], where the data is modeled in a low-dimensional latent space by maximizing a lower bound on the log-likelihood of the data
[36]. However, GANs are not designed to provide exact density estimates and VAEs only approximate the under-lying true distribution with intractable likelihoods, posing challenges in both training and inference.
In contrast, autoregressive models [14, 32, 45, 46] and normalizing ﬂows [3, 12, 13, 18, 25] are exact inference ap-proaches, which estimate the exact likelihood of the data. 1Code available at https://github.com/visinf/pixelpyramids
Figure 1. Motivation for PixelPyramids. An image from CelebA
[23] (top left) is decomposed into a Paired Pyramid [43] (top right) through an invertible, lossless mapping. Considering the mutual information between pixels as a function of pixel distance on the
CelebA dataset (bottom), there is signiﬁcant spatial dependence in the original image (blue, circles), even between distant pixels. The
ﬁne components of the Paired Pyramid at different levels (other lines) have signiﬁcantly more localized dependencies, decreasing further as the resolution increases. Our PixelPyramids exploit this for efﬁcient block-autoregressive generative image modeling.
Autoregressive models factorize the joint target distribution into a product of conditional distributions with a certain ordering over the dimensions, thereby encoding complex dependencies in the data distribution for effective density estimation. However, the sequential dependency structure makes efﬁcient parallelization difﬁcult. Normalizing ﬂows, on the other hand, map the input data to a known base dis-tribution through a series of invertible transformations, pro-viding efﬁcient sampling. However, the invertibility con-straint limits their expressiveness and their density estima-tion performance lags behind that of autoregressive models.
Recent work [31, 35, 50] aims to improve the computa-tionally expensive (autoregressive) or constrained (normal-izing ﬂows) exact inference models through multi-scale fea-ture representations of images, such as that from Wavelet
or pyramid representations. The key idea is that instead of directly modeling the complex image distribution, it is decomposed into a series of simpler pixel representations, which can be encoded with comparatively less complex functional forms of exact inference models. However, these approaches either rely on a speciﬁc design choice of pixel orderings to reduce autoregressive connections or contain higher quantization levels, which makes encoding difﬁcult.
In this work, we propose PixelPyramids – an expressive and computationally efﬁcient block-autoregressive model for the discrete joint distribution of pixels in images. We make the following contributions: (i) Our PixelPyramids leverage ideas from lossless image coding, speciﬁcally a low-entropy multi-scale representation based on so-called
Paired Pyramids [43] (cf . Fig. 1, top), to encode images in a coarse-to-ﬁne manner, where the conditional generative model at each pyramid scale is conditioned on the coarse(r) image from the previous level. The subsampled images and the scale-speciﬁc components are encoded in the same number of pixels and at the same quantization level as the original image, resulting in an efﬁcient exact inference ap-proach for density estimation.2 (ii) The conditional gener-ative model for the ﬁne component at each scale exploits a
U-Net architecture [37] to encode global context from the coarser scales. A Convolutional LSTM [40] is used to cap-ture the spatial pixel dependencies within each ﬁne com-ponent. By conditioning on the coarser scales, the spatial dependency structure of the ﬁne component at each scale is more localized (cf . Fig. 1, bottom), especially as we go to higher resolutions in the pyramid. Thus, long-range depen-dencies at each scale can be modeled with fewer autoregres-sive steps in a computationally efﬁcient setup.
O (log N ) sampling steps for an N
Furthermore, (iii) a key advantage of PixelPyramids is that the ﬁne components at different scales are conditionally independent of each other and can, therefore, be trained in parallel. This makes the model applicable for density esti-mation and synthesis of high-resolution images. PixelPyra-mids require
N im-× (N 2) age, which is signiﬁcantly more efﬁcient than the sequential steps of fully autoregressive approaches. Fi-nally, (iv) we show that our PixelPyramids yield state-of-the-art density estimates and high-quality image synthe-sis on standard image datasets including CelebA-HQ [23],
LSUN [49], and ImageNet [38], as well as on the high-resolution 1024 1024 CelebA-HQ dataset at a much lower computational cost than previous exact inference models.
O
× 2.