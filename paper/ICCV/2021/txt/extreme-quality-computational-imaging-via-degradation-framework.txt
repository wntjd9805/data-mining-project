Abstract
To meet the space limitation of optical elements, free-form surfaces or high-order aspherical lenses are adopted in mobile cameras to compress volume. However, the appli-cation of free-form surfaces also introduces the problem of image quality mutation. Existing model-based deconvolu-tion methods are inefﬁcient in dealing with the degradation that shows a wide range of spatial variants over regions.
And the deep learning techniques in low-level and physics-based vision suffer from a lack of accurate data. To address this issue, we develop a degradation framework to estimate the spatially variant point spread functions (PSFs) of mo-bile cameras. When input extreme-quality digital images, the proposed framework generates degraded images shar-ing a common domain with real-world photographs. Sup-plied with the synthetic image pairs, we design a Field-Of-View shared kernel prediction network (FOV-KPN) to per-form spatial-adaptive reconstruction on real degraded pho-tos. Extensive experiments demonstrate that the proposed approach achieves extreme-quality computational imaging and outperforms the state-of-the-art methods. Furthermore, we illustrate that our technique can be integrated into ex-isting postprocessing systems, resulting in signiﬁcantly im-proved visual quality. 1.

Introduction
Combined with powerful hardware and manual-made image signal processing (ISP) systems, mobile cameras have achieved great success. Recently, smartphones have become the primary source of photographs, and the pursuit of better imaging results has increased even more. However, to meet the space limitation of lenses, high-order aspherical lenses or free-form surfaces are applied in mobile cameras.
Most of them aim to compress the volume of wide-angle lenses [17], but in the meanwhile, they also introduce the problem of image quality mutation (as shown in Fig. 2).
Recent cameras have shifted some of these correction tasks from lens design to ISP systems to correct this abrupt (b) Degraded (a) Image captured by HUAWEI HONOR 20 (c) HUAWEI ISP (d) ZMX+ FOV-KPN (e) DF + SelfDeblur (f) DF + KPN (g) DF + FOV-KPN
Figure 1. The reconstructions of a real-world image (a) captured by HUAWEI HONOR 20. (b) is the primary degradation, (c) is the output of HUAWEI ISP. Results of the proposed FOV-KPN model trained using image pairs simulated by (d) Zemax(cid:2) (ZMX). (e), (f), and (g) are the restorations of SelfDeblur [39], KPN [31], and
FOV-KPN model (Ours), respectively, all of them trained on the data generated by our degradation framework (DF). degradation. However, different steps of the traditional ISP are independent of each other, where errors accumulate and magnify in the following steps. So for the postprocessing pipeline, how to accurately estimate the abrupt degradation and correct it are the keys to reconstruction.
One approach is to estimate the camera intrinsic degra-dation kernel through iterative optimization [33]. However, for the PSFs that show a wide range of spatial variation over regions, optimization methods have difﬁculty converg-ing. The other way is reconstructing the degraded image with deep learning method [20], which depends heavily on the accuracy of data pairs. Moreover, because the degrada-tion of diverse cameras is different, it is urgent to develop feasible and convenient approaches to generate the train-ing data designed for each camera. In [37], the researchers
mance of the proposed degradation framework and network architecture. Moreover, we prove that the proposed solu-tion can replace the traditional ISP in practical scenarios, resulting in signiﬁcantly improved image quality. 2.