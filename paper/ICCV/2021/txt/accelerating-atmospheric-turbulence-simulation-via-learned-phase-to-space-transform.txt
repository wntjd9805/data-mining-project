Abstract
Fast and accurate simulation of imaging through atmo-spheric turbulence is essential for developing turbulence mitigation algorithms. Recognizing the limitations of pre-vious approaches, we introduce a new concept known as the phase-to-space (P2S) transform to significantly speed up the simulation. P2S is built upon three ideas: (1) re-formulating the spatially varying convolution as a set of in-variant convolutions with basis functions, (2) learning the basis function via the known turbulence statistics models, (3) implementing the P2S transform via a light-weight net-work that directly converts the phase representation to spa-tial representation. The new simulator offers 300× – 1000× speed up compared to the mainstream split-step simulators while preserving the essential turbulence statistics. 1.

Introduction
Despite several decades of research, imaging through at-mospheric turbulence remains an open problem in optics and image processing. The challenge is not only in recon-structing images from a stack of distorted frames but also in a less known image formation model that can be used to formulate and evaluate image reconstruction algorithms such as deep neural networks. Simulating images distorted by atmospheric turbulence has received considerable atten-tion in the optics community [29, 3, 11, 24], but using these simulators to develop deep learning image reconstruction algorithms remains a challenge as there is no physically jus-tifiable approach to synthesize large-scale datasets at a low computational cost for training and testing.
Recognizing the demand for a fast, accurate, and open-source simulator, we present a new method to generate a dense-grid image distorted by turbulence with theoretically verifiable statistics. The simulator consists of mostly op-tics/signal processing steps and a lightweight shallow neu-ral network to perform a new concept called the Phase-to-Space (P2S) transform. By parallelizing the computation 24.36 sec / frame (GPU) (a) Hardie et al. [11] 0.026 sec / frame (GPU) (b) Ours
Figure 1. This paper presents a new turbulence simulator that is substantially (1000×) faster than the prior art, while preserving the essential turbulence statistics. (a) Input (real) (b) [17]+U-Net
Figure 2. Using our simulator to synthesize training set for train-ing an image reconstruction network (U-Net [28]) offers a consid-erable amount of improvement in image quality. The network is identical for both (b) and (c); only the simulator used to synthesize the training data is different. (c) Ours+U-Net across the pixels, the simulator offers a 1000× speed-up compared to the mainstream approach as shown in Figure 1.
When using the new simulator to synthesize training data to train a deep neural network image reconstruction model, the resulting network outperforms the same architecture trained with data synthesized by a less sophisticated simulator, as illustrated in Figure 2.
An overview of the proposed simulator is illustrated in
Figure 3. Our proposed approach is based on linking the following two ideas:
• Convolution via basis functions (Section 3.1). While conventional approaches model the turbulence distor-tion as a spatially varying convolution, we reformu-Figure 3. This paper introduces three ideas to significantly speed up the simulation. The three ideas are: (Section 3.1) Approximat-ing the spatially varying convolution by invariant convolutions, (Section 3.2) learning the basis representation via known turbu-lence statistics, (Section 3.3) implementing the Phase-to-Space transform network. late the problem by modeling the distortion as a sum of spatially invariant convolutions. The idea is to uti-lize a basis representation of the point spread functions (PSFs). This concept is similar to the prior work of
[23], but in a different context.
• Learning the basis functions (Section 3.2). To enable the previous idea, we need to have the basis functions.
This is done by utilizing [5] to draw Zernike samples for all high-order aberrations. Then, principal compo-nent analysis is used to construct the basis functions as proposed by Mao et al. [21]. This is also reminis-cent to the dictionary approach proposed by Hunt et al.
[13].
The missing piece between these two ideas is the re-lationship between the basis coefficients in the phase and spatial domains. This is an open problem, and there is no known analytic solution. We circumvent this diffi-culty by introducing a new concept known as the Phase-to-Space transform (Section 3.3). To do so, we construct a lightweight shallow neural network to transform from the phase domain to the spatial domain. Integrating this net-work into the two aforementioned ideas, our overall simu-lator adheres to the physics while offering significant speed up and additional reconstruction utility. 2.