Abstract
Transferability of adversarial examples is of central im-portance for attacking an unknown model, which facilitates adversarial attacks in more practical scenarios, e.g., black-box attacks. Existing transferable attacks tend to craft ad-versarial examples by indiscriminately distorting features to degrade prediction accuracy in a source model without aware of intrinsic features of objects in the images. We argue that such brute-force degradation would introduce model-specific local optimum into adversarial examples, thus limiting the transferability. By contrast, we propose the Feature Importance-aware Attack (FIA), which disrupts important object-aware features that dominate model de-cisions consistently. More specifically, we obtain feature importance by introducing the aggregate gradient, which averages the gradients with respect to feature maps of the source model, computed on a batch of random transforms of the original clean image. The gradients will be highly cor-related to objects of interest, and such correlation presents invariance across different models. Besides, the random transforms will preserve intrinsic features of objects and suppress model-specific information. Finally, the feature importance guides to search for adversarial examples to-wards disrupting critical features, achieving stronger trans-ferability. Extensive experimental evaluation demonstrates the effectiveness and superior performance of the proposed
FIA, i.e., improving the success rate by 9.5% against nor-mally trained models and 12.8% against defense models as compared to the state-of-the-art transferable attacks. Code is available at: https://github.com/hcguoO0/FIA 1.

Introduction
Deep neural networks (DNNs) have achieved superior performance in many vision tasks, e.g., image classifica-tion [17, 12], object detection [9, 25], semantic segmenta-*Hengchang Guo is the corresponding author.
Figure 1. Comparison of traditionally indiscriminate attacks (top row) and our feature importance-aware attacks (bottom row). Ad-versarial images are generated on the source model (VGG16) and used to attack the target model (Inception-V3). Our attacks will suppress important features and promote trivial features, thus sig-nificantly defocusing/misleading the models as visualized by the attention maps, i.e., our adversarial example causes the source and target models not only failed to capture the important object but also focus on trivial regions. tion [4, 22], face recognition [31, 27], etc. However, de-spite the impressive progress, recent studies showed that
DNNs are vulnerable to adversarial examples [30] which are crafted by adding carefully designed perturbations to fool DNNs. Adversarial attacks have raised great con-cern for DNN-based applications, especially in safety- and security-sensitive areas like autonomous driving. Mean-while, adversarial examples also play an important role in investigating the internal drawbacks of neural networks and improving their robustness.
Many works [30, 10, 18, 3, 34, 33, 35] have been pro-posed to generate adversarial examples, which can be di-vided into two categories, i.e., white-box attacks vs. black-box attacks, according to the knowledge owned by attack-ers. With the progress of adversarial attacks, the more challenging black-box attacks have attracted more atten-tion. A common type of black-box attacks [33, 14, 2] is to craft adversarial examples by estimating gradients based on queried information (e.g., probability vectors and hard la-bels), which is referred to as the query-based attack. Those
query-based attacks may be impractical in the real world since excessive queries would not be allowed. By contrast, another typical black-box attack, called transfer-based at-tack, relies on the cross-model transferability of adversar-ial examples [21] (i.e., adversarial examples crafted on one model could successfully attack other models for the same task), which is more practical and flexible.
However, adversarial examples crafted by traditional at-tacking methods (e.g., FGSM [10], BIM [18], etc.) often exhibit weak transferability due to overfitting to the source model. Therefore, some studies attempted to alleviate such overfitting by introducing extra operations during the opti-mization to improve transferability, e.g., random transfor-mation [35], translation operation [6]. Recently, [36, 7, 23] performed attacks in the intermediate layers directly to en-hance transferability. Instead of disturbing the output layer, these feature-level attacks maximize internal feature distor-tion and achieve higher transferability. However, existing methods generate adversarial examples by indiscriminately distorting features without aware of the intrinsic features of objects in the images, thus easily trapped into model-specific local optimum. Because classifiers tend to extract any available signal to maximize classification accuracy, even those imperceptible noises implied in the images [15], the model will learn extra “noisy” features together with intrinsic features of objects, while the “noisy” features are treated equally with object-related features to support the final decision, and such “noisy” features will be model-specific. Therefore, the adversarial examples crafted by ex-isting methods tend to distort such model-specific features, thus overfitting to the source model and hindering the trans-ferability of the adversarial examples.
This paper proposes a Feature Importance-aware Attack (FIA), which significantly improves the transferability of adversarial examples by disrupting the important object-aware features that are supposed to dominate the decision of different models. Against model-specific features, we in-troduce aggregate gradient, which will effectively suppress model-specific features while at the same time providing object-aware importance of the features. As illustrated in
Fig. 1, compared to traditionally indiscriminate attacks, the adversarial image from the proposed FIA significantly de-focuses the models, i.e., failed to capture the important fea-tures of the object. Meanwhile, the models are misled to fo-cus on those trivial areas. More specifically, random trans-formations (we adopt random pixel dropping) are first ap-plied to the original images. Since the transformed images will preserve the spatial structure and texture but variating non-semantic details, the features from them will be con-sistent on the object-aware features but fluctuated on non-object (i.e., model-specific “noisy”) features. With respect to these features, gradients are averaged to statistically sup-press those fluctuated model-specific features. Meanwhile, object-aware/important features are preserved to guide the generation of more transferable adversarial examples since the feature importance is highly correlated to objects of in-terest and consistent across different models.
Our main contributions are summarized as follows.
• We propose Feature Importance-aware Attack (FIA) that enhances the transferability of adversarial exam-ples by disrupting the critical object-aware features that dominate the decision of different models.
• We analyze the rationale behind the relatively low transferability of existing works, i.e., overfitting to model-specific “noisy” features, against which we in-troduce aggregate gradient to guide the generation of more transferable adversarial examples.
• Extensive experiments on diverse classification models demonstrate the superior transferability of adversarial examples generated by the proposed FIA as compared to state-of-the-art transferable attacking methods. 2.