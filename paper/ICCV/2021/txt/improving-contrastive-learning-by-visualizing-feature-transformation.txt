Abstract
Contrastive learning, which aims at minimizing the dis-tance between positive pairs while maximizing that of neg-ative ones, has been widely and successfully applied in un-supervised feature learning, where the design of positive and negative (pos/neg) pairs is one of its keys.
In this paper, we attempt to devise a feature-level data manipu-lation, differing from data augmentation, to enhance the generic contrastive self-supervised learning. To this end, we first design a visualization scheme for pos/neg score1 distribution, which enables us to analyze, interpret and un-derstand the learning process. To our knowledge, this is the first attempt of its kind. More importantly, leveraging this tool, we gain some significant observations, which in-spire our novel Feature Transformation proposals includ-ing the extrapolation of positives. This operation creates harder positives to boost the learning because hard pos-itives enable the model to be more view-invariant. Be-sides, we propose the interpolation among negatives, which provides diversified negatives and makes the model more discriminative.
It is the first attempt to deal with both challenges simultaneously. Experiment results show that our proposed Feature Transformation can improve at least 6.0% accuracy on ImageNet-100 over MoCo baseline, and about 2.0% accuracy on ImageNet-1K over the MoCoV2 baseline. Transferring to the downstream tasks success-fully demonstrate our model is less task-bias. Visualization tools and codes: https://github.com/DTennant/
CL-Visualizing-Feature-Transformation. 1.

Introduction
Finetuning from ImageNet [34] supervised pre-train net-works [16, 37, 19] for down-stream tasks, such as object detection [27, 31, 32] and semantic segmentation [28, 5], is a de facto dominant approach in computer vision com-munity. But recently self-supervised contrastive learn-*Equally-contributed and this work is done at JD AI Research.
†Corresponding author. 1Pos/neg score indicates cosine similarity of pos/neg pair. (a) Observation (b) Proposed Method (c) Performance Gain
Figure 1. The motivation of visualizing the score distribution. (a) It draws the score distribution of positive pairs for m (the momentum in MoCo[14]) being 0.99 and 0.9, showing that smaller positive scores generally need longer time to converge and obtain better accuracy. (b) Inspired by (a), we apply extrapolation on positive pairs to slightly decrease the scores, generating harder positives. (c) Leveraging the extrapolation of positives, we improve the per-formance from 71.1% (the blue) to 72.8% (the orange). The per-formance increase is consistent with the change of distribution.
The mean score of positive pairs changes from blue plot (before extrapolation) to orange plot (after extrapolation). ing achieves comparable transfer performance without the human-provided annotations. One of the key issues of con-trastive learning is to design positive and negative (pos/neg) pairs to learn an embedding space such that the positives stay closer in the space while the negatives are pushed away.
Most existing approaches [4, 6, 40, 7] acquire pos/neg pairs by data augmentation, which exploits various views of the same image to form positive pairs. For example,
CMC[39] uses the luminance and chrominance color chan-nel of an image as two views. InfoMin [40] demonstrates that incremental data augmentations indeed lead to decreas-ing mutual information between views and thus improve transfer performance.
In other words, an effective posi-tive pair prefers to convey more variance of one instance.
With a series of promotions, the contrastive learning meth-ods based on data augmentations [4, 6, 40, 7] are achieving closer to the fully supervised performance on ImageNet[6].
Most previous data augmentations (e.g., cropping, color distortion) are directly sourced from human intuitions, which may lack much interpretability, thus they can not guarantee their effectiveness. We argue, however, that the feature-level data manipulation (i.e., feature transformation) can provide more explainable or effective pos/neg pairs to
enhance the feature embedding. To this end, we first design a scheme to visualize the pos/neg pair score distributions during the training. We believe that, from these score distri-butions, we can reveal and explain how the model parame-ter values affect its performance. The visualization can help us trace back the training process. Moreover, it enables us to observe the characteristics of the pos/neg pairs, and then invent more effective feature transformations (FT).
Figure 1 demonstrates the motivation of score visualiza-tion. By plotting the score distributions under different mo-mentum values of MoCo [14], we can clearly observe that the case of m = 0.99 has smaller positive scores while achieves better performance. A small positive score indi-cates less similarity between the pair, which means this pos-itive pair actually carrying large view variance of one ex-ample. Actually, this is consistent with the goal of feature learning, which targets at a more view-invariant visual rep-resentation. Therefore, we conjecture that “hard positives” are the ones conveying large view variance of a sample. In-spired by this observation, we introduce an extrapolation operation on positive pairs to increase view variance and thus acquire hard positives. Figure 1(c) shows that the ex-trapolation of positives can boost the model performance from the “blue” one to the “orange” one.
Besides, to make full use of negative features, we pro-pose the random interpolation among negatives, which in-tuitively provides diversified negatives for each training step and makes the model more discriminative.
Unlike the traditional data augmentation, our feature transformation does not bring additional training examples.
Instead, it aims at reshaping the feature distribution by ma-nipulating both positive and negative pairs. Basically, our feature transformation will create hard positives and diver-sified negatives to learn a more view-invariant (hard posi-tive) and a more discriminative (diversified negatives) rep-resentation. It is directly driven by the performance of the learned representation, while data augmentation is kind of blind to the performance. Furthermore, our feature transfor-mation makes the model less “task-bias”, which means we can achieve performance improvement for various down-stream tasks. It has been verified by our experiments on ob-ject detection, instance segmentation, and long-tailed clas-sification with significant improvement.
Both our visualization tool and feature transformation are generic, and can be applied to various self-supervised contrastive learning including MoCo[14], SimCLR[6],
InfoMin[40], SwAv[4], SimSiam[8]. In the following sec-tions, we employ the classic model MoCo to demonstrate our framework. To summarize, our contributions include:
• We are the first to design a visualization tool to ana-lyze and interpret how the score distribution of pos/neg pairs affects the model’s capability. The visualization also helps us come into some significant observations.
• Inspired by the observations on the model visualiza-tion, we propose a simple yet effective feature trans-formation, which creates both “hard positives” and
“diversified negatives” to enhance the training. The feature transformations enable to learn more “view-invariant” and discriminative representations.
• We conduct thorough experiments and our model achieves the state-of-the-art performance. In addition, the experiments on the downstream tasks successfully demonstrate our model is less task biased. 2.