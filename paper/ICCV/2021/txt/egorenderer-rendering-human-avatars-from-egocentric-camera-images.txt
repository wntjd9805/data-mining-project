Abstract
We present EgoRenderer, a system for rendering full-body neural avatars of a person captured by a wearable, egocentric ﬁsheye camera that is mounted on a cap or a
VR headset. Our system renders photorealistic novel views of the actor and her motion from arbitrary virtual camera locations. Rendering full-body avatars from such egocen-tric images come with unique challenges due to the top-down view and large distortions. We tackle these challenges by decomposing the rendering process into several steps, including texture synthesis, pose construction, and neural image translation. For texture synthesis, we propose Ego-DPNet, a neural network that infers dense correspondences between the input ﬁsheye images and an underlying para-metric body model, and to extract textures from egocentric inputs. In addition, to encode dynamic appearances, our approach also learns an implicit texture stack that captures detailed appearance variation across poses and viewpoints.
For correct pose generation, we ﬁrst estimate body pose from the egocentric view using a parametric model. We then synthesize an external free-viewpoint pose image by project-ing the parametric model to the user-speciﬁed target view-point. We next combine the target pose image and the tex-tures into a combined feature image, which is transformed into the output color image using a neural image translation network. Experimental evaluations show that EgoRenderer is capable of generating realistic free-viewpoint avatars of a person wearing an egocentric camera. Comparisons to sev-eral baselines demonstrate the advantages of our approach. 1.

Introduction
The goal of this work is to render full-body avatars with realistic appearance and motion of a person wearing an ego-centric ﬁsheye camera from arbitrary external camera view-points (Figure 1). Such egocentric capture and rendering
∗Work partly conducted during TH’s internship at MPI-INF
Figure 1: Based on a wearable ﬁsheye camera setup (a), we propose EgoRenderer, which is trained for a single person and can produce full-body avatars of the person from new viewpoints and in new poses (c)(d) by taking as input an egocentric image (b) captured by the ﬁsheye camera. enables new applications in sport performance analysis or health care. Real-time free-viewpoint rendering of self-embodied avatars is also important in virtual reality (VR) and augmented reality (AR) applications, notably telepres-ence. A key advantage of our approach is that it uses a lightweight and compact sensor that could be mounted to glasses, headsets or caps, and that it is fully mobile. There-fore, actors can freely roam and are not limited to stay in conﬁned spaces visible to external multi-camera setups.
We approach free-viewpoint neural avatar rendering from our egocentric view by a combination of new solu-tions for egocentric pose estimation, appearance transfer, and free-viewpoint neural rendering; each of these need to be tailored to the challenging egocentric top-down ﬁsh-eye perspective with strong distortions and self-occlusions.
Most established pose estimation methods employ external outside-in camera views [27, 28, 37, 7, 9] and are not di-rectly applicable to our setting.
Some recent approaches are designed to estimate 3D skeletal pose from head-mounted ﬁsheye cameras [54, 49].
However, our setting requires a denser pixel wise estimation of egocentric pose and shape, as dense correspondences are prerequisite to transfer the texture appearance of a person from egocentric to external views (Figure 3). Similarly, re-cent neural rendering-based pose transfer methods enable creation of highly realistic animation videos of humans un-der user-speciﬁed target motion [4, 1, 19, 51]. However, all of these are tailored to external oustside-in views, such that target motions already need to be speciﬁed as skeletal pose or template mesh sequences from the extrinsic cam-era view. We face the additional challenge of transferring appearance and pose from the starkly distorted egocentric view to the external view. To enable highly realistic appear-ance and pose transfer of the actor wearing the camera to an arbitrary external view, even in more general scene condi-tions, EgoRenderer decomposes the rendering pipeline into texture synthesis, pose construction, and neural image translation, as shown in Figure 4.
Texture Synthesis. In contrast to most aforementioned image-based pose transfer methods for outside-in views [4, 38], EgoRenderer explicitly builds an estimation of surface textures of a person on top of a parametric body model.
Speciﬁcally, we extract explicit (color) textures from ego-centric images and learn implicit textures from a multi-view dataset in a training phase. We then combine them to form our full texture representation of the person. Compared to static color texture maps, the learned implicit textures bet-ter capture detailed appearance variation across poses and viewpoints. To extract (partial) textures for visible body parts from egocentric images, we create a large synthetic dataset (see Figure 2) and train an Ego-DPNet network tai-lored to our setup to infer the dense correspondences be-tween the input egocentric images and an underlying para-metric body model, as shown in Figure 3.
Pose Construction. Different from earlier neural hu-man rendering methods that expect target poses as input, irrespective of where these targets are from [38], EgoRen-derer works end-to-end. We have to exactly reproduce pose and appearance seen in an egocentric image from any ex-ternal viewpoint. We support neural rendering of the target view by projecting the 3D parametric model from egocen-tric camera space to the target viewpoint, enabling us to also transfer the partially visible texture appearance.
Neural Image Translation. Pose construction enables us to render the 3D model in the desired external view us-ing both implicit textures and color textures. We transform these images into the ﬁnal color image by means of a neu-ral image translation network. Experiments show that this implicit to explicit rendering approach outperforms direct pose-to-image translation.
Our qualitative and quantitative evaluations demonstrate that our EgoRenderer system generalizes better to novel viewpoints and poses than baseline methods on our test set.
To summarize, our contributions are as follows: 1) A large synthetic ground truth training dataset of top-down ﬁsheye images and an Ego-DPNet network tailored to our ﬁsheye camera setup to predict dense correspondence to
Figure 2: Examples from our synthetically rendered ﬁsheye training dataset (top-right: ground truth DensePose). Our dataset features a large variety of poses, human body ap-pearance, and realistic backgrounds.
Figure 3: DensePose [9] performs poorly on images cap-tured by our setup, sometimes failing to detect the human (second row). Our DensePose predictions are on the right. a parametric body model from egocentric images. 2) An end-to-end EgoRenderer system that takes single ego-centric images as input and generates full-body avatar ren-derings from external user-deﬁned viewpoints. 2.