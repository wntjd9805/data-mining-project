Abstract
High
Data format
Fixed-point
Model quantization has emerged as a mandatory tech-nique for efﬁcient inference with advanced Deep Neural
Networks (DNN) by representing model parameters with fewer bits. Nevertheless, prior model quantization ei-ther suffers from the inefﬁcient data encoding method thus leading to noncompetitive model compression rate, or re-quires time-consuming quantization aware training pro-cess. In this work, we propose a novel Adaptive Floating-Point (AFP) as a variant of standard IEEE-754 ﬂoating-point format, with ﬂexible conﬁguration of exponent and mantissa segments. Leveraging the AFP for model quanti-zation (i.e., encoding the parameter) could signiﬁcantly en-hance the model compression rate without accuracy degra-dation and model re-training. We also want to highlight that our proposed AFP could effectively eliminate the com-putationally intensive de-quantization step existing in the dynamic quantization technique adopted by the famous ma-chine learning frameworks (e.g., pytorch, tensorRT, etc.).
Moreover, we develop a framework to automatically opti-mize and choose the adequate AFP conﬁguration for each layer, thus maximizing the compression efﬁcacy. Our exper-iments indicate that AFP-encoded ResNet-50/MobileNet-v2 only has ∼0.04/0.6% accuracy degradation w.r.t its full-precision counterpart.
It outperforms the state-of-the-art works by 1.1% in accuracy using the same bit-width while reducing the energy consumption by 11.2×, which is quite impressive for inference. Code is released at: https:
//github.com/MXHX7199/ICCV_2021_AFP 1.

Introduction
The great success of deep learning depends on the avail-ability of data and the continuous growth of deep learning systems’ computing capability. However, deploying DNN
*Corresponding author: Li Jiang.
Data format
Floating-point
Data format based on FP
Our Method y c a r u c c
A
Data format
INT
INT8
LQ-Net
APT
Kmeans
ADMM
LSQ
PACT  Biscaled-FXP
Need De-quantization 
& Re-quantization
Low
High
Hardware Cost
Figure 1: Hardware cost versus prediction accuracy, using various DNN quantization methods. The blue [9] and or-anges represent quantization methods [8, 17, 4, 13, 14, 35], using ﬂoating-point format or ﬁxed-point format, though with high prediction accuracy, have relatively high compu-tation cost. In contrast, the green represents quantization method [12] using the integer data format, with relatively low accuracy but low-cost. The square indicates that the quantization method requires retraining or ﬁne-tune, while the triangle and ours are not. on edge devices, such as Internet-of-Things devices or mo-bile phones, is challenging because of the limited comput-ing and storage resources and the energy budget provided by these edge devices. These result in the large latency, which is the main concern during the inference. For instance, it takes 16 seconds on mobile to complete an image recogni-tion using VGG16 [25], which is intolerable for most appli-cations [18]. Therefore, it is essential to compress the DNN for lower storage requirements and simpler arithmetic oper-ations to improve hardware efﬁciency.
Among various compression techniques, DNN quanti-zation techniques map the continuous weights into discrete space. Thus weights can be encoded with lower bit-width
binary string, signiﬁcantly reducing the model size. Follow-ing this idea, various quantization methods [8, 4, 12, 9, 17] have been proposed. These quantization methods can be generally separated into non-uniform methods, and uniform methods—the representative prior works should be aligned with those in Fig. 1. The non-uniform quantization method, represented by the deep compression [9], uses k-means to cluster the weights, and the quantized values are denoted as indexes. The uniform quantization method, represented by INT8 [12], maps the weight values into uniformly dis-tributed integers. Meanwhile, power-of-two based quanti-zation (e.g., INQ [35], APT [17]) maps the weight values to the exponential space, then simplify the expensive multipli-cation operation into shift operation.
In addition, for low-bit quantization, retraining DNN models is needed to mitigate the introduced quantization er-ror (the square in Fig. 1). Since many users are incapable of retraining DNN due to the lack of computing-resource or retraining data, quantization without retraining becomes the most popular compression method in many real-world scenarios [20]. In most non-uniform quantization methods, the operands involved in the calculation belong to 32-bit
Floating-Point format (FP32), while uniform quantization is generally in the integer format (INT). Moreover, from the hardware deployment perspective, FP32-based quantization methods have sufﬁcient representation range and precision to make quantization errors small, but they are hardware-intensive [3] compared to INT-based quantization [10].
During inference, the main concerns include latency and energy consumption: low latency is critical for real-time in-teractions, while low energy consumption can help compa-nies reduce cost in data-centers and improve the endurance of edge devices. Dynamic quantization allows lower energy math operations on hardware platforms and faster inference with only a small drop in accuracy. The key idea of the dy-namic quantization is that we will dynamically determine the scaling factor for activation based on the range of data observed at runtime [20]. This means the scaling factor is
“tuned” to retain as much information as possible for the activations of each layer. However, most dynamic quantiza-tion methods [12, 17, 4, 33, 13, 28] have to perform the de-quantization and re-quantization process to rescale param-eters with the aim of ensuring accuracy, as TensorRT does.
This adds the complex quantizer after each layer, leading to higher energy consumption and longer latency, thereby diminishing the efﬁciency improvements of quantization.
As the trade-off of prior quantization methods in terms of data format precision (i.e., quantization accuracy) and hard-ware efﬁciency, we develop a ﬂoating-point representation variant, named Adaptive Floating-Point (AFP). In contrast to the conventional 32-bit ﬂoating-point representation with
ﬁxed data format (i.e., the sign bit, exponent bits, and man-tissa bits) and bit-widths for each segment, we inherit its data format but make the bit-width of AFP segments con-ﬁgurable. Therefore, to minimize the quantization-caused accuracy degradation and hardware-efﬁciency, the AFP rep-resentation can be optimized w.r.t speciﬁc target DNN. Our contribution in this work can be summarized as:
• We propose a novel Adaptive Floating-Point (AFP) representation whose segmentation (i.e., bit-widths of exponent and mantissa part) are fully conﬁgurable.
Such AFP format is utilized to encode (i.e., quantize)
DNN weights and activations for negligible accuracy degradation, while signiﬁcantly reducing the compu-tation cost. Meanwhile, we fuse dynamic quantization to improve performance by combining de-quantization and re-quantization with AFP format, where all of the calculations are done in AFP without having to convert to FP32, so there is less hardware overhead.
• To leverage the proposed AFP format for DNN quanti-zation, we develop a comprehensive framework to au-tomatically tune the AFP conﬁguration (i.e., bit-widths of exponent and mantissa), using Bayesian optimiza-tion as the solver. Given a DNN to be quantized, our framework can provide the optimal setting in terms of the accuracy and computation cost.
• To demonstrate the efﬁcacy of our proposed quanti-zation method and corresponding framework, we con-duct comprehensive experiments on the large-scale
ImageNet dataset, with popular MobileNet-v2 and
ResNet-50. The experiments show that our method outperforms prior ones. To be speciﬁc, the decrease in Top-1 accuracy of quantized ResNet-50 is 0.04%,
MobileNet is 0.6% on ImageNet with 5-bit on average, exceeding sta exceeding state-of-the-art accuracy. range: ~1e     to ~3e
-38
-38
FP32 sign 8-bit exponent 23-bit mantissa range: ~1e     to ~3e
-38
-38
TF32 sign 8-bit exponent range: ~5.9e     to ~6.5e
-8 4 10-bit mantissa
FP16 sign 5-bit exponent 10-bit mantissa range: ~1e     to ~3e
-38
-38
BF16 sign 8-bit exponent 7-bit mantissa range: -127 to 128 sign 7-bit integer
INT8
Figure 2: Comparison numeric format with INT8, FP32/16,
BFP16 and TF32. 2.