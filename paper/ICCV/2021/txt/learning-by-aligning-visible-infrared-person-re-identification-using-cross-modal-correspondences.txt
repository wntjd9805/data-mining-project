Abstract
We address the problem of visible-infrared person re-identification (VI-reID), that is, retrieving a set of person images, captured by visible or infrared cameras, in a cross-modal setting. Two main challenges in VI-reID are intra-class variations across person images, and cross-modal dis-crepancies between visible and infrared images. Assuming that the person images are roughly aligned, previous ap-proaches attempt to learn coarse image- or rigid part-level person representations that are discriminative and general-izable across different modalities. However, the person im-ages, typically cropped by off-the-shelf object detectors, are not necessarily well-aligned, which distract discriminative person representation learning. In this paper, we introduce a novel feature learning framework that addresses these problems in a unified way. To this end, we propose to exploit dense correspondences between cross-modal person im-ages. This allows to address the cross-modal discrepancies in a pixel-level, suppressing modality-related features from person representations more effectively. This also encour-ages pixel-wise associations between cross-modal local fea-tures, further facilitating discriminative feature learning for
VI-reID. Extensive experiments and analyses on standard
VI-reID benchmarks demonstrate the effectiveness of our approach, which significantly outperforms the state of the art. 1.

Introduction
Person re-identification (reID) aims at retrieving person images, captured across multiple cameras, with the same identity as a query person. It provides a wide range of ap-plications, including surveillance, security, and pedestrian analysis, and has gained a lot of attention over the last decade [40, 47]. Most reID methods formulate the task as a single-modality retrieval problem, and focus on finding matches, e.g., between RGB images. Visible cameras are incapable of capturing appearances of persons, particularly
*Equal contribution. †Corresponding author.
Figure 1: An example of dense cross-modal correspondences be-tween RGB and IR images from the SYSU-MM01 dataset [36].
For the visualization purpose only, we show the top 20 matches according to similarities between local person representations learned without (left) and with (right) our approach. Our person representations are robust to the cross-modal discrepancies, while being highly discriminative, especially for person regions. (Best viewed in color.) important for person reID, under poor-illumination condi-tions (e.g., at night time or dark indoors). Infrared (IR) cam-eras, on the other hand, work well, regardless of visual light, capturing an overall scene layout, while not taking scene details, such as texture and color. Accordingly, visible-IR person re-identification (VI-reID), that is, retrieving IR per-son images of the same identity as an RGB query and vice versa, has recently been of great interest [36].
VI-reID is extremely challenging due to intra-class vari-ations (e.g. viewpoint, pose, illumination and background clutter), noisy samples (e.g. misalignment and occlusion), and cross-modal discrepancies between RGB and IR im-ages. Visual attributes and statistics of RGB/IR images are significantly different from one another [36]. VI-reID meth-ods based on convolutional neural networks (CNNs) allevi-ate the discrepancies using cross-modal metric losses [7, 37, 41] along with a modality discriminator [4] to learn person representations robust to the cross-modal discrepancies, and further refine the representations with self-attention [39] or disentanglement techniques [3]. These approaches focus on learning coarse image-level or rigid part-level represen-tations, assuming that person images are roughly aligned.
Misaligned features from RGB and IR images, however, have an adverse effect on handling the cross-modal discrep-ancies, distracting learning person representations.
In this paper, we propose to leverage dense correspon-dences between cross-modal images during training for VI-reID. To this end, we encourage person representations of RGB images to reconstruct those from IR images of the same identity, which often depict different appearances due to viewpoint and pose variations, and vice versa. We achieve this by establishing dense cross-modal correspon-dences between RGB and IR person images in a probabilis-tic way. We incorporate parameter-free person masks to fo-cus on the reconstructions of person regions, while discard-ing others including background or occluded regions. We also introduce novel ID consistency and dense triplet losses using pixel-level associations, allowing the network to learn more discriminative person representations. Dense cross-modal correspondences align pixel-level person representa-tions from RGB and IR image explicitly, which is beneficial to person representation learning for VI-reID due to two main reasons. First, by enforcing semantically similar re-gions from RGB and IR images to be embedded nearby, we encourage the network to extract features invariant to the input modalities, even from misaligned RGB and IR per-son images. Second, by encouraging a local association, we enforce the network to focus on extracting discrimina-tive pixel-wise local features, which further facilitates the person representation learning. The network trained using our framework is thus able to offer local features that are robust to cross-modal discrepancies and highly discrimina-tive (Fig. 1), which are aggregated to form a final person representation for VI-reID, without any additional parame-ters at test time. Experimental results and extensive analy-ses on standard VI-reID benchmarks demonstrate the effec-tiveness and efficiency of our approach. The main contribu-tions of this paper can be summarized as follows:
• We propose a novel feature learning framework for VI-reID using dense cross-modal correspondences that al-leviates the discrepancies between multi-modal images effectively, while further enhancing the discriminative power of person representations.
• We introduce ID consistency and dense triplet losses to train our network end-to-end, which help to extract dis-criminative person representations using cross-modal correspondences.
• We achieve a new state of the art on standard VI-reID benchmarks and demonstrate the effectiveness and efficiency of our approach through extensive ex-periments with ablation studies. 2.