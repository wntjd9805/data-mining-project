Abstract
This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fun-damental components for training self-supervised ViT. We observe that instability is a major issue that degrades accu-racy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects.
We discuss the currently positive evidence as well as chal-lenges and open questions. We hope that this work will pro-vide useful data points and experience for future research. 1.

Introduction
Unsupervised pre-training has revolutionized natural language processing (NLP) [36, 15, 37, 4]. In computer vi-sion, the un-/self-supervised pre-training paradigms differ from their NLP counterparts in at least two aspects: (i) the learners in NLP are masked auto-encoders, while in vision the recently popular choices are Siamese networks (e.g.,
[20, 10, 18, 7]); (ii) the backbone architectures in NLP are self-attentional Transformers [42], while in vision the com-mon choice is convolutional [27]—yet non-attentional— deep residual networks (ResNets) [21]. To complete the big picture of self-supervised learning in vision, and towards closing the gap of pre-training methodology between vision and language, it is of scientiﬁc merit to investigate these differences.
This work focuses on training Transformers with the leading self-supervised frameworks in vision. This in-vestigation is a straightforward extension given the recent progress on Vision Transformers (ViT) [16]. In contrast to prior works [9, 16] that train self-supervised Transformers with masked auto-encoding, we study the frameworks that are based on Siamese networks, including MoCo [20] and framework linear probing: iGPT [9] iGPT [9]
MoCo v3
MoCo v3
MoCo v3
MoCo v3
MoCo v3 end-to-end ﬁne-tuning: masked patch pred. [16]
MoCo v3
MoCo v3 model params acc. (%) iGPT-L iGPT-XL
ViT-B
ViT-L
ViT-H
ViT-BN-H
ViT-BN-L/7
ViT-B
ViT-B
ViT-L 1362M 6801M 86M 304M 632M 632M 304M 86M 86M 304M 69.0 72.0 76.7 77.6 78.1 79.1 81.0 79.9† 83.2 84.1
State-of-the-art Self-supervised Transformers in
Table 1.
ImageNet classiﬁcation, evaluated by linear probing (top panel) or end-to-end ﬁne-tuning (bottom panel). Both iGPT [9] and masked patch prediction [16] belong to the masked auto-encoding paradigm. MoCo v3 is a contrastive learning method that com-pares two (224×224) crops. ViT-B, -L, -H are the Vision Trans-formers proposed in [16]. ViT-BN is modiﬁed with BatchNorm, and “/7” denotes a patch size of 7×7. †: pre-trained in JFT-300M. others [10, 18, 7].
Unlike standard convolutional networks whose training practice has been extensively studied thanks to continuous community effort, ViT models are new and their recipes are yet to be established. In this work, we go back to ba-sics and investigate the fundamental components of train-ing deep neural networks: the batch size, learning rate, and optimizer. We ﬁnd that under various cases, instability is a major issue that impacts self-supervised ViT training.
Interestingly, we observe that unstable ViT training may not result in catastrophic failure (e.g., divergence); instead, it can cause mild degradation in accuracy (e.g., 1∼3%).
Such a degree of degradation may not be noticeable, unless a more stable counterpart is available for comparison. To the best of our knowledge, this phenomena is rare in the lit-erature of training convolutional networks1, and we believe this problem and its hidden degradation are worth noticing.
To demonstrate the possible harm of instability, we investigate a simple trick that can improve stability in practice. Based on an empirical observation on gradient changes, we freeze the patch projection layer in ViT, i.e., we use ﬁxed random patch projection. We empirically show that this trick alleviates the instability issue in several sce-narios and consistently increases accuracy.
*: equal contribution. 1See also postscript on a related discussion.
We benchmark and ablate self-supervised ViT in a va-riety of cases. We provide ViT results in several self-supervised frameworks. We conduct ablations on architec-ture designs and discuss the implications. Furthermore, we explore scaling up the ViT models, including the non-trivial
ViT-Large and ViT-Huge [16] — the latter has 40× more computation than ResNet-50 [21]. Based on these experi-mental results, we discuss both the currently positive evi-dence as well as the challenges and open questions.
We report that self-supervised Transformers can achieve strong results using a contrastive learning framework, com-pared against masked auto-encoding (Table 1). This behav-ior of Transformers differs from the existing trend in NLP.
Moreover, as a promising signal, our bigger self-supervised
ViT can achieve better accuracy, unlike the ImageNet-supervised ViT in [16] whose accuracy degrades if get-ting bigger. For instance, for the very big ViT-Large, our self-supervised pre-training can outperform its supervised pre-training counterpart for transfer learning in certain cases. This presents a proof-of-concept scenario where self-supervised pre-training is needed.
In addition, we report that our self-supervised ViT models have competitive results vs. the big convolutional
ResNets in prior art [11, 18]. On one hand, this compari-son shows the potential of ViT, especially considering that it achieves these results using relatively “fewer inductive bi-ases” [16]. On the other hand, we suggest that there could be room for self-supervised ViT models to further improve.
As one example, we observe that removing the position em-bedding in ViT only degrades accuracy by a small margin.
This reveals that self-supervised ViT can learn strong rep-resentations without the positional inductive bias, but it also implies that the positional information has not been sufﬁ-ciently exploited.
In summary, we believe that the evidence, challenges, and open questions in this study are worth knowing, if self-supervised Transformers will close the gap in pre-training between vision and language. We hope our data points and experience will be useful to push this frontier. 2.