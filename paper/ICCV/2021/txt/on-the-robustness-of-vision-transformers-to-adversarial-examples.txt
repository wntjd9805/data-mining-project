Abstract
Recent advances in attention-based networks have shown that Vision Transformers can achieve state-of-the-art or near state-of-the-art results on many image classiﬁcation tasks. This puts transformers in the unique position of being a promising alternative to traditional convolutional neural networks (CNNs). While CNNs have been carefully stud-ied with respect to adversarial attacks, the same cannot be
In this paper, we study the said of Vision Transformers. robustness of Vision Transformers to adversarial examples.
Our analyses of transformer security is divided into three parts. First, we test the transformer under standard white-box and black-box attacks. Second, we study the transfer-ability of adversarial examples between CNNs and trans-formers. We show that adversarial examples do not readily transfer between CNNs and transformers. Based on this
ﬁnding, we analyze the security of a simple ensemble de-fense of CNNs and transformers. By creating a new attack, the self-attention blended gradient attack, we show that such an ensemble is not secure under a white-box adver-sary. However, under a black-box adversary, we show that an ensemble can achieve unprecedented robustness without sacriﬁcing clean accuracy. Our analysis for this work is done using six types of white-box attacks and two types of black-box attacks. Our study encompasses multiple Vision
Transformers, Big Transfer Models and CNN architectures trained on CIFAR-10, CIFAR-100 and ImageNet. 1.

Introduction
For vision tasks, convolutional neural networks (CNNs) [20] are the de facto architecture [37, 19]. On the other hand, language processing (NLP), attention-based transformers are one of the most commonly used models [35]. Based on the success of transformers in
NLP, various works have attempted to apply self-attention in natural
[4, 36]. in [12],
In particular, to image processing (both with and without CNNs) the training of tasks self-attention transformers is achieved by processing the image in patches. The training in [12] is unique in that the transformer is ﬁrst trained on the dataset ImageNet-21K (or JFT) before training on a smaller dataset, to achieve near state-of-the-art results on ImageNet, CIFAR-10 and
CIFAR-100. These types of transformers are referred to as Vision Transformers (ViT) [12].
It is important to note that the same kind of training regime can be applied to CNNs.
In [19], they also propose training on a large dataset (ImageNet-21K or JFT) and ﬁne tuning on a smaller dataset. Using this approach, CNNs are also able to achieve state-of-the-art results on ImageNet, CIFAR-10 and CIFAR-100. CNNs trained in this manner are referred to as Big Transfer Models (BiT-M) [19].
While CNNs are popular for vision tasks, they are not without deﬁciencies. It has been widely documented that
CNNs are vulnerable to adversarial examples [33, 14]. Ad-versarial examples are benign input images to which small perturbations are added. This perturbation causes the CNN to misclassify the image with high conﬁdence. Broadly speaking, an attacker creates an adversarial example using one of two threat models. Under a white-box adversary [5], the attacker has access to the CNN’s parameters (architec-ture and trained weights). The adversary can directly obtain gradient information from the model to create an adversarial example. The other type of threat is a black-box adversary.
In this scenario, the attacker does not know the CNN’s pa-rameters or architecture but can repeatedly query the CNN, or build their own synthetic CNN to estimate gradient infor-mation and generate adversarial examples.
It has also been shown that adversarial examples gener-ated using CNNs exhibit transferability [28, 21, 29]. Here, transferability refers to the fact that adversarial examples crafted to fool one CNN are often misclassiﬁed by other
CNNs as well. Overall, CNNs have an expansive body of
literature related to adversarial attacks [6, 11, 10] and de-fenses [23, 5, 34]. In contrast, Vision Transformers have not been closely studied in the adversarial context. In this work, we investigate how the advent of Vision Transformers advance the ﬁeld of adversarial machine learning. Here we speciﬁcally focus on image based adversarial attacks. Our paper is organized as follows: In Section 2 we ﬁrst discuss some related NLP work. We then break our analysis of Vi-sion Transformers into several related questions:
Do Vision Transformers provide any improvement in se-curity over CNNs under a white-box adversary? We explore this question in Section 3 by attacking Vision Transform-ers, Big Transfer Models and conventional CNNs (ResNets) with six standard white-box adversarial machine learning attacks. We show that under a white-box adversary, Vision
Transformers are just as vulnerable (insecure) as other mod-els. In Section 4, we further delve into white-box attacks and ask: How transferable are adversarial examples be-tween Vision Transformers and other models? We perform a transferability study with eight CIFAR-10 and CIFAR-100 models (this includes four Vision Transformers, two
Big Transfer Models, and two ResNets). We also study the transferability of ImageNet Vision Transformers using seven models (three Vision Transformers, two Big Trans-fer Models, and two ResNets). From our experiments we observe an interesting phenomenon. The transferability be-tween Vision Transformers and other non-transformer mod-els is unexpectedly low.
How can the transferability phenomena be leveraged to provide security? This is the topic of our ﬁnal question in
Sections 5 and 6. We further break this question down into white-box and black-box analyses. First, we consider a white-box adversary. We develop a new white-box attack called the Self-Attention blended Gradient Attack (SAGA).
Using SAGA, we show it is not possible to leverage the transferability phenomena to achieve white-box security.
However, achieving black-box security is still possible. To demonstrate this, we consider a black-box attacker that can leverage transfer style [29] and query-based attacks [8]. We show under this threat model, a simple ensemble of Vision
Transformers and Big Transfer Models can achieve an un-precedent level of robustness, without sacriﬁcing clean ac-curacy. Finally, in Section 7, we offer concluding remarks. 2.