Abstract
Example Gabor filter
Learned CNN filter
Convolutional layers in CNNs implement linear filters which decompose the input into different frequency bands.
However, most modern architectures neglect standard prin-ciples of filter design when optimizing their model choices regarding the size and shape of the convolutional kernel. In this work, we consider the well-known problem of spectral leakage caused by windowing artifacts in filtering opera-tions in the context of CNNs. We show that the small size of CNN kernels make them susceptible to spectral leakage, which may induce performance-degrading artifacts. To ad-dress this issue, we propose the use of larger kernel sizes along with the Hamming window function to alleviate leak-age in CNN architectures. We demonstrate improved clas-sification accuracy on multiple benchmark datasets includ-ing Fashion-MNIST, CIFAR-10, CIFAR-100 and ImageNet with the simple use of a standard window function in con-volutional layers. Finally, we show that CNNs employing the Hamming window display increased robustness against various adversarial attacks. Our code is available online1. 1.

Introduction
A fundamental component in deep image recognition networks is the ability to non-linearly stack learned, share-able, linear mappings. The canonical example is the lin-ear convolution operator in CNNs [15, 24, 47], while in re-cent visual Transformer models the query, key and values are token-shared linear mappings acting on pixel embed-dings [6, 8]. These linear mappings, which in the visual domain typically take the form of filters, allow image fea-ture learning. Such learnable, hierarchical, shareable, fea-ture detectors are fundamental to the great success of deep learning [1, 5, 26], and a better understanding of these filters may broadly impact the whole field.
Image filters, such as local, oriented edge detectors, provide a highly reusable decomposition of the input im-age [7, 34, 46] and are accurate models of early biological vision [20]. From a deep, hierarchical feature learning per-1https://github.com/ntomen/Windowed-Convolutions-for-CNNs
Figure 1. Windowing artifacts cause spectral leakage in the fre-quency domain, when a filter is not tapered off at the boundaries in the space domain. The top row shows example filters in space domain, while the bottom row are their corresponding frequency domains. Left: Example Gabor (bandpass) filter with severe trun-cation (kernel size 7 × 7) leads to spectral leakage in its frequency response due to sinc artifacts. The same filter with negligible trun-cation (kernel size 49×49) is a good quality bandpass filter. Right:
A standard 7x7 CNN kernel trained on CIFAR-10 struggles to learn good quality bandpass filters, as the use of small kernel sizes typically lead to severe truncation. We propose using the standard
Hamming window to taper off the kernels in space domain, which enables good quality bandpass frequency responses. spective, it is interesting to ask how specialized reusable fil-ters should be, and explore their response selectivity. In par-ticular, here we investigate the role of frequency-selectivity of learned filters in deep networks.
To investigate frequency-selectivity, we consider spec-tral leakage—which is a well-known [13, 35, 39] artifact in generic filtering operations—in the context of CNNs. The building block of CNNs, the convolution operator, can be thought of as a linear filter, which, due to the small kernel sizes employed in modern CNNs, is susceptible to spectral leakage. Although a well-studied concept in digital sig-nal [35] and image processing [11, 19], we observe that a broader understanding of spectral leakage in deep networks has largely been neglected.
Spectral leakage, in the broad sense of the term, is when an operation on a signal introduces unwanted frequency
components to the result of that operation. In practice, the term leakage is typically used when a filter lets through fre-quency components of a signal outside of its intended pass-band due to windowing artifacts. For linear filters imple-mented via discrete convolution or cross-correlation opera-tors, a kernel with finite size can be interpreted as a trun-cated version of an infinite, ideal filter. The finite size of the discrete kernel, within which the filter assumes non-zero values, represents a multiplication of an infinite kernel and a rectangular function in space domain, which translates as a convolution with a sinc function in frequency domain [39].
When a two-dimensional bandpass filter, such as a Gabor, is severely truncated, the rectangular function introduces win-dowing artifacts to the frequency response in the form of
‘ripples’ of the sinc function (Fig. 1, left).
Here we explore the effect of leakage artifacts in image classification performance in CNNs. We show that due to the typical choice of small kernel sizes, CNNs have little freedom to avoid rectangular truncation functions, which make them susceptible to spectral leakage (Fig. 1, right).
We investigate the impact of leakage artifacts on benchmark classification tasks and demonstrate that the simple use of a standard window function which reduces leakage can im-prove classification accuracy. Furthermore, we show that windowed CNNs are more robust against certain types of adversarial examples.
Our contributions can be summarized as:
• We investigate the impact of spectral leakage in CNNs.
Although spectral leakage is a fundamental concept in classical signal processing, its impact on CNN perfor-mance has not been explicitly explored before.
• We employ principles of good filter design, which are largely ignored in CNN models, to propose the use of larger kernels with the standard Hamming window function, which is tapered off at the kernel boundaries to alleviate spectral leakage.
• We demonstrate improvements to classification accu-racy in benchmark datasets including Fashion-MNIST,
CIFAR-10, CIFAR-100 and ImageNet with the simple use of a standard window function.
• We show that windowed CNNs display increased ro-bustness against certain adversarial attacks including
DeepFool and spatial transformation attacks. 2.