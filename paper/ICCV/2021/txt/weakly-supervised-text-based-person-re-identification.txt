Abstract
The conventional text-based person re-identiﬁcation methods heavily rely on identity annotations. However, this labeling process is costly and time-consuming. In this pa-per, we consider a more practical setting called weakly supervised text-based person re-identiﬁcation, where only the text-image pairs are available without the requirement of annotating identities during the training phase. To this end, we propose a Cross-Modal Mutual Training (CMMT) framework. Speciﬁcally, to alleviate the intra-class vari-ations, a clustering method is utilized to generate pseudo labels for both visual and textual instances. To further re-ﬁne the clustering results, CMMT provides a Mutual Pseudo
Label Reﬁnement module, which leverages the clustering results in one modality to reﬁne that in the other modality constrained by the text-image pairwise relationship. Mean-while, CMMT introduces a Text-IoU Guided Cross-Modal
Projection Matching loss to resolve the cross-modal match-ing ambiguity problem. A Text-IoU Guided Hard Sample
Mining method is also proposed for learning discrimina-tive textual-visual joint embeddings. We conduct exten-sive experiments to demonstrate the effectiveness of the pro-posed CMMT, and the results show that CMMT performs fa-vorably against existing text-based person re-identiﬁcation methods. Our code will be available at https:// github.com/X-BrainLab/WS_Text-ReID. 1.

Introduction
Text-based Person Re-Identiﬁcation (Re-ID) [20] is a challenging task that aims to retrieve the corresponding person images by textual descriptions.
In recent years, numerous fully-supervised textual-visual embedding meth-ods [30, 15, 25, 23, 18] have made great progress. These methods follow a similar learning scheme: 1) The identity loss is utilized to suppress the intra-class variations in each
*Corresponding author
Figure 1. Illustration of (a) fully supervised text-based person re-identiﬁcation, (b) our proposed weakly supervised text-based per-son re-identiﬁcation, (c) intra-class variation in both textual and visual modalities and (d) cross-modal matching ambiguity. modality; 2) The cross-modal matching is supervised by au-tomatically generated positive or negative labels, based on whether it originates from the same identity or not. It can be observed that they heavily rely on the identity annotations, as shown in Figure 1(a). However, the identity labeling pro-cess across multiple non-overlapping camera views is costly and time-consuming. In this work, we consider a more prac-tical setting called weakly supervised text-based person Re-ID, where only text-image pairs are available without any identity annotations in the training phase, as illustrated in
Figure 1(b).
There are two main challenges due to the absence of identity annotations. 1) It is difﬁcult to mitigate the effect caused by the intra-class variations in both textual and vi-sual modalities. As shown in Figure 1(c), descriptions of a person may be syntactically different. Meanwhile, images captured by different cameras are always visually affected
by dramatic variations with respect to illumination, human pose, view angle, background, etc. Existing clustering-based methods [10, 9, 22] can resolve this problem to some extent by unsupervised representation learning with pseudo labels in each modality. However, different from unsuper-vised person Re-ID, the relationship between textual and visual modalities can be further leveraged to reﬁne the clus-tering results. 2) As shown in Figure 1(d), it leads to a cross-modal matching ambiguity problem that, for one textual de-scription, it is unable to assign positive or negative labels to all the images, except its paired one, when learning the cross-modal matching.
To address the aforementioned problems, we propose a Cross-Modal Mutual Training (CMMT) framework to facilitate visual-textual representation learning for weakly supervised text-based person Re-ID. First, in order to re-duce the intra-class variations, a clustering method is lever-aged to generate preliminary pseudo labels for both textual and visual instances. To further improve the clustering re-sults, CMMT provides a Mutual Pseudo Label Reﬁnement (MPLR) module, which utilizes the clustering results in one modality to reﬁne that in the other modality constrained by the pairwise relationship between textual and visual modali-ties. Second, to mitigate the cross-modal matching ambigu-ity, CMMT employs a Text-IoU Guided Cross-Modal Pro-jection Matching (Text-IoU CMPM) loss, which introduces a heuristic metric termed Intersection over Union of Text (Text-IoU) to assign similarity soft-labels. Furthermore, a Text-IoU guided Hard Sample Mining (Text-IoU HSM) method is presented to learn discriminative visual-textual joint embeddings by exploring the similarity consistency of embedding features and textual phrases.
Our contributions are listed as follows: 1. To the best of our knowledge, it is the ﬁrst work that particularly addresses the problem of weakly super-vised text-based person Re-ID. 2. The Mutual Pseudo Label Reﬁnement module is pro-posed for pseudo label reﬁnement to suppress intra-class variations by leveraging the pairwise relationship between textual and visual modalities. 3. We introduce Text-IoU, which measures the similari-ties of textual descriptions in phrase-level by treating the phrases as multi-labels. Text-IoU is further utilized to prompt cross-modal matching and hard sample min-ing. 4. Extensive experiments and component studies are con-ducted to demonstrate the superiority of the proposed approach for weakly supervised text-based person Re-ID. Experiment results show that, without any identity supervision, the proposed method even outperforms the state-of-the-art fully supervised text-based person
Re-ID methods. 2.