Abstract
Few-shot learning aims to adapt knowledge learned from previous tasks to novel tasks with only a limited amount of labeled data. Research literature on few-shot learning ex-hibits great diversity, while different algorithms often ex-cel at different few-shot learning scenarios. It is therefore tricky to decide which learning strategies to use under dif-ferent task conditions. Inspired by the recent success in Au-tomated Machine Learning literature (AutoML), in this pa-per, we present Meta Navigator, a framework that attempts to solve the aforementioned limitation in few-shot learning by seeking a higher-level strategy and proffer to automate the selection from various few-shot learning designs. The goal of our work is to search for good parameter adapta-tion policies that are applied to different stages in the net-work for few-shot classification. We present a search space that covers many popular few-shot learning algorithms in the literature, and develop a differentiable searching and decoding algorithm based on meta-learning that supports gradient-based optimization. We demonstrate the effective-ness of our searching-based method on multiple benchmark datasets. Extensive experiments show that our approach significantly outperforms baselines and demonstrates per-formance advantages over many state-of-the-art methods. 1.

Introduction
Convolutional Neural Networks (CNNs) have become indispensable in a variety of computer vision tasks [27, 39–41, 55, 58, 59]. A crucial reason is that the knowledge learned by CNNs can be transferred across different vision tasks in the form of hierarchical feature representations.
Nevertheless, a sufficiently large amount of annotated data is still necessary to achieve good generalization accuracy due to CNNs’ data-hungry properties, which inevitably hin-ders the application of CNNs in real-world scenarios.
*Corresponding author: G. Lin (e-mail: gslin@ntu.edu.sg)
Figure 1 – Comparison of some popular few-shot learning algo-rithms under various few-shot learning task settings. All models are based on the same network architecture, with weights pre-trained on miniImageNet dataset. Cross-domain experiments are evaluated on the
CUB dataset. Existing few-shot learning algorithms are highly task-specific and are outperformed by a simple fine-tune baseline when the domain difference is large and more support data are available.
Few-shot learning is proposed as a promising direction to alleviate the need for exhaustively labeled data by explor-ing an extreme case where only a few labeled data is avail-able to undertake a novel task based on prior knowledge learned on previous tasks. A typical application scenario is few-shot image classification [11,37,44]. Literature on few-shot learning exhibits great diversity, while different algo-rithms often excel at different few-shot learning scenarios.
Fig. 1 compares some popular few-shot learning algorithms on different few-shot learning tasks. Here we consider three test cases, including 1) the extreme low-shot case, i.e., 1-shot; 2) a medium-shot case, where the size of support set is relatively larger than the common benchmarks, e.g., 10-shot; 3) a cross-domain case where the training and testing tasks are sampled from different domains. As is shown, ex-isting few-shot learning algorithms are highly task-specific and no single algorithm can show superiority over others across all tasks. In particular, when the domain difference is large, the compared few-shot learning algorithms can not sufficiently utilize the increasing number of support data to accommodate the domain difference, while the simple fine-tuning strategy can beat all other few-shot learning meth-ods, although it is significantly outperformed by others in the 1-shot case due to over-fitting. Therefore, it is almost impossible to find one single optimal few-shot learner that works well for all tasks. This makes many few-shot learning algorithms difficult to be applied as a general tool to solve the data scarcity issue in machine learning, even though they can perform very well on some specific benchmarks.
In recent years, there is a surging interest in automat-ing the design of machine learning algorithms (AutoML), instead of relying too much on heuristic manual designs.
In particular, the idea of AutoML has been successfully applied to Neural Architecture Search (NAS) [26, 45, 52], where the model learns to identify high-performance archi-tectures by exploring a large candidate architecture space.
With the same intuition, in this work, we attempt to solve the aforementioned limitation in few-shot learning by seek-ing a higher-level strategy and take initiatives to automate the selection of few-shot learning designs. The goal of our work is to search for good parameter adaptation policies that are applied to different stages in the network for few-shot learning. The search space in our network include two parts: the policy to adapt convolutional layers in different stages of the backbone and the policy to obtain class pro-totypes in the classifier, which together construct a hierar-chical policy search space. At each network stage, various candidates policies are available for adapting the parame-ters, and the whole search space covers many popular meta-learning algorithms in the literature, such as Prototypical
Networks [37], Matching Networks [44], baseline++ [2],
MAML [11], etc.
In order to search from a pool of discrete adaptation poli-cies, we develop a differentiable searching algorithm based on meta-learning that allows efficient gradient-based opti-mization. Inspired by the differentiable designs in NAS lit-erature [26], our searching system is built upon a continuous relaxation of the discrete meta-learning policy, where each candidate policy is associated with a learnable policy se-lection indicator. However, as each adaptation policy is an optimization process rather than a differentiable operation, directly porting the formulation in DARTS [26] would not suffice. To tackle this issue, we further associate each pol-icy with a group of policy-specific model parameters. Then, the decision of choosing the optimal policy becomes jointly learning the policy selection indicators as well as the policy parameters. The searching is conducted via a bi-level op-timization paradigm based on meta-learning. Specifically, the optimization goal in the inner loop is to adapt the pa-rameters in each candidate policy using the support data in sampled tasks, while the optimization objective in the outer loop alternates between learning the policy-specific param-eters and learning the policy selection indicators. Dur-ing searching, we progressively decode the supernet from front to back stages, with fine-tune in between, based on a perturbation-based policy selection scheme [45] that mea-sures each policy’s influence on the supernet. At the end of the training, each network stage is associated with an adap-tation policy with parameters learned.
To validate the effectiveness of our design, we conduct various experiments on multiple benchmark datasets, in-cluding the challenging cross-domain experiments. Our experiment results show that our searched-based model not only outperforms the random search baseline but also demonstrates significant performance advantages over many previous methods covered in our search space. Our main contributions are summarized as follows:
• Our work is the first attempt to search meta-learning designs for few-shot learning tasks.
• We propose a hierarchical policy search space that cov-ers many previous meta-learning algorithms.
• We develop a differentiable meta-learning policy searching algorithm that can conduct policy searching efficiently by meta-learning.
• Experiments on five popular datasets show that our method significantly outperforms the baselines and achieves new state-of-the-art results on many bench-marks.
Next we review some related work. 2.