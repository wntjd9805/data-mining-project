Abstract
Recently, machine learning models have demonstrated to be vulnerable to backdoor attacks, primarily due to the lack of transparency in black-box models such as deep neural networks. A third-party model can be poisoned such that it works adequately in normal conditions but behaves mali-ciously on samples with specific trigger patterns. However, the trigger injection function is manually defined in most ex-isting backdoor attack methods, e.g., placing a small patch of pixels on an image or slightly deforming the image before poisoning the model. This results in a two-stage approach with a sub-optimal attack success rate and a lack of com-plete stealthiness under human inspection.
In this paper, we propose a novel and stealthy back-door attack framework, LIRA, which jointly learns the op-timal, stealthy trigger injection function and poisons the model. We formulate such an objective as a non-convex, constrained optimization problem. Under this optimization framework, the trigger generator function will learn to ma-nipulate the input with imperceptible noise to preserve the model performance on the clean data and maximize the at-tack success rate on the poisoned data. Then, we solve this challenging optimization problem with an efficient, two-stage stochastic optimization procedure. Finally, the pro-posed attack framework achieves 100% success rates in sev-eral benchmark datasets, including MNIST, CIFAR10, GT-SRB, and T-ImageNet, while simultaneously bypassing ex-isting backdoor defense methods and human inspection. 1.

Introduction
Machine learning models, especially deep neural net-works (DNNs), have recently achieved state-of-the-art per-formance in various applications and tasks, ranging from conventional research topics such as computer vision [24, 22] and natural language processing [13, 16] to distant fields such as games [44, 6], computational advertising [57, 54], and structural biology [1, 14]. However, along with the evo-lution, recent works have shown DNN models are vulnera-ble to various categories of adversarial attacks [37, 46, 29], which might be attributed to the lack of model trans-parency and explainability. Among these attacks, evasion attacks such as adversarial examples [7, 34] attempt to fool a trained model by manipulating the inputs in the in-ference phase, while causative attacks including poison-ing [35, 43, 58] and backdoor attacks [32, 30, 10, 20] seek to maliciously alter the model in the training phase.
Recently, the backdoor attack has attracted a lot of at-tention. The increasing complexity of model building that promoted training outsourcing and machine learning as a service (MLaaS) has also yielded security deficiencies in the supply chain [12, 55]. Existing literature on backdoor attacks [32, 30, 10, 20] has demonstrated that by inject-ing a backdoor trigger (usually a specific pattern such as a small square) to a small portion of the training data, the trained DNN induces misclassifications while facing inputs with the presence of this trigger. In addition, the model be-haves normally on clean inputs, which makes this type of attack hard to detect. As this field of research has evolved, the strength and capabilities of these attacks have increased, leading to methodologies that work with stealthier trig-gers [51] or compromise extended scenarios [4, 53].
Aligning with the research direction of adversarial ex-amples [23, 33, 52], one property of interest for the back-door attack is also to improve the fidelity of poisoned im-ages that are used to inject the backdoor and hence reduce the perceptual detectability by human observers. To this end, several works have indeed adopted adversarial exam-ple generation in crafting poisoned images that have im-proved visual quality or indistinguishability from vanilla training data [47, 26]. Blended and other novel trigger patterns have also been investigated [30, 5, 31]. Still, al-though the attacker carefully crafts the backdoor triggers in these works before poisoning the model, their trigger patterns can be detected by visual inspection. A very re-cent work, WaNet [36], creates stealthier backdoor images with manually designed warping transformation triggers and achieves state-of-the-art results in both attack success
Figure 1: Visualization of backdoor images from different methods. We use the examples from [36]. Images on top from left to right: the original image, images generated by patch based BadNets [21], blended backdoor [10], sinusoidal strips based backdoor (SIG) [5], reflection backdoor (ReFool) [31], warping based backdoor (WaNet) [36], and the proposed LIRA.
Bottom images are residual maps that are amplified by 2×. It is clear that the images generated by our method is natural and undetectable, as seen in the residual. For further illustration, we present the residual that is amplified by 500× (rightmost). rate and stealthiness against defense. tible. One example is shown in Figure 1.
In this paper, we propose a novel framework that simul-taneously “learns” to generate the perfect yet invisible trig-ger and poisons the classifier. We first formulate the pro-cess of finding the optimal trigger and the optimal classifier in a constrained optimization problem. Then, we propose an effective yet simple alternating stochastic optimization process to solve such a problem. The algorithm allows us to learn to generate the optimal trigger while successfully poisoning the classifier whose performance on the clean data is unchanged (compared to a vanilla classifier trained only on clean data). Paired with a visually imperceptible noise generation trigger function, the trigger patterns gener-ated by our method are extremely difficult to detect, which can successfully pass both the conducted human inspection test (with a significant improvement over the existing back-door attacks) and several backdoor defense mechanisms.
We showcase our backdoor images in Figure 1. We call our method Learnable Imperceptible and Robust Backdoor
Attack (LIRA).
Our technical contributions are summarized below:
• We propose a novel non-convex, constrained opti-mization problem, which unifies the process of gener-ating the trigger patterns and poisoning the classifier.
To solve this problem, we propose an efficient stochas-tic optimization algorithm that first alternates between finding the optimal trigger function and the optimal poisoned classifier in the highly non-linear parameter space, then fine-tunes only the poisoned classifier.
• We propose a stealthy conditional trigger generation function (also called the transformation function in this work), which can generate remarkably stealthy backdoor images whose residuals with respect to their clean versions are only 1/1000-1/200x of the inputs.
As a result, our backdoor attack is visually impercep-• Finally, we achieve state-of-the-art attack performance and stealthiness against both human inspection and ex-isting defense mechanisms. 2.