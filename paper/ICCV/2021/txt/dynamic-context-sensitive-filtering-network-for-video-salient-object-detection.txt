Abstract
The ability to capture inter-frame dynamics has been critical to the development of video salient object detec-tion (VSOD). While many works have achieved great suc-cess in this ﬁeld, a deeper insight into its dynamic na-In this work, we aim to an-ture should be developed. swer the following questions: How can a model adjust it-self to dynamic variations as well as perceive ﬁne differ-ences in the real-world environment; How are the tem-poral dynamics well introduced into spatial information over time? To this end, we propose a dynamic context-sensitive ﬁltering network (DCFNet) equipped with a dy-namic context-sensitive ﬁltering module (DCFM) and an effective bidirectional dynamic fusion strategy. The pro-posed DCFM sheds new light on dynamic ﬁlter generation by extracting location-related afﬁnities between consecutive frames. Our bidirectional dynamic fusion strategy encour-ages the interaction of spatial and temporal information in a dynamic manner. Experimental results demonstrate that our proposed method can achieve state-of-the-art perfor-mance on most VSOD datasets while ensuring a real-time speed of 28 fps. The source code is publicly available at https://github.com/OIPLab-DUT/DCFNet. 1.

Introduction
Videos as one of the most engaging mediums strike a deep connection with humans. As a fundamental task in video processing, video salient object detection (VSOD) aims to explore this connection and segment most visually
*Equal Contributions
†Corresponding Author
Figure 1. Architecture comparison of our dynamic ﬁltering based method (d) with 3D Convolution (a), Optical Flow (b) and ConvL-STM (c) based methods. distinctive regions in videos. This task has drawn broad at-tention due to a wide range of applications in video object segmentation [19, 33, 48], visual tracking [52], video cap-tioning [36], video compression [17, 14] and medical anal-ysis [18, 22]. Compared with still-image based SOD tasks,
VSOD does not only suffer from processing a huge amount of data but also is directly affected by temporal dynamics.
The substantial differences make VSOD more challenging than still-image based SOD task.
Most existing VSOD methods, which can be classi-ﬁed into 3D convolution based [24, 25], ConvLSTM based
[12, 44] and optical ﬂow based [28, 43] methods as shown in Figure 1, employ ﬁxed parameter layers during infer-ence. Given that our world is constantly changing, per-forming convolution with dynamic parameters conditioned on inputs can better adapt to dynamic real-world environ-ments [3, 34]. However, directly applying the dynamic ﬁl-tering mechanism to the VSOD task may fail to comprehen-sively utilize inter-frame contextual information. Therefore, this may impair these methods to achieve high accuracy for saliency prediction.
Moreover, when any events that happen in the real world are condensed into seconds, the pixels in different frames can be temporally inconsistent over time. Such time tak-ing the form of objects moving between consecutive frames makes VSOD very challenging. For instance, both moving foreground and background objects in a video clip enable some representative VSOD methods to be less effective, as illustrated in Figure 2. Given that spatial and temporal do-mains are entangled in video, sufﬁcient spatiotemporal fu-sion is the cornerstone of VSOD. It further extends how the temporal dynamics are incorporated into spatial information over time.
In this paper, we strive to confront challenges towards accurate VSOD. The primary challenge towards this goal is to design a model capable of not only adapting to dynamic changes but also distinguishing ﬁne differences in the real-world environment. The second challenge is to dynamically formulate the cross-domain complementarity, adaptively al-lowing more effective fusion. The key aspect in the success of our method is in its ability to better dynamically adjust it-self to our constantly-changing world. Concretely, our con-tributions are fourfold:
• We propose a dynamic context-sensitive ﬁltering mod-ule (DCFM). DCFM can estimate the location-related afﬁnity weights to dynamically generate context-thus promoting the sensitive convolution kernels, model’s adaptability to constantly changing scenes.
• We introduce a bidirectional dynamic fusion strategy to encourage the bidirectional interaction between spa-tial and temporal domains. As a result, the proposed strategy helps our network combine cross-domain fea-tures and ensures high stability for saliency detection in the challenging scenes.
• Furthermore, we conduct extensive experiments on 5 widely-used datasets and demonstrate that our method outperforms 12 state-of-the-art VSOD approaches in terms of 3 evaluation metrics. Especially, our ap-proach reduces the MAE metric by 34.8% and 27.3% on SegV2 [26] and DAVIS [39] respectively, which are dominated by fast and moderate moving objects respectively, showing the adaptability of our model in different video scenarios.
• The proposed DCFM can be extended to improve the existing still-image SOD based models. Experiments demonstrate that compared with the original models, the new ones embedded with the DCFM achieve better performance on all the evaluation metrics. (a) RGB (b) GT (c) Ours (d) MGA (e) RCR (f) PCSA
Figure 2. Sample prediction results of our methods compared to
MGA [28], RCRNet [57] and PCSA [13]. Column ’RGB’ shows raw images of a consecutive video sequence from DAVIS dataset.
Column ’GT’, ’Ours’, ’MGA’, ’RCR’ and ’PCSA’ denote ground truth, corresponding predictions from our methods, MGA, RCR-Net and PCSA respectively. 2.