Abstract
Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classiﬁcation. The
ViT model splits each image into a sequence of tokens with
ﬁxed length and then applies multiple Transformer layers to model their global relation for classiﬁcation. However,
ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We ﬁnd it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low train-ing sample efﬁciency; 2) the redundant attention backbone design of ViT leads to limited feature richness for ﬁxed com-putation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vi-sion Transformer (T2T-ViT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an ef-ﬁcient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on
ImageNet. It also outperforms ResNets and achieves com-parable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384×384 on ImageNet. 1 1.

Introduction
Self-attention models for language modeling like Trans-formers [37] have been recently applied to vision tasks, including image classiﬁcation [5, 12, 43], object detec-*Work done during an internship at Yitu Tech. 1Code: https://github.com/yitu-opensource/T2T-ViT
Figure 1. Comparison between T2T-ViT with ViT, ResNets and
MobileNets when trained from scratch on ImageNet. Left: per-formance curve of MACs vs. top-1 accuracy. Right: performance curve of model size vs. top-1 accuracy. tion [3, 61] and image processing like denoising, super-resolution and deraining [4]. Among them, the Vision
Transformer (ViT) [12] is the ﬁrst full-transformer model that can be directly applied for image classiﬁcation. In par-ticular, ViT splits each image into 14×14 or 16×16 patches (a.k.a., tokens) with ﬁxed length; then following practice of the transformer for language modeling, ViT applies trans-former layers to model the global relation among these to-kens for classiﬁcation.
Though ViT proves the full-transformer architecture is promising for vision tasks, its performance is still inferior to that of similar-sized CNN counterparts (e.g. ResNets) when trained from scratch on a midsize dataset (e.g., Im-ageNet). We hypothesize that such performance gap roots in two main limitations of ViT: 1) the straightforward tok-enization of input images by hard split makes ViT unable to model the image local structure like edges and lines, and thus it requires signiﬁcantly more training samples (like
JFT-300M for pretraining) than CNNs for achieving similar performance; 2) the attention backbone of ViT is not well-designed as CNNs for vision tasks, which contains redun-dancy and leads to limited feature richness and difﬁculties in model training.
To verify our hypotheses, we conduct a pilot study to investigate the difference in the learned features of ViT-L/16 [12] and ResNet50 [15] through visualization in Fig. 2.
We observe the features of ResNet capture the desired local
Figure 2. Feature visualization of ResNet50, ViT-L/16 [12] and our proposed T2T-ViT-24 trained on ImageNet. Green boxes highlight learned low-level structure features such as edges and lines; red boxes highlight invalid feature maps with zero or too large values. Note the feature maps visualized here for ViT and T2T-ViT are not attention maps, but image features reshaped from tokens. For better visualization, we scale the input image to size 1024 × 1024 or 2048 × 2048. structure (edges, lines, textures, etc.) progressively from the bottom layer (conv1) to the middle layer (conv25). How-ever, the features of ViT are quite different: the structure information is poorly modeled while the global relations (e.g., the whole dog) are captured by all the attention blocks.
These observations indicate that the vanilla ViT ignores the local structure when directly splitting images to tokens with
ﬁxed length. Besides, we ﬁnd many channels in ViT have zero value (highlighted in red in Fig. 2), implying the back-bone of ViT is not efﬁcient as ResNets and offers limited feature richness when training samples are not enough.
We are then motivated to design a new full-transformer vision model to overcome above limitations. 1) Instead of the naive tokenization used in ViT [12], we propose a pro-gressive tokenization module to aggregate neighboring To-kens to one Token (named Tokens-to-Token module), which can model the local structure information of surrounding tokens and reduce the length of tokens iteratively. Speciﬁ-cally, in each Token-to-Token (T2T) step, the tokens output by a transformer layer are reconstructed as an image (re-structurization) which is then split into tokens with over-lapping (soft split) and ﬁnally the surrounding tokens are aggregated together by ﬂattening the split patches. Thus the local structure from surrounding patches is embedded into the tokens to be input into the next transformer layer.
By conducting T2T iteratively, the local structure is aggre-gated into tokens and the length of tokens can be reduced by the aggregation process. 2) To ﬁnd an efﬁcient back-bone for vision transformers, we explore borrowing some architecture designs from CNNs to build transformer lay-ers for improving the feature richness, and we ﬁnd “deep-narrow” architecture design with fewer channels but more layers in ViT brings much better performance at compara-ble model size and MACs (Multi-Adds). Speciﬁcally, we investigate Wide-ResNets (shallow-wide vs deep-narrow structure) [52], DenseNet (dense connection) [21], ResneXt structure [44], Ghost operation [14, 59] and channel atten-tion [20]. We ﬁnd among them, deep-narrow structure [52] is the most efﬁcient and effective for ViT, reducing the pa-rameter count and MACs signiﬁcantly with nearly no degra-dation in performance. This also indicates the architecture engineering of CNNs can beneﬁt the backbone design of vision transformers.
Based on the T2T module and deep-narrow backbone ar-chitecture, we develop the Tokens-to-Token Vision Trans-former (T2T-ViT), which signiﬁcantly boosts the perfor-mance when trained from scratch on ImageNet (Fig. 1), and is more lightweight than the vanilla ViT. As shown in Fig. 1, our T2T-ViT with 21.5M parameters and 4.8G MACs can achieve 81.5% top-1 accuracy on ImageNet, much higher than that of ViT [12] with 48.6M parameters and 10.1G
MACs (78.1%). This result is also higher than the popu-lar CNNs of similar size, like ResNet50 with 25.5M param-eters (76%-79%). Besides, we also design lite variants of
T2T-ViT by simply adopting fewer layers, which achieve comparable results with MobileNets [17, 32] (Fig. 1).
To sum up, our contributions are three-fold:
• For the ﬁrst time, we show by carefully designing transformers architecture (T2T module and efﬁcient backbone), visual transformers can outperform CNNs at different complexities on ImageNet without pre-training on JFT-300M.
• We develop a novel progressive tokenization for ViT and demonstrate its advantage over the simple tok-enization approach by ViT, and we propose a T2T module that can encode the important local structure for each token.
• We show the architecture engineering of CNNs can beneﬁt the backbone design of ViT to improve the fea-ture richness and reduce redundancy. Through exten-sive experiments, we ﬁnd deep-narrow architecture de-sign works best for ViT. 2.