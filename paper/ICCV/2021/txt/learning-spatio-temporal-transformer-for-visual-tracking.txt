Abstract
In this paper, we present a new tracking architecture with an encoder-decoder transformer as the key compo-nent. The encoder models the global spatio-temporal fea-ture dependencies between target objects and search re-gions, while the decoder learns a query embedding to pre-dict the spatial positions of the target objects. Our method casts object tracking as a direct bounding box prediction problem, without using any proposals or predefined an-chors. With the encoder-decoder transformer, the predic-tion of objects just uses a simple fully-convolutional net-work, which estimates the corners of objects directly. The whole method is end-to-end, does not need any postprocess-ing steps such as cosine window and bounding box smooth-ing, thus largely simplifying existing tracking pipelines. The proposed tracker achieves state-of-the-art performance on multiple challenging short-term and long-term benchmarks, while running at real-time speed, being 6× faster than
Siam R-CNN [54]. Code and models are open-sourced at https://github.com/researchmm/Stark. 1.

Introduction
Visual object tracking is a fundamental yet challeng-ing research topic in computer vision. Over the past few years, based on convolutional neural networks, object track-ing has achieved remarkable progress [28, 11, 54]. How-ever, convolution kernels are not good at modeling long-range dependencies of image contents and features, because they only process a local neighborhood, either in space or time. Current prevailing trackers, including both the offline
Siamese trackers and the online learning models, are almost all built upon convolutional operations [2, 44, 3, 54]. As a consequence, these methods only perform well on model-ing local relationships of image content, but being limited to capturing long-range global interactions. Such deficiency may degrade the model capacities for dealing with the sce-narios where the global contextual information is important
∗Work performed when Bin Yan was an intern of MSRA.
† Corresponding authors: Houwen Peng (houwen.peng@microsoft.com),
Dong Wang (wdice@dlut.edu.cn).
Figure 1: Comparison with state-of-the-arts on LaSOT [15]. We visualize the Success performance with respect to the Frames-Per-Seconds (fps) tracking speed. The circle size indicates a weighted sum of the tracker’s speed (x-axis) and success score (y-axis). The larger, the better. Ours-ST101 and Ours-ST50 indicate the pro-posed trackers with ResNet-101 and ResNet-50 as backbones, re-spectively. Better viewed in color. for localization, such as the objects undergoing large-scale variations or getting in and out of views frequently.
The problem of long-range interactions has been tackled in sequence modeling through the use of transformer [53].
Transformer has enjoyed rich success in tasks such as natural language modeling [13, 46] and speech recogni-tion [40]. Recently, transformer has been employed in dis-criminative computer vision models and drawn great atten-tion [14, 5, 41]. Inspired by the recent DEtection TRans-former (DETR) [5], we propose a new end-to-end tracking architecture with encoder-decoder transformer to boost the performance of conventional convolution models.
Both spatial and temporal information are important for object tracking. The former one contains object appearance information for target localization, while the latter one in-cludes the state changes of objects across frames. Previous
Siamese trackers [28, 59, 16, 7] only exploit the spatial in-formation for tracking, while online methods [63, 66, 11, 3] use historical predictions for model updates. Although be-ing successful, these methods do not explicitly model the relationship between space and time. In this work, consider-ing the superior capacity on modeling global dependencies, we resort to transformer to integrate spatial and temporal
information for tracking, generating discriminative spatio-temporal features for object localization.
More specifically, we propose a new spatio-temporal ar-chitecture based on the encoder-decoder transformer for visual tracking. The new architecture contains three key components: an encoder, a decoder and a prediction head.
The encoder accepts inputs of an initial target object, the current image, and a dynamically updated template. The self-attention modules in the encoder learn the relation-ship between the inputs through their feature dependencies.
Since the template images are updated throughout video se-quences, the encoder can capture both spatial and tempo-ral information of the target. The decoder learns a query embedding to predict the spatial positions of the target ob-ject. A corner-based prediction head is used to estimate the bounding box of the target object in the current frame.
Meanwhile, a score head is learned to control the updates of the dynamic template images.
Extensive experiments demonstrate that our method es-tablishes new state-of-the-art performance on both short-term [20, 43] and long-term tracking benchmarks [15, 25].
For instance, our spatio-temporal transformer tracker sur-passes Siam R-CNN [54] by 3.9% (AO score) and 2.3% (Success) on GOT-10K [20] and LaSOT [15], respectively.
It is also worth noting that compared with previous long-term trackers [9, 54, 62], the framework of our method is much simpler. Specifically, previous methods usually con-sist of multiple components, such as base trackers [11, 57], target verification modules [23], and global detectors [47,
In contrast, our method only has a single network 21]. learned in an end-to-end fashion. Moreover, our tracker can run at real-time speed, being 6× faster than Siam R-CNN (30 v.s. 5 fps) on a Tesla V100 GPU, as shown in Fig. 1
Considering recent trends of over-fitting on small-scale benchmarks, we collect a new large-scale tracking benchmark called NOTU, integrating all sequences from
NFS [24], OTB100 [58], TC128 [33], and UAV123 [42].
In summary, this work has four contributions.
• We propose a new transformer architecture dedicated to visual tracking. It is capable of capturing global fea-ture dependencies of both spatial and temporal infor-mation in video sequences.
• The whole method is end-to-end, does not need any postprocessing steps such as cosine window and bounding box smoothing, thus largely simplifying ex-isting tracking pipelines.
• The proposed trackers achieve state-of-the-art perfor-mance on five challenging short-term and long-term benchmarks, while running at real-time speed.
• We construct a new large-scale tracking benchmark to alleviate the over-fitting problem on previous small-scale datasets. 2.