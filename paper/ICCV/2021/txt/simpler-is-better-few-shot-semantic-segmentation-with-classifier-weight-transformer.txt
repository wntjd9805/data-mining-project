Abstract
Support Mask
Meta-learned
A few-shot semantic segmentation model is typically composed of a CNN encoder, a CNN decoder and a simple classiﬁer (separating foreground and background pixels).
Most existing methods meta-learn all three model compo-nents for fast adaptation to a new class. However, given that as few as a single support set image is available, ef-fective model adaption of all three components to the new class is extremely challenging. In this work we propose to simplify the meta-learning task by focusing solely on the simplest component – the classiﬁer, whilst leaving the en-coder and decoder to pre-training. We hypothesize that if we pre-train an off-the-shelf segmentation model over a set of diverse training classes with sufﬁcient annotations, the encoder and decoder can capture rich discriminative fea-tures applicable for any unseen classes, rendering the sub-sequent meta-learning stage unnecessary. For the classi-ﬁer meta-learning, we introduce a Classiﬁer Weight Trans-former (CWT) designed to dynamically adapt the support-set trained classiﬁer’s weights to each query image in an in-ductive way. Extensive experiments on two standard bench-marks show that despite its simplicity, our method outper-forms the state-of-the-art alternatives, often by a large mar-gin. Code is available on https://github.com/zhiheLu/CWT-for-FSS. 1.

Introduction has achieved
Semantic segmentation remarkable progress in the past ﬁve years thanks to the availability of large-scale labeled datasets and advancements in deep learning algorithms [5, 7, 21]. Nonetheless, relying on many training images with exhaustive pixel-level anno-tation for every single class, existing methods have poor scalability to new classes.
Indeed, the high annotation cost has hindered the general applicability of semantic segmentation models. For instance, creating the COCO
Support
Query
Encoder
Encoder
A
Decoder
Prediction r e i f i s s a l
C
A
Adaptive module
Training
Train a linear classifier (a) Existing methods
Masked average pooling
Support Mask
Support
Query
Encoder
Encoder
Decoder
Training
Pre-trained (b) Ours r e i f i s s a l
C
A d e t p a d
A r e i f i s s a l
C
Meta-learned
Prediction
Figure 1. Illustrating the model training difference for 1-shot sce-nario between (a) existing few-shot segmentation methods and (b) ours. A few-shot segmentation model is typically composed of a deep CNN encoder, a deep CNN decoder and a much simpler classiﬁer. (a) Previous training methods usually meta-learn all the three parts (i.e., the whole model), so that all three can adapt to a new class represented by an annotated support set image to per-form segmentation on a query image. This adaptation is intrinsi-cally difﬁcult and sub-optimal due to complex model design and rather limited supervision available for the adaptation.
Instead, (b) we propose to meta-learn the simple classiﬁer part only whilst pre-training and then freezing the encoder and decoder, making few-shot adaptation to new classes much more tractable. dataset [17] took over 70,000 worker hours even for only 80 common object categories. Inspired by the signiﬁcant efforts in few-shot image classiﬁcation [27, 11, 29, 37], few-shot learning has been introduced into semantic seg-mentation recently [25, 22, 3, 34, 36, 40, 41]. A few-shot segmentation method eliminates the need of labeling a large set of training images. This is typically achieved by meta learning which enables the model to adapt to a new class represented by a support set consisting of as few as a single image.
A fundamental challenge faced by any few-shot segmen- 
model adaptation. Our assumption is that if we pre-train an off-the-shelf segmentation model over a set of diverse train-ing classes, the encoder and decoder can already capture a rich set of discriminative segmentation features suitable for not only the training classes but also the unseen test classes, i.e., being class-agnostic. We need to then focus on adapt-ing the classiﬁer part alone. Indeed, we found that if we simply do the pre-training and use the support set of a new class to train a classiﬁer (i.e., without meta-learning), the result is already comparable to that obtained by the state-of-the-art methods (see Sec. 4). However, this naive base-line cannot adapt to each query image, which is critical for our problem due to the large intra-class variation (Figure 2). Without sufﬁcient training samples in the support set to accommodate this intra-class variation, a few-shot seg-mentation model must adapt to each query image as well.
We hence further propose a novel meta-learning framework that employs a Classiﬁer Weight Transformer (CWT) to dy-namically adapt the support-set trained classiﬁer’s weights to each query image in an inductive way, i.e., adaptation occurs independently on each query image.
We make the following contributions in this work: (1)
We propose a novel model training paradigm for few-shot semantic segmentation. Instead of meta-learning the whole, complex segmentation model, we focus on the simplest classiﬁer part to make new-class adaptation more tractable. (2) We introduce a novel meta-learning algorithm that lever-ages a Classiﬁer Weight Transformer (CWT) for adapt-ing dynamically the classiﬁer weights to every query sam-ple. (3) Extensive experiments with two popular back-bones (ResNet-50 and ResNet-101) show that the proposed method yields a new state-of-the-art performance, often sur-passing existing alternatives, especially on 5-shot case, by a large margin. Further, under a more challenging yet practi-cal cross-domain setting, the margin becomes even bigger. 2.