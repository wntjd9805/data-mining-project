Abstract
Tracking by detection, the dominant approach for online multi-object tracking, alternates between localization and association steps. As a result, it strongly depends on the quality of instantaneous observations, often failing when objects are not fully visible.
In contrast, tracking in hu-mans is underlined by the notion of object permanence: once an object is recognized, we are aware of its physical existence and can approximately localize it even under full occlusions. In this work, we introduce an end-to-end train-able approach for joint object detection and tracking that is capable of such reasoning. We build on top of the re-cent CenterTrack architecture, which takes pairs of frames as input, and extend it to videos of arbitrary length. To this end, we augment the model with a spatio-temporal, recur-rent memory module, allowing it to reason about object lo-cations and identities in the current frame using all the pre-vious history. It is, however, not obvious how to train such an approach. We study this question on a new, large-scale, synthetic dataset for multi-object tracking, which provides ground truth annotations for invisible objects, and propose several approaches for supervising tracking behind occlu-sions. Our model, trained jointly on synthetic and real data, outperforms the state of the art on KITTI and MOT17 datasets thanks to its robustness to occlusions. 1.

Introduction
Consider the video sequence from the KITTI dataset [24] shown in Figure 1. A man on the left walks behind the moving car and is not visible anymore. Yet, there is no question that he is still there, and did not simply vanish.
Moreover, we can approximately infer his location at that moment. This capability is known to cognitive scientists as object permanence, and is observed in infants at a very early age [3, 53].
In adults, understanding that occluded objects do not disappear is important for tasks like driving.
In this work, we propose a deep learning-based method for multi-object tracking that is capable of such reasoning.
Virtually all modern multi-object tracking algorithms
Figure 1. Video frames from the KITTI dataset with outputs of
CenterTrack [66] (above), and our method (below). By modeling object permanence, our approach is able to hallucinate trajectories of fully occluded instances, such as the person behind the car. operate in the tracking-by-detection paradigm. That is, they use an existing object detector to localize objects of in-terest in every frame of a video, and then link them into tracks, either in an online [9, 11], or in an ofﬂine man-ner [5, 6, 10, 34, 42]. In this work we focus on the online setting, where a method needs to associate current detec-tions with previously established trajectories [9, 54, 59, 62].
A major limitation of these methods is that the localization step is completely independent from the previous history, thus, once an object becomes partially or fully occluded, the detector fails and the trajectory gets broken (see Fig-ure 1, top). Recently, several approaches combine detection and tracking in a single model [7, 66]. They take pairs of frames as input and output detections together with pair-wise associations. While these methods improve tracking robustness, they can only handle single-frame occlusions.
In this work, we propose an end-to-end trainable, on-line approach for multi-object tracking that leverages object permanence as an inductive prior. To this end, we ﬁrst ex-tend the recent CenterTrack architecture [66] from pairs of frames as input to arbitrary video sequences. The frames are processed by a convolutional gated recurrent unit (Con-vGRU) [4] that encodes the spatio-temporal evolution of objects in the input video, taking the entire history into ac-count. As a result, it can can reason about locations of par-tially and fully occluded instances using the object perma-nence assumption (see Figure 1, bottom).
Supervising this behavior is a major challenge. No track-ing dataset currently provides consistent annotations for oc-cluded objects at a large scale (see Figure 5) because of
In this paper, we instead pro-the associated uncertainty. pose to use synthetic data. Using the Parallel Domain (PD) [1] simulation platform, we generate a dataset of syn-thetic videos that automatically provides accurate labels for all objects, irrespective of their visibility (see Figure 2). We then use this dataset to analyze various approaches for su-pervising tracking behind occlusions with both ground-truth and pseudo-ground-truth labels for occluded instances.
Despite the progress in simulation, the domain gap be-tween synthetic and real videos may limit performance.
As we show in Section 4.3, a model directly trained on synthetic videos indeed underperforms when applied to real-world multi-object tracking benchmarks such as
KITTI [24]. However, we ﬁnd that this domain gap can be overcome by just training our model jointly on synthetic and real data, supervising invisible objects only on synthetic se-quences. This allows to learn complex behaviors across do-mains, including trajectory hallucination for fully occluded instances although this was never labeled in the real world.
Our contributions are three-fold. (1) We propose an end-to-end trainable architecture for joint object detection and tracking that operates on videos of arbitrary length in
Sec. 3.2. (2) We demonstrate how this architecture can be trained to hallucinate trajectories of fully invisible objects in Sec. 3.3. (3) We show how to supervise our method with a mix of synthetic and real data in Sec. 3.4, and val-idate it on the KITTI [24] and MOT17 [38] real-world benchmarks in Sec. 4, outperforming the state of the art.
Source code, models, and data are publicly available at https://github.com/TRI-ML/permatrack. 2.