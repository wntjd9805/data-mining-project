Abstract
Autonomous navigation requires structured representa-tion of the road network and instance-wise identification of the other traffic agents. Since the traffic scene is defined on the ground plane, this corresponds to scene understanding in the bird’s-eye-view (BEV). However, the onboard cam-eras of autonomous cars are customarily mounted horizon-tally for a better view of the surrounding, making this task very challenging. In this work, we study the problem of ex-tracting a directed graph representing the local road net-work in BEV coordinates, from a single onboard camera image. Moreover, we show that the method can be ex-tended to detect dynamic objects on the BEV plane. The semantics, locations, and orientations of the detected ob-jects together with the road graph facilitates a comprehen-sive understanding of the scene. Such understanding be-comes fundamental for the downstream tasks, such as path planning and navigation. We validate our approach against powerful baselines and show that our network achieves su-perior performance. We also demonstrate the effects of various design choices through ablation studies. Code: https://github.com/ybarancan/STSU 1.

Introduction
Road scene understanding is crucial for autonomous driving since it forms the interface between perception and planning. The fundamental task is to understand both the road network structure and the other traffic agents in the surrounding. Currently, the go-to solution is offline generated HD-maps combined with a modular perception stack [22, 41, 30, 36, 8]. For existing solutions to work, not only the precise localization in the HD-map but also understanding the dynamic parts of the scene is neces-sary [30, 44]. To achieve these requirements, most solutions use several sensors, including cameras and LIDAR. How-ever, using expensive sensors and offline HD-maps limit the scalability of autonomous driving as they increase the cost of operation and limit self-driving cars to operate in geo-graphically restricted areas.
Figure 1. Our method extracts a directed graph that represents the local road network from a single frontal image. First, centerlines are estimated alongside a directed graph where vertices are center-lines, and edges show the connectivity. Then the existence and di-rection of the edges are estimates. Green dots indicate start points, and red dots indicate end points of centerlines. Traffic flows from green to red. This representation can be achieved thanks to the directional nature of the proposed method.
In this work we proposes an end-to-end vision method that performs lane-graph extraction and object detection given only a front-facing camera image. Our method di-rectly estimates the graph structure of the road network and spline curves representing centerlines of individual lanes, as shown in Fig. 1. Besides estimating the road graph, our model can also detect objects such as cars, pedestrians, and others, directly on the BEV plane, as shown in Fig. 2. The output format of our method is ideal for downstream plan-ning [2, 9] and prediction [12, 45, 20, 37] tasks, which re-quire both the lane-graph and the location and class of ob-jects. In fact, such a requirement can also be understood simply by observing the provided labels of existing datasets, such as [4], which provide the labels in a structured form.
Often, existing approaches map the structured labels into other forms, such as semantic masks, to perform scene un-derstanding [14]. The downstream tasks, however, require the structured form of these understandings [26, 34, 25, 19].
Understanding HD-maps is a challenging problem, mainly due to the complex topological changes. Recovering such topological structure coherently from a single image remains to be an unexplored problem. This work addresses this challenging problem for the first time while also de-tecting objects in the scene directly in the BEV coordinates.
Existing works either focus on (i) HD-map extraction from dense 3D points [19] or (ii) the detection of road lanes from a single image [21]. Other variants, such as BEV semantic understanding, also exist [40, 29, 32]. Note that the HD-map reconstruction of [19] is much more topologically chal-lenging than the lane detection problem of [21]. Our work aims to achieve results similar to [19] using the image input setup of [21]. Additionally, we aim to detect objects using the same model as for structured HD-map predictions.
We represent the HD-map as a directed graph in BEV coordinates, whose edges are the road segments and the di-rection represents the traffic flow. We model each road seg-ment using a Bezier curve, with starting and end points. The connections between the predicted segments are modeled using an assignment matrix. For the prediction, we make use of a transformer network, which is supervised by using the Hungarian algorithm at the output end. The predicted segments, along with their connectivity, defines a full lane graph HD-map. Our transformer network further predicts the parameters of 2D BEV objects. The object prediction branch is supervised, similar to the road segments. Two ex-ample outputs of our method for both lane graph HD-map and object estimation are shown in Fig. 2. To this end, our major contributions can be summarized as follows.
• We propose a unified method for structured BEV road network graph estimation and object detection from a single onboard monocular camera image.
• The results obtained by the proposed method are sig-nificantly superior to the compared baselines.
Figure 2. Our method can handle very complex cross roads scenes as well multiple object instances. Pedestrian is marked with circle. clouds vs. single image input, (ii) highway lane boundaries vs. lane centerlines in an unrestricted setting.
BEV semantics understanding: Because of its practical use, scene understanding in BEV using images has recently gathered significant attention [40, 35, 5]. Some methods
In this also combine images with LIDAR data [33, 16]. regard, methods developed in [40, 31] use a single image to understand the BEV HD-map semantics. Similarly, the method proposed in [5] uses video data for the same task.
Methods in this category do not offer structured output suit-able for many downstream tasks. These methods may be used for general scene understanding. However, their usage for the task of motion planning and navigation is not triv-ial. Furthermore, up to our knowledge, no existing method provides instance-level predictions on the BEV using single image input. Note that the method proposed in this paper predicts both HD-map and the road objects simultaneously, using one input image and a single neural network. 2.