Abstract
Fine-tuning from pre-trained ImageNet models has been a simple, effective, and popular approach for various com-puter vision tasks. The common practice of fine-tuning is to adopt a default hyperparameter setting with a fixed pre-trained model, while both of them are not optimized for spe-cific tasks and time constraints. Moreover, in cloud comput-ing or GPU clusters where the tasks arrive sequentially in a stream, faster online fine-tuning is a more desired and re-alistic strategy for saving money, energy consumption, and
CO2 emission. In this paper, we propose a joint Neural Ar-chitecture Search and Online Adaption framework named
NASOA towards a faster task-oriented fine-tuning upon the request of users. Specifically, NASOA first adopts an of-fline NAS to identify a group of training-efficient networks to form a pretrained model zoo. We propose a novel joint block and macro level search space to enable a flexible and effi-cient search. Then, by estimating fine-tuning performance via an adaptive model by accumulating experience from the past tasks, an online schedule generator is proposed to pick up the most suitable model and generate a personalized training regime with respect to each desired task in a one-shot fashion. The resulting model zoo1 is more training effi-cient than SOTA models, e.g. 6x faster than RegNetY-16GF, and 1.7x faster than EfficientNetB3. Experiments on mul-tiple datasets also show that NASOA achieves much better improving around 2.1% accuracy fine-tuning results, i.e. than the best performance in RegNet series under various constraints and tasks; 40x faster compared to the BOHB. 1.

Introduction
Fine-tuning using pre-trained models becomes the de-facto standard in the field of computer vision because of
∗ Corresponding Author: Xiaodan Liang xdliang328@gmail.com 1The efficient training model zoo (ET-NAS) has been released at: https://github.com/NAS-OA/NASOA its impressive results on various downstream tasks such as fine-grained image classification [36, 50], object detection
[19, 23, 54] and segmentation [8, 30]. [25, 19] verified that fine-tuning pre-trained networks outperform training from scratch.
It can further help to avoid over-fitting [10] as well as reduce training time significantly [19]. Due to those merits, many cloud computing and AutoML pipelines pro-vide fine-tuning services for an online stream of upcoming users with new data, different tasks and time limits. In or-der to save the user’s time, money, energy consumption, or even CO2 emission, an efficient online automated fine-tuning framework is practically useful and in great demand.
Thus, we propose to explore faster online fine-tuning.
The conventional practice of fine-tuning is to adopt a set of predefined hyperparameters for training a predefined model [27]. It has three drawbacks in the current online set-ting: 1) The design of the backbone model is not optimized for the upcoming fine-tuning task and the selection of the backbone model is not data-specific. 2) A default setting of hyperparameters may not be optimal across tasks and the training settings may not meet the time constraints provided by users. 3) With the incoming tasks, the regular diagram is not suitable for this online setting since it cannot memorize and accumulate experience from the past fine-tuning tasks.
Thus, we propose to decouple our faster fine-tuning prob-lem into two parts: finding efficient fine-tuning networks and generating optimal fine-tuning schedules pertinent to specific time constraints in an online learning fashion.
Recently, Neural Architecture Search (NAS) algorithms demonstrate promising results on discovering top-accuracy architectures, which surpass the performance of hand-crafted networks and saves human’s efforts [58, 31, 32, 38, 49, 40, 48, 56] as well as studying NAS across tasks and datasets [9, 13]. However, those NAS works usually fo-cus on inference time/FLOPS optimization and their search space is not flexible enough which cannot guarantee the op-timality for fast fine-tuning. In contrast, we resort to devel-oping a NAS scheme with a novel flexible search space for fast fine-tuning. On the other hand, hyperparameter opti-mization (HPO) methods such as grid search [3], Bayesian optimization (BO) [46, 34], and BOHB [14] are used in deep learning and achieve good performance. However, those search-based methods are computationally expensive and require iterative “trial and error”, which violate our goal for faster adaptation time.
In this work, we propose a novel Neural Architecture
Search and Online Adaption framework named NASOA.
First, we conduct an offline NAS for generating an effi-cient fine-tuning model zoo. We design a novel block-level and macro-structure search space to allow a flexible choice of the networks. Once the efficient training model zoo is created offline NAS by Pareto optimal models, the online user can enjoy the benefit of those efficient training net-works without any marginal cost. We then propose an on-line learning algorithm with an adaptive predictor to mod-eling the relation between different hyperparameter, model, dataset meta-info and the final fine-tuning performance.
The final training schedule is generated directly from se-lecting the fine-tuning regime with the best predicted per-formance. Benefiting from the experience accumulation via online learning, the diversity of the data and the increasing results can further continuously improve our regime gener-ator. Our method behaves in a one-shot fashion and doesn’t involve additional searching cost as HPO, endowing the ca-pability of providing various training regimes under differ-ent time constraints. We also theoretically prove the con-vergence of the optimality of our proposed online model.
Extensive experiments are conducted on multiple widely used fine-tuning datasets. The searched model zoo ET-NAS is more training efficient than SOTA ImageNet models, e.g. 5x training faster than RegNetY-16GF, and 1.7x faster than
EfficientNetB3. Moreover, by using the whole NASOA, our online algorithm achieves superior fine-tuning results in terms of both accuracy and fine-tuning speed, i.e. im-proving around 2.1% accuracy than the best performance in RegNet series under various tasks; saving 40x computa-tional cost comparing to the BOHB method.
Our contributions are summarized as follows:
• We make the first effort to propose a faster fine-tuning pipeline that seamlessly combines the training-efficient
NAS and online adaption algorithm. Our NASOA can ef-fectively generate a personalized fine-tuning schedule of each desired task via an adaptive model for accumulating experience from the past tasks.
• The proposed novel joint block/macro level search space enables a flexible and efficient search. The resulting model zoo ET-NAS is more training efficient than very strong ImageNet SOTA models e.g. EfficientNet, Reg-Net. All the ET-NAS models have been released to help the community skipping the computation-heavy NAS stage and directly enjoy the benefit of NASOA.
• The whole NASOA pipeline achieves much better fine-tuning results in terms of both accuracy and fine-tuning efficiency than current fine-tuning best practice and HPO method, e.g. , 40x faster compared to the BOHB method. 2.