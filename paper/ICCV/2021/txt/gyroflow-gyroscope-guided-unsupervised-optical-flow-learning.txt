Abstract
Existing optical flow methods are erroneous in challeng-ing scenes, such as fog, rain, and night because the ba-sic optical flow assumptions such as brightness and gra-dient constancy are broken. To address this problem, we present an unsupervised learning approach that fuses gy-roscope into optical flow learning. Specifically, we first convert gyroscope readings into motion fields named gyro field. Second, we design a self-guided fusion module to fuse the background motion extracted from the gyro field with the optical flow and guide the network to focus on motion details. To the best of our knowledge, this is the first deep learning-based framework that fuses gyroscope data and image content for optical flow learning. To vali-date our method, we propose a new dataset that covers reg-ular and challenging scenes. Experiments show that our method outperforms the state-of-art methods in both regu-lar and challenging scenes. Code and dataset are available at https://github.com/megvii-research/GyroFlow. 1.

Introduction
Optical flow estimation is a fundamental yet essential computer vision task that has been widely applied in vari-ous applications such as object tracking [1], visual odom-etry [4], and image alignments [23]. The original for-mulation of the optical flow was proposed by Horn and
Schunck [10], after which the accuracy of optical flow estimation algorithms has been improved steadily. Early traditional methods minimize pre-defined energy functions with various assumptions and constraints [35]. Deep learning-based methods directly learn the per-pixel regres-sion through convolutional neural networks, which can be divided into supervised [6, 40, 43] and unsupervised meth-ods [41, 36]. The former methods are primarily trained on synthetic data [6, 3] due to the lack of ground-truth labels.
*Corresponding author
Figure 1. (a) Input low-light frame. (b) Optical flow result from existing baseline method ARFlow [30]. (d)
Result from our GyroFlow. (c) Ground-Truth.
In contrast, the later ones can be trained on abundant and diverse unlabeled data by minimizing the photometric loss between two images. Although existing methods achieve good results, they rely on image contents, requiring images to contain rich texture and similar illumination conditions.
On the other hand, gyroscopes do not rely on image contents, which provide angular velocities in terms of roll, pitch, and yaw that can be converted into 3D motion, widely used for system control [26] and the HCI of mobiles [8].
Among all potential possibilities [2, 27, 14], one is to fuse the gyro data for the motion estimation. Hwangbo et al. proposed to fuse gyroscope to improve the robustness of
KLT feature tracking [14]. Bloesch et al. fused gyroscope for the ego-motion estimation [2]. These attempts demon-strate that if the gyroscope is integrated correctly, the per-formance and the robustness of the method can be largely improved.
Given camera intrinsic parameters, gyro readings can be converted into motion fields to describe background mo-tion instead of dynamic object motion because it is confined
It is engaging that gyroscopes do not to camera motion. require the image contents but still produce reliable back-ground camera motion under conditions of poor texture or
dynamic scenes. Therefore, gyroscopes can be used to im-prove the performance of optical flow estimation in chal-lenging scenes, such as poor texture or inconsistent illumi-nation conditions.
In this paper, we propose GyroFlow, a gyroscope-guided unsupervised optical flow estimation method. We combine the advantages of image-based optical flow that recovers motion details based on the image content with those of a gyroscope that provides reliable background camera motion independent of image contents. Specifically, we first con-vert gyroscope readings into gyro fields that describe back-ground motion given the image coordinates and the cam-era intrinsic. Second, we estimate optical flow with an un-supervised learning framework and insert a proposed Self-Guided Fusion (SGF) module that supports the fusion of the gyro field during the image-based flow calculation. Fig. 1 shows an example, where Fig. 1 (a) represents the input of a night scene with poor image texture, and Fig. 1 (c) is the ground-truth optical flow between two frames. Image-based methods such as ARFlow [30] (Fig. 1 (b)) can produce the dynamic object motion but fail to compute the background motion in the sky, where no texture is available. Fig. 1 (d) shows our GyroFlow fusion result. As seen, both global motion and motion details can be retained. From experi-ments, we notice that motion details can be better recovered if global motion is provided.
To validate our method, we propose a dataset GOF (Gyroscope Optical Flow) containing scenes under 4 differ-ent categories with synchronized gyro readings, including one regular scene (RE) and three challenging cases as low light scenes (Dark), foggy scenes (Fog), and rainy scenes (Rain). For quantitative evaluations, we further propose a test set, which includes accurate optical flow labels by the method [29], through extensive efforts. Note that existing flow datasets, such as Sintel [3], KITTI [7, 38] cannot be used for the evaluation due to the absence of the gyroscope readings. To sum up, our main contributions are:
• We propose the first DNN-based framework that fuses gyroscope data into optical flow learning.
• We propose a self-guided fusion module to effectively realize the fusion of gyroscope and optical flow.
• We propose a dataset for the evaluation. Experiments show that our method outperforms existing methods. 2.