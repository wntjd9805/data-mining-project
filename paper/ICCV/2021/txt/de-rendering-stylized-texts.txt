Abstract
Editing raster text is a promising but challenging task.
We propose to apply text vectorization for the task of raster text editing in display media, such as posters, web pages, or advertisements. In our approach, instead of applying image transformation or generation in the raster domain, we learn a text vectorization model to parse all the rendering param-eters including text, location, size, font, style, effects, and hidden background, then utilize those parameters for recon-struction and any editing task. Our text vectorization takes advantage of differentiable text rendering to accurately re-produce the input raster text in a resolution-free parametric format. We show in the experiments that our approach can successfully parse text, styling, and background information in the unified model, and produces artifact-free text editing compared to a raster baseline. 1.

Introduction
Typography is the art of visually arranging letters and text. Typography greatly affects how people perceive tex-tual content in graphic design. Graphic designers carefully arrange and stylize text to convey their message in dis-play media, such as posters, web pages, or advertisements.
In computer vision, letters and texts have been predomi-nantly the subject of optical character recognition (OCR), where the goal is to identify characters in the given image.
This includes both OCR in printed documents or automatic scene text recognition for navigation. However, often, typo-graphic information of text, such as font, outline, or shadow, is ignored in text recognition because the OCR does not tar-get at reproducing typography in the output.
In this paper, we consider raster text editing as a text vectorization problem. There have been a few attempts at editing raster text mainly in scene images [32, 43, 45]. Pre-vious work frames text editing as a style transfer problem in the pixel domain, where the goal is to apply the original style and effect in the input image to the target characters in the destination. However, the major limitation of the style
Figure 1. The proposed text editing approach. Once the rendering parameters are recovered, we can apply any text editing and style manipulation. transfer approach is that 1) it is hard to avoid artifacts in the resulting image, and 2) a pixel-based model is resolution-dependent. Our main target is display media. Compared to scene text images [32, 43, 45] that often involve external noise due to lighting condition, we often observe noise-free texts in display media which is prone to small artifacts on reconstruction. Editing in the vector format has a clear ad-vantage in display media, as rendering results in consistent and sharp drawing at any resolution. Our approach is equiv-alent to viewing text recognition as a de-rendering prob-lem [42], where the goal is to predict rendering parameters.
Figure 1 illustrates our approach to the task of raster text editing.
Text vectorization can be an ill-posed problem. For edit-ing raster text in the vector format, we need to first parse characters, text styling information such as font and ef-fects, and hidden background pixels. Once those render-ing parameters are recovered from the input image, we can edit and render the target text using a rasterizer. Our ap-proach hence has to solve three sub-problems: 1) OCR, 2) background inpainting, and 3) styling attribute recognition.
OCR has a long history of research that dates back to the 1870s [34]. In parallel, inpainting has been studied in nu-merous literature [25, 50, 31, 24]. It might look straight-forward to add styling attribute recognition to those two problems for our task. However, parsing various stylistic at-tributes is not trivial, as the presence of multiple rendering effects can easily lead to an ill-posed decomposition prob-lem. For example, it is impossible to decompose the border and fill colors if two colors are the same. For solving such an ill-posed problem, we train neural networks to predict statistically plausible parameters.
Our model parses text, background, and styling attributes in two steps; we obtain the initial rendering parameters from feedforward inference, then refine the parameters by feed-back inference. Our feedback refinement incorporates dif-ferentiable rendering to reproduce the given raster text, and fit the parameters to the raster input using reconstruction loss. Following SynthText [9], we train our feedforward model on text images we generate using a rendering engine.
In the experiments, we show that our vectorization model accurately parses the text information. Further, we demon-strate that we can successfully edit texts by an off-the-shelf rendering engine using the parsed rendering parameters.
We summarize our contributions below.
• We formulate raster text editing as a de-rendering problem, where the task is to parse potentially ill-posed rendering parameters from the given image.
• We propose a vectorization model to parse detailed text information, using forward and backward infer-ence procedure that takes advantage of differentiable rendering.
• We empirically show that our model achieves high quality parsing of rendering parameters. We also demonstrate that the parsed information can be suc-cessfully utilized in a 2D graphics engine for down-stream edit tasks. 2.