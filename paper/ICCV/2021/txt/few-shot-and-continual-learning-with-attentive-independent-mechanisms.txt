Abstract
Deep neural networks (DNNs) are known to perform well when deployed to test distributions that shares high similarity with the training distribution. Feeding DNNs with new data sequentially that were unseen in the training dis-tribution has two major challenges — fast adaptation to new tasks and catastrophic forgetting of old tasks. Such difficul-ties paved way for the on-going research on few-shot learn-ing and continual learning. To tackle these problems, we introduce Attentive Independent Mechanisms (AIM). We in-corporate the idea of learning using fast and slow weights in conjunction with the decoupling of the feature extraction and higher-order conceptual learning of a DNN. AIM is de-signed for higher-order conceptual learning, modeled by a mixture of experts that compete to learn independent con-cepts to solve a new task. AIM is a modular component that can be inserted into existing deep learning frameworks. We demonstrate its capability for few-shot learning by adding it to SIB and trained on MiniImageNet and CIFAR-FS, show-ing significant improvement. AIM is also applied to ANML and OML trained on Omniglot, CIFAR-100 and MiniIma-geNet to demonstrate its capability in continual learning.
Code made publicly available at https://github. com/huang50213/AIM-Fewshot-Continual. 1.

Introduction
Humans have the ability to learn new concepts continu-ally while retaining previously learned concepts [11]. While learning new concepts, prior concepts that were learned are leveraged to form new connections in the brain [4, 52].
The plasticity of the human brain plays an important role on the forming of novel neuronal connections for learn-ing new concepts. Current deep learning methods are in-efficient in remembering old concepts after being fed with new concepts, also widely know as catastrophic forgetting
[34, 23]. Deep neural networks (DNNs) trained in an end-to-end fashion also has difficulty in learning new tasks in a sample efficient manner [12]. It is conjectured that the cause of catastrophic forgetting and inefficiency in learn-ing new tasks is from the stability-plasticity dilemma [2].
Stability is required so that previously learned informa-tion can be retained through the limitation of abrupt weight changes. Plasticity on the other hand encourages large weight changes, resulting in the fast acquisition of new con-cepts with the trade-off of forgetting old concepts.
It is believed that by scaling up the currently available architecture, DNNs are able to generalize better [7, 41, 10].
Tremendous effort is placed into neural architecture search (NAS) [28, 54, 49, 39, 32] with the hypothesis that improve-ments on a structural level introduce inductive bias that im-proves the generalizability of a neural network. As most of the prior arts are evaluated on benchmark datasets that are distributed similarly to the training set that it is trained on, the evaluation results are not a good measure of the gener-alization. We argue that the ability to adapt, acquire new knowledge and recall previously learned information plays an important role in reaching true generalization. The im-portance of learning to learn, i.e. meta-learning, has shone the spotlight on two major research direction that we will focus on — few-shot learning and continual learning. In few-shot learning [12, 37, 45, 14], the goal is to learn novel concepts with as few samples as possible, i.e. evaluating the capability of adapting to new tasks. Whereas in continual learning, the ability to learn an increasing amount of con-cepts while not forgetting old ones is evaluated.
Following OML [22], we separate the feature extraction part and the decision making part of the network, defined in
OML as representation learning network (RLN) and predic-tion learning network (PLN) respectively. The fast and slow learning in OML is performed on an architecture level, i.e.
RLN is updated in the outer loop (slow weights) and PLN is updated in the inner loop (fast weights). Such approach has proven to be helpful in learning sparse representation that are beneficial for fast adaptation and prevention of catas-trophic forgetting. We take one step further by introducing sparsity on an architectural level, accomplished through the introduction of Attentive Independent Mechanisms (AIM).
AIM is composed of a set of mechanisms that competitively
attend to the input representation, having mechanisms that are closely related to the input representation being acti-vated during inference. AIM can be understood as a mixture of experts competing to explain an incoming representation, hence only the mechanisms that best explain the input repre-sentation will be updated, leading to a sparse representation or modeling on an architectural level. Having sparse mod-eling on an architectural level for higher-order representa-tions has its benefits, as only the experts or mechanisms that best explain a task will be involved in the learning process, helping in the acceleration of learning new concepts and the mitigation of catastrophic forgetting. To demonstrate the potential of AIM as a fundamental building block for fast learning without forgetting, we demonstrate its strength on few-shot classification [12, 43, 53] and continual learning
[5, 22, 23] benchmarks.
Our contributions are as follows: (1) In Section 3, we give a detailed description and formulation of AIM — a novel module that can be used for few-shot and contin-ual learning. (2) We apply AIM on few-shot learning and continual learning tasks in Section 4.1 and Section 4.2 re-spectively. Qualitative and quantitative results are shown for both learning tasks, giving readers an insight on the importance of having AIM in the context of few-shot and continual learning. For few-shot classification, experiments are performed on CIFAR-FS and MiniImageNet whereas for continual learning, experiments are performed on Om-niglot, CIFAR-100 and MiniImageNet. Substantial im-provement in accuracy over prior arts are shown. 2.