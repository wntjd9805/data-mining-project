Abstract
Recent works find that AI algorithms learn biases from data. Therefore, it is urgent and vital to identify biases in
AI algorithms. However, the previous bias identification pipeline overly relies on human experts to conjecture poten-tial biases (e.g., gender), which may neglect other underlying biases not realized by humans. To help human experts better find the AI algorithms’ biases, we study a new problem in this work – for a classifier that predicts a target attribute of the input image, discover its unknown biased attribute.
To solve this challenging problem, we use a hyperplane in the generative model’s latent space to represent an im-age attribute; thus, the original problem is transformed to optimizing the hyperplane’s normal vector and offset. We propose a novel total-variation loss within this framework as the objective function and a new orthogonalization penalty as a constraint. The latter prevents trivial solutions in which the discovered biased attribute is identical with the target or one of the known-biased attributes. Extensive experiments on both disentanglement datasets and real-world datasets show that our method can discover biased attributes and achieve better disentanglement w.r.t. target attributes. Fur-thermore, the qualitative results show that our method can discover unnoticeable biased attributes for various object and scene classifiers, proving our method’s generalizability for detecting biased attributes in diverse domains of images. 1.

Introduction
Although the performance of deep neural networks is greatly improved by training on large-scale datasets, worri-some biases are also learned by AI algorithms. Thus it is imperative to identify AI algorithms’ biases, whereas the previous bias identification pipeline [5, 39] has some short-comings. First, it overly relies on human experts to speculate potential biases (Step 1 in Fig. 1 (a)), which may leave other unconsidered biases unexposed. For example, people may
Figure 1: While the previous pipeline (a) overly relies on human efforts to identify biases, we devise a new automated framework (b) that helps humans discover the unknown biased attribute of an image classifier. In step 1, a genera-tive models’ biased attribute hyperplane in the latent space is optimized by our proposed optimization objective and constraints. In step 2, humans can interpret the semantic meaning of the biased attribute hyperplane from the trans-formation in the synthesized traversal images. For example, images changing from “young” to “old” imply the biased attribute age. conjecture legally protected attributes (e.g., age, gender1) for face image classifiers and consider “scene bias” for the action recognition task [7]. However, people may neglect other unnoticeable biased attributes such as hair length [3] and the presence of children [53] spuriously correlated with gender. Although they are not legally protected attributes, not considering these biased attributes may still lead to un-fairness against different genders [53]. Second, the previous pipeline also needs expensive human efforts to collect test-ing images and annotate biased attributes (Step 2 in Fig. 1 (a)) for analyzing classifier’s predictions. When one wants to analyze biases in a new domain of images (e.g., object 1In this paper, we use gender to denote visually perceived gender, which
The code is available at https://git.io/J3kMh. does not indicate the person’s true gender identity.
and scene categories in ImageNet [11] and Place365 [56]), massive human efforts of image collection and labeling are needed for each new image domain, which is not scalable. In addition, as a down-stream task of bias identification, many de-biasing methods [7, 10, 45, 54] also require well-defined biased attributes and annotations as inputs and supervisions to mitigate corresponding biases. As a result, if the previous pipeline does not identify the biases due to either negligence of biases or limited annotation budget, the biases will not be mitigated by those de-biasing methods. Therefore, it is urgent to discover unknown AI biases with less human effort.
To this end, we study a novel problem by defining the unknown biased attribute discovery task: for a classifier that predicts a target attribute of the input image, discover its unknown biased attribute. The “target attribute” stands for the attribute for prediction. The “biased attribute” means an attribute that violates the fairness criteria [18, 31] and differs from the target attribute. For example, if a gender classifier has different predictions over female images of different skin colors, then the skin tone attribute is the biased attribute. The
“unknown” has two levels of meanings. First, it indicates that the biased attribute that is expected to be discovered is not presumed by humans, not to mention annotating images with the biased attribute labels. That is, the traditional pipeline in Fig. 1 (a) does not meet the requirement of “unknown.”
Second, “unknown” also implies that human experts may have already known some biased attributes and expect a different one. After completing this task, the discovered biased attributes can be used as inputs for other down-stream tasks, such as algorithmic de-biasing.
We propose a novel framework (Fig. 1 (b)) for this new task by solving two challenges. The first difficulty is how to represent and learn the “attribute” without any presump-tions or labels. To tackle this problem, we base our method on some findings in [23, 46, 48] that the hyperplane in the latent space of generative models can linearly separate an attribute’s values. Hence, we represent the unknown bi-ased attribute as an optimizable hyperplane in a generative model’s latent space (see “biased attribute hyperplane” in
Fig. 1 (b)). Different from previous methods [23, 46, 48] using labels of attribute as the supervision, we propose total variation loss (
LV in Fig. 1 (b)) to optimize the hyperplane that induces the violation of the fairness criterion, without requiring any attribute labels. The second challenge is how to ensure the discovered biased attribute is different from the known ones. We propose orthogonalization penalty ( in
Fig. 1 (b)) to encourage disentanglement between the biased attribute and known attributes. We also use to prevent the biased attribute from being identical with the classifier’s target attribute. Finally, to enable humans to interpret the se-mantic meaning of the optimized hyperplane, a sequence of images, dubbed as traversal images, are generated based on the optimized hyperplane. The variation along the traversal
L⊥
L⊥ images is the semantic meaning of the optimization result.
As shown in Fig. 1, the synthesized traversal face images gradually transform from “young” to “old,” indicating that the biased attribute found by our method is age. In sum-mary, compared with the previous pipeline (Fig. 1 (a)), our framework first lets the optimization actively find the biased attribute (Step 1 in Fig. 1 (b)) and postpones human involve-ment to the final step (Step 2 in Fig. 1 (b)), which not only automatically discovers the unknown biases that human may not realize, but also exempts human efforts from annotating biased attributes on testing images.
We conduct three experiments to verify the effectiveness of our method. In the first experiment, two disentanglement datasets [20, 32] are used for creating large-scale experi-mental settings for evaluation. In the second experiment, we conduct experiments on two face datasets [23, 34] for discovering biased attributes in face attribute classifiers. The first two experiments show that our method can correctly discover the biased attribute. In the third experiment, we apply our method for discovering the biased attribute in other domains of images, such as objects and scenes. The qualitative results and the user study show that our method can discover unnoticeable biases from classifiers pretrained on ImageNet [11] and Place365 [56], proving our method’s generalizability for finding biases in various image domains.
The contributions of this work are as follows. First, we propose a novel unknown biased attribute discovery task for discovering unknown biases from classifiers. Solving the problems in this task can help humans better identify clas-sifiers’ biases. Second, we propose a novel method for this new task by optimizing the total variation loss and the or-thogonalization penalty without any presumptions or labels of biased attributes. Lastly, we design comprehensive exper-iment settings and evaluation metrics to verify the effective-ness of our method, which can also be used as benchmarks for future works. Furthermore, many related fields can be benefited from our new framework for discovering unknown biases, such as algorithmic de-biasing, dataset audition, etc. (more discussions in Appendix H.5). 2.