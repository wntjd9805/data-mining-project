Abstract
This paper studies the problem of learning self-supervised representations on videos. In contrast to image modality that only requires appearance information on objects or scenes, video needs to further explore the relations between multiple frames/clips along the temporal dimension. However, the re-cent proposed contrastive-based self-supervised frameworks do not grasp such relations explicitly since they simply uti-lize two augmented clips from the same video and compare their distance without referring to their temporal relation.
To address this, we present a contrast-and-order representa-tion (CORP) framework for learning self-supervised video representations that can automatically capture both the ap-pearance information within each frame and temporal infor-mation across different frames. In particular, given two video clips, our model first predicts whether they come from the same input video, and then predict the temporal ordering of the clips if they come from the same video. We also propose a novel decoupling attention method to learn symmetric similarity (contrast) and anti-symmetric patterns (order).
Such design involves neither extra parameters nor compu-tation, but can speed up the learning process and improve accuracy compared to the vanilla multi-head attention. We extensively validate the representation ability of our learned video features for the downstream action recognition task on
Kinetics-400 and Something-something V2. Our method out-performs previous state-of-the-arts by a significant margin. 1.

Introduction
In recent years, self-supervised learning methods have be-come increasingly popular for a number of problems, includ-ing masked language models in natural language processing
[7], and jigsaw solving [35], rotation prediction [15], con-trastive learning [20, 5], etc. For vision tasks, to ameliorate the dependence on large amounts of manually annotated data required by fully-supervised learning methods. Among the self-supervised methods in visual tasks, contrastive learning
[20, 5] has shown great potential on image tasks; the transfer-Figure 1. Motivation of this work. Some videos can be recognized by the simple appearance information in a single frame, e.g., “play-ing guitar”,“ riding camel”, and “getting a haircut” categories in the
Kinetics dataset. Some videos of actions have similar appearance, and require complex temporal-level relation understanding. For example, if we play a video of the action “push something from right to left” in reverse, it will not be “push something from left to right”, but “pull something from left to right”. ability of the learned model often even exceeds supervised models for many popular image downstream tasks like detec-tion [38], segmentation [31] and key point estimation [21].
The challenges of labeling are greater for videos, as op-posed to static images, since videos include a time dimen-sion, making them more expensive to collect and annotate.
Consequently, there is greater need for powerful and prac-tical self-supervised learning algorithms to analyze videos.
Recently, [19] and [37] have applied contrastive learning methods to the task of learning representations of videos. In these frameworks, their objective mainly aims to pull rep-resentations of two augmented clips from the same video closer in the embedding space, while those from clips origi-nating from different videos are pushed farther apart.
However, we argue that directly employing these ap-proaches (or simple extensions) is not enough for learning sufficiently detailed information from videos through self-supervised learning, since video-analysis tasks are more complex than image tasks as shown in Fig. 1. Besides cap-turing the static appearance information within each frame (e.g., playing guitar, riding a camel), video learning also needs to understand the relations between multiple frames/-clips (e.g., distinguishing push and pull actions). [48, 49, 55] shows that temporal relations are vital for video learning, while current contrastive solutions do not explicitly involve temporal modeling processes. They only utilize two video clips from the same video and force representations from different timestamps to be similar.
To address this limitation, we propose a Contrast and
Order RePresentation (CORP) framework to incorporate temporal modeling into the self-supervised learning task.
Our idea is conceptually simple: given two video clips, our model first learns whether they come from the same video (contrast), and then classifies which clip happens earlier if from the same video (order). The contrast module extracts the appearance information, such as shapes and edges while the order module models temporal reasoning.
Concretely, we propose our method with two distinct implementations. The first implementation, called CORPm, is illustrated in Figure 2 left (More details will be given later in the paper). Here, we randomly sample 2K augmented video clips from two videos (K clips per video) to form
K(2K − 1) ordered pairs. For each two-clip pair, there are three possible relations between the two clips: 1) they are not from the same videos; 2) they are from the same video, and the first clip in the pair precedes the second in time; 3) they are from the same video, and the second clip in the pair is the one that occurs first in time. Our model is trained to minimize the classification error.
The second implementation CORPf is a twin of CORPm with the SimCLR [5] design (Figure 2 right). Given a batch of B videos, we sample two augmented video clips for each video (2B video clips in total). For each clip, the SimCLR-based method aims to solve a contrastive pretest, i.e. find the clip that is derived from the same video from the remaining (2B −1) clips. Our model further predicts whether the found clip occurs earlier in time than the given clip using an addi-tional objective. Generally, SimCLR framework optimizes (2B − 1)-way classification, while our model converts it to a more challenging (4B − 2)-way classification task.
For different fractions of mismatched pairs (not from the same video) in the training data, the two models learn differ-ent patterns, thus work on different scenarios. In the CORPf (f ewer clips per video) model, the primary task of (2B − 1) classification is more challenging than the within-class or-dering. Therefore, the CORPf model pays more attention to appearance patterns (similar to SimCLR) that enable it to disambiguate same-clip entries, with lower emphasis on tem-poral reasoning patterns. On the other hand, CORPm (more clips per video) model focuses more on temporal relation patterns. The positive and negative pairs for the contrast task are sampled equally, and can hence learn more of temporal patterns from the order task on videos.
Our self-supervised models are validated on two popular benchmark datasets, Kinetics400 and Something-something
V2. We evaluate the learned video representations by lin-ear evaluation [5, 20] following conventional practice, i.e., training a linear classifier with features extracted by the frozen backbone. As shown in Figure 1, the two datasets are different in terms of appearance and temporal relation. Kinet-ics400 dataset is the scenario where appearance information is vital while Something-something is the other scenario where temporal clues are more important. On Kinetics400, our CORPm model performs worse than the contrastive based method CVRL [37], however, our CORPf model can outperform CVRL with a clear margin. On Something-something V2, our CORPf model achieves 41.7% top-1 accuracy, which is a 10% improvement over the contrastive-based method. The CORPm model achieves an even higher accuracy of 48.8%, minimizing the performance gap with supervised learning (58.4%). Our extensive ablation studies verify the effectiveness of our methods, especially that both appearance and temporal relations are learned by our method and they are both vital for video tasks. 2.