Abstract
Low-cost monocular 3D object detection plays a funda-mental role in autonomous driving, whereas its accuracy is still far from satisfactory. In this paper, we dig into the 3D object detection task and reformulate it as the sub-tasks of object localization and appearance perception, which benefits to a deep excavation of reciprocal information un-derlying the entire task. We introduce a Dynamic Feature
Reflecting Network, named DFR-Net, which contains two novel standalone modules: (i) the Appearance-Localization
Feature Reflecting module (ALFR) that first separates task-specific features and then self-mutually reflects the recipro-cal features; (ii) the Dynamic Intra-Trading module (DIT) that adaptively realigns the training processes of various sub-tasks via a self-learning manner. Extensive experiments on the challenging KITTI dataset demonstrate the effective-ness and generalization of DFR-Net. We rank 1st among all the monocular 3D object detectors in the KITTI test set (till
March 16th, 2021). The proposed method is also easy to be plug-and-play in many cutting-edge 3D detection frame-works at negligible cost to boost performance. The code will be made publicly available. 1.

Introduction
Building on the promising progress achieved by 2D ob-ject detection in recent years [27, 26], vision- and LiDAR-based 3D object detection have received increasing atten-tion from both industry and academia due to their critical roles in outdoor autonomous driving [14] and indoor robotic navigation. 3D object detectors based on expensive LiDAR
*indicates equal contribution
Figure 1. Comparison of the baseline method (D4LCN [8]) and our proposed DFR-Net. (a) The baseline method first uses the en-coder (“E”) to extract RGB and predicted depth features and then adopts independent heads to decode the shared features for 3D detection tasks. (b) In our method, we first cluster the sub-tasks with common characteristics to build up separated task streams and then exploit the reciprocal information between the streams via self-mutual feature reflecting. (c) The ground truth, the predic-tion of the baseline method, and our prediction are shown in the bird’s-eye view (BEV) pseudo LiDAR for better visualization. sensors [35, 29, 10, 11] have been widely developed and excelled in 3D object detection, whereas a much cheaper alternative, i.e., monocular 3D object detection, remains an open and challenging research field.
Monocular 3D object detectors can be roughly divided
into three categories according to different input data rep-resentations: RGB image-based, pseudo LiDAR-based, and depth-assisted image-based methods: (i) RGB image-based methods aim to leverage geometry constraints [21] or se-mantic knowledge [4] to explore 2D-3D geometric con-sistency for recovering 3D location and dimension. How-ever, the performance is still far from satisfactory due to the lack of reliable depth prior and the variance of the object scale caused by perspective projection. (ii) Pseudo LiDAR-based methods [32, 33] utilize depth estimation to recon-struct point clouds from image pixels. Afterwards cutting-edge LiDAR-based approaches such as [23, 29] can be di-rectly borrowed. Recent works [32, 33, 37] have demon-strated the effectiveness of pseudo LiDAR-based methods.
Nevertheless, due to the inaccurate depth prediction, a lack of RGB context as well as the inherent difference between real- and pseudo-LiDAR, the performance is limited. (iii)
Depth-assisted methods such as [8, 30] focus on the inte-gration strategy of RGB and depth features, whereas the network is unable to resolve the inferior 3D localization due to the mis-estimated depth map. In other words, the perfor-mance relies heavily on the quality of depth maps.
Humans can get some hints about 3D information even from monocular cues because the brain has the capability to utilize reciprocal visual information from different per-ception tasks [9], e.g., object localization and appearance perception (classification). For example, if we know the category and size of an object, we will know how far away the object is. On the other hand, if we know the localization and fuzzy scale of a distant or occluded unknown object, we may accordingly guess its category.
Inspired by humans’ object perception system, we in-troduce a novel dynamic feature reflecting network for monocular 3D object detection, named DFR-Net. A novel appearance-localization feature reflecting module (ALFR) is designed, where 3D detection tasks are divided into two categories, the appearance perception tasks and the object localization tasks. Distinct tasks are sent to one of the two streams accordingly to delve into the task-specific fea-tures within each task, where reciprocal features between two categories self-mutually reflect. Here the terminol-ogy “reflect” denotes task-wise implicit feature awareness and correlation. To further optimize the multi-task learn-ing, we propose a dynamic intra-trading module, named
DIT, which realigns the training process of two sub-tasks in a self-learning manner. Figure 1 shows the comparison of independent heads (baseline D4LCN [8]) and our pro-posed DFR-Net. DFR-Net exploits and leverages recipro-cal appearance-localization features for 3D reasoning and achieves superior performance. The proposed module is demonstrated to be effective on various image-based and depth-assisted image-based backbone networks (e.g., M3D-RPN [2] and D4LCN [8]).
Our main contributions are summarized as follows:
• We introduce a simple yet effective dynamic feature reflecting network (DFR-Net) for monocular 3D ob-ject detection, which exploits the reciprocal informa-tion underlying the task, allowing the sub-tasks to ben-efit from each other to alleviate the ill-posed problem of monocular 3D perception.
• We present an appearance-localization feature reflect-ing module (ALFR) that first separates two task streams and then self-mutually reflects the subtask-aware features.
• We investigate a dynamic intra-trading module (DIT) that reweights different task losses to realign the multi-task training process in a self-learning manner.
• We achieve new state-of-the-art monocular 3D object detection performance on the KITTI benchmark. The method can be plug-and-play in many other frame-works to boost the performance at negligible cost. 2.