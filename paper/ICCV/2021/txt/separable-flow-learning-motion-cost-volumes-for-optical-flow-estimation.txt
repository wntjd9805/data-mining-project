Abstract
Full-motion cost volumes play a central role in current state-of-the-art optical flow methods. However, constructed using simple feature correlations, they lack the ability to en-capsulate prior, or even non-local knowledge. This creates artifacts in poorly constrained ambiguous regions, such as occluded and textureless areas. We propose a separable cost volume module, a drop-in replacement to correlation cost volumes, that uses non-local aggregation layers to ex-ploit global context cues and prior knowledge, in order to disambiguate motions in these regions. Our method leads both the now standard Sintel and KITTI optical flow bench-marks in terms of accuracy, and is also shown to generalize better from synthetic to real data. 1.

Introduction
Optical flow is the task of estimating per-pixel 2D mo-tion between two images or video frames. This low-level vision task is a fundamental building block of many higher level tasks, such as object tracking, scene reconstruction and video compression. A common approach to this task, used in both hand designed [5, 19] and more modern deep-learning methods [53, 54], is to first compute a cost volume for motions of all pixels, then use this to infer or refine a motion per pixel. While state-of-the-art methods [54, 62] tend to use this approach, it suffers from two key challenges.
First, the cost volume size is exponential in the dimension-ality of the search space. Therefore memory and computa-tion requirements for optical flow, with its 2D search space, grow quadratically with the range of motion. In contrast, such costs for the 1D stereo matching task grow only lin-early with the range of disparity. Secondly, resolving ambi-guities caused by occlusion, lack of texture, or other such is-sues requires a more global, rather than local, understanding of the scene, as well as prior knowledge. Cost volumes gen-erally do not encapsulate such information, leaving the job of resolving such ambiguities to the second stage of each method. As Fig. 1 & 4 illustrate, this makes it harder to
Code: https://github.com/feihuzhang/SeparableFlow (a) Input view (b) Ground truth
RAFT
Ours (c) RAFT [54] output & cost volume (d) Our output & cost volume
Figure 1: Performance illustrations. (a) Input view from Sintel. (b) Ground truth optical flow. (c) The optical flow result and 2D motion cost volume (for a single pixel in the circled region) of the state of the art, RAFT [54]. (d) Result and cost volume (for the same pixel) learned by our Separable Flow. RAFT does not predict motion accurately in the ambiguous regions, such as oc-clusions (highlighted by the circle). Indeed, there are many false peaks in the cost volume for this region. In contrast, Separable
Flow predicts accurate flow results in these challenging regions, by integrating separable, non-local matching cost aggregations. The resulting learned cost volume has one large peak, that correctly matches the ground truth. See sec. 4.2 for more details. compute accurate motion in such regions.
This work proposes a new separable cost volume compu-tation module, which plugs into existing cost-volume-based optical flow frameworks, with two key innovations that ad-dress these challenges. The first is to separate the 2D motion of optical flow into two independent 1D problems, horizon-tal and vertical motion, compressing the 4D cost volume
into two smaller 3D volumes using a self-adaptive separa-tion layer. This factored representation significantly reduces the memory and computing resources required to infer (and thus also learn) the cost volumes, making them linear in the range of motion, without loss in accuracy. Moreover, it enables the second innovation: the use of non-local ag-gregation layers to learn a refined cost volume. Such layers have previously been used for 1D stereo problems [67, 68], where they improve both accuracy in ambiguous regions, and cross-domain generalization. We apply them here to optical flow for the first time, learning cost volumes with non-local, prior knowledge via a one-step motion regres-sion that is able to predict a low-resolution (i.e. 1/8), but high-quality motion. This prediction also serves as a better input to the interpolation and refinement module.
We train and evaluate our Separable Flow module on the standard Sintel [7] and KITTI [16] optical flow datasets. We achieve the current best accuracy among all published opti-cal flow methods on both these benchmarks. Moreover, in the cross-domain case of training on synthetic and testing on real data (i.e. KITTI), our results improve the previous state of the art by a greater margin, even outperforming some
DNN models (e.g. FlowNet2 [28] and PWC-Net [53]) fine-tuned on the target KITTI scenes. We provide an ablation study to show how much of this improvement is attributable to each of our contributions. We reiterate that any optical flow framework that computes a cost volume can benefit from these improvements. 2.