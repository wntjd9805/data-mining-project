Abstract
Deep neural networks are known to be extremely vul-nerable to adversarial examples under white-box setting.
Moreover, the malicious adversaries crafted on the surro-gate (source) model often exhibit black-box transferabil-ity on other models with the same learning task but hav-ing different architectures. Recently, various methods are proposed to boost the adversarial transferability, among which the input transformation is one of the most effec-tive approaches. We investigate in this direction and ob-serve that existing transformations are all applied on a sin-gle image, which might limit the adversarial transferabil-ity. To this end, we propose a new input transformation based attack method called Admix that considers the input image and a set of images randomly sampled from other categories. Instead of directly calculating the gradient on the original input, Admix calculates the gradient on the input image admixed with a small portion of each add-in image while using the original label of the input to craft more transferable adversaries. Empirical evaluations on standard ImageNet dataset demonstrate that Admix could achieve significantly better transferability than existing in-put transformation methods under both single model set-ting and ensemble-model setting. By incorporating with existing input transformations, our method could further improve the transferability and outperforms the state-of-the-art combination of input transformations by a clear margin when attacking nine advanced defense models un-der ensemble-model setting. Code is available at https:
//github.com/JHL-HUST/Admix. 1.

Introduction
A great number of works [7, 2, 1] have shown that deep neural networks (DNNs) are vulnerable to adversarial ex-amples [31, 7], i.e. the malicious crafted inputs that are
*Corresponding author. indistinguishable from the legitimate ones but can induce misclassification on the deep learning models. Such vul-nerability poses potential threats to security-sensitive appli-cations, e.g. face verification [28], autonomous driving [6] and has inspired a sizable body of research on adversar-ial attacks [22, 2, 21, 4, 16, 5, 38, 18]. Moreover, the ad-versaries often exhibit transferability across neural network models [25], in which the adversarial examples generated on one model may also mislead other models. The adver-sarial transferability matters because hackers may attack a real-world DNN application without knowing any informa-tion of the target model. However, under white-box set-ting where the attacker has complete knowledge of the tar-get model, existing attacks [2, 11, 1, 21] have demonstrated great attack performance but with comparatively low trans-ferability against models with defense mechanisms [21, 33], making it inefficient for real-world adversarial attacks.
To improve the transferability of adversarial attacks, var-ious techniques have been proposed, such as advanced gra-dient calculations [4, 18, 35], ensemble-model attacks [19, 15], input transformations [38, 5, 18, 10] and model-specific methods [36]. The input transformation (e.g. randomly re-sizing and padding, translation, scale etc.) is one of the most effective approaches. Nevertheless, we observe that exist-ing methods are all applied on a single input image. Since adversarial attacks aim to mislead the DNNs to classify the adversary into other categories, it naturally inspires us to ex-plore whether we could further enhance the transferability by incorporating the information from other categories.
The mixup operation, that linearly interpolates two ran-dom images and corresponding labels, is firstly proposed as a data augmentation approach to improve the generaliza-tion of standard training [41, 34, 40]. Recently, mixup is also used for inference [24] or adversarial training [12, 14] to enhance the model robustness. Since mixup adopts the information of a randomly picked image, we try to directly adopt mixup to craft adversaries but find that the attack per-formance decays significantly under white-box setting with little improvement on transferability. To craft highly trans-ferable adversaries with the information from other cate-gories but not harm the white-box attack performance, we propose a novel attack method called Admix that calculates the gradient on the admixed image combined with the orig-inal input and images randomly picked from other cate-gories. Unlike mixup that treats the two images equally and mixes their labels accordingly, the admix operation adds a small portion of the add-in image from other categories to the original input but does not change the label. Thus Admix attack could obtain diverse inputs for gradient calculation.
Empirical evaluations on standard ImageNet dataset [26] demonstrate that, compared with existing input transforma-tions [38, 5, 18], the proposed Admix attack achieves sig-nificantly higher attack success rates under black-box set-ting and maintains similar attack performance under white-box setting. By incorporating Admix with other input trans-formations, the transferability of the crafted adversaries could be further improved. Besides, the evaluation of the integrated method under the ensemble-model setting [19] against nine advanced defense methods
[17, 37, 39, 20, 8, 3, 27, 23] demonstrates that the final integrated method, termed Admix-TI-DIM, outperforms the state-of-the-art SI-TI-DIM [18] by a clear margin of 3.4% on average, which further demonstrates the high effectiveness of Admix. 2.