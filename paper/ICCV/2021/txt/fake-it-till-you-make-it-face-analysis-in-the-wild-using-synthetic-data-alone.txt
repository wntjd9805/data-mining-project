Abstract
We demonstrate that it is possible to perform face-related computer vision in the wild using synthetic data alone. The community has long enjoyed the beneﬁts of synthesizing training data with graphics, but the domain gap between real and synthetic data has remained a problem, especially for human faces. Researchers have tried to bridge this gap with data mixing, domain adaptation, and domain-adversarial training, but we show that it is possible to synthesize data with minimal domain gap, so that models trained on syn-thetic data generalize to real in-the-wild datasets. We de-scribe how to combine a procedurally-generated parametric 3D face model with a comprehensive library of hand-crafted assets to render training images with unprecedented real-ism and diversity. We train machine learning systems for face-related tasks such as landmark localization and face parsing, showing that synthetic data can both match real data in accuracy as well as open up new approaches where manual labeling would be impossible. 1.

Introduction
When faced with a machine learning problem, the hardest challenge often isn’t choosing the right machine learning model, it’s ﬁnding the right data. This is especially difﬁ-cult in the realm of human-related computer vision, where concerns about the fairness of models and the ethics of de-ployment are paramount [31].
Instead of collecting and labelling real data, which is slow, expensive, and subject to bias, it can be preferable to synthesize training data using computer graphics [68]. With synthetic data, you can guar-antee perfect labels without annotation noise, generate rich labels that are otherwise impossible to label by hand, and have full control over variation and diversity in a dataset.
Rendering convincing humans is one of the hardest prob-lems in computer graphics. Movies and video games have shown that realistic digital humans are possible, but with
*Denotes equal contribution. https://microsoft.github.io/FaceSynthetics
Figure 1. We render training images of faces with unprecedented realism and diversity. The ﬁrst example above is shown along with 3D geometry and accompanying labels for machine learning. signiﬁcant artist effort per individual [22, 26]. While it’s possible to generate endless novel face images with recent self-supervised approaches [27], corresponding labels for supervised learning are not available. As a result, previous work has resorted to synthesizing facial training data with simpliﬁcations, with results that are far from realistic. We have seen progress in efforts that attempt to cross the domain gap using domain adaptation [60] by reﬁning synthetic im-ages to look more real, and domain-adversarial training [13] where machine learning models are encouraged to ignore differences between the synthetic and real domains, but less work has attempted to improve the quality of synthetic data itself. Synthesizing realistic face data has been considered so hard that we encounter the assumption that synthetic data cannot fully replace real data for problems in the wild [60].
In this paper we demonstrate that the opportunities for synthetic data are much wider than previously realised, and are achievable today. We present a new method of acquiring training data for faces – rendering 3D face models with an unprecedented level of realism and diversity (see Figure 1).
With a sufﬁciently good synthetic framework, it is possible
Template face
+ texture
Figure 2. We procedurally construct synthetic faces that are realistic and expressive. Starting with our template face, we randomize the identity, choose a random expression, apply a random texture, attach random hair and clothing, and render the face in a random environment.
+ environment
+ expression
+ identity
+ clothes
+ hair to create training data that can be used to solve real world problems in the wild, without using any real data at all.
It requires considerable expertise and investment to de-velop a synthetics framework with minimal domain gap.
However, once implemented, it becomes possible to gener-ate a wide variety of training data with minimal incremental effort. Let’s consider some examples; say you have spent time labelling face images with landmarks. However, you suddenly require additional landmarks in each image. Re-labelling and verifying will take a long time, but with syn-thetics, you can regenerate clean and consistent labels at a moment’s notice. Or, say you are developing computer vision algorithms for a new camera, e.g. an infrared face-recognition camera in a mobile phone. Few, if any, hardware prototypes may exist, making it hard to collect a dataset.
Synthetics lets you render faces from a simulated device to develop algorithms and even guide hardware design itself.
We synthesize face images by procedurally combining a parametric face model with a large library of high-quality artist-created assets, including textures, hair, and clothing (see Figure 2). With this data we train models for common face-related tasks: face parsing and landmark localization.
Our experiments show that models trained with a single generic synthetic dataset can be just as accurate as those trained with task-speciﬁc real datasets, achieving results in line with the state of the art. This opens the door to other face-related tasks that can be conﬁdently addressed with synthetic data instead of real.
Our contributions are as follows. First, we describe how to synthesize realistic and diverse training data for face anal-ysis in the wild, achieving results in line with the state of the art. Second, we present ablation studies that validate the steps taken to achieve photorealism. Third is the synthetic dataset itself, which is available from our project webpage: https://microsoft.github.io/FaceSynthetics. 2.