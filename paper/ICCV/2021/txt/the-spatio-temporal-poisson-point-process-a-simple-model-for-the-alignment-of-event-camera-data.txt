Abstract
Event cameras, inspired by biological vision systems, provide a natural and data efficient representation of vi-sual information. Visual information is acquired in the form of events that are triggered by local brightness changes.
However, because most brightness changes are triggered by relative motion of the camera and the scene, the events recorded at a single sensor location seldom correspond to the same world point. To extract meaningful informa-tion from event cameras, it is helpful to register events that were triggered by the same underlying world point. In this work we propose a new model of event data that captures its natural spatio-temporal structure. We start by develop-ing a model for aligned event data. That is, we develop a model for the data as though it has been perfectly reg-istered already. In particular, we model the aligned data as a spatio-temporal Poisson point process. Based on this model, we develop a maximum likelihood approach to reg-istering events that are not yet aligned. That is, we find transformations of the observed events that make them as likely as possible under our model. In particular we extract the camera rotation that leads to the best event alignment.
We show new state of the art accuracy for rotational veloc-ity estimation on the DAVIS 240C dataset [20]. In addition, our method is also faster and has lower computational com-plexity than several competing methods. Code: https:
//github.com/pbideau/Event-ST-PPP 1.

Introduction
Inspired by biological vision systems, event cameras [14, 25, 33, 4, 5] mimic certain biological features of the hu-man vision system, such as recording brightness changes as events, asynchronously, and at high temporal resolution. (a) Method overview (b) Video frame (c) Unaligned events,
‘blurred’ event image. (d) Aligned events, sharp event image.
Figure 1. Alignment of event data by maximizing the joint prob-ability of a set of events pA(Rω(O)). Top row: Events are plotted in red/blue depending on their polarity. The projection of events onto the 2D image plane is shown in black - indicating the qual-ity of their alignment over time. Aligned events projected onto 2D lead to sharp edge map, where as unaligned events are dis-persed over the image plane. Bottom row: video frame, accumu-lated events, accumulated aligned events.
This relatively new way of acquiring visual information dif-fers significantly from classical frame-based video record-ings, leading to new research directions in computer vision and drawing close connections to robotics and the cogni-tive sciences. Prior work has shown that event data is rich enough to recover high quality brightness images, even in high-speed and high dynamic range (HDR) scenarios [26], and it allows early stage information processing such as mo-tion perception and recognition [35, 12]. Despite these ad-vantages, current vision algorithms still struggle to unlock the benefits of events cameras.
The problem of aligning event camera data. In this pa-per we focus on event camera data that comes from a mov-ing camera in a static or nearly static environment. Because of the camera motion, as the camera records events through time, the events at a fixed camera pixel correspond to differ-ent points in the world. Conversely, many events recorded at different sensor pixel locations are corresponding to the same world point. This makes it more difficult to interpret event camera data. Finding transformations of the events that map each event triggered by the same world point to the same pixel location of the camera sensor can be called alignment or registration of the events. In this paper, we propose a method for alignment based on a new probabilis-tic model for event camera data.
Panoramas of events. To describe our model and al-gorithm, we draw analogies with image panoramas created using RGB images. By warping a set of images taken from different camera positions into the same shared set of co-ordinates, a set of images may be combined into a larger composite image, or panorama, of a scene.
The same idea can be applied to event data: transforming the location of each individual event so that it is transformed into a shared coordinate system [27, 11].1 Doing this with event data is challenging, since it is more difficult to estab-lish correspondences in event data than among images.
Instead, many approaches to registering event camera data are based upon a simple intuitive observation [19, 30, 15, 21, 7]. If we form an “aggregate” event camera image by simply recording the number of events at each pixel over some period of time, then these aggregate images tend to be sharper when the events are well-aligned (Figure 1(d)), and blurrier when the events are less well-aligned (Figure 1(c)).
Leveraging this observation, one tries to find a set of trans-formations that maximize the sharpness of the aggregate image. These methods, discussed in detail in the related work section, mostly differ in their definition of defining sharpness, i.e., in their loss functions.
Congealing and probabilistic models of alignment. In this paper, we introduce a new, more effective method for event alignment.
It is related to a probabilistic method for aligning traditional images known as congealing [13], which does not use any explicit correspondences. Instead, one measures the degree to which a set of images are jointly aligned. To measure the quality of the joint image align-ment, one considers the entropy of the set of pixels at each image location. If a location has the same pixel value across all of the images, it has minimum entropy. If it has many different pixel values, it has high entropy. By transform-ing the images so that the sum of these pixelwise entropies 1In the event camera literature, the term ‘panorama’ is usually applied to alignment over sequences in which the camera has large displacements, resulting in a panorama much larger than a single camera frame. However, the same term can be applied to registering short sequences of event camera data, which creates panoramas only slightly larger than a single frame. is minimized, the images naturally move into alignment.
Since minimizing entropies is equivalent to maximizing pixel likelihoods under a non-parametric distribution, con-gealing can also be seen as a maximum likelihood method (see [13] for more details).
Contributions. We present a novel probabilistic model for event camera data. It allows us to evaluate the likeli-hood of the event data captured at a particular event camera pixel. By introducing transformations to move the data into a common coordinate system, we show that by maximiz-ing the likelihood of the data under this model with respect to these transformations, we naturally retrieve an accurate registration of the event data. That is, we develop a prob-abilistic, maximum likelihood method for the joint align-ment of event data. We support this novel approach by pro-viding new state-of-the-art results. We have substantially higher accuracy than recently published methods, and are also among the fastest. In addition, we reassess how eval-uations on these de facto benchmarks are done, and argue that a new approach is needed. 2.