Abstract
In this work, we propose a camera self-calibration algo-rithm for generic cameras with arbitrary non-linear distor-tions. We jointly learn the geometry of the scene and the ac-curate camera parameters without any calibration objects.
Our camera model consists of a pinhole model, a fourth or-der radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While tradi-tional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric con-sistency. This requires learning the geometry of the scene, and we use Neural Radiance Fields (NeRF). We also pro-pose a new geometric loss function, viz., projected ray dis-tance loss, to incorporate geometric consistency for com-plex non-linear camera models. We validate our approach on standard real image datasets and demonstrate that our model can learn the camera intrinsics and extrinsics (pose) from scratch without COLMAP initialization. Also, we show that learning accurate camera models in a differen-tiable manner allows us to improve PSNR over baselines.
Our module is an easy-to-use plugin that can be applied to
NeRF variants to improve performance. The code and data are currently available at https://github.com/POSTECH-CVLab/SCNeRF 1.

Introduction
Camera calibration is one of the crucial steps in com-puter vision. Through this process, we learn how the incom-ing rays map to pixels and thus connect the images to the physical world. Thus, it is a fundamental step in many ap-plications such as autonomous driving, robotics, augmented reality, and many more.
Camera calibration is typically done by placing calibra-tion objects (e.g., a checkerboard pattern) in the scene and estimating the camera parameters using the known geome-try of the calibration objects. However, in many cases, cal-ibration objects are not readily available and can interfere with the perception tasks when cameras are deployed in the wild. Thus, calibrating without any external objects, or self-calibration, has been an important research topic; ﬁrst pro-posed in Faugeras et al. [4]. The paper has spurred many follow-ups, some of which propose to globally optimize or embed constraints into the self-calibration optimization pro-cess [14, 24, 1, 2].
Although there has been much progress in developing self-calibration algorithms, all these methods have limita-tions: 1) the camera model used in self-calibration is a sim-ple linear pinhole camera model. This camera-model design cannot incorporate generic non-linear camera noise that is prevalent in all commodity cameras resulting in less accu-rate camera calibration. 2) self-calibration algorithms use only a sparse set of image correspondences, and direct pho-tometric consistency has not been used for self-calibration. 3) they use correspondences from a non-differentiable pro-cess and do not improve the 3D geometry of the objects, which could improve the camera model. Let us discuss each limitation in detail.
First, a linear pinhole camera model can be formulated 1
as Kx where K ∈ R3×3 and x is a homogeneous 3D coor-dinate. This linear model can simplify a camera model and computation, but real lenses have complex non-linear dis-tortions which allow capturing accurate mapping between the real world and images [5, 15, 20, 18]. However, tra-ditional self-calibration algorithms assume linear camera models for computational efﬁciency at the cost of accuracy.
Second, conventional self-calibration methods solely rely on the geometric loss or constraints based on the epipo-lar geometry, such as Kruppa’s method [9, 7, 11] that only uses a set of sparse correspondences extracted from a non-differentiable process. This could lead to diverging results with extreme sensitivity to noise when a scene does not have enough interest points. On the other hand, photomet-ric consistency is a physically-based constraint that forces the same 3D point to have the same color in all valid view-points. It can create a large number of physically-based con-straints to learn accurate camera parameters.
Lastly, conventional self-calibration methods use an off-the-shelf non-differentiable feature matching algorithm and do not improve or learn the geometry. It is well known that the better we know the geometry of the scene, the more ac-curate the camera model gets. This fact is essential since the geometry of the scene is the sole source of input for self-calibration.
In this work, we propose a self-calibration algorithm for generic camera models that end-to-end learn parameters for the basic pinhole model and radial distortion and non-linear camera noise. For this, our algorithm jointly learns geome-try together with a uniﬁed end-to-end differentiable frame-work that allows better geometry to improve camera param-eters. In particular, we use the implicit volumetric represen-tation or Neural Radiance Fields [13] for the differentiable scene geometry representation.
We also propose a geometric consistency designed for our camera model and train the system together with the photometric consistency for self-calibration, which pro-vides a large set of constraints. The novel geometric consis-tency forces rays from corresponding points on images to be close to each other, which overcomes the pinhole cam-era assumption in the conventional geometric losses derived from Kruppa’s method [9, 7, 11] for self-calibration [24].
Experimentally, we show that our models can learn cam-era parameters, including intrinsics and extrinsics, without the standard COLMAP initialization. Also, when the ini-tialization values for these camera parameters are given, we
ﬁne-tune the camera parameters accurately, which improves the underlying geometry and novel view synthesis. We test our model on ﬁsh-eye images with COLMAP learned cam-era radial distortion parameters to analyze the distortion model and show that our model outperforms the baselines by a signiﬁcant margin. In addition, we show that our non-linear camera model is modular and can be applied to NeRF variants such as NeRF [23] and NeRF++ [25]. 2.