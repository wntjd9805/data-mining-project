Abstract
In egocentric videos, the face of a wearer capturing the video is never captured. This gives a false sense of se-curity that the wearer’s privacy is preserved while shar-ing such videos. However, egocentric cameras are typi-cally harnessed to wearer’s head, and hence, also capture wearer’s gait. Recent works have shown that wearer gait signatures can be extracted from egocentric videos, which can be used to determine if two egocentric videos have the same wearer. In a more damaging scenario, one can even recognize a wearer using hand gestures from egocen-tric videos, or identify a wearer in third person videos such as from a surveillance camera. We believe, this could be a death knell in sharing of egocentric videos, and fatal for egocentric vision research. In this work, we suggest a novel technique to anonymize egocentric videos, which cre-ate carefully crafted, but small, and imperceptible optical
ﬂow perturbations in an egocentric video’s frames. Impor-tantly, these perturbations do not affect object detection or action/activity recognition from egocentric videos but are strong enough to dis-balance the gait recovery process. In our experiments on benchmark EPIC-Kitchens dataset, the proposed perturbation degrades the wearer recognition per-formance of [42], from 66.3% to 13.4%, while preserving the activity recognition performance of [10] from 89.6% to 87.4%. To test our anonymization with more wearer recognition techniques, we also developed a stronger, and more generalizable wearer recognition method based on camera egomotion cues. The approach achieves state-of-the-art (SOTA) performance of 59.67% on EPIC-Kitchens, compared to 55.06% by [42]. However, the accuracy of our recognition technique also drops to 12% using the proposed anonymizing perturbations. 1.

Introduction
Egocentric videos are typically captured through wear-able cameras harnessed on a person’s head, recording in a hand-free style, without any explicit user interven-tion. The unique perspective gives a wearer ability to share his/her experience as seen through his/her own eyes.
Figure 1: Wearer recognition from egocentric videos is a critical privacy breach. In this paper we propose to add carefully crafted, but subtle 3D rotations in the egocentric videos which can lead to the failure of wearer recognition techniques. Row 1 shows orig-inal frames of an egocentric video, whereas 3rd rows shows the frames after proposed transformation. Note that the transforma-tions are imperceptible. We show in our experiments that they also do not affect the performance of other video analysis tasks. On the other hand, naively adding random rotations (row 2) causes vi-sual distortion (note the red portions near the borders due to large rotations), but does not lead to signiﬁcant deterioration in wearer recognition ability as shown in our experiments.
The potential utility of such cameras in applications like law-enforcement, geriatric care, and life-logging has also spawned serious research in egocentric videos. The com-puter vision community has responded with a variety of large scale publicly available egocentric video datasets viz.
FPSI [8], EGTEA [22], and EPIC-Kitchens [5]. Some notable tasks in egocentric vision taken-up so-far include egocentric video summarization, temporal segmentation, as well as object, action, and activity recognition from ﬁrst-person viewpoint [16, 45, 20, 33, 28, 19, 9, 46, 15, 44, 24].
While the wearable, hands-free, and always-on nature of egocentric cameras have been one of the reasons for their popularity, they have also brought in unique privacy con-cerns. Hence, both general users, as well as researchers, have been careful in not collecting the videos in private places such as bedrooms or toilets. However, outdoor ego-centric videos, and especially the scenarios when no other person is seen in the videos are generally considered safe and shared freely. This has been due to the popular per-ception that since wearer’s face is not visible in the ego-centric videos, hence there is no privacy risk in public shar-ing of such videos. Hoshen and Peleg [15] have shattered this false belief, and showed that one could train a classi-ﬁer over optical ﬂow to identify the camera wearer of an egocentric video. Recently, Thapar et al. [41] have further extended the privacy breach to an open-set scenario. Their result is also more alarming, since they show the capability to cross-recognize the wearer gait captured from egocentric videos against a gait captured from a third-person surveil-lance video. This allows them to even know the face behind an egocentric video! In an extension of their work, the au-thors have shown a similar capability using hand gestures as seen in egocentric videos [42].
It is important to note that the problem does not exist in videos captured from hand-held, or third-person cameras.
Firstly, these cameras do not capture the photographer’s gait or any other biometric proﬁle. Secondly, when the face of a person is visible in such a video, it is easy to mask it out by blurring or a similar technique. In the case of egocen-tric videos, the privacy breach is happening through ﬁne-grained frame-to-frame optical ﬂow, and it is not clear what can be masked out to prevent the leak.
Such a serious and well-exposed privacy breach de-mands an immediate solution. Once the community heeds to this warning seriously, sharing egocentric video data at a large scale would become impossible. This can be a huge potential threat for ongoing egocentric research as most of the recent results in the ﬁeld have been obtained through data-intensive deep neural network architectures.
The focus of this work is to generate a perturbation to transform any given egocentric video (vi), corresponding to some camera wearer (i) into an anonymized egocen-tric video (˜vi), which blocks wearer recognition techniques from extracting gait signatures of the wearer i from ˜vi. To preserve the utility of these videos, such a transformation must not signiﬁcantly modify the video such that, (a) al-teration should be imperceptible, and (b) the transformed video (˜vi) should remain usable for other egocentric video analysis tasks viz. object, or activity recognition. Such an anonymizing transformation for egocentric videos is possi-ble because most analysis tasks, e.g., activity or attribute recognition performed over egocentric videos, typically do not need subject-speciﬁc features/cues. To the contrary, the techniques for these tasks should be identity agnostic (ir-respective of the identity of the camera wearer) to avoid dataset over-ﬁtting. Hence identity features can be safely suppressed or perturbed without affecting the performance of other egocentric video analysis tasks of interest.
A candidate solution for anonymizing transformation is the addition of noise to the egocentric videos. Here, we note that most state-of-the-art (SOTA) egocentric video analy-sis techniques are based on deep neural networks (DNNs), and are trained on original and augmented data (with noise).
Hence, they are already resistant to some amount of noise in the data. The addition of a large amount of noise may block wearer identity recognition. However, it may also cause signiﬁcant collateral damage by degrading the over-all video quality, and adversely affecting the performance of other egocentric video analysis tasks as well.
Contributions: The speciﬁc contributions of this paper are: (1) We propose a novel video transformation technique to prevent privacy leak in egocentric videos by blocking wearer recognition, but without affecting performance of other video analysis tasks. This plugs an immediate and ur-gent security hole exposed by recent works and restores the safe sharing of egocentric datasets. (2) We show that the proposed transformation degrades the wearer recognition performance of [42] on benchmark EPIC-Kitchens dataset from 66.3% to 13.4%, whereas recognition performance of
[41] degrades from 85.5% to 15.8% on FPSI dataset [8].
On the other hand, despite such degradation in recognition performance, the activity recognition accuracy of the SOTA
[10] on perturbed videos merely changes from 92% to 91%. (3) To further consolidate our work, we propose a novel wearer recognition technique utilizing self-attention and ar-c-softmax loss. The technique is more generalizable, and achieves SOTA performance of 59.67% in comparison to 55.06% by [42]. However, the proposed perturbation causes our recognition technique to also fail causing drop in accu-racy to 12%. 2.