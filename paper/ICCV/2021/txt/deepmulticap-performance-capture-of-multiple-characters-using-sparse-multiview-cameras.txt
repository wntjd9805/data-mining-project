Abstract
We propose DeepMultiCap, a novel method for multi-person performance capture using sparse multi-view cam-eras. Our method can capture time varying surface de-tails without the need of using pre-scanned template mod-els. To tackle with the serious occlusion challenge for close interacting scenes, we combine a recently proposed pixel-aligned implicit function with parametric model for robust reconstruction of the invisible surface areas. An effec-tive attention-aware module is designed to obtain the ﬁne-grained geometry details from multi-view images, where high-ﬁdelity results can be generated.
In addition to the spatial attention method, for video inputs, we further pro-pose a novel temporal fusion method to alleviate the noise and temporal inconsistencies for moving character recon-struction. For quantitative evaluation, we contribute a high quality multi-person dataset, MultiHuman, which con-sists of 150 static scenes with different levels of occlusions and ground truth 3D human models. Experimental results demonstrate the state-of-the-art performance of our method and the well generalization to real multiview video data, which outperforms the prior works by a large margin. 1.

Introduction
Recent years have witnessed great progress in vision-based human performance capture, which is promising to
* Equal contribution
Code and dataset available: http://liuyebin.com/dmc/dmc.html enable various applications (e.g., tele-presence, sportscast, gaming and mixed reality) with enhanced interactive and immersive experiences. To achieve surprisingly detailed geometry and texture reconstruction, dense camera rigs even equipped with sophisticated lighting systems are in-troduced [56, 8, 29, 28, 20, 2]. However, the extremely expensive and professional setups limited their popular-ity. Although other light-weight multi-view human per-formance capture systems have achieved impressive re-sults even in real-time, they still relies on pre-scanned tem-plates [36, 35], custom-designed RGBD [15, 13] or com-mercial RGBD [61, 62] sensors, or limited to single-person reconstruction [51, 18, 25, 19].
Beneﬁting from the fast improvement of deep implicit functions for 3D representations, recent methods [47, 48, 32] are able to recover the 3D body shape only from a sin-gle RGB image. Compared with the voxel-based [53, 69] or mesh-based [40, 1] representations, an implicit function guides the deep learning models to notice geometric details in a more efﬁcient way. Speciﬁcally, PIFu [47, 32] achieves plausible single human reconstruction using only RGB im-ages, and PIFuHD [48] further utilizes normal maps and high resolution images to generate more detailed results.
Despite the prominent performance in digitizing 3D human body, both PIFu [47] and PIFuHD [48] suffer from several drawbacks when extending the frameworks to multi-person scenarios and multi-view setups. Firstly, the average-pooling-based multi-view feature fusion strat-egy in PIFu will lead to over-smoothed outputs when high frequency details (e.g., normal maps) are included in multi-1
view features. More importantly, in the two approaches, reconstruction results are only promised with ideal input images without severe occlusions in multi-person perfor-mance capture scenarios. The reconstruction performance of [47, 48] will be signiﬁcantly deteriorated due to the lack of observations caused by severe occlusions.
To address the aforementioned problems, we propose a novel framework to perform multi-person reconstruction from multi-view images. First of all, inspired by [54], we design an spatial attention-aware module to adaptively aggregate information from multi-view inputs. The mod-ule is effective to capture and merge the geometric de-tails from different view points, and ﬁnally contributing to the signiﬁcant improvement of results under multi-view se-tups. Moreover, for multi-person reconstruction, we further combine the attention module with parametric models, i.e.,
SMPL to enhance the robustness while maintaining the ﬁne-grained details. The SMPL model serves as a 3D geome-try proxy which compensates for the missing information where occlusions take place. With the semantic informa-tion provided by SMPL, the network is capable of recon-structing complete human bodies even under close interac-tive scenarios. Finally, when dealing with moving charac-ters from video, we propose a temporal fusion method by weighting the signed distance ﬁeld (SDF) across the time domain, which further enhance the temporal consistency of the reconstructed dynamic 3D sequences.
Another urgent problem is that the lack of high-quality scans of multi-person interactive scenarios in the commu-nity makes it difﬁcult for accurately evaluating multi-person performance capture systems like ours. To ﬁll this gap and better evaluate the performance of our system, we con-tribute a novel dataset, MultiHuman, which consists of 150 high-quality scans with each containing from 1 to 3 multi-person interactive actions (including both natural and close interactions). The dataset is further divided into several cat-egories according to the level of occlusions and number of persons in the scene, where a detailed evaluation can be conducted. Experimental results demonstrate the state-of-the-art performance and well generalization capacity of our approach. In general, the main contribution in this work can be summarized as follows:
• We propose a novel framework for high-ﬁdelity multi-view reconstruction for multi-person interactive sce-narios. By leveraging the human shape and pose prior for resolving the ambiguities introduced by severe oc-clusions, we achieve the state-of-the-art performance even with partial observations in each view.
• We design an efﬁcient spatial attention-aware mod-ule to obtain ﬁne-grained details for multi-view se-tups, and introduce a novel temporal fusion method to reduce the reconstruction inconsistencies for moving characters from video inputs.
• We contribute an extremely high quality 3D model dataset containing of 150 multi-person interacting scenes. The dataset can be used for training and evalu-ation of related topics in future research. 2.