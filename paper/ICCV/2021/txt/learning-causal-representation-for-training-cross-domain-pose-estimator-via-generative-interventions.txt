Abstract 3D pose estimation has attracted increasing attention with the availability of high-quality benchmark datasets.
However, prior works show that deep learning models tend to learn spurious correlations, which fail to generalize be-yond the specific dataset they are trained on. In this work, we take a step towards training robust models for cross-domain pose estimation task, which brings together ideas from causal representation learning and generative adver-sarial networks. Specifically, this paper introduces a novel framework for causal representation learning which ex-plicitly exploits the causal structure of the task. We con-sider changing domain as interventions on images under the data-generation process and steer the generative model to produce counterfactual features. This help the model learn transferable and causal relations across different domains.
Our framework is able to learn with various types of unla-beled datasets. We demonstrate the efficacy of our proposed method on both human and hand pose estimation task. The experiment results show the proposed approach achieves state-of-the-art performance on most datasets for both do-main adaptation and domain generalization settings. 1.

Introduction 3D pose estimation has been attracting increasing atten-tion due to its numerous applications in human-computer interaction, action recognition and privacy preservation ap-plication [9, 59, 68]. In recent years, the deep learning mod-els have achieved tremendous improvement with advance in model architecture [7, 17, 57], novel loss functions [24, 35], and availability of quality datasets [6, 36, 21]. Despite its success, existing methods still struggle to generalize beyond the domain of training data, where a well-trained model is unable to detect precise joints locations in unfamiliar sub-∗Corresponding authors
Figure 1: Overview of the training process of robust pose estimator with generative interventions. Given a set of do-mains and content, we train a generator that produce coun-terfactual features to intervene the training of an estimator. jects or unseen views (i.e. cross-domain pose estimation).
The deficiency on cross-domain pose estimation can be attributed to dataset biases [61] or shortcut learning [11], which means that deep learning models are prone to learn dataset-dependent spurious correlations based on statistical associations [1, 2, 4, 20, 48]. This characteristic becomes problematic when the correlations are not consistent across domains. For 3D pose estimation task, an example of spu-rious correlation could be the connection between the ap-pearance of clothes/skin and joints. Generally, this is not a problem during the inference stage as long as the data fol-lows the same distribution. However, the test samples could comprise individuals with skin color or clothes that are dif-ferent from the training dataset. Hence, the trained model’s performance might not be as good as expected.
Prior works show that generalizing beyond training do-main requires a model to learn not only the statistical as-sociations between variables, but also the underlying causal relations [52]. Causal relations reflect the fundamental data-generating mechanism, which tends to be universal and in-variant across different domains [49], and provides the most transferable and confident information to unseen domains.
For example, composing a shot on photography involves both content (e.g. person, object, etc.) and a specific domain
(e.g. background, viewpoint, etc.). Even though the domain may differ, the photo’s semantics would remain consistent as long as the content is unchanged. The goal of causal rep-resentation learning is to learn a representation exposing the causal relation which is invariant under different interven-tions. This allows a learning framework to train predictive models that are robust against the changes in domain that naturally occur in the real world.
In this paper, we propose a novel method for learning causal representations, which is subsequently used to train a robust model for cross-domain pose estimation task. The proposed method is based on the observation that the causal generative process of an image, which assumes the data is constructed from a content variable and a domain variable, is domain- or dataset-invariant. Building on prior work
[12, 18], we consider changing the domain variable as an intervention on the images. We then do such interventions by steering the generative models to produce counterfactual features from a specified content and random noise. Finally, by enforcing similarity between the distribution of represen-tations learned with different interventions, the model can learn transferable and causal relations across different do-mains. An overview of the pose estimator training with the counterfactual representation is shown in Figure 1.
The main contributions of our work are as follows:
• We propose a novel framework for causal representation learning to generate out-of-distribution features. We ex-plicitly exploit the causal structure of the task and show how to learn causal representations by steering the gen-erative model to produce counterfactual features, which simulates domain interventions on images.
• We demonstrate the effectiveness of the counterfactual feature generator by utilizing the generated features to train models for pose estimation task. Not only can our method enhance the cross-domain pose estimation per-formance (i.e. train with both source domain data and un-labeled target domain data), but also generalize well to domain generalization setting (i.e. train with both source domain data and unlabeled unconstrained dataset).
• We conduct experiments on both human pose and hand pose estimation task. The ablation studies examine var-ious components of the proposed framework, as well as the impact of different mixture of training datasets. We also discuss why increasing source dataset and interven-tion can improve performance. 2.