Abstract 1.

Introduction
Occlusions pose a significant challenge to optical flow algorithms that rely on local evidences. We consider an oc-cluded point to be one that is imaged in the reference frame but not in the next, a slight overloading of the standard def-inition since it also includes points that move out-of-frame.
Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work re-lies on CNNs to learn occlusions, without much success, or requires multiple frames to reason about occlusions us-ing temporal smoothness. In this paper, we argue that the occlusion problem can be better solved in the two-frame case by modelling image self-similarities. We introduce a global motion aggregation module, a transformer-based approach to find long-range dependencies between pixels in the first image, and perform global aggregation on the corresponding motion features. We demonstrate that the optical flow estimates in the occluded regions can be sig-nificantly improved without damaging the performance in non-occluded regions. This approach obtains new state-of-the-art results on the challenging Sintel dataset, improv-ing the average end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At the time of submission, our method ranks first on these benchmarks among all pub-lished and unpublished approaches. Code is available at https://github.com/zacjiang/GMA.
How can we estimate the 2D motion of a point we only see once? This is the problem faced by optical flow al-gorithms for points that become occluded between frames.
Estimating the optical flow, that is, the apparent motion of pixels in an image as the camera and scene move, is a clas-sic problem in computer vision studied since the seminal work of Horn and Schunck [14]. There are many factors that make optical flow prediction a hard problem, including large motions, motion and defocus blur, and featureless re-gions. Among these challenges, occlusion is one of the most difficult and under-explored. In this paper, we propose an approach that specifically targets the occlusion problem in the case of two-frame optical flow prediction.
We first define what we mean by occlusion in the con-text of optical flow estimation. In this paper, an occluded point is defined as a 3D point that is imaged in the reference frame but is not visible in the matching frame. This defi-nition incorporates several different scenarios, such as the query point moving out-of-frame or behind another object (or itself), or another object moving in front of the query point, in the active sense. One particular case of occlusion is shown in Figure 1, where part of the blade moves out-of-frame.
The challenge posed by occlusions can be understood by looking at the underlying assumptions of optical flow algo-rithms. Traditional optical flow algorithms apply the bright-Reference Frame
Matching Frame
Flow (RAFT [38])
Flow (Ours)
Flow (Ground Truth)
Error (RAFT [38])
Error (Ours)
Figure 2. Recovering hidden motions. In row 1, the bottom left corner of the ground moves out-of-frame, but reasoning that it belongs to the background allows the motion to be recovered from other parts of the image. In row 2, the girl’s staff is mostly occluded in the second frame, but strong cues from the visible parts can resolve its motion. Our approach can estimate many hidden motions despite the presence of occlusions. The flow maps and the error maps have been fetched from the Sintel server [8]. Best viewed in colour on a screen. ness constancy constraint [14], where pixels related by the flow field are assumed to have the same intensities.
It is clear that occlusions are a direct violation of such a con-straint. In the deep learning era, correlation (cost) volumes
[15] are used to give a matching cost for each potential dis-placement of a pixel. However, correlations of appearance features are unable to give meaningful guidance for learning the motion of occluded regions. Most existing approaches use smoothness terms in an MRF to interpolate occluded motions [9] or use CNNs to directly learn the neighbouring relationships, hoping to learn to estimate occluded motions based on the neighbouring pixels [38, 36]. However, state-of-the-art methods still fail to estimate occluded motions correctly when occlusions are more significant and local ev-idence is insufficient to resolve the ambiguity.
In contrast, humans are able to synthesise information from across the image and apply plausible motion mod-els to accurately estimate occluded motions. This capabil-ity is valuable to emulate, because we fundamentally care about recovering the real 3D motion of objects in a scene, for which estimating occluded motion is necessary. Down-stream applications, including tracking and activity detec-tion [23], can also benefit from short-term predictions of the motion of occluded points, particularly if they reappear later or exhibit some characteristic of interest (e.g., high-velocity out-of-frame motions).
Let us consider how to estimate these hidden motions for the two-frame case. When direct (local) matching informa-tion is absent, the motion information has to be propagated from other pixels. Using convolutions to propagate this in-formation has the drawback of limited range since convo-lution is a local operation. We propose to aggregate the motion features with a non-local approach. Our design is based on the assumption that the motions of a single object (in the foreground or background) are often homogeneous.
One source of information that is overlooked by existing works is self-similarities in the reference frame. For each pixel, understanding which other pixels are related to it, or which object it belongs to, is an important cue for accu-rate optical flow predictions. That is, the motion informa-tion of non-occluded self-similar points can be propagated to the occluded points.
Inspired by the recent success of transformers [39], we introduce a global motion aggrega-tion (GMA) module, where we first compute an attention matrix based on the self-similarities of the reference frame, then use that attention matrix to aggregate motion features.
We use these globally aggregated motion features to aug-ment the successful RAFT [38] framework and demonstrate new state-of-the-art results in optical flow estimation, such as those examples in Figure 2.
The key contributions of our paper are as follows. We show that long-range connections, implemented using the attention mechanism of transformer networks, are highly beneficial for optical flow estimation, particularly for re-solving the motion of occluded pixels where local infor-mation is insufficient. We show that self-similarities in the reference frame provide an important cue for selecting the long-range connections to prioritise. We demonstrate that our global motion feature aggregation strategy leads to a significant improvement in optical flow accuracy in oc-cluded regions, without damaging the performance in non-occluded regions, and analyse this extensively. We improve the average end-point error (EPE) by 13.6% (2.86 → 2.47) on Sintel Final and 13.7% (1.61 → 1.39) on Sintel Clean, compared to the strong baseline of RAFT [38]. Our ap-proach ranks first on both datasets at the time of submission. 2.