Abstract
Self-supervised learning holds promise in leveraging large amounts of unlabeled data, however much of its progress has thus far been limited to highly curated pre-training data such as ImageNet. We explore the effects less-curated image of contrastive learning from larger, datasets such as YFCC, and find there is indeed a large dif-ference in the resulting representation quality. We hypoth-esize that this curation gap is due to a shift in the distri-bution of image classes—which is more diverse and heavy-tailed—resulting in less relevant negative samples to learn from. We test this hypothesis with a new approach, Divide and Contrast (DnC), which alternates between contrastive learning and clustering-based hard negative mining. When pretrained on less curated datasets, DnC greatly improves the performance of self-supervised learning on downstream tasks, while remaining competitive with the current state-of-the-art on curated datasets. 1.

Introduction
Recent developments in self-supervised learning have shown that it is possible to learn high-level representa-tions of object categories from unlabeled images [39, 43, 88, 12, 97], phonetic information from speech [69, 79] and language understanding from raw text [21, 101]. The most studied benchmark in self-supervised learning is Ima-geNet [20], where representations learned from unlabeled images can surpass supervised representations, both in terms of their data-efficiency and transfer-learning perfor-mance [13, 34].
One of the caveats with self-supervised learning on Im-ageNet is that it is not completely “self-supervised”. The training set of ImageNet, on which the representations are learned, is heavily curated and required extensive human ef-fort to create [20]. In particular, ImageNet contains many fine-grained classes (such as subtly different dog breeds), each one containing roughly the same number of images.
While this consistency may facilitate the learning of high-level visual representations, limiting self-supervised learn-ing to such curated datasets risks biasing their development
Figure 1. Linear evaluation on ImageNet of representations learned from a large-scale uncurated dataset using ResNet-50.
Divide and Contrast (DnC) is better able to handle the diverse and long-tailed distribution of images and improves more with longer training. X-axis represents total computation, in ImageNet-equivalent epochs. towards methods which require this consistency, limiting their applicability to more diverse downstream tasks and larger datasets for pre-training.
In this paper we assess how well recent self-supervised learning methods perform on downstream tasks (including
ImageNet) when they are pre-trained on significantly less curated datasets, such as YFCC100M [87]. We observe a notable drop in performance of over 9% Top-1 accuracy (from 74.3% to 65.3%) for a ResNet50 model trained with the current state-of-the-art in self-supervised learning.
We hypothesize that this curation gap is due to the heavy-tailed nature of images collected in the wild, which present much more diverse content, breaking the global consistency exploited in previous datasets. We test this hy-pothesis with a new method, Divide and Contast (DnC), which attempts to recover local consistency in subsets of the larger, uncurated dataset, such that self-supervised learning methods can learn high-level features that are specific to each subset. We find that such semantically coherent sub-sets can be straightforwardly obtained by clustering the rep-resentations of standard self-supervised models.
Divide and Contrast (DnC) proceeds by training individ-ual “expert” models on each subset and distilling them into
a single model. As a result, DnC can be used in combi-nation with any self-supervised learning technique, and re-quires the same amount of computation, as each expert is trained for significantly less time. Finally, this computation is trivially parallelized, allowing it to be scaled to massive datasets.
The remainder of this paper is structured as follows. We first review related work in self-supervised learning. We then present a new stronger baseline (MoCLR) which im-proves over current contrastive methods, matching the per-formance of the current state-of-the-art (BYOL [34]). Next we present the main method, Divide and Contrast, and how this model can be used together with any SSL method. In the experiments we evaluate the different hypotheses that support DnC, and compare its ability to learn from uncu-rated dataset with existing methods. 2.