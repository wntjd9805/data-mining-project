Abstract
In this paper, we present InSeGAN, an unsupervised 3D generative adversarial network (GAN) for segmenting (nearly) identical instances of rigid objects in depth im-ages. Using an analysis-by-synthesis approach, we design a novel GAN architecture to synthesize a multiple-instance depth image with independent control over each instance.
InSeGAN takes in a set of code vectors (e.g., random noise vectors), each encoding the 3D pose of an object that is rep-resented by a learned implicit object template. The genera-tor has two distinct modules. The ﬁrst module, the instance feature generator, uses each encoded pose to transform the implicit template into a feature map representation of each object instance. The second module, the depth image ren-derer, aggregates all of the single-instance feature maps output by the ﬁrst module and generates a multiple-instance depth image. A discriminator distinguishes the generated multiple-instance depth images from the distribution of true depth images. To use our model for instance segmentation, we propose an instance pose encoder that learns to take in a generated depth image and reproduce the pose code vectors for all of the object instances. To evaluate our ap-proach, we introduce a new synthetic dataset, “Insta-10,” consisting of 100,000 depth images, each with 5 instances of an object from one of 10 classes. Our experiments on
Insta-10, as well as on real-world noisy depth images, show that InSeGAN achieves state-of-the-art performance, often outperforming prior methods by large margins. 1.

Introduction
Identifying (nearly) identical instances of objects is a problem that is ubiquitous in daily life. For example, when taking a paperclip from a container, choosing an apple from a box, or removing a book from a library shelf, humans subconsciously solve this problem because we have an un-derstanding of what the individual instances are. How-∗Work done as part of an internship at MERL.
Figure 1. Segmentations and single instances disentangled by In-SeGAN on two multiple-instance depth images (Left: Nut with 5 instances; Right: Cone with 10 instances—challenging).
In-SeGAN needs only unlabelled multiple-instance depth images for training. For each input image, the hallucinated depth image (“ren-dered image”) and the single instances disentangled from the depth image (“rendered single instances”) are shown. We use depth pooling (Z-buffering) and thresholding to produce instance seg-mentation (“segmentation”) from the generated single instances.
Note that our method learns the shape of the object automatically. ever, when robots are deployed for such a picking task, they need to be able to identify the instances for planning their grasp and approach [30, 2]. Such a problem is common-place in large manufacturing, industrial, and agricultural contexts [43, 42, 40, 16, 20]. Examples include an indus-trial robot picking parts from a bin, a warehouse robot pick-ing and placing packages into a delivery truck, or even a fruit-picking robot picking identical fruits in a supermarket.
In these scenarios, the robot’s owners often have no access to a 3D model of the object to be picked, and annotating in-dividual instances for training can be costly, inconvenient, and unscalable. However, they may have access to a large number of unlabeled images each containing multiple in-stances of the object, such as depth images of boxes as they
travel on a conveyor belt from production to a packaging section. Our goal in this paper is to build an unsupervised instance segmentation algorithm using unlabeled depth im-ages, each containing multiple identical instances of a 3D object.
Our problem setting is very different from the instance segmentation setups that are typically considered, such as that of Mask-RCNN [12], 3D point cloud segmenta-tion [28], scene understanding [9], and others [10, 21, 25].
While these methods usually consider segmenting instances from cluttered backgrounds, our backgrounds are usually simple; however, the foreground instances can be heav-ily (self-)occluded or may vary drastically in appearance across their poses (see Fig. 1 for example). Prior methods to solve our instance segmentation problem use 3D CAD mod-els [18], ﬁt the 3D instances using primitive shapes [11], or use classical image-matching techniques to identify the instances [4, 33]. More recently, some have attempted to solve this task using deep learning approaches. For ex-ample, in Wu et al. [41], a 3D rendering framework is presented that is trained to infer the segmentation masks;
In the however, their losses are prone to local minima. recent IODINE [9], MONET [5], and Slot Attention [29] deep models, the focus is on RGB scene decomposition, and may not generalize to segmenting foreground instances from each other.
In this paper, we present a general unsupervised frame-work for instance segmentation in depth images, which we call InSeGAN. Our model is inspired by a key observation made in several recent works (e.g., [23, 32]) that random noise that is systematically injected into a generative adver-sarial network (GAN) can control various attributes of the generated images. A natural question then is whether we can generate an image with a speciﬁc number of instances feeding in the respective number of random noise vectors.
If so, then instance segmentation could be reduced to sim-ply decoding a test image into several noise vectors, each of which generates its respective instance. InSeGAN imple-ments this idea using a combination of a 3D GAN and an image encoder within an analysis-by-synthesis framework, illustrated in Fig. 2. The training data consist of an unla-beled collection of depth images, each image consisting of n instances of a rigid object. InSeGAN learns an implicit 3D representation of the object shape and a pose decoder that maps random noise vectors to 3D rigid transformations.
The generator has two stages. In the ﬁrst stage, the decoded 3D transformation is applied to the implicit object template, which an instance feature generator converts into a feature-map representation of a single object instance. After the
ﬁrst stage generates n such instance representations from n random noise vectors, the second stage aggregates these instance representations and feeds them into a depth image renderer to produce synthetic depth images that are simi-lar in distribution to the training images, as enforced via a discriminator. To achieve instance segmentation, we train an encoder that takes as input a generated multiple-instance depth image and encodes it into a latent space in which it must match the random noise vectors that originally gener-ated the images in the GAN stream, thus closing the genera-tion cycle. At inference time, a given depth image ﬁrst goes through the encoder to get its set of single-instance latent vectors; these are then fed into the GAN to synthesize each instance (each image segment) individually. Results on two example test images are shown in Fig. 1.
While the task of instance segmentation has been ap-proached in various contexts, there is no existing dataset that encompasses this task in the context we are after in this paper. For example, images in standard datasets such as MSCOCO [27] and CityScapes [6] contain objects of several different classes and background, which may not belong to a common latent space. We introduce a new dataset, dubbed “Insta-10,” consisting of 10 object classes and 10,000 depth images per class. Each image was ren-dered using a physics engine that simulated a bin into which 5 instances of an object are randomly dropped, resulting in arbitrary poses of the objects in the rendered depth images.
The instances can have signiﬁcant occlusions and size vari-ations (due to varying distances from the camera), making the task very challenging. We use this dataset to compare our scheme with closely related methods. We also apply our instance segmentation approach to a real-world dataset of blocks in noisy depth images. Our results show that In-SeGAN outperforms all of the prior methods by a signiﬁ-cant margin on most of the object classes.
We now summarize this paper’s primary contributions:
• We propose InSeGAN, a 3D GAN that learns to gener-ate multiple-instance depth images from sets of random noise vectors in an unsupervised manner.
• We propose a two-stage generator structure for In-SeGAN, in which the ﬁrst stage generates a feature map representation of each instance, and the second aggre-gates these single-instance feature maps and renders a multiple-instance depth image.
• To enable segmentation, we propose an instance pose en-coder that encodes a multiple-instance depth image into a set of latent vectors that would generate it. To train this encoder, we introduce novel cycle-consistency losses.
• We have created a new large-scale and challenging dataset, Insta-10, which we are making public to ad-vance research on this topic.
• Our experiments on synthetic and real datasets demon-strate that InSeGAN achieves state-of-the-art perfor-mance. On the Insta-10 dataset, InSeGAN shows a relative improvement of nearly 35% against the recent method of Wu et al. [41] and nearly 9.3% against Lo-catello et al. [29].
2.