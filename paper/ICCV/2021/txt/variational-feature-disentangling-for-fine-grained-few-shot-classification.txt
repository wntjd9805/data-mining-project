Abstract
Data augmentation is an intuitive step towards solving the problem of few-shot classification. However, ensuring both discriminability and diversity in the augmented sam-ples is challenging. To address this, we propose a feature disentanglement framework that allows us to augment fea-tures with randomly sampled intra-class variations while preserving their class-discriminative features. Specifically, we disentangle a feature representation into two compo-nents: one represents the intra-class variance and the other encodes the class-discriminative information. We assume that the intra-class variance induced by variations in poses, backgrounds, or illumination conditions is shared across all classes and can be modelled via a common distribution.
Then we sample features repeatedly from the learned intra-class variability distribution and add them to the class-discriminative features to get the augmented features. Such a data augmentation scheme ensures that the augmented features inherit crucial class-discriminative features while exhibiting large intra-class variance. Our method signif-icantly outperforms the state-of-the-art methods on multi-ple challenging fine-grained few-shot image classification benchmarks. Code is available at: https://github. com/cvlab-stonybrook/vfd-iccv21 1.

Introduction
Fine-grained visual data are hard to collect and costly to annotate [49, 46, 15]. Fine-grained visual datasets of-ten become quite long-tailed and lead to classifiers over-fitting to the abundant classes when trained in vanilla set-tings. Fine-grained few-shot learning (FSL) methods alle-viate this problem since they learn discriminative class fea-tures, among visually similar classes, using as few as 5 or 1 training instances.
Augmenting the few-shot classes by generating addi-*Work done prior to Amazon
Figure 1: Nearest “real sample” neighbors of the aug-mented examples. We train data augmentation methods using the base classes and search for the nearest-neighbors of the generated samples in the novel classes of the CUB dataset. The input images are shown in the first column.
Each row shows the nearest neighbors of some augmented features computed from: ∆-encoder [38] (1st and 2nd rows) and our method (3rd and 4th rows). Green borders indicate that the images have the same class as the input image and red borders indicate otherwise. tional data is a straightforward way to mitigate issues of overfitting in FSL. Nevertheless, generating diverse data reliably remains an open question [19, 40]. The gener-ated samples should contain the class-discriminative fea-tures while exhibiting high intra-class diversity. A typical data synthesis approach is generating new samples based on adversarial frameworks [45, 25, 11, 55, 2, 20, 21, 22].
However, these methods suffer from a lack of diversity in the generated samples as adversarial training often mode-collapses. Another approach is the feature transfer that transfers the intra-class variance from the base classes, which have many training samples, to augment features for the novel classes, in which only few samples are available
[38, 52, 12]. These methods are based on a common as-sumption that intra-class variations induced by poses, back-grounds, or illumination conditions are shared across cat-egories. The intra-class variations are either modelled as low-level statistics [52] or pairwise variations [38, 12] and are applied directly on the novel samples.
In this pa-per, we discuss two potential issues with these approaches.
First, these transformations can introduce certain class-discriminative features that could alter the class-identity of the transformed features. For example, only 8.7% of the augmented features using the ∆-encoder[38] have their nearest “real sample” neighbors belong to the same classes as the original samples (see Fig. 1). Second, the extracted variations might not be relevant to a specific novel sample, i.e., some bird species would never appear in sea back-grounds. Applying irrelevant variations would result in noisy or meaningless samples and degrade classification re-sults (see Sec. 6.1). These two issues are more pronounced for fine-grained classification since a small change in fea-ture space might change the category of the feature due to the small inter-class distances.
We address these issues in this paper via a novel data augmentation framework. First, we disentangle each fea-ture into two components: one that captures the intra-class variance, which we refer as intra-class variance features, and the other that encodes the class-discriminative features.
Second, we model intra-class variance via a common dis-tribution from which we can easily sample the new intra-class variations that are relevant for diversifying a specific instance. We show that both the feature disentanglement and the distribution of intra-class variability can be approx-imated using data from the base classes and it generalizes well to the novel classes. The two key supervision signals that drive the training of our framework are: 1) A classifica-tion loss that ensures that the class-discriminative features contain class specific information, 2) A Variational Auto-Encoder (VAE) [18] system that explicitly models intra-class variance via an isotropic Gaussian distribution.
Our method works especially well for fine-grained datasets where the intra-class variations are similar across classes, achieving state-of-the-art few-shot classification performances on the CUB[49], NAB[46], and Stanford
Dogs[15] datasets, outperforming previous methods [38, 24] by a large margin. We show in our analyses that the data generated by our method lies closely to the real-and-unseen features of the same class and can closely approximate the distribution of the real data.
To sum up, our contributions are: 1. We are the first to propose a VAE-based feature disen-tanglement method for fine-grained FSL. 2. We show that we can train such a system using suffi-cient data from the base classes. We can sample from the learnt distribution to obtain relevant variations to diversify novel training instances in a reliable manner. 3. Our method outperforms state-of-the-art FSL methods in multiple fine-grained datasets by a large margin. 2.