Abstract
We present a passive non-line-of-sight method that infers the number of people or activity of a person from the obser-vation of a blank wall in an unknown room. Our technique analyzes complex imperceptible changes in indirect illumi-nation in a video of the wall to reveal a signal that is corre-lated with motion in the hidden part of a scene. We use this signal to classify between zero, one, or two moving people, or the activity of a person in the hidden scene. We train two convolutional neural networks using data collected from 20 different scenes, and achieve an accuracy of ≈ 94% for both tasks in unseen test environments and real-time online set-tings. Unlike other passive non-line-of-sight methods, the technique does not rely on known occluders or controllable light sources, and generalizes to unknown rooms with no re-calibration. We analyze the generalization and robustness of our method with both real and synthetic data, and study the effect of the scene parameters on the signal quality.1 1.

Introduction
Consider a situation where one would like to recover in-formation about a hidden scene in an unknown room with-out directly peeking inside. Staring at the blank wall of the room from outside may reveal nothing to the naked eye, yet the wall reflects extremely faint but meaningful patterns of light from the hidden scene. We show that by analyz-ing a video of the blank wall, we can infer information about a person’s activity or classify the number of people in a hidden region of the scene, with no prior calibration or knowledge of the environment. Real-time in-situ use of such a non-line-of-sight (NLOS) method can be critical for search and rescue operations, law enforcement, emergency response, fall detection for the elderly, and detection of hid-den pedestrians for intelligent vehicles [6, 29].
NLOS imaging techniques have previously explored re-lated imaging setups using both active and passive meth-1Code, data, and video available at wallcamera.csail.mit.edu.
Figure 1: The light originating from the lamps takes in-finitely many paths to reach the wall on the left. Motion of people in the room changes the mutual visibility between the wall and the rest of the scene, which induces subtle changes in patterns of light observed on the wall. Notice that there are no direct shadows, as the persons do not oc-clude the direct paths connecting the left wall and the lamps. ods. Active methods interact with the environment using flashlights, projectors, lasers, WiFi signals, or sound to ex-tract information about the hidden scenes [1, 3, 10, 12, 16, 20, 23, 24, 28, 42]. Removing the requirement for prob-ing devices, passive approaches only use cameras, without controlling the lighting conditions or interacting with the environment in any manner. Existing passive methods typ-ically exploit known occluders such as a visible corner or a hidden table fan, which act as accidental imaging devices
[4, 5, 7, 32, 34, 35, 38, 39, 40, 45]. In such work, the occlud-ers provide structure to the light transport, enabling recon-struction of human-readable images and tracks of motion in the hidden scene from the observed indirect illumination.
In contrast, we do not rely on occluders between the scene of interest (the persons) and the observed blank wall.
Instead, we leverage the complex temporal variation of in-direct illumination between a person, the room, and poten-tially a second person. We use a video of the blank wall and show that a learning-based approach is able to recover in-formation about the hidden scene. Specifically, we demon-strate automatic classification of the number (between zero, one, and two) and activity of the moving people. Figure 1 illustrates light transport [21] occurring in a typical un-controlled room environment with multiple light sources, objects, and people. The seemingly featureless illumination on the left wall is in fact a complex mixture of light reflected along different paths in the scene. The dominant compo-nent is the direct illumination from the light sources them-selves, but a small fraction is contributed by light bouncing from the objects in the hidden scene, and yet smaller frac-tion from light that has bounced multiple times. The pattern of light cast onto the wall depends on a complex interplay of the mutual visibility of the geometry and the materials in the hidden scene. People moving in the scene affect light paths, creating new interreflections and blocking other con-tributions, which leads to subtle temporal variations in the light reaching the wall. In a typical video imaging setup, the magnitude of the temporally changing illumination is extremely low compared to the imaging noise: in our mea-surements, often −20 dB to as low as −35 dB. The spatial characteristics of these variations are also elusive as they depend on the scene, the illumination, and the relative posi-tion of the persons.
Despite the extremely low signal levels and high noise in the observed videos of the wall, we extract a signal that summarizes the essential motion in the hidden scene by pro-jecting the video into a 2D representation (Section 4). We use this 2D representation of the video to train convolutional neural networks to perform two tasks: classifying the activ-ity of a person between walking, jumping, waving hands, crouching, and no activity (all objects being static), and in-ferring whether zero, one, or two people are present in the hidden scene (Section 5). Our models trained on 20 differ-ent scenes achieve an accuracy of 94.4% in classifying the number of people and 93.7% in activity recognition on the held out test set of 5 unseen scenes. Our method works in real time with almost no latency without any scene specific calibration. We further investigate the impact of the prop-erties of the scene and human motion on this task using a theoretical framework and a synthetic model of the setup (Section 6). 2.