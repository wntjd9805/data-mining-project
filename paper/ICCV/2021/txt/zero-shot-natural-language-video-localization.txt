Abstract
Understanding videos to localize moments with natural language often requires large expensive annotated video re-gions paired with language queries. To eliminate the an-notation costs, we make a first attempt to train a natu-ral language video localization model in zero-shot man-ner. Inspired by unsupervised image captioning setup, we merely require random text corpora, unlabeled video collec-tions, and an off-the-shelf object detector to train a model.
With the unpaired data, we propose to generate pseudo-supervision of candidate temporal regions and correspond-ing query sentences, and develop a simple NLVL model to train with the pseudo-supervision. Our empirical vali-dations show that the proposed pseudo-supervised method outperforms several baseline approaches and a number of methods using stronger supervision on Charades-STA and
ActivityNet-Captions. 1.

Introduction language queries, natural
On increasing demands of understanding videos to search with natural language video localization (NLVL) has been actively investigated in recent literature [19,35,39,43,44,57]. The task targets to lo-calize a temporal moment in a video by a natural language query.
In recent years, significant performance improve-ments on benchmark datasets has been made, facilitated by the advances on deep learning methods [19, 39, 43, 45] and massively annotated data [2, 19, 26, 37, 58].
As illustrated in Fig. 1-(a), the annotations consist of a temporal region in a video (start time, end time) and a cor-responding query sentence. However, obtaining such paired annotation is laborious and expensive. To alleviate the an-notation cost, a number of recent works addressed weakly-supervised setup of NLVL [12, 21, 33] which aims to lo-calize a moment without the temporal alignment of given query sentence. Although it eliminates the annotation cost
∗: equal contribution. †: corresponding author. § now at U. of Minnesota,
Twin Cities. Code: https://github.com/gistvision/PSVL
Figure 1: Tasks with different levels of supervision. (a) Super-vised NLVL (queries and temporal regions on video) (b) Weakly-Supervised NLVL [21,33] (query on videos) (c) Unsupervised Im-age Captioning [15,29] (on images) (d) Proposed Zero-shot NLVL (on videos). of specifying start and end points of the query sentence in video (illustrated in Fig. 1-(b)), the remaining cost of anno-tating natural language query is still considerable [15].
To avoid the costly annotations, we propose zero-shot
NLVL (ZS-NLVL) task setup which aims to learn an NLVL model without any paired annotation, the first in the litera-ture to our best knowledge. Inspired by [15, 29] addressing an image captioning task only with unpaired images, nat-ural language corpora, and an object detector (Fig. 1-(c)), we propose to train an NLVL model by leveraging easily accessible unpaired data including videos, natural language corpora, and an off-the-shelf object detector, with no knowl-edge about video data to localize [3, 15]. We depict the given data for the zero-shot NLVL setup in Fig. 1-(d).
To address this task, we approach to generate pseudo-supervision of candidate temporal regions in video and cor-responding sentences to train an NLVL model. The pseudo-supervision approach has several benefits as follows. First,
it provides interpretable resources (i.e., generated regions and sentences) to train an NLVL model. the pseudo-supervision can serve as initial annotation sugges-tions to human labelers to reduce the annotation cost or to accelerate the annotation process. Finally, the pseudo-supervision can be readily applicable to the existing ‘fully supervised’ NLVL models (Sec. 4.1.3).
Second,
Generating the pseudo supervision for NLVL involves two challenges: 1) finding meaningful temporal regions to be possibly queried and 2) obtaining corresponding query sentences for the temporal regions found. To find the possi-ble temporal regions, we propose to cluster visual informa-tion (Sec. 3.1). Once we have the candidate (predicted) tem-poral regions, we obtain the corresponding (paired) query sentences. For that, we propose to find nouns visible in the frame by the off-the-shelf object detector and predict verbs that are likely appearing together with the detected objects by exploiting noun-verb statistical co-occurrence patterns from the language corpora (Sec. 3.2). We call the set of nouns and verbs as a pseudo query.
Since the pseudo query is structure-less unlike the nat-ural language queries from the supervised data and not all the proposed event regions might be meaningful, we fur-ther propose a simple NLVL model which is better suited to such pseudo-supervision. We call this framework of train-ing an NLVL model with the temporal region proposals and pseudo-query generation, as Pseudo-Supervised Video Lo-calization or PSVL (Sec. 4.1).
Our empirical studies show that our PSVL exhibits competitive accuracy, sometimes outperforming the models with stronger supervision on widely used two benchmarks.
We summarize our contributions as follows:
• We propose the first zero-shot NLVL task.
• We propose an pseudo supervising framework (PSVL) to predict temporal event regions and corresponding query sentences from a video.
• We propose a simple NLVL model architecture.
• We establish baselines for the zero-shot NLVL task and compare it with stronger supervision. 2.