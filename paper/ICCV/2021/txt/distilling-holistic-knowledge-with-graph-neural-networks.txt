Abstract
Knowledge Distillation (KD) aims at transferring knowl-edge from a larger well-optimized teacher network to a smaller learnable student network. Existing KD methods have mainly considered two types of knowledge, namely the individual knowledge and the relational knowledge. How-ever, these two types of knowledge are usually modeled in-dependently while the inherent correlations between them are largely ignored. It is critical for sufficient student net-work learning to integrate both individual knowledge and relational knowledge while reserving their inherent corre-lation. In this paper, we propose to distill the novel holis-tic knowledge based on an attributed graph constructed among instances. The holistic knowledge is represented as a unified graph-based embedding by aggregating individ-ual knowledge from relational neighborhood samples with graph neural networks, the student network is learned by distilling the holistic knowledge in a contrastive manner.
Extensive experiments and ablation studies are conducted on benchmark datasets, the results demonstrate the effec-tiveness of the proposed method. The code has been pub-lished in https://github.com/wyc-ruiker/HKD 1.

Introduction
Deep Neural Networks (DNNs) have shown tremendous success in various applications [11, 26, 10, 25, 7, 36]. How-ever, their success heavily relies on extensive computational and storage resources, which are usually unavailable in em-bedded and mobile systems. To reduce the cost while main-taining satisfactory, knowledge distillation [12] is proposed to transfer knowledge from a larger well-trained teacher network to a smaller learnable student network, hoping that
*Equal Contribution
â€ Corresponding Author
Figure 1. Comparison between Individual/Relational/Holistic
Knowledge Distillation. The blue circle indicates the teacher rep-resentation, and the green circle indicates the student represen-tation. The red arrow denotes the knowledge transfer from the teacher network to the student network. The yellow area in the holistic KD indicates the unified graph-based representation. the transferred knowledge will benefit the student network.
The knowledge distilled from the teacher network has played the central role in knowledge distillation. Among two types of existing knowledge distillation methods, knowledge have been widely studied, namely the individ-ual knowledge and the relational knowledge. The individ-ual knowledge is extracted from each data instance inde-pendently and provides more favorable supervision than the discrete labels, including logits [12], feature representations
[27, 21] and feature maps [24, 35, 17], etc. The relational knowledge [22, 18, 20, 16] is extracted from pairs of in-stances which is invariant to the difference between archi-tectures of the teacher network and the student network.
Despite the success of the above two types of knowl-edge, existing methods have extracted them independently, ignoring their inherent correlations. However, each type of knowledge that extracted independently will be insufficient for the student network learning, especially when the ca-pability of the teacher network is limited. Intuitively, the
individual knowledge and the relational knowledge can be treated as two views of the same teacher network, which are naturally correlated. The closely related instances tend to have similar individual features and shared patterns, which is critical for more discriminative student network learn-ing. Simultaneously integrating the individual and rela-tional knowledge while reserving their inherent correlation is of primal importance for knowledge distillation.
To resolve the above limitations, we propose the Holistic
Knowledge Distillation (HKD) method with graph neural networks. We introduce a novel holistic knowledge which is an integration of both individual knowledge and relational knowledge. Given the feature representations and predic-tions learned by the teacher and the student network, we first build an attributed graphs for each network, where each node denotes an instance, the node attributes denote the learned feature representation, the edges among instances are constructed by the K-nearest-neighbor (KNN) on the predictions. Inspired by the recent success of Graph Neu-ral Networks (GNNs) [10, 16] in simultaneously model-ing network topology and node attributes, we extract the holistic knowledge by aggregating node attributes from the neighborhood samples in the attributed graph, represented as a unified graph-based embedding. Figure 1 illustrates the comparison among the individual, relational and holis-tic knowledge. We also theoretically prove that existing individual knowledge and relational knowledge are special cases of holistic knowledge under certain conditions.
Given the holistic knowledge represented by graph-based embedding, a naive way of knowledge distillation is directly aligning the embedding of the same instance from the teacher and the student network. However, since the stu-dent network usually has lower capability than the teacher network, force aligning the graph-based embedding is too strict for transferring the shared patterns in the neighbor-hood and holistic knowledge. Instead, HKD aims at max-imizing the mutual information between the graph-based representation from the teacher and the student network, which is optimized with InfoNCE estimator[19] in a con-trastive manner. The holistic knowledge guides the student network learning in two ways: first, the student should learn similar instance features and relational neighborhood as the teacher network; second, the student should capture similar patterns from the neighborhood instances in the attributed graph. The memory bank technique is also employed to fur-ther improve the training efficiency. To conclude, we sum-marize our contributions as follows: 1. We propose Holistic Knowledge Distillation (HKD), a novel method to efficiently distill holistic knowledge for the student network learning. 2. The proposed HKD method employs graph neural net-works to simultaneously integrate both the individual and relational knowledge into a unified representation, where their inherent relationship can be reserved. 3. We conduct extensive experiments on benchmark datasets to evaluate the performance of HKD and moti-vation of holistic knowledge, the results demonstrates the effectiveness of the proposed HKD method. 2.