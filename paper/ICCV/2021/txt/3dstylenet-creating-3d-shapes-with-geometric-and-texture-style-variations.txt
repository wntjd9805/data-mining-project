Abstract 1.

Introduction
We propose a method to create plausible geometric and texture style variations of 3D objects in the quest to democ-ratize 3D content creation. Given a pair of textured source and target objects, our method predicts a part-aware affine transformation field that naturally warps the source shape to imitate the overall geometric style of the target. In ad-dition, the texture style of the target is transferred to the warped source object with the help of a multi-view differ-entiable renderer. Our model, 3DSTYLENET, is composed of two sub-networks trained in two stages. First, the geo-metric style network is trained on a large set of untextured 3D shapes. Second, we jointly optimize our geometric style network and a pre-trained image style transfer network with losses defined over both the geometry and the rendering of the result. Given a small set of high-quality textured objects, our method can create many novel stylized shapes, resulting in effortless 3D content creation and style-ware data aug-mentation. We showcase our approach qualitatively on 3D content stylization, and provide user studies to validate the quality of our results. In addition, our method can serve as a valuable tool to create 3D data augmentations for com-puter vision tasks. Extensive quantitative analysis shows that 3DSTYLENET outperforms alternative data augmen-tation techniques for the downstream task of single-image 3D reconstruction.
The remarkable success of neural image style transfer has demonstrated deep learning as a powerful tool to cre-ate artistic images [13, 21, 44, 29, 30, 14, 28], with both casual and professional applications [24]. Although editing 3D content is arguably a more arduous and time-consuming task, which makes automatic tools especially attractive, equally successful formulation of style transfer for the 3D domain has not been proposed. At the same time, the de-mand for 3D content is growing due to the popularity of gaming, AR/VR, 3D animated films, and virtual simulation.
In our work, we seek a style transfer formulation applicable to 3D content creation, including both the geometric shape of objects and their color texture.
Prior works in 3D style transfer address either shape or color stylization, but do not extend to both of these critical attributes of 3D objects. Classical methods consider defor-mation transfer from one object to another with the guid-ance of shape correspondence [49] and the transfer of the texture map from one shape to another by minimizing dis-tortion [41, 40]. Deep learning methods likewise addressed either geometry [33, 35, 58] or texture stylization [39].
For example, [33] proposes an energy optimization frame-work based on surface normals for geometry cubification, while [58] deforms a shape by utilizing a neural network to predict a cage defining a smooth warp of the shape. In [39] and [25], the style of an artistic painting is transferred to the texture or surface of a 3D object. None of the above
methods is able to perceive and transfer the geometric and texture style jointly from one 3D object to another.
In this paper, we aim to create novel geometric and tex-ture variations of 3D objects by transferring the geometric and texture style from one 3D object to another. Unlike ex-isting approaches, our method performs joint optimization over shape and texture to ensure consistency of the final 3D object. Our method treats some simple geometric relation-ships(e.g., relative scales, positions, rotations, etc.) between the semantic parts of a 3D object as a global shape style, which can be abstracted with a set of ellipsoids. We model geometric style transfer with a part-aware affine transfor-mation field, defined based on the ellipsoids, that warps the semantic parts of source shape to be in similar relationships to these parts in target shape. We design a neural network to perform this task, which we train on a large dataset of 3D shapes.
In order to achieve high-quality texture style transfer, a proper alignment of the object geometry is re-quired. We therefore couple and jointly optimize our geo-metric style network with a pre-trained image style transfer network [28] for joint geometric and texture style transfer using losses defined over multi-view rendering produced by a differentiable renderer [27].
Our 3DSTYLENET creates variations of shapes in their style space, yielding a shape creation tool which can be used by naive users for 3D content creation. To validate the qual-ity of our results, we conduct a user study which shows that our method can produce higher-quality results than a strong baseline that combines a SOTA shape deformation method and a SOTA image style transfer method. Furthermore, we show that our 3DSTYLENET can also serve as a 3D data augmentation method for improving the performance of downstream computer vision models. We showcase our approach as a data augmentation strategy for the task of sin-gle image 3D reconstruction, demonstrating boosts in per-formance over strong baseline augmentation techniques. 2.