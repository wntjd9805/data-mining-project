Abstract
Fast arbitrary neural style transfer has attracted widespread attention from academic, industrial and art communities due to its flexibility in enabling various ap-plications. Existing solutions either attentively fuse deep style feature into deep content feature without considering feature distributions, or adaptively normalize deep content feature according to the style such that their global statistics are matched. Although effective, leaving shallow feature unexplored and without locally considering feature statis-tics, they are prone to unnatural output with unpleasing lo-cal distortions. To alleviate this problem, in this paper, we propose a novel attention and normalization module, named
Adaptive Attention Normalization (AdaAttN), to adaptively perform attentive normalization on per-point basis. Specif-ically, spatial attention score is learnt from both shallow and deep features of content and style images. Then per-point weighted statistics are calculated by regarding a style feature point as a distribution of attention-weighted out-put of all style feature points. Finally, the content fea-ture is normalized so that they demonstrate the same lo-cal feature statistics as the calculated per-point weighted style feature statistics. Besides, a novel local feature loss is derived based on AdaAttN to enhance local visual qual-ity. We also extend AdaAttN to be ready for video style transfer with slight modifications. Experiments demon-strate that our method achieves state-of-the-art arbitrary image/video style transfer. Codes and models are available on https://github.com/wzmsltw/AdaAttN. 1.

Introduction
Given a content image Ic and a style image Is, artis-tic style transfer aims at applying style patterns of Is onto
Ic while preserving content structure of Ic simultaneously,
*This work was done when Songhua Liu was an intern at VIS, Baidu.
†Corresponding authors.
Figure 1. Results generated by our AdaAttN methods for arbitrary image/video style. Animated clips at the right side can be found in our supplementary material. which is widely used in computer-aid art generation. The seminal work of Gatys et al. [9] proposed an image opti-mization method that iteratively minimizes the joint content and style loss in the feature space of a pre-trained deep neu-ral network. This time-consuming optimization process has motivated researchers to explore more efficient approaches.
Johnson et al. [18] alternatively considered using a feed-forward network to generate rendered images directly and enabled real time style transfer. Since the learned model can only work for one specific style, this method and its following works [40, 34, 35, 21, 27, 39, 16, 20] are catego-rized to Per-Style-Per-Model method [17]. In the literature, there are Multiple-Style-Per-Model solutions [7, 2, 23, 44] and Arbitrary-Style-Per-Model [14, 3, 22, 28, 15, 24, 6, 5, 43, 32, 42, 11] methods. In the latter case, a model can ac-cept any style image as input and produce stylized results in a single forward pass once upon the model is trained. There-fore, it is the most flexible and attracts increasing attention from academic, industrial, and art communities.
Nevertheless, arbitrary style transfer is far from being solved. Enabling flexibility sacrifices local style pattern modeling capability for an arbitrary style transfer network.
For example, the pioneering work [14] proposed a simple yet effective method AdaIN, which transfers global mean and variance of a style image to a content image in the
feature space to support arbitrary input style image. Since mean and variance of features are calculated globally, lo-cal details and point-wise patterns are largely dismissed and thus the local stylization performance is largely degraded
[28]. Similar trade-off between flexibility and capability also exists in [5, 22, 15, 24, 10], where all local feature points of the content image are processed by the same trans-formation function based on style images. To enhance the locality awareness of arbitrary style transfer models, re-cently, attention mechanism is adopted in multiple works
[28, 6, 43] for this task. Their common intuition is that a model should pay more attention to those feature-similar areas in the style image for stylizing a content image re-gion. Such attention mechanism has been proved to be ef-fective for generating more local style details in arbitrary style transfer. Unfortunately, while improving the perfor-mance, it fails to totally solve this problem and the local distortions still occur.
It is not so hard to reveal the reasons of the above dilemma posed to the attention mechanism. Digging into the details of current attention based solutions for arbitrary style transfer, it can be easily figured out that 1) the designed attention mechanisms are commonly based on deep CNN features on higher abstraction levels and the low-level de-tails are dismissed; 2) the attention scores are usually used to re-weight feature maps of the style image and the re-weighted style feature is simply fused into content feature for decoding. Deep CNN features based attention strategy leaves the low-level patterns of images at shallow network layers unexplored. Thus the attention scores may focus lit-tle on low-level textures and are dominated by high-level semantics. Meanwhile, the spatial re-weighting of style fea-tures followed by fusion of re-weighted style features and content features, as done in SANet [28] (Figure 3(b)), works without consideration of feature distribution.
To this end, we attempt to address these issues and get a better balance between style pattern transferring and con-tent structure preserving. Motivated by lessons learned through the above analysis, we propose a novel atten-tion and normalization module named Adaptive Attention
Normalization (AdaAttN) for arbitrary style transfer. It can adaptively perform attentive normalization on per-point ba-sis for feature distribution alignment. In more detail, spatial attention score is learnt from both shallow and deep fea-tures of content and style images. Then, per-point weighted statistics is calculated by regarding a style feature point as a distribution of attention-weighted output of all spa-tial feature points. Finally the content feature is normal-ized so that its local feature statistics are the same as the per-point weighted style feature statistics. In this way, the attention module takes into account both shallow and deep
CNN features of the style image as well as the content im-age. Meanwhile, alignment of per-point feature statistics from the content feature to the modulated style feature is achieved. Based on AdaAttN module, a novel optimization objective named local feature loss and a new arbitrary im-age style transfer pipeline are derived. Our contributions can be summarized as follows:
• We introduce a novel AdaAttN module for arbitrary style transfer. It takes both shallow and deep features into account for attention score calculation and prop-erly normalizes content feature such that feature statis-tics are well aligned with attention-weighted mean and variance maps of style features on per-point basis.
• A new optimization objective called local feature loss is proposed. It helps the model training and improves arbitrary style transfer quality by regularizing local features of the generated image.
• Extensive experiments and comparisons with other state-of-the-art methods are performed to demonstrate the effectiveness of our proposed method.
• Further extension of our model for video style transfer via simply introducing cosine-distance based attention and image-wise similarity loss can result in stable and appealing results. 2.