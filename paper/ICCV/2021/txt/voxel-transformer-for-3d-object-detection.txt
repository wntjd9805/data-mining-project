Abstract
We present Voxel Transformer (VoTr), a novel and effec-tive voxel-based Transformer backbone for 3D object de-tection from point clouds. Conventional 3D convolutional backbones in voxel-based 3D detectors cannot efﬁciently capture large context information, which is crucial for ob-ject recognition and localization, owing to the limited re-ceptive ﬁelds. In this paper, we resolve the problem by intro-ducing a Transformer-based architecture that enables long-range relationships between voxels by self-attention. Given the fact that non-empty voxels are naturally sparse but nu-merous, directly applying standard Transformer on voxels is non-trivial. To this end, we propose the sparse voxel module and the submanifold voxel module, which can operate on the empty and non-empty voxel positions effectively. To fur-ther enlarge the attention range while maintaining compa-rable computational overhead to the convolutional counter-parts, we propose two attention mechanisms for multi-head attention in those two modules: Local Attention and Dilated
Attention, and we further propose Fast Voxel Query to ac-celerate the querying process in multi-head attention. VoTr contains a series of sparse and submanifold voxel modules, and can be applied in most voxel-based detectors. Our pro-posed VoTr shows consistent improvement over the convolu-tional baselines while maintaining computational efﬁciency on the KITTI dataset and the Waymo Open dataset. 1.

Introduction 3D object detection has received increasing attention in autonomous driving and robotics. Detecting 3D objects from point clouds remains challenging to the research com-munity, mainly because point clouds are naturally sparse and unstructured. Voxel-based detectors transform irregu-lar point clouds into regular voxel-grids and show superior performance in this task. In this paper, we propose Voxel
Transformer (VoTr), an effective Transformer-based back-bone that can be applied in most voxel-based detectors to
∗ Equal contribution. 1 The Chinese University of Hong Kong 2 Na-tional University of Singapore 3 Huawei Noah’s Ark Lab 4 HKUST 5 Sun
Yat-Sen University † Corresponding author: xu.hang@huawei.com (a) 3D convolutional network (b) Voxel Transformer
Figure 1. Illustration of the receptive ﬁeld obtained by the 3D con-volutional network and our proposed VoTr. In (a), the orange cube denotes a single 3D convolutional kernel, and the yellow voxels are covered by the maximum receptive ﬁeld centered at the red voxel. In (b), the red voxel denotes a querying voxel, and the blue voxels are the respective attending voxels for this query in voxel attention. Our observation is that a single self-attention layer in
VoTr can cover a larger region than the whole convolutional back-bone, and it can also maintain enough ﬁne-grained 3D structures. further enhance detection performance.
Previous approaches can be divided into two branches.
Point-based approaches [26, 19, 34, 35] directly operate and generate 3D bounding boxes on point clouds. Those approaches generally apply point operators [23, 16] to ex-tract features directly from point clouds, but suffer from the sparse and non-uniform point distribution and the time-consuming process of sampling and searching for neighbor-ing points. Alternatively, voxel-based approaches [43, 33, 37, 5, 36] ﬁrst rasterize point clouds into voxels and apply 3D convolutional networks to extract voxel features, and then voxels are transformed into a Bird-Eye-View (BEV) feature map and 3D boxes are generated on the BEV map.
Compared with the point-based methods which heavily rely on time-consuming point operators, voxel-based ap-proaches are more efﬁcient with sparse convolutions, and can achieve state-of-the-art detection performance.
The 3D sparse convolutional network is a crucial com-ponent in most voxel-based detection models. Despite its advantageous efﬁciency, the 3D convolutional backbones cannot capture rich context information with limited re-ceptive ﬁelds, which hampers the detection of 3D ob-jects that have only a few voxels. For instance, with a commonly-used 3D convolutional backbone [33] and the voxel size as (0.05m, 0.05m, 0.1m) on the KITTI dataset, layer is only the maximum receptive ﬁeld in the last
(3.65m, 3.65m, 7.3m), which can hardly cover a car with the length over 4m. Enlarging the receptive ﬁelds is also in-tractable. The maximum theoretical receptive ﬁeld of each voxel is roughly proportional to the product of the voxel size V , the kernel size K, the downsample stride S, and the layer number L. Enlarging V will lead to the high quantiza-tion error of point clouds. Increasing K leads to the cubic growth of convoluted features. Increasing S will lead to a low-resolution BEV map which is detrimental to the box prediction, and increasing L will add much computational overhead. Thus it is computationally extensive to obtain large receptive ﬁelds for the 3D convolutional backbones.
Given the fact that the large receptive ﬁeld is heavily needed in detecting 3D objects which are naturally sparse and in-complete, a new architecture should be designed to encode richer context information compared with the convolutional backbone.
Recently advances [6, 2, 41] in 2D object classiﬁca-tion, detection, and segmentation show that Transformer is a more effective architecture compared with convolu-tional neural networks, mainly because long-range relation-ships between pixels can be built by self-attention in the
Transformer modules. However, directly applying standard
Transformer modules to voxels is infeasible, mainly ow-ing to two facts: 1) Non-empty voxels are sparsely dis-tributed in a voxel-grid. Different from pixels which are densely placed on an image plane, non-empty voxels only account for a small proportion of total voxels, e.g., the non-empty voxels normally occupy less than 0.1% of the to-tal voxel space on the Waymo Open dataset [29]. Thus instead of performing self-attention on the whole voxel-grids, special operations should be designed to only attend to those non-empty voxels efﬁciently. 2) The number of non-empty voxels is still large in a scene, e.g., there are nearly 90k non-empty voxels generated per frame on the
Waymo Open dataset. Therefore applying fully-connected self-attention like the standard Transformer is computation-ally prohibitive. New methods are thus highly desired to enlarge the attention range while keeping the number of at-tending voxels for each query in a small value.
To this end, we propose Voxel Transformer (VoTr), a
Transformer-based 3D backbone that can be applied upon voxels efﬁciently and can serve as a better substitute for the conventional 3D convolutional backbones. To effec-tively handle the sparse characteristic of non-empty voxels, we propose the sparse voxel module and the submanifold voxel module as the basic building blocks of VoTr. The sub-manifold voxel modules operate strictly on the non-empty voxels, to retain the original 3D geometric structure, while the sparse voxel modules can output features at the empty locations, which is more ﬂexible and can further enlarge the non-empty voxel space. To resolve the problem that non-empty voxels are too numerous for self-attention, we further propose two attention mechanisms: Local Attention and Dilated Attention, for multi-head attention in the sparse and submanifold voxel modules. Local Attention focuses on the neighboring region to preserve detailed information.
Dilated Attention obtains a large attention range with only a few attending voxels, by gradually increasing the search step. To further accelerate the querying process for Lo-cal and Dilated Attention, we propose Fast Voxel Query, which contains a GPU-based hash table to efﬁciently store and lookup the non-empty voxels. Combining all the above components, VoTr signiﬁcantly boosts the detection perfor-mance compared with the convolutional baselines, while maintains computational efﬁciency.
Our main contributions can be summarized as follows: 1) We propose Voxel Transformer, the ﬁrst Transformer-based 3D backbone for voxel-based 3D detectors. 2) We propose the sparse and submanifold voxel module to handle the sparsity characteristic of voxels, and we fur-ther propose special attention mechanisms and Fast Voxel
Query for efﬁcient computation. 3) Our VoTr consistently outperforms the convolutional baselines and achieves the state-of-the-art performance with 74.95% LEVEL 1 mAP for vehicle and 82.09% mAP for moderate car class on the Waymo dataset and the KITTI dataset respectively. 2.