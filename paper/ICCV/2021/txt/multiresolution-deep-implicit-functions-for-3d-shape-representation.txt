Abstract
We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical representation that can recover fine geometry detail, while being able to perform global oper-ations such as shape completion. Our model represents a complex 3D shape with a hierarchy of latent grids, which can be decoded into different levels of detail and also achieve better accuracy. For shape completion, we propose latent grid dropout to simulate partial data in the latent space and therefore defer the completing functionality to the de-coder side. This along with our multires design significantly improves the shape completion quality under decoder-only latent optimization. To the best of our knowledge, MDIF is the first deep implicit function model that can at the same time (1) represent different levels of detail and allow progres-sive decoding; (2) support both encoder-decoder inference and decoder-only latent optimization, and fulfill multiple applications; (3) perform detailed decoder-only shape com-pletion. Experiments demonstrate its superior performance against prior art in various 3D reconstruction tasks. 1.

Introduction
In recent years, deep implicit functions (DIF) have gained much popularity as a 3D shape representation in applica-tions such as compression [30], shape completion [8], neural rendering [24, 32], and super-resolution [4].
In contrast to explicit representations such as point clouds, voxels, or meshes, a 3D shape is encoded into a compact latent vector, which when combined with a sampled 3D location as input to a decoder can be used to evaluate an implicit function for surface reconstruction.
In this paper, our objective is to design a DIF for shape representation that has three main properties: 1⃝ represent shapes with arbitrarily fine details (adding more bits to the representation provides more details), 2⃝ support both encoder-decoder inference and decoder-only latent optimiza-tion, and can be applied to different tasks, and 3⃝ enable
*Work done while the author was an intern at Google.
Figure 1: Example results of our model for auto-encoding (row 1 and 2) and shape completion (row 3) in different levels of detail. Green dots represent the observed depth pixels for the completion task. detail-preserving shape completion from inputs with large unobserved regions. These properties are all important for a shape representation. Yet, to the best of our knowledge, no prior method has achieved all three properties.
Existing DIF methods can be classified into global and local approaches. Early methods mostly belong to the global category [26, 3, 22, 37, 23], where a single latent vector is used to represent the whole shape. These approaches learn to encode a global shape prior in a compact latent space, which can then be leveraged to fulfill various reconstruction tasks.
However, due to the limited capacity of the latent space and the global nature of these approaches, global methods usually lack fine-grained detail.
More recently, local approaches [18, 1] have been pro-posed. These methods divide the space into local regions and encode each one with a latent vector. Such local repre-sentations provide better accuracy and generalization when representing shapes, especially under decoder-only latent optimization. However, they do not model a global prior. As a result, they cannot be used for shape completion with large unobserved regions since in such regions there is no data to optimize the latent vectors. To overcome this issue, [12, 4]
use an encoder to regress local latent vectors from incom-plete inputs. However, their methods are limited to encoder-decoder inference when doing shape completion. Compared to decoder-only latent optimization, encoder-decoder infer-ence has less flexibility on the inputs and is less accurate for preserving detail in observed regions.
In this paper, we propose a novel 3D representation: Mul-tiresolution Deep Implicit Function (MDIF). The core idea is to represent a shape as a multiresolution hierarchy of latent vectors, where each level encodes different frequencies of an implicit function. The higher levels of our representa-tion provide the global shape and the lower levels provide fine detail. Different from local methods [12, 4], MDIF has a one-decoder-per-level architecture, where each decoder produces a residual with respect to its parent level, like a
Haar wavelet [6]. This simplifies learning of fine detail and enables progressive decoding to achieve arbitrary levels of detail (see Figure 1).
To enable detailed shape completion with decoder-only latent optimization, we further propose to use global connec-tion across levels as well as applying dropout on the latent codes. The global connection serves to integrate global pri-ors into lower levels to compensate for missing observations.
Meanwhile, applying dropout on the latent codes simulates partial observation in the latent space during training, and therefore forces the decoders to learn to complete shapes under encoder-less scenario.
Overall, our model has the following merits: 1. Can represent complex shapes with high accuracy, and allows progressive decoding for different levels of de-tail. 2. Supports both encoder-decoder inference and decoder-only latent optimization, and is effective for different applications as illustrated in the experimental results. 3. Enables detailed decoder-only shape completion that accurately preserves detail in observed regions while producing plausible results in unobserved regions. 2.