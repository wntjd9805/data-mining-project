Abstract w/o supernet-Π model w/ supernet-Π model
Recently proposed neural architecture search (NAS) methods co-train billions of architectures in a supernet and estimate their potential accuracy using the network weights detached from the supernet. However, the ranking correla-tion between the architectures’ predicted accuracy and their actual capability is incorrect, which causes the existing NAS methods’ dilemma. We attribute this ranking correlation problem to the supernet training consistency shift, includ-ing feature shift and parameter shift. Feature shift is iden-tiﬁed as dynamic input distributions of a hidden layer due to random path sampling. The input distribution dynamic affects the loss descent and ﬁnally affects architecture rank-ing. Parameter shift is identiﬁed as contradictory parameter updates for a shared layer lay in different paths in different training steps. The rapidly-changing parameter could not preserve architecture ranking. We address these two shifts simultaneously using a nontrivial supernet-⇧ model, called
⇧-NAS. Speciﬁcally, we employ a supernet-⇧ model that contains cross-path learning to reduce the feature consis-tency shift between different paths. Meanwhile, we adopt a novel nontrivial mean teacher containing negative samples to overcome parameter shift and model collision. Further-more, our ⇧-NAS runs in an unsupervised manner, which can search for more transferable architectures. Extensive experiments on ImageNet and a wide range of downstream tasks (e.g., COCO 2017, ADE20K, and Cityscapes) demon-strate the effectiveness and universality of our ⇧-NAS com-pared to supervised NAS. See Codes1. 1.

Introduction
Automatic neural architecture search (NAS) has been an intense longing in machine learning in the past four years.
*Jiefeng Peng and Jiqi Zhang are co-ﬁrst authors and share equal con-tributions. Their names are listed in alphabetical order.
†Corresponding Author. 1Code: https://github.com/Ernie1/Pi-NAS input path i path j (a) Feature shift. Left: Without the supernet-⇧ model, there is a feature shift between different paths’ feature maps. Right: With supernet-⇧ model, the feature shift is signiﬁcantly reduced. path i path j (b) Parameter shift. Different colors represent the distribution of parameters in different iterations. Left: without our nontrivial mean teacher, the parameter has signiﬁcantly varying distribu-tions in training. Right: with our nontrivial mean teacher, the parameter shift is signiﬁcantly reduced.
Figure 1: Illustration of supernet training consistency shift.
Early works use reinforcement learning [69] or evolution-ary algorithms [40] to discover high-performance architec-tures in the search space. The searching procedure usually costs thousands of GPU days for large datasets, as each sampled architecture needs training from scratch. Recently, to alleviate the heavy burden, weight sharing NAS meth-ods [16, 9, 20, 54, 33, 30, 15, 36, 17, 1, 8] are widely used, where candidate architectures share weights and train simultaneously in a supernet2. After training, a candi-date subnet’s weights detached from the supernet are used to predict its actual performance. Despite the remarkable progress in efﬁciency, weight-sharing NAS’s effectiveness 2A supernet is an over-parameterized network that integrates the entire search space. Each architecture within the search space corresponds to a supernet’s sub-net capturing the required operations.
is still unstable, i.e., it has a low ranking correlation between candidates’ actual accuracies and accuracies estimated in supernet. In short, inaccurate architecture ranking is an in-evitably critical problem in today’s NAS.
In this paper, we attribute the ranking correlation prob-lem to the supernet training consistency shift, including fea-ture shift and parameter shift. Feature shift is identiﬁed as dynamic input distributions of a hidden layer. Speciﬁcally, a given layer’s input feature maps always have an uncertain distribution due to random path sampling (see Figure 1a, left). This distribution uncertainty can hurt the architecture ranking correlation. Precisely, we can use the loss to mea-sure the architecture accuracy, and we can link the accuracy ascent to gradient descent. Based on the back-propagation rule, a stable input distribution can guarantee a good rank-ing correlation. In contrast, the input distribution dynamic affects the loss descent and ﬁnally affects architecture rank-ing. Parameter shift is identiﬁed as contradictory param-eter updates for a given layer. In supernet training, a given layer will always be present in different paths from itera-tion to iteration (see Figure 1b, left). The parameter in this layer may have a contradictory update from iteration to it-eration. These unstable updates lead to varying parameters’ distributions, hurting the architecture ranking correlation in two ways. On the one hand, stable parameters can ensure a correct loss descent and guarantee an accurate architec-ture ranking, while frequent parameter change could not preserve architecture ranking. On the other hand, varying parameters can also result in a feature shift, further hurting architecture ranking correlation. In summary, both feature shift and parameter shift can hurt the architecture ranking correlation. Detailed experimental analysis in Section 4 provide solid evidence to support this analysis.
Motivated by consistency regularization methods [29, 44], we propose a nontrivial supernet-⇧ model, called ⇧-NAS, to reduce these two shifts simultaneously. Specif-ically, to cope with the feature shift, we propose a novel supernet-⇧ model. We evaluate each data point through two randomly sampled paths, then apply a consistency cost between the two predictions to penalize the feature consis-tency shift between different paths. As shown in Figure 1a (right), our method can signiﬁcantly reduce the feature shit and thus can improve the architecture ranking correlation.
To address the parameter shift, we propose a novel non-trivial mean teacher model by maintaining an exponential moving average of weights in supernet teacher. Although a mean teacher can stabilize the parameters in single net-work training, it could be trapped in a trivial solution and lead to a model collision in supernet training. Our nontriv-ial mean teacher novelly contains appropriate negative sam-ples to avoid such a model collision. An impressive result of our method in reducing the parameter shift is shown in Fig-ure 1b (right). In brief, our ⇧-NAS can reduce the supernet training consistency shift and thus improve the architecture ranking, which is critical for NAS’s effectiveness.
One by-product that could not be ignored is that our ⇧-NAS runs in an unsupervised manner, which has an addi-tional gain that existing supervised NAS methods do not have. Concretely, similar to unsupervised representation learning that can learn general features, our ⇧-NAS can search for more transferrable and universal architectures than supervised NAS counterparts.
Since the “good architectures” in previous NAS search spaces usually have considerable computation complexity, using these search spaces for evaluation lacks interpretabil-ity. To evaluate our ⇧-NAS, we design a nontrivial search space based on 16-layer ResNet-50. Our searched mod-els on this space achieve a state-of-the-art top-1 accuracy of 81.6% on ImageNet, surpassing ResNeSt-50 by 0.5% with comparable computation cost. We also validate ⇧-NAS on NAS-Bench-201 with CIFAR-10, beating state-of-the-art NAS methods and verifying our method’s effective-ness. In addition, our ⇧-NAS models keep state-of-the-art on many downstream tasks (e.g., COCO 2017 detection and segmentation, ADE20K segmentation, and Cityscapes seg-mentation), demonstrating the universality of our ⇧-NAS.
Overall, this paper makes three contributions.
• We attribute the inaccurate architecture ranking to the supernet training consistency shift, including feature and parameter shifts. Then we provide a detailed em-pirical analysis of how these two shifts are making
NAS methods ineffective.
• We propose a ⇧-NAS method with two key compo-nents, i.e., a supernet-⇧ model and a nontrivial mean teacher, to address feature shift and parameter shift, re-spectively. Notably, our nontrivial mean teacher model introduces appropriate negative samples to avoid being trapped in a trivial solution.
• Our ⇧-NAS method shares the merit of unsupervised representation learning, i.e., the universality property.
We can search for architectures that are more trans-ferrable and universal than supervised NAS methods.
Substantial empirical results are obtained on ImageNet and a wide range of downstream tasks to demonstrate the effectiveness and universality of our ⇧-NAS. 2.