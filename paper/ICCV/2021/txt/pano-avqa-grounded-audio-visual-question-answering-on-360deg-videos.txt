Abstract 360◦ videos convey holistic views for the surroundings of a scene.
It provides audio-visual cues beyond pre-determined normal ﬁeld of views and displays distinctive spatial relations on a sphere. However, previous bench-mark tasks for panoramic videos are still limited to eval-uate the semantic understanding of audio-visual relation-ships or spherical spatial property in surroundings. We propose a novel benchmark named Pano-AVQA as a large-scale grounded audio-visual question answering dataset on panoramic videos. Using 5.4K 360◦ video clips har-vested online, we collect two types of novel question-answer pairs with bounding-box grounding: spherical spatial re-lation QAs and audio-visual relation QAs. We train sev-eral transformer-based models from Pano-AVQA, where the results suggest that our proposed spherical spatial embed-dings and multimodal training objectives fairly contribute to a better semantic understanding of the panoramic sur-roundings on the dataset. 1.

Introduction
Due to their capacity to capture entire surroundings with-out a restriction in the ﬁeld of view, 360◦ videos have been gaining increasing popularity as a novel medium to record real-life scenery. As illustrated in Fig. 1, unlike conven-tional normal ﬁeld-of-view (NFoV) videos, 360◦ videos allow users to attend to any regions of interest from the original real-life surroundings. As publicly available 360◦ videos surge from video-sharing platforms (e.g., YouTube) and their applications of omnidirectional perception quickly spread from autonomous vehicles [1, 2], robotics [3, 4] to virtual & augmented reality [5, 6], visual understanding in 360◦ videos has warranted serious attentions in computer vision research.
The wide ﬁeld of view of 360◦ videos brings forth new challenges in visual understanding that are under-emphasized in the NFoV video understanding, includ-ing spherical spatial reasoning and audio-visual reasoning.
Since 360◦ videos are encoded in a spherical ambient space, spatial reasoning in 360◦ video, namely spherical spatial reasoning, requires a novel approach to recognizing various relations between the objects all around. Moreover, 360◦ videos contain more diverse visual sources of sounds than conventional videos, which allows richer contextual audio-visual correspondences. Given that spatial attention for vi-sual and auditory stimuli is inherent and even aligned in human [7], capturing the link among visual and auditory signals from panoramic videos can be highly beneﬁcial to real-life scene understanding.
These two of the main cornerstones of 360◦ video un-derstanding, namely spherical spatial reasoning and audio-visual reasoning, have been actively addressed by previous works, including automatic cinematography [8], panoramic saliency detection [9, 10], and self-supervised spatial audio generation [11]. Nonetheless, no known task incorporates linguistic queries to tackle the tasks in 360◦ video domain.
To this end, we propose spatial and audio-visual question answering on 360◦ videos as a novel benchmark task for 360◦ video understanding.
In this work, we introduce the Pano-AVQA dataset as a new 360◦ video question answering dataset that necessitates
ﬁne-grained incorporation of visual, audio, and language modality on panoramic videos. We collect openly available 360◦ videos from online and annotate them with (audio, video, relationship) description pairs; as a result, we con-tribute 20K spatial and 31.7K audio-visual question-answer pairs with bounding box grounding from 5.4K panoramic video clips.
Upon this dataset, we propose a transformer [12]-based spatial and audio-visual question answering framework.
By attending to the context provided by other modalities throughout training, our model learns to fuse holistic in-formation from the panoramic surroundings. For this, we suggest quaternion-based coordinate representation for ac-curate spatial representation and an auxiliary task of audio skewness prediction that are broadly applicable to multi-channel audio inputs.
We summarize our main contributions as follows. 1. We propose novel benchmark tasks on spatial and audio-visual question answering on 360◦ videos to-wards a holistic semantic understanding of omnidirec-tional surroundings. 2. Since there is no existing dataset for this objective to the best of our knowledge, we contribute Pano-AVQA as the ﬁrst large-scale spatial and audio-visual question answering dataset on 360◦ videos, consisting of 51.7K question-answer pairs with bounding box grounding. 3. We design an audio-visual question answering model for 360◦ videos that effectively fuses multimodal cues from the panoramic sphere. We incorporate this model with several baseline systems and evaluate them on the
Pano-AVQA dataset. 2.