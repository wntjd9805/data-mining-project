Abstract
Modern recognition systems require large amounts of su-pervision to achieve accuracy. Adapting to new domains requires significant data from experts, which is onerous and can become too expensive. Zero-shot learning requires an annotated set of attributes for a novel category. Annotating the full set of attributes for a novel category proves to be a tedious and expensive task in deployment. This is especially the case when the recognition domain is an expert domain.
We introduce a new field-guide-inspired approach to zero-shot annotation where the learner model interactively asks for the most useful attributes that define a class. We evalu-ate our method on classification benchmarks with attribute annotations like CUB, SUN, and AWA2 and show that our model achieves the performance of a model with full annota-tions at the cost of significantly fewer number of annotations.
Since the time of experts is precious, decreasing annotation cost can be very valuable for real-world deployment. 1.

Introduction
Modern recognition systems require vast amounts of la-beled data. This is infeasible to acquire in many domains, especially when the classes in question involve subtle dis-tinctions that require an expert: experts have limited time and availability and cannot annotate thousands of images.
This has motivated research into zero-shot learning (ZSL) where the goal is to build effective recognition models from class descriptions alone. The name “zero-shot” suggests that the labeling effort for the annotator is zero. However, this is not true: in current ZSL systems, the annotator must specify for each class hundreds of attributes for thousands of classes (Figure 1 (left)). For example, in one of our preliminary ex-periments, it took an ornithologist more than 15 minutes to fully describe the 312 different attributes in CUB; annotating all 200 classes would have taken the expert more than 50 hours! For ZSL to truly decrease the annotator’s burden, the cost of attribute description has to be significantly cheaper.
Past work on addressing this concern has looked at using freely available text from the internet, e.g., Wikipedia[7,
Figure 1. Left: For each new class in the CUB dataset, an annotator must label 28 attributes (and 312 different quantities). The attributes are multidimensional and continuous-valued, Right: A field guide example of a bird class. There is no full attribute description; instead the bird is described by a few key attributes that distinguish it from close cousins. 16, 2, 23, 43], or relying on word embeddings of class names[3, 8, 24, 1]. While such “unsupervised” ZSL is in-triguing, neither class names nor Wikipedia descriptions are intended to be used for identifying classes. Class names are terse and may often be based on location rather than appearance (e.g.,“Northern Royal Albatross”). Wikipedia articles might likewise include irrelevant information (e.g., about etymology) and miss vital visual details. As such, we find that these approaches result in a substantial reduction in accuracy of almost 20 points(see Figure 4 (middle row)).
This suggests that expert-provided visual characteristics or attributes are indeed crucial for performance.
How can we record expert knowledge of class distinctions as completely as possible while still reducing their burden?
One could simply reduce the vocabulary of attributes that the expert has to specify. But doing so might preclude impor-tant class distinctions, resulting in a dramatic reduction in accuracy. A better approach is provided by field guides that experts write to train human novices. In these guides, for each class, the expert first identifies a very similar previously defined class, and then specifies only the most important attributes that distinguish the two classes (Figure 1 (right)).
This allows for concise descriptions that are both easy to write for the expert and also complete enough for the novice.
What if one used these kinds of descriptions for ZSL?
While intuitive, in our experiments we find that this field guide-based approach is even worse than simply choosing a random smaller, fixed vocabulary of attributes to begin 1
with. This is because it assumes that what the expert thinks are important attributes, are in fact what the machine finds relevant to make class distinctions. But machines are not people; the attributes that humans find most salient may not in fact be salient to machines. More broadly, what people find to be obvious class distinctions might appear extremely subtle to the machine and vice versa. Thus, we need a new approach that similarly reduces annotator effort by focusing only on important attributes, but that crucially relies on the machine to define attribute importance.
With these considerations in mind, we propose a new learning interface for ZSL learners to learn from experts. For each novel class, the expert first identifies a close cousin that the learner already knows about. The learner then chooses the attributes that it thinks will be most informative for it to learn and actively queries the expert for just these attributes.
This learning space brings to the fore the question of query strategies: how must the learner choose attributes to query that maximize performance (achieve good accuracy) but min-imize expert effort (choose informative attributes)? Similar problems are explored in active learning, where learners must choose unlabeled data points to label. However, where active learning techniques have a priori access to unlabeled examples to make a range of measurements (e.g., prediction uncertainty), in our case the learner must choose attributes to query for a completely unseen class.
We address this challenge by proposing multiple novel kinds of query strategies to learn from this new active ZSL interface. We design strategies based on a new measure of attribute uncertainty based on class taxonomies and a notion of expected change in model predictions. We also design strategies for when we have access to an image of the novel class, thus generalizing to the regime of combined zero- and few-shot learning. Our proposed query strategies can work out-of-the-box with existing zero-shot learners.
We experiment with three datasets, namely CUB, SUN and AWA2, and show significant reduction in annotation costs while keeping the performance on par with the full an-notation models. With only 35% of annotations, we get close to full model performance on SUN and CUB. Our approach also significantly outperforms prior unsupervised ZSL work on CUB and SUN without a single attribute annotation. Our contributions are:
• A new field-guide-inspired active ZSL approach to collect expert annotations that is more accurate than using class names/textual descriptions and more time-efficient than an-notating a full attribute description.
• New query strategies (e.g., based on uncertainty and ex-pected model change), to actively query expert attributes to rapidly train the learner. Our results suggest that thinking about what information to acquire from the annotator, and what interface the expert uses, is a promising direction for building accurate recognition models that are easy to train. 2.