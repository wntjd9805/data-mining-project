Abstract
Learning temporally consistent foreground opacity from videos, i.e., video matting, has drawn great attention due to the blossoming of video conferencing. Previous approaches are built on top of image matting models, which fail in maintaining the temporal coherence when being adapted to videos. They either utilize the optical ﬂow to smooth frame-wise prediction, where the performance is dependent on the selected optical ﬂow model; or naively combine feature maps from multiple frames, which does not model well the correspondence of pixels in adjacent frames. In this paper, we propose to enhance the temporal coherence by Consistency-Regularized Graph Neural
Networks (CRGNN) with the aid of a synthesized video matting dataset. CRGNN utilizes Graph Neural Networks (GNN) to relate adjacent frames such that pixels or regions that are incorrectly predicted in one frame can be corrected by leveraging information from its neighboring frames. To generalize our model from synthesized videos to real-world videos, we propose a consistency regularization technique to enforce the consistency on the alpha and foreground when blending them with different backgrounds.
To evaluate the efﬁcacy of CRGNN, we further collect a real-world dataset with annotated alpha mattes. Compared with state-of-the-art methods that require hand-crafted trimaps or backgrounds for modeling training, CRGNN generates favorably results with the help of unlabeled real training dataset. The source code and datasets are avail-able https://github.com/TiantianWang/VideoMatting-CRGNN.git. at 1.

Introduction
Video matting aims to estimate the foreground opacity (alpha matte) of each video frame. It has drawn much at-tention recently due to the blossoming of video conferenc-ing. Typically, the predicted alpha matte can be utilized to create new composites for video editing. Unlike the binary segmentation task, matting produces soft masks that better represent object boundaries or transparent material. Simply segmenting the foreground regions does not synthesize re-Figure 1: Matting results of different models. The ﬁrst row shows the image and ground truth. The second row represents the predictions of a video matting method [34] (Left) and our method (Right). The third row shows the blended image generated by the foreground and predicted alpha. Clearly, our method can predict more subtle details on the hairs. alistic image or video composition results due to the neglect of the transition zone. To obtain accurate video matting, we need to guarantee that: (i) alpha mattes extracted on indi-vidual frames should accurately represent the object to be extracted, i.e., the spatial accuracy, and (ii) extracted mattes should not result in noticeable temporal jitter, i.e., the tem-poral coherence. Compared to spatial accuracy, temporal coherence is often more important in video matting as the human visualization system is more sensitive to temporal inconsistency when watching a video [42].
However, due to the lack of a large-scale video matting dataset, previous methods usually build video matting sys-tems on top of image matting models. For instance, one naive way is to directly apply an image matting approach frame by frame. However, this will cause inconsistent alpha prediction across frames. To improve the temporal coher-ence of alpha mattes, the previous methods usually utilize
the optical ﬂow [25, 39, 27, 35] to smooth frame-wise pre-diction, or leverage a stack of nearby video frames to exploit motion cues [34]. These methods still lead to several issues.
First, warping information from the reference frame to the query frame relies on the quality of optical ﬂow being used.
Normally, a faster solution of optical ﬂow produces inac-curate propagation, while a more accurate one is usually time-consuming. Furthermore, merely combining multiple frames in the feature level ignores the interactions between frames, and does not model the motion ﬂow of pixels in time.
In this paper, we focus on the two challenges for video matting. First, how to produce temporally coherent alpha predictions with the existing image matting dataset [48]?
Second, how to mitigate the domain gap when transfer-ring the model trained on the composited dataset to the real videos? We propose the Consistency-Regularized Graph
Neural Networks (CRGNN) to address these two chal-lenges. We ﬁrst design a graph neural network, in space and time, with the aid of a composited video matting dataset to enhance the temporal coherence. Second, a consistency reg-ularization technique is proposed to generalize our model pretrained on the composited dataset to the real one.
In particular, we construct a fully-connected graph neu-ral network to enhance temporal coherence by exploiting
In this the interactive relation between different frames. graph, the nodes denote video frames and edges link a pair of neighboring frames which are represented by the pair-wise relation. With the graph structure, we encourage in-formation to be propagated across frames, in order to com-plement the information for the missing pixels in the cur-rent frame and smooth the predictions over time. As shown in Figure 1, the proposed method can generate more de-tailed structures compared to the video-based method [34] that does not exploit the interaction between frames, which demonstrates the advantages of the graph neural network for recovering missing pixels assisted by neighboring frames.
As another important contribution to assist the above train-ing process, we also propose a new composited video mat-ting dataset in which alphas are manually annotated against the green screen videos.
To address the second challenge, we need to adapt our model – supervised trained on the composited dataset, to real videos. As such, we introduce a consistency-regularized adversarial learning scheme. On the one hand, we enforce a consistency loss: we blend the prediction of the alpha and the foreground with a random new back-ground, forwarding this new image to have a new version of alpha/foreground pairs, and encouraging them to be con-sistent. On the other hand, we introduce a discriminator to better differentiate the composited frames and real ones in an adversarial manner. To verify the efﬁcacy of the pro-posed method, we evaluate our method on a new real-world dataset in which the alpha mattes are carefully extracted from the background.
Compared to the existing methods which either utilize trimaps or backgrounds as the input for modeling train-ing, our background-free method achieves better perfor-mance against the state-of-the-arts on the composited and real datasets with the help of unlabeled real training dataset.
Our contributions can be summarized in three aspects:
• We propose a graph neural network to fully exploit the interactive relationship between multiple video frames to enhance the temporal coherence with the assist of a composited video matting dataset. the model
• We present a consistency regularization technique to trained on the composited video adapt frames to the real ones, which can enhance the con-sistency on the alpha and foreground.
• We propose two large-scale composited datasets and one manually annotated real dataset for the future de-velopment of this area. Extensive experiments are con-ducted on the proposed datasets, showing that the pro-posed method performs favorably against the state-of-the-arts. 2.