Abstract
Pseudo-LiDAR-based methods for monocular 3D ob-ject detection have received considerable attention in the community due to the performance gains exhibited on the
KITTI3D benchmark, in particular on the commonly re-ported validation split. This generated a distorted impres-sion about the superiority of Pseudo-LiDAR-based (PL-based) approaches over methods working with RGB images only. Our ﬁrst contribution consists in rectifying this view by pointing out and showing experimentally that the vali-dation results published by PL-based methods are substan-tially biased. The source of the bias resides in an overlap between the KITTI3D object detection validation set and the training/validation sets used to train depth predictors feeding PL-based methods. Surprisingly, the bias remains also after geographically removing the overlap. This leaves the test set as the only reliable set for comparison, where published PL-based methods do not excel. Our second con-tribution brings PL-based methods back up in the ranking with the design of a novel deep architecture which intro-duces a 3D conﬁdence prediction module. We show that 3D conﬁdence estimation techniques derived from RGB-only 3D detection approaches can be successfully integrated into our framework and, more importantly, that improved per-formance can be obtained with a newly designed 3D conﬁ-dence measure, leading to state-of-the-art performance on the KITTI3D benchmark. 1.

Introduction
By providing information about pose, location and cate-gory of objects in the 3D space, 3D object detection con-stitutes an enabling technology for applications like au-tonomous driving or augmented reality. To obtain accurate localisation performance, existing solutions rely on depth information inferred from stereo cameras or derived from
Light Detection and Ranging (LiDAR) sensors. The down-sides of both variants are an increase of costs, the necessity of involved recalibration routines and the inhibition of the product design form factors due to fabrication constraints.
Figure 1: Performance of state-of-the-art 3D detection methods on the KITTI3D validation and test sets1. RGB-based methods (orange circles) exhibit a low performance discrepancy between the two sets, whereas Pseudo-LiDAR-based methods (green triangles) perform much better (up to 10 AP) on validation than on test. This indicates a bias, which we display by means of a blue-toned colormap.
These results also show that the best performing RGB-based methods generally beneﬁt from exploiting a 3D Con-ﬁdence (circled orange circles), a component which has not yet been introduced in any PL-based methods.
To overcome these issues, an emerging branch of 3D ob-ject detection methods is entirely based on monocular cam-eras [1, 9, 19, 20, 25, 27, 29]. Monocular cameras are a cheap alternative to the expensive LiDAR or stereo setups, but, at the same time, incur a substantially increased algo-rithmic complexity due to the absence of depth observa-tions. Indeed, accurate estimation of an objects’ distance to the camera is the most difﬁcult task in monocular, image-1We took as reference the performance on class Car in the Moderate difﬁculty, computed with the AP |R40 metric i.e. the one used as reference on the ofﬁcial KITTI3D benchmark.
based 3D object detection, making it an ill-posed problem.
Despite the development of methods which focus on in-creasing the generalization with respect to distance [1, 27], monocular image-based methods still lag far behind their
LiDAR or stereo-based counterparts.
A recent line of works [20, 31] has leveraged Convo-lutional Neural Networks (CNNs) for image-based depth predictions as depth substitute in monocular 3D object detection algorithms. Pseudo-LiDAR (PL) [29, 32] was promoted as a particularly effective depth representation, reporting impressive results on the challenging KITTI3D benchmark [8]. It essentially mimics a LiDAR signal for a RGB image by projecting each 2D pixel from its corre-sponding, estimated depth map into 3D space. With the resulting 3D point cloud, the 3D detection task is usually approached by applying state-of-the-art LiDAR-based (and thus 3D point-based) detection algorithms. PatchNet [18] has recently refuted 3D points as the source of PL’s ef-fectiveness by providing an equivalently performing imple-mentation based on stacking 3D world coordinates as 2D maps. While this eliminates the claims of PL being advan-tageous due to its 3D point-based representation, their abla-tions conﬁrmed the importance of operating on transformed 2D image coordinates incorporating camera intrinsics (fo-cal length and principal point).
In this paper we argue that PL-based approaches, and more in general approaches that take depth as input, have in-troduced a distorted perception in the research community about their performance in the monocular setting with re-spect to other state-of-the-art methods that use RGB-images only. We identiﬁed two main reasons behind the issue, which constitute the two main contributions of this paper.
First contribution. State of the art PL-based methods re-port excellent performance on the KITTI3D validation set but do not show the same gains on the test set.
In this work we perform an in-depth experimental study to ana-lyze the reasons behind such inconsistency and demonstrate that top performing PL-based methods adopt a training pro-tocol which artiﬁcially leads to high average precision on the validation set. The issue is evident in Fig. 1, where the discrepancy between the KITTI3D validation and test set performance of PL-based methods (green triangles) is much more pronounced than RGB-based methods (orange circles). Indeed, the depth estimation algorithms on which
PL-based methods heavily rely are usually trained by in-cluding ≈ 30% of the validation set data used for 3D ob-ject detection. Despite this issue was mentioned brieﬂy in [29, 28], this biased training protocol was later used in many subsequent PL-based methods. This clearly indicates the necessity and the relevance for the community of a more detailed analysis, which we provide in this paper.
Second contribution. The outcome of a fair comparison on test set of PL-based methods against RGB-only based approaches on the KITTI3D benchmark is currently favour-ing more the latter ones. On the ﬂip side, we found that published PL-based methods are penalized by the complete lack of a proper 3D conﬁdence score which, as shown in
Fig. 1 (circled orange circles), is becoming a fundamental component of state-of-the-art RGB-only methods. In this paper we propose, for the ﬁrst time, to endow PL-based methods with a mechanism for predicting a 3D conﬁdence, demonstrating remarkable performance gains. In particular, we show that, following previous RGB-only based methods
[25], also in the case of PL, 3D conﬁdence can be trained by directly regressing the expected loss. While this works well in practice, it is sensitive to the scale of the loss and, hence, requires some hyperparameter tuning. Moreover, it suffers from the issue of becoming overconﬁdent as the training progresses towards an overﬁtting regime.
In the spirit of addressing those two issues, we open a novel direc-tion and successfully explore the possibility of having 3D conﬁdences expressed in relative terms. Our novel ﬁnding leads to improved performance and sets the new state-of-the-art on the KITTI3D benchmark. 2.