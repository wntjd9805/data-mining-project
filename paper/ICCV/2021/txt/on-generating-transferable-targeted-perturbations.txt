Abstract
While the untargeted black-box transferability of adver-sarial perturbations has been extensively studied before, changing an unseen model’s decisions to a speciﬁc ‘tar-geted’ class remains a challenging feat.
In this paper, we propose a new generative approach for highly trans-ferable targeted perturbations (TTP). We note that the ex-isting methods are less suitable for this task due to their reliance on class-boundary information that changes from one model to another, thus reducing transferability. In con-trast, our approach matches the perturbed image ‘distri-bution’ with that of the target class, leading to high tar-geted transferability rates. To this end, we propose a new objective function that not only aligns the global distribu-tions of source and target images, but also matches the local neighbourhood structure between the two domains.
Based on the proposed objective, we train a generator function that can adaptively synthesize perturbations spe-ciﬁc to a given input. Our generative approach is in-dependent of the source or target domain labels, while consistently performs well against state-of-the-art methods on a wide range of attack settings. As an example, we achieve 32.63% target transferability from (an adversari-ally weak) VGG19BN to (a strong) WideResNet on Ima-geNet val. set, which is 4 higher than the previous best
⇥ generative attack and 16 better than instance-speciﬁc it-erative attack. Code is available at: https://github. com/Muzammal-Naseer/TTP.
⇥ 1.

Introduction
We study the challenging problem of targeted transfer-ability of adversarial perturbations. In this case, given an input sample from any source category, the goal of the ad-versary is to change the decision of an unknown model to a speciﬁc target class (e.g., misclassify any painting im-age to Fire truck, see Fig. 1). This task is signiﬁcantly more difﬁcult than merely changing the decision to a ran-Target 
Domain
Source 
Domain
Natural Images e t u h c a r a
P
Q k c u r
T e r i
F
OR 4
Paintings
OR
Target
<
Latent Space
Adversarial
Augmented 
Adversarial
Maximize Distribution Agreement
Q
: Augmenter 4 : Generator
< : Discriminator
Figure 1: Attack Overview (TTP): Instead of ﬁnding perturbations speciﬁc to a class-boundary information learned by a model, TTP seeks to match global distribution statistics between the source and the target domains. Speciﬁcally, our generator function is trained to maximize agreement between the perturbed source distribution, its augmented version and the target distribution in the feature
Importantly, our attack can function in an unsupervised space. fashion and does not require source domain to be the same as tar-get (e.g., perturbations can be learned from paintings to transfer on natural images). dom target class or any similar class (e.g., changing ‘cat’ to ‘aeroplane’ is more difﬁcult than altering the decision to ‘dog’). Target transferability can therefore lead to goal-driven adversarial perturbations that provide desired con-trol over the attacked model. However, target transferabil-ity remains challenging for the current adversarial attacks
[25, 23, 4, 43, 14, 13, 12, 20, 41] that transfer adversarial noise in a black-box setting, where architecture and training mechanism of the attacked model remain unknown, and the attack is restricted within a certain perturbation budget.
We observe that modest performance of existing meth- 
For example, ods on targeted transferability is due to their reliance on class-boundary information learned by the model which iterative instance-lacks generalizability. speciﬁc attacks rely on the classiﬁcation score information to perturb a given sample, thereby ignoring the global class-speciﬁc information [25, 4, 43, 13]. Such adversarial direc-tions also vary across different models [23], leading to poor target transferability [23, 4]. On the other hand, although universal and generative perturbations are designed to en-code global noise patterns [26, 35, 31], they still exploit the class impressions learned by a neural network which alone are not fully representative of the target distribution, thereby achieving only modest black-box fooling rates [36]. Fur-thermore, they are dependent on the classiﬁcation informa-tion, necessitating a supervised pretrained model for gen-erator’s guidance and therefore cannot directly work with unsupervised features [1, 9]. Another group of techniques exploit intermediate features, but they either ﬁnd untargeted perturbations by design [28, 37] or are limited in their ca-pacity to transfer targeted perturbations [20, 14, 12, 13].
We introduce a novel generative training framework which maps a given source distribution to a speciﬁc target distribution by maximizing the mutual agreement between the two in the latent space of a pretrained discriminator. Our main contributions are:
• Generative Targeted Transferability: We propose a novel generative approach to learn transferable targeted ad-versarial perturbations. Our unique training mechanism allows the generator to explore augmented adversarial space during training which enhances the transferability of adversarial examples during inference (Sec. 3.1).
• Mutual Distribution Matching: Our training approach is based on maximizing the mutual agreement between the given source and the target distribution. Therefore, our method can provide targeted guidance to train the gener-ator without the need of classiﬁcation boundary informa-tion. This allows an attacker to learn targeted generative perturbations from the unsupervised features [1, 9] and eliminate the cost of labelled data (Sec. 3.2).
• Neighbourhood Similarity Matching: Alongside global distribution matching, we introduce batch-wise neigh-bourhood similarity matching objective between adver-sarial and target class samples to maximize the local alignment between the two distributions (Sec. 3.3).
Our extensive experiments on various ImageNet splits and
CNN architectures show state-of-the-art targeted transfer-ability against naturally and adversarially trained models, stylized models and input-processing based defenses. The results demonstrate our beneﬁt compared to recent targeted instance-speciﬁc as well as other generative methods. Fur-ther, our attack demonstrates rapid convergence. 2.