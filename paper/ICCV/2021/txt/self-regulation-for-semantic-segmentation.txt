Abstract
In this paper, we seek reasons for the two major failure cases in Semantic Segmentation (SS): 1) missing small ob-jects or minor object parts, and 2) mislabeling minor parts of large objects as wrong classes. We have an interesting finding that Failure-1 is due to the underuse of detailed fea-tures and Failure-2 is due to the underuse of visual contexts.
To help the model learn a better trade-off, we introduce sev-eral Self-Regulation (SR) losses for training SS neural net-works. By “self”, we mean that the losses are from the model per se without using any additional data or super-vision. By applying the SR losses, the deep layer features are regulated by the shallow ones to preserve more details; meanwhile, shallow layer classification logits are regulated by the deep ones to capture more semantics. We conduct ex-tensive experiments on both weakly and fully supervised SS tasks, and the results show that our approach consistently surpasses the baselines. We also validate that SR losses are easy to implement in various state-of-the-art SS models, e.g., SPGNet [7] and OCRNet [62], incurring little com-putational overhead during training and none for testing1. 1.

Introduction
Semantic Segmentation (SS) aims to label each image pixel with its corresponding semantic class [37].
It is an indispensable computer vision building block in the scene understanding systems, e.g., autonomous driving [49] and medical imaging [18]. Thanks to the development of deep convolutional neural networks [20, 52] and the labour in-put in pixel-level annotations [8, 39], the research for SS has experienced a great progress, e.g., top-performing mod-els can segment about 85% objects in the complex natural scenes [33]. Intriguingly, we particularly seek reasons for the 15% failure cases which generally fall into two cate-gories: 1) missing small objects or minor object parts, e.g., the “horse legs” in Figure 1 (d), and 2) mislabeling minor
*Corresponding author. 1The code is available at: https://github.com/dongzhang89/SR-SS
Figure 1. Our inspections on two failure cases of the baseline method DeepLab-v2 [5] in (d) and (h), for which (a) and (g) are input images, respectively. The case in (d) has the flaw of miss-ing “horse legs”. The visualization of shallow-layer features in (b) shows that the model indeed attends to “horse legs” but also to background noises. Applying our SR, in (e), reduces noises while maintaining the desirable attention on “horse legs”. In addition, deep-layer features in (c) and (f) also prove the effectiveness of
SR. The case in (h) confuses between object classes (i.e., “horse” and “cow”). The revision in (i) is obtained by adding class-level cross-entropy losses (i.e., using our MEA loss on DeepLab-v2). parts of large objects as wrong classes, e.g., a part of “cow” is wrongly marked as “horse” in Figure 1 (h).
To this end, we present our empirical inspections in Fig-ure 1. We find in Figure 1 (b) that the minor object parts are clearly visible in the shallow-layers of the model, making use of which could address the issue in Failure-1, as in Fig-ure 1 (e). In the literature, there is indeed a popular solution
— reshaping and then combining feature maps from differ-ent layers for prediction, e.g., Hourglass [40], SegNet [2],
U-Net [45] and HRNet [52]. Such implementation typically relies on one of the three specific operations: pixel-wise ad-dition [2, 36, 45], map-wise concatenation [5, 52, 71] and 1
pixel-wise transformation [53, 65, 72]. However, they are expensive for deep backbones, e.g., SPGNet [7] and HR-Net [52] take 2.04× and 1.87× of model parameters more than their baselines, respectively.
Besides, what we observe from the “cow” example in
Figure 1 is that penalizing the model with the image-level classification loss can mitigate Failure-2 [67, 70]: pixel-level confusion between the foreground objects (i.e., the
“cow”) and the local pixel cues (i.e., the “horse” part). The intuitive reason is that this loss penalizes the prediction of an unseen class-level context (i.e., “cow” can not have a
“horse” mouth). In this paper, we make use of this intuition and introduce a formal loss function to enhance the contex-tual encoding for each image.
Now, we present our overall approach, called Self-Regulation (SR), which has three major advantages: 1) generic to be implemented in any deep backbones, 2) with-out using any auxiliary data or supervision, and 3) with little overhead for training and none for testing. As shown in Fig-ure 2, given a backbone network, we first add a pair of clas-sifier and segmenter at each layer (e.g., head networks for multi-label classification and semantic segmentation), and feed them with the corresponding feature maps. Then, we apply the proposed Self-Regulation for the features and log-its by introducing the following three operations: I) Reg-ulating the predictions of each pair with ground-truth la-bels, i.e., the image-level classification labels and the pixel-level mask labels, respectively; II) Using the feature map of the shallowest layer — Shallow Teacher — to regulate all the subsequent deeper-layer feature maps — Deep Stu-dents; III) Using the prediction of the deepest layer classi-fication logits — Deep Teacher — to regulate all the previ-ous shallow-layer classification logits — Shallow Students. for pixel-level feature maps, we
Here is our punchline: use one Shallow Teacher to teach many Deep Students; for image-level classification logits, we use one Deep Teacher to teach many Shallow Students.
Operation I is the standard supervised training that spreads over all the layers. This computation overhead (e.g., multiple-exit network [44]) allows the ground-truth to supervise each layer at its earliest convenience. Opera-tion II and Operation III aim to balance the trade-off among these supervisions. To implement them, we introduce a self-regulation method inspired by knowledge distillation (KD) [21, 35, 42]. Different from the conventional KD that needs to train a teacher model in prior, our “distillation” occurs in the same model during training. The shallowest layer retains the most details, so its segmentation behavior (e.g., encoded in its feature maps and then decoded by the segmentation logits) can teach the subsequent deeper lay-ers not to forget the details to avoid feature underuse (e.g.,
Failure-1 in Figure 1); while the deepest layer retain the highest-level contextual semantics, so its classification be-Figure 2. An illustrative example of implementing our proposed self-regulation (SR) to a plain Conv network. It is first converted to a multiple-exit architecture (MEA) where “Seg.&Cls.” are a pair of classifier and segmenter for each layer. Shallow-to-deep
SR means shallow-layer features regulate deep layers. Deep-to-shallow SR means deep-layer predictions regulate shallow layers. havior (i.e., class logits) can teach the previous shallower layers not to ignore the contexts to avoid semantic underuse (e.g., Failure-2 in Figure 1).
Note that Operation I at each layer also protects any overuse. Specifically, if Operation II unnecessarily imposes shallow details on deep contexts, the image-level classifica-tion logits at deep layers will penalize it; similarly, if Op-eration III unnecessarily imposes deep contexts on shallow details, the pixel-level segmentation logits at shallow layers will discourage it. Such “shallow to deep and back” reg-ulation collaborate with each other to improve the overall segmentation. Our empirical results on different baselines, e.g., DeepLab-v2 [5], SegNet [2], SPGNet [7], and OCR-Net [62], show that our SR is helpful to balance the use of low-/high-level semantics among different layers.
In summary, our contributions are two-fold: 1) A novel set of SR operations that tackle the two key issues in SS tasks while introducing little overhead for training and none for testing; and 2) Through extensive experiments in both fully-supervised and weakly-supervised SS settings, we val-idate that the proposed SR operations can be easily plugged-and-play and consistently improve various baseline models by a large margin. 2.