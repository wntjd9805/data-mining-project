Abstract
In this paper, we propose Parametric Contrastive Learn-ing (PaCo) to tackle long-tailed recognition. Based on the-oretical analysis, we observe supervised contrastive loss tends to bias on high-frequency classes and thus increases the difficulty of imbalanced learning. We introduce a set of parametric class-wise learnable centers to rebal-ance from an optimization perspective. Further, we ana-lyze our PaCo loss under a balanced setting. Our anal-ysis demonstrates that PaCo can adaptively enhance the intensity of pushing samples of the same class close as more samples are pulled together with their correspond-ing centers and benefit hard example learning. Experi-ments on long-tailed CIFAR, ImageNet, Places, and iNat-uralist 2018 manifest the new state-of-the-art for long-tailed recognition. On full ImageNet, models trained with PaCo loss surpass supervised contrastive learning across various ResNet backbones, e.g., our ResNet-200 achieves 81.8% top-1 accuracy.
Our code is avail-able at https://github.com/dvlab-research/
Parametric-Contrastive-Learning. 1.

Introduction
Convolutional neural networks (CNNs) have achieved great success in various tasks, including image classification
[22, 43], object detection [31, 34] and semantic segmenta-tion [55]. With neural network search [60, 33, 45, 13, 4], performance of CNNs further boosts. Impressive progress highly depends on large-scale and high-quality datasets, such as ImageNet [40], MS COCO [32] and Places [59].
When dealing with real-world applications, generally we face the long-tailed distribution problem – a few classes contain many instances, while most classes contain only a few instances. Learning in such an imbalanced setting is challenging as the low-frequency classes can be easily over-whelmed by high-frequency ones. Without considering this situation, CNNs will suffer from significant performance degradation.
Figure 1: Comparison with state-of-the-arts on ImageNet-LT [35]. Inference time is calculated with a batch of 64 im-ages on Nvidia GeForce 2080Ti GPU. The same experimen-tal setting is adopted for comparison. ResNet-50, ResNeXt-50, and ResNeXt101 are used for Balanced Softmax [38],
Decouple [28], and PaCo. For RIDE [49], various numbers of expert with RIDEResNet and RIDEResNeXt are adopted.
PaCo significantly outperforms recent SOTA. Detailed num-bers for RIDE is in the supplementary file.
Contrastive learning [9, 21, 10, 19, 7] is a major re-search topic due to its success in self-supervised represen-tation learning. Khosla et al. [29] extend non-parametric contrastive loss into non-parametric supervised contrastive loss by leveraging label information, which trains represen-tation in the first stage and learns the linear classifier with the fixed backbone in the second stage. Though supervised contrastive learning works well in a balanced setting, for im-balanced datasets, our theoretical analysis shows that high-frequency classes will have a higher lower bound of loss and contribute much higher importance than low-frequency classes when equipping it in training.
This phenomenon leads to model bias on high-frequency classes and increases the difficulty of imbalanced learning.
As shown in Fig. 2, when the model is trained with super-vised contrastive loss on ImageNet-LT, the gradient norm varying from the most frequent class to the least one is rather steep. In particular, the gradient norm dramatically decreases for the top 200 most frequent classes.
Previous work [1, 25, 15, 20, 8, 41, 38, 28, 49, 14, 46, 14, 57] explored rebalancing in traditional supervised cross-models trained with PaCo also outperform the ones by su-pervised contrastive learning on such balanced datasets. Our key contributions are as follows.
• We identify the shortcoming of supervised contrastive learning under an imbalanced setting – it tends to bias towards high-frequency classes.
• We extend supervised contrastive loss to the PaCo loss, which is more friendly to imbalance learning, by intro-ducing a set of parametric class-wise learnable centers.
• Equipped with the PaCo loss, we create new record across various benchmarks for long-tailed recognition.
Moreover, experimental results on full ImageNet and
CIFAR validate the effectiveness of PaCo under a bal-anced setting. 2.