Abstract
We propose replacing scene text in videos using deep style transfer and learned photometric transformations.
Building on recent progress on still image text replacement, we present extensions that alter text while preserving the ap-pearance and motion characteristics of the original video.
Compared to the problem of still image text replacement, our method addresses additional challenges introduced by video, namely effects induced by changing lighting, motion blur, diverse variations in camera-object pose over time, and preservation of temporal consistency. We parse the problem into three steps. First, the text in all frames is normalized to a frontal pose using a spatio-temporal trans-former network. Second, the text is replaced in a single reference frame using a state-of-art still-image text replace-ment method. Finally, the new text is transferred from the reference to remaining frames using a novel learned image transformation network that captures lighting and blur ef-fects in a temporally consistent manner. Results on syn-thetic and challenging real videos show realistic text trans-fer, competitive quantitative and qualitative performance, and superior inference speed relative to alternatives. We in-troduce new synthetic and real-world datasets with paired text objects. To the best of our knowledge this is the first attempt at deep video text replacement. 1.

Introduction
We address the problem of realistically altering scene text in videos. Our primary application is to create person-alized content for marketing and promotional purposes. An example would be to replace a word on a store sign with a personalized name or message, as shown in Figure 1. Other applications include language translation, text redaction for
*Corresponding author: bg.vijay.k@gmail.com
Figure 1: Our method replaces the scene text in the original video (”COFFEE” in upper row) with a personalized string (”ROBERT” in bottom row) while preserving the original geometry, appearance, and temporal properties. privacy, and augmented reality. For research purposes, the ability to realistically manipulate scene text also enables augmentation of datasets for training text detection, recog-nition, tracking, erasure, and adversarial attack detection.
Traditionally text editing in images is performed manually by graphic artists, a process that typically entails a long painstaking process to ensure the original geometry, style and appearance are preserved. For videos, this effort would be considerably more arduous.
Recently several attempts have been made to automate text replacement in still images based on principles of deep style transfer ([22], [1], [31], [26], [29]). We leverage this progress to tackle the problem of text replacement in videos.
In addition to the challenges faced in the still image case, video text replacement must respect temporal consistency and model effects such as lighting changes, blur induced by camera and object motion. Furthermore, over the course of the video, the pose of the camera relative to the text object can vary widely, and hence text replacement must be able to handle diverse geometries. A logical method to solving the problem would be to train an image-based text style trans-1
fer module on individual frames, while incorporating tem-poral consistency constraints in the network loss. The prob-lem with such an approach is that the network performing text style transfer (an already non-trivial task) is now ad-ditionally burdened with handling geometric and motion-induced effects encountered in video. We show in experi-ments that current still-image text replacement techniques such as [26, 13] do not robustly handle such effects. We therefore take a different approach. We first extract text re-gions of interest (ROI) and train a spatio-temporal trans-former network (STTN) to frontalize the ROIs in a tempo-rally consistent manner. We then scan the video and select a reference frame with high text quality, measured in terms of text sharpness, size, and geometry. We perform still-image text replacement on this frame using a state-of-art method SRNet [26] trained on video frames. We then trans-fer the new text onto other frames with a novel text prop-agation module (TPM) that takes into account changes in lighting and blur effects with respect to the reference frame.
TPM takes as input the reference and current frame from the original video, infers an image transformation between the pair, and applies it to the altered reference frame gener-ated by SRNet. Crucially, TPM takes temporal consistency into account when learning pairwise image transforms. Our framework, dubbed STRIVE (Scene Text Replacement In
VidEos) is summarized in Figure 2.
To our knowledge this is the first attempt at replacing scene text in videos. We make the following contributions: 1) A modular pipeline that disentangles the problem of text replacement in a single reference frame from modeling the flow of the replaced text within the scene over time. Such a parsing of the problem into simpler subtasks serves to both simplify training and reduce computations during inference. 2) A learned parametric differential image transformation that captures temporal photometric changes between a pair of aligned ROIs in the original video, and applies it to ROIs in the text-altered video. The transform consists of a learn-able blur/sharpness operator, and is trained on synthetic data and fine-tuned via self-supervision on real-world images. 3) New synthetic and real-world datasets comprising a di-versity of annotated scene text objects in videos. A sub-set of the videos comprise triplets of ROIs with aligned
The source text, datasets are available at https://striveiccv2021. github.io/STRIVE-ICCV2021/. text, and plain background. target 2.