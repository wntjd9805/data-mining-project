Abstract
In this paper, we propose enhancing monocular depth estimation by adding 3D points as depth guidance. Un-like existing depth completion methods, our approach per-forms well on extremely sparse and unevenly distributed point clouds, which makes it agnostic to the source of the 3D points. We achieve this by introducing a novel multi-scale 3D point fusion network that is both lightweight and efficient. We demonstrate its versatility on two different depth estimation problems where the 3D points have been acquired with conventional structure-from-motion and Li-DAR. In both cases, our network performs on par with state-of-the-art depth completion methods and achieves signifi-cantly higher accuracy when only a small number of points is used while being more compact in terms of the num-ber of parameters. We show that our method outperforms some contemporary deep learning based multi-view stereo and structure-from-motion methods both in accuracy and in compactness. 1.

Introduction
Depth estimation from 2D images is a classical computer vision problem that has been mostly tackled with methods from multiple view geometry [15, 45]. Conventional stereo, structure-from-motion and SLAM approaches are already well-established and integrated to many practical applica-tions. However, they rely on feature detection and match-ing that can be challenging especially when the scene lacks distinct details, and as a result the 3D reconstruction often becomes sparse and incomplete.
More recently, learning-based approaches have been in-troduced that enable dense depth estimation by exploiting priors learned from training images. In particular, monoc-ular depth estimation that leverages only a single image to-gether with learned priors has become a popular area of re-search, where deep neural networks are used to implement models that directly predict a depth map for given input im-age [39, 4, 17, 33, 55, 19]. While the basic idea is simple and attractive, the accuracy of the monocular depth estima-tion methods is limited by the lack of strong geometric con-straints such as parallax. Thus, considerably more accurate depth maps can be achieved with deep learning based multi-view stereo methods [53, 54, 34, 2]. However, the accuracy comes at the cost of increased computational complexity as multiple images need to be aggregated by the network to produce a single depth map.
Another approach for dense depth estimation is to start from depth sensors like LiDARs, and use depth completion to interpolate the missing depth values based on RGB data.
Despite of impressive results achieved by recent methods such as [56, 36, 18] they are mainly suitable for cases with relatively high 3D point density, but do not perform well with sparse point clouds.
In this paper, we start from monocular depth estima-tion and use a set of 3D points as constraints to obtain high-quality and dense depth maps as illustrated in Fig-ure 1. The main difference of our approach to previous depth completion methods is that the point cloud can be extremely sparse and unevenly sampled, which enables us-ing various methods for acquiring the 3D data, including conventional multi-view stereo, structure-from-motion and
SLAM pipelines but also range sensors such as LiDARs.
We argue that sparsity is important as it provides flexibil-ity and cost savings to depth sensing. For example, in mo-bile imaging, existing AR frameworks, i.e. ARCore [23],
ARKit [22], and AREngine [24], provide sparse 3D point clouds, while in robotics and autonomous driving applica-tions, low-resolution range sensors become sufficient. To this end, we propose a novel learning-based scheme for fus-ing RGB and 3D point data. More specifically, our contri-butions are the following:
• We introduce a novel multi-scale 3D point fusion neu-ral network architecture, which is more lightweight than the existing state-of-the-art depth completion methods while being able to efficiently exploit the ge-ometric constraints provided by a sparse set of 3D points.
• We demonstrate state-of-the-art results on NYU-Depth-v2 and KITTI datasets with a network that uses only a fraction of the number of parameters compared to the other recent architectures.
Figure 1. Dense depth prediction on the NYU-Depth-v2 [44] test set. A point cloud is produced by a conventional point-based sparse reconstruction method. The sparse 3D points and a single RGB image are fed to the network to estimate a high-quality depth map. The dense reconstructed point cloud (top right) preserves the geometry well. The chart (bottom right) shows values along the A-B line of the estimated depth map, confidence map, and the sampling points. The estimated depth value tend towards sampling point values. Confidence value around sampling point areas are higher. (All values in the chart are normalized, and sparse point are enhanced for visualization)
• We also show that our method combined with 3D point clouds obtained by using COLMAP [43, 42] outper-form recent deep learning based multi-view stereo and structure-from-motion methods both in accuracy and in compactness. 2.