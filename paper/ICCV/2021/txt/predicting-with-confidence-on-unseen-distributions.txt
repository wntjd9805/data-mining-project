Abstract
Recent work has shown that the accuracy of machine learning models can vary substantially when evaluated on a distribution that even slightly differs from that of the train-ing data. As a result, predicting model performance on pre-viously unseen distributions without access to labeled data is an important challenge with implications for increasing the reliability of machine learning models. In the context of distribution shift, distance measures are often used to adapt models and improve their performance on new domains, however accuracy estimation is seldom explored in these investigations. Our investigation determines that common distributional distances such as Frechet distance or Maxi-mum Mean Discrepancy, fail to induce reliable estimates of performance under distribution shift. On the other hand, we ﬁnd that our proposed difference of conﬁdences (DoC) approach yields successful estimates of a classiﬁer’s perfor-mance over a variety of shifts and model architectures. De-spite its simplicity, we observe that DoC outperforms other methods across synthetic, natural, and adversarial distribu-tion shifts, reducing error by (> 46%) on several realistic and challenging datasets such as ImageNet-Vid-Robust and
ImageNet-Rendition. 1.

Introduction
Even under the best of conditions, machine learning models are still susceptible to large variations in perfor-mance whenever the test data does not come from the same distribution as the training data. For instance, recent dataset replication studies have shown that despite concerted efforts to closely mirror the data generating process of the training set, model accuracy still changed substantially on the new test sets [43, 63, 37]. Machine learning models deployed in
*equal contribution
Figure 1: The average conﬁdence (AC) over a distribu-tion is a natural predictor of a model’s accuracy. Mod-els are often poorly calibrated, and while AC provides an overly optimistic prediction of accuracy, the difference of conﬁdences (DoC) between distributions provides a use-(Left) Conﬁdence his-ful estimate of accuracy changes. tograms of a ResNet-101 over the ImageNet validation set and ImageNet-Sketch. Average conﬁdences (AC) are higher than the corresponding accuracies. Our method, dif-ference of conﬁdences (DoC) can be computed from AC over the validation set and ImageNet-Sketch. (Right) shows the average conﬁdences (AC) as predictors of accuracy over various natural distribution shifts, contrasted with differ-ence of conﬁdences (DoC). DoC consistently improves over the baseline. real-world environments invariably encounter different data distributions. Hence reliable estimates of how well a model will perform on a new test set are critical. For instance, if a practitioner wants to use an X-ray classiﬁer on data from a new hospital, they require a good estimate of their clas-siﬁer’s performance on this previously unseen distribution to have conﬁdence in the results. In some settings it maybe
prohibitively costly to acquire new labeled data each time a model encounters a distribution shift. In order to mitigate the consequences of unexpected performance changes, we explore how to best predict changes in a model’s accuracy when we only have access to unlabeled data.
We investigate the underexplored problem of Automatic model Evaluation [8] over various model architectures and forms of natural and synthetic distribution shift. Distribu-tional distances such as Frechet distance [24, 65, 8], Max-imum Mean Discrepancy (MMD) [2] , and discriminative discrepancy [1, 11, 13], which are common tools for align-ment in domain adaptation, are evaluated as features in a regression model for predicting accuracy on unseen distri-butions. Recognizing the relationship of this problem with that of predictive uncertainty, we also evaluate the utility of model conﬁdences and entropy at predicting accuracy.
Learning our regression models over a set of synthetic distribution shifts, we show that common distributional dis-tances fail to reliably predict accuracy changes on natural distribution shifts. While most approaches are able to en-code useful information about held-out forms of synthetic distribution shift, they do not produce useful encodings for natural distribution shifts, where a regression-free average conﬁdence baseline AC outperforms them.
Surprisingly, however, we discover that a substantial amount of information about both synthetic and natural dis-tribution shifts is encoded in the difference of conﬁdences (DoC) of the classiﬁer’s predictions between the base (i.e., training) distribution and the previously unseen target distri-bution. In Figure 1, we show how DoC can be directly used to estimate the accuracy gap between base and target distri-butions. Treating DoC as a feature, we obtain regression models which substantially outperform all other methods and reduce predictive error by nearly half (46%) across all challenging natural distribution shifts such as the ImageNet-VidRobust [49] and ImageNet-Rendition [18] datasets. Our work demonstrates that it is possible to attain high quality estimates of a model’s accuracy across a variety of model architectures and types of distributions shifts. 2.