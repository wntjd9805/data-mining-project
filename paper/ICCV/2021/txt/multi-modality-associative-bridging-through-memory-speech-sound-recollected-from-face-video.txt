Abstract
In this paper, we introduce a novel audio-visual multi-modal bridging framework that can utilize both audio and visual information, even with uni-modal inputs. We exploit a memory network that stores source (i.e., visual) and tar-get (i.e., audio) modal representations, where source modal representation is what we are given, and target modal rep-resentations are what we want to obtain from the mem-ory network. We then construct an associative bridge be-tween source and target memories that considers the inter-relationship between the two memories. By learning the interrelationship through the associative bridge, the pro-posed bridging framework is able to obtain the target modal representations inside the memory network, even with the source modal input only, and it provides rich information for its downstream tasks. We apply the proposed frame-work to two tasks: lip reading and speech reconstruction from silent video. Through the proposed associative bridge and modality-specific memories, each task knowledge is en-riched with the recalled audio context, achieving state-of-the-art performance. We also verify that the associative bridge properly relates the source and target memories. 1.

Introduction
Recently, many studies are dealing with diverse infor-mation from multiple sources finding relationships among them [40]. Especially, deep learning based multi-modal learning has drawn big attention with its powerful perfor-mance. While the classic approaches [20, 5, 52, 53] need to design each modal feature manually, using Deep Neural
Networks (DNNs) has the advantage of automatically learn-ing meaningful representation from each modality. Many applications including action recognition [15, 25], object detection [11], and image/text retrieval [64] have shown the effectiveness of multi-modal learning through DNNs by an-alyzing a phenomenon in multi-view.
*Both authors have contributed equally to this work.
†Corresponding author
Figure 1. Illustration of audio-visual multi-modal learning. (a) Fu-sion of two modalities. (b) Learning from a common latent space of two modalities. (c) The proposed framework provides an asso-ciative bridge between two modalities through memory. The audio (i.e., target) modality is recalled from memory by querying the vi-sual (i.e., source) modality. Then, both the visual and the recalled audio modalities are utilized for a downstream task.
Audio-visual data is one of the main ingredients for multi-modal applications such as synchronization [7, 8], speech recognition [1, 38], and speech reconstruction from silent video [39, 3]. Along with the rapid increase of the demands for audio-visual applications, research efforts on how to efficiently handle audio-visual data have been made.
There are two main streams on handling audio-visual data.
First is to extract features from the two modalities and fuse them to achieve complementary effect, as shown in Fig.1 (a). Such researches [38, 1, 36] try to find the most suit-able architecture of DNNs to fuse the modalities. Com-monly used methods are early fusion, late fusion, and in-termediate fusion. These fusion methods are known to be simple, yet effectively improve the performance of a given task. However, since both modalities are necessary for the fusion, these methods cannot work when one of the modal-ities is missing. Second is finding a common hidden rep-resentation of two modalities by training DNNs (Fig.1 (b)).
Different from the first method, it can utilize the shared in-formation of both modalities from the learned cross-modal
representation with uni-modal inputs. This can be achieved by finding the common latent space of different modalities using metric learning [7, 8] or resembling the other modal-ity which contains rich information for a given task using knowledge distillation [63]. However, reducing the hetero-geneity gap [21], induced by inconsistent distribution of dif-ferent modalities, is still considered as a challenging prob-lem [19, 37].
In this paper, we propose a novel multi-modal bridging framework, especially in audio speech modality and visual face modality. The proposed framework brings the advan-tages of the two aforementioned audio-visual multi-modal learning methods, while alleviating the problems that each method contains. That is, it can obtain both audio and visual contexts during inference even when the uni-modal input is provided only. This gives explicit complementary knowl-edge with the multi-modal information to uni-modal tasks which could suffer from information insufficiency. Further-more, our work can be free from finding a common repre-sentation of different modalities, as shown in Fig.1(c).
To this end, we propose to handle the audio-visual data through memory network [55, 32] which contains two modality-specific memories: source-key memory and target-value memory. Each memory stores visual and audio features arranged in pairs, respectively. Then, an associa-tive bridge is constructed between the two modality-specific memories, to access the target-value memory by querying the source-key memory with source modal representation.
Thus, when one modality (i.e., source) is given, the pro-posed framework can recall the other saved modality (i.e., target) from target-value memory through the associative bridge. This enables it to complement the information of uni-modal input with the recalled target modal information.
Therefore, we can enrich the task-solving ability of a down-stream task. The proposed framework is verified on two ap-plications using audio-visual data: lip reading, and speech reconstruction from silent video by using visual modality as source modality and audio modality as target modality.
In summary, the major contributions of this paper are as follows:
• We propose a novel audio-visual multi-modal bridging framework that enables it to utilize the information of multi-modality (i.e., audio and visual modalities) with uni-modal (i.e., visual) input during inference.
• We verify the effectiveness of the proposed framework lip reading and speech recon-on two applications: struction from silent video and achieve state-of-the-art performances. Moreover, we visualize that the asso-ciative bridge adequately relates the source and target memories.
• Through the proposed modality-specific memory oper-ation (i.e., querying by source modality and recalling target modality), it does not need to find a common latent space of different modalities. We analyze it by comparing the proposed framework with the methods finding a common latent space of multi-modal data. 2.