Abstract
Recently, several frameworks for self-supervised learn-ing of 3D scene flow on point clouds have emerged. Scene flow inherently separates every scene into multiple moving agents and a large class of points following a single rigid sensor motion. However, existing methods do not lever-age this property of the data in their self-supervised train-ing routines which could improve and stabilize flow predic-tions. Based on the discrepancy between a robust rigid ego-motion estimate and a raw flow prediction, we generate a self-supervised motion segmentation signal. The predicted motion segmentation, in turn, is used by our algorithm to attend to stationary points for aggregation of motion infor-mation in static parts of the scene. We learn our model end-to-end by backpropagating gradients through Kabsch’s algorithm and demonstrate that this leads to accurate ego-motion which in turn improves the scene flow estimate. Us-ing our method, we show state-of-the-art results across mul-tiple scene flow metrics for different real-world datasets, showcasing the robustness and generalizability of this ap-proach. We further analyze the performance gain when per-forming joint motion segmentation and scene flow in an ab-lation study. We also present a novel network architecture for 3D LiDAR scene flow which is capable of handling an order of magnitude more points during training than previ-ously possible. 1.

Introduction
Reasoning about the motion of dynamic objects in a scene is crucial for robotic applications such as autonomous driving. One representation for such motion is scene flow, a set of displacement vectors between two consecutive time frames, forming a 3D vector field Ft→t+1. Its application allows to transform points Pt recorded in the first frame
∗ Joint first authors with equal contribution.
Figure 1: Scene Flow Prediction by SLIM. Flow vectors are colored by their direction and magnitude as illustrated with the color wheel. Our method correctly annotates the background as static and produces consistent results for the two vehicles, matching the ground truth scene flow. into the second frame via p(t+1) i + fi. Another rep-resentation is the binary classification of points into either moving or stationary, known as motion segmentation.
= p(t) i
This paper tackles the problem of simultaneous LiDAR scene flow estimation and motion segmentation using a deep network. Existing works treat these two problems in-dependently. Most of them rely on large amounts of labeled data to obtain state-of-the-art results (see Section 2.2). Self-supervised learning attempts to alleviate this problem, but existing scene flow networks using weak or self-supervision have critical drawbacks: Currently, all of them require downsampling of point clouds from around 128,000 points for typical LiDAR frames to no more than 8,192 points dur-ing training or they train and evaluate on datasets with 1-to-1 point correspondences, see Section 2.4.
Our approach trains well even on datasets with no 1-to-1 correspondences and can handle one to two orders of magnitude more points per point cloud than previous ap-proaches for self-supervised scene flow. We build on top of
RAFT [39], the state-of-the-art for optical flow, and extend
the network to perform iterative scene flow estimation and motion segmentation on PointPillar-based Bird’s-Eye-View (BEV) feature representations of point clouds [20]. This architecture realizes an excellent trade-off between gener-alizability, accuracy, and computational efforts. Using the consistency of motion for the stationary background, we are able to extract training targets for self-supervised mo-tion segmentation. No additional data or labels besides raw, consecutively recorded point clouds are required for train-ing, the cues for self-supervised classification come from the typical motion profiles of these classes alone.
Our main contributions are:
• Our method is the first point-cloud-based scene flow estimation method that can classify points as ”moving” and ”stationary” based on self-supervised training.
• Our method significantly outperforms previous meth-ods in point-cloud-based scene flow estimation, espe-cially in terms of generalization to previously unseen data, which we demonstrate on multiple datasets, in both the self-supervised and fully supervised setting.
• Our novel network architecture can handle signif-icantly more points than current weakly or self-supervised approaches. 2.