Abstract
The backbone of traditional CNN classifier is generally considered as a feature extractor, followed by a linear layer which performs the classification. We propose a novel loss function, termed as CAM-loss, to constrain the embedded feature maps with the class activation maps (CAMs) which indicate the spatially discriminative regions of an image for particular categories. CAM-loss drives the backbone to ex-press the features of target category and suppress the fea-tures of non-target categories or background, so as to ob-tain more discriminative feature representations. It can be simply applied in any CNN architecture with neglectable additional parameters and calculations. Experimental re-sults show that CAM-loss is applicable to a variety of net-work structures and can be combined with mainstream reg-ularization methods to improve the performance of image classification. The strong generalization ability of CAM-loss is validated in the transfer learning and few shot learn-ing tasks. Based on CAM-loss, we also propose a novel
CAAM-CAM matching knowledge distillation method. This method directly uses the CAM generated by the teacher net-work to supervise the CAAM generated by the student net-work, which effectively improves the accuracy and conver-gence rate of the student network. 1.

Introduction
In the past few years, convolutional neural networks (CNNs) have achieved excellent performance in many vi-sual classification tasks. To handle the increasingly com-plex data, CNNs have continuously been improved with deeper structures (AlexNet [22], VGGNet [32], ResNet
[14], ResNext [46], DenseNet [18]). However, deep net-works are prone to overfitting while they get stronger learn-ing ability. Many researchers have proposed effective reg-ularization solutions, such as Dropout [33], Weight Decay
[10], Stochastic Depth [19], Mixup [54], Shakedrop [47],
Cutmix [51]. An alternative solution is to design differ-ent loss functions to obtain more distinguishing feature rep-∗Equal contribution.
†Corresponding author.
Figure 1. Some examples to illustrate our motivation. A ResNet-50 model trained on ImageNet is adopted. “GT” represents the ground truth label. “CE” represents cross entropy loss. “ours” rep-resents CAM-loss. Black bounding boxes show main differences between (b) and (c), while white ones show main differences be-tween (c) and (d). resentations, which increase intra-class compactness and inter-class separability. Inspired by such idea, contrastive loss [12], triplet loss [30], center loss [45] are proposed to bring in additional constraints on the basis of cross entropy loss. Unfortunately, they usually dramatically increase the computational cost. L-Softmax [26] and SM-Softmax [25] are proposed to modify the original softmax function math-ematically, leading to potentially larger angular separabil-ity between feature vectors.
Implicit semantic data aug-mentation (ISDA) loss [41] is proposed to optimize the up-per bound of expected cross-entropy loss. However, when adopting the above loss functions, an input image is repre-sented by a one-dimensional feature vector, which collapses the spatial information.
In this paper, we propose to construct a novel loss func-tion by leveraging the class activation maps (CAMs [56]) with rich spatial information. CAM indicates the spatial discriminative regions to identify a particular category. It is easily obtained by computing a weighted sum of the fea-ture maps of the last convolutional layer. In fact, we can also obtain a class-agnostic activation map (CAAM [2]) by computing the sum of the feature maps directly, which in-dicates the spatial distribution of the embedded features. To describe our motivation visually, in Figure 1 we show some validation images misclassified by a pre-trained ResNet-50 [14] model with cross entropy loss on ImageNet [29].
The CAMs of target categories, CAAMs and output labels are shown in columns (b), (c), and (e), respectively. A vi-sual conclusion is that CAAMs generally show larger ac-tivated areas and richer features than CAMs of target cate-gories. Unfortunately, the redundant feature representations (black bounding box areas in column (c)) result in the con-fidence scores of non-target categories (red labels in col-umn (e)) exceeding those of target categories (green labels in column (e)), which lead to misclassification. For exam-ple, the expression of body makes the model misidentify an ox as a black bear, and the expression of ears and mane makes the model misidentify a horse as a dog. Intuitively, if we constrain CAAMs closer to CAMs of target categories, features of the target categories will be expressed well and those of non-target categories will be suppressed simulta-neously. This effectively enforces intra-class compactness and inter-class separability.
Based on the above inspiration, we construct a new loss function, termed as CAM-loss, by minimizing the distance between the CAAM and the CAM of target category for each training image. CAM-loss drives the backbone to learn more discriminative feature representations from a spatial perspective. We train another ResNet-50 model with CAM-loss, and show CAAMs and output labels of the same im-It shows that CAAMs pro-ages in Figure 1 (d) and (f). duced by CAM-loss are usually cleaner than those produced by cross entropy loss (comparing column (c) with column (d)). Some features of non-target categories are suppressed (white bounding box areas in column (d)), which greatly improves the accuracy of labels (comparing column (e) with column (f)). In fact, extensive experiments show that CAM-loss effectively improves the performances of various clas-sification models.
As an independent loss module, CAM-loss can also be combined with the mainstream regularization methods to improve their performances. Furthermore, we verify the strong generalization ability of CAM-loss in transfer learn-ing and few shot learning tasks. CAM-loss particularly boosts the baseline method by 7.04% (1-shot) and 4.75% (5-shot) on CUB [38], 2.78% (1-shot) and 1.68% (5-shot) on Mini-ImageNet [37] in the setting of few shot learning.
This is attributed to the key role of CAM-loss in reducing the negative effect of image background.
In the traditional teacher-student knowledge distillation framework, the existing methods all use a certain type of teacher knowledge to supervise the same type of student knowledge, such as soft target [15], hints [28], attention map [52], relationship between samples [27] or layers [49].
Inspired by CAM-loss, we propose a different idea to match different types of knowledge between teacher and student, that is, to directly supervise the CAAMs generated by the student network with the CAMs generated by the teacher network. We call it CAAM-CAM matching (CCM) knowl-edge distillation. The experimental results show that CCM can effectively improve the accuracy and convergence rate of the student network.
The main contributions of our work are:
• We propose a novel loss function CAM-loss from the perspective of spatial information.
It can effec-tively improve the classification performance of vari-ous CNN models with neglectable additional param-eters and calculations, and easily be combined with the mainstream regularization methods to achieve the state-of-the-art on CIFAR-100 and ImageNet.
• CAM-loss shows strong generalization capability in transfer learning and few shot learning tasks. In par-ticular, CAM-loss significantly improves the perfor-mance of few shot image classification.
• We propose a novel knowledge distillation method named CAAM-CAM matching, which matches differ-ent types of knowledge between teacher (CAMs) and student (CAAMs), and improves the accuracy and con-vergence rate of the student network simultaneously. 2.