Abstract
We present ARCH++, an image-based method to recon-struct 3D avatars with arbitrary clothing styles. Our re-constructed avatars are animation-ready and highly real-istic, in both the visible regions from input views and the unseen regions. While prior work shows great promise of reconstructing animatable clothed humans with various topologies, we observe that there exist fundamental limi-tations resulting in sub-optimal reconstruction quality. In this paper, we revisit the major steps of image-based avatar reconstruction and address the limitations with ARCH++.
First, we introduce an end-to-end point based geometry en-coder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features. Second, in order to address the occupancy ambi-guity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to jointly estimate the occu-pancy in both the posed and canonical spaces. Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints. In the experiments, we demonstrate improve-ments over the state of the art on both public benchmarks and user studies in reconstruction quality and realism. 1.

Introduction
Digital humans have become an increasingly important building block for numerous AR/VR applications, such as video games, social telepresence [48, 39] and virtual try-on.
Towards truly immersive experiences, it is crucial for these avatars to obtain higher level of realism that goes beyond the uncanny valley [45]. Building a photorealistic avatar in-volves many manual works by artists or expensive capture systems under controlled environments [14, 21, 49], limit-ing access and increasing cost. Therefore, it is vital to revo-lutionize reconstruction techniques with minimal prerequi-site (e.g., a selfie) for future digital human applications.
Recent human models reconstructed from a single im-age combine category-specific data prior with image ob-* This work was done as part of Tong He’s internship at Facebook,
Sausalito, CA, USA. The corresponding author is Yuanlu Xu.
Figure 1. Given an image of a subject in arbitrary pose (left), our method could generate photorealistic avatars in both the posed in-put space (middle) as well as auto-rigged canonical space (right). servations [72, 31, 66]. Among which, template-based ap-proaches [32, 34, 67, 3, 9] nevertheless suffer from lack of fidelity and difficulty supporting clothing variations; while non-parametric reconstruction methods [55, 75, 56, 23], e.g., using implicit surface functions, do not provide intu-itive ways to animate the reconstructed avatar despite im-In the recent work ARCH [26], the au-pressive fidelity. thors propose reconstructing non-parametric human model using pixel-aligned implicit functions [55] in a canonical space, where all reconstructed avatars are transformed to a common pose. To do so, a parametric human body model is exploited to determine the transformations. By transfer-ring skinning weights, which encode how much each ver-tex is influenced by the transformation of each body joint, from the underling body model, the reconstruction results are ready to animate. However, we observe that the advan-tages of a parametric body model and pixel-aligned implicit functions are not fully exploited.
In this paper we introduce ARCH++, which revisits the major steps of animatable avatar reconstruction from im-ages and addresses the limitations in the formulation and representation of the prior work. First, current implicit function based methods mainly use hand-crafted features as the 3D space representation, which suffers from depth ambiguity and lacks human body semantic information. To address this, we propose an end-to-end geometry encoder
based on PointNet++ [52, 53], which expressively describes the underlying 3D human body. Second, we find the unpos-ing process to obtain the canonical space supervision causes topology change (e.g., removing self-intersecting regions) and consequently the articulated reconstruction fails to ob-tain the same level of accuracy in the original posed space.
Therefore, we present a co-supervising framework where occupancy is jointly predicted in both the posed and canon-ical spaces, with additional constraints on the cross-space consistency. This way, we benefit from both: supervision in the posed space allows the prediction to retain all the de-tails of the original scans; while canonical space reconstruc-tion can ensure the completeness of a reconstructed avatar.
Last, image-based avatar reconstruction often suffers from degraded geometry and texture in the occluded regions. To make the problem more tractable, we first infer surface nor-mals and texture of the occluded regions in the image do-main using image translation networks, and then refine the reconstructed surface with a moulding-inpainting scheme.
In the experiments, we evaluate ARCH++ on photore-alistically rendered synthetic images as well as in-the-wild images, outperforming prior works based on implicit func-tions and other design choices on public benchmarks.
The contributions of ARCH++ include: 1) a point-based geometry encoder for implicit functions to directly extract human shape and pose priors, which is efficient and free from quantization errors; 2) we are the first to point out and study the fundamental issue of determining target oc-cupancy space: posed-space fidelity vs. canonical-space completeness. Albeit ignored before, we outline the pros and cons of different spaces, and propose a co-supervising framework of occupancy fields in joint spaces; 3) we dis-cover image-based surface attribute estimation could ad-dress the open problem of view-inconsistent reconstruction quality. Our moulding-inpainting surface refinement strat-egy generates 360◦ photorealistic 3D avatars. 4) our method demonstrates enhanced performance on the brand new task of image-based animatable avatar reconstruction. 2.