Abstract
Image harmonization, aiming to make composite images look more realistic, is an important and challenging task.
The composite, synthesized by combining foreground from one image with background from another image, inevitably suffers from the issue of inharmonious appearance caused by distinct imaging conditions, i.e., lights. Current solu-tions mainly adopt an encoder-decoder architecture with convolutional neural network (CNN) to capture the context of composite images, trying to understand what it looks like in the surrounding background near the foreground. In this work, we seek to solve image harmonization with Trans-former, by leveraging its powerful ability of modeling long-range context dependencies, for adjusting foreground light to make it compatible with background light while keep-ing structure and semantics unchanged. We present the de-sign of our harmonization Transformer frameworks with-out and with disentanglement, as well as comprehensive ex-periments and ablation study, demonstrating the power of
Transformer and investigating the Transformer for vision.
Our method achieves state-of-the-art performance on both image harmonization and image inpainting/enhancement, indicating its superiority. Our code and models are avail-able at https://github.com/zhenglab/HarmonyTransformer. 1.

Introduction
Combining regions of different photographs into a realis-tic composite is a fundamental problem in many vision and graphics applications, such as image compositing, mosaic-ing, editing, and scene completion [33]. However, the com-posite, synthesized by combining the foreground from one image with the background from another image, inevitably suffers from the issue of inharmonious appearance between foreground and background caused by distinct imaging con-*Corresponding author: Haiyong Zheng (zhenghaiyong@ouc.edu.cn).
This work was supported by the National Natural Science Foundation of China under Grant Nos. 61771440 and 41776113.
Figure 1. We create two composite images about pig that ﬂies (top) and tiger at party (bottom), also show harmonization comparison between state-of-the-art DoveNet [9] and our method. ditions (e.g., day and night, sunny and cloudy, outdoors and indoors). Therefore, making the composite look more re-alistic, namely image harmonization, is an important and challenging task [33, 35, 10, 9].
Image harmonization aims to adjust the foreground to make it compatible with the background on the appear-ance. Essentially, the appearance of a natural image de-pends on various factors in the scene, such as illumination, material, and shape [41, 1]. For a composite image, the foreground and the background are considered semantically harmonious, although sometimes it might be impractical or unreasonable (e.g., pig that ﬂies and tiger at party in Fig-ure 1). Thus, the inharmony of composite images is mainly caused by the distinct lights in the different scenes between foreground and background while imaging, for instance, a tiger captured in the wild under natural light as foreground and the party captured in a hall under artiﬁcial lighting as background, yielding inharmonious color appearance be-cause an object appears coloured due to the way it interacts with light. Hence, adjusting the foreground color to make it compatible with the background color, while keeping the structure and semantics unchanged, are crucial and essential for harmonizing composite images.
Traditional harmonization methods have focused on bet-ter matching techniques to ensure consistent appearance be-tween foreground and background, by transferring hand-crafted statistics such as color and texture [39, 33]. Re-cently, deep harmonization models and large-scale datasets have been developed to address this challenging task [35,
10, 9], achieving better performance beneﬁting from deep model and big data. Current deep models mainly adopt an encoder-decoder CNN architecture, which employs an en-coder to capture the context of the composite image and a decoder to reconstruct the harmonized image, trying to un-derstand what it looks like in the surrounding background region near the foreground region.
Actually, the encoder-decoder CNN tackle image har-monization with a two-stage process: harmonizing fore-ground with background and reconstructing the harmonized image. Essentially, the ﬁrst stage works on adjusting fore-ground color with background color to make them com-patible, while the second stage devotes to recovering orig-inal structure and semantics. However, since CNN inher-ently has the inductive bias of locality, a shallow CNN can only capture the context of surrounding background near the foreground, and without global background context, it might be not enough for better adjustment to make the color of foreground and background consistent. Besides, previ-ous methods adopt U-Net [32] with successive contraction, which has the ability to capture global context, but the in-harmony might be introduced again to reconstruction via skip connections from encoder to decoder as a side effect.
Recently, Transformer [36] won renown as a new type of neural network, which can capture long-range context dependencies, thanks to the self-attention design. Instead of RNN and LSTM, Transformer was ﬁrst applied to natu-ral language processing (NLP) tasks where it achieved sig-niﬁcant improvements [36, 12, 4]. Nowadays Transformer is also showing it is a viable alternative to CNN by being applied to computer vision (CV) tasks, such as object de-tection [5, 43], image recognition [14], and image process-ing [6]. Thus, in this work, we seek to solve image harmo-nization with Transformer, by leveraging its powerful abil-ity of modeling long-range context, to satisfy the require-ment of harmonization on capturing global context.
Inspired by the observation that adjusting light plays a key role in harmonizing images [16], we move one step forward. Based on intrinsic image [2] and Retinex the-ory [26, 25] with the assumption of ideal Lambertian sur-face, the light intensity values represented in an image ac-tually encode all the characteristics of corresponding scene points, thus, in order to adjust the light of composite images, it is intuitive to separate material-dependent reﬂectance for light-dependent illumination re-rendering with disentangled background light for better harmonization. Therefore, in our work, we further devise to harmonize composite images by capturing the “light” from the background and put it on the “material” via disentangled harmonization Transformer.
Our contributions include: (1) we design and build the
ﬁrst harmonization Transformer frameworks without and with disentangled representation; (2) we explore and ana-lyze the harmonization Transformer in the aspects of input, encoder/decoder, head, and layer; (3) we present compre-hensive experiments to show the efﬁcacy of both Trans-former and disentanglement, achieving performance sub-stantially better than previous methods on image harmo-nization; (4) we illustrate the utility of our framework in two extra vision tasks, i.e., image inpainting and image en-hancement, both producing very competitive results. 2.