Abstract
Being able to learn dense semantic representations of images without supervision is an important problem in com-puter vision. However, despite its signiﬁcance, this problem remains rather unexplored, with a few exceptions that con-sidered unsupervised semantic segmentation on small-scale datasets with a narrow visual domain.
In this paper, we make a ﬁrst attempt to tackle the problem on datasets that have been traditionally utilized for the supervised case. To achieve this, we introduce a two-step framework that adopts a predetermined mid-level prior in a contrastive optimiza-tion objective to learn pixel embeddings. This marks a large deviation from existing works that relied on proxy tasks or end-to-end clustering. Additionally, we argue about the im-portance of having a prior that contains information about objects, or their parts, and discuss several possibilities to obtain such a prior in an unsupervised manner.
Experimental evaluation shows that our method comes with key advantages over existing works. First, the learned pixel embeddings can be directly clustered in semantic groups using K-Means on PASCAL. Under the fully unsu-pervised setting, there is no precedent in solving the se-mantic segmentation task on such a challenging benchmark.
Second, our representations can improve over strong base-lines when transferred to new datasets, e.g. COCO and
DAVIS. The code is available1. 1.

Introduction
The problem of assigning dense semantic labels to im-ages, formally known as semantic segmentation, is of great importance in computer vision as it ﬁnds many applications, including autonomous driving, augmented reality, human-computer interaction, etc. To achieve state-of-the-art per-formance in this task, fully convolutional networks [45] are typically trained on datasets [15, 20, 44] that contain a large number of fully-annotated images. However, obtain-ing accurate, pixel-wise semantic labels for every image in a dataset is a labor-intensive process that costs signiﬁcant
*Authors contributed equally 1github.com/wvangansbeke/Unsupervised-Semantic-Segmentation.git
Figure 1. We learn pixel embeddings for semantic segmentation in an unsupervised way. First, we predict object mask proposals using unsupervised saliency. Second, we use the obtained masks as a prior in a self-supervised optimization objective. Finally, the pixel embeddings can be clustered or ﬁne-tuned to a semantic seg-mentation of the image. amounts of time and money [4]. To remedy this situation, weakly-supervised methods leveraged weaker forms of su-pervision, such as scribbles [43, 65, 66, 72, 80], bounding boxes [16, 37, 56, 80], clicks [5], and image-level tags [56, 66, 80], while semi-supervised methods [16, 26, 28, 56, 57] used only a fraction of the dataset as labeled examples, both of which require substantially less human annotation effort.
Despite the continued progress, the vast majority of seman-tic segmentation works still rely on some form of annota-tions to train the neural network models.
In this paper, we look at the problem from a different perspective, namely self-supervised representation learn-ing. More concretely, we aim to learn pixel-level repre-sentations or embeddings for semantic segmentation with-out using ground-truth. If we obtain a good pixel embed-ding that is discriminative w.r.t. the semantic classes, we can directly cluster the pixels into semantic groups using
K-Means. This tackles the semantic segmentation problem under the fully unsupervised setup. Alternatively, if a lim-ited number of annotated examples are available, the repre-sentations can be further ﬁne-tuned under a semi-supervised or transfer learning setup. In this paper, we primarily focus on the fully unsupervised setup, but include additional ﬁne-tuning experiments for the sake of completeness.
Unsupervised or self-supervised techniques [36] were recently being employed to learn rich and effective visual representations without external supervision. The obtained representations can subsequently be used for a variety of purposes, including task transfer learning [24], image clus-tering [2, 3, 71], semi-supervised classiﬁcation [12], etc.
Popular representation learning techniques used an instance discrimination task [78], that is treating every image as a separate class, to generate representations in an unsuper-vised way. Images and their augmentations are considered as positive examples of the class, while all other images are treated as negatives.
In practical terms, the instance dis-crimination task is formulated as a non-parametric classi-ﬁcation problem, and a contrastive loss [23, 54] is used to model the distribution of negative instance classes.
Purushwalkam and Gupta [61] showed that contrastive self-supervised methods learn to encode semantic informa-tion, since two views of the same image will always show a part of the same object, and no objects from other cate-gories. However, under this setup, there is no guarantee that the representations also learn to differentiate between pixels belonging to different semantic classes. For example, when foreground-background pairs frequently co-occur, e.g. cat-tle grazing on farmland, pixels belonging to the two classes can share their representation. This renders existing works based on instance discrimination less appropriate w.r.t. our goal of learning semantic pixel embeddings. To address these limitations, we propose to learn pixel-level, rather than image-level representations, in a self-supervised way.
The proposed method consists of two steps. First, we leverage an unsupervised saliency estimator to mine object mask proposals from the dataset. This mid-level visual prior transfers well across different datasets. In the second step, we use a contrastive framework to learn pixel embeddings.
The object mask proposals are employed as a prior - we pull embeddings from pixels belonging to the same object to-gether, and contrast them against pixels from other objects.
The generated representations are evaluated on the semantic segmentation task following standard protocols. The frame-work is illustrated in Figure 1.
Our contributions are: (1) We propose a two-step frame-work for unsupervised semantic segmentation, which marks a large deviation from recent works that relied on proxy tasks or end-to-end clustering. Additionally, we argue about the importance of having a mid-level visual prior which incorporates object-level information. This contrasts with earlier works that grouped pixels together based upon low-level vision tasks like boundary detection. (2) The proposed method is the ﬁrst able to tackle the semantic segmentation task on a challenging dataset like PASCAL under the fully unsupervised setting. (3) Finally, we report promising re-sults when transferring our representations to other datasets.
This shows that adopting a mid-level visual prior can be useful for self-supervised representation learning. 2.