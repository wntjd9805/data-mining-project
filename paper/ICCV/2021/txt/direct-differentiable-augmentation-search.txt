Abstract
Data augmentation has been an indispensable tool to improve the performance of deep neural networks, how-ever the augmentation can hardly transfer among differ-ent tasks and datasets. Consequently, a recent trend is to adopt AutoML technique to learn proper augmentation policy without extensive hand-crafted tuning.
In this pa-per, we propose an efﬁcient differentiable search algorithm called Direct Differentiable Augmentation Search (DDAS).
It exploits meta-learning with one-step gradient update and continuous relaxation to the expected training loss for ef-ﬁcient search. Our DDAS can achieve efﬁcient augmen-tation search without relying on approximations such as
Gumbel-Softmax or second order gradient approximation.
To further reduce the adverse effect of improper augmen-tations, we organize the search space into a two level hi-erarchy, in which we ﬁrst decide whether to apply aug-mentation, and then determine the speciﬁc augmentation policy. On standard image classiﬁcation benchmarks, our
DDAS achieves state-of-the-art performance and efﬁciency tradeoff while reducing the search cost dramatically, e.g.
In addition, we also use 0.15 GPU hours for CIFAR-10.
DDAS to search augmentation for object detection task and achieve comparable performance with AutoAugment [8], while being 1000× faster. Code will be released in https:
//github.com/zxcvfd13502/DDAS_code 1.

Introduction
Due to the “data hungry” nature of deep neural net-works (DNN), data augmentation techniques, such as ﬂip-ping, rotation, cropping, and color jittering, are essential tools to improve the performance. Data augmentation cre-ates rich variation of data samples to reduce over-ﬁtting issues caused by the high complexity of DNN. Although various hand-crafted data augmentation techniques [11, 19, 36, 39, 41, 45] are proposed recently, it’s non-trivial to com-bine and adapt them when confronting a new task or dataset.
This procedure usually requires expertise and extensive ex-periments to determine the optimal conﬁguration. For ex-(a) CIFAR-100 (b) ImageNet
Figure 1. Comparison between DDAS and other state-of-the-art
Automatic Augmentation works, including AA [8], RA [9], PBA
[18], Fast AA [25], Faster AA [14] and DADA [24]. Higher accu-racy and lower search cost (upper left) are preferred. Our DDAS is signiﬁcantly more efﬁcient while achieving even better perfor-mance. ample, an improper augmentation may not only be useless, but also may introduce harmful outliers in training.
AutoAugment (AA) [8] is the pioneering work for auto-matic augmentation policy search. It utilizes an RL-based algorithm to search for optimal policy within a search space of 16 augmentation operations. During search, AA max-imizes the accuracy on the validation set by optimizing 3 parameters: augmentation operation, its probability and magnitude, (op, prob, mag). Despite its impressive per-formance on image classiﬁcation and detection tasks, the search cost of AA is still prohibitive for democratizing the technique, e.g. it requires thousands of GPU hours just for searching on a small dataset like CIFAR-10. A popular ap-proach is to make the optimization differentiable, which makes the gradient estimation more efﬁcient. Following works, such as [4, 14, 18, 24, 25], manage to decrease the search cost to 0.1 − 0.2 hours, but with visible performance degradation.
On the other hand, AA only searches for a ﬁxed aug-mentation policy while several works [17, 34] point out that dynamic augmentation policy may result in better performance. Following works such as OHL-Auto-Aug (OHLAA) [26] or Adverisial Augment (AdvAA) [42] pro-pose online augmentation search manners that augmenta-tion search is conducted together with training and differ-ent augmentation policies will be used for different training
epochs. These online approaches achieve signiﬁcant im-provements over the ofﬂine counterpart, however their costs remain high, and the policies can hardly transfer to different training tasks.
To achieve faster search with better performance, we introduce Direct Differentiable Augmentation Search (DDAS). Inspired by recent work on meta-learning [29,32], our goal is to ﬁnd an augmentation policy which maximizes the network performance after one step gradient update. In-spired by OHLAA, we organize the augmentation policy into a two level hierarchy: we ﬁrstly determine the proba-bility of augmenting the data, then we decide the probabil-ity of each augmentation operation. In this way, the pol-icy has a chance to discard all augmentations. Then the differentiable search can be derived without tricks such as
Gumbel-Softmax [20] or second order gradient approxima-tion [29], as the probabilities naturally become weights of samples when considering the expectation of loss.
We verify the effectiveness of our DDAS on various models and datasets, including CIFAR-10/100 [22] and Im-ageNet [10]. All results show that our DDAS can achieve competitive or better performance while dramatically re-ducing the search cost. In Fig.1, we visualize the compari-son between DDAS and other methods on CIFAR-100 and
ImageNet. We can see that our proposed DDAS is on the testing accu-Pareto optimal curve of the search cost v.s. racy. In addition, we also try to search for augmentation policy for object detection task. As far as we know, this task is rarely tackled due to its prohibitive cost. Thanks to our highly efﬁcient search method, we could achieve results comparable with previous work [46] that adopted AutoAug-ment for object detection, while costing 1000× less GPU hours. Our contributions can be summarized as follows: 1. We propose Direct Differentiable Augmentation
Search (DDAS), an efﬁcient differentiable augmenta-tion policy search algorithm. Through meta-learning with one-step gradient update, we can achieve efﬁcient and effective augmentation search. 2. We propose a compact yet ﬂexible search space by ex-plicitly modeling the probability of adopting augmen-tation. This design along with the epoch-wise policy reduces the adverse risk of aggressive augmentation. 3. Besides the thorough evaluation experiments for im-age classiﬁcation, we are the ﬁrst work to demonstrate efﬁcient augmentation search for object detection (20
GPU hours) is feasible. 2.