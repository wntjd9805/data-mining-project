Abstract
Video-and-Language Inference is a recently proposed task for joint video-and-language understanding. This new task requires a model to draw inference on whether a natu-ral language statement entails or contradicts a given video clip.
In this paper, we study how to address three crit-ical challenges for this task: judging the global correct-ness of the statement involved multiple semantic meanings, joint reasoning over video and subtitles, and modeling long-range relationships and complex social interactions. First, we propose an adaptive hierarchical graph network that achieves in-depth understanding of the video over complex interactions. Specifically, it performs joint reasoning over video and subtitles in three hierarchies, where the graph structure is adaptively adjusted according to the semantic structures of the statement. Secondly, we introduce seman-tic coherence learning to explicitly encourage the semantic coherence of the adaptive hierarchical graph network from three hierarchies. The semantic coherence learning can fur-ther improve the alignment between vision and linguistics, and the coherence across a sequence of video segments. Ex-perimental results show that our method significantly out-performs the baseline by a large margin. 1.

Introduction
Understanding video story involves analyzing and sim-ulating human vision, language, thinking, and behavior, which is a significant challenge to current machine learning technology [21]. Recently, with the advances of large-scale video datasets [1, 5, 8, 24, 49], joint video-and-language un-derstanding has received increased attention. Several video-and-language tasks have been proposed, such as video cap-∗Siliang Tang is the corresponding author.
Figure 1: The first two rows show a video clip paired with its aligned subtitles. The third row shows a statement with multiple semantic meanings. tioning [16, 48, 61, 14, 27, 13, 41, 50], text-to-video tempo-ral grounding [15, 3, 6, 29, 36, 59, 63], and video question answering [28, 60, 47, 23, 25, 37]. In particular, Video-and-Language Inference (VLI) [33] is a recently proposed task to foster deeper investigations in video-and-language under-standing. Given a video clip with aligned subtitles and a nat-ural language statement based on the video content, a model needs to infer whether the statement entails or contradicts the given video clip. To support the study of this new task, a large-scale dataset, named VIOLIN (VIdeO-and-Language
INference), is introduced.
Compared with TVQA/video captioning where most QA pairs/captions focus on identifying explicit visual cues (e.g., objects, actions, persons), VLI is more challenging and re-quires more sophisticated reasoning skills, such as inter-preting human emotions and relations, understanding the events, and inferring causal relations of events throughout the video. First, a single statement may involve multiple se-mantic meanings, making it harder to judge the global cor-rectness. As demonstrated in Figure 1, the statement con-sists of three semantic phrases. If the model recognizes the central meaning and the temporal meaning but ignores the
causal meaning, it may make a wrong prediction. Secondly,
VLI requires jointly reasoning over video and subtitles to achieve in-depth understanding of complex plots. To infer the causal meaning that the man pretends it is his own office, the model needs to jointly understand information from the video part and subtitle part. From the video part, the man and the woman are in the same office, and the man takes the phone from the woman. From the subtitle part, the man lies that the woman is his secretary. Only by combining the con-text from both the video and subtitle can the model further draw the inference. Thirdly, VLI requires reasoning for di-verse interactions among characters and complex event dy-namics over diverse scenarios. The VIOLIN dataset is col-lected from diverse sources to cover realistic visual scenes, including 5885 movie clips in addition to TV shows used in
TVQA. The average clip length is 35.20s, while the length of most clips in TVQA is less than 15s.
In this paper, we propose a novel adaptive hierarchical graph reasoning with semantic coherence approach to over-come the aforementioned challenges. First, we introduce an adaptive graph construction mechanism to identify the multiple semantic meanings of the statement. This enables our approach to adaptively adjust the graph structure ac-cording to the semantic structures of the statement for the global correctness. Then, we present an adaptive hierarchi-cal graph network (AHGN) to jointly reason over video and subtitles and model the complex social interactions. Specif-ically, we perform adaptive graph reasoning in three hierar-chies: 1) segment-level reasoning, which achieves in-depth understanding of the video segments via utilizing the in-herent alignment and complementary nature between visual frames and subtitles; 2) temporal-level reasoning, which models the long-range dependencies and diverse interac-tions between different segments to draw a global video understanding; 3) global-level reasoning, which judges the global correctness of the statement by incorporating the in-ferences from different reasoning steps.
Furthermore, the semantic coherence throughout AHGN is crucial to achieving global understanding of the video.
Therefore, we introduce a novel semantic coherence learn-ing (SCL) method to encourage the cross-modal semantic coherence at the segment level and the cross-level semantic coherence between temporal level and global level. Specif-ically, the semantic coherence learning contains two reg-ularization terms: an optimal transport distance term that measures the cross-modality alignment between the visual nodes and subtitle nodes, and a mutual information term that evaluates the semantic coherence between the temporal nodes and global nodes.
The experiments show that our approach significantly outperforms the baselines by a large margin, and further ab-lation study demonstrates the effectiveness of each compo-nent. In summary, our contributions are mainly three folds:
• We propose a novel adaptive hierarchical graph net-work (AHGN) that performs joint reasoning over video and subtitles in three hierarchies, where the graph reasoning structure is adaptively adjusted ac-cording to the semantic structures of the statement.
• Our semantic coherence learning (SCL) method im-proves the alignment between video and subtitles, and the coherence across a sequence of video segments.
• Extensive experiments show that our method signifi-cantly outperforms the baseline by a large margin. 2.