Abstract
High-fidelity face digitization solutions often combine multi-view stereo (MVS) techniques for 3D reconstruction and a non-rigid registration step to establish dense corre-spondence across identities and expressions. A common problem is the need for manual clean-up after the MVS step, as 3D scans are typically affected by noise and outliers and contain hairy surface regions that need to be cleaned up by artists. Furthermore, mesh registration tends to fail for extreme facial expressions. Most learning-based meth-ods use an underlying 3D morphable model (3DMM) to ensure robustness, but this limits the output accuracy for extreme facial expressions.
In addition, the global bot-tleneck of regression architectures cannot produce meshes that tightly fit the ground truth surfaces. We propose ToFu,
Topological consistent Face from multi-view, a geometry inference framework that can produce topologically con-sistent meshes across facial identities and expressions us-ing a volumetric representation instead of an explicit un-derlying 3DMM. Our novel progressive mesh generation network embeds the topological structure of the face in a feature volume, sampled from geometry-aware local fea-tures. A coarse-to-fine architecture facilitates dense and accurate facial mesh predictions in a consistent mesh topol-ogy. ToFu further captures displacement maps for pore-level geometric details and facilitates high-quality render-ing in the form of albedo and specular reflectance maps.
These high-quality assets are readily usable by production studios for avatar creation, animation and physically-based skin rendering. We demonstrate state-of-the-art geometric and correspondence accuracy , while only taking 0.385 sec-onds to compute a mesh with 10K vertices, which is three orders of magnitude faster than traditional techniques. The code and the model are available for research purposes at https://tianyeli.github.io/tofu. 1.

Introduction
Creating high-fidelity digital humans is not only highly sought after in the film and gaming industry, but is also gain-ing interest in consumer applications, ranging from telep-resence in AR/VR to virtual fashion models and virtual as-sistants. While fully automated single-view avatar digitiza-tion solutions exist [28, 29, 42, 56, 63], professional studios still opt for high resolution multi-view images as input, to ensure the highest possible fidelity and surface coverage in a controlled setting [8, 23, 25, 40, 41, 46, 50] instead of un-constrained input data. Typically, high-resolution geometric details (< 1mm error) are desired along with high resolu-tion physically-based material properties (at least 4K). Fur-thermore, to build a fully rigged face model for animation, a
large number of facial scans and alignments (often over 30) are performed, typically following some conventions based on the Facial Action Coding System (FACS).
A typical approach used in production consists of using a multi-view stereo acquisition process to capture detailed 3D scans of each facial expression, and a non-rigid regis-tration [8, 36] or inference method [37] is used to warp a 3D face model to each scan in order to ensure consistent mesh topology. Between these two steps, manual clean-up is often necessary to remove artifacts and unwanted surface regions, especially those with facial hair (beards, eyebrows) as well as teeth and neck regions. The registration process is often assisted with manual labeling tasks for correspon-dences and parameter tweaking to ensure accurate fitting. In a production setting, a completed rig of a person can easily take up to a week to finalize.
Several recent techniques have been introduced to au-tomate this process by fitting a 3D model directly to a cali-brated set of input images. The multi-view stereo face mod-eling method of [21] is not only particularly slow, but relies on dynamic sequences and carefully tuned parameters for each subject to ensure consistent parameterization between expressions.
In particular facial expressions that are not captured continuously cannot ensure accurate topological consistencies. More recent deep learning approaches [4, 63] use a 3D morphable model (3DMM) inference to obtain a coarse initial facial expression, but require a refinement step based on optimization to improve fitting accuracy. Those methods are limited in fitting extreme expressions due to the constraints of linear 3DMMs and fitting tightly to the ground-truth face surfaces due to the global nature of their regression architectures. The additional photometric refine-ment also tends to fit unwanted regions like facial hair.
We introduce a new volumetric approach for consistent 3D face mesh inference using multi-view images. Instead of relying explicitly on a mesh-based face model such as 3DMM, our volumetric approach is more general, allowing it to capture a wider range of expressions and subtle defor-mation details on the face. Our method is also three orders of magnitude faster than conventional methods, taking only 0.385 seconds to generate a dense 3D mesh (10K vertices) as well as produce additional assets for high-fidelity produc-tion use cases, such as albedo, specular, and high-resolution displacement maps.
To this end, we propose a progressive mesh generation network that can infer a topologically consistent mesh di-rectly. Our volumetric architecture predicts vertex locations as probability distributions, along with volumetric features that are extracted using the underlying multi-view geome-try. The topological structure of the face is embedded into this architecture using a hierarchical mesh representation and coarse-to-fine network.
Our experiments show that ToFu is capable of produc-ing highly accurate geometry consistent with topology au-tomatically, while existing methods either rely on manual clean-up and parameter tuning, or are less accurate espe-cially for subjects with facial hair. Since we can ensure a consistent parameterization across facial identities and ex-pressions without any human input, our solution is suitable for scaled digitization of high-fidelity facial avatars, We not only reduce the turn around time for production, but is also provide a critical solution for generating large facial datasets, which is often associated with excessive manual labor. Our main contributions are:
• A novel volumetric feature sampling and refinement model for topologically consistent 3D mesh recon-struction from multi-view images.
• An appearance capture network to infer high-resolution skin details and appearance maps, which, combined with the base mesh, forms a complete pack-age suitable for production in animation and photore-alisitic rendering.
• We demonstrate state-of-the-art performance for com-bined geometry and correspondence accuracy, while achieving mesh inference at near interactive rates.
• Code and model are publicly available. 2.