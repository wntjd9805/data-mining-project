Abstract
This paper presents a self-supervised method for learn-ing reliable visual correspondence from unlabeled videos.
We formulate the correspondence as finding paths in a joint space-time graph, where nodes are grid patches sampled from frames, and are linked by two type of edges: (i) neigh-bor relations that determine the aggregation strength from intra-frame neighbors in space, and (ii) similarity relations that indicate the transition probability of inter-frame paths across time. Leveraging the cycle-consistency in videos, our contrastive learning objective discriminates dynamic ob-jects from both their neighboring views and temporal views.
Compared with prior works, our approach actively explores the neighbor relations of central instances to learn a latent association between center-neighbor pairs (e.g., “hand – arm”) across time, thus improving the instance discrimina-tion. Without fine-tuning, our learned representation out-performs the state-of-the-art self-supervised methods on a variety of visual tasks including video object propagation, part propagation, and pose keypoint tracking. Our self-supervised method also surpasses some fully supervised al-gorithms designed for the specific tasks. 1.

Introduction
Learning temporal correspondence — a problem of learning “what went where”— is closely related to many fundamental vision tasks, such as video object tracking [46, 26, 45], video object segmentation [48, 4, 44, 29, 32], and flow estimation [7, 14].
In essence, it corresponds to a query-target matching problem, which relies on an affinity to match a physical point (or patch) in the query frame t to that in the target frame t + k. One practical issue is col-lecting dense annotations from large-scale videos, which costs large human efforts.
It motivates numerous self-supervised methods [45, 50, 24, 22, 21, 47, 15] to learn dynamic objects from unlabeled videos by leveraging the cycle-consistency in time as a free supervisory signal.
Figure 1. How to find the correspondence of a small object such as the dog tail in a video? We argue that the query-target matching desires both longer views (temporal dynamics) and broader views (neighbor relations) to distinguish similar instances. We capture these two cues in a graph to learn correspondence.
Recent approaches learn strong representations by con-structing long-range views mainly from two perspectives: (i) learning pixel-level correspondences by a single-step as-sociation [24, 47], or (ii) learning patch-level correspon-dences by a multi-step association [15]. The single-step association can be viewed as a pixel-level affinity between two patches at timesteps t and t + k, aiming to transform the pixel colors. Such transformation requires a determin-istic correspondence to locate the target patch at t + k, which is achieved by training an extra unsupervised patch tracker [50]. However, the underlying assumption that cor-responding pixels have the same color may be violated, e.g., inevitable lighting changes and deformation in future frames, thereby hindering the model from using longer tem-poral cues. Recently, Jabri et al. [15] formulate a multi-step association that connects corresponding patches at ev-ery timestep between t and t + k in a form of Markov chain.
At each step, the patch-level affinity links all patches of two adjacent frames, preserving all possible correspondences in the video. The learning process thus benefits from a longer-range clip with all intermediate views available. Unfortu-nately, finding an “optimal” correspondence is not easy due
to the struggling matching between similar instances where image patches only capture a very narrow view of them.
Hence, we identify another key ingredient for better query-target matching — seeing broader — which is ig-nored in existing methods. Let us take a closer look of an example shown in Figure 1. How do human track the right dog tail across frames, and avoid being confused by the left dog tail from the similar instance? (i) Seeing longer: how the shape of the tail changes over time is indeed a crucial cue, and it can be utilized in the multi-step association [15]. (ii) Seeing broader: it is easier to discriminate the dog tails from a broader view by considering neighboring informa-tion around them, such as the dog body features, as well as the dog-person interaction. However, it cannot be achieved by straightforwardly enlarging the patch size, as the detailed structures or features will be missed.
In this paper, we propose to learn correspondence by seeing both broader and longer via a graph-based frame-work. We represent the video as a joint space-time graph where nodes are grid patches and edges are two type of re-lations, i.e., neighbor relations and similarity relations. The big graph can thus be decomposed into two sub-graphs. (i) Neighbor Relation Graph: we start by constructing a small graph for each node, which is linked to intra-frame nodes that are located in a sliding neighborhood.
Initial-ized with the topological prior, the edges learn to guide the aggregation of neighboring node representations to the cen-tral node. The updated node representation thus captures a broader view of the neighborhood. (ii) Similarity Graph: we then connect inter-frame nodes with the pairwise sim-ilarity, under the updated node representations. All these edges form a multi-step association for a long-range clip.
Given the joint graph, the prediction of the long-range correspondence can be computed as a path (a combination of similarity-based edges) along the graph. To induce su-pervision, we adopt palindrome sequences [15] for training, which provide the walker with a target, i.e., returning to the start point. In contrast to the prior work [15], our path-level constraint provides contrastive learning signals from both the temporal views and neighboring views, leading to more reliable matching among similar instances. Moreover, we perform a random but attentive walk on the large graph by wisely dropping the “common-fate” [51] nodes accord-ing to the pixel discrepancy of each node, encouraging the model to focus on more informative node pairs. Below, we summarize the major contributions of this work.
• First, we design a joint video graph that models neigh-bor relations in space and similarity relations in time for visual correspondence learning.
• Second, we formulate the contrastive learning as a ran-dom but attentive walk on the graph to learn discrim-inative representations from seeing both temporal and neighboring views of instances.
• Third, our method outperforms state-of-the-art self-supervised approaches on a variety of visual tasks, e.g., object, part propagation, and pose tracking. It also sur-passes some task-specific fully supervised algorithms. 2.