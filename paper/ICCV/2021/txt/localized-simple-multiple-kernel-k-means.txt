Abstract
As a representative of multiple kernel clustering (MKC), simple multiple kernel k-means (SimpleMKKM) is recently put forward to boosting the clustering performance by op-timally fusing a group of pre-speciﬁed kernel matrices. De-spite achieving signiﬁcant improvement in a variety of ap-plications, we ﬁnd out that SimpleMKKM could indiscrimi-nately force all sample pairs to be equally aligned with the same ideal similarity. As a result, it does not sufﬁciently take the variation of samples into consideration, leading to unsatisfying clustering performance. To address these is-sues, this paper proposes a novel MKC algorithm with a
“local” kernel alignment, which only requires that the sim-ilarity of a sample to its k-nearest neighbours be aligned with the ideal similarity matrix. Such an alignment helps the clustering algorithm to focus on closer sample pairs that shall stay together and avoids involving unreliable sim-ilarity evaluation for farther sample pairs. After that, we theoretically show that the objective of SimpleMKKM is a special case of this local kernel alignment criterion with normalizing each base kernel matrix. Based on this obser-vation, the proposed localized SimpleMKKM can be readily implemented by existing SimpleMKKM package. Moreover, we conduct extensive experiments on several widely used benchmark datasets to evaluate the clustering performance of localized SimpleMKKM. The experimental results have demonstrated that our algorithm consistently outperforms the state-of-the-art ones, verifying the effectiveness of the proposed local kernel alignment criterion. The code of Lo-calized SimpleMKKM is publicly available at: https:// github.com/xinwangliu/LocalizedSMKKM . 1.

Introduction
Multiple kernel clustering (MKC) provides an elegant framework to group samples into different clusters by ex-tracting complementary information from multiple sources
[25, 21, 26, 8, 5, 18, 22, 29, 10, 7, 27, 28]. Given a group of
*Corresponding Author pre-deﬁned kernel matrices, MKC integrates the available multiple kernel information to categorize data items with similar structures or patterns into the same group, which has been intensively studied and widely applied into vari-ous applications [9, 14, 9, 23, 17, 16, 13]. For example, the work in [11] proposes a multiple kernel k-means clustering algorithm with a matrix-induced regularization term to re-duce the redundancy of the selected kernels. A local kernel alignment variant is then developed by sufﬁciently consid-ering the variation among sample, which is experimentally veriﬁed to enhance the clustering performance in [9]. By assuming an optimal kernel residing in the neighborhood of the combined kernels, the work in [14] proposes an optimal neighborhood multiple kernel clustering algorithm, which improves the clustering performance by enhancing the rep-resentability of the learned optimal kernel. Differently, late fusion based multiple kernel clustering strategy seeks to exploit the complementary information in kernel partition space to achieve consensus on partition level [23]. Specif-ically, the pioneering work in [23] proposes to maximally align the multiple base partitions with the consensus parti-tion, which enjoys considerable algorithm acceleration and satisfactory clustering performance. Along this line, an ef-fective and efﬁcient late fusion based algorithm is proposed in [12] to handle incomplete multi-view data.
As a representative of MKC, a novel simple multiple ker-nel k-means (SimpleMKKM) is recently proposed [15]. In-stead of jointly minimizing the kernel weights and cluster-ing partition matrix, SimpleMKKM takes a minimization on kernel weights and maximization on clustering partition matrix optimization framework, leading to an intractable min-max optimization. After that, it is equivalently trans-formed into a minimization problem and a reduced gradient algorithm is designed to solve the resultant optimization.
This algorithm is validated to be efﬁcient for optimization, robust against the noisy views, and has attracted intensive attention of many researchers.
Although the recently proposed SimleMKKM bears the aforementioned merits, we observe that it strictly aligns the combined kernel matrix with an “ideal” similarity gener-ated by the clustering partition matrix in a global way. This could indiscriminately force all sample pairs to be equally aligned with the same ideal similarity. As a result, it does not effectively handle the variation among samples and ig-nore local structures, which could lead to unsatisfying clus-tering performance. To address the above issue, we pro-pose to calculate the kernel alignment in a “local” manner, which only requires that the generated combined kernel be aligned with the ideal similarity matrix locally in the k-nearest neighborhood of each sample. Such an alignment guides the clustering algorithm to focus on closer sample pairs that shall stay together and avoid involving unreliable similarity evaluation for farther sample pairs. By this way, our proposed algorithm could sufﬁciently consider the vari-ation among samples, leading to improved clustering per-formance. We then derive the objective function of our al-gorithm based on the minimization-maximization optimiza-tion framework of SimpleMKKM. After that, we theoreti-cally show that SimpleMKKM is a special case of our pro-posed algorithm. Base on this observation, our proposed local variant can be readily implemented by SimpleMKKM packages via simply normalizing each base kernel. Com-prehensive experiments have been conducted on several benchmark datasets, and the results have well validated the effectiveness of the proposed localized SimpleMKKM. The main contributions of this paper are summarized as follows,
• We, for the ﬁrst time, identify that the recently pro-posed SimpleMKKM cannot effectively handle the variation among kernel matrices, and develop a local kernel alignment criterion to address this issue.
• We theoretically reveal the connection between Sim-pleMKKM and our proposed algorithm, and point out that the former is a special case of ours.
• Extensive experiments are conducted on several public datasets to evaluate the effectiveness of our proposed algorithm. As indicated, the experimental results have demonstrated that our algorithm consistently outper-forms the state-of-the-art competitors, verifying its ef-fectiveness and efﬁciency. 2.