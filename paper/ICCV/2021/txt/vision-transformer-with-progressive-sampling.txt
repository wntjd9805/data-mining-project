Abstract
Transformers with powerful global relation modeling abilities have been introduced to fundamental computer vi-sion tasks recently. As a typical example, the Vision Trans-former (ViT) directly applies a pure transformer architec-ture on image classiﬁcation, by simply splitting images into tokens with a ﬁxed length, and employing transformers to learn relations between these tokens. However, such naive tokenization could destruct object structures, assign grids to uninterested regions such as background, and introduce interference signals. To mitigate the above issues, in this paper, we propose an iterative and progressive sampling strategy to locate discriminative regions. At each itera-tion, embeddings of the current sampling step are fed into a transformer encoder layer, and a group of sampling off-sets is predicted to update the sampling locations for the next step. The progressive sampling is differentiable. When combined with the Vision Transformer, the obtained PS-ViT network can adaptively learn where to look. The proposed
PS-ViT is both effective and efﬁcient. When trained from scratch on ImageNet, PS-ViT performs 3.8% higher than the vanilla ViT in terms of top-1 accuracy with about 4× fewer parameters and 10× fewer FLOPs. Code is available at https://github.com/yuexy/PS-ViT. 1.

Introduction
Transformers [39, 11] have become the de-facto standard architecture for natural language processing tasks. Thanks to their powerful global relation modeling abilities, re-searchers attempt to introduce them to fundamental com-puter vision tasks such as image classiﬁcation [6, 38, 12, 44, 32], object detection [56, 4, 53, 10, 37] and image seg-mentation [40] recently. However, transformers are initially
∗Equal Contribution. (a) (b)
Figure 1. Comparison between the naive tokenization scheme in
ViT [12] and the progressive sampling in our proposed PS-ViT. (a) The naive tokenization scheme generates a sequence of image patches which are embedded and then fed into a stack of trans-formers. (b) Our PS-ViT iteratively samples discriminative loca-tions. ×N indicates N sampling iterations. tailored for processing mid-size sequences, and of quadratic computational complexity w.r.t. the sequence length. Thus, they cannot directly be used to process images with massive pixels.
To overcome the computational complexity issue, the pi-oneer Vision Transformer (ViT) [12] adopts a naive tok-enization scheme that partitions one image into a sequence of regularly spaced patches, which are linearly projected into tokens. In this way, the image is converted into hun-dreds of visual tokens, which are fed into a stack of trans-former encoder layers for classiﬁcation. ViT attains ex-cellent results, especially when pre-trained on large-scale datasets, which proves that full-transformer architecture is
Figure 2. Comparisons between PS-ViT with state-of-the-art networks in terms of top-1 accuracy on ImageNet, parameter number, FLOPs, and speed. The chart on the left, middle and right show top-1 accuracy vs. parameter numbers, FLOPs and speed respectively. The speed is tested on the same V100 with a batch size of 128 for fair comparison. a promising alternative for vision tasks. However, the lim-itations of such a naive tokenization scheme are obvious.
First, the hard splitting might separate some highly seman-tically correlated regions that should be modeled with the same group of parameters, which destructs inherent object structures and makes the input patches to be less informa-tive. Figure 1 (a) shows that the cat head is divided into several parts, resulting in recognition challenges based on one part only. Second, tokens are placed on regular grids irrespective of the underlying image content. Figure 1 (a) shows that most grids focus on the uninterested background, which might lead to the interesting foreground object is sub-merged in interference signals.
The human vision system organizes visual information in a completely different way than indiscriminately process-ing a whole scene at once. Instead, it progressively and se-lectively focuses attention on interesting parts of the visual space when and where it is needed while ignoring uninter-ested parts, combining information from different ﬁxations over time to understand the scene [33].
Inspired by the procedure above, we propose a novel transformer-based progressive sampling module, which is able to learn where to look in images, to mitigate the issues caused by the naive tokenization scheme in ViT [12]. In-stead of sampling from ﬁxed locations, our proposed mod-ule updates the sampling locations in an iterative manner.
As shown in Figure 1 (b), at each iteration, tokens of the current sampling step are fed into a transformer encoder layer, and a group of sampling offsets is predicted to update the sampling locations for the next step. This mechanism utilizes the capabilities of the transformer to capture global information to estimate offsets towards regions of interest, by combining with the local contexts and the positions of current tokens.
In this way, attention progressively con-verges to discriminative regions of images step by step as what human vision does. Our proposed progressive sam-pling is differentiable, and readily plugged into ViT instead of the hard splitting, to construct end-to-end Vision Trans-forms with Progressive Sampling Networks dubbed as PS-ViT. Thanks to task-driven training, PS-ViT tends to sample object regions correlated with semantic structures. More-over, it pays more attention to foreground objects while less to ambiguous background compared with simple tokeniza-tion.
The proposed PS-ViT outperforms the current state-of-the-art transformer-based approaches when trained from scratch on ImageNet. Concretely, it achieves 82.3% top-1 accuracy on ImageNet which is higher than that of the recent ViT’s variant DeiT [38] with only about 4× fewer parameters and 2× fewer FLOPs. As shown in Fig-ure 2, we observe that PS-ViT is remarkably better, faster, and more parameter-efﬁcient compared to state-of-the-art transformer-based networks ViT and DeiT. 2.