Abstract
We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impres-sive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360◦ scenes, it of-ten finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We intro-duce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic at-tributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments,
DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions. Our project website is available at https://www.ajayj.com/dietnerf. 1.

Introduction
In the novel view synthesis problem, we seek to rerender a scene from arbitrary viewpoint given a set of sparsely sam-pled viewpoints. View synthesis is a challenging problem that requires some degree of 3D reconstruction in addition to high-frequency texture synthesis. Recently, great progress has been made on high-quality view synthesis when many observations are available. A popular approach is to use Neu-ral Radiance Fields (NeRF) [25] to estimate a continuous neural scene representation from image observations. During training on a particular scene, the representation is rendered
Figure 1. Neural Radiance Fields are trained to represent a scene by supervising renderings from the same pose as ground-truth observations (MSE loss). However, when only a few views are available, the problem is underconstrained. NeRF often finds degen-erate solutions unless heavily regularized. Based on the principle that “a bulldozer is a bulldozer from any perspective”, our pro-posed DietNeRF supervises the radiance field from arbitrary poses (DietNeRF cameras). This is possible because we compute a se-mantic consistency loss in a feature space capturing high-level scene attributes, not in pixel space. We extract semantic representa-tions of renderings using the CLIP Vision Transformer [28], then maximize similarity with representations of ground-truth views. In effect, we use prior knowledge about scene semantics learned by single-view 2D image encoders to constrain a 3D representation. from observed viewpoints using volumetric ray casting to compute a reconstruction loss. At test time, NeRF can be ren-dered from novel viewpoints by the same procedure. While conceptually very simple, NeRF can learn high-frequency view-dependent scene appearances and accurate geometries that allow for high-quality rendering.
Still, NeRF is estimated per-scene, and cannot benefit from prior knowledge acquired from other images and ob-jects. Because of the lack of prior knowledge, NeRF requires
Figure 2. Few-shot view synthesis is a challenging problem for Neural Radiance Fields. (A) When we have 100 observations of an object from uniformly sampled poses, NeRF estimates a detailed and accurate representation that allows for high-quality view synthesis purely from multi-view consistency. (B) However, with only 8 views, the same NeRF overfits by placing the object in the near-field of the training cameras, leading to misplaced objects at poses near training cameras and degeneracies at novel poses. (C) We find that NeRF can converge when regularized, simplified, tuned and manually reinitialized, but no longer captures fine details. (D) Finally, without prior knowledge about similar objects, single-scene view synthesis cannot plausibly complete unobserved regions, such as the left side of an object seen from the right. In this work, we find that these failures occur because NeRF is only supervised from the sparse training poses. a large number of input views to reconstruct a given scene at high-quality. Given 8 views, Figure 2B shows that novel views rendered with the full NeRF model contain many ar-tifacts because the optimization finds a degenerate solution that is only accurate at observed poses. We find that the core issue is that prior 3D reconstruction systems based on rendering losses are only supervised at known poses, so they overfit when few poses are observed. Regularizing NeRF by simplifying the architecture avoids the worst artifacts, but comes at the cost of fine-grained detail.
Further, prior knowledge is needed when the scene recon-struction problem is underdetermined. 3D reconstruction systems struggle when regions of an object are never ob-served. This is particularly problematic when rendering an object at significantly different poses. When rendering a scene with an extreme baseline change, unobserved regions during training become visible. A view synthesis system should generate plausible missing details to fill in the gaps.
Even a regularized NeRF learns poor extrapolations to un-seen regions due to its lack of prior knowledge (Figure 2D).
Recent work trained NeRF on multi-view datasets of sim-ilar scenes [44, 38, 32, 37, 42] to bias reconstructions of novel scenes. Unfortunately, these models often produce blurry images due to uncertainty, or are restricted to a single object category such as ShapeNet classes as it is challenging to capture large, diverse, multi-view data.
In this work, we exploit the consistency principle that
“a bulldozer is a bulldozer from any perspective”: objects share high-level semantic properties between their views.
Image recognition models learn to extract many such high-level semantic features including object identity. We transfer prior knowledge from pre-trained image encoders learned on highly diverse 2D single-view image data to the view synthesis problem. In the single-view setting, such encoders are frequently trained on millions of realistic images like
ImageNet [7]. CLIP is a recent multi-modal encoder that is trained to match images with captions in a massive web scrape containing 400M images [28]. Due to the diversity of its data, CLIP showed promising zero- and few-shot transfer performance to image recognition tasks. We find that CLIP and ImageNet models also contain prior knowledge useful for novel view synthesis.
We propose DietNeRF, a neural scene representation based on NeRF that can be estimated from only a few pho-tos, and can generate views with unobserved regions. In addition to minimizing NeRF’s mean squared error losses at known poses in pixel-space, DietNeRF penalizes a semantic consistency loss. This loss matches the final activations of
CLIP’s Vision Transformer [9] between ground-truth images and rendered images at different poses, allowing us to super-vise the radiance field from arbitrary poses. In experiments, we show that DietNeRF learns realistic reconstructions of objects with as few as 8 views without simplifying the un-derlying volumetric representation, and can even produce reasonable reconstructions of completely occluded regions.
To generate novel views with as few as 1 observation, we fine-tune pixelNeRF [44], a generalizable scene representa-tion, and improve perceptual quality. 2.