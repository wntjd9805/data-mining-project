Abstract
In this paper, we propose an anchor-free single-stage
LiDAR-based 3D object detector – RangeDet. The most no-table difference with previous works is that our method is purely based on the range view representation. Compared with the commonly used voxelized or Bird’s Eye View (BEV) representations, the range view representation is more com-pact and without quantization error. Although there are works adopting it for semantic segmentation, its perfor-mance in object detection is largely behind voxelized or
BEV counterparts. We ﬁrst analyze the existing range-view-based methods and ﬁnd two issues overlooked by previ-ous works: 1) the scale variation between nearby and far away objects; 2) the inconsistency between the 2D range image coordinates used in feature extraction and the 3D
Cartesian coordinates used in output. Then we deliber-ately design three components to address these issues in our RangeDet. We test our RangeDet in the large-scale
Waymo Open Dataset (WOD). Our best model achieves 72.9/75.9/65.8 3D AP on vehicle/pedestrian/cyclist. These results outperform other range-view-based methods by a large margin, and are overall comparable with the state-of-the-art multi-view-based methods. Codes will be released at https://github.com/TuSimple/RangeDet. 1.

Introduction
LiDAR-based 3D object detection is an indispensable technology in the autonomous driving scenario. Though shared some similarities, object detection in the 3D sparse point cloud is fundamentally different from its 2D coun-terpart. The key is to efﬁciently represent the sparse and unordered point clouds for subsequent processing. Several
*The ﬁrst two authors contribute equally to this work and are listed in the alphabetical order.
Figure 1. Different views in LiDAR-based 3D object detection. popular representations include Bird’s Eye View (BEV) [9, 38, 37], Point View (PV) [25], Range View (RV) [11, 18] and fusion of them [24, 44, 33], which are shown in Fig.1.
Among them, BEV is the most popular one. However, it in-troduces quantization error when dividing the space into the voxels or pillars, which is unfriendly for the distant objects that may only have few points. To overcome this drawback, the point view representation is usually incorporated. Point view operators [22, 23, 34, 31, 35, 30, 17] can extract effec-tive features from unordered point clouds, but they are dif-ﬁcult to scale up to large-scale point cloud data efﬁciently.
The range view is widely adopted in semantic segmen-tation task [19, 36, 42, 43], but it is rarely used in object detection individually. However, in this paper, we argue that the range view itself is the most compact and informa-tive way for representing the LiDAR point clouds because it is generated from a single viewpoint. It essentially forms a 2.5D [7] scene instead of a full 3D point cloud. Conse-quently, organizing the point cloud in range view misses no
information. The compactness also enables fast neighbor-hood queries based on range image coordinates, while point view methods usually need a time-consuming ball query al-gorithm [23] to get the neighbors. Moreover, the valid de-tection range of range-view-based detectors can be as far as the sensor’s availability, while we have to set a threshold for the detection range in BEV-based 3D detectors. Despite its advantages, an intriguing question raised, Why are the re-sults of range-view-based LiDAR detection inferior to other representation forms?
Indeed some works have made attempts to make use of the range view from the pioneering work VeloFCN [11] to
LaserNet [18] to the recently proposed RCD [1]. How-ever, there is still a huge gap between the pure range-view-based method and the BEV-based method. For example, on
Waymo Open Dataset (WOD) [29], they are still lower than state-of-the-art methods by a large margin.
To liberate the power of range view representation, we examine the designs of the current range-view-based detec-tors and found several overlooked facts. These points seem simple and obvious, but we ﬁnd that the devils are in the details. Properly handling these challenges is the key to high-performance range-view-based detection.
First, the challenge of detecting objects with sparse points in BEV is converted to the challenge of scale varia-tion in the range image, which is never seriously considered in the range-view-based 3D detector.
Second, the 2D range view is naturally compact, which makes it possible to adopt high resolution output without huge computational burden. However, how to utilize such characteristics to improve the performance of detectors is ignored by current range-image-based designs.
Third and most important, unlike in 2D image, though the convolution on range image is conducted on 2D pixel coordinates, while the output is in the 3D space. This point suggests an inferior design in the current range-view-based detectors: both the kernel weight and aggregation strategy of standard convolution ignore this inconsistency, which leads to severe geometric information loss even from the very beginning of the network.
In this paper, we propose a pure range-view-based framework – RangeDet, which is a single-stage anchor-free detector designated to address the aforementioned chal-lenges. We analyze the defects of the existing range-view-based 3D detector and point out the aforementioned three key challenges that need to be addressed. For the ﬁrst challenge, we propose a simple yet effective Range Con-ditioned Pyramid to mitigate it. For the second challenge, we use weighted Non-Maximum Suppression to remedy the issue. For the third one, we propose Meta-Kernel to capture 3D geometric information from 2D range view representa-tion. In addition to these techniques, we also explore how to transfer common data augmentation techniques from 3D space to the range view. Combining all the techniques, our best model achieves comparable results with state-of-the-art works in multiple views. And we surpass previous pure range-view-based detectors by a margin of 20 3D AP in ve-hicle detection. Interestingly, in contrast to common belief,
RangeDet is more advantageous for farther or small objects than BEV representation. 2.