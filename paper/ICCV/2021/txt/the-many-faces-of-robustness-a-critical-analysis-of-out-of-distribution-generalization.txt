Abstract
We introduce four new real-world distribution shift datasets consisting of changes in image style, image blur-riness, geographic location, camera operation, and more.
With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We ﬁnd that using larger models and artiﬁcial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work.
We ﬁnd improvements in artiﬁcial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pre-trained with 1000× more labeled data. Overall we ﬁnd that some methods consistently help with distribution shifts in tex-ture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes.
Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness. 1.

Introduction
While the research community must create robust models that generalize to new scenarios, the robustness literature
[6, 9] lacks consensus on evaluation benchmarks and con-tains many dissonant hypotheses. Hendrycks et al., 2020 [14]
ﬁnd that many recent language models are already robust to many forms of distribution shift, while others [37, 10] ﬁnd that vision models are largely fragile and argue that data aug-mentation offers one solution. In contrast, other researchers
[30] provide results suggesting that using pretraining and improving in-distribution test set accuracy improves natural robustness, whereas other methods do not.
*Equal contribution. 1UC Berkeley, 2UChicago, 3Google. Code is available at https://github.com/hendrycks/imagenet-r.
Prior works have also offered various interpretations of empirical results, such as the Texture Bias hypothesis that convolutional networks are biased towards texture, harming robustness [10]. Additionally, some authors posit a funda-mental distinction between robustness on synthetic bench-marks vs. real-world distribution shifts, casting doubt on the generality of conclusions drawn from experiments conducted on synthetic benchmarks [30].
It has been difﬁcult to arbitrate these hypotheses because existing robustness datasets vary multiple factors (e.g., time, camera, location, etc.) simultaneously in unspeciﬁed ways
[26, 16]. Existing datasets also lack diversity such that it is hard to extrapolate which methods will improve robustness more broadly. To address these issues and test the methods outlined above, we introduce four new robustness datasets and a new data augmentation method.
First we introduce ImageNet-Renditions (ImageNet-R), a 30,000 image test set containing various renditions (e.g., paintings, embroidery, etc.) of ImageNet object classes.
These renditions are naturally occurring, with textures and local image statistics unlike those of ImageNet images, al-lowing us to compare against gains on synthetic robustness benchmarks.
Next, we investigate the effect of changes in the image capture process with StreetView StoreFronts (SVSF) and
DeepFashion Remixed (DFR). SVSF contains business store-front images collected from Google StreetView, along with metadata allowing us to vary location, year, and even the camera type. DFR leverages the metadata from DeepFash-ion2 [8] to systematically shift object occlusion, orienta-tion, zoom, and scale at test time. Both SVSF and DFR provide distribution shift controls and do not alter texture, which remove possible confounding variables affecting prior benchmarks.
Additionally, we collect Real Blurry Images, which con-sists of 1,000 blurry natural images from a 100-class sub-set of the ImageNet classes. This benchmark serves as a real-world analog for the synthetic blur corruptions of the
ImageNet-C benchmark [12]. With it we ﬁnd that synthetic corruptions correlate with corruptions that appear in the wild, 1
Figure 1: Images from three of our four new datasets ImageNet-Renditions (ImageNet-R), DeepFashion Remixed (DFR), and StreetView StoreFronts (SVSF). The SVSF images are recreated from the public Google StreetView. Our datasets test robustness to various naturally occurring distribution shifts including rendition style, camera viewpoint, and geography. contradicting speculations from previous work [30]. more robustness datasets.
Finally, we contribute DeepAugment to increase robust-ness to some new types of distribution shift. This augmenta-tion technique uses image-to-image neural networks for data augmentation. DeepAugment improves robustness on our newly introduced ImageNet-R benchmark and can also be combined with other augmentation methods to outperform a model pretrained on 1000× more labeled data.
We use these new datasets to test four overarching classes of methods for improving robustness:
• Larger Models: increasing model size improves robust-ness to distribution shift [12, 35].
• Self-Attention: adding self-attention layers to models improves robustness [16].
• Diverse Data Augmentation: robustness can increase through data augmentation [37].
• Pretraining: pretraining on larger and more diverse datasets improves robustness [25, 13].
After examining our results on these four new datasets as well as prior benchmarks, we can rule out several previous hypotheses while strengthening support for others. As one example, we ﬁnd that synthetic data augmentation robust-ness interventions improve accuracy on ImageNet-R and real-world image blur distribution shifts, which lends cre-dence to the use of synthetic robustness benchmarks and also reinforces the Texture Bias hypothesis. In the conclusion, we summarize the various strands of evidence for and against each hypothesis. Across our many experiments, we do not
ﬁnd a general method that consistently improves robust-ness, and some hypotheses require additional qualiﬁcations.
While robustness is often spoken of and measured as a single scalar property like accuracy, our investigations show that robustness is not so simple. Our results show that future robustness research requires more thorough evaluation using 2.