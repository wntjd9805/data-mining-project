Abstract
Recently, Cross-Modal Hamming space Retrieval (CMHR) regains ever-increasing attention, mainly beneﬁt-ing from the excellent representation capability of deep neu-ral networks. On the other hand, the vulnerability of deep networks exposes a deep cross-modal retrieval system to various safety risks (e.g., adversarial attack). However, at-tacking deep cross-modal Hamming retrieval remains un-derexplored.
In this paper, we propose an effective Ad-versarial Attack on Deep Cross-Modal Hamming Retrieval, dubbed AACH, which fools a target deep CMHR model in a black-box setting. Speciﬁcally, given a target model, we
ﬁrst construct its substitute model to exploit cross-modal correlations within hamming space, with which we create adversarial examples by limitedly querying from a target model. Furthermore, to enhance the efﬁciency of adver-sarial attacks, we design a triplet construction module to exploit cross-modal positive and negative instances. In this way, perturbations can be learned to fool the target model through pulling perturbed examples far away from the pos-itive instances whereas pushing them close to the negative ones. Extensive experiments on three widely used cross-modal (image and text) retrieval benchmarks demonstrate the superiority of the proposed AACH. We ﬁnd that AACH can successfully attack a given target deep CMHR model with fewer interactions, and that its performance is on par with previous state-of-the-art attacks. 1.

Introduction
Deep Neural Networks (DNNs) have been widely adopted to improve the retrieval performance in CMHR, where the early network layers capture the implicit struc-ture of cross-modal data, and binary codes are derived from a deeper network layer. Generally, the DNN architecture is trained to build the cross-modal correlations by detect-ing the semantic similarity or dissimilarity between differ-ent modalities. Inspired by such superior representation ca-*Equal contribution.
†Corresponding author. (cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:16)(cid:55)(cid:72)(cid:91)(cid:87) (cid:43)(cid:68)(cid:86)(cid:75)(cid:3)(cid:41)(cid:88)(cid:81)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81) (cid:38)(cid:85)(cid:82)(cid:86)(cid:86)(cid:16)(cid:48)(cid:82)(cid:71)(cid:68)(cid:79)(cid:3)(cid:39)(cid:68)(cid:87)(cid:68)(cid:3)(cid:51)(cid:68)(cid:76)(cid:85) (cid:43)(cid:68)(cid:80)(cid:80)(cid:76)(cid:81)(cid:74)(cid:3)(cid:54)(cid:83)(cid:68)(cid:70)(cid:72)(cid:3) (cid:19)(cid:20)(cid:20) (cid:1878) (cid:19)(cid:19)(cid:20) (cid:20)(cid:20)(cid:20) (cid:19)(cid:19)(cid:19) (cid:20)(cid:19)(cid:19) (cid:1876) (cid:92) (a) Cross-Modal Hamming Retrieval (cid:49)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72) (cid:52)(cid:88)(cid:72)(cid:85)(cid:92) (cid:51)(cid:72)(cid:85)(cid:87)(cid:88)(cid:85)(cid:69)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81) (cid:36)(cid:71)(cid:89)(cid:72)(cid:85)(cid:86)(cid:68)(cid:85)(cid:76)(cid:68)(cid:79) (cid:40)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72) (cid:36)(cid:81)(cid:70)(cid:75)(cid:82)(cid:85) (cid:36)(cid:87)(cid:87)(cid:68)(cid:70)(cid:78) (cid:51)(cid:88)(cid:86)(cid:75) (cid:51)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:89)(cid:72) (cid:49)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72) (cid:51)(cid:88)(cid:79)(cid:79) (cid:51)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:89)(cid:72) (b) Attack on Cross-Modal Hamming Retrieval
Figure 1: Regular cross-modal Hamming retrieval and our triplet-based cross-modal Hamming attack. pability of DNN, many efforts have been focused on em-ploying deep networks to enhance the correlations between modalities through learning a common representation in shared space. However, the robustness and stability of DNN structures have been largely overlooked: even the most ac-curate deep learning models can be easily deceived by a well-designed perturbation which is visually imperceptible to the human eye. Therefore, the growing costs and risks of the potential model failures have led to the study of adver-sarial attacks. In this paper we focus on a practical cross-modal Hamming adversarial attack that fulﬁlls two criteria: 1) the attack is designed for a black-box setting, where the target cross-modal network is normally unavailable, and the attacker can only interact with the target model by querying it; 2) the query efﬁciency should be highly prioritized con-sidering the practical case, that is, frequent and high volume queries will be easily discovered by defenders.
Despite plenty of adversarial attacks proposed in the lit-erature, the main attention only focuses on the problem of adversarial examples learning for image-based classiﬁca-tion or retrieval within a single modality. Little effort has been devoted to investigating how adversarial examples af-fect deep Hamming learning in cross-modal retrieval. There exist great differences in learning adversaries between the
existing classiﬁcation task and CMHR. As shown in Fig. 1a, given a query instance from one modality (e.g., image),
CMHR is applied to map original data into binary codes, and then execute bit-wise XOR operation to return semanti-cally related instances from another modality (e.g., text). In contrast, the attack on CMHR devotes to fooling a well-trained model to return semantically unrelated instances.
Therefore, the traditional classiﬁcation-oriented adversar-ial attacks are not suitable in CMHR. The pioneering work
CMLA [17] is the ﬁrst attempt to design adversarial sam-ples to deceive a target deep CMHR model. But, CMLA is not applicable to the practical cases in two main aspects.
First, CMLA is constructed for a white-box setting, where attackers need full knowledge of the target model, includ-ing model architectures and parameters. Second, the label information of query instances is required in CMLA, which is also not available in reality. Therefore, practical adver-sary example learning in CMHR is still an open problem.
Actually, considering the nature of CMHR is to explore and preserve the similar semantic structure among instances, we can design the triplet-based cross-modal Hamming attack as shown in Fig. 1b. The adversarial perturbation is learned and added into the query instance to manipulate the original similarity structure, reducing the distance of the query to the negative instance and enlarging its distance to the positive instance.
In this paper, we propose an effective Adversarial At-tack on Deep Cross-Modal Hamming Retrieval (AACH).
To be speciﬁc, AACH mainly focuses on attacking a deep cross-modal Hamming retrieval model in a black-box set-ting, which thus is more applicable to practical cases. In addition, to reduce the cost and risk during querying a tar-get model, we propose the cross-modal triplet construction module, where cross-modal positive and negative instances of the query are exploited to boost the learning of adver-sarial perturbations. We highlight the contributions of this work as follows:
• An adversarial example learning method for cross-modal Hamming retrieval is proposed under the black-box setting. Through constructing a surrogate model of the target networks, the proposed AACH learns cross-modal adversarial examples only by limitedly querying the target model, without any prior knowl-edge about the target model.
• To fully take advantage of the limited information ac-quired from target model, a simple yet effective cross-modal triplet construction module is designed, with which our surrogate model learns adversarial examples by mining cross-modal positive and negative instances in Hamming space.
• We evaluate the proposed AACH by attacking several state-of-the-art cross-modal retrieval models on three popular benchmarks, MIRFlickr-25K, NUS-WIDE, and MS COCO. Extensive results demonstrate the ef-fectiveness of the proposed triplet construction mod-ule and the capacity of our AACH in attacking a target deep CMHR model. 2.