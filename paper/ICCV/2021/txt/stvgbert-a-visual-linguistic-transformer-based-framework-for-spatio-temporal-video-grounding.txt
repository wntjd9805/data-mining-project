Abstract
Spatio-temporal video grounding (STVG) aims to lo-calize a spatio-temporal tube of a target object in an untrimmed video based on a query sentence. In this work, we propose a one-stage visual-linguistic transformer based framework called STVGBert for the STVG task, which can simultaneously localize the target object in both spatial and temporal domains. Speciﬁcally, without resorting to pre-generated object proposals, our STVGBert directly takes a video and a query sentence as the input, and then produces the cross-modal features by using the newly introduced cross-modal feature learning module ST-ViLBert. Based on the cross-modal features, our method then generates bound-ing boxes and predicts the starting and ending frames to produce the predicted object tube. To the best of our knowl-edge, our STVGBert is the ﬁrst one-stage method, which can handle the STVG task without relying on any pre-trained object detectors. Comprehensive experiments demonstrate our newly proposed framework outperforms the state-of-the-art multi-stage methods on two benchmark datasets Vid-STG and HC-STVG. 1.

Introduction
Vision and language play important roles for human to understand the world.
In recent years, with remarkable progress of deep neural networks, various vision-language tasks (e.g., image captioning [15, 22], dense video cap-tion [37, 36] and visual grounding [10, 35]) have attracted increasing attention from researchers.
Spatial-Temporal Video Grounding (STVG), which was introduced in the recent work [39], is a new and challeng-ing vision-language task. Given an untrimmed video and a
*Dong Xu is the corresponding author. textual description of an object, the STVG task aims to pro-duce a spatio-temporal tube (i.e., a sequence of bounding boxes [21, 20]) for the target object described by the given text description. Different from the existing grounding tasks in images, both spatial and temporal localizations are re-quired in the STVG task. Besides, how to effectively align visual and textual information through cross-modal feature learning in both spatial and temporal domains is also a key issue for accurately localizing the target object, especially in the challenging scenarios where different persons often perform similar actions within one scene.
Spatial localization in images/videos is a related visual grounding task, and spatial localization results have been improved in recent works [13, 35, 14, 27, 30, 31, 12, 29, 2]. In most existing works, a pre-trained object detector is often required to pre-generate object proposals. However, these approaches suffer from the following limitations: (1)
The localization performance heavily relies on the quality of the pre-generated object proposals. (2) It is difﬁcult for a pre-trained object detector to be well generalized to any new datasets with unseen classes. (3) Additional training data and computational cost are required for pre-training the object detectors. Although the recent works [34, 10, 16, 33] have attempted to remove the pre-generation process in the image grounding task, such efforts have not been made for the video grounding task.
For the STVG task, we are required to conduct localiza-tion both spatially and temporally. Intuitively, we can solve this task by using a two-stage approach, in which the tem-poral visual grounding methods [5, 1] are ﬁrst used to local-ize the starting and ending frames of the target objects, and spatial localization is then performed by using the spatial visual grounding approaches [3, 17, 29] on the temporally trimmed videos. However, by handling the two sub-tasks separately, the pipeline becomes more complicated as each sub-task is handled by an independent network. Moreover,
we can also learn better representations by solving both spa-tial localization and temporal localization in an end-to-end optimized network. Therefore, it is desirable to propose a uniﬁed one-stage framework for the STVG task.
Motivated by the above observations, in this work, we propose a one-stage visual-linguistic transformer based framework STVGBert for the STVG task, which can di-rectly generate spatio-temporal object tubes from the input videos and query descriptions without relying on any pre-trained object detectors. Speciﬁcally, our method ﬁrst takes a pair of video clip and textual query as the input to produce the cross-modal features. The cross-modal features are then used to produce the bounding box for each frame as well as predict the starting and ending frames, which are then used to generate the spatio-temporal tubes for the target object.
Considering that promising results have been achieved by using transformers for various tasks [41, 40, 28], our
STVGBert also builds on a visual-linguistic transformer.
Speciﬁcally, the key component is a cross-modal feature learning module called ST-ViLBert. Different from the rele-vant work ViLBERT [15] which only encodes temporal in-formation, our newly proposed ST-ViLBert also preserves spatial information in the visual input feature. As a result, our STVGBert can effectively learn the cross-modal repre-sentation based on both spatial and temporal visual infor-mation and produce the spatio-temporal object tubes with-out requiring any pre-trained object detectors. We evalu-ate our proposed framework STVGBert on two benchmark datasets, VidSTG [39] and HC-STVG [25], and the ex-periments demonstrate that our framework outperforms all state-of-the-art methods.
Our contributions can be summarized as follows: (1) We propose a new one-stage visual-linguistic trans-former based framework STVGBert for the spatio-temporal video grounding task. To the best of our knowledge, this is the ﬁrst end-to-end optimized
STVG framework that does not require any pre-trained object detectors. (2) We introduce a new cross-modal feature learning mod-ule, ST-ViLBert, to model spatio-temporal information and align cross-modal representation at the same time. (3) Comprehensive experiments conducted on two bench-mark datasets, VidSTG and HC-STVG, demonstrate the effectiveness of our framework for the STVG task. Our one-stage scheme outperforms all multi-stage state-of-the-art methods by a signiﬁcant margin. 2.