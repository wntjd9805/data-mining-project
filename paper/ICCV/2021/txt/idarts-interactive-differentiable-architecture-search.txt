Abstract
Differentiable Architecture Search (DARTS) improves the efficiency of architecture search by learning the archi-tecture and network parameters end-to-end. However, the intrinsic relationship between the architecture’s parameters is neglected, leading to a sub-optimal optimization process.
The reason lies in the fact that the gradient descent method used in DARTS ignores the coupling relationship of the pa-rameters and therefore degrades the optimization. In this paper, we address this issue by formulating DARTS as a bi-linear optimization problem and introducing an Interactive
Differentiable Architecture Search (IDARTS). We first de-velop a backtracking backpropagation process, which can decouple the relationships of different kinds of parameters and train them in the same framework. The backtracking method coordinates the training of different parameters that fully explore their interaction and optimize training. We present experiments on the CIFAR10 and ImageNet datasets that demonstrate the efficacy of the IDARTS approach by achieving a top-1 accuracy of 76.52% on ImageNet without additional search cost vs. 75.8% with the state-of-the-art
PC-DARTS. 1.

Introduction
The goal of Neural Architecture Search (NAS) is to au-tomatically design neural architectures to replace traditional manual architecture design. NAS has had a significant im-pact on computer vision, in part by reducing this need for
*Co-first author.
†Corresponding author. manual work. Recently, Liu et al.
[18] proposed dif-ferentiable architecture search (DARTS) as an alternative that makes architecture search more efficient. DARTS re-laxes the search space to be continuous and differentiable.
DARTS learns the weight of each operation with gradient descent so that the architecture can be optimized with re-spect to its validation set performance by gradient descent.
Despite its sophisticated design, DARTS is still subject to a large yet redundant space of network architectures and thus suffers from significant memory and computation over-head. To address the problems of DARTS, researchers have proposed alternative formulations [3, 13, 30, 2, 33, 4, 11].
Among them, PC-DARTS [30] reduces redundancy in the network space by performing a more efficient search with-out compromising the performance. PC-DARTS only sam-ples a subset of channels in a super-net during the search to reduce computation and introduces edge normalization to stabilize the search for network connectivity by explicitly learning an extra set of edge-selection parameters.
However, these DARTS alternatives ignore the intrinsic relationship between different kinds of parameters, and as a result, the selected architecture is sub-optimal due to an insufficient training process. The reason lies in the fact that the coupling relationship will affect the training of the net-work architecture to its limit before it is selected or left out.
To address this issue, we introduce a bilinear model into
DARTS and develop a new backpropagation method to de-couple the hidden relationships among variables to facilitate the optimization process. To the best of our knowledge, few works have formulated DARTS as a bilinear problem.
In the paper, we address these issues by formulating
DARTS as a bilinear optimization problem and devel-oping the efficient Interactive Differentiable Architecture
Figure 1. An overview of IDARTS. (a) α and β are coupled in IDARTS. The edge and operation (βl and αl) are coupled during the neural architecture search. xi and xj represent node 0 and node 2, respectively. xj = αl,m · βl · Wl,m ⊗ xi is specifically described in Eq. 2. (b) A backtracking method is introduced to coordinate the training of different parameters, which can fully explore their interaction during training. The dotted line results indicate that the lack of backtracking leads to the inefficient training of α, and the solid line indicates an efficient training of IDARTS.
Search (IDARTS). Fig. 1 shows the framework of IDARTS.
Fig. 1(b) shows that the dotted line results are inefficient t1 and compared with IDARTS shown in the solid line. t2 mark the results where the architecture parameter α is backtracked.
IDARTS coordinates the training of differ-ent parameters and fully explores the interaction between them based on the backtracking method. Our method al-lows operations to be selected only when they are suffi-ciently trained. We evaluate our IDARTS on image clas-sification and conduct experiments on the CIFAR10 and
ImageNet datasets. The experimental results show that
IDARTS achieves superior performance compared to exist-ing DARTS approaches [30, 33, 4]. Our contributions are summarized as follows:
• We provide the first attempt to formulate DARTS as a bilinear optimization problem. IDARTS decouples the relationship of different kinds of parameters to suffi-ciently train them in the same framework.
• We introduce a backtracking method to coordinate the training of different parameters. The backtracking fully explores the potential of the parameters in the ar-chitecture search through their interaction during the optimization process.
• Extensive experiments demonstrate that
IDARTS achieves better performance than prior arts on the CI-FAR10 and ImageNet datasets, with a top-1 accuracy of 97.68% on CIFAR10 and 76.52% on ImageNet. 2.