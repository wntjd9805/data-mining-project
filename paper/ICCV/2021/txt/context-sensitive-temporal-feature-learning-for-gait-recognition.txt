Abstract
Although gait recognition has drawn increasing research attention recently, it remains challenging to learn discrim-inative temporal representation since the silhouette differ-ences are quite subtle in spatial domain. Inspired by the observation that humans can distinguish gaits of differ-ent subjects by adaptively focusing on temporal sequences with different time scales, we propose a context-sensitive temporal feature learning (CSTL) network in this paper, which aggregates temporal features in three scales to ob-tain motion representation according to the temporal con-textual information. Specifically, CSTL introduces rela-tion modeling among multi-scale features to evaluate fea-ture importances, based on which network adaptively en-hances more important scale and suppresses less important scale. Besides that, we propose a salient spatial feature learning (SSFL) module to tackle the misalignment prob-lem caused by temporal operation, e.g., temporal convo-lution. SSFL recombines a frame of salient spatial fea-tures by extracting the most discriminative parts across the whole sequence. In this way, we achieve adaptive tempo-ral learning and salient spatial mining simultaneously. Ex-tensive experiments conducted on two datasets demonstrate the state-of-the-art performance. On CASIA-B dataset, we achieve rank-1 accuracies of 98.0%, 95.4% and 87.0% un-der normal walking, bag-carrying and coat-wearing con-ditions. On OU-MVLP dataset, we achieve rank-1 ac-curacy of 90.2%. The source code will be published at https://github.com/OliverHxh/CSTL. 1.

Introduction
Gait recognition is a long-distance biological identifica-tion technology, which relies on the walking patterns of
*Contributed equally.
†Corresponding author: fengbin@hust.edu.cn. (a) Two sequences from subject ’53’ and ’119’ on CASIA-B can be dis-tinguished relying on short-term temporal clues, e.g., several frames at the beginning. (b) Two sequences from subject ’39’ and ’77’ on CASIA-B, which have to be distinguished relying on long-term temporal clues, e.g., all of the frames.
Figure 1. Illustration that humans can distinguish gaits of various subjects by adaptively focusing on temporal fragments with differ-ent time scales. Color bar indicates the human focus distribution.
Darker color represents more attention needed for corresponding frames. Best viewed in color. human beings, and reveals great application potential on identity recognition [20, 1, 22]. Although gait recognition has drawn increasing research attention recently, it remains challenging to learn discriminative temporal representation since the silhouette differences in spatial domain are quite subtle.
Moreover, as mentioned in [6], body parts possess di-verse motion patterns which requires temporal modeling to take multi-scale representation into consideration. Multi-layer temporal convolution has been widely used in current methods [6, 28, 18, 31, 32] to model temporal information in multiple scales. They aggregated multi-scale temporal features in a summation or a concatenation way. However, these manners are not flexible enough to adapt to variation of complex motion and realistic factors, i.e., occlusion of clothing and change of camera viewpoints, since the fusion method of multi-scale features is fixed. Thus, the perfor-mance is hindered especially considering gait is a kind of fine-grained motion pattern, whose identification of sub-jects depends on the diverse expression on tiny motion of local body.
It can be seen from life experience that humans distin-guish gait sequences of different subjects by adaptively fo-cusing on temporal fragments with different time scales. A qualitative illustration is given in Fig.1, where voting re-sults from seven volunteers are used to calculate the fo-cus distribution. In Fig.1(a), the differences between two gait sequences are so obvious that we can distinguish them by observing several frames from beginning. On the con-trary, in Fig.1(b), differences between two sequences are quite subtle that we have to observe more frames to distin-guish them. Therefore, in this situation, short-term clues are not enough to make a distinction between the two subjects.
Long-term features need to be considered since they pro-vide richer temporal information. Hence, the adaptive ad-justment among multi-scale temporal features leads to flex-ible focus along temporal dimension, which offers a new perspective for gait modeling.
Motivated by such observation, we propose a context-sensitive temporal feature learning (CSTL) network for gait recognition. The core idea of this method is to integrate multi-scale temporal features according to the contextual information along temporal dimension, which allows infor-mation communications among different scales. Here, con-textual information is obtained by evaluating the relations among multi-scale temporal features, which reflects diverse motion information existing in context features. CSTL pro-duces temporal features in three temporal scales, i.e., frame-level, short-term and long-term, which are complementary to each other. The frame-level features retain frame charac-teristics at each time instant. The short-term features cap-ture local temporal contextual clues, which are sensitive to temporal locations and beneficial to model micro motion patterns. The long-term features, on behalf of motion fea-tures across all frames, reveal global action periodicities of different body parts, which are invariant for temporal loca-tions. Then, the relation modeling among these temporal features guides the network to adaptively enhance or sup-press temporal features with different scales, then generates appropriate temporal descriptions for motion learning on different body parts. This method provides the possibility of modeling complex motion, which makes it very suitable for gait recognition.
Further, during the investigation of temporal modeling, we notice the misalignment problem in temporal model-ing that has not been investigated in gait recognition yet.
As shown in Fig.2, the same pixel locations from differ-ent frames may correspond to different foregrounds and backgrounds. Naturally, the utilization of temporal oper-ations, e.g., temporal convolutions and temporal poolings, may result in blurry and overlapped appearances. To ad-dress such issue, we propose a salient spatial feature learn-Figure 2. Illustration of misalignment problem caused by tempo-ral convolution, since pixels of same spatial locations in different frames may correspond to different semantic content. ing (SSFL) module to select discriminative spatial clues across the whole sequence, which is considered as a sup-plement to remedy the corruption in appearance features.
The adaptive temporal modeling and salient spatial learning provide complementary properties for each other.
On one hand, CSTL mainly considers temporal modeling and SSFL focuses on spatial learning. Specifically, CSTL produces temporal aggregation of multi-scale clues which describes motion patterns, and SSFL generates recombi-nant frame features which involve with still images. On the other hand, CSTL aggregates temporal clues in a soft-attention way and SSFL selects salient spatial features in a hard-attention manner. In a word, by jointly investigat-ing motion learning and spatial mining simultaneously, we achieve outstanding performance over the existing methods.
The major contributions of this paper can be summarized as the following three aspects:
• In this paper, we propose a temporal modeling network
CSTL to fuse multi-scale temporal features in an adap-tive way, which considers the cross-scale contextual information as a guidance for temporal aggregation.
• we propose a salient spatial feature learning (SSFL) module to remedy the misalignment problem caused by temporal operation. SSFL extracts salient spatial features from different frames to form a recombinant frame which maintains high quality spatial features.
• Extensive experiments conducted on two popular datasets CASIA-B [30] and OU-MVLP [24] demon-strate the state-of-the-art performance of our method.
And further ablation experiments prove the effective-ness of the proposed modules. 2.