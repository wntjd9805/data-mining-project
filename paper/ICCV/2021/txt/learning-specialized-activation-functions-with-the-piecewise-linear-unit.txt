Abstract
The choice of activation functions is crucial for modern deep neural networks. Popular hand-designed activation functions like Rectiﬁed Linear Unit(ReLU) and its variants show promising performance in various tasks and mod-els. Swish, the automatically discovered activation func-tion, has been proposed and outperforms ReLU on many challenging datasets. However, it has two main draw-backs. First, the tree-based search space is highly dis-crete and restricted, which is difﬁcult for searching. Sec-ond, the sample-based searching method is inefﬁcient, mak-ing it infeasible to ﬁnd specialized activation functions for each dataset or neural architecture. To tackle these draw-backs, we propose a new activation function called Piece-wise Linear Unit(PWLU), which incorporates a carefully designed formulation and learning method.
It can learn specialized activation functions and achieves SOTA perfor-mance on large-scale datasets like ImageNet and COCO.
For example, on ImageNet classiﬁcation dataset, PWLU im-proves 0.9%/0.53%/1.0%/1.7%/1.0% top-1 accuracy over
Swish for ResNet-18/ResNet-50/MobileNet-V2/MobileNet-V3/EfﬁcientNet-B0. PWLU is also easy to implement and efﬁcient at inference, which can be widely applied in real-world applications. 1.

Introduction
Activation functions are fundamental components for modern deep neural networks, which affect both the expres-siveness and the optimization of models. As a crucial design choice, the combination of Rectiﬁed Linear Units(ReLU)[7, 15, 25] with deep neural networks shows six times faster in convergence compared to tanh nonlinearties[18]. With non-saturating gradients, ReLU mitigates the vanishing gradi-ent problem and shows robust performance across different tasks and neural architectures.
Beyond ReLU, many new activation functions have been proposed like Leaky ReLU[23], PReLU[10], ELU[4],
SELU[16], etc. These human-designed activation functions are either ﬁxed or have a few learnable parameters. All
ResNet-18 2.5 2 1.5 1 0.5 0
-0.5
ResNet-50
EfficientNet-B0
MobileNet-V3
MobileNet-V2
APL
PAU
PReLU
F-ReLU
Swish
PWLU
Figure 1. Comparing improvements over ReLU on ImageNet dataset. We run experiments on ﬁve different architectures for each activation function and show the improvements over ReLU base-line. The proposed PWLU consistently outperforms other methods across architectures. these variants are similar in shape, and their gains are not consistent across tasks which limits their applications[26].
Besides human-designed activation functions, automatic searched activations function, Swish[26], show better per-formance across tasks.
Swish adopts a tree-structured search space based on unary and binary functions, which is searched by reinforcement learning. Although showing improvements over ReLU, Swish also has its limitations.
First, the tree-structured search space is highly discrete and restricted. A slight change in one composing unit may result in an entirely different activation function, making it difﬁ-cult to search. Second, the sample-based searching method typically requires evaluating hundreds to thousands of can-didate activation functions, which is computationally ex-pensive. It is infeasible to use such an inefﬁcient search-ing process to ﬁnd specialized activation functions for each dataset or architecture. Instead, the searched Swish is used for most conditions. There are also methods like APL[1] and PAU[24] that use universal approximators as activation
functions. Contrary to Swish, they are mostly contiguous with respect to their parameters and can be directly opti-mized by gradients. However, these methods suffer from complicated formulations that may lead to unstable learn-ing or inefﬁcient inference. Besides, simply using gradients may not be sufﬁcient to fully optimize these formulations.
To overcome these drawbacks and exploit the full po-tential of activation functions, we propose a new method called Piecewise Linear Unit(PWLU). Our method is com-posed of two parts: the piecewise-linear-based formulation and the gradient-based learning method. The carefully de-signed formulation is ﬂexible, easy for learning and efﬁ-cient for inference. First, it covers a wide range of scalar functions which raise the potential of ﬁnding good activa-tion functions. Second, it is mostly differentiable with re-spect to its parameters and can be easily learned by gra-dients. Third, its computation is very simple at infer-ence, which is efﬁcient and practical for real-world appli-cations. To effectively learn activations under this formu-lation, we identify the input-boundary misalignment prob-lem and compensate normal gradient-based learning with statistic-based realignment. Our learning method is both effective and efﬁcient, making it possible to learn spe-cialized activation functions for each dataset or architec-ture. With the formulation and learning method, PWLU clearly outperforms Swish and other activation functions on large-scale datasets like ImageNet[28] and COCO[21], across a wide variety of architectures. An overall com-parison on ImageNet dataset for different activation func-tions is shown in Figure 1. Particularly our method outper-forms Swish for 0.9%/0.53%/1.0%/1.7%/1.0% top-1 accu-racy for ResNet-18/ResNet-50/MobileNet-V2/MobileNet-V3/EfﬁcientNet-B0. The learned activation functions show different preferences across architectures and layers, which conﬁrms the beneﬁts of learning specialized activation functions. To summarise, our contributions are as follows:
• We propose a new formulation of activations based on piecewise linear functions, which is ﬂexible, easy for learning and efﬁcient for inference.
• We identify the input-boundary misalignment problem in gradient-based learning and propose a new learning method that is both effective and efﬁcient.
• With the proposed formulation and learning method, we demonstrate the beneﬁts of learning specialized ac-tivation functions on large-scale datasets and different architectures. 2.