Abstract
State-of-the-art object detection approaches typically rely on pre-trained classiﬁcation models to achieve better performance and faster convergence. We hypothesize that classiﬁcation pre-training strives to achieve translation in-variance, and consequently ignores the localization aspect of the problem. We propose a new large-scale pre-training strategy for detection, where noisy class labels are avail-In this set-able for all images, but not bounding-boxes. ting, we augment standard classiﬁcation pre-training with a new detection-speciﬁc pretext task. Motivated by the noise-contrastive learning based self-supervised approaches, we design a task that forces bounding boxes with high-overlap to have similar representations in different views of an im-age, compared to non-overlapping boxes. We redesign
Faster R-CNN modules to perform this task efﬁciently. Our experimental results show signiﬁcant improvements over existing weakly-supervised and self-supervised pre-training approaches in both detection accuracy as well as ﬁne-tuning speed. 1.

Introduction
We address the problem of large-scale weakly supervised pre-training for detection, where we assume that noisy clas-siﬁcation labels are available for images, but localization (bounding-boxes) information is missing. Almost all state-of-the-art approaches use pre-trained classiﬁcation models and ﬁne-tune them for detection tasks. Fine-tuning mainly yields two signiﬁcant beneﬁts: (a) improved accuracy and (b) speedup in training for detection. Recently, there has been a lot of work on large-scale [51, 34] pre-training of classiﬁcation models with noisy labels from the web. How-ever, the beneﬁts are more pronounced for classiﬁcation tasks compared to detection or instance segmentation [34].
We hypothesize that pre-training for classiﬁcation tasks overemphasizes translation invariance [26] as shown in
Fig. 1. Different crops of an image that share similar con-tent but do not have high overlap are required to be similar to each other. As seen in the ﬁgure, this runs contrary to the
Figure 1. Consider the bounding boxes shown on the left. In clas-siﬁcation pre-training, it is desirable for the model to learn similar representations for all of them to incorporate translation invari-ance. However, this is undesirable for detection, which requires bounding boxes with small or no overlap to be dissimilar. This forms the core of our pre-training method, which uses a pretext task to force the model to distinguish the non-overlapping boxes. detection objective and could result in feature representa-tions that are sub-par for the target detection task. In theory, this could be handled by pre-training a detection model on a large dataset from scratch. However, this is impractical due to the huge effort required to annotate bounding boxes for images at that scale. Instead, we propose to supplement the standard classiﬁcation pre-training task with a novel self-supervised pretext task that is closer to detection.
Recent self-supervised approaches such as MoCo[18] have shown that maximizing the agreement between two views (constructed via transformations) of the same image and minimizing it for different images works really well for feature learning. These tasks typically require a dictionary-lookup, wherein one view serves as the query, while the other view is part of a dictionary. In our approach, we ex-tend this idea to detection. We use a bounding box from one-view of an image as a query to retrieve the same bound-ing box (or a box highly overlapping with it) from another view of the image. We refer to this task as query-box lookup. This ensures that boxes with sufﬁciently high over-lap are similar to each other, while non-overlapping boxes have distinct representations.
Ideally, the query-box should be retrieved from the set of all bounding boxes in the image. However, this is too
large to handle. Hence, we restrict the look-up to a smaller, but representative set of “proposal” boxes. These propos-als should include high-quality hard-negatives to make the retrieval task useful. Fortunately, this proposal-set construc-tion problem has been solved by Region proposal network (RPN) in the context of object detection for Faster R-CNN.
In our approach, we adapt RPN to instead construct query-speciﬁc proposals. We refer to it as the contrastive RPN (CRPN). Similarly, we also adapt the Region of Interest (ROI-head) module from Faster R-CNN to carry out the retrieval task, and refer to it as the contrastive ROI-head (CROI-head). This design has the advantage of making our model-architecture similar to Faster R-CNN detection model. We refer to our approach as PreDet.
We show that PreDet pre-trained on a dataset of 50M im-ages with noisy hashtags as labels provides two main bene-ﬁts over existing approaches: higher average precision (AP) and faster ﬁne-tuning. When ﬁne-tuning a ResNeXt-101-32x8d Mask R-CNN model for the standard 90k iterations on MS-COCO [31], initializing from PreDet achieves 3.4% and 2.9% absolute improvement in APbox compared to Im-ageNet pre-trained model and self-supervised SEER [13] model pre-trained on 1B images respectively. More impres-sively, ﬁne-tuning PreDet for just 90k iterations also outper-forms these models, when they are ﬁne-tuned for longer du-ration (6 × −9× more) by 1.3%. We observe similar gains for RetinaNet models and other detection datasets (LVIS-v1 [17] and PASCAL VOC [11]). We also conduct exten-sive experiments to understand the effect of model capacity and target dataset size, and ﬁnd PreDet to be particularly impactful for larger models and smaller target datasets. 2.