Abstract
We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated ob-jects learned from images. While recent advances in 3D im-plicit representation have made it possible to learn models of complex objects, learning pose-controllable representa-tions of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representa-tion of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance ﬁeld at each 3D location.
In this way, the proposed method represents pose-dependent changes without signiﬁcantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efﬁcient and can general-ize well to novel poses. The code is available for research purposes at https://github.com/nogu-atsu/NARF. 1.

Introduction
In this work, we aim to learn a representation for render-ing novel views and poses of 3D articulated objects, such as human bodies, from images. Our approach follows the inverse graphics paradigm [27, 26, 37, 28] of analyzing an image by attempting to synthesize it with compact graphics codes. These codes are typically disentangled to allow for rendering of scenes/objects with ﬁne-grained control over individual appearance properties such as object location, pose, lighting, texture, and shape. For the case of humans, synthesis of novel views and poses can be useful for appli-cations such as movie making, photo editing, virtual cloth-ing [64, 29] and motion transfer [33, 4].
Various inverse graphics based approaches have been speciﬁcally designed for static scenes [60, 21, 54], rigid ob-jects [9, 68, 62, 56, 3], blend shapes for keypoints [61, 4] and dense meshes [64, 33]. However, efﬁcient deformation
Figure 1. Training pipeline of Disentangled NARF (NARFD).
NARF is an efﬁcient pose-aware 3D representation trained from only pose-annotated images. The learned representation is part-based and able to render novel poses of articulated 3D objects by changing the input object pose conﬁgurations. modeling of articulated 3D objects using neural networks remains a challenging task due to the large variance of joint locations (especially for endpoints such as hands), severe self-occlusions, and high non-linearity in forward kinematic transformations [69]. Though work has been done to en-able explicit control over the underlying human pose [33] and key point locations [4], their neural rendering meth-ods are either limited to 2D [4], which prevents modeling of view-dependent appearance [7], or based on mesh repre-sentations [33], where rendering quality can be affected by the resolution of the discrete template mesh.
Recent progress on the implicit representation of 3D ob-jects and scenes, such as signed distance functions [20, 43] and occupancy ﬁelds [12, 38], has greatly promoted the de-velopment of the inverse graphics paradigm. Such repre-sentations are lightweight in model size, continuous, and differentiable, making them highly practical in compari-son with the previously-dominant volumetric representa-tions [11, 16, 24, 39, 46, 55, 57, 67]. Particularly, Milden-1
hall et al. [40] propose the neural radiance ﬁeld (NeRF) that takes a single continuous 5D coordinate (3D spatial location and 2D viewing direction) as input and outputs the volume density and view-dependent emitted radiance at each spa-tial location. Combined with a classical differentiable vol-ume rendering technique [23], it is able to synthesize novel views by learning from a sparse set of input views of static scenes. NeRF completely discards the mesh-based repre-sentation and replaces it with a radiance-based model which can effectively and efﬁciently encode view-dependent ap-pearance, enabling it to reproduce scenes of complex ge-ometry with high ﬁdelity.
In this paper, we extend NeRF to an articulated NeRF, called a Neural Articulated Radiance Field (NARF), to rep-resent articulated 3D objects. Accounting for 3D articula-tion within the NeRF framework is a challenging problem because a complex, non-linear relationship exists between a kinematic representation of 3D articulations and the re-sulting radiance ﬁeld, making it hard to model implicitly in a neural network [69]. In addition, the radiance ﬁeld at a given 3D location is inﬂuenced by at most a single artic-ulated part and its parents along the kinematic tree, while the full kinematic model is provided as input. As a result, dependencies of the output to irrelevant parts may inadver-tently be learned, which is known to hurt model generaliza-tion to poses unseen in training [65].
To address these issues, we propose a method that pre-dicts the radiance ﬁeld at a 3D location based on only the most relevant articulated part. This part is identiﬁed using a set of sub-networks that output the probability for each part given the 3D location and the 3D geometric conﬁgu-ration of the parts. The spatial conﬁgurations of parts are computed explicitly with a kinematic model, rather than modeled implicitly in the network. A NARF then predicts the density and view-dependent radiance of the 3D location conditioned on the properties of only the selected part. An overview of the method is shown in Fig. 1.
The presented NARF has the following properties:
• It learns a disentangled representation of camera view-point, bone parameters, and bone pose, allowing these properties to be individually controlled in rendering.
• A dense 3D representation is learned from a sparse set of 2D images with pose annotations of the articulated object, which could potentially be obtained through external pose estimation techniques on multi-view im-ages with known camera parameters [22].
• Part segmentation is learned from images with pose annotation. Additional supervision is not needed.
• NARF can be trained for articulated objects of various shape and appearance, through the use of an autoen-coder that extracts latent shape and appearance vectors which are additionally disentangled.
With this approach, it becomes possible to render both novel views and poses of articulated 3D objects from pose anno-tated 2D images with little increase in computational com-plexity. 2.