Abstract
In unconstrained real-world surveillance scenarios, per-son re-identification (Re-ID) models usually suffer from dif-ferent low-level perceptual variations, e.g., cross-resolution and insufficient lighting. Due to the limited variation range of training data, existing models are difficult to general-ize to scenes with unknown perceptual interference types.
To address the above problem, in this paper, we propose two disjoint data-generation ways to complement existing training samples to improve the robustness of Re-ID mod-els. Firstly, considering the sparsity and imbalance of sam-ples in the perceptual space, a dense resampling method from the estimated perceptual distribution is performed.
Secondly, to dig more representative generated samples for identity representation learning, we introduce a graph-based white-box attacker to guide the data generation pro-cess with intra-batch ranking and discriminate attention. In addition, two synthetic-to-real feature constraints are intro-duced into the Re-ID training to prevent the generated data from bringing domain bias. Our method is effective, easy-to-implement, and independent of the specific network ar-chitecture. Applying our approach to a ResNet-50 base-line can already achieve competitive results, surpassing state-of-the-art methods by +1.2% at Rank-1 on the MLR-CUHK03 dataset. 1.

Introduction
Person re-identification (Re-ID) aims to identify the sample person across non-overlapping cameras, which can be viewed as a sub-task of image retrieval. However, due to identity-unrelated drastic variations, learning a robust and discriminative identity representation for real-world Re-ID in unconstrained scenarios is challenging. In general, these variations can be divided roughly into two categories: low-level perceptual variations (referred to as visual degrada-tion), such as resolution and illumination; and high-level semantic variations, including view, pose, occlusions, etc.
*Co-first authors.
†Corresponding author. (a) A lightweight image adjuster which can predict the resolution qual-ity score of input images (Red Line) and adjust the image resolutions based on given values (Green Line). (b) Taking the overall resolution distribution of the data source as prior knowledge, global-aware dense resolution augmentation can be performed for each sample (Green Line). An intra-batch white-box attacker is further introduced to provide guidance from high-level vision tasks (Red Line).
Figure 1. An overview of proposed Global-Aware and Attack-Guided perceptual data generation (GAAG).
Compared with the former, semantic variations have been explored sufficiently by existing Re-ID methods, e.g., pre-defined regional partition [36, 35, 30] and human part align-ment [18, 51]. In this work, we focus on low-level percep-tual variations and take cross-resolution Re-ID as the main task.
Due to the powerful representation learning capability, deep convolutional neural networks-based Re-ID models
[26, 43, 29] can effectively deal with these variations in con-strained scenarios. However, since real-world applications are more diverse and unpredictable, these deep models that rely on training data heavily are difficult to generalize to unseen situations. Although collecting enough labeled data is a feasible solution, it is too expensive and impractical to build a manually labeled database covering all possible sit-uations. Therefore, many works attempt to complement the
training data by adjusting the original samples [25, 48] or synthesizing new ones [47, 28, 46].
Conventional augmentations, e.g., random crop and ran-dom horizontal flip, have been widely used for the Re-ID task. Thus we mainly discuss data augmentation methods based on synthetic strategies and classify them into two types: (1) Engine-based generation. Based on 3D ren-dering engines, controllable person generation [2, 1, 34] with different poses, backgrounds, etc., can be realized.
Although these methods promote quantitative analysis of how visual factors influence the Re-ID system, artificial images are unsuitable as training data due to significant differences in style and appearance with real-world data. (2) GAN-based generation. Generative Adversarial Net-works (GANs) [11] are also widely used for data genera-tion. To our best knowledge, Zheng et al. [47] firstly in-troduce GANs into Re-ID for unlabeled data generation.
By interpolating or swapping the disentangled intermediate features, [28, 46] achieve sufficiently realistic image gener-ation. However, due to the limited number of original sam-ples that provide prior information, the synthesized images lack sufficient diversity.
In this paper, to alleviate the interference of low-level perceptual variations in real-world surveillance scenarios, we propose a new Global-Aware and Attack-Guided per-ceptual data generation approach (GAAG) by combining disentangled image generation and adversarial attack.
Specifically, we design a lightweight disentangled gen-erative model (denoted as Adjuster in Figure 1) with two functions to predict the perceptual quality score of an input image and adjust the input image based on a given score.
The first function aims to estimate the overall distribution of perceptual variations in the data source, while the second one takes the estimated distribution as prior knowledge to perform global-aware dense augmentation on each training instance. In this way, the perceptual diversity of training data can be enriched without changing identity semantics.
However, it is not optimal to directly use this augmenta-tion for Re-ID because the task-related knowledge has not been introduced. Inspired by the research on vulnerabilities of deep neural networks [3, 31], we argue that white box attacks can serve as a bridge between our adjuster and the
Re-ID backbone. Therefore, we further utilize it to provide task-related guidance for our data generation.
In addition, we observe that although most augmented samples have natural appearances, many of them inevitably contain artifacts and noise. These subtle flaws bring am-biguity to identity representation learning and cannot be removed by popular domain adaptation methods [1, 44].
To handle this problem, we further introduce synthetic-to-real feature constraints for simultaneously narrowing do-main gap and improving the robustness of identity features.
Experiments on several cross-resolution re-id benchmarks confirm the effectiveness of our approach.
To summarize, our main contributions are as follows:
• We propose a novel global-aware and attack-guided perceptual data generation framework to complement existing training data for Re-ID against the low-level perceptual variations.
• We design a lightweight disentangled generative model which can estimate and manipulate the resolu-tion component of images. It can be easily applied to other perceptual types with few modifications.
• To alleviate the domain bias caused by synthetic sam-ples, we introduce synthetic-to-real feature constraints for narrowing domain gap and regularizing identity feature manifolds.
• The proposed method can be easily integrated into existing deep models without bringing any inference cost. Only in combination with the classic ResNet-50, it can already achieve competitive performance against the state-of-the-art methods on challenging cross-resolution Re-ID benchmarks. 2.