Abstract
It is common practice to represent spoken languages at their phonetic level. However, for sign languages, this implies breaking motion into its constituent motion primi-tives. Avatar based Sign Language Production (SLP) has traditionally done just this, building up animation from se-quences of hand motions, shapes and facial expressions.
However, more recent deep learning based solutions to SLP have tackled the problem using a single network that esti-mates the full skeletal structure.
We propose splitting the SLP task into two distinct jointly-trained sub-tasks.
The first translation sub-task translates from spoken language to a latent sign language representation, with gloss supervision. Subsequently, the animation sub-task aims to produce expressive sign lan-guage sequences that closely resemble the learnt spatio-temporal representation. Using a progressive transformer for the translation sub-task, we propose a novel Mixture of
Motion Primitives (MOMP) architecture for sign language animation. A set of distinct motion primitives are learnt during training, that can be temporally combined at infer-ence to animate continuous sign language sequences.
We evaluate on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset, presenting exten-sive ablation studies and showing that MOMP outperforms baselines in user evaluations. We achieve state-of-the-art back translation performance with an 11% improvement over competing results. Importantly, and for the first time, we showcase stronger performance for a full translation pipeline going from spoken language to sign, than from gloss to sign. 1.

Introduction
Sign languages are visual languages used for communi-cation by the Deaf communities. Analogous to phonemes in speech, sign languages can be broken down into cheremes, the smallest distinctive structural units [58]. Cheremes can be represented as motion primitives, a set of manual and
Figure 1: An overview of the proposed SLP sub-tasks of
Translation and Animation. non-manual motions1 that are combined to represent all sign language utterances. Such phonetic representations are typ-ically used by linguists for annotation [29, 62] or in graphi-cal based avatars for sign generation.
Sign Language Production (SLP), the translation from spoken language sentences to continuous sign language sequences, requires both an accurate translation and ex-pressive animation. Previous work has traditionally tack-led these two sub-task as a single task with one network
[52, 59, 68], leading to under-expressive production. Al-though previous SLP models have used gloss2 as an inter-mediate representation [60], this creates an information bot-tleneck that disregards the contextual information available in the original text.
In this paper, we propose to formulate SLP as two dis-tinct but jointly-trained sub-tasks as can be seen in Figure 1: 1) An initial translation from spoken language to a sign lan-guage representation, with gloss supervision; 2) Subsequent animation of a visual sign language sequence. This can be viewed as analogous to a text-to-speech pipeline with an initial translation into phonemes and a subsequent vocali-sation. However, we do not force a gloss information bot-tleneck but instead condition learning on gloss, resulting in 1Manual features are the hand shape and motion, whereas non-manuals are the mouthings and facial expressions 2Glosses are a written sign representation of sign, defined as minimal lexical items.
significant performance increases.
We utilise a progressive transformer model as our trans-lation backbone [52]. Sign language representations are learnt per frame using the gloss supervision. This prompts the sub-network to learn meaningful representations for the end goal of sign language production.
To animate expressive sign language sequences, we pro-pose a novel Mixture of Motion Primitives (MOMP) net-work. Based on a Mixture-of-Expert (MoE) architecture, we learn a combination of distinct motion primitives that are able to produce an infinite number of unique sign lan-guage utterances. Due to the continuous nature of sign lan-guage, we apply expert blending on a per frame basis, thus enabling different experts to be activated for separate sec-tions of the output sequence.
As the subset of motion primitives is significantly smaller than the full set of signs, the animation sub-task is reduced to a gating network that selects the correct prim-itive to animate for specific sections of the full sequence.
This also enables a scaling of SLP models to larger datasets, with new signs being a novel combination of the learnt primitives. We represent experts as masked transformer en-coders, using self-attention to learn unique structural mo-tions. We use a further transformer encoder model for the gating network, thus building, to the best of our knowledge, the first full transformer-based MoE architecture.
We evaluate on the challenging PHOENIX14T corpus, performing an extensive ablation study of the proposed net-work and conducting a user evaluation that highlights the animation quality of the MOMP. Furthermore, we achieve state-of-the-art SLP back translation results and showcase, for the first time, stronger performance for a full transla-tion pipeline that produces sign sequences directly from the source spoken language, than from an intermediate gloss representation.
The contributions of this paper can be summarised as:
• A novel transformer-based MoE architecture, Mixture of Motion Primitives (MOMP), that combines learnt motion primitives at the frame level.
• The first SLP model to separately model the sub-tasks of translation and animation.
• State-of-the-art SLP performance and user evaluation results on the PHOENIX14T dataset.
• The first SLP model to achieve higher performance for a full translation pipeline going from spoken language to sign, than from gloss to sign. 2.