Abstract 3D deep learning has been increasingly more popular for a variety of tasks including many safety-critical appli-cations. However, recently several works raise the security issues of 3D deep models. Although most of them consider adversarial attacks, we identify that backdoor attack is in-deed a more serious threat to 3D deep learning systems but remains unexplored. We present the backdoor attacks in 3D point cloud with a unified framework that exploits
In par-the unique properties of 3D data and networks. ticular, we design two attack approaches on point cloud: the poison-label backdoor attack (PointPBA) and the clean-label backdoor attack (PointCBA). The first one is straight-forward and effective in practice, while the latter is more sophisticated assuming there are certain data inspections.
The attack algorithms are mainly motivated and developed by 1) the recent discovery of 3D adversarial samples sug-gesting the vulnerability of deep models under spatial trans-formation; 2) the proposed feature disentanglement tech-nique that manipulates the feature of the data through opti-mization methods and its potential to embed a new task. Ex-tensive experiments show the efficacy of the PointPBA with over 95% success rate across various 3D datasets and mod-els, and the more stealthy PointCBA with around 50% suc-cess rate. Our proposed backdoor attack in 3D point cloud is expected to perform as a baseline for improving the ro-bustness of 3D deep models. 1.

Introduction 3D deep learning has been developed rapidly in the past few years, which makes it the prime option for various real-world deployments, such as autonomous driving [7], scene reconstruction [25] and medical data analysis [34], in which
*Equal contribution.
†Corresponding author.
Figure 1. Activation of backdoored models with the interaction trigger and orientation trigger. Original point cloud data will be classified correctly, however, with a certain trigger like an inter-action object (e.g., a small ball) nearby or a small change of ori-entation (rotation perpendicular to the horizontal plane), the point cloud data will be classified as the target label. life safety issues are usually involved. As more and more attentions have been paid to this field, recently researchers have started to acknowledge and account for the security problems of 3D deep learning systems. For example, a few works have investigated the adversarial attack in the 3D do-main [46, 44, 51].
Compared to the adversarial attack, an insidious threat to the deep learning system called backdoor attack, or Neural
Trojan [23, 6, 11] is even more damaging. The backdoor attack injects a small proportion of poisoned data in train-ing and activates malicious functionality by implanting a specified trigger to the test data during inference. This at-tack could happen when using a publicly available dataset or a pretrained model that is potentially from untrustwor-thy sources. Indeed, it is reported that industry practitioners worry about data poisoning much more than other threats such as adversarial attack [15].
The backdoor attack in 3D could be a huge potential threat. On the one hand, real-world 3D point cloud data usually comes with the noise caused by the nature of the col-lection process, e.g., the objects might be partly occluded, spatially deformed, or packed with noisy points. Therefore, it is easy for the adversary to activate malicious functional-ity with triggers in the disguise of noise. On the other hand, due to the coordinate-based representation and necessarily sampling before processing, 3D point clouds are frequently sparse and irregular, which may complicate data integrity verification or even human inspection.
To the best of our knowledge, there is no work regarding the backdoor attacks in 3D. It is also nontrivial to extend the existing 2D backdoor attack methods to 3D deep learn-ing. We emphasize the following issues and obstacles: 1) the data structure of 3D point cloud is intrinsically different from that of 2D images, thus, the design of the backdoor triggers for the pixel-based image can not be directly ap-plied to 3D data; 2) the models processing 3D point clouds have completely different structures than the models of 2D deep learning, resulting in a plethora of unique properties that add to the complexity of the backdoor attacks.
Considering the above challenges, we propose a frame-work to investigate backdoor attacks in the 3D domain and hope it could be a baseline for further studies. We first in-troduce a unified form of the backdoor trigger implanting function for the 3D point cloud. Following the framework, we then illustrate two examples of 3D triggers motivated by real-world scenarios, namely the orientation trigger and interaction trigger. Fig. 1 shows the visualization. A pertur-bation analysis of the proposed trigger is then provided. It helps restrict the perturbation caused by the trigger so that the attack will be more rational and harder to defend against.
Based on the proposed trigger form, we first propose the standard backdoor attack on 3D triggers. Although being straightforward, it consistently delivers a high attack suc-cess rate across various datasets and deep models. More-over, it remains very effective even when the location and size are changing dynamically for interaction trigger or the rotation angle is relatively small for orientation trigger. To further alleviate the security concerns, we propose a clean-label attack algorithm that utilizes the recent development of 3D adversarial attacks and the feature disentanglement technique. Compared to previous attacks, the clean-label attack is a much stealthier attack that does not change la-bels, thus can bypass label inspection or data filtering [39].
To summarize, our contributions are three-fold:
• To the best of our knowledge, this is the first work consid-ering the backdoor attacks in the 3D domain. Motivated by the unique properties of 3D data, we propose a unified framework to investigate backdoor triggers in 3D as well as the perturbation analysis to restrict the attack ability in a reasonable way.
• We experimentally show the efficacy of the proposed trig-gers via a standard backdoor attack. The results further reveal the vulnerability of 3D models under spatial trans-formation and the possibility of backdoor attacks in the 3D domain.
• Inspired by the rotation-based 3D adversarial attacks, we develop a technique called feature disentanglement, which crafts perturbed data through optimization meth-ods. It makes the model relatively hard to learn the se-mantic label from the feature-disentangled data but easy to learn a correlation between the label and the implanted trigger. Equipped with this technique, we design a clean-label attack. It is stealthier than the poison-label attack and has broader application scenarios. 2.