Abstract
Instance segmentation in 3D scenes is fundamental in many applications of scene understanding. It is yet chal-lenging due to the compound factors of data irregularity and uncertainty in the numbers of instances. State-of-the-art methods largely rely on a general pipeline that first learns point-wise features discriminative at semantic and instance levels, followed by a separate step of point group-ing for proposing object instances. While promising, they have the shortcomings that (1) the second step is not super-vised by the main objective of instance segmentation, and (2) their point-wise feature learning and grouping are less effective to deal with data irregularities, possibly resulting in fragmented segmentations. To address these issues, we propose in this work an end-to-end solution of Semantic
Superpoint Tree Network (SSTNet) for proposing object in-stances from scene points. Key in SSTNet is an interme-diate, semantic superpoint tree (SST), which is constructed based on the learned semantic features of superpoints, and which will be traversed and split at intermediate tree nodes for proposals of object instances. We also design in SST-Net a refinement module, termed CliqueNet, to prune super-points that may be wrongly grouped into instance propos-als. Experiments on the benchmarks of ScanNet and S3DIS show the efficacy of our proposed method. At the time of submission, SSTNet ranks top on the ScanNet (V2) leader-board, with 2% higher of mAP than the second best method.
The source code in PyTorch is available at https:// github.com/Gorilla-Lab-SCUT/SSTNet. 1.

Introduction
The task of 3D instance segmentation is fundamental in many applications concerned with 3D scene understanding.
Given an observed scene of point cloud reconstructed from depth cameras via multi-view fusion techniques [8, 14], the task is to both assign semantic labels of pre-defined object
*Correspondence to Kui Jia <kuijia@scut.edu.cn> categories to individual scene points, and differentiate those belonging to different object instances. Learning to achieve 3D instance segmentation is challenging at least in the fol-lowing aspects: (1) observed scene points are usually sparse and irregular, which poses difficulties for learning point-wise classification based on shape features of local (and possibly global) contexts around individual points; (2) the unknown number of object instances in a scene introduces additional uncertainties to the problem of learning point-instance associations that is already combinatorial; (3) even though point-wise classification and point-instance associa-tions can be conducted, learning consistencies among spa-tially adjacent points are not guaranteed, which may cause fragmented segmentations, especially around object bound-aries (cf. Fig. 1 for an illustration).
State-of-the-art methods [16, 37, 9], e.g., those rank-ing top on the ScanNet benchmark [7], tackle (some of) the above challenges with the following general pipeline.
They first train networks to learn point-wise features that are discriminative at both the semantic and instance levels, followed by a separate step of point clustering that groups together those believed to be on same instances, using the learned point-wise features. While promising, they have the following shortcomings. Firstly, the second step of point clustering is independent of network training, whose results are thus not guaranteed by guiding towards the ground-truth groupings of object instances. Secondly, while superpoints
[19] have been commonly used for semantic segmentation of 3D points [18, 4], when coming to instance segmenta-tion, these state-of-the-art methods, except OccuSeg [15], choose to conduct both feature learning and grouping in a point-wise manner, which takes away their chance to lever-age the geometric regularities established at the mid-level shape representation of superpoints.
To overcome these shortcomings, we are motivated to develop an end-to-end solution for proposing object in-stances from an observed scene of points. Considering that a superpoint represents a geometrically homogeneous neighborhood, we choose to work with superpoints pre-computed from the scene points, and the problem of in-stance segmentation boils down as learning a network that groups superpoints on same object instances. In this work, we design such a solution called Semantic Superpoint Tree
Network (SSTNet), as illustrated in Fig. 2. Similar to ex-isting methods, SSTNet starts with a backbone that learns point-wise semantic and instance-level features; differently from them, SSTNet immediately aggregates these features as superpoint-wise ones efficiently via point-wise pooling.
Key in SSTNet is an intermediate, semantic superpoint tree (SST), with the superpoints as its tree leaves. SST is constructed based on the pooled semantic (and instance-level) features of superpoints, and will be traversed and split by the subsequent SSTNet module of binary classifi-cation; starting from the root, a proposal of object instance is formed as the superpoints of a tree branch when non-splitting decision is made at the intermediate tree node that spans the branch (cf. Fig. 3 for an illustration). Our tree construction is highly efficient by choosing ways of feature inheritance from leaves to the root and pair-wise similar-ity metric, which support fast algorithms such as nearest-neighbor chain [35]. We note that erroneous assignments of superpoints to instances may occur when constructing and traversing the tree. To compensate, we design a subse-quent refinement module termed CliqueNet, which converts each proposed branch as a graph clique and learns to prune some of the branch nodes. A ScoreNet [16] is finally used to evaluate the generated proposals, which gives instance segmentation results of our SSTNet.
Thorough experiments on the benchmark datasets of
ScanNet [7] and S3DIS[1] show the efficacy of our pro-posed method. Notably, SSTNet outperforms all existing methods on the two benchmarks, and at the time of sub-mission, it ranks top on the ScanNet (V2) leaderboard, with 2% higher of mAP than the second best method. We finally summarize our technical contributions as follows.
• We propose an end-to-end solution of Semantic Super-point Tree Network (SSTNet) to directly propose and evaluate object instances from observed 3D scenes. By working with superpoints, our method enjoys the ben-efit of geometric regularity that supports consistent and sharp segmentations, especially at object boundaries.
• We choose a strategy of divisive grouping in SSTNet, which first builds the tree, followed by tree traversal for object proposal via node splitting. By constructing the tree with appropriate node merging and feature in-heritance, our strategy is an order of magnitude faster than the alternative, agglomerative grouping, thus en-abling efficient training and inference of SSTNet.
• Considering that erroneous assignments of superpoints to instances may occur when constructing and travers-ing the tree, we design a refinement module in SST-Net, termed CliqueNet, which converts each proposed branch as a graph clique and learns to prune some of the branch nodes. Experiments show its efficacy.
Figure 1. Visualization of example instance segmentation results from an existing, point-wise grouping method (PointGroup[16], left) and our SSTNet (right). Different colors represent segmented instances. 2.