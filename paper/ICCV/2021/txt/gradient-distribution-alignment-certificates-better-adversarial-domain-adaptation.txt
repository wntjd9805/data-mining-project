Abstract
The latest heuristic for handling the domain shift in un-supervised domain adaptation tasks is to reduce the data distribution discrepancy using adversarial learning. Recent studies improve the conventional adversarial domain adap-tation methods with discriminative information by integrat-ing the classifier’s outputs into distribution divergence mea-surement. However, they still suffer from the equilibrium problem of adversarial learning in which even if the dis-criminator is fully confused, sufficient similarity between two distributions cannot be guaranteed. To overcome this problem, we propose a novel approach named feature gra-dient distribution alignment (FGDA)1. We demonstrate the rationale of our method both theoretically and empirically.
In particular, we show that the distribution discrepancy can be reduced by constraining feature gradients of two do-mains to have similar distributions. Meanwhile, our method enjoys a theoretical guarantee that a tighter error upper bound for target samples can be obtained than that of con-ventional adversarial domain adaptation methods. By inte-grating the proposed method with existing adversarial do-main adaptation models, we achieve state-of-the-art perfor-mance on two real-world benchmark datasets.
Illustration of Feature Gradient Distribution Align-Figure 1. ment (FGDA). (a)-(c): When features of two domains distribute very differently due to large domain shift, their gradients in non-overlapping regions may disperse in distinct parts of the highly complicated decision boundary, which leads to a large gradient distribution discrepancy. With the gradient alignment, the over-(a)-(b): lapping region is enlarged to reduce the domain shift.
Domain shift measured by the conventional adversarial domain adaptation method tends to be zero when two mean features are close enough. In this case, conventional methods fail to further reduce the domain shift. (b)-(c): Even if the distance between two mean features is small, the domain shift measured by our method (in terms of feature gradient discrepancy) can be still observed due to obvious different gradients in non-overlapping regions. FGDA can certificate a further domain shift reduction. 1.

Introduction
Deep Neural Networks (DNNs) have achieved impres-sive performance for various applications such as image classification [12, 16], object detection [11, 26] and seman-tic segmentation [21, 24]. However, DNNs may not gener-alize well on new data due to the data distribution shift prob-lem which manifest in many different ways, such as sample selection bias [5], class distribution shift [19], and covariate shift [29]. Unsupervised Domain Adaptation (UDA) aims to address domain shift with access to labeled source data
*Corresponding author: Kaizhu Huang (kaizhu.huang@xjtlu.edu.cn). 1The codes is available at https://github.com/gzqhappy/FGDA. and unlabeled target data [8]. The fundamental objective is to infer the domain-invariant representations [32].
Among current deep architectures, the adversarial do-main adaptation (ADA) approaches [25, 22, 30, 15] are widely investigated and achieve state-of-the-art perfor-mance. As one seminal work, Domain-Adversarial Neu-ral Networks (DANN) integrates adversarial learning and domain adaptation into a mini-max game [8]. A domain discriminator is learned to distinguish the source distribu-tion from the target one, while a deep classification model learns transferable representations that are indistinguishable for the domain discriminator. Recent successful methods
revealed that a discriminative distribution alignment enables a better domain adaptation [25, 22, 30, 15, 27, 38]. The key idea of those studies is leveraging the discriminative infor-mation delivered by the classifier’s outputs or predictions for target discriminative representation learning.
Although the discriminative information can help pro-mote the performance on domain adaptation, we argue that the domain shift still presents one major challenge which limits further performance improvement. Such drawback comes from the equilibrium challenge of adversarial learn-ing [2] in which even if the discriminator is fully confused, there is no guarantee that two distributions are sufficiently similar, as shown in Fig. 1 (a) and (b).
To tackle this problem, we propose a novel method called feature gradient distribution alignment (FGDA) in order to further reduce domain shift. Specifically, FGDA learns to reduce distribution discrepancy of feature gradi-ent between two domains in the manner of the adversarial learning between the feature extractor and the discriminator.
When the equilibrium is reached, the value of the feature distribution discrepancy can be minimal.
We borrow the insight of adversarial perturbations from
[10, 1, 37] to simply describe the principle of the proposed method. Input gradients of samples can be considered as sensitive directions which perturb the input least in order to change the model’s output most [1]. Intuitively, the fea-ture gradient direction of one sample may tend to point to the region of its nearest decision boundary. Furthermore, feature gradients apart from each other are probably differ-ent significantly since they point to the distinct part of the highly complicated decision boundary (as typically seen in
DNNs); the feature gradients closer to each other may share a similar direction. Therefore, aligning the feature gradi-ents encourages learning the latent representations which enforce the two domain distributions to stay closer. As such, the feature distribution discrepancy can be reduced. For the merit of our method, compared with conventional domain adaptation methods, our method can further reduce the do-main shift even if the mean features of two domains are close to each other as shown in Fig. 1, which is also theo-retically analyzed as later seen in Section 3.7. Importantly, we further prove that aligning the feature gradients leads to a tighter upper bound than conventional adversarial domain adaptation methods with respect to the expected error on target samples.
In a nutshell, our key contributions are listed as follows:
• We propose a novel method FGDA where adversarial learning is adopted to align the feature gradients for re-ducing distribution discrepancy. Compared with con-ventional methods, our model can further reduce the domain shift even if the means of the source and target distributions are close to each other.
• We prove the efficacy of our method both theoreti-cally and empirically. In particular, we show that our method can obtain a tighter upper bound than conven-tional domain adaptation methods.
• We conduct extensive experiments to show that the proposed approach is not only able to reduce domain discrepancy but also offers improvement consistently over current feature-based adversarial domain adap-tation methods. Particularly, our approach achieves state-of-the-art performances on tasks of UDA. 2.