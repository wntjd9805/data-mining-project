Abstract
Lensless cameras provide a framework to build thin imaging systems by replacing the lens in a conventional camera with an amplitude or phase mask near the sen-sor. Existing methods for lensless imaging can recover the depth and intensity of the scene, but they require solving computationally-expensive inverse problems. Furthermore, existing methods struggle to recover dense scenes with large depth variations. In this paper, we propose a lensless imag-ing system that captures a small number of measurements using different patterns on a programmable mask. In this context, we make three contributions. First, we present a fast recovery algorithm to recover textures on a fixed num-ber of depth planes in the scene. Second, we consider the mask design problem, for programmable lensless cameras, and provide a design template for optimizing the mask pat-terns with the goal of improving depth estimation. Third, we use a refinement network as a post-processing step to identify and remove artifacts in the reconstruction. These modifications are evaluated extensively with experimental results on a lensless camera prototype to showcase the per-formance benefits of the optimized masks and recovery al-gorithms over the state of the art. 1.

Introduction
Lensless cameras have traditionally been developed and used for imaging high-energy radiations such as X-ray and
Gamma ray, where fabricating lenses is either extremely expensive or physically unrealizable [8, 9, 12].
In the last few years, such lensless cameras have been proposed for imaging in visible and infrared wavebands as well; here, the absence of the lens provides advantages in thin form-factor imaging [5, 6], better depth-of-field tradeoffs in microscopy [1, 2], reduced cost in infrared/multi-spectral imaging [20, 24], and provides the ability to do inference from coded measurements [24, 29].
Depth estimation is an integral part of lensless imaging.
The basic principle of a lensless camera is that the sensor records the summation of the measurements associated with 27.3mm 100.0mm 91.3mm 311.1mm m a
C p e e w
S s r u
O
Figure 1: Examples of two 3D scenes reconstructed at different depth planes from eight sensor measurements using SweepCam [16] and our pro-posed method. the scene points. Each scene point casts its own unique sen-sor measurement, depending on its three-dimensional (3D) spatial location and radiance. Thus, the 3D scene informa-tion is encoded in the sensor measurements, but its recovery requires us to solve a nonlinear inverse problem. Further, it is important to model this depth dependence in the mea-surements especially since ignoring it results in significant reduction in the quality of reconstructions.
Existing methods for 3D lensless imaging can be divided into two categories. One category of methods estimate the 3D scene with a single image measurement [1, 2, 4, 7, 33].
These methods jointly estimate the image and depth map of a 3D scene by solving an optimization problem using iterative techniques. Since the number of variables to be es-timated is much larger than the number of measurements, the recovery problem is severely under-determined and the methods rely on some prior knowledge about the 3D scene.
For instance, [2] assumes that 3D volume is sparse and solves an ℓ1 norm-based optimization problem to estimate a 3D image. Another drawback of these optimization-based methods is their large computational cost and run time.
The second category of methods capture multiple measure-ments, each with a different mask, which makes the 3D recovery of dense scenes possible [16, 32]. The most re-lated work that concerns designing mask patterns for multi-ple measurements is SweepCam [16], which captures multi-ple measurements of the same scene using a programmable, shifting mask and estimates one plane in the 3D scene at a time. The recovery of a single plane is much faster than joint recovery of the entire 3D scene, but the number of mask patterns needed to achieve artifact-free reconstruction of a single depth plane can be large (in the range of 100–
400). Our proposed method falls in the second category, but instead of estimating a single plane, we recover the 3D scene at multiple depth planes jointly from a smaller num-ber of measurements, using a simple algorithm. A compar-ison of our method and SweepCam is shown in Figure 1 for the recovery of two 3D scenes using eight sensor measure-ments.
The proposed algorithm exploits the well-known fact that convolutional systems are diagonalized in the Fourier domain. Given that the measurement operator associated with each scene depth is well-approximated as a convo-lution, we show that the joint depth and image recovery problem can be significantly simplified when we model the scene intensities in the spatial frequency domain. Hence, instead of solving a single large linear system for image in-tensity across all of the depth planes, we solve several small linear systems in parallel, with each system associated with a different frequency coefficient.
To further improve the quality of estimated image planes, we implement the forward imaging operation and the recov-ery algorithm as a differentiable network and optimize over the mask patterns to minimize the reconstruction loss. We validate in real experiments that the learned mask patterns provide significantly better estimates of image planes for a variety of scenes compared to other commonly used mask patterns. Finally, we train a neural network to remove any artifacts in the estimated planes and convert them into an all-in-focus image and a continuous-valued depth map.
Contributions. The main contributions of this paper are as follows.
• We present a fast algorithm to recover multi-plane im-ages for the 3D scene from lensless measurements in the
Fourier domain.
• We implement a multi-plane lensless camera and Fourier-domain recovery algorithm as a differentiable network to optimize mask patterns for the lensless camera.
• We train a neural network to map the estimated multi-plane images into an all-in-focus image and a continuous-valued depth map.
• We built a prototype lensless camera with a pro-grammable mask and report several experiments to val-idate our proposed methods and a comparison with exist-ing methods.
Limitations. Despite the advantages listed above, the mask-based lensless camera that we propose here has some limitations. One main limitation, common to many lensless cameras, is the light throughput and limited dynamic range of the sensor. A second set of limitations stem from the assumption of the convolutional model, a common assump-tion in 3D lensless imaging work. The convolutional model fails to account for small sensor area that crop the measure-ments, non-Lambertian scenes, and the sensor’s angular re-sponse. In our experiments, we observe some artifacts in those scenarios, but they are usually local; for example, ar-tifacts from sensor cropping only affect image boundaries and specular reflections only produce artifacts in local re-gions, and do not affect other parts of images. Finally, we assume the scene is static in the duration that multiple mea-surements are captured. However, this work significantly reduces the number of measurement required, and hence the duration in which the scene is required to be static, com-pared to previous methods [16]. 2.