Abstract
Humans perform co-saliency detection by first summa-rizing the consensus knowledge in the whole group and then searching corresponding objects in each image. Previous methods usually lack robustness, scalability, or stability for the first process and simply fuse consensus features with im-age features for the second process. In this paper, we pro-pose a novel consensus-aware dynamic convolution model to explicitly and effectively perform the “summarize and search” process. To summarize consensus image features, we first summarize robust features for every single image using an effective pooling method and then aggregate cross-image consensus cues via the self-attention mechanism. By doing this, our model meets the scalability and stability re-quirements. Next, we generate dynamic kernels from con-sensus features to encode the summarized consensus knowl-edge. Two kinds of kernels are generated in a supplemen-tary way to summarize fine-grained image-specific consen-sus object cues and the coarse group-wise common knowl-edge, respectively. Then, we can effectively perform object searching by employing dynamic convolution at multiple scales. Besides, a novel and effective data synthesis method is also proposed to train our network. Experimental results on four benchmark datasets verify the effectiveness of our proposed method. Our code and saliency maps are avail-able at https://github.com/nnizhang/CADC. 1.

Introduction
Co-salient object detection (Co-SOD) mimics the human visual system to distinguish common and salient objects when viewing a group of relevant images. Although var-ious Co-SOD methods have been proposed, let us review this problem from the humans perspective. Given a group of images, humans can not segment the co-salient object in each image directly. Instead, they need to first observe all images and summarize the consensus knowledge about
*Corresponding author.
Figure 1. Main idea of our proposed method. what kind of objects this group is focusing on. Then, they look back at each image and search the corresponding ob-jects. We call this process “summarize and search”, which is illustrated in Figure 1. A similar explanation can also be found in [48]. Therefore, we can model Co-SOD in such an intuitive way to summarize the consensus knowledge first and then search consensus objects in each image.
Previous models can also be explained from such a point.
For consensus knowledge summarization, early traditional methods employed graph models [18] or clustering meth-ods [7, 39] to learn the common patterns. However, their models lack end-to-end learning, thus limiting the model performance. Some recent deep models [36, 35, 27] chose to concatenate and convolve all image features for summa-rizing the consensus knowledge. However, convolution can only aggregate the information at the same location among different images, while co-salient objects often show vari-ations in scales and locations in different images. Hence, these models may easily fail in consensus summarization.
Using non-local dependencies [34] to summarize the con-sensus cues is another choice [8]. However, this method lacks scalability since it is computationally prohibitive for processing a large number of images. Some other work
[17] adopted recurrent networks to summarize the consen-sus cues step by step. However, recurrent models define an input order for image sequences, thus lacking model stabil-ity since different input orders will lead to different results.
For consensus object searching, many works [36, 35, 17, 32, 40, 27, 47] directly fused the consensus feature
with image-specific features via summation or concatena-tion operations. [46] and [5] fused co-attention maps with the image-specific information via element-wise multipli-cation. Such simple methods conduct object searching by linear information fusion, which can not fully exploit the guidance of the summarized consensus knowledge. Be-sides, [48] computed channel-wise weight for each single image feature based on its similarity with the consensus rep-resentation, which can be seen as an attribute-wise object searching method. We argue that direct spatial searching might be more accurate and easy to learn.
In this paper, we propose a novel consensus-aware dy-namic convolution (CADC) model directly from the “sum-marize and search” point of view. The image features of the whole group are first summarized and then the consensus knowledge is encoded as dynamic kernels, which capture the appearance traits of common objects. Next, the search-ing step is performed by using the kernels to convolve the image features to obtain final results, as shown in Figure 1.
However, adopting dynamic convolution for Co-SOD re-quires delicate model design. We propose to summarize the consensus knowledge via first summarizing the feature of every single image and then integrating cross-image con-sensus features. For the first step, we propose to use a multi-scale max-pooling module to achieve position and scale ro-bust features. For the second step, we leverage the self-attention mechanism [31]. In this way, our model can meet the needs for scalability and stability. For consensus-aware dynamic kernel generation, we propose to simultaneously construct image adaptive kernels and a common kernel. The former is generated for each image separately to capture fine-grained image-specific cues while the latter is gener-ated for the whole group to summarize coarse group-wise common knowledge. Theoretically, the latter can serve as a supplement and regularization for the former to avoid them focusing too much on the image-specific information. We also generate efficient large dynamic kernels to further con-sider spatial structures and enlarge the searching range.
Besides, considering the lack of training data in the Co-SOD field, we propose a novel and effective data synthesis method by fusing common objects with unrelated salient objects in two different ways to mimic the real-world sce-narios. It can largely improve the Co-SOD performance and shows superiority when compared with previous methods.
Our main contributions can be summarized as follows.
• From the “summarize and search” perspective, we pro-pose a novel CADC model for Co-SOD. Dynamic ker-nels are generated to summarize the consensus knowl-edge and object searching is performed using dynamic convolution.
• We propose to combine multi-scale max-pooling and self-attention models to obtain consensus features with both model scalability and stability.
• We construct two types of dynamic kernels in a sup-plementary way to capture image-specific cues and the group-wise common knowledge, respectively.
• We develop a novel and more effective data synthesis method to mimic the challenging scenarios in the real world for Co-SOD models training.
• Our CADC network achieves new state-of-the-art Co-SOD results. 2.