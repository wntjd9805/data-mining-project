Abstract
In the low-bit quantization ﬁeld, training Binarized Neu-ral Networks (BNNs) is the extreme solution to ease the de-ployment of deep models on resource-constrained devices, having the lowest storage cost and signiﬁcantly cheaper bit-wise operations compared to 32-bit ﬂoating-point coun-terparts. In this paper, we introduce Sub-bit Neural Net-works (SNNs), a new type of binary quantization design tai-lored to compress and accelerate BNNs. SNNs are inspired by an empirical observation, showing that binary kernels learnt at convolutional layers of a BNN model are likely to be distributed over kernel subsets. As a result, unlike existing methods that binarize weights one by one, SNNs are trained with a kernel-aware optimization framework, which exploits binary quantization in the ﬁne-grained con-volutional kernel space. Speciﬁcally, our method includes a random sampling step generating layer-speciﬁc subsets of the kernel space, and a reﬁnement step learning to adjust these subsets of binary kernels via optimization. Experi-ments on visual recognition benchmarks and the hardware deployment on FPGA validate the great potentials of SNNs.
For instance, on ImageNet, SNNs of ResNet-18/ResNet-34 with 0.56-bit weights achieve 3.13/3.33× runtime speed-up and 1.8× compression over conventional BNNs with moderate drops in recognition accuracy. Promising re-sults are also obtained when applying SNNs to binarize both weights and activations. Our code is available at https://github.com/yikaiw/SNN . 1.

Introduction
To enable easy deployment of Convolutional Neural Net-works (CNNs) on mobile devices, many attempts are de-voted to improving network efﬁciency, e.g., reducing the
∗This research was done when Yikai Wang was an intern at Intel Labs
China, supervised by Anbang Yao who is responsible for correspondence.
Figure 1. Frequencies of different binary kernels in each layer, col-lected on well-trained ResNet-20 models. Left: a standard BNN model, with 1-bit per weight, which converts each ﬂoating-point kernel to one of the 512 binary kernels. Right: the proposed SNN model, with 0.56-bit per weight. Instead of using 512 binary ker-nels, 0.56-bit SNN only adopts 32 binary kernels for each layer, achieving larger compression and acceleration ratios than BNN. storage overheads or computational costs. Existing methods on this effort include efﬁcient architectural designs [11, 28], pruning [8, 17], network quantization [14, 19, 35, 37], knowledge distillation [9, 27], etc. Among these works, net-work quantization converts full-precision weights and acti-vations to low-bit discrete values, which can be particularly hardware-friendly. Binary Neural Networks (BNNs) [1], re-garded as the extreme case of quantization, resort to repre-senting networks with 1-bit values including only ±1. In addition, when weights and activations are both binarized, addition and multiplication operations can be replaced by cheap bit-wise operations. Under this circumstance, BNNs achieve remarkable compression and acceleration perfor-mance. Given the great potentials of BNNs, the commu-nity has made numerous efforts to narrow the accuracy gap between full-precision models and BNNs, e.g., introducing scaling factors to lessen the quantization error [26], or re-taining the kernel information by normalizations [25]. De-spite the advances in accuracy, there remains a less-explored direction of further compressing and accelerating BNNs.
A common understanding in the network quantization
ﬁeld is that BNNs are the extreme case enjoying the best compression and acceleration performance, via convert-ing 32-bit ﬂoating-point weights/activations into 1-bit ones.
However, in this work we challenge this common wisdom.
The main goal of this paper is to train a neural network with lower than 1-bit per weight, which is thus even more com-pressed than a conventional BNN model. We are motivated by an inspiring observation. Regarding a 3×3 full-precision convolutional kernel, its binarized counterpart belongs to the set which has |{±1}3×3| = 512 different binary kernels.
In Figure 1 (Left), we visualize the distribution of binary kernels in a binarized ResNet-20 [7] on CIFAR10 [15]. The distribution indicates that binary kernels in a well-trained
BNN model tend to be clustered to a subset at each layer, especially in deep layers where there are more ﬁlters. Such clustering distribution motivates us that potentially a more compressed model can be attained by learning to identify these important subsets which contain most binary kernels, which is illustrated in Figure 1 (Right). For a 3 × 3 con-volutional layer with cout output channels, if we choose a subset which consists of 2τ binary kernels (e.g., τ = 5), we will approximately obtain a compression ratio of τ 9 and an acceleration ratio of 2τ (see Sec. 4.4 and Sec. 4.5). cout
To determine the optimal subsets, we propose a method with two progressive steps. In the ﬁrst step, we randomly sample layer-speciﬁc subsets of binary kernels. To alleviate the potential impact of randomness, in the second step, we reﬁne the subsets by optimization. Models learnt with our full method are named Sub-bit Neural Networks (SNNs).
Figure 1 (Right) illustrates the learnt subsets after training an SNN model. With subsets reﬁnement, SNNs tend to learn similar cluster regions as BNNs. To the best of our knowledge, this is the ﬁrst method that simultaneously com-presses and accelerates BNNs in a quantization pipeline.
Detailed experiments on image classiﬁcation verify that our SNNs achieve impressively large compression and ac-celeration ratios over BNNs while maintaining high accu-racies. Besides, the hardware deployment proves that our
SNNs using ResNets as test examples bring more than 3× speed up over their BNN counterparts on ImageNet [4].
On the one hand, SNNs introduce a new perspective for the design space and the optimization of binary neu-ral network quantization. Particularly, SNNs uncover and leverage the relationships of clustered binary kernel distri-butions in different layers of BNN models. On the other hand, SNNs open a new technical direction for compressing and accelerating BNNs while maintaining their hardware-friendly properties, creating potentially valuable opportu-nities for specialized hardware designs conditioned on the advantages of SNNs. 2.