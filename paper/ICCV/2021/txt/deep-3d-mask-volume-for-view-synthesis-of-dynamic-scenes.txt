Abstract
Image view synthesis has seen great success in recon-structing photorealistic visuals, thanks to deep learning and various novel representations. The next key step in immer-sive virtual experiences is view synthesis of dynamic scenes.
However, several challenges exist due to the lack of high-quality training datasets, and the additional time dimen-sion for videos of dynamic scenes. To address this issue, we introduce a multi-view video dataset, captured with a custom 10-camera rig in 120FPS. The dataset contains 96 high-quality scenes showing various visual effects and hu-man interactions in outdoor scenes. We develop a new al-gorithm, Deep 3D Mask Volume, which enables temporally-stable view extrapolation from binocular videos of dynamic scenes, captured by static cameras. Our algorithm ad-dresses the temporal inconsistency of disocclusions by iden-tifying the error-prone areas with a 3D mask volume, and replaces them with static background observed throughout the video. Our method enables manipulation in 3D space as opposed to simple 2D masks, We demonstrate better tempo-ral stability than frame-by-frame static view synthesis meth-ods, or those that use 2D masks. The resulting view synthe-sis videos show minimal ﬂickering artifacts and allow for larger translational movements. 1.

Introduction
Recent advances in view synthesis have shown promis-ing results in creating immersive virtual experiences from images. Nonetheless, in order to reconstruct compelling and intimate interaction with the virtual scene, the ability to incorporate temporal information is much needed. In this paper, we study a speciﬁc setup where the input videos are from static, binocular cameras and novel views are mostly extrapolated from the input videos, similar to the case in
StereoMag[48]. We believe that this case is useful as dual-and multi-camera smartphones are gaining traction and it could also prove to be interesting for 3D teleconferencing, surveillance or playback on virtual reality headsets. More-over, we can acquire the dataset from a static camera rig as shown in Fig.1. Although we can apply state-of-the-art image view synthesis algorithms [48, 41, 28, 37] on each in-dividual video frame, the results lack temporal consistency and often show ﬂickering artifacts. The issues mostly come from the unseen occluded regions as the algorithm predicts them on a per-frame basis. The resulting estimations are not consistent across the time dimension and it causes some regions to become unstable when shown in a video.
In this paper, we address the temporal inconsistency when extrapolating views by exploiting the static back-ground information across time. To this end, we employ a 3D mask volume, which allows manipulation in 3D space as opposed to a 2D mask, to reason about moving objects in the scene and reuse static background observations across the video. As shown in Fig.4, we ﬁrst promote the instan-taneous and background inputs into two sets of multiplane images (MPI)[48] via an MPI network. Then, we warp the same set of input images to create a temporal plane sweep volume, providing information about the 3D structure of the scene. The mask network converts this volume to a 3D mask volume which allows us to blend between the two sets of MPIs. Finally, the blended MPI volume can render novel views with minimal ﬂickering artifacts.
To train this network, we also introduce a new multi-view video dataset to address the lack of publicly available data. We build a custom camera rig comprised of 10 ac-tion cameras and capture high-quality 120FPS videos with the static rig (see Fig.1). Our dataset contains 96 dynamic scenes of various outdoor environments and human interac-tions. We show that the proposed method generates tem-porally stable results against previous state-of-the-art meth-ods, while only using two input views.
Our contributions can be summarized as:
• a multi-view video dataset composed of 96 dynamic scenes (Sec. 3);
• a novel 3D volumetric mask able to segment dy-namic objects from static background in 3D, produc-ing higher-quality and temporally stable results than state-of-the-art methods (Sec. 4.2). 2.