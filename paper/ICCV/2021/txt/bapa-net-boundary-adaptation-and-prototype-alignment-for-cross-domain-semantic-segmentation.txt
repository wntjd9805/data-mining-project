Abstract
Existing cross-domain semantic segmentation methods usually focus on the overall segmentation results of whole objects but neglect the importance of object boundaries.
In this work, we find that the segmentation performance can be considerably boosted if we treat object boundaries properly. For that, we propose a novel method called
BAPA-Net, which is based on a convolutional neural net-work via Boundary Adaptation and Prototype Alignment, under the unsupervised domain adaptation setting. Specif-ically, we first construct additional images by pasting ob-jects from source images to target images, and we de-velop a so-called boundary adaptation module to weigh each pixel based on its distance to the nearest boundary pixel of those pasted source objects. Moreover, we pro-pose another prototype alignment module to reduce the domain mismatch by minimizing distances between the class prototypes of the source and target domains, where boundaries are removed to avoid domain confusion dur-ing prototype calculation. By integrating the boundary adaptation and prototype alignment, we are able to train a discriminative and domain-invariant model for cross-domain semantic segmentation. We conduct extensive ex-periments on the benchmark datasets of urban scenes (i.e.,
GTA5→Cityscapes and SYNTHIA→Cityscapes). And the promising results clearly show the effectiveness of our
BAPA-Net method over existing state-of-the-art for cross-domain semantic segmentation. Our implementation is available at https://github.com/manmanjun/BAPA-Net. 1.

Introduction
Because of the powerful representation ability of deep convolutional neural networks [26], it has deeply boosted the performance of the computer vision tasks including im-age recognition [45, 21], object detection [16, 32], seman-tic segmentation [33, 63, 3], etc. They all require plentiful
*The corresponding author
Figure 1. Comparisons between (a) the ground truth annotations and (b) the segmentation result of an existing unsupervised domain adaptation method [57] of a sample target image. It is obvious that the segmentation results of boundary pixels are worse than the in-ner pixels (e.g., the inner part of the rider in (c) and the vegetation in (d) are predicted perfectly, while the boundary pixels are not). images and accurate annotations to train high-performance models. Compared with image recognition, semantic seg-mentation is more complex and aims at classifying each pixel in an image. Therefore, collecting the annotations for segmentation is an extremely expensive and laborious pro-cess (e.g., 90 minutes per image for Cityscapes [7]).
A natural alternative is to collect the well-annotated syn-thetic data from the simulation platform where it can auto-matically render the various scenes (e.g., sunny, rain, foggy street) with a much lower cost. For example, [43] builds a large-scale urban scene dataset obtained from the GTA5 video game. However, the trained model on such syn-thetic data will suffer from a significant performance drop as there exists a considerable domain discrepancy between
source and target domains. A variety of unsupervised do-main adaptation methods are proposed to maximally elim-inate the domain discrepancy through adversarial feature learning [22, 6, 13], entropy minimization [52, 53, 5], self-training [65, 66, 31, 11], etc.
However, we observe that current state-of-the-art meth-ods often focus on the overall segmentation results of whole objects but neglect the importance of object boundaries.
Taking the Fig. 1 as an example, some pixels along the boundaries of the person and tree are wrongly classified.
The reason is that the near-boundary pixels and inner-object pixels are different, as the receptive field of the bound-ary sample might contain pixels from other classes, mak-ing near-boundary pixels difficult to classify. This becomes even worse in the Unsupervised Domain Adaptation (UDA) scenario, where a considerable distribution mismatch exists between source and target domains.
Therefore, the segmentation performance can be con-siderably boosted if we treat object boundaries prop-erly.
To achieve this, we propose a novel method called Boundary Adaptation and Prototype Alignment Net-work (BAPA-Net). Specifically, we first construct addi-tional images by pasting objects from source images to tar-get images, and we develop a so-called boundary adap-tation module to weigh each pixel based on its distance to the nearest boundary pixel of those pasted source ob-jects. Moreover, we propose another prototype alignment module to reduce the domain mismatch by minimizing dis-tances between the class prototypes of the source and tar-get domains, where boundaries are removed to avoid do-main confusion during prototype calculation. By inte-grating the boundary adaptation and prototype alignment, we are able to train a discriminative and domain-invariant model for cross-domain semantic segmentation. The pro-posed method outperforms the state-of-the-art counterparts by a large margin on the benchmarks of GTA5→Cityscapes and SYNTHIA→Cityscapes respectively, which verifies the effectiveness of our BAPA-Net.
The main contributions of our work can be summarized as follows:
• We reveal a critical finding that existing cross-domain semantic segmentation methods neglect the impor-tance of object boundary. Thus we propose a novel approach called Boundary Adaptation and Proto-type Alignment (BAPA-Net) to maximally exploit the boundary properly.
• We develop a so-called boundary adaptation module to weigh each pixel around the boundary and a new pro-totype alignment module to build more reliable proto-types by removing the domain confused boundaries so that the domain mismatch between source and target domains can be reduced effectively.
• We conduct extensive experiments on the benchmark settings for urban scenes (i.e., GTA5→Cityscapes and
SYNTHIA→Cityscapes), and the experimental results demonstrate the effectiveness of our proposed method. 2.