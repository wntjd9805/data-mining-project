Abstract
Video Question Answering (Video QA) aims to give an answer to the question through semantic reasoning be-tween visual and linguistic information. Recently, han-dling large amounts of multi-modal video and language information of a video is considered important in the in-dustry. However, the current video QA models use deep features, suffered from signiﬁcant computational complex-ity and insufﬁcient representation capability both in train-ing and testing. Existing features are extracted using pre-trained networks after all the frames are decoded, which is not always suitable for video QA tasks.
In this paper, we develop a novel deep neural network to provide video
QA features obtained from coded video bit-stream to re-duce the complexity. The proposed network includes sev-eral dedicated deep modules to both the video QA and the video compression system, which is the ﬁrst attempt at the video QA task. The proposed network is predominantly model-agnostic. It is integrated into the state-of-the-art net-works for improved performance without any computation-ally expensive motion-related deep models. The experimen-tal results demonstrate that the proposed network outper-forms the previous studies at lower complexity. https:
//github.com/Nayoung-Kim-ICP/VQAC 1.

Introduction
Recent advances in artiﬁcial intelligence (AI) have brought signiﬁcant attention to the multidisciplinary re-search area of computer vision (CV) and natural langue processing (NLP). Video question answering (QA) aims to give a reasonable answer by jointly conducting visual un-derstanding and language-speciﬁc reasoning. It has a num-ber of real-time emerging intelligent applications such as human-AI interactions and communication systems.
Previous video QA studies have focused on develop-Figure 1. Motivation of the proposed VQAC-baseline network ar-chitecture. It retrieves residue and motion vectors (MVs) from a coded bit-stream from only the partial decoding to save compu-tational resources. The compressed-domain features are used for generating a motion-appearance aggregation (MA+) feature. ing sophisticated deep learning models to resolve diverse
In recent stud-reasoning problems in multimodal data. ies [14, 43, 9, 45, 12], the QA models incorporated exter-nal memories [14, 12] and attention mechanisms [42, 45] to improve performance. Nevertheless, the previous stud-ies straightforwardly used the same baseline neural network architecture for extracting video features and question fea-tures. A convolutional neural network (CNN) and 3D-CNN
[34, 6] are used for an appearance feature and a motion feature, respectively. A recurrent neural network (RNN) is used for a question feature [11, 25]. Given with the sepa-rately generated features, the previous models were used to understand semantic relations to answer the question. How-ever, several studies indicated that the current approaches suffered from signiﬁcantly degraded performance when the features lacked sufﬁcient representation capabilities [5, 10].
It is problematic that the current baseline model naively
uses a pre-trained neural network for extracting features.
Few studies have redesigned a baseline structure to pro-vide more efﬁcient features in the video QA task, likely be-cause many computational resources are required to exploit
QA features both for training and testing. 3D-CNN is ex-tremely complex, despite being developed for representing a homogenous motion. For lightweight features, we intro-duce compressed-domain features that are included in a bit-stream of coded video data. Video compression enables a sequence of frames to be reconstructed using only a few anchor frames named an intra-coded frame (I-frame) with complete RGB data and several ingredients for prediction such as residue and a motion vector (MV). Because most video content is compressed in advance and the residue and
MVs are readily obtained as intermediate outputs during de-compression, various CV tasks could be facilitated in the previous studies [33, 31, 3, 38].
This paper proposes a time-efﬁcient video QA network using compressed-domain video features (VQAC) to im-prove performance at lower complexity.
Previous QA works are difﬁcult to apply directly to compressed video data. Conventional video features such as C3D [34] and I3D
[6] can only be created if complete video frames are avail-able after decompression. However, full decompression re-quires extra latency and extensive storage, which further de-teriorates computational complexity for feature extraction.
In our framework, for an appearance feature, a pre-trained CNN [30, 16] is applied only to I-frames to avoid any delay or latency, as depicted in Fig. 1, because non-anchor P- and B-frames are only available after the I-frames are fully reconstructed. For motion, residue and MVs are
ﬁrst retrieved with only the partial decoding of P- and B-frames to avoid their full reconstruction. The compressed-domain features are then used for generating motion fea-tures to replace the existing 3D-CNN.
Our approach is the ﬁrst attempt to apply compressed-domain features to video QA tasks. Previously, Shou et al.
[29] and Wu et al. [38] proposed to exploit compressed-domain features in action recognition tasks, identifying only a few representative motions in a video. Compared with other CV tasks, the video QA model needs to achieve a more comprehensive and semantically aligned interpreta-tion of a video and query. However, it is computationally intractable to learn such features in end-to-end, consider-ing the nature of multimodal data. These problems moti-vate us to apply the compressed-domain features to the QA task. While the previous studies have only few choices of the pre-trained features, the compressed features are readily obtained by decompressing existing data.
The VQAC network produces video QA features that consider different modalities and more efﬁcient alignments.
Fig. 1 shows an overall scheme of the VQAC-baseline net-work. The network creates a motion-appearance aggrega-tion (MA+) feature as output. It is promptly generated by warping the current appearance feature using a MV and adapting to temporal dynamics using a residue. The MA+ feature is fused with question features to a decision net-work to exploit inter-modal correlation, which is crucial for video-related multimodal tasks. The VQAC-baseline can be used as a standalone model to operate very fast.
Furthermore, the network can be integrated into the ex-isting video QA models because the baseline network is predominantly model agnostic. Previous studies [14, 12] have attempted to improve performance by understanding global contexts over a video. The current state-of-the-art networks commonly use memory modules to retain global appearance and motion features using read and write oper-ations. Therefore, we present a VQAC-integration model that uses both the proposed QA features and some global features by combining the baseline with the existing model, which maintains global video and question features.
Our primary contributions are summarized as follows:
• We present a VQAC-baseline network to resolve the major drawbacks of the previous video QA features: signiﬁcant computational complexity and insufﬁcient representation capability. We introduce compressed-domain features and develop several dedicated mod-ules to both the video QA and the video compression system, which is the ﬁrst attempt at the video QA task.
• We develop a VQAC-integration network to integrate the baseline model for improved performance without any computationally expensive motion models [34].
The VQAC-integration model outperforms the previ-ous studies for various video QA datasets. 2.