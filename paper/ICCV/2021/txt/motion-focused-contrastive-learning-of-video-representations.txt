Abstract
Motion, as the most distinct phenomenon in a video to involve the changes over time, has been unique and crit-ical to the development of video representation learning.
In this paper, we ask the question: how important is the motion particularly for self-supervised video representation learning. To this end, we compose a duet of exploiting the motion for data augmentation and feature learning in the regime of contrastive learning. Speciﬁcally, we present a
Motion-focused Contrastive Learning (MCL) method that regards such duet as the foundation. On one hand, MCL capitalizes on optical ﬂow of each frame in a video to tem-porally and spatially sample the tubelets (i.e., sequences of associated frame patches across time) as data augmenta-tions. On the other hand, MCL further aligns gradient maps of the convolutional layers to optical ﬂow maps from spa-tial, temporal and spatio-temporal perspectives, in order to ground motion information in feature learning. Exten-sive experiments conducted on R(2+1)D backbone demon-strate the effectiveness of our MCL. On UCF101, the lin-ear classiﬁer trained on the representations learnt by MCL achieves 81.91% top-1 accuracy, outperforming ImageNet supervised pre-training by 6.78%. On Kinetics-400, MCL achieves 66.62% top-1 accuracy under the linear protocol. 1.

Introduction
The key difference between video and image is the di-mension of time, which derives a particular form of mo-tion information in a video. The state-of-the-art works of-ten delve into motion in different ways, e.g., long/short term dependencies [29, 43, 21], temporal structure/orders
[19, 20, 47], and temporal pooling [42, 49], to enhance video understanding. The underlying foundation behind these advances generally originates from the improvement of representation learning via the exploration of motion in-formation. Most recently, self-supervised representation learning is gaining signiﬁcant momentum [2, 4, 14], and
*This work was performed at JD AI Research. the number of self-supervised learning papers practically exploded. In particular, contrastive learning, as a memory-based self-supervised learning approach, is extended to video domain [13, 50] and further closes the gap between self-supervised and supervised video representation learn-ing. A valid question then emerges as how important is the motion for self-supervised video representation learning?
In an effort to answer the question, we look into the problem, in the context of contrastive learning, from two different perspectives: 1) leveraging motion information in achieving data augmentations, and 2) taking motion into ac-count in the optimization of feature learning. In a video, the motion of different regions is inherently various and the ve-locity of motion measures the rate of change in position of the region with respect to a frame of reference. In general, the regions with larger velocities have much richer informa-tion and are potentially more advantageous for contrastive learning. As a result, we capitalize on motion information from both spatial and temporal dimensions to carefully sam-ple the sequences of patches across frames, i.e., tubelets, as augmentations, and examine how the ﬁrst issue affects self-supervised video representation learning. To study the second one, we encourage the grounding of motion infor-mation explicitly in feature learning by aligning gradient maps of the convolutional layers to motion (optical ﬂow) maps. As such, feature learning executes the optimization with respect to motion information.
To consolidate the idea of exploring the motion informa-tion in video sequence for self-supervised video represen-tation learning, we present a novel Motion-focused Con-trastive Learning (MCL) method. Speciﬁcally, we leverage unsupervised TV-L1 algorithm [53] to extract the dense op-tical ﬂow of each frame in a video and compute the mo-tion boundaries as in [6] to obtain the motion map. A video is divided into a set of ﬁxed-length video clips and the spatio-temporal motion map (ST-motion) of each video clip consists of the sequential motion maps of all the frames in the clip. MCL then performs a motion-focused spatio-temporal sampling to select tubelets as data augmentations.
Technically, MCL applies a 3D average pooling on the
spatio-temporal motion map to measure the clip-level mo-tion, which indicates the degree of motion of each clip. The clips with relatively large clip-level motion are chosen as the clip candidates for temporal augmentation. Next, MCL employs a temporal pooling on the spatio-temporal motion map of each clip candidate to estimate the motion from spa-tial viewpoint (S-motion) and localize the spatial patches, which are temporally consistent across frames, as tubelets.
Furthermore, in feature learning, MCL extracts the spatio-temporal motion map of each tubelet and executes a spa-tial/temporal pooling on such spatio-temporal motion map to output the motion map from temporal/spatial viewpoint (T/S-motion) of the tubelet. The gradient with regard to the feature map of a convolutional layer by back-propagation is produced in spatial, temporal and spatio-temporal manner, respectively, to align with S-motion, T-motion, ST-motion through minimizing the mean squared error in between.
MCL integrates the alignments into contrastive learning framework as constraints in addition to InfoNCE loss.
The main contribution of the work is the proposal of leveraging motion information to boost self-supervised video representation learning on the recipe of contrastive learning. This leads to the elegant views of how to ef-fectively sample spatio-temporal augmentations in terms of motion, and how to integrate motion information into the optimization of feature learning, which are problems not yet fully understood. We demonstrate that our self-supervised method MCL surpasses ImageNet supervised pre-training on two video benchmarks and the experiments on two downstream video tasks also validate our MCL. 2.