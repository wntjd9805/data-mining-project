Abstract
In various imaging problems, we only have access to compressed measurements of the underlying signals, hin-dering most learning-based strategies which usually require pairs of signals and associated measurements for train-ing. Learning only from compressed measurements is im-possible in general, as the compressed observations do not contain information outside the range of the forward sens-ing operator. We propose a new end-to-end self-supervised framework that overcomes this limitation by exploiting the equivariances present in natural signals. Our proposed learning strategy performs as well as fully supervised meth-ods. Experiments demonstrate the potential of this frame-work on inverse problems including sparse-view X-ray com-puted tomography on real clinical data and image inpaint-ing on natural images. Code has been made available at: https://github.com/edongdongchen/EI. 1.

Introduction
Linear inverse problems are ubiquitous in computer vi-sion and signal processing, appearing in multiple important applications such as super-resolution, image inpainting and computed tomography (CT). The goal in these problems consists of recovering a signal x from measurements y, that is inverting the forward process y = Ax + (cid:15), (1) which is generally a challenging task due to the ill-conditioned operator A and noise (cid:15).
In order to obtain a stable inversion, traditional approaches have used linear re-construction, i.e. A†y, where the estimate is restricted to the range space of A(cid:62), or model-based approaches that reduce the set of plausible reconstructions x using prior informa-tion (e.g. sparsity). Leveraging the powerful representation properties of deep neural networks, a different approach is taken by recent end-to-end learning solutions which learn the inverse mapping directly from samples (x, y). However, all of these approaches require ground truth signals x for learning the reconstruction function x = f (y), which hin-ders their applicability in many real-world scenarios where ground truth signals are either impossible or expensive to obtain. This limitation raises the natural question: can we learn the reconstruction function without imposing strong priors and without knowing the ground-truth signals?
Here, we show that typical properties of physical mod-els such as rotation or shift invariance, constitute mild prior information that can be exploited to learn beyond the range space of A(cid:62). We present an end-to-end equivariant imag-ing framework which can learn the reconstruction function from compressed measurements y alone for a single for-ward operator. As shown in Figure 1, the equivariant imag-ing approach performs almost as well as having a dataset of ground truth signals x and signiﬁcantly outperforms simply enforcing the measurement consistency Af (y) = y in the training process. We show both theoretically and empiri-cally that the proposed framework is able to identify the sig-nal set and learn the reconstruction function from few train-ing samples of compressed observations without access-ing ground truth signals x. Experimental results demon-strate the potential of our framework through qualitative and quantitative evaluation on inverse problems. Speciﬁ-cally our contributions are as follows: 1. We present a conceptually simple equivariant imaging paradigm for solving inverse problems without ground truth. We show how invariances enable learning be-yond the range space of the adjoint of the forward op-erator, providing necessary conditions to learn the re-construction function. 2. We show that this framework can be easily incorpo-rated into deep learning pipelines using an additional loss term enforcing the system equivariance. 3. We validate our approach on sparse-view CT recon-struction and image inpainting tasks, and show that our approach obtains reconstructions comparable to fully supervised networks trained with ground truth signals. 1.1.