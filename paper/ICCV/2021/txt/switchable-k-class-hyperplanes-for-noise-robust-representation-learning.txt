Abstract
Optimizing the K-class hyperplanes in the latent space has become the standard paradigm for efﬁcient represen-tation learning. However, it’s almost impossible to ﬁnd an optimal K-class hyperplane to accurately describe the la-tent space of massive noisy data. For this potential problem, we constructively propose a new method, named Switch-able K-class Hyperplanes (SKH), to sufﬁciently describe the latent space by the mixture of K-class hyperplanes. It can directly replace the conventional single K-class hyper-plane optimization as the new paradigm for noise-robust representation learning. When collaborated with the pop-ular ArcFace on million-level data representation learn-ing, we found that the switchable manner in SKH can ef-fectively eliminate the gradient conﬂict generated by real-world label noise on a single K-class hyperplane. More-over, combined with the margin-based loss functions (e.g.
ArcFace), we propose a simple Posterior Data Clean strat-egy to reduce the model optimization deviation on clean dataset caused by the reduction of valid categories in each
K-class hyperplane. Extensive experiments demonstrate that the proposed SKH easily achieves new state-of-the-art on IJB-B and IJB-C by encouraging noise-robust represen-tation learning. Our code will be available at https:
//github.com/liubx07/SKH.git. 1.

Introduction
Optimizing the K-class hyperplane in the latent space to encourage intra-class compactness and inter-class dis-crepancy has become the standard paradigm for efﬁcient
∗Equally-contributed.
†Corresponding author. (a) Single 3-class hyperplane (b) Other 3-class hyperplane (c) Switchable 3-class hyperplanes
Figure 1. Illustration of the 3-class hyperplane in latent space of noisy data. (a) Conventional single 3-class hyperplane. (b) Try to ﬁnd another 3-class hyperplane. (C) The proposed Switchable 3-class hyperplanes. representation learning. Beneﬁt from it, million-level face recognition has achieved remarkable improvement in recent years [21, 17, 26, 6, 25]. From the early CASIA-WebFace
[31] to more recent MegaFace [15], MSCeleb-1M [11] and
Celeb500K [3], the growing scale of training dataset intro-duces more complex data distribution and also inevitably introduces real-world noise. This leads to that it’s almost impossible to ﬁnd an optimal K-class hyperplane to accu-rately describe the latent space with massive noisy data.
We explore this potential problem by a latent space with
3 categories as shown in Fig. 1(a). Optimizing a single 3-class hyperplane in this latent space is extremely difﬁ-cult and the gradient conﬂict generated by the local training samples collapses the hyperplane optimization. Fig. 1(b) attempts to optimize some local outliers but biases other samples. For this potential problem, we constructively pro-pose a new method, named Switchable K-class Hyperplanes (SKH), to sufﬁciently describe the latent space with mas-sive noisy data. We introduce the mixture of K-class hy-perplanes and optimize them by a switchable manner as shown in Fig. 1(c). To sufﬁciently describe the latent space in Fig. 1(c), we can adopt a simple greedy mechanism based on loss value to perform K-class hyperplane switch.
According to the aforementioned analysis, given a ran-dom latent space, we can always describe it better than traditional training paradigm by assigning multiple K-class hyperplanes. We found that directly replacing the con-ventional K-class hyperplane by SKH can be effective for noise-robust representation learning. There are two types of conﬂict caused by label noise, intra-class conﬂict (dif-ferent identities with same label ID) and inter-class conﬂict (same identity with different label IDs). In latent space, the intra-class compactness and inter-class discrepancy is sig-niﬁcant to improve the robustness of representation. We take the conventional single K-class hyperplane to demon-strate the inﬂuence of label noise on latent space optimiza-tion. For different identities with same ID in intra-class conﬂict, we optimize the latent space by maximizing the inner product of its feature and a single class center. It’s hard to provide the efﬁcient hyperplane to compact them as shown in Fig. 1(a). For same identity with different IDs in inter-class conﬂict, we will minimize the inner product of features in one ID and class center of the other ID. How-ever, they represent a same identity and forcing them apart makes the hyperplane unreliable.
Although the presence of label noise complicates latent space, the proposed Switchable K-class Hyperplanes can ef-ﬁciently alleviate this issue. It encourages different identi-ﬁes with the same ID to select different K-class hyperplanes to avoid intra-class conﬂict. Also, the inter-class conﬂict can be eliminated by assigning samples of different IDs (ac-tually the same identity) to optimize different hyperplanes.
This makes them independent of each other and avoids gen-erating gradient conﬂict on the same hyperplane as shown in Fig. 1(c). Extensive experimental results demonstrate that the proposed SKH provides a new state-of-the-art for noise-robust representation learning. Moreover, for noise-robust representation learning, we can further clean the training data by dropping intra high-conﬁdent noise sam-ples and merging inter samples with high-similar centers in different K-class hyperplanes. After this, we can effec-tively improve intra-class compactness and inter-class dis-crepancy, and achieve comparable performance compared to the model trained on the manually cleaned dataset.
To sum up, the contribution of this papers is threefold: (1) A novel Switchable K-class Hyperplanes - We in-troduce a mixture of K-class hyperplanes with a switch-able manner to better describe the latent space with com-plex noisy data. It can effectively improve the robustness of noisy data training. (2) A posterior data clean strategy - We can further clean the training data jointly considering intra-class noise and inter-class noise. This process is easy to perform and effec-tively improves performance. (3) Superior performance on noise-robust representation learning in face recognition - We apply SKH to different types of label noise and conduct extensive experiments to thoroughly evaluate its superiority to other methods. It pro-vides a new state-of-the-art for noise-robust representation learning. 2.