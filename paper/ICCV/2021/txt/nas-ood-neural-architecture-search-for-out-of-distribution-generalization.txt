Abstract
Recent advances on Out-of-Distribution (OoD) gener-alization reveal the robustness of deep learning models against distribution shifts. However, existing works focus on OoD algorithms, such as invariant risk minimization, domain generalization, or stable learning, without consid-ering the influence of deep model architectures on OoD gen-eralization, which may lead to sub-optimal performance.
Neural Architecture Search (NAS) methods search for ar-chitecture based on its performance on the training data, which may result in poor generalization for OoD tasks. In this work, we propose robust Neural Architecture Search for
OoD generalization (NAS-OoD), which optimizes the archi-tecture with respect to its performance on generated OoD data by gradient descent. Specifically, a data generator is learned to synthesize OoD data by maximizing losses com-puted by different neural architectures, while the goal for architecture search is to find the optimal architecture pa-rameters that minimize the synthetic OoD data losses. The data generator and the neural architecture are jointly opti-mized in an end-to-end manner, and the minimax training process effectively discovers robust architectures that gen-eralize well for different distribution shifts. Extensive ex-perimental results show that NAS-OoD achieves superior performance on various OoD generalization benchmarks with deep models having a much fewer number of param-eters. In addition, on a real industry dataset, the proposed
NAS-OoD method reduces the error rate by more than 70% compared with the state-of-the-art method, demonstrating the proposed method’s practicality for real applications.
*Nanyang Ye is the corresponding author.
Figure 1. NAS-OoD performs significantly better than existing
OoD generalization baselines in terms of test accuracy and net-work parameter numbers. The upper left points are better than lower right ones because they have higher test accuracy and lower parameter numbers. 1.

Introduction
Deep learning models have encountered significant per-formance drop in Out-of-Distribution (OoD) scenarios [4, 26], where test data come from a distribution different from that of the training data. With their growing use in real-world applications in which mismatches of test and train-ing data distributions are often observed [25], extensive ef-forts have been devoted to improving generalization abil-ity [30, 3, 20, 5]. Risk regularization methods [3, 1, 41] aim to learn invariant representations across different train-ing environments by imposing different invariant risk reg-ularization. Domain generalization methods [30, 31, 9, 48] learn models from multiple domains such that they can gen-eralize well to unseen domains. Stable learning [27, 28, 20] focuses on identifying stable and causal features for predic-tions. Existing works, however, seldom consider the effects
of architectures on generalization ability. On the other hand, some pioneer works suggest that different architectures show varying OoD generalization abilities [22, 12, 33].
How a network’s architecture affects its ability to handle
OoD distribution shifts is still an open problem.
Conventional Neural Architecture Search (NAS) meth-ods search for architectures with maximal predictive per-formance on the validation data that are randomly divided from the training data [49, 36, 34, 44]. The discovered architectures are supposed to perform well on unseen test data under the assumption that data are Independent and
Identically Distributed (IID). While novel architectures dis-covered by recent NAS methods have demonstrated supe-rior performance on different tasks with the IID assump-tion [40, 18, 45, 24], they may suffer from over-fitting in
OoD scenarios, where the test data come from another dis-tribution. A proper validation set that can evaluate the per-formance of architectures on the test data with distribution shifts is crucial in OoD scenarios.
In this paper, we propose robust NAS for OoD gener-alization (NAS-OoD) that searches architectures with max-imal predictive performance on OoD examples generated by a conditional generator. An overview of the proposed method is illustrated in Figure 2. To do NAS and train an
OoD model simultaneously, we follow the line of gradient-based methods for NAS [34, 42, 8, 23, 44], however, we ex-tend that on several fronts. The discrete selection of archi-tectures is relaxed to be differentiable by building all can-didate architectures into a supernet with parameter sharing and adopting a softmax choice over all possible network op-erations. The goal for architecture search is to find the op-timal architecture parameters that minimize the validation loss under the condition that the corresponding network pa-rameters minimize the training loss.
Instead of using part of the training set as the valida-tion set, we train a conditional generator to map the original training data to synthetic OoD examples as the validation data. The parameters of the generator are updated to max-imize the validation loss computed by the supernet. This update encourages the generator to synthesize data having a different distribution from the original training data since the supernet is optimized to minimize the error on the train-ing data. To search for the architectures with optimal OoD generalization ability, the architecture parameters are opti-mized to minimize the loss on the validation set containing synthetic OoD data. This minimax training process effec-tively drives both the generator and architecture search to improve their performance and finally derive the robust ar-chitectures that perform well for OoD generalization.
Our main contributions can be summarized as follows: 1. To the best of our knowledge, NAS-OoD is the first at-tempt to introduce NAS for OoD generalization, where a conditional generator is jointly optimized to synthe-size OoD examples helping to correct the supervisory signal for architecture search. 2. NAS-OoD gets the optimal architecture and all opti-mized parameters in a single run. The minimax train-ing process effectively discovers robust architectures that generalize well for different distribution shifts. 3. We take the first step to understanding the OoD gen-eralization of neural network architectures systemati-cally. We provide a statistical analysis of the searched architectures and our preliminary practice shows that architecture does influence OoD robustness. 4. Extensive experimental results show that NAS-OoD outperforms the previous SOTA methods and achieves the best overall OoD generalization performance on various types of OoD tasks with the discovered archi-tectures having a much fewer number of parameters. 2.