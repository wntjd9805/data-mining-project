Abstract
Human can easily recognize visual objects with lost in-formation: even losing most details with only contour re-served, e.g. cartoon. However, in terms of visual perception of Deep Neural Networks (DNNs), the ability for recogniz-ing abstract objects (visual objects with lost information) is still a challenge. In this work, we investigate this issue from an adversarial viewpoint: will the performance of DNNs decrease even for the images only losing a little informa-tion? Towards this end, we propose a novel adversarial at-tack, named AdvDrop, which crafts adversarial examples by dropping existing information of images. Previously, most adversarial attacks add extra disturbing information on clean images explicitly. Opposite to previous works, our proposed work explores the adversarial robustness of DNN models in a novel perspective by dropping imperceptible de-tails to craft adversarial examples. We demonstrate the ef-fectiveness of AdvDrop by extensive experiments, and show that this new type of adversarial examples is more difficult to be defended by current defense systems. 1.

Introduction
Deep Neural Networks (DNNs) have demonstrated their outstanding performance across many applications such as computer vision [24] and natural language processing [47].
Though DNNs have great achievement in these tasks, espe-cially in computer vision, they are known to be vulnerable to adversarial examples. Adversarial examples of DNNs were first discovered by Szegedy et al.
[42], which are crafted by adding malicious perturbation on clean images to generate undesirable consequences. Various methods have been proposed to generate adversarial examples [20, 32, 7].
Typically, the generated adversarial perturbation is bounded by a small norm ball, which guarantees the resultant images
“look like” benign images.
Interestingly, Ilyas et al. [27] empirically demonstrated
Work done when Ranjie Duan interns at Alibaba Group, China
Code is available at https://github.com/RjDuan/AdvDrop
†Correspondence to: A. K. Qin & Yun Yang
Figure 1: Adv. images generated by PGD and AdvDrop.
Compared to the clean images, the adversarial images gen-erated by AdvDrop have fewer details composed of fewer colors, with the decreasing in size (by 15% and 7%). that adversarial perturbation can be non-robust features for
DNNs. That is to say, regarding adversarial perturbation, they are meaningful features for DNNs, but meaningless and imperceptible for humans. So we wonder, whether it is possible to craft adversarial examples in an opposite paradigm? Rather crafting adversarial examples by adding adversarial perturbation (or non-robust features) on clean images, we drop certain features from clean images that are imperceptible to humans but essential for DNNs which fur-ther lead to DNNs failing to recognize the resultant images.
Towards this end, we propose a novel adversarial attack named AdvDrop, which crafts adversarial images by drop-ping less perceptible details from clean images. For exam-ple, as shown in Figure 1, both adversarial images gener-ated by PGD [32] and AdvDrop look indistinguishable from the clean images at first glance. However, when you look closely, PGD generates extra details (composed of more colors) at the cost of extra storage (larger image size). In contrast, the proposed AdvDrop drops existing details such as subtle texture-like information from clean images, and the local patch is composed of less colors compared with the other images. As the figure indicates, the lost brittle de-1
Figure 2: Interpolation between the clean image and adversarial images generated by AdvDrop and PGD. tails from benign images result in DNNs failing to recognize the resultant images correctly.
Dropping information of images can be achieved in ei-ther spatial domain (e.g. color quantization [25, 34]) or fre-quency domain (e.g. JPEG compression [44]). In our work, we consider developing proposed AdvDrop in frequency domain. Principally, we can drop various features of an image to generate adversaries. This preliminary study by us focuses on the frequency domain because we choose to use the “image details” as the feature of interest to be dropped, which can be well quantized in the frequency domain. This choice is due to native insensitivity of hu-In this work, AdvDrop man eyes to fine image details. first transforms images from spatial domain to frequency domain, then reduces some frequency components of the transformed images quantitatively. Figure 2 shows the pro-cess of AdvDrop, which performs attack following an op-posite mechanism to PGD. The proposed AdvDrop starts from removing subtle details (e.g. textures) and the resultant image is almost indistinguishable from the clean one. When increasing the amount of dropped information, the resul-tant adversarial image finally turns to be somewhat “blank”.
Here,“blank” denotes pure color which presents (almost) no information for recognizing a specific object for DNNs.
We then perform comprehensive evaluation on the pro-posed AdvDrop. It can achieve high attack success rates in both targeted and untargeted settings on ImageNet [12].
We also evaluate the effectiveness of AdvDrop in terms of defense methods. Various defense methods have been pro-posed to defend against adversarial examples [32, 4, 35, 46].
Current defense methods are less effective against adver-sarial examples generated by AdvDrop as they are gen-erated with a rather different paradigm. Moreover, since the adversaries are generated by AdvDrop via losing infor-mation, they are somewhat robust to denoising-based de-fenses. Typically, the denoising-based method removes the generated adversarial perturbation and accordingly defends against adversaries (Figure 3). For adversaries generated by
AdvDrop, however, denoising-based defenses take no ef-fect and the resultant images are still adversarial for DNNs.
We hope this finding will motivate devising more effective defense approaches against AdvDrop. In addition, to better understand the mechanism of AdvDrop and the properties of generated adversarial examples by AdvDrop, we provide
Figure 3: Adversarial images under denoising-based de-fense. Adversarial perturbation generated by PGD could be mitigated by applying denoising strategies, but with almost no effect on adversaries generated by AdvDrop. visualizations of the dropped information by AdvDrop, and perform a further analysis together with the attention of the
DNNs. In summary, this paper has made the following con-tributions:
• We propose a novel adversarial attack named
AdvDrop, which is a totally different paradigm from previous attacks. AdvDrop crafts adversarial images by dropping existing details of clean images. It opens new doors to generate adversarial attacks for DNNs.
• We conduct comprehensive experiments and demon-strate the effectiveness of AdvDrop on targeted and untargeted attack settings. We also empirically show that current defense methods become less effective against adversarial examples generated by AdvDrop compared to other attacks.
• Finally, we visualize the dropped information and the attention of the DNNs to interpret the adversaries gen-erated by AdvDrop.
This paper is organized as follows.