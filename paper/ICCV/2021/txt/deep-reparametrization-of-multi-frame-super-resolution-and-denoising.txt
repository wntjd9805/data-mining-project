Abstract
We propose a deep reparametrization of the maximum a posteriori formulation commonly employed in multi-frame image restoration tasks. Our approach is derived by intro-ducing a learned error metric and a latent representation of the target image, which transforms the MAP objective to a deep feature space. The deep reparametrization allows us to directly model the image formation process in the la-tent space, and to integrate learned image priors into the prediction. Our approach thereby leverages the advantages of deep learning, while also beneﬁting from the principled multi-frame fusion provided by the classical MAP formu-lation. We validate our approach through comprehensive experiments on burst denoising and burst super-resolution datasets. Our approach sets a new state-of-the-art for both tasks, demonstrating the generality and effectiveness of the proposed formulation. 1.

Introduction
Multi-frame image restoration (MFIR) is a fundamental computer vision problem with a wide range of important ap-plications, including burst photography [2, 24, 39, 63] and remote sensing [12, 32, 46]. Given multiple degraded and noisy images of a scene, MFIR aims to reconstruct a clean, sharp, and often higher-resolution output image. By effec-tively leveraging the information contained in different in-put images, MFIR approaches are able to reconstruct richer details that cannot be recovered from a single image.
As a widely embraced paradigm [15, 17, 36, 47], MFIR is addressed by ﬁrst modelling the image formation process as, xi = H(cid:0)φmi(y)(cid:1) + ηi. In this model, the original image y is affected by the scene motion φmi, image degradation
H, and noise ηi, resulting in the observed image xi. As-suming the noise ηi follows an i.i.d. Gaussian distribution, the original image y is reconstructed from the set of noisy observations {xi}N 1 by ﬁnding the maximum a posteriori (MAP) estimate,
ˆy = arg min y
N (cid:88) i=1 (cid:13) (cid:13)xi − H (φmi(y)) (cid:13) 2 2 + R(y) , (cid:13) (1) where R(y) is the imposed prior regularization.
While the MAP formulation (1) has enjoyed much pop-ularity, there are several challenges when employing it in real-world settings. The formulation (1) assumes that the degradation operator H is known, which is not often the case. Moreover, it requires manually tuning the regular-izer R(y) for good performance [17, 19, 21]. Despite these shortcomings, the MAP formulation (1) provides an elegant modelling of the MFIR problem, and a principled way of fusing information from multiple frames. This inspires us to formulate a deep MFIR method that leverages the com-pelling advantages of (1), while also beneﬁting from the end-to-end learning of the degradation operator H and the regularizer R.
We propose a deep reparametrization of the classical
MAP objective (1). Our approach is derived as a gener-alisation of the image space reconstruction problem (1), by transforming the MAP objective to a deep feature space.
This is achieved by ﬁrst introducing an encoder network that replaces the L2 norm in (1) with a learnable error met-ric, providing greater ﬂexibility. We then reparametrize the target image y with a decoder network, allowing us to solve the optimization problem in a learned latent space. The de-coder integrates strong learned image priors into the predic-tion, effectively removing the need of a manually designed regularizer R. Our deep reparametrization also allows us to directly learn the effects of complex degradation operator
H in the deep latent space of our formulation. To further improve the robustness of our model to e.g. varying noise levels and alignment errors, we introduce a network compo-nent that estimates the certainty weights of all observations in the objective.
We validate the proposed approach through extensive experiments on two multi-frame image restoration tasks, namely RAW burst super-resolution, and burst denoising.
Our approach sets a new state-of-the-art on both tasks by outperforming recent deep learning based approaches (see
Fig. 1). We further perform extensive ablative experiments, carefully analysing the impact of each of our contributions. 2.