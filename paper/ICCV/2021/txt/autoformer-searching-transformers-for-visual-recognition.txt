Abstract
Recently, pure transformer-based models have shown great potentials for vision tasks such as image classiﬁca-tion and detection. However, the design of transformer net-works is challenging. It has been observed that the depth, embedding dimension, and number of heads can largely af-fect the performance of vision transformers. Previous mod-els conﬁgure these dimensions based upon manual craft-ing. In this work, we propose a new one-shot architecture search framework, namely AutoFormer, dedicated to vision transformer search. AutoFormer entangles the weights of different blocks in the same layers during supernet train-ing. Beneﬁting from the strategy, the trained supernet al-lows thousands of subnets to be very well-trained. Specif-ically, the performance of these subnets with weights in-herited from the supernet is comparable to those retrained from scratch. Besides, the searched models, which we re-fer to AutoFormers, surpass the recent state-of-the-arts such as ViT and DeiT. In particular, AutoFormer-tiny/small/base achieve 74.7%/81.7%/82.4% top-1 accuracy on ImageNet with 5.7M/22.9M/53.7M parameters, respectively. Lastly, we verify the transferability of AutoFormer by providing the performance on downstream benchmarks and distil-lation experiments. Code and models are available at https://github.com/microsoft/Cream. 1.

Introduction
Vision transformer recently has drawn signiﬁcant atten-tion in computer vision because of its high model capability and superior potentials in capturing long-range dependen-cies. Building on top of transformers [52], modern state-of-the-art models, such as ViT [13] and DeiT [50], are able to learn powerful visual representations from images and achieve very competitive performance compared to previ-ous convolutional neural network models [17, 25].
However, the design of transformer neural architectures is nontrivial. For example, how to choose the best network
∗Equal contributions. Work performed when Minghao is an intern of
MSRA. † Corresponding author: houwen.peng@microsoft.com.
Figure 1. The comparison between AutoFormers and transformer-based, convolution-based and architecture-searched models, such as DeiT [50], and ResNet [18]. depth, embedding dimension and/or head number? These factors are all critical for elevating model capacity, yet ﬁnd-ing a good combination of them is difﬁcult. As seen in
Fig. 2, increasing the depth, head number and MLP ratio (the ratio of hidden dimension to the embedding dimension in the multi-layer perceptron) of transformers helps achieve higher accuracy at ﬁrst but get overﬁt after hitting the peak value. Scaling up the embedding dimension can improve model capability, but the accuracy gain plateaus for larger models. These phenomenons demonstrate the challenge of designing optimal transformer architectures.
Previous works on designing vision transformers are based upon manual crafting, which heavily relies on hu-man expertise and typically requires a deal of trial-and-error [13, 50, 67]. There are a few works on automat-ing transformer design using neural architecture search (NAS) [45, 55]. However, they are all concentrated on nat-ural language tasks, such as machine translation, which are quite different from computer vision tasks. As a result, it is hard to generalize prior automatic search algorithms to ﬁnd effective vision transformer architectures.
In this work, we present a new architecture search algo-rithm, named AutoFormer, dedicated to ﬁnding pure vision transformer models. Our approach mainly addresses two challenges in transformer search. 1) How to strike a good
Figure 2. Adjust a baseline model with different embedding dimension (e), depth (d), MLP ratio (r), and number of heads (h) coefﬁcients under the same training recipe, where MLP ratio( the ratio of hidden dimension to the embedding dimension in the multi-layer perceptron).
We set the baseline model with d = 12, r = 4, e = 384, h = 6. Note: number of heads does not affect the model size and complexity if we ﬁx the Q-K-V dimension. combination of the key factors in transformers, such as net-work depth, embedding dimension and head number? 2)
How to efﬁciently ﬁnd out various transformer models that
ﬁt different resource constraints and application scenarios?
To tackle the challenges, we construct a large search space covering the main changeable dimensions of trans-former, including embedding dimension, number of heads, query/key/value dimension, MLP ratio, and network depth.
This space contains a vast number of transformers with di-verse structures and model complexities. In particular, it al-lows the construction of transformers to use different struc-tures of building blocks, thus breaking the convention that all blocks share an identical structure in transformer design.
To address the efﬁciency issue, inspired by BigNAS [65] and slimmable networks [65, 66], we propose a supernet training strategy called weight entanglement dedicated to transformer architecture. The central idea is to enable dif-ferent transformer blocks to share weights for their common parts in each layer. An update of weights in one block will affect all other ones as a whole, such that the weights of different blocks are maximally entangled during training.
This strategy is different from most one-shot NAS methods
[16, 8, 59], in which the weights of different blocks are in-dependent for the same layer, as visualized in Fig. 5.
We observe a surprising phenomenon when using the proposed weight entanglement for transformer supernet training: it allows a large number of subnets in the super-net to be very well-trained, such that the performance of these subnets with weights inherited from the supernet are comparable to those retrained from scratch. This advan-tage allows our method to obtain thousands of architectures that can meet different resource constraints while maintain-ing the same level of accuracy as training from scratch in-dependently. We give a detailed discussion in Section 3.4 exploring the underlying reasons of weight entanglement.
We perform a evolutionary search with a model size con-straint over the well-trained supernets to ﬁnd promising transformers. Experiments on ImageNet [11] demonstrate that our method achieves superior performance to the hand-crafted state-of-the-art transformer models. For instance, as shown in Fig. 1, with 22.9M parameters, Autoformer achieves a top-1 accuracy of 81.7%, being 1.8% and 2.9% better than DeiT-S [50] and ViT-S/16 [13], respectively. In addition, when transferred to downstream vision classiﬁ-cation datasets, our AutoFormer also performs well with fewer parameters, achieving better or comparable results to the best convolutional models, such as EfﬁcientNet [49].
In summary, we make three major contributions in this paper. 1) To our best knowledge, this work is the ﬁrst ef-fort to design an automatic search algorithm for ﬁnding vi-sion transformer models. 2) We propose a simple yet ef-fective framework for efﬁcient training of transformer su-pernets. Without extra ﬁnetuning or retraining, the trained supernet is able to produce thousands of high quality trans-formers by inheriting weights from it directly. Such merit allows our method to search diverse models to ﬁt different resource constraints. 3) Our searched models, i.e., Auto-Formers, achieve the state-of-the-art results on ImageNet among the vision transformers, and demonstrate promising transferability on downstream tasks. 2.