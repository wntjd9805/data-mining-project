Abstract
Video highlight detection plays an increasingly impor-tant role in social media content filtering, however, it re-mains highly challenging to develop automated video high-light detection methods because of the lack of temporal an-notations (i.e., where the highlight moments are in long videos) for supervised learning. In this paper, we propose a novel weakly supervised method that can learn to de-tect highlights by mining video characteristics with video level annotations (topic tags) only. Particularly, we ex-ploit audio-visual features to enhance video representation and take temporal cues into account for improving detec-tion performance. Our contributions are threefold: 1) we propose an audio-visual tensor fusion mechanism that effi-ciently models the complex association between two modal-ities while reducing the gap of the heterogeneity between the two modalities; 2) we introduce a novel hierarchical temporal context encoder to embed local temporal clues in between neighboring segments; 3) finally, we alleviate the gradient vanishing problem theoretically during model op-timization with attention-gated instance aggregation. Ex-tensive experiments on two benchmark datasets (YouTube
Highlights and TVSum) have demonstrated our method out-performs other state-of-the-art methods with remarkable improvements. 1.

Introduction
Recently, the rise of short-form video sharing applica-tions (e.g., TikTok and Reels) has attracted world-wide at-tention on the Internet. From a content producer’s point of view, it is not a delightful experience for them to trim long videos and localize those highlight segments manu-ally. Therefore, an automated method is needed desperately to identify highlight clips from untrimmed videos.
Video highlight detection has received many interests in the field of computer vision. Numerous methods [24, 31, 28] have been proposed to automatically tailor highlight from untrimmed videos tagged with a specific topic or key-word, which can be generally divided into two categories,
*Equal contribution.
†Corresponding author: yuan.gao2@gmail.com
Figure 1. Temporal relationship reasoning for highlight detection.
Part (A) contains different surfing tutorial sections with little tem-poral cues to infer highlight. However, in part (B), clips in the box with dashed line contain take off and wipe out footages, which indicate the occurrence of surfing, and they encoded contextual features among these clips that can be used to infer the highlight. i.e., supervised learning and weakly supervised learning based methods. Supervised methods [22, 9, 11] generally trained a ranker detector with frame-level annotations to rank the highlight segments, of which scores were higher than those of non-highlight segments. However, annotat-ing the frame-wise highlight manually is extremely labor-intensive and time-consuming. To overcome this problem, weakly supervised methods [20, 3, 30] use weakly labeled videos to train the model. For methods, videos are usu-ally divided into two categories based on whether there are topic-specific segments presented: a positive video contains at least one highlighted snippet and a negative one should have no highlights. The trained highlight detector needs to learn from those topic-specific video snippets to iden-tify highlights for unseen videos. However, these weakly supervised methods have limited capacity in terms of: (1) effectively capturing the complex interactions between au-dio and vision streams in videos while maintaining the effi-ciency; (2) enhancing the semantic continuity modeling by leveraging the so-far unexplored temporal evolution among video segments.
For the former bottleneck, previous approaches [12, 29] tended to adopt linear fusion schemes (e.g., concatenation
or summation between two modalities) to convey video rep-resentation. Nonetheless, such linear fusion methods were not able to fully capture the complex association between the two modalities due to the distinct feature distribution of each modality since audio does not always correspond to the visual frames. Subsequently, bilinear pooling [8] was designed to fuse two kinds of features through modeling the pairwise interaction. However, one drawback is that it requires a great number of parameters to train leading inef-ficient training for video highlight detection, which is noto-riously resource-heavy while facing the risk of overfitting.
As a high-rank tensor can be decomposed into several ma-trices and a core tensor [16], we introduce an audio-visual tensor fusion scheme and apply the low-rank constraint on the core tensor which can not only provide rich video repre-sentation by modeling the modalities interaction efficiently, but can also reduce the number of trainable parameters of the model which serves as the regularization.
In another aspect, most of existing methods [30, 12] tend to handle video segments individually while temporal evo-lution across consecutive segments is not adequately ex-ploited. As depicted in Figure 1, due to the temporal char-acteristics of the video, those non-highlight segments may provide fruitful clues for inferring the highlight and fore-shadow the occurrence of the highlight. For further illus-tration, the beginning of the highlight event or the ending would indicate the happening of the highlight segment, e.g., taking off and wiping out footages suggest the occurrence of surfing (demonstrated in Figure 1 (B)). Inspired by this, we propose a hierarchical temporal context encoding scheme, which exploits the temporal context relationships among lo-cal adjacent segments via utilizing the temporal cues for the first time. In particular, a video segment encodes the con-textual information from its neighbor segments in a hierar-chical paradigm, which is beneficial for higher-order con-tent interaction among segments. Therefore, the temporal contextual feature includes the representation of the original segment, and also models local dependency among adjacent segments.
In conclusion, we propose a low-rank audio-visual tensor fusion mechanism and hierarchical temporal context encoding scheme to address the above limitations, which we believe are important signs of progress for weakly supervised video highlight detection.
Finally, considering that only video-level annotations are available, correctly classifying videos can provide useful inductive bias for topic-specific highlight detection since a video may contain highlights of various events. Therefore, we exploit this advantage by introducing a novel attention-gated instance aggregation module, which derives represen-tative video score from individual segment scores. More importantly, the gradient vanishing issue occurs constantly when the score of a segment in the video is high, and tra-ditional methods [2, 4] do not have theoretic insight for the solution. By contrast, our theoretical analysis has proved that the proposed instance aggregation module can effec-tively alleviate this problem.
The main contributions of our study can be highlighted as follows:
• We develop a low-rank audio-visual tensor fusion mechanism to capture the complex association be-tween two modalities, which can efficiently generate informative audio-visual fused features.
• We propose a novel hierarchical scheme to encode temporal contextual features among video snippets with temporal cues for the first time in the video high-light detection. Experimental results demonstrate that our model outperforms competing approaches by a significant margin.
• We introduce an attention-gated instance aggregation module to formulate the video score and exploit induc-tive bias for topic-specific highlight detection. More-over, theoretical analysis shows that it can ease the gradient vanishing problem during optimization effec-tively. 2.