Abstract
Physical-world adversarial attacks based on universal adversarial patches have been proved to be able to mis-lead deep convolutional neural networks (CNNs), expos-ing the vulnerability of real-world visual classification sys-tems based on CNNs. In this paper, we empirically reveal and mathematically explain that the universal adversarial patches usually lead to deep feature vectors with very large norms in popular CNNs.
Inspired by this, we propose a simple yet effective defending approach using a new feature norm clipping (FNC) layer which is a differentiable module that can be flexibly inserted in different CNNs to adaptively suppress the generation of large norm deep feature vectors.
FNC introduces no trainable parameter and only very low computational overhead. However, experiments on multiple datasets validate that it can effectively improve the robust-ness of different CNNs towards white-box universal patch attacks while maintaining a satisfactory recognition accu-racy for clean samples. 1.

Introduction
Deep convolutional neural networks (CNNs) have achieved remarkable success on various computer vision tasks. However, researches have shown that most CNNs are vulnerable to adversarial attacks [14, 19], where mali-ciously crafted perturbations are added to the input image to fool the network. The existence of adversarial examples under different constraints has become a serious concern to visual systems and applications based on CNNs espe-cially in safety-critical domains. Compared to the conven-tional adversarial attacks which add perturbation with input norm constraints directly to the whole digital image, physi-cal world attack is relatively more challenging. In the phys-ical world, it is hard and expensive to accurately manip-ulate pixels that may scatter all around the image. A more promising way is to generate a spatially localized patch-like
*Corresponding author.
Figure 1. Illustration of the proposed method based on feature norm clipping (FNC) layers. perturbation in which the pixel values can be arbitrarily se-lected. Usually, such a patch is generated so that it is ef-fective no matter what the image is and where it is placed in the image to achieve real-world attacking robustness.
Such an approach is called the universal adversarial patch attack [2], which has remained as the most effective and widely adopted way to attack real-world computer vision systems built upon CNNs including image classifier [2, 8], object detector [4, 24, 7] and face recognizer [17].
Despite its clear threat to real-world applications, re-search on defending against the universal adversarial patch attack is still limited. Some previous defending approaches, including Digital Watermarking (DW) [5] and Local Gra-dient Smoothing (LGS) [13], are based on patch detection following empirical clues. Lacking theoretical foundations, their performance usually drops dramatically when facing white-box or adaptive attacks in which the defending strat-egy is transparent to attackers [1]. By iteratively generat-ing adversarial patches in the training process, the adver-sarial training can also be used in defending against patch attacks [22]. However, such an approach requires extra training with extremely high computational overhead and there has been no work to show its feasibility for large-scale datasets like ImageNet [10] by far. Chiang et al. [3] pro-posed a certified defense against the adversarial patch when the output lies in the interval bound. However, this esti-mated bound gets looser with the increase of the network
depth, leading to the difficulty in its scaling up to commonly used deep networks like ResNet [6] and Inception [18].
In this paper, we propose an effective defending method against universal adversarial patch attacks based on a math-ematical analysis of how a universal adversarial patch im-pact deep feature representations. Generally, a universal ad-versarial patch only occupies a small image area. However, it can mislead the CNN to predict the same wrong target class no matter where it is placed and what the original im-age is. How is this achieved? Empirical observations re-veal that when an adversarial patched image is passing for-ward through a CNN, usually the norm of the feature vec-tor spatially located at the position of the adversarial patch is significantly larger than that of other feature vectors.
This makes sense considering that under the widely adopted global average pooling strategy, a large norm feature vector can dominate in deciding the direction of the pooling re-sult no matter what other feature vectors are. Such a phe-nomenon exists only when the adversarial perturbation is localized like adversarial patches. This can be explained by the fact that the effective receptive field (ERF) of a practi-cal CNN is exponentially centered and is obviously smaller than the theoretical receptive field [11]. As such, the impact of the adversarial patch on the feature map has to be spa-tially concentrated. This indicates that most feature vectors will not be essentially affected, leading to inevitable exis-tence of large norm feature vector at the patch position.
We present a mathematical explanation of the phe-nomenon, based on which a defending method is proposed.
Specifically, we propose to restrict the norm of deep fea-tures on different layers through the forward propagation by introducing Feature Norm Clipping (FNC) layers as illus-trated in Fig. 1. FNC is a differentiable layer and can be in-serted among the cascaded modules in CNNs, e.g. the resid-ual block in ResNet. FNC reduces the variance of feature norms by one-side clipping, so as to prevent the generation of feature vectors with extremely large norms. Analysis on
ERF demonstrates that the influence of the adversarial patch can be intrinsically weakened by FNC, leading to improved classification accuracy against adversarial patch attacks. We conduct extensive experiments on CIFAR10 [9] and Ima-geNet [10] with different CNN architectures and attacking methods. Significant improvements on robustness against white-box adversarial patch attacks are achieved. More-over, in contrast to previous defending methods [5, 13], our proposed approach is implemented in an end-to-end man-ner with very low computational overhead in both training and inference, leading to its high applicability to real-word visual classification systems based on CNNs. tion. 2) We propose a simple yet effective defending method using the FNC layer to weaken the effect of the adversar-ial patch by restricting the generation of large norm feature vectors. 3) We achieve improvements in adversarial accu-racy for different networks and datasets. Our method out-performs state-of-the-art patch defending methods. 2.