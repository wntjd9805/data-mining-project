Abstract
Mixed-precision networks allow for a variable bit-width quantization for every layer in the network. A major lim-itation of existing work is that the bit-width for each layer must be predefined during training time. This allows lit-tle flexibility if the characteristics of the device on which the network is deployed change during runtime.
In this work, we propose Bit-Mixer, the very first method to train a meta-quantized network where during test time any layer can change its bit-width without affecting at all the overall network’s ability for highly accurate inference. To this end, we make 2 key contributions: (a) Transitional Batch-Norms, and (b) a 3-stage optimization process which is shown ca-pable of training such a network. We show that our method can result in mixed precision networks that exhibit the desir-able flexibility properties for on-device deployment without compromising accuracy. Code will be made available. 1.

Introduction
Despite their unprecedented accuracy, directly deploy-ing Deep Neural Networks on devices with limited com-putational resources and/or power constraints remains pro-hibitive. To address this problem, a series of related re-search directions have emerged such as network prun-ing [29, 35, 25], network compression [24, 40, 27], neural architecture search [28, 6] and network quantization. The later offers the most straightforward improvements as using fewer bits for the weights and activations significantly re-duces the compute and storage requirements. For example, switching from FP32 to Int-8 precision, a 4× improvement in terms of speed and storage is obtained without any bells and whistles. This paper is on mixed-precision networks which allow for a variable bit-width quantization for every layer in the network.
Mixed-bit precision networks allow for a finer granular-ity of quantization at a layer level and, hence, offer prac-tical advantages in terms of finding a more optimal trade-off between efficiency (i.e. speed) and memory require-ments, and network accuracy. While this is more flexible than having the same bit-width across the whole network, mixed-bit precision approaches have also their own limi-tations. Firstly, due to an ever-growing number of differ-ent hardware platforms that a developer needs to support, each with its own unique characteristics and capabilities, quantizing networks, partially or fully, with mixed-bit pre-cision in order to obtain an optimal trade-off between accu-racy and speed becomes challenging. Secondly, and more importantly, even on the same device, due to either other concurrent processes running, battery level, temperature, or simply prioritization, the available resources can vary. Ide-ally, a network should be able to dynamically react to these changes and adapt its quantization level per layer or mod-ule on the fly without incurring undesirable, or even more importantly, unpredictable penalties on inference accuracy.
The method we propose in this paper, coined Bit-Mixer, attempts to provide an answer to the aforementioned chal-lenges. Bit-Mixer shifts away the focus from finding the optimal bit-with allocation per layer during training as done in all previous work. Instead, we propose to train a meta-quantized network that during test time can switch to any quantization level for any layer in the network. Train-ing such meta-networks is however non-trivial due to the ex-ponential number of unique combinations, the weight shar-ing constraint across different bit-widths, and the drastic variations in representational power that occur when the bit-width changes (e.g. 4 bits vs 1 bit). To this end, we make the following contributions: 1. Transitional Batch-Norms: To properly compensate for the distribution shift that arises when a change in the bit-width occurs between two consecutive layers, for each transition between different bit-widths, we propose to learn a separate batch normalization layer, coined Transitional Batch-Norm. 2. 3-stage Optimization: We firstly propose an efficient 2-stage process to train an intermediate meta-network which at runtime can select different bit-widths which however are shared across the entire network. Then, a 3-rd final stage is introduced to gradually transition
I1
I2
In
I1
I2
In
I1
I2
In
...
...
W1
...
W1
...
...
Wl
...
Wl
...
...
WL
...
WL
O1
O2
On
O1
O2
On
O1
O2
On (a) Independent: Each bit-width re-quires training a new network with inde-pendent weights. (b) Adabits: A single network can be quantized to any of n bit-widths at run-time. All layers inside the network share the same bit-width. (c) Proposed method (Bit-Mixer): A single network whose individual layers can be quantized at runtime to any bit-width, without any re-training.
Figure 1: Comparison between prior network quantization paradigms (a,b) and ours (c). Our method is the only one re-sulting in an exponential number of mixed precision networks that one can choose from to fit the device characteristics and computational resources available on-the-fly. from the intermediate meta-network to the final one where the quantization level can be randomly selected at a block or layer level. Notably, our meta-network uses a single, shared set of weights. 3. We conducted several ablation studies which shed light into the behaviour of several components of our method. Moreover, building on top of the findings of
[11], we analyze Bit-Mixer’s sub-nets exploring the inter-dependencies between the accuracy and the quan-tization level selected for a given layer. Finally, we ex-tensively evaluated the accuracy of the proposed Bit-Mixer across different architectures and model sizes. 2.