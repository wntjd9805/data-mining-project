Abstract
The goal of few-shot learning (FSL) is to recognize a set of novel classes with only few labeled samples by exploit-ing a large set of abundant base class samples. Adopting a meta-learning framework, most recent FSL methods meta-learn a deep feature embedding network, and during infer-ence classify novel class samples using nearest neighbor in the learned high-dimensional embedding space. This means that these methods are prone to the hubness problem, that is, a certain class prototype becomes the nearest neighbor of many test instances regardless which classes they belong to. However, this problem is largely ignored in existing FSL studies. In this work, for the first time we show that many
FSL methods indeed suffer from the hubness problem. To mitigate its negative effects, we further propose to employ z-score feature normalization, a simple yet effective trans-formation, during meta-training. A theoretical analysis is provided on why it helps. Extensive experiments are then conducted to show that with z-score normalization, the per-formance of many recent FSL methods can be boosted, re-sulting in new state-of-the-art on three benchmarks. 1.

Introduction
In recent years, the advances of deep convolutional neu-ral networks (CNNs) have had profound impacts on a vari-ety of vision areas, such as object recognition [45, 38, 13], semantic segmentation [26, 4], and even image generation
[33, 42]. To train an effective CNN model for visual recog-nition, a large number of manually labeled training samples are often required. However, obtaining sufficient training data is often expensive and sometimes even infeasible (e.g., for rare object categories). One solution to the data hun-gry nature of deep recognition models is few-shot learning (FSL) [20, 21], which aims to recognize a set of novel ob-*Corresponding author. ject classes with only few labeled samples by exploiting a set of base classes each containing ample samples.
Recent FSL methods typically follow the meta-learning framework and adopt episodic training [8, 47, 49, 2, 19, 9, 59, 7, 63]. That is, they train their models over a large num-ber of meta-tasks/episodes sampled from the abundant base class images. This is to imitate the few-shot classification tasks for the novel classes. Specifically, each episode is con-structed by sampling N base/novel classes with K labeled samples in each class as the support set and a set of query images to be classified. Existing meta-learning methods dif-fer in which part of the recognition model, comprising a fea-ture embedding network and a classifier, is meta-learned. It is noted that most recent FSL methods [47, 2, 59, 63] focus on meta learning the embedding network. Once the model learned, during inference, the support set samples are used to construct class prototypes in that embedding space, and the classification of query samples is done by the simple nearest neighbor (NN) search.
Using NN in a high dimensional embedding space makes these FSL methods prone to the hubness problem [34, 43, 50]. Specifically, in a high dimensional space, nearest neighbor suffers from the existence of hubs, i.e., the class prototypes which are the nearest neighbors of many test samples, regardless which classes they belong to. These hubs thus clearly harm the recognition performance. To il-lustrate the hubness problem, let us take a concrete example.
Denote k-occurrence N (h) k (x) as the number of times that a sample x occurs among the k nearest neighbors of all other points in a dataset. We visualize the distribution of N (h) on the test set of miniImageNet [52] in Figure 1(a), where a four-block CNN Conv4-64 pre-trained on the training set is used. We also calculate the hubness measure skewness
SN (h) of the distribution (see the detailed definition in Sec-tion 4.3). It can be observed that the distribution is heavily skewed to the right, i.e., a large number of samples have low
N (h) values while a small group of samples are frequently 5 5 5
(a) Before ZN (b) After ZN (c) Before ZN (d) After ZN
Figure 1. Visualizations on the test set of miniImageNet with a total of 12,000 samples using Conv4-64 pre-trained on the training set. (a) – (b) Visualizations of the distributions of N (h) using the original features and the z-score normalized (ZN) features, respectively.
S denotes the skewness of a distribution, whose absolute value is larger when the distribution is more skewed. (c) – (d) Visualizations 5 of the cosine similarities among 20 class centers using the original features and the z-score normalized features, respectively. µ(cos(x, ¯x)) and σ(cos(x, ¯x)) are respectively the mean and standard deviation of the cosine similarity between a sample and the dataset mean.
N (h) 5 visited. The same observation holds when the pre-trained embedding network is meta-learned using recent FSL meth-ods [47, 2, 63]. This provides direct evidence that the hub-ness problem indeed exists in FSL. However, as far as we know, this problem has been largely ignored.
In order to remedy the problem, we must first identify the potential causes for it. It is discovered that one cause for hubness is actually the widely used batch normaliza-tion (BN) [15] and non-negative activation functions (e.g.,
ReLU) in the deep embedding CNNs. In particular, we find that with BN and ReLU, the output feature vectors with non-negative elements often have similar directions in the feature space. To show this, in Figure 1(c), we visualize the cosine similarities among 20 class centers (i.e., feature mean of samples belonging to the same classes) also on the test set of miniImageNet using the pre-trained Conv4-64.
Besides, we calculate the cosine similarities of all samples to the dataset mean (i.e., the mean of all feature vectors) and obtain the statistical mean and standard deviation. We can clearly see from Figure 1(c) that these feature vectors are very much alike in terms of the direction, meaning sam-ples of different classes can form clusters. This problem is supposed to be rectified by the subsequent classification layer. However, with NN search in metric-based FSL and no classification layer for the rescue, it must be addressed.
Our solution to the hubness problem in FSL is thus on deploying alternative normalization strategies during pre-training or episodic training. Particularly, we discover that z-score normalization (ZN), a simple transforming opera-tion at the feature level, can offer an effective solution.
More concretely, with ZN, for each feature vector extracted by the embedding network, every component of it first sub-tracts the mean of all components and then is divided by the standard deviation of all components. Note that ZN is applied to each feature vector independently during both training and inference, and thus the inductive FSL setting 5 is still strictly followed in this paper. We visualize the dis-tribution of N (h) after applying ZN in Figure 1(b) and also the heat map of cosine similarities calculated with normal-ized features in Figure 1(d). From Figure 1(b), we can see that the distribution of the 5-occurrence is pulled back to the left and the value of skewness is much smaller. From
Figure 1(d), we can also observe that the samples of dif-ferent classes in the normalized embedding space are more separable. We show that this simple operation works during both pre-training and episodic training (see Table 1).
Our main contributions are three-fold: (1) To the best of our knowledge, we are the first to bring to light the hubness problem in the context of FSL. (2) We propose to alleviate the negative effects of the hubness problem in FSL by em-ploying the z-score feature normalization. We also provide theoretical analysis on why it works. (3) Comprehensive ex-periments are carried out to demonstrate that the simple ZN operation can boost a variety of embedding/metric-based
FSL methods which dominated the state-of-the-art lately.
The code and models will be released soon. 2.