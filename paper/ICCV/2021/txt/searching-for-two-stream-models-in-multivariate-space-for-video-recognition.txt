Abstract
Conventional video models rely on a single stream to capture the complex spatial-temporal features. Recent work on two-stream video models, such as SlowFast network and
AssembleNet, prescribe separate streams to learn comple-mentary features, and achieve stronger performance. How-ever, manually designing both streams as well as the in-between fusion blocks is a daunting task, requiring to ex-plore a tremendously large design space. Such manual ex-ploration is time-consuming and often ends up with sub-optimal architectures when computational resources are limited and the exploration is insufficient. In this work, we present a pragmatic neural architecture search approach, which is able to search for two-stream video models in giant spaces efficiently. We design a multivariate search space, including 6 search variables to capture a wide variety of choices in designing two-stream models. Furthermore, we propose a progressive search procedure, by searching for the architecture of individual streams, fusion blocks and attention blocks one after the other. We demonstrate two-stream models with significantly better performance can be automatically discovered in our design space. Our searched two-stream models, namely Auto-TSNet, consistently out-perform other models on standard benchmarks. On Kinet-ics, compared with the SlowFast model, our Auto-TSNet-L model reduces FLOPS by nearly 11× while achieving the same accuracy 78.9%. On Something-Something-V2, Auto-TSNet-M improves the accuracy by at least 2% over other methods which use less than 50 GFLOPS per video. 1.

Introduction
Video recognition requires to learn both spatial and tem-poral features, which is arguably more challenging than image recognition. Many efforts have been made to ex-tend single-stream image architectures for video recogni-tion, such as C3D [32], I3D [2], S3D [45], R(2+1)D [34],
TSN [37], and TSM [16]. However, such single-stream models often underperform two-stream models where each
*Work done during an internship at Facebook AI.
†Correspondence to Zhicheng Yan <zyan3@fb.com>.
Figure 1: Results on Kinetics-400. Comparing the FLOPs and accuracy with state-of-the-art models, our Auto-TSNet models achieve better accuracy-to-complexity trade-off.
For a fair comparison, we report the FLOPs for each video at inference time, taking into account the different number of views used by each method. stream takes a separate input and learns spatial-temporal representations that are complementary to each other [29, 9, 6]. In the pioneering two-stream ConvNet [29], a separate temporal stream is added which takes multi-frame optical flow as input to better learn temporal information. Recently,
SlowFast network [9] adds a fast pathway, which operates at a high frame rate, and captures temporal information at a finer granularity.
Compared with the single-stream model, the number of design choices grows exponentially for the two-stream models as we need to take into account the additional com-plexity from the second stream, the feature fusion between the streams. Prior hand-crafted two-stream models mit-igate such challenges by largely reusing existing single-stream architectures, and only explore a limited number of customized design choices for each stream. For example, in two-stream ConvNet [29], the second temporal stream shares the same architecture as the spatial stream, which doubles the overall computational cost of the model.
In the SlowFast network [9], the fast pathway only differs from the slow pathway by using 1D temporal convolutions and uniformly reducing the feature channels to balance the accuracy-to-complexity (ATC) trade-off. We hypothesize
that the existing two-stream models are sub-optimal, and pose the following question: Can we more thoroughly ex-plore the design space of two-stream video architectures and discover models with better performance?
In this work, we present a pragmatic neural architec-ture search (NAS) approach, which can effectively explore large design spaces and discover high-performance two-stream models automatically. Unlike hand-crafted two-stream models where streams often use similar architec-tures, we encourage distinctive architectures at each stream, and jointly search both streams to learn complementary in-formation. The core of our approach is a carefully pre-scribed multi-variate search space, which contains 6 search variables, including inter-stream fusion blocks, attention blocks, temporal/spatial kernel sizes, output channels, and expansion rates of building blocks. All of them have a sub-stantial impact on both the accuracy and complexity of the learned model. Together they represent a wide variety of design choices for two-stream models.
It is computationally challenging to explore such gigan-tic search spaces efficiently. We propose a multi-step pro-gressive procedure to decompose the large search space by only searching a smaller number of design choices at a time. For the basic search procedure, we adopt PAR-SEC [3] which is more memory efficient than other differ-ential NAS methods by avoiding instantiating all choices of the search variables simultaneously, and only sampling one architecture at a time. Unlike the process of manually de-signing architectures which often favors uniform choices of search variables, searching in our space leads to the discov-ery of the Auto-TSNet models, which select more nonuni-form choices for different components of the models. With extensive experiments, we demonstrate Auto-TSNet models substantially outperform others with more uniform choices on Kinetics-400 [13], shown in Figure 1, and Something-Something V2 [11] dataset.
Our main contributions are summarized below.
• We prescribe a multi-variate search space to accommo-date the large variations in designing two-stream video models, by including 6 different search variables, each of which has a significant impact on the model accuracy and complexity.
• We decompose the search of two-stream models into mul-tiple steps, and sequentially search different parts of the model, which renders the exploration in such a large space more efficient.
• The discovered Auto-TSNet models are distinct from the hand-crafted ones by selecting more nonuniform choices for different components. We evaluate them on two large action recognition benchmarks, and confirm their supe-rior performance over other models. 2.