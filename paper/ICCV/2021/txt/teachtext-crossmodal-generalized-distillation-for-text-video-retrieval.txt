Abstract
In recent years, considerable progress on the task of text-video retrieval has been achieved by leveraging large-scale pretraining on visual and audio datasets to construct powerful video encoders. By contrast, despite the natural symmetry, the design of effective algorithms for exploiting large-scale language pretraining remains under-explored.
In this work, we are the first to investigate the design of such algorithms and propose a novel generalized distilla-tion method, TEACHTEXT, which leverages complementary cues from multiple text encoders to provide an enhanced supervisory signal to the retrieval model. Moreover, we extend our method to video side modalities and show that we can effectively reduce the number of used modalities at test time without compromising performance. Our ap-proach advances the state of the art on several video re-trieval benchmarks by a significant margin and adds no computational overhead at test time. Last but not least, we show an effective application of our method for eliminat-ing noise from retrieval datasets. Code and data can be found at https://www.robots.ox.ac.uk/˜vgg/ research/teachtext/. 1.

Introduction
The focus of this work is text-video retrieval—the task of identifying which video among a pool of candidates best matches a natural language query describing its content.
Video search has a broad range of applications across do-mains such as wildlife monitoring, security, industrial pro-cess monitoring and entertainment. Moreover, as human-ity continues to produce video at ever-increasing scale, the ability to perform such searches effectively and efficiently takes on critical commercial significance to video hosting platforms such as YouTube.
A central theme of recently proposed retrieval methods has been the investigation of how to best use multiple video
*Equal contribution. †Corresponding authors.
Figure 1. Distilling the knowledge from multiple text encoders for stronger text-video retrieval. Prior works [18, 28, 32] have shown the considerable benefit of transitioning from video en-coders that ingest a single modality (left) to multi-modal video en-coders (centre). In this work, we show that retrieval performance can be further significantly enhanced by learning from multiple text encoders through the TEACHTEXT algorithm which imposes no additional cost during inference. Text-to-video retrieval perfor-mance gain (geometric mean of R1-R5-R10) is reported for a [28] model as well as for our method on the MSR-VTT [55] dataset. modalities to improve performance.
In particular, archi-tectures based on mixtures-of-experts [28, 32] and multi-modal transformers [18] have shown the benefit of making use of diverse sets of pre-trained models for related tasks (such as image classification, action recognition and ambi-ent sound classification) as a basis for video encoding dur-ing training and testing.
In this work, we explore whether commensurate gains could be achieved by leveraging multiple text embeddings learned on large-scale written corpora. Different from video embeddings using multiple modalities and pretrain-ing tasks, it is less obvious that there is sufficient diversity among collections of text embeddings to achieve a mean-ingful boost in performance. In fact, our inspiration stems from a careful investigation of the performance of differ-ent text embeddings across a range of retrieval benchmarks (Fig. 2). Strikingly, we observe not only that there is consid-erable variance in performance across text embeddings, but also that their ranking is not consistent, strongly supporting the idea of using multiple text embeddings.
Motivated by this finding, we propose a simple algo-rithm, TEACHTEXT, to effectively exploit the knowledge
captured by collections of text embeddings. Our approach requires a “student” model to learn from a single or multi-ple “teacher” retrieval models with access to different text embeddings by distilling their text-video similarity matrices into an enhanced supervisory signal. As shown in Fig. 1,
TEACHTEXT is capable of delivering a significant perfor-mance gain. Moreover, this gain is complementary to that of adding more video modalities to the video encoder but importantly, unlike the addition of video modalities, does not incur additional computational cost during inference.
Our main contributions can be summarised as follows: (1) We propose the TEACHTEXT algorithm, which lever-ages the additional information given by the use of multiple text encoders; (2) We show that directly learning the re-trieval similarity matrix between the joint query video em-beddings, which to the best of our knowledge is novel, is an effective generalized distillation technique for this task (and we compare our approach to alternatives among prior work such as uni-modal relationship distillation [37]); (3) We show an application of our approach in eliminating noise from modern training datasets for the text-video retrieval task; (4) We demonstrate the effectiveness of our approach empirically, achieving state of the art performance on six text-video retrieval benchmarks. 2.