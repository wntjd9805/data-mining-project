Abstract
Automatic augmentation methods have recently become a crucial pillar for strong model performance in vision tasks. While existing automatic augmentation methods need to trade off simplicity, cost and performance, we present a most simple baseline, TrivialAugment, that out-performs previous methods for almost free. TrivialAugment is parameter-free and only applies a single augmentation to each image. Thus, TrivialAugment's effectiveness is very unexpected to us and we performed very thorough exper-iments to study its performance. First, we compare Triv-ialAugment to previous state-of-the-art methods in a variety of image classification scenarios. Then, we perform mul-tiple ablation studies with different augmentation spaces, augmentation methods and setups to understand the crucial requirements for its performance. Additionally, we provide a simple interface to facilitate the widespread adoption of automatic augmentation methods, as well as our full code base for reproducibility1. Since our work reveals a stag-nation in many parts of automatic augmentation research, we end with a short proposal of best practices for sustained future progress in automatic augmentation methods.
Input image
Sample strength
Sample augmentation and apply it
Figure 1: A visualization of TA. For each image, TA (uni-formly) samples an augmentation strength and an augmen-tation. This augmentation is then applied to the image with the sampled strength.
Method
AA
RA
Fast AA
TA (ours)
Search
CIFAR-10 CIFAR-100 SVHN ImageNet
Overhead ShakeShake WRN WRN ResNet 40 - 800× 4 - 80× 1× 0× 77.6 77.6 77.6 78.1 98.0 98.0 98.0 98.2 82.9 83.3 82.7 84.3 98.9 99.0 98.8 98.9
Table 1: TrivialAugment compares very favourably to previous augmentation methods.
In this table we sum-marize some results from Table 2 and present augmentation search overhead estimates. 1.

Introduction
Data Augmentation is a very popular approach to in-crease generalization of machine learning models by gen-erating additional data.
It is applied in many areas, such as machine translation [4], object detection [6] or semi-supervised learning [20].
In this work, we focus on the application of data augmentation to image classification
[3, 12].
Image augmentations for image classification generate novel images based on images in a dataset, which are likely to still belong to the same classification category. This way the dataset can grow based on the biases that come with the augmentations. While data augmentations can yield consid-erable performance improvements, they do require domain knowledge. An example of an augmentation, with a likely class-preserving behaviour, is the rotation of an image by some small number of degrees. The image’s class is still recognized by humans and so this allows the model to gen-eralize in a way humans expect it to generalize.
Automatic augmentation methods are a set of methods that design augmentation policies automatically. They have been shown to improve model performance significantly across tasks [2, 23, 20].
Automatic augmentation methods have flourished espe-cially for image classification in recent years [2, 13, 14, 9] with many different approaches that learn policies over aug-1. https://github.com/automl/trivialaugment
mentation combinations. The promise of this field is to learn custom augmentation policies that are strong for a par-ticular model and dataset. While the application of an aug-mentation policy found automatically is cheap, the search for it can be much more expensive than the training itself.
In this work, we challenge the belief that the resulting augmentation policies of current automatic augmentation methods are actually particularly well fit to the model and dataset. We do this by introducing a trivial baseline method that performs comparably to more expensive augmentation methods without learning a specific augmentation policy per task. Our method does not even combine augmentations in any way. We fittingly call it TrivialAugment (TA).
The contributions of this paper are threefold:
• We analyze the minimal requirements for well-performing automatic augmentation methods and pro-pose TrivialAugment (TA), a trivial augmentation baseline that poses state-of-the-art performance in most setups. At the same time, TA is the most prac-tical automatic augmentation method to date.
• We comprehensively analyze the performance of TA and multiple other automatic augmentation methods in many setups, using a unified open-source codebase to compare apples to apples.
• We make recommendations on the practical usage of automatic augmentation methods and collect best prac-tices for automatic augmentation research. Addition-ally, we provide our code for easy application and fu-ture research. 2.