Abstract
Low-light video enhancement is an important task. Previ-ous work is mostly trained on paired static images or videos.
We compile a new dataset formed by our new strategy that contains high-quality spatially-aligned video pairs from dy-namic scenes in low- and normal-light conditions. We built it using a mechatronic system to precisely control the dy-namics during the video capture process, and further align the video pairs, both spatially and temporally, by identify-ing the system’s uniform motion stage. Besides the dataset, we propose an end-to-end framework, in which we design a self-supervised strategy to reduce noise, while enhanc-ing the illumination based on the Retinex theory. Exten-sive experiments based on various metrics and large-scale user study demonstrate the value of our dataset and effec-tiveness of our method. The dataset and code are available at https://github.com/dvlab-research/SDSD. 1.

Introduction
To enhance underexposed images and videos captured in low light is a longstanding task in computer vision.
It is challenging since underexposed input does not has much scene structural information. Also, dark areas are typically dominated by noise with low signal-to-noise ratios (see Fig-ure 1(a)). When enhancing such input, one may end up with amplified noise and undesirable visual artifacts in results, as shown in Figure 1(b)&(c). These issues could be exagger-ated for videos taken from dynamic scenes, in which the cameras move largely. In this paper, we focus on enhancing underexposed videos taken from low-light dynamic scenes.
Many methods [34, 18, 9, 6, 25, 20, 4] have been proposed to enhance underexposed images/videos based on deep neu-ral networks via supervised learning. Often these meth-ods learn a mapping from images/videos taken in low-light condition to those with normal lighting. They generally do not deal with videos of dynamic scenes or severely-*Equal Contribution. (a) Input (b) SMOID [14] (c) Auto-Tone in Lightroom (d) Ours
Figure 1: An example frame (a) from a challenging underex-posed frame enhanced by a SOTA method (b), a commercial software (c), and our method (d). Our result exhibits clearer details with distinct contrast and less noise. underexposed videos corrupted by heavy noise. A major reason comes from the lack of suitable datasets – there is no real-world spatially-aligned video pair in high quality for dynamic scenes.
The inherent difficulty of constructing such a dataset is the following. First, to prepare this type of video pair means that one needs to capture two videos – one in low-light and the other in normal light of the same dynamic scene with iden-tical camera motion. Second, it has to precisely align every pair of corresponding frames in the two videos, both spatially and temporally. Lastly, while beam splitters could be used to alleviate some of the constraints for building a dynamic-scene high-quality dataset, quality of captured videos would be limited [14].
As a result, existing datasets, such as those of [1, 5, 27], provide mainly paired images. Chen et al. [4] built a paired video dataset of static scenes, and Jiang et al. [14] released a paired-video dataset of dynamic scenes in limited qual-ity. Our first goal in this work is to construct a new dataset with high-quality spatially-aligned video pairs that feature dynamic scenes.
Besides, for videos in low-light conditions, noise often dominates. When we light up video frames, noise can be
undesirably amplified, leading to various visual artifacts in the enhancement results.
In this work, our second goal is to develop a new solution to enhance underexposed videos, taking noise into account.
Our contribution is the following. First, we release a new dataset of 150 high-quality spatially-aligned videos that fea-ture the same dynamic scenes in low- and normal-light con-ditions. To ensure the alignment and quality of the videos, we built a mechatronic alignment system, in which we as-sembled an electric slide rail and mounted a professional camera on it; see Figure 2. Using this system, we captured videos of nearly-identical camera motion, thereby reducing the effort needed to align the low- and normal-light videos for temporal and spatial consistency. The constructed dataset is named as SDSD dataset, standing for “Seeing Dynamic
Scenes in the Dark.”
Second, we formulate an end-to-end framework for en-hancing underexposed videos. We emphasize noise reduc-tion and illumination enhancement simultaneously in our method. For noise reduction, we formulate a self-supervised strategy for learning, while for the illumination enhance-ment, we predict an illumination map from each input frame based on the Retinex theory [16].
Our dataset is the first high-quality paired video dataset for dynamic scenes, featuring high-resolution video pairs of the same scene and motion in low- and normal-light con-ditions. Trained on our new dataset, our framework works decently for enhancing underexposed videos, even in ex-tremely low-light conditions. To evaluate and demonstrate the applicability and robustness of our new approach, we conducted comprehensive experiments to compare it with a rich set of state-of-the-art methods on our constructed dataset and SMID dataset [4]. Further, we conducted a large-scale user study with 100 participants, showing that our results are visually more pleasing and accurate than previous methods.
Figure 2: The devices in our mechatronic system. In the top row, from left to right is Canon EOS 6D Mark II, the electric machine (to drive the motion of the camera), the controller (to set the starting and ending points for motion), and an ND filter. We mount the camera and the electric machine on the electric slide rail, as shown in the bottom row. 2.