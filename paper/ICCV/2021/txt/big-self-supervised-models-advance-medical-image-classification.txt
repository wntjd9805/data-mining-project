Abstract
Self-supervised pretraining followed by supervised fine-tuning has seen success in image recognition, especially when labeled examples are scarce, but has received lim-ited attention in medical image analysis. This paper stud-ies the effectiveness of self-supervised learning as a pre-training strategy for medical image classification. We con-duct experiments on two distinct tasks: dermatology con-dition classification from digital camera images and multi-label chest X-ray classification, and demonstrate that self-supervised learning on ImageNet, followed by additional self-supervised learning on unlabeled domain-specific med-ical images significantly improves the accuracy of medical image classifiers. We introduce a novel Multi-Instance Con-trastive Learning (MICLe) method that uses multiple im-ages of the underlying pathology per patient case, when available, to construct more informative positive pairs for self-supervised learning. Combining our contributions, we achieve an improvement of 6.7% in top-1 accuracy and an improvement of 1.1% in mean AUC on dermatology and chest X-ray classification respectively, outperforming strong supervised baselines pretrained on ImageNet. In ad-dition, we show that big self-supervised models are robust to distribution shift and can learn efficiently with a small number of labeled medical images. 1.

Introduction
Learning from limited labeled data is a fundamental problem in machine learning, which is crucial for medi-cal image analysis because annotating medical images is time-consuming and expensive. Two common pretraining approaches to learning from limited labeled data include: (1) supervised pretraining on a large labeled dataset such as
ImageNet, (2) self-supervised pretraining using contrastive learning (e.g., [16, 8, 9]) on unlabeled data. After pretrain-ing, supervised fine-tuning on a target labeled dataset of in-terest is used. While ImageNet pretraining is ubiquitous in medical image analysis [46, 32, 31, 29, 15, 20], the use of self-supervised approaches has received limited attention.
Self-supervised approaches are attractive because they en-∗Former intern at Google. Currently at Georgia Institute of Technology.
†{shekazizi, skornblith, iamtingchen, natviv, mnorouzi}@google.com
Figure 1: Our approach comprises three steps: (1) Self-supervised pretraining on unlabeled ImageNet using SimCLR [8]. (2) Additional self-supervised pretraining using unlabeled medical images. If multiple images of each medical condition are avail-able, a novel Multi-Instance Contrastive Learning (MICLe) is used to construct more informative positive pairs based on different im-ages. (3) Supervised fine-tuning on labeled medical images. Note that unlike step (1), steps (2) and (3) are task and dataset specific. able the use of unlabeled domain-specific images during pretraining to learn more relevant representations.
This paper studies self-supervised learning for medi-cal image analysis and conducts a fair comparison be-tween self-supervised and supervised pretraining on two distinct medical image classification tasks: (1) dermatol-ogy skin condition classification from digital camera im-ages, (2) multi-label chest X-ray classification among five pathologies based on the CheXpert dataset [23]. We ob-serve that self-supervised pretraining outperforms super-vised pretraining, even when the full ImageNet dataset (14M images and 21.8K classes) is used for supervised pre-training. We attribute this finding to the domain shift and discrepancy between the nature of recognition tasks in Im-top-1 accuracy, even in a highly competitive production setting. On chest X-ray classification, self-supervised learning outperforms strong supervised baselines pre-trained on ImageNet by 1.1% in mean AUC.
• We demonstrate that self-supervised models are robust and generalize better than baseslines, when subjected to shifted test sets, without fine-tuning. Such behavior is desirable for deployment in a real-world clinical setting. 2.