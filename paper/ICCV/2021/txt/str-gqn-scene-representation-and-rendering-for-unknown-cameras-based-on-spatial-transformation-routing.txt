Abstract
Geometry-aware modules are widely applied in recent deep learning architectures for scene representation and rendering. However, these modules require intrinsic cam-era information that might not be obtained accurately. In this paper, we propose a Spatial Transformation Routing (STR) mechanism to model the spatial properties without applying any geometric prior. The STR mechanism treats the spatial transformation as the message passing process, and the relation between the view poses and the routing weights is modeled by an end-to-end trainable neural net-work. Besides, an Occupancy Concept Mapping (OCM) framework is proposed to provide explainable rationals for scene-fusion processes. We conducted experiments on sev-eral datasets and show that the proposed STR mechanism improves the performance of the Generative Query Network (GQN). The visualization results reveal that the routing pro-cess can pass the observed information from one location of some view to the associated location in the other view, which demonstrates the advantage of the proposed model in terms of spatial cognition. 1.

Introduction
Understanding the structure of 3D scenes from the 2D observations is a fundamental topic in the field of computer vision. With the progress of the geometry-based model, re-searchers have developed several techniques to recover the 3D geometry from 2D views via optimization [10, 13] and machine learning [21, 24, 25, 20, 4, 27, 26, 5]. Different from reconstructing the explicit 3D geometry, Generative
Query Network (GQN) [3] constructs the implicit scene representation and achieves novel view rendering merely based on the observed images and the pose information.
Figure 1. The basic concept of the proposed model, which treats the spatial transformation as a message passing process.
Unfortunately, GQN does not consider the spatial property and only has a weak generalization ability. For example,
GQN fails to generalize the knowledge learned from the scene containing two objects to the scene containing four objects. Based on the geometric optics and camera models, some recent works combine the geometry-aware operations with GQN so that the model can be applied to more complex scenes. For instance, E-GQN [19] utilizes epipolar line to search the features in the observations. GRNN [22] applies camera projection and unprojection to pass the information between 2D views and a voxel-based feature memory.
However, geometry-aware operations rely on accurate camera intrinsic parameters which require additional cali-bration process and can only be used in a fixed and simple imaging situation. To design a general model which can be flexibly applied to different visual sensors with differ-ent imaging situations, an interesting question thus arises: can the model learn the spatial transformation property of the 3D space through observations without applying explicit geometry priors such as camera projection matrix, field of view, and the distortion coefficients? Aiming at this goal, we design a new architecture based on GQN to separate the process of spatial transformation and feature extraction, so as to learn the content-independent concept of spatial cog-nition. Following this idea, we propose a Spatial Trans-formation Routing (STR) mechanism, which treats the transformation between world space and view space as a message-passing process. As shown in Fig. 1, the observed features are first passed to several locations in the world space. After fusing the features of different observations in the world space, the features are then passed to the view space of the query pose. The relations between the view poses and the routes of message passing are modeled by a neural network, which is end-to-end trainable.
Besides, after investigating the scene fusion operation and the associated meanings of the extracted features, we develop Occupancy Concept Mapping (OCM) to fuse the features of different observations with a probability model and constrain the scale of scene representation. Leveraging
STR mechanism and OCM, we introduce Spatial Trans-formation Routing Generative Query Network (STR-GQN) (illustrated in Fig. 2) to achieve scene representa-tion and rendering. The evaluation results on several bench-marks reveal that the proposed STR-GQN improves the per-formance of the baseline GQN model and achieves higher generalization ability. Moreover, we visualize the message passing results of Gaussian signals to disclose and explain how our model achieves spatial transformation. 2.