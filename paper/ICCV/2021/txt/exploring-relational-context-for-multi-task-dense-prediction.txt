Abstract
The timeline of computer vision research is marked with advances in learning and utilizing efficient contextual rep-resentations. Most of them, however, are targeted at im-proving model performance on a single downstream task.
We consider a multi-task environment for dense prediction tasks, represented by a common backbone and independent task-specific heads. Our goal is to find the most efficient way to refine each task prediction by capturing cross-task contexts dependent on tasks’ relations. We explore various attention-based contexts, such as global and local, in the multi-task setting and analyze their behavior when applied to refine each task independently. Empirical findings confirm that different source-target task pairs benefit from different context types. To automate the selection process, we pro-pose an Adaptive Task-Relational Context (ATRC) module, which samples the pool of all available contexts for each task pair using neural architecture search and outputs the optimal configuration for deployment. Our method achieves state-of-the-art performance on two important multi-task benchmarks, namely NYUD-v2 and PASCAL-Context. The proposed ATRC has a low computational toll and can be used as a drop-in refinement module for any supervised multi-task architecture. 1.

Introduction
The role of context in computer vision is hard to over-state; most notable breakthroughs boil down to a clever extraction [30], learning [26], and utilization [25] of contex-tual representations. The success of Convolutional Neural
Networks (CNN) is largely due to their inherent ability to capture the local context and build very deep [41] contextual hierarchies within the model. Recently, the progressive adop-tion of the attention mechanism in computer vision [51] has brought forth more flexible context descriptions conditioned on the interdependence of individual pixels, while steadily replacing the traditional convolutional building blocks [11].
Multi-Task Learning (MTL) [6] is concerned with shar-ing representations between tasks. Motivated by the obser-Figure 1. Schematic of the task relational context (orange overlay) for the marked pixel (orange cross) of target task semantic segmen-tation. Our algorithm selects one distillation context type for each source task (dashed lines represent a switch). Alternatively, the connection can be severed by choosing none. The procedure is analogous for all other target tasks. vation that representations of visual tasks are often highly correlated [56], recent works [50, 44] focusing on multi-task dense prediction have extended context extraction across tasks through soft-gated message passing. Referred to as multi-modal distillation in the literature [50], the idea is to augment the high-level representations of downstream target tasks by selectively aggregating complementary features of a set of source tasks. The gating function in the distillation thereby learns to focus on useful cross-task information flow.
Despite their effectiveness, current multi-modal distilla-tion schemes [50, 44] suffer from two main limitations: (1)
The employed gates only regulate information flow based on the source task feature values. As such, the distillation module fails to capture task interactions fully. (2) Each tar-get pixel exclusively receives information from its source counterpart, i.e., the message passing is restricted locally.
Compelled by these drawbacks, we propose a new type of
attention-driven multi-modal distillation scheme, based on three key contributions: 1. Increase the expressivity of the cross-task gate by con-ditioning it on the interdependence of source and target task pixels. Our multi-modal distillation scheme is therefore relational. 2. Enable global cross-task message passing by enlarging the receptive field of the distillation scheme. We refer to each pixel’s distillation receptive field as its distillation context. 3. Customize the distillation context type for each source-target task pair. We formulate five context type candi-dates (global, local, T-label, S-label, none) and adapt the type automatically with respect to each source-target task pair in a given architecture (see Fig. 1).
Contributions 1 and 2 are addressed by leveraging and adapting the scaled-dot product attention mechanism [45] for multi-modal distillation. For contribution 3, we repur-pose modern Neural Architecture Search (NAS) methods to automatically find the optimal context type for each source-target task connection. Overall, we present a novel Adaptive
Task-Relational Context (ATRC) module which can be used as a drop-in module for CNNs to refine any dictionary of supervised dense prediction tasks. We show its effectiveness empirically with the architecture shown in Fig. 2: a single neural network for all tasks with a shared backbone of RGB input, multiple task-specific heads, and ATRC distillation modules to refine each task’s predictions.
The paper is structured as follows: Sec. 2 provides an overview of related work; Sec. 3.1 introduces the architecture of ATRC; Sec. 3.2 explains the types of relational contexts in consideration; Sec. 3.3 covers the adaptation of the context type through NAS techniques; Sec. 4 provides the empirical study details and verifies the proposed method state-of-the-art performance on several important benchmarks; Sec. 5 concludes the paper. 2.