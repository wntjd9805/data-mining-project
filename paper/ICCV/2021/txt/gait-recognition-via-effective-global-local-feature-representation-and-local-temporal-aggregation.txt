Abstract
Gait recognition is one of the most important biometric technologies and has been applied in many ﬁelds. Recent gait recognition frameworks represent each gait frame by descriptors extracted from either global appearances or lo-cal regions of humans. However, the representations based on global information often neglect the details of the gait frame, while local region based descriptors cannot cap-ture the relations among neighboring regions, thus reduc-ing their discriminativeness.
In this paper, we propose a novel feature extraction and fusion framework to achieve discriminative feature representations for gait recognition.
Towards this goal, we take advantage of both global vi-sual information and local region details and develop a
Global and Local Feature Extractor (GLFE). Speciﬁcally, our GLFE module is composed of our newly designed mul-tiple global and local convolutional layers (GLConv) to ensemble global and local features in a principle manner.
Furthermore, we present a novel operation, namely Local
Temporal Aggregation (LTA), to further preserve the spa-tial information by reducing the temporal resolution to ob-tain higher spatial resolution. With the help of our GLFE and LTA, our method signiﬁcantly improves the discrimi-nativeness of our visual features, thus improving the gait recognition performance. Extensive experiments demon-strate that our proposed method outperforms state-of-the-art gait recognition methods on two popular datasets. 1.

Introduction
Gait recognition is a biometric technology depending on the uniqueness of human walking posture. Since human gait can be captured in long-distance conditions and the recog-nition process does not need the subject’s cooperation, gait recognition can be widely applied in many ﬁelds, such as video surveillance, intelligent transportation, etc. However,
*Shunli Zhang is the corresponding author. the performance of gait recognition suffers from many con-ditions, e.g. changing clothing, carrying conditions, cross-view, speed change, and resolution [4, 14, 24]. Therefore, improving the performance of gait recognition in a complex external environment is still highly desirable.
Recently, many existing gait recognition methods em-ploy convolutional neural networks (CNNs) to generate gait feature representations and achieve better recognition per-formance than the traditional approaches. In general, the feature representations can be divided into two categories: global and local feature based representation. Global fea-ture based representation methods extract gait features from whole gait frames. Shiraga et al. [18] use 2D CNNs to ex-tract global gait features from Gait Energy Image (GEI).
Chao et al. [3] also use 2D CNNs to extract global features at the frame-level. Local feature based representation meth-ods extract and combine local gait features from local gait parts. Zhang et al. [27] partition human gaits into different local parts and use multiple separate 2D CNNs to extract lo-cal features. Fan et al. [5] design a focal convolution layer to further extract local features from feature maps.
However, the aforementioned methods only utilize either global or local feature for representation, thus limiting the recognition performance. In particular, the global feature representations may not pay enough attention to the details of the gait, while the local feature representations may lose the global context information of the gait and neglect the relations among local regions. Moreover, Wolf et al.[20] introduce 3D CNNs to extract the robust spatial-temporal gait features. However, traditional 3D CNNs requires ﬁxed-length gait sequences for classiﬁcation and thus are not able to address different length of videos directly.
To address the above issues, in this paper, we propose a novel cross-view gait recognition framework by learn-ing effective representations from global and local features.
Speciﬁcally, we build a new feature extraction module, called Global and Local Feature Extractor(GLFE), in the 3D CNNs framework to attain discriminative representa-tions from both global and local information of gait frames.
In the GLFE module, we design a new Global and Local
Convolutional layer (GLConv) to extract both global and lo-cal features in a principle way. The global feature extractor focuses on the entire visual gait appearance, while the local one pay attention to the gait details. Then, the GLFE mod-ule is composed of multiple GLConv layers. By combining global and local gait feature maps, the GLFE module is able to attain more discriminative feature representation.
Since existing 2D CNNs based methods [5, 3] usually use a spatial pooling layer to downsample feature resolu-tion, the spatial information will be lost gradually. To fully exploit spatial information, we develop a novel Local Tem-poral Aggregation (LTA) operation to replace the traditional spatial pooling layer and aggregate temporal information in local clips. In this fashion, we leverage the temporal reso-lution to attain higher spatial resolution.
Since the proposed method employs 3D convolutions, the temporal convolution is applied to aggregating tempo-ral information.
The main contributions of this paper are three-fold. 1) We present a novel gait recognition framework to ob-tain a discriminative gait representation. In this framework, we introduce a new Global and Local Feature Extraction (GLFE) module with our Global and Local Convolutional layers (GLConv). 2) We propose a novel Local Temporal Aggregation (LTA) operation to aggregate local temporal information while preserving the spatial information. 3) The proposed method has been evaluated on pub-lic datasets CASIA-B and OUMVLP. The experimental re-sults demonstrate that it can achieve state-of-the-art perfor-mance, especially in complex conditions. 2.