Abstract
Generalization beyond the training distribution is a core challenge in machine learning. The common practice of mixing and shuffling examples when training neural net-works may not be optimal in this regard. We show that parti-tioning the data into well-chosen, non-i.i.d. subsets treated as multiple training environments can guide the learning of models with better out-of-distribution generalization. We describe a training procedure to capture the patterns that are stable across environments while discarding spurious ones. The method makes a step beyond correlation-based learning: the choice of the partitioning allows injecting in-formation about the task that cannot be otherwise recovered from the joint distribution of the training data.
We demonstrate multiple use cases with the task of visual question answering, which is notorious for dataset biases.
We obtain significant improvements on VQA-CP, using envi-ronments built from prior knowledge, existing meta data, or unsupervised clustering. We also get improvements on GQA using annotations of “equivalent questions”, and on multi-dataset training (VQA v2 / Visual Genome) by treating them as distinct environments. 1.

Introduction
The best of machine learning models can sometimes be right for the wrong reasons [2, 21, 26, 69]. The ubiqui-tous paradigm of empirical risk minimization (ERM) pro-duces models that capture all statistical patterns present in the training data1. However, not all of these patterns are re-liable and reflective of the task of interest. Some of them result from confounding factors, sampling biases, and other annotation artifacts specific to a given dataset. We call these patterns spurious2 and a model that relies on them will gen-1 We use patterns and correlations interchangeably to refer to statisti-cal relationships between observed random variables, typically the input(s) and output(s) of a supervised learning task. 2 The literature also uses dataset biases to refer to spurious correlations between inputs and outputs that are dataset-specific [69].
Figure 1. Datasets for visual question answering contain biases and spurious correlations: the first few words of a question are as-sociated with a peaky distribution over answers (blue histograms).
Models that guess their answers using these correlations gener-alize poorly. We improve by partitioning the data into multi-ple training environments across which the spurious correlations vary (green histograms) while reliable correlations are stable. Our training procedure produces a model that relies on these stable cor-relations such that it generalizes much better at test time. eralize poorly to test data obtained in different conditions (i.e. out-of-distribution or OOD data).
The limits of ERM on OOD data are oftentimes over-looked and eclipsed by the common practice of evaluating on test data i.i.d. to the training data – a central assumption of classical learning theory [70]. The awareness of these limits has grown with that of their practical implications, from poor transfer across datasets [69] to biases and fair-ness issues [1] and vulnerability to adversarial inputs [27].
As a result, benchmarks with OOD test sets are becoming increasingly common in vision and NLP [2, 4, 9, 29, 37, 44].
This paper presents a training paradigm to improve OOD generalization.
The study of generalization in computer vision has a long history [20, 41, 48]. The rise in popularity of high-level tasks like visual question answering (VQA) [7], vi-sual dialogue [19], or vision-and-language navigation [6] has made the topic even more important. The complex-ity of these tasks and the combinatorial explosion of the size of their input domain make it impossible to process
training data densely spanning this space. Models trained with ERM are then more likely to latch on spurious corre-lations (because they are often easier to fit [58]) rather than on the true reasoning process that underlies the task [36].
VQA was shown empirically to be a prime example of this issue. Many methods have been proposed to address it [2, 13, 17, 27, 29, 47, 55].
We propose a general method to improve OOD gen-eralization. We discourage the model from using spurious correlations that only appear in subsets of the training data, and rather ensure that it uses reliable ones that are more likely to generalize at test time. More precisely, we first partition the data into multiple training environments [8] such that spurious correlations vary across environments while reliable ones remain stable. We later describe mul-tiple strategies to build such environments, using unsuper-vised clustering, prior knowledge, and auxiliary annotations in existing datasets. Second, we train multiple copies of a neural network, one per environment. Some of their weights are shared across environments, while others are subject to a variance regularizer in parameter space. This leads the model to extract features that are stable across envi-ronments (i.e. features that do not represent environment-specific properties) since they are optimized to be predictive under a classifier common to all environments (as encour-aged by the variance regularizer). Additional intuitions and reasons why this approach is superior to ERM are discussed in Section 3.3.
We provide empirical evidence of improvements in three distinct use cases on the task of VQA. First, we demonstrate improved resilience to language biases with
VQA-CP [2]. Second, on GQA [34], we show how to use annotations of equivalent questions (some training ques-tions being rephrasings of others). We obtain substantial gains over simple data augmentation with these equivalent questions in a small-data regime. Third, we show a small benefit in training a model on multiple datasets by treating
VQA v2 [26] and Visual Genome QA [42] as two environ-ments rather than one aggregated dataset. Of these three use cases, the first is the most well-studied but our method has a much wider scope than the VQA-CP dataset.
The contributions of this paper are summarized as follows. 1. We propose to partition existing datasets into training en-vironments to improve generalization. We describe the requirements for the partitioning and a procedure to train neural networks to rely on stable correlations across en-vironments while ignoring spurious ones. 2. We apply the method to three use cases with the task of
VQA: (1) resilience to language biases, (2) leveraging known relations of equivalence between specific training questions, and (3) multi-dataset training. 3. We provide empirical evidence of clear improvements and a extensive sensitivity analysis to hyperparameters and implementation choices. 2.