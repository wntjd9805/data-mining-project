Abstract
Recent studies have demonstrated the vulnerability of deep neural networks against adversarial examples.
In-spired by the observation that adversarial examples often lie outside the natural image data manifold and the intrin-sic dimension of image data is much smaller than its pixel space dimension, we propose to embed high-dimensional input images into a low-dimensional space and apply regu-larization on the embedding space to push the adversarial examples back to the manifold. The proposed framework is called Embedding Regularized Classiﬁer (ER-Classiﬁer), which improves the adversarial robustness of the classi-ﬁer through embedding regularization. Besides improving classiﬁcation accuracy against adversarial examples, the framework can be combined with detection methods to de-tect adversarial examples. Experimental results on several benchmark datasets show that, our proposed framework achieves good performance against strong adversarial at-tack methods. 1.

Introduction
It has been shown that Deep Neural Networks (DNNs) are vulnerable to adversarial examples that are generated by adding carefully crafted perturbations to original im-ages [44, 27]. This phenomenon brings out security con-cerns for practical applications of deep learning. Despite many adversarial training methods have been proposed for defending against adversarial examples [35, 50, 38, 39], we propose a novel defense and detection method from a dif-ferent point of view.
Recent work showed that adversarial examples often lie outside the natural image data manifolds [24, 45]. Since the model was trained using data in the manifold, the model naturally mis-classiﬁes on examples outside the manifold.
Therefore, applying regularization to push the adversar-ial examples back to the natural image data manifold may help improve the robustness of neural networks (see Fig-ure 1). Furthermore, a consensus in the high-dimensional data analysis community is that, a method working well on (cid:4)(cid:8)(cid:9)(cid:17)(cid:11)(cid:5)(cid:14)(cid:10)(cid:19)(cid:5)(cid:16)(cid:10)(cid:13)(cid:12) (cid:3)(cid:5)(cid:16)(cid:17)(cid:14)(cid:5)(cid:11) (cid:2)(cid:10)(cid:15)(cid:16)(cid:14)(cid:10)(cid:6)(cid:17)(cid:16)(cid:10)(cid:13)(cid:12) (cid:1)(cid:7)(cid:18) (cid:2)(cid:10)(cid:15)(cid:16)(cid:14)(cid:10)(cid:6)(cid:17)(cid:16)(cid:10)(cid:13)(cid:12)
Figure 1. Natural and adversarial images are from different distri-butions the high-dimensional data is because the data is not really of high-dimension [30]. Inspired by the observation that the intrinsic dimension of image data is actually much smaller than its pixel space dimension [30] and adversarial exam-ples lie outside the natural image data manifold [24, 45], we propose a defense framework called Embedding Regu-larized Classiﬁer (ER-Classiﬁer) shown in Figure 2.
The difference between ER-Classiﬁer and general deep classiﬁer is that the extracted feature is regularized by a discriminator part in ER-Classiﬁer. To be more speciﬁc, we introduce a discriminator in the latent space which tries to separate the generated code vectors (output of the en-coder network) from the ideal code vectors (sampled from a prior distribution, i.e., a standard Gaussian distribution).
Employing a similar powerful competitive mechanism as demonstrated by Generative Adversarial Networks [17], the discriminator enforces the embedding space of the model to follow the prior distribution. This regularization process can help remove the effect of adversarial distortion and push the adversarial example back to the natural data manifold.
Another difference is that the embedding space dimension of ER-classiﬁer is much smaller, which makes it easier for
ER-Classiﬁer to apply regularization.
We compare ER-Classiﬁer with other state-of-the-art de-fense methods on MNIST, CIFAR10, STL10 and Tiny Im-agenet. Experimental results demonstrate that our proposed
ER-Classiﬁer outperforms other methods by a large margin.
To sum up, this paper makes the following four main con-tributions:
• A novel uniﬁed end-to-end robust deep neural net-Encoder (cid:15)(cid:22)(cid:23)(cid:16)(cid:24)
Classifier (cid:12)(cid:21)(cid:23)(cid:10)(cid:24) (cid:16)(cid:11)(cid:14)(cid:18)
Generated Code
Ideal Code
D(cid:3)(cid:8)(cid:2)(cid:7)(cid:3)(cid:4)(cid:3)(cid:5)(cid:1)(cid:9)(cid:6)(cid:7) (cid:13)(cid:20)(cid:23)(cid:10)(cid:24) (cid:17)(cid:11)(cid:14)(cid:19)
Figure 2. Overview of ER-Classiﬁer framework work framework against adversarial attacks is pro-posed, where embedding space regularization is ap-plied to remove the adversarial effect.
• An objective is induced to minimize the optimal trans-port cost between the true class distribution and the framework output distribution, guiding the framework to project the input image to a low-dimensional space without losing important features for classiﬁcation.
• A detection process is proposed to further improve the robustness of the framework.
• Extensive experiments demonstrate the robustness of our proposed ER-Classiﬁer framework under the white-box attacks, and show that ER-Classiﬁer out-performs other state-of-the-art approaches on several benchmark image datasets. 2.