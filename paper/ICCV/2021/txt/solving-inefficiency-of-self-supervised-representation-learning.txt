Abstract
Self-supervised learning (especially contrastive learn-ing) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing con-trastive learning methods suffer from very low learning ef-ficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accu-racy. In this paper, we reveal two contradictory phenom-ena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilar-ity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differenti-ate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from exces-sive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters.
To simultaneously overcome these two problems, we pro-pose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the posi-tive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli
Distribution model. We extensively evaluate our framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate our model’s su-periority (e.g., the learning efficiency) over the latest state-of-the-art methods by a clear margin. See Codes1. 1.

Introduction
Recently, self-supervised learning (SSL) has shown re-markable results in representation learning. Among them, the results of contrastive learning are most promising in the
Figure 1. A comparison of learning efficiency among different SSL methods using ResNet-50. Here, the x-axis represents the train-ing epochs of SSL, and the y-axis stands for the top-1 accuracy of ImageNet linear evaluation. All methods have lower learning efficiency than supervised learning, but our approach has a signif-icantly higher learning efficiency than the existing SSL methods. (best view in color) computer vision tasks. Notable works include MoCo v1/v2
[24, 8], SimCLR [7], BYOL [22], and SimSiam [9]. For example, on ImageNet [36], the top-1 accuracy of BYOL is 74.3%, which is close to that of supervised learning, i.e., 76.4% [49, 1, 28, 32] (see “goal line” in Figure 1). De-spite the promising accuracies and high expectations, the learning efficiency of the state-of-the-art SSL methods is about ten times lower than the supervised learning meth-ods. For instance, the supervised learning method typically takes about 100 epochs to train a ResNet50 on ImageNet. In comparison, SimCLR and BYOL have to cost 1,000 epochs, and MoCo v2 needs to cost 800 epochs (See Figure 1).
Attempting to address this issue, we rethink existing SSL methods’ mechanism and attribute their inherited draw-back to two opposing problems, i.e., under-clustering and over-clustering. Specifically, during batch training, con-trastive learning randomly crops each image two times to obtain two views and study the similarity between these two views (called a positive sample pair 2). Meanwhile, some methods also study the dissimilarity between cross-image views (called negative sample pairs3). The opti-mization objective is to reduce the distance between posi-*Corresponding Author. 1https://github.com/wanggrun/triplet 2e.g., View A and View B of Image X 3e.g., View A of Image X and View B of Image Y
tive sample pairs and enlarge the distance between negative sample pairs. As suggested by metric learning [12], suf-ficient negative sample pairs are required to guarantee the learning efficiency. Otherwise, lacking negative samples – whether due to the GPU memory constraints like SimCLR or (ii) algorithm design like BYOL and SimSiam [9] – can make different object categories having overlaps. This is identified as the under-clustering problem. One evidence of under-clustering is shown in Table 14. As a result of under-clustering, SimCLR and BYOL have low learning ef-ficiency because the model cannot efficiently discover the dissimilarity between inter-class samples. On the contrary, excessive negative samples can lead to an opposite problem, i.e., over-clustering, which implies the negative samples are false negative and the model over-clusters samples of the same actual categories into different clusters. In an extreme case, there would be 1.28M clusters for ImageNet! One evi-dence of over-clustering is in Table 15. Over-clustering also results in low learning efficiency since it encourages dissim-ilarity between intra-class samples in vain. As reported by
[54, 3], over-clustering can lead to unnecessary harmful rep-resentation learning. For example, [15, 14] obtains an un-satisfied performance due to directly clarifying CIFAR-10 into 50K clusters. MoCo v1/v2 cannot further increase the accuracy, even leveraging the momentum to store plenty of negative samples. In summary, existing contrastive learning cannot avoid the under-clustering or over-clustering prob-lems, so their learning efficiency is still low.
To tackle the above under-clustering and over-training problems, a few pioneering works have been proposed to analyze the negative samples’ role in the contrastive loss
[10, 35, 4, 27]. As opposed to these methods that use over-complicated contrastive losses, we propose an SSL frame-work using a quite simple truncated triplet loss. Specif-ically, a triplet loss can maximize the relative distance between the positive and negative pairs for each triplet unit. Having plenty of triplets, we can address the under-clustering problem because wealthy triplets contain rich negative pairs that guarantee a considerable distance be-tween negative sample pairs. Triplet loss largely addresses the under-clustering issue but raises the over-clustering problem. Hence, we propose a novel truncated triplet loss to avoid over-clustering samples from the same category into different clusters. The truncated triplets are ensured with confidence guaranteed by the Bernoulli Distribution model.
This significantly improves SSL’s learning efficiency and 4We calculate the class center for each category and compute the dis-tances for every two class centers. These center-to-center distances are averaged to form a class divergence. We keep the variance equal so we can just compare class divergence. The small class divergence in Table 1 indicates BYOL does suffer under-clustering. 5We use Pr(ω|A) (defined in Section 5) to denote the possibility of containing a false negative samples in a batch. The high probabilities in
Table 1 verify MoCo v2 indeed suffers over-clustering.
Under-clustering (Divergence)
Over-clustering (Pr(ω|A))
BYOL 5.3711
MoCo v2 1.0
Ours 7.6803
Ours 0.0110
Table 1. Qualitative analysis of over-/ under- clustering. We use
Pr(ω|A) (the larger, the higher over-clustering risk) and class di-vergence (the smaller, the higher under-clustering risk) to measure the over-/ under- clustering level, respectively. leads to state-of-the-art performance (See Figure 1).
In summary, our contribution is three-fold.
• We analyze the existing best-performing contrastive learning methods and attribute their low learning in-efficiency to the under-clustering and over-clustering, which result in unnecessary harmful representation learning just to memorize the data.
• To address the under-clustering and over-training prob-lem, we propose a novel SSL framework using a trun-cated triplet loss. Precisely, we employ a triplet loss containing rich negative samples to address the under-clustering problem, and our triplet loss uses trun-cated/trimmed triplets to avoid over-clustering, guar-anteed by the Bernoulli Distribution model.
• Our method significantly improves SSL’s learning effi-ciency and thus leads to state-of-the-art performance in several large-scale benchmarks (e.g., ImageNet [36],
SYSU-30k [46], and COCO 2017 [29]) and varieties of downstream tasks. 2.