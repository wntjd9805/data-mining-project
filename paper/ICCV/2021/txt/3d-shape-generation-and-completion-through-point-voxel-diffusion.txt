Abstract
We propose a novel approach for probabilistic genera-tive modeling of 3D shapes. Unlike most existing models that learn to deterministically translate a latent vector to a shape, our model, Point-Voxel Diffusion (PVD), is a unified, probabilistic formulation for unconditional shape genera-tion and conditional, multi-modal shape completion. PVD marries denoising diffusion models with the hybrid, point-voxel representation of 3D shapes. It can be viewed as a series of denoising steps, reversing the diffusion process from observed point cloud data to Gaussian noise, and is trained by optimizing a variational lower bound to the (con-ditional) likelihood function. Experiments demonstrate that
PVD is capable of synthesizing high-fidelity shapes, com-pleting partial point clouds, and generating multiple com-pletion results from single-view depth scans of real objects. 1.

Introduction
Generative modeling of 3D shapes has extensive appli-cations across vision, graphics, and robotics. To perform
Project page at https://alexzhou907.github.io/pvd well in these downstream applications, a good 3D genera-tive models should be faithful and probabilistic. A faith-ful model generates shapes that are realistic to humans and, in cases where conditional inputs such as depth maps are available, respects such partial observations. A probabilis-tic model captures the under-determined, multi-modal na-ture of the generation and completion problem: it may sam-ple and produce diverse shapes from scratch or from partial observations. As shown in Figure 1, when only the back of a chair is visible, good generative models should be able to produce multiple possible completed chairs, including those with arms and those without.
Existing shape generation models can be roughly di-vided into two categories. The first operates on 3D vox-els [43, 14, 45, 2], a natural extension of 2D pixels.
While being straightforward to use, voxels demand pro-hibitively large memory when scaled to high dimensions, and are thus unlikely to produce results with high fidelity.
The second class of models studies point cloud genera-tion [1, 11, 48, 46, 16, 17] and has produced promising re-sults. While being more faithful, these approaches typically view point cloud generation as a point generation process conditioned on shape encoding, which is obtained by de-terministic encoders. When performing shape completion, 1
these approaches are therefore unable to capture the multi-modal nature of the completion problem.
Recently, a new class of generative models, named prob-abilistic diffusion models, have achieved impressive perfor-mance on 2D image generation [35, 13, 36]. These ap-proaches learn a probabilistic model over a denoising pro-cess. Diffusion is supervised to gradually denoise a Gaus-sian noise to a target output, such as an image. Methods along this line, such as DDPM [13], are inherently proba-bilistic and produce highly realistic 2D images.
Extending diffusion models to 3D is, however, techni-cally highly nontrivial: a direct application of diffusion models on either voxel and point representation results in poor generation quality. This is because, first, pure vox-els are binary and therefore not suitable for the proba-bilistic nature of diffusion models; second, point clouds demand permutation-invariance, which imposes infeasible constraints on the model. Experiments in Section 4.1 also verifies that a straightforward extension does not lead to rea-sonable results. that
We propose Point-Voxel Diffusion (PVD), a proba-bilistic and flexible shape generation model tack-les the above challenges by marrying denoising diffusion models with the hybrid, point-voxel representation of 3D shapes [24]. A point-voxel representation builds structured locality into point cloud processing; integrated with denois-ing diffusion models, PVD suggests a novel, probabilistic way to generate high-quality shapes by denoising a Gaus-sian noise and to produce multiple completion results from a partial observation, as shown in Figure 1.
A unique strength of PVD is that it is a unified, proba-bilistic formulation for unconditional shape generation and conditional, multi-modal shape completion. While multi-modal shape completion is a highly desirable feature in applications such as digital design or robotics, past works on shape generation primarily use deterministic shape en-coders and decoders to output a single possible completion in voxels or a point cloud. In contrast, PVD can perform both unconditional shape generation and conditional shape completion in an integrated framework, requiring only min-imal modifications to the training objective. It is thus capa-ble of sampling multiple completion results depending on diffusion initialization.
Experiments demonstrate that PVD is capable of syn-thesizing high-fidelity shapes, outperforming multiple state-of-the-art methods. PVD also delivers high-quality results on multi-modal shape completion from partial observations such as a partial point cloud or a depth map.
In partic-ular, we show that PVD does well on multi-modal com-pletion on multiple synthetic and real datasets, including
ShapeNet [4], PartNet [27], and single-view depth scans of real objects in the Redwood dataset [5]. 2.