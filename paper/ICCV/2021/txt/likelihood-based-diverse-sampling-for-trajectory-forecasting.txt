Abstract
Forecasting complex vehicle and pedestrian multi-modal distributions requires powerful probabilistic approaches.
Normalizing ﬂows (NF) have recently emerged as an attrac-tive tool to model such distributions. However, a key draw-back is that independent samples drawn from a ﬂow model often do not adequately capture all the modes in the under-lying distribution. We propose Likelihood-Based Diverse
Sampling (LDS), a method for improving the quality and the diversity of trajectory samples from a pre-trained ﬂow model. Rather than producing individual samples, LDS pro-duces a set of trajectories in one shot. Given a pre-trained forecasting ﬂow model, we train LDS using gradients from the model, to optimize an objective function that rewards high likelihood for individual trajectories in the predicted set, together with high spatial separation among trajecto-ries. LDS outperforms state-of-art post-hoc neural diverse forecasting methods for various pre-trained ﬂow models as well as conditional variational autoencoder (CVAE) mod-els. Crucially, it can also be used for transductive trajectory forecasting, where the diverse forecasts are trained on-the-ﬂy on unlabeled test examples. LDS is easy to implement, and we show that it offers a simple plug-in improvement over baselines on two challenging benchmarks. Code is at: https://github.com/JasonMa2016/LDS 1.

Introduction
A key challenge facing self-driving cars is accurately forecasting the future trajectories of other vehicles. These future trajectories are often diverse and multi-modal, requir-ing a forecasting model to predict not a single ground truth future but the full range of plausible futures [25].
With the increasing abundance of driving data [4, 6], a promising approach is to learn a deep generative model from data to predict the probability distribution over fu-ture trajectories [24, 15, 18, 37, 33, 34, 36, 30]. However, due to natural biases, sampling i.i.d. from a deep generative model’s prior distribution may fail to cover all modes in the
Figure 1: (a) At this intersection, 90% of cars turn right, and 10% drive straight in our training dataset. (b) A normaliz-ing ﬂow trajectory predictor trained on this data, sampled 100 times i.i.d., does not produce any straight trajectories. (c) With our LDS sampler plugged in, the same predictor generates both straight and right-turn trajectories with just 2 samples. Details are in Appendix B. trajectory distribution, especially given the uneven distribu-tion of real-world trafﬁc maneuvers. Consider the scenario in Figure 1(a) and a normalizing ﬂow (NF) [32] forecast-ing model [35, 34]. The i.i.d. forecasts from the ﬂow model in Figure 1(b) successfully capture the major mode corre-sponding to turning right; however, for driving safely at this intersection, we must also anticipate the minor mode corre-sponding to vehicles driving straight.
We propose a general, post-hoc approach, called
Likelihood-Based Diverse Sampling (LDS), for enhancing the quality and the diversity of samples from a pre-trained generative model. The key idea is that rather than drawing i.i.d. samples from the generative model, LDS learns a sam-pling distribution over an entire set of trajectories, which jointly maximizes two objectives: (i) the likelihood of the trajectories according to the model, and (ii) a robust goal-based diversity objective that encourages high ﬁnal spatial separation among trajectories. Intuitively, these two objec-tives together encourage a set of forecasts to cover modes in the underlying trajectory distribution. The result of running
LDS on our toy example is shown in Figure 1(c); it correctly discovers the minor mode of heading straight and distributes samples over both modes. Figure 2 provides an overview of the LDS objective and architecture. Because our technique leverages trajectory likelihood under the learned underly-ing generative model, it is naturally suited for NF-based
Figure 2: Left: Fitted on data at this intersection where most vehicles turn right and a small fraction head straight, (1): LDS predicts a set of paths that cover both modes and are realistic, (2): Optimizing for only model likelihood generates samples that are realistic but miss the minor mode (small star) at the top, and (3): Optimizing for only diversity generates samples that may cover both modes but are not realistic. Right: LDS architecture overview. LDS replaces standard i.i.d. sampling in the ﬂow model with a learned joint distribution over a set of samples in the latent space. These samples allow the pre-trained
ﬂow model to output diverse and realistic trajectories. models, which compute the exact likelihood of their gener-ated samples. To the best of our knowledge, our method is the ﬁrst diverse sampling technique tailored for NF models.
We note, however, that our technique can also be applied to other likelihood-based models—e.g., we can handle VAEs by using the evidence lower-bound for the likelihood.
A key advantage of LDS is that it can be leveraged in the setting of transductive learning [38].
In particular, since
LDS does not require paired trajectories (history and fu-ture) for training, it can directly tailor the sampler to im-prove predictions for a given novel test instance, even with-out any prior training. By specializing to this test instance, transductive learning can outperform supervised learning which cannot perform test-time adaptation. To the best of our knowledge, the tranductive setting has not been previ-ously considered for trajectory forecasting, but we believe it most closely mirrors the forecasting task autonomous ve-hicles face in the real world.
LDS is simple to implement, requiring fewer than 30 lines of code. We evaluate LDS on nuScenes [4] and Fork-ing Paths [25], two challenging single-future and multi-future [25] benchmarks. Our experiments demonstrate that
LDS provides a reliable performance boost when plugged into various existing NF/CVAE forecasting models and out-performs competing approaches. These results further im-prove when LDS is applied for transductive forecasting. 2.