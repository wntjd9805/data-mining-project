Abstract
We tackle catastrophic forgetting problem in the con-text of class-incremental learning for video recognition, which has not been explored actively despite the popular-ity of continual learning. Our framework addresses this challenging task by introducing time-channel importance maps and exploiting the importance maps for learning the representations of incoming examples via knowledge dis-tillation. We also incorporate a regularization scheme in our objective function, which encourages individual fea-tures obtained from different time steps in a video to be un-correlated and eventually improves accuracy by alleviating catastrophic forgetting. We evaluate the proposed approach on brand-new splits of class-incremental action recognition benchmarks constructed upon the UCF101, HMDB51, and
Something-Something V2 datasets, and demonstrate the ef-fectiveness of our algorithm in comparison to the existing continual learning methods that are originally designed for image data. 1.

Introduction
Human activity recognition in a large-scale video dataset is a crucial step for high-level video understanding, and various approaches have been studied actively in the com-puter vision community [2, 17, 21, 36, 40]. If the videos containing unseen classes of actions are presented in a se-quential manner, where the examples in the previously ob-served classes are either inaccessible or accessible in lim-ited amounts, one needs to adapt the current model to the new data without forgetting critical knowledge of the seen examples learned in the past. The machine learn-ing paradigm to handle such challenges is called class-incremental learning, and Figure 1 illustrates a training data stream for the learning framework.
While researchers have been studying action recognition problems using deep neural networks [2, 17, 21, 36, 40], continual learning in videos has not been studied actively.
It is natural to claim that video-based recognition tasks are also prone to suffer from catastrophic forgetting [23] for the
Figure 1: Illustration of class-incremental learning scenario.
At each incremental step, the model learns the knowledge of new classes that are disjoint from the classes it has seen so far. Simultaneously, the model learns not to forget the knowledge of old classes that are either completely inac-cessible or accessible in limited amounts. knowledge learned from training data provided in the past, as in the image domain. Actually, the catastrophic forget-ting problem is particularly problematic in video-learning tasks because deep neural networks with shared parameters are typically applied to multiple segments or frames, result-ing in acceleration of the forgetting issue and it is difficult to store many video exemplars in memory to preserve the information about the previous tasks effectively.
Despite critical needs for class-incremental learning in the video domain, existing approaches [3, 6, 15, 20, 30, 43] have focused on static images only, which fails to model temporal variations and dynamics across spatial features. A single action instance is often composed of multiple sub-actions and the feature dynamics aligned with the subac-tions are indeed critical information for action recognition.
For example, Figure 2 demonstrates that both of Pole Vault and Javelin Throw share a subaction of running with a long stick at the beginning but become distinct by whether the actor jumps or not at the end. This observation leads to a fundamental question about how to maintain crucial spatio-temporal information within individual videos using limited
(a) An example of Pole Vault (b) An example of Javelin Throw
Figure 2: Subactions of action instances. Distinctive subac-tions are key to distinguish one action from another. Our al-gorithm estimates which channels are important along with the temporal dimension for class-incremental learning. memory for continual learning.
This paper presents a novel framework for class-incremental learning for action recognition based on tem-porally attentive knowledge distillation. Our claim is that the representations for individual subactions should be dis-tilled with different weights depending on their relevance and uniqueness to target classes and maintained for better utilization in the future stages. To realize this idea, we draw our attention to a joint space defined by frames in a video and channels in a feature map, and quantify impor-tance over the space while minimizing feature redundancy across frames. Specifically, we estimate the importance in the joint space for a video by measuring how much the ac-tivation in the space affects classification losses. The com-puted importance provides the information about where to attend for knowledge distillation in class-incremental learn-ing scenarios. Also, to enforce the model to learn more dis-tinctive features across frames, we penalize the redundancy in the features extracted from the sampled frames.
The representations of video data require more computa-tion resources for processing and storing, which makes con-tinual learning in videos more challenging especially when some exemplars for the tasks considered earlier need to be stored in memory. So, the proposed class-incremental learn-ing framework employs a frame-based video representation method—Temporal Shift Module (TSM) [21], and reduces computational cost for training significantly compared to 3D CNNs based on video volumes [2, 8, 9, 36, 37] and their variations [40, 47].
The contributions of this paper are summarized below:
• We introduce an efficient class-incremental learning technique for action recognition in videos by adopting a simple frame-based feature representation method to store exemplars for the tasks learned in the past.
• Our algorithm estimates time-channel importances and distills knowledge with the importance weight while encouraging the diversity of the features in each frame for regularization and enhance the performance of our target model.
• The proposed approach presents remarkable accu-racy gains on the multiple standard action recognition benchmarks with brand-new splits compared to the ex-isting methods designed in the image domain.
Our paper is organized as follows. We first discuss re-lated works about continual learning in Section 2. Section 3 describes the proposed class-incremental learning approach in the context of action recognition. We present experimen-tal results on the standard action recognition datasets with new splits for continual learning in Section 4, and make the conclusion in Section 5. 2.