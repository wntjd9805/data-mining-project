Abstract
Transformer, as a strong and ﬂexible architecture for modelling long-range relations, has been widely explored in vision tasks. However, when used in video inpainting that requires ﬁne-grained representation, existed method still suffers from yielding blurry edges in detail due to the hard patch splitting. Here we aim to tackle this problem by proposing FuseFormer, a Transformer model designed for video inpainting via ﬁne-grained feature fusion based on novel Soft Split and Soft Composition operations. The soft split divides feature map into many patches with given overlapping interval. On the contrary, the soft composi-tion operates by stitching different patches into a whole fea-ture map where pixels in overlapping regions are summed up. These two modules are ﬁrst used in tokenization be-fore Transformer layers and de-tokenization after Trans-former layers, for effective mapping between tokens and features. Therefore, sub-patch level information interaction is enabled for more effective feature propagation between neighboring patches, resulting in synthesizing vivid con-tent for hole regions in videos. Moreover, in FuseFormer, we elaborately insert the soft composition and soft split in-to the feed-forward network, enabling the 1D linear layers to have the capability of modelling 2D structure. And, the sub-patch level feature fusion ability is further enhanced.
In both quantitative and qualitative evaluations, our pro-posed FuseFormer surpasses state-of-the-art methods. We also conduct detailed analysis to examine its superiori-ty. Code and pretrained models are available at https:
//github.com/ruiliu-ai/FuseFormer. (cid:3)The ﬁrst three authors contribute equally to this work.
Figure 1. Illustration of different patch split/composition strategies for Transformer model. The top row shows hard split/composition, based on which the trained model generates rough inpainting re-sults. The bottom row shows soft split/composition, based on which the trained model generates smooth results due to interac-tion of features between neighbor patches. Double arrow indicates the corresponding overlapped regions between adjacent patches. 1.

Introduction
Transformer has recently gained increasing attention in various vision tasks such as classiﬁcation [8, 42], object de-tection [28, 47] and image generation [18, 16]. Interesting-ly, Transformer is suitable to video inpainting, a vision task that depends on the information propagation between ﬂow-ing pixels across frames to ﬁll the spatiotemporal holes with plausible and coherent content in a video clip.
Spatial Temporal Transformer Net (STTN) [43] is the pi-oneer work for investigating the use of Transformer in video inpainting. However, its multi-scale variant of self-attention intertwined with fully convolutional networks makes it hard to exploit rich experience from other Transformer models due to large structural differences. On the other hand, re-cent Vision Transformer (ViT) [8] demonstrates the strong capability of vanilla Transformer [34] in vision recognition task. These motivate us to build a Video inpainting Base-line with vanilla Transformer (ViB-T), which differs from
ViT in 2 aspects: a) the tokens are embedded from patch-es of multiple frames instead of a single frame; b) a light convolutional encoder and decoder before and after Trans-former block is exploited to relieve the computational bur-den caused by high resolution frames. Experiment veriﬁes that this simple baseline can reach competitive performance with STTN [43] under similar computation cost.
Nevertheless, similar to all existing patch-based Trans-former models [8, 42], the hard split operation used in ViB-T makes it unable to effectively encode sub-token (sub-patch) level representations. Since the attention score is calculated between different tokens, there is no direct sub-token level feature interaction. For us, human beings, frag-menting an image into many non-overlapping patches poses a challenging task to composite them back into an origi-nal image with masked regions ﬁlled. This is the same for deep learning systems: the lack of accurate sub-token level feature interaction can lead to inconsistent content between neighboring patches. As shown in Fig.1, to accurately re-build the black circle on the canvas, every token correspond-ing to an image patch has to understand not only the patch level information but also sub-patch level information. As a result, in order to fully unleash the power of Transformers in video inpainting tasks, an improved patch splitting man-ner and a better sub-token level feature fusion mechanism to maintain pixel level-feature accuracy is in demand.
To achieve this goal, we propose a Soft Split (SS) module as well as its corresponding Soft Composition (SC) module.
Built upon the simple and straightforward ViB-T baseline model, we propose to softly split images into patches with overlapping regions and correspondingly, to softly compos-ite these overlapped patches back to images. Speciﬁcally, in the soft split module, we exploit an unfold operation with kernel size greater than stride to softly split the input image into overlapping 2D patches and are ﬂattened as 1D token-s. On the contrary, in the soft composition module, tokens are reshaped to 2D patches maintaining their original sizes, and then each pixel is registered to its original spatial loca-tion according to the kernel size and stride used in soft split module. During this process, features of the pixels located in the overlapping area are fused from multiple overlapping neighboring patches’ corresponding areas, thus providing sub-token level feature fusion. We design a baseline ViB-T model equipped with the Soft Split and Soft Composition modules as ViB-S where S stands for soft operations. And we ﬁnd that the ViB-S model easily surpasses the state-of-the-art video inpainting model STTN [43] with minimum extra computation cost.
Finally, we propose a Fusion Feed Forward Network (F3N) to replace the two-layer MLPs in the standard Trans-former model, which is dubbed as FuseFormer, to fur-ther improve its sub-token fusion ability for learning ﬁne-grained feature, yet without extra parameters. In the F3N, between the two fully-connected layers, we reshape each 1D token back to 2D patch with its original spatial shape and then softly composite them to be a whole image. The overlapping features of pixel at overlapping regions would sum up the corresponding value from all neighboring patch-es for further ﬁne-grained feature fusion. Then the patches are softly split and ﬂattened into 1D vectors, which are fed to the second MLP. In this way, sub-token segment corre-sponding to the same pixel location are matched and reg-istered without extra learnable parameters, and information of the same pixel location from different patches are aggre-gated. Subsequently, our FuseFormer model consisting of
F3N even surpasses our strong baseline ViB-S by a signiﬁ-cant margin, both qualitatively and quantitatively.
Based on these novel designs, our proposed FuseFormer network achieves effective and efﬁcient performance in video restoration and object removal. We testify the supe-riority of the proposed model to other state-of-the-art video inpainting approaches by thorough qualitative and quanti-tative comparisons. We further conduct ablation study to show how each component of our model beneﬁts the in-painting performance.
In summary, our contributions are three-fold: 1. We ﬁrst propose a simple yet strong Transformer base-line for video inpainting, and propose a soft split and composition method to boost its performance. 2. Based on the proposed strong baseline and novel soft operations, we propose FuseFormer, a sub-token fu-sion enabled Transformer model with no extra param-eters. 3. Extensive experiments demonstrate the superiority of
FuseFormer over state-of-the-art approaches in video inpainting, both qualitatively and quantitatively. 2.