Abstract
Vector graphic documents present visual elements in a resolution free, compact format and are often seen in cre-ative applications. In this work, we attempt to learn a gen-erative model of vector graphic documents. We define vec-tor graphic documents by a multi-modal set of attributes associated to a canvas and a sequence of visual elements such as shapes, images, or texts, and train variational auto-encoders to learn the representation of the documents. We collect a new dataset of design templates from an online service that features complete document structure includ-ing occluded elements. In experiments, we show that our model, named CanvasVAE, constitutes a strong baseline for generative modeling of vector graphic documents. 1.

Introduction
In creative workflows, designers work on visual presen-tation via vector graphic formats. 2D vector graphics rep-resent images in a compact descriptive structure; instead of spatial array of pixels, graphic documents describe a canvas and arrangement of visual elements such as shapes or texts in a specific format like SVG or PDF. Vector graphics are crucial in creative production for its resolution-free repre-sentation, human interpretability, and editability. Because of its importance in creative applications, there has been a long but active history of research on tracing vector graphic representation from a raster image [28, 14, 2, 20, 26].
In this work, we study a generative model of vector graphic documents. While raster-based generative models show tremendous progress in synthesizing high-quality im-ages [10, 24, 12], there has been relatively scarce studies on vector graphic documents [33, 3, 17]. Although both raster and vector graphics deal with images, vector graphics do not have canvas pixels and cannot take advantage of the current mainstream approach of convolutional neural net-works without rasterization, which is typically not differen-tiable [19]. Learning a generative model of vector graphics therefore imposes us unique challenges in 1) how to repre-sent complex data structure of vector graphic formats in a unified manner, 2) how to formulate the learning problem, and 3) how to evaluate the quality of documents.
We address the task of generative learning of vec-tor graphics using a variational auto-encoder (VAE) [13], where we define documents by a multi-modal combination of canvas attributes and a sequence of element attributes.
Unlike conditional layout inference [33, 15, 17], we con-sider unconditional document generation including both a canvas and variable number of elements. Our architecture, named CanvasVAE, learns an encoder that projects a given graphic document into a latent code, and a decoder that re-constructs the given document from the latent code. We adopt Transformer-based network [5] in both the encoder and the decoder to process variable-length sequence of el-ements in a document. The learned decoder can take ran-domly sampled latent code to generate a new vector graphic document. For our study, we collect a large-scale dataset of design templates for our study that offers complete doc-ument structure and content information. In evaluation, we propose to combine normalized metrics for all attributes to measure the overall quality of reconstruction and genera-tion. We compare several variants of CanvasVAE architec-ture and show that a Transformer-based model constitutes a strong baseline for the vector graphic generation task.
We summarize our contributions in the following. 1. We propose the CanvasVAE architecture for the task of unconditional generative learning of vector graphic documents, where we model documents by a struc-tured, multi-modal set of canvas and element at-tributes. 2. We build Crello dataset, which is a dataset consisting of large number of design templates and features com-plete vector information including occluded elements. 3. We empirically show that our Transformer-based vari-ant of CanvasVAE achieves a strong performance in both document reconstruction and generation. 2.