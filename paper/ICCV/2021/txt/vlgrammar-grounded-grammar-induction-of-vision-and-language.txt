Abstract
Cognitive grammar suggests that the acquisition of lan-guage grammar is grounded within visual structures. While grammar is an essential representation of natural language, it also exists ubiquitously in vision to represent the hi-erarchical part-whole structure. In this work, we study grounded grammar induction of vision and language in a joint learning framework. Speciﬁcally, we present VLGram-mar, a method that uses compound probabilistic context-free grammars (compound PCFGs) to induce the language grammar and the image grammar simultaneously. We pro-pose a novel contrastive learning framework to guide the joint learning of both modules. To provide a benchmark for the grounded grammar induction task, we collect a large-scale dataset, PARTIT, which contains human-written sen-tences that describe part-level semantics for 3D objects.
Experiments on the PARTIT dataset show that VLGram-mar outperforms all baselines in image grammar induction and language grammar induction. The learned VLGram-mar naturally beneﬁts related downstream tasks. Specif-ically, it improves the image unsupervised clustering ac-curacy by 30%, and performs well in image retrieval and text retrieval. Notably, the induced grammar shows superior generalizability by easily generalizing to unseen categories.
Code and pre-trained models are released at https:// github.com/evelinehong/VLGrammar. 1.

Introduction
Natural and man-made dynamical systems tend to have a nested multi-scale organization, which might be a general property of all physical and biological systems. According to [37], building complex stable systems requires the re-use of stable sub-systems that can be assembled to build larger systems. Therefore, exploring the low-dimensional structures in sensory data is critical for understanding the world and helping the design, interpretation, and general-ization of artiﬁcial intelligent systems. Similarly, inducing the underlying structures and grammars from raw sensory inputs, e.g., vision and language [11, 51, 39, 23, 38, 5, 40,
Figure 1: An example of a sentence parse tree aligned with an image parse tree. The arrow lines represent production rules of the image grammar and the language grammar. The dashed lines represent alignment between the constituents of two modalities. 54, 48, 32, 14, 26, 15], has been a long-standing challenge in the ﬁeld of artiﬁcial intelligence (AI).
With the development of unsupervised learning tech-niques, the unsupervised grammar induction for natural language [33, 34, 21, 20] has recently made satisfying progress. These works formulate the grammar induction of language as a self-contained system that relies solely on tex-tual corpora. Following this trend, [35, 53] propose the vi-sually grounded grammar induction. They empirically show that if the constituents in a sentence’s parse tree are well aligned with the image that the sentence describes, the in-duced grammar will be more accurate.
Visually grounded grammar induction takes one step fur-ther towards cognitive grammar [24, 25], a concept from
linguistic theory. Cognitive grammar argues that it is point-less to analyze grammatical units without reference to their semantics, which is grounded and structured by patterns of perception, such as vision. However, previous works ground all the constituents of a sentence with the embedding of a single image [35, 53]. They focus on aligning the image feature to language grammar but miss the hierarchical struc-tures in the image. This is inconsistent with cognitive gram-mar’s notion that a constituent’s semantic value does not reside in one individual image base, but rather in the rela-tionship between the substructure and the base.
Part-whole relationships are crucial in semantic struc-tures [16]. For example, the constituent “two arms” in Fig. 1 does not simply refer to a chair, but instead refers to the chair’s arms. Thus, it is necessary to align the language grammar with the hierarchical structures in physical ob-jects. As shown in Fig. 1, a visual object can be parsed into parts with hierarchical structures, and constituents that de-scribe parts of an object can be naturally grounded with the parts at different hierarchies.
While the study of the hierarchical structure of images has a long history [11, 51, 32, 14, 13, 45, 54], the structure is mainly pre-deﬁned by human and static across images.
Therefore, challenges remain as: (1) how to represent ﬂex-ible part-whole hierarchies that vary with images using an identical network [14], and (2) how to learn structure auto-matically without pre-deﬁned templates. One possible way is to learn the image grammar that parses an object into parts. Instead of allocating neurons to represent nodes in the parse graph, we can use neurons to represent grammar rules. The grammar rules are general for all the images and can be recursively re-used to handle arbitrarily complicated objects (e.g., a chair can have an arbitrary number of legs).
Inspired by the above ideas, we present VLGrammar, a framework that jointly learns image and language gram-mar. Speciﬁcally, we use compound probabilistic context-free grammars (compound PCFGs), which parameterize a
PCFG’s rule probabilities with neural networks and relax the context-free constraints with a latent compound vari-able. To achieve grounded learning, we calculate an align-ment score between the image parse tree and the language parse tree, and use a contrastive loss to learn the compound
PCFGs for both image and language jointly.
To obtain data that contains multi-modal part-whole in-formation for learning grounded grammars, we collect a large-scale dataset, PARTIT, which contains 10,613 manu-ally annotated descriptive sentences paired with the images of objects and parts. The sentences collected via Amazon
Mechanic Turk (AMT) describe the detailed object and part semantics for 3D objects.
Experiments on the proposed PARTIT dataset show that our proposed VLGrammar outperforms all baselines in both image grammar induction and language grammar induction.
Moreover, it naturally beneﬁts related downstream tasks, for example, improving the accuracy of unsupervised part clus-tering from „40% to „70%, and achieving better perfor-mance in the image-text retrieval tasks. Our image gram-mar trained on chair and table can be easily general-ized to unseen categories such as bed and bag. Qualitative studies also show that our method is capable of predicting part-whole hierarchies and recursive structures of objects, as well as constituency parsing of sentences.
Our contributions can be summarized as follows:
• To benchmark the grounded grammar induction prob-lem, we collect a large-scale dataset, PARTIT, which contains human-written sentences describing both object-level and part-level semantics for 3D objects.
• We propose VLGrammar, which utilizes compound
PCFGs to induce grounded grammars for both vision and language by enforcing the cross-modal alignment.
• We conduct extensive experiments on the PARTIT dataset. The results demonstrate the superiority of the proposed VLGrammar on the grammar induction and downstream tasks, such as unsupervised part clustering and image-text retrieval. VLGrammar also shows great generalization ability on unseen object categories. 2.