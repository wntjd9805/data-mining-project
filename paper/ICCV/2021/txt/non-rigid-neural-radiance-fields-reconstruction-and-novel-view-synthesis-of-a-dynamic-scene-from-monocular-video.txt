Abstract 1.

Introduction
We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes
RGB images of a dynamic scene as input (e.g., from a monocular video recording), and creates a high-quality space-time geometry and appearance representation. We show that a single handheld consumer-grade camera is suf-ficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, e.g. a ‘bullet-time’ video effect. NR-NeRF disentangles the dynamic scene into a canonical volume and its deformation. Scene deforma-tion is implemented as ray bending, where straight rays are deformed non-rigidly. We also propose a novel rigidity net-work to better constrain rigid regions of the scene, leading to more stable results. The ray bending and rigidity net-work are trained without explicit supervision. Our formula-tion enables dense correspondence estimation across views and time, and compelling video editing applications such as motion exaggeration. Our code will be open sourced.
Free viewpoint rendering is a well-studied problem due to its wide range of applications in movies and vir-tual/augmented reality [72, 9, 47]. In this work, we are in-terested in dynamic scenes, which change over time, from novel user-controlled viewpoints. Traditionally, multi-view recordings are required for free viewpoint rendering of dy-namic scenes [94, 82, 55]. However, such multi-view cap-tures are expensive and cumbersome. We would like to en-able the setting in which a casual user records a dynamic scene with a single, moving consumer-grade camera. Ac-cess to only a monocular video of the deforming scene leads to a severely under-constrained problem. Most existing ap-proaches thus limit themselves to a single object category, such as the human body [23, 87, 31] or face [12]. Some ap-proaches allow for the reconstruction of general non-rigid objects [99, 17, 32, 68], but most methods only reconstruct the geometry without the appearance of the objects in the scene. In contrast, our objective is to reconstruct a general dynamic scene, including its appearance, such that it can be rendered from novel spatio-temporal viewpoints.
Recent neural rendering approaches have shown impres-sive novel-view synthesis of general static scenes from multi-view input [78]. These approaches represent scenes using trained neural networks and rely on less constraints about the type of scene, compared to traditional approaches.
The closest prior work to our method is NeRF [45], which learns a continuous volume of the scene encoded in a neu-ral network using multiple camera views. However, NeRF assumes the scene to be static. Neural Volumes [40] is an-other closely related approach that uses multiple views of a deforming scene to enable free viewpoint rendering. How-ever, it uses a fixed-size voxel grid to represent the recon-struction of the scene, restricting the resolution. In addi-tion, it requires multi-view input for training, which limits the applicability to in-the-wild outdoor settings or existing monocular footage. Our new neural rendering approach in-stead targets the more challenging setting of using just a monocular video of a general dynamic scene. Due to the non-rigidity, each image of the video records a different, de-formed state of the scene, violating the constraints of stan-dard neural rendering approaches. Our approach disentan-gles the observations in any image into a canonical scene and its deformations, without direct supervision on either.
We tackle this problem using several innovations. We represent the non-rigid scene by two components: (1) a canonical neural radiance field for capturing geometry and appearance and (2) the scene deformation field. The canon-ical volume is a static representation of the scene encoded as a Multi-Layered Perceptron (MLP), which is not directly supervised. This volume is deformed into each individ-ual image using the estimated scene deformation. Specifi-cally, the scene deformation is implemented as ray bending, where straight camera rays can deform non-rigidly. The ray bending is modeled using an MLP that takes point samples on the ray as well as a latent code for each image as in-put. Both the ray bending and the canonical scene MLPs are jointly trained using the monocular observations. Since the ray bending MLP deforms the entire space independent of camera parameters, we can render the deforming volume from static or time-varying novel viewpoints after training.
The ray bending MLP disentangles the geometry of the scene from the scene deformations. The disentanglement is an underconstrained problem, which we tackle with fur-ther innovations. Our method assigns a rigidity score to every point in the canonical volume, which allows for the deformations to not affect the static regions in the scene.
This rigidity component is jointly learned without any di-rect supervision. We also introduce multiple regularizers as additional soft-constraints: A regularizer on the deforma-tion magnitude of the visible deformations encourages only sparse deformations of the volume, and thus helps to con-strain the canonical volume. An additional divergence reg-ularizer preserves the local shape, thereby constraining the
Figure 2. We bend straight rays ¯r from the deformed volume using a deformation-dependent ray-bending network b′ and a deformation-independent rigidity network w into a single static canonical neural radiance field volume v. representation of hidden (partially occluded) regions that are not visible throughout the full video.
Our results show high-fidelity reconstruction and novel view synthesis for a wide range of non-rigid scenes. Fig. 2 contains an overview of our method. To summarize, our main technical contributions are as follows:
• A free viewpoint rendering method, NR-NeRF, that only requires a monocular video of the dynamic scene (Sec. 3).
The spatio-temporal camera trajectory for test-time novel view synthesis can differ significantly from the trajectory of the input video. Moreover, we can extract dense corre-spondences relating arbitrary (input or novel) frames.
• A rigidity network which can segment the scene into non-rigid foreground and rigid background without being di-rectly supervised (Sec. 3.2).
• Regularizers on the estimated deformations which con-strain the problem by encouraging small volume preserv-ing deformations (Sec. 3.3).
• Several extensions for handling of view dependence and multi-view data, and applications of our technique for simple scene editing (Secs. 3).
We compare NR-NeRF to several methods for neural novel view rendering (Sec. 4). See our supplementary video for visualizations and Sec. 5 for a discussion. 2.