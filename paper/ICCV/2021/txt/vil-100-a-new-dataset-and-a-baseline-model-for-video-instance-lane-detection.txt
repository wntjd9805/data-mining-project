Abstract
Lane detection plays a key role in autonomous driving.
While car cameras always take streaming videos on the way, current lane detection works mainly focus on individ-ual images (frames) by ignoring dynamics along the video.
In this work, we collect a new video instance lane detection (VIL-100) dataset, which contains 100 videos with in total 10,000 frames, acquired from different real trafﬁc scenar-ios. All the frames in each video are manually annotated to a high-quality instance-level lane annotation, and a set of frame-level and video-level metrics are included for quanti-tative performance evaluation. Moreover, we propose a new baseline model, named multi-level memory aggregation net-work (MMA-Net), for video instance lane detection. In our approach, the representation of current frame is enhanced by attentively aggregating both local and global memory features from other frames. Experiments on the new col-lected dataset show that the proposed MMA-Net outper-forms state-of-the-art lane detection methods and video ob-ject segmentation methods. We release our dataset and code at https://github.com/yujun0-0/MMA-Net. 1.

Introduction
In recent years, autonomous driving has received numer-ous attention in both academy and industry. One of the most
*Yujun Zhang and Lei Zhu are the joint ﬁrst authors of this work.
†Wei Feng (wfeng@ieee.org) is the corresponding author. fundamental and challenging task is lane detection in traf-ﬁc scene understanding. Lane detection assists car driving and could be used in advanced driving assistance system (ADAS) [28, 27, 42]. However, accurately detecting lanes in real trafﬁc scenarios is very challenging, due to many harsh scenarios, e.g., severe occlusion, bad weather condi-tions, dim or dazzle light.
With the advancement of deep learning, lane detection has achieved signiﬁcant progress in recent years by annotat-ing and training on large-scale real data [25, 37, 14, 30, 33].
However, most of the existing methods are focused on image-based lane detection, while in autonomous driving, car camera always collects streaming videos. It is highly desirable to extend deep-learning based lane detection from image level to video level since the latter can leverage tem-poral consistency to resolve many in-frame ambiguities, such as occlusion, lane damage etc. The major obstacle for this extension is the lack of a video dataset with ap-propriate annotations, both of which are essential for deep network training. Existing lane datasets (e.g., TuSimple
[43], Culane [33], ApolloScape [15] and BDD100K [49]) either support only image-level detection or lack temporal instance-level labels. However, according to the United Na-tions Regulation No.157 [1] for autonomous and connected vehicles, continuous instance-level lane detection in videos is indispensable for regular/emergency lane change, trajec-tory planning, autonomous navigation, etc.
To address above issues, in this work, we ﬁrst collect a new video instance lane detection (VIL-100) dataset (see
Figure 1 for examples). It contains 100 videos with 10,000 frames, covering 10 common line-types, multiple lane in-stances, various driving scenarios, different weather and lighting conditions. All the video frames are carefully an-notated with high-quality instance-level lane masks, which could facilitate the community to explore further in this
ﬁeld. Second, we develop a new baseline model, named multi-level memory aggregation network (MMA-Net).
Our MMA-Net leverages both local memory and global memory information to enhance multiple CNN features ex-tracted from the current key frame. To be speciﬁc, we take past frames of the original video to form a local memory and past frames of a shufﬂed ordered video as a global memory, and the current video frame as the query is segmented using features extracted from video frames of both local memory and global memory. A local and global memory aggregation (LGMA) module is devised to attentively aggregate these multi-level memory features, and then all CNN features are integrated together to pro-duce the video instance lane detection results. Finally, we present a comprehensive evaluation of 10 state-of-the-art models on our VIL-100 dataset, making it the most complete video instance-level lane detection bench-mark. Results show that our MMA-Net signiﬁcantly out-performs existing methods, including single-image lane de-tectors [25, 37, 14, 30, 33], and video instance object seg-mentation methods [18, 50, 32, 24, 44]. 2.