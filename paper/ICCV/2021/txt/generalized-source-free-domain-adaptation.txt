Abstract
Domain adaptation (DA) aims to transfer the knowledge learned from a source domain to an unlabeled target domain.
Some recent works tackle source-free domain adaptation (SFDA) where only a source pre-trained model is available for adaptation to the target domain. However, those methods do not consider keeping source performance which is of high practical value in real world applications. In this paper, we propose a new domain adaptation paradigm called General-ized Source-free Domain Adaptation (G-SFDA), where the learned model needs to perform well on both the target and source domains, with only access to current unlabeled target data during adaptation. First, we propose local structure clustering (LSC), aiming to cluster the target features with its semantically similar neighbors, which successfully adapts the model to the target domain in the absence of source data. Second, we propose sparse domain attention (SDA), it produces a binary domain speciﬁc attention to activate dif-ferent feature channels for different domains, meanwhile the domain attention will be utilized to regularize the gradient during adaptation to keep source information. In the exper-iments, for target performance our method is on par with or better than existing DA and SFDA methods, speciﬁcally it achieves state-of-the-art performance (85.4%) on VisDA, and our method works well for all domains after adapting to single or multiple target domains. Code is available in https://github.com/Albert0147/G-SFDA. 1.

Introduction
Though achieving great success, deep neural networks typically require a large amount of labeled data for training.
However, collecting labeled data is often laborious and ex-pensive. To tackle this problem, Domain Adaptation (DA) methods aim to transfer knowledge learned from label-rich datasets (source domains) to other unlabeled datasets (tar-*Corresponding Author. get domains), by reducing the domain shift between labeled source and unlabeled target domains.
A crucial requirement in most DA methods is that they require access to the source data during adaptation, which is often impossible in many real-world applications, such as deploying domain adaptation algorithms on mobile devices where the computation capacity is limited, or in situations where data-privacy rules limit access to the source domain.
Because of its relevance and practical interest, the source-free domain adaptation (SFDA) setting, where instead of source data only source pretrained model is available, has started to get traction recently [14, 15, 18, 20, 46]. Among these methods, SHOT [20] and 3C-GAN [18] are most re-lated to this paper which is for close-set DA where source and target domains have the same categories. 3C-GAN [18] is based on target-style image generation by a conditional
GAN, and SHOT [20] proposes to transfer the source hy-pothesis, i.e. the ﬁxed source classiﬁer, to the target data, together with maximizing mutual information.
However, in many practical situations models should per-form well on both the target and source domain. For example, we would desire a recognition model deployed in an urban environment which works well for all four seasons (domains) after adapting model to the seasons sequentially. As shown in [47], the source performance of some DA methods will de-grade after adaptation even with source data always at hand.
And the current SFDA methods focus on the target domain by ﬁne tuning the source model, leading to forgetting on old domains. Thus, existing methods cannot handle the situation described above. A simple way to address this setting is by just storing the source and target model, however, we aim for memory-efﬁcient solutions that scale sub-linear with the number of domains. Therefore, in this paper, we propose a new DA paradigm where the model is expected to perform well on all domains after source-free domain adaptation. We call this setting Generalized Source-free Domain Adaptation (G-SFDA). For simplicity, in the paper we will ﬁrst focus on a single target domain, and then we describe how to extend to Continual Source-free Domain Adaptation.
In this paper, to perform adaptation to the target domain without source data, we ﬁrst propose Local Structure Cluster-ing (LSC), that clusters each target feature together with its nearest neighbors. The motivation is that one target feature should have similar prediction with its semantic close neigh-bors. To keep source performance, we propose to use sparse domain attention (SDA), applied to the output of the feature extractor, activating different feature channels depending on the particular domain. The source domain attention will be used to regularize the gradient during target adaptation to prevent forgetting of source information. With LSC and
SDA, the adapted model can achieve excellent performance on both source and target domains. In the experiments, we show that for target performance our method is on par with or better than existing DA and SFDA methods on several benchmarks, speciﬁcally achieving state-of-the-art perfor-mance on VisDA (85.4%), while simultaneously keeping good source performance. We also extend our method to
Continual Source-free Domain Adaptation, where there is more than one target domain, further demonstrating the efﬁ-ciency of our method.
We summarize our contributions as follows:
• We propose a new domain adaptation paradigm denoted as Generalized Source-free Domain Adaptation (G-SFDA), where the source-pretrained model is adapted to target domains while keeping the performance on the source domain, in the absence of source data.
• We propose local structure clustering (LSC) to achieve source-free domain adaptation, which utilizes local neighbor information in feature space.
• We propose Sparse domain attention (SDA) which acti-vates different feature channels for different domains, and regularizes the gradient of back propagation dur-ing target adaptation to keep information of the source domain.
• In experiments, we show that where existing methods suffer from forgetting and obtain bad performance on the source domain, our method is able to maintain source domain performance. Furthermore, when fo-cusing on the target domain our method is on par with or better than existing methods, especially we achieve state-of-the-art target performance on VisDA. 2.