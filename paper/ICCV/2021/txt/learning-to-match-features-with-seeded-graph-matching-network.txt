Abstract
Matching local features across images is a fundamen-tal problem in computer vision. Targeting towards high ac-curacy and efficiency, we propose Seeded Graph Matching
Network, a graph neural network with sparse structure to reduce redundant connectivity and learn compact represen-tation. The network consists of 1) Seeding Module, which initializes the matching by generating a small set of reli-able matches as seeds. 2) Seeded Graph Neural Network, which utilizes seed matches to pass messages within/across images and predicts assignment costs. Three novel opera-tions are proposed as basic elements for message passing: 1) Attentional Pooling, which aggregates keypoint features within the image to seed matches. 2) Seed Filtering, which enhances seed features and exchanges messages across im-ages. 3) Attentional Unpooling, which propagates seed fea-tures back to original keypoints. Experiments show that our method reduces computational and memory complexity sig-nificantly compared with typical attention-based networks while competitive or higher performance is achieved. 1.

Introduction
Establishing reliable correspondences across images is an essential step to recover relative camera pose and scene structure in many computer vision tasks, sush as Structure-from-Motion (SfM) [37], Multiview Stereo(MVS) [17] and
Simultaneous Localization and Mapping (SLAM) [31]. In classical pipelines, correspondences are obtained by nearest neighbour search (NN) of local feature descriptors and are usually further pruned by heuristic tricks, such as mutual nearest neighbour check (MNN) and ratio test (RT) [24].
In the past few years, great efforts have been made on designing learnable matching strategy. Early works [60, 43, 62] in this direction utilize PointNet [32]-like networks to reject outliers of putative correspondences. In these works, the correspondence coordinates are fed into permutation-equivariant networks, then inlier likelihood scores are pre-dicted for each correspondence. Despite showing exciting results, these methods are limited in two aspects: 1) They
Figure 1. The designs of message passing layer. (a) SuperGlue densely connects every node in the graph, resulting in O(N 2) (b) Instead, the proposed network, computational complexity.
SGMNet, adopts pooling/unpooling operations, reducing the com-plexity to O(N K), where K ≪ N in general. operate on pre-matched correspondences, whereas finding more matches than vanilla nearest-neighbour matching is impossible. 2) They only reason about the geometric distri-bution of putative correspondences, neglecting the critical information of original local visual descriptors.
Another thread of methods cast feature matching as a graph matching problem [35, 5, 49], which mitigates the limit of vanilla nearest neighbour correspondences. The representative work, SuperGlue [35], constructs densely-connected graphs between image keypoints to exchange messages about both visual and geometric context. How-ever, the superior performance comes along with high com-putation and memory cost, especially when being applied to keypoints of larger number (e.g. up to 10k). As illus-trated in Fig. 1(a), the message passing layer of SuperGlue first calculates similarity scores exhaustively between every two nodes, then gathers features to pass messages densely in the graph. This results in computational complexity of
O(N 2C) for matrix multiplications, and memory occupa-tion of O(N 2) to hold the attention matrix, supposing that the keypoint number is N and feature channel is C. The complexity increases even drastically for deeper graph net-works. Given this, exploring more efficient and compact message passing operation is of practical significance.
Besides the major efficiency bottleneck, it is debatable
if such densely-connected graph introduces too much re-dundancy or insignificant message exchange that may hin-der the representation ability, especially in the context of feature matching where the match set is highly outlier-contaminated and a large portion of keypoints are unrepeat-able. As a result, most graph edges from SuperGlue [35] tend to have zero strength, as reported in its original paper and also observed in our experiments. This phenomenon in-dicates that even a sparse graph is largely sufficient and less distracted from unnecessary message exchanges.
In this paper, we propose Seeded Graph Matching Net-work (SGMNet) to mitigate above limitations from two as-pects. First, inspired by guided matching approaches [10, 42, 29], we design a Seeding Module that initializes the matching from a small set of reliable matches so as to more effectively identify inlier compatibility. Second, we draw inspiration from graph pooling operations [61, 55], and con-struct a Seeded Graph Neural Network whose graph struc-ture is largely sparsified to lower the computation and re-duce the redundancy. Specifically, three operations are pro-posed to construct our message passing blocks. As illus-trated in Fig. 1(b), instead of densely attending to all fea-tures within/across images, original keypoint features are first pooled by 1) Attentional Pooling through a small set of seed nodes, of which the features will be further enhanced by 2) Seed Filtering, and finally recovered back to original keypoints through 3) Attentional Unpooling.
By using seeds as attention bottleneck between images, the computational complexity for attention is reduced from
O(N 2C) to O(N KC), where K is the number of seeds.
When K ≪ N , for example, 8k features are pooled into 512 seeds, the actual computation will be significantly cut down. We evaluate SGMNet under different tasks to demonstrate both its efficiency and effectiveness, and sum-marize our contributions threefold:
• A seeding mechanism is introduced in graph matching framework to effectively identify inlier compatibility.
• A greatly sparsified graph neural network is designed that enables more efficient and clean message passing.
• Competitive or higher accuracy is reported with remark-ably improved efficiency over dense attentional GNN. As an example, when matching 10k features, SGMNet runs 7 times faster and consumes 50% less GPU memory than
SuperGlue. 2.