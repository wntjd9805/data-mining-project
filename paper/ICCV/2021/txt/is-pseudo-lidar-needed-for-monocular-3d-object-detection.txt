Abstract
Recent progress in 3D object detection from single im-ages leverages monocular depth estimation as a way to produce 3D pointclouds, turning cameras into pseudo-lidar sensors. These two-stage detectors improve with the accu-racy of the intermediate depth estimation network, which can itself be improved without manual labels via large-scale self-supervised learning. However, they tend to suffer from overﬁtting more than end-to-end methods, are more com-plex, and the gap with similar lidar-based detectors remains signiﬁcant. In this work, we propose an end-to-end, single stage, monocular 3D object detector, DD3D, that can ben-eﬁt from depth pre-training like pseudo-lidar methods, but without their limitations. Our architecture is designed for effective information transfer between depth estimation and 3D detection, allowing us to scale with the amount of unla-beled pre-training data. Our method achieves state-of-the-art results on two challenging benchmarks, with 16.34% and 9.28% AP for Cars and Pedestrians (respectively) on the KITTI-3D benchmark, and 41.5% mAP on NuScenes. 1.

Introduction
Detecting and accurately localizing objects in 3D space is crucial for many applications, including robotics, au-tonomous driving, and augmented reality. Hence, monoc-ular 3D detection is an active research area [42, 57, 39, 69], owing to its potentially wide-ranging impact and the ubiq-uity of cameras. Leveraging exciting recent progress in depth estimation [11, 13, 15, 16, 31], pseudo-lidar detec-tors [69, 40, 57] ﬁrst use a pre-trained depth network to compute an intermediate pointcloud representation, which is then fed to a 3D detection network. The strength of pseudo-lidar methods is that they monotonically improve with depth estimation quality, e.g., thanks to large scale training of the depth network on raw data.
*equal contribution
Code: https://github.com/TRI-ML/dd3d
Figure 1: We introduce a single-stage 3D object detector,
DD3D, that combines the best of both pseudo-lidar meth-ods (scaling with depth pre-training) and end-to-end meth-ods (simplicity and generalization performance). Our detec-tor features a simple training protocol of depth pre-training and detection ﬁne-tuning, compared to pseudo-lidar meth-ods that require an additional depth ﬁne-tuning step and tend to overﬁt to depth errors.
However, regressing depth from single images is inher-ently an ill-posed inverse problem. Consequently, errors in depth estimation account for the major part of the gap between pseudo-lidar and lidar-based detectors, a problem compounded by generalization issues that are not fully un-derstood yet [56]. Simpler end-to-end monocular 3D detec-tors [3, 39] seem like a promising alternative, although they do not enjoy the same scalability beneﬁts of unsupervised pre-training due to their single stage nature.
In this work, we aim to get the best of both worlds: the scalability of pseudo-lidar with raw data and the simplicity and generalization performance of end-to-end 3d detectors.
To this end, our main contribution is a novel fully convo-lutional single-stage 3D detection architecture, DD3D (for
Dense Depth-pre-trained 3D Detector), that can effectively leverage monocular depth estimation for pre-training (see
Figure 1). Using a large corpus of unlabeled raw data, we show that DD3D scales similarly to pseudo-lidar methods, and that depth pre-training improves upon pre-training on large labeled 2D detection datasets like COCO [37], even with the same amount of data.
Our method sets a new state of the art on the task of monocular 3D detection on KITTI-3D [14] and nuScenes [5] with signiﬁcant improvements compared to previous state-of-the-art methods. The simplicity of its training procedure and its end-to-end optimization allows for effective use of large-scale depth data, leading to im-pressive multi-class detection accuracy. 2.