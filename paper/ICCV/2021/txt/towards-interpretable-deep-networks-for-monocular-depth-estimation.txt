Abstract
Deep networks for Monocular Depth Estimation (MDE) have achieved promising performance recently and it is of great importance to further understand the interpretability of these networks. Existing methods attempt to provide post-hoc explanations by investigating visual cues, which may not explore the internal representations learned by deep networks. In this paper, we find that some hidden units of the network are selective to certain ranges of depth, and thus such behavior can be served as a way to interpret the inter-nal representations. Based on our observations, we quan-tify the interpretability of a deep MDE network by the depth selectivity of its hidden units. Moreover, we then propose a method to train interpretable MDE deep networks without changing their original architectures, by assigning a depth range for each unit to select. Experimental results demon-strate that our method is able to enhance the interpretability of deep MDE networks by largely improving the depth se-lectivity of their units, while not harming or even improving the depth estimation accuracy. We further provide compre-hensive analysis to show the reliability of selective units, the applicability of our method on different layers, models, and datasets, and a demonstration on analysis of model er-ror. Source code and models are available at https:// github.com/youzunzhi/InterpretableMDE. 1.

Introduction
Monocular Depth Estimation (MDE) has drawn a lot of attention since it is critical for further applications like 3D scene understanding or autonomous driving, due to the less requirement and cost compared to depth estimation using stereo image pairs. Eigen et al. [10] first utilize convolu-*Corresponding author is Guanbin Li.
Figure 1. Visualization of feature maps. (a) and (b) refer to the feature map visualization of Unit 5 in layer MFF and Unit 26 in layer D (cf. Section 5) of [18] (ResNet-50), respectively. (c) and (d) refer to Unit 63 in layer D and Unit 0 in layer MFF of the interpretable counterpart trained by our method, respectively (best viewed in color). We show that (b) has activations over different depth ranges, while our results in (c) and (d) focus on distant or close depth, which allows more interpretability of the model. tional neural networks to perform MDE; since then numer-ous approaches based on deep neural networks have been proposed and significantly improve state-of-the-art perfor-mance [13, 18, 42, 26]. However, only few studies fo-cus on the interpretability of these MDE networks [46].
Since depth estimation can be closely related to downstream tasks like autonomous driving, the lack of interpretability on
MDE models could potentially cause critical consequences.
Figure 2. A comparison of the accuracy drop rate when units are ablated successively in different orders. The units are sorted by their depth selectivity and then successively ablated in two re-versed order. The accuracy in the y-axis drops faster when units with the higher selectivity are ablated before the less selective ones.
In general, understanding deep networks is of great ne-cessity. Previous works on the interpretability of deep net-works for vision mainly focus on image classification [44, 2] or image generation [3]. On depth estimation, Hu et al. [20] and Dijk et al. [9] analyze how deep networks es-timate depth from single images by investigating the visual cues in input images, on the level of pixels or semantics, respectively. However, they still treat the networks as black boxes, resulting in less exploration of the internal represen-tations learned by the MDE networks.
In addition, such post-hoc explanations may not present the whole story of interpretable machine learning models as discussed in [33].
Although there exists interpretable models for computer vi-sion tasks, such as image classification [45, 5], object detec-tion [41] or person re-identification [28], these tasks have quite a different characteristics from MDE and are not di-rectly applicable to MDE.
Recently, numerous methods try to discover what neu-rons in neural networks look for [30, 2, 11, 32]. It is shown that neuron units generally extract features that can be inter-preted as various levels of semantic concept, from textures and patterns to objects and scenes. Moreover, to learn in-terpretable neural networks, one option is to disentangle the representations learned by internal filters, which makes the filters more specialized [45, 27]. Inspired by these works, we observe that in deep MDE networks, some hidden units are selective to some ranges of depth. For example, in
Fig. 1(a), we visualize several feature maps of one unit in a layer of the network from [18]. This unit is obviously more activated in the distant regions of the input images. We fur-ther dissect the units by collecting their averaged response
Figure 3. Dissection results on units. (a) and (b) are units of layer
D and layer MFF in [18] (ResNet-50), where it shows diverse ranges of selectivity. Using our proposed interpretable model, we consistently increase the selectivity over all the units, e.g., (c) and (d), which improves the model interpretability. on depth ranges (see Section 3.1 later for more details), and
Fig. 3(a) shows that for some units, activations are higher for some certain ranges of depth.
To quantify this observation, we then compute its depth selectivity for each unit (detailed in Section 3.2). To evalu-ate the meaningfulness of depth selectivity, we successively ablate units and see how the performance of the network drops accordingly. We first sort the 128 units of the net-work from [18] by their selectivity and then successively ablate units from the most selective unit to the least one, and then do the same thing similarly in the reversed way.
In Fig. 2, the performance of the MDE model drops much quicker when more selective units are ablated earlier than less selective ones. Based on the observations stated above, we argue that for an MDE deep network, a unit is more im-portant when it is more depth selective, and the behavior of its units can be interpreted by telling which ranges of depth
activated most by those units. Therefore, the interpretability of a deep network for MDE can be quantified by the depth selectivity of its internal units.
However, in the existing MDE model, despite that some units can be interpreted as being selective for some ranges of depth, most of them have little interpretability. For ex-ample, Fig. 1(b) and Fig. 3(b) show feature map visual-izations and dissection results of typical units in the net-work from [18], which have less interpretability. Therefore, to achieve an MDE model with better interpretability, we propose a simple yet effective interpretable deep network for MDE by maximizing the selectivity of internal units.
Our method can be applied to existing deep MDE networks without modifying their original architectures or requiring any additional annotations. More importantly, we show that it is possible to learn our interpretable model without harm-ing its depth performance, which creates potential discus-sions in explainable AI along the trade-off between inter-pretability and model performance [34]. The experimental results show that the our interpretable models achieve com-petitive or even better performance than the original MDE models, while the interpretability is largely improved.
Contributions. To summarize, this work has the follow-ing contributions: (1) we quantify the interpretability of deep networks for MDE based on the depth selectivity of models’ internal units; (2) we propose a novel method to learn interpretable deep networks for MDE without mod-ifying the original network’s architecture or requiring any additional annotations; (3) we empirically show that our method effectively improves the interpretability of deep
MDE networks, while not harming or even improving the depth accuracy, and further validate the reliability and ap-plicability of the proposed method. 2.