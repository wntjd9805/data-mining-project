Abstract
Many video analysis tasks require temporal localization for the detection of content changes. However, most existing models developed for these tasks are pre-trained on general video action classification tasks. This is due to large scale annotation of temporal boundaries in untrimmed videos be-ing expensive. Therefore, no suitable datasets exist that en-able pre-training in a manner sensitive to temporal bound-aries.
In this paper for the first time, we investigate model pre-training for temporal localization by introducing a novel boundary-sensitive pretext (BSP) task. Instead of re-lying on costly manual annotations of temporal boundaries, we propose to synthesize temporal boundaries in existing video action classification datasets. By defining different ways of synthesizing boundaries, BSP can then be simply conducted in a self-supervised manner via the classification of the boundary types. This enables the learning of video representations that are much more transferable to down-stream temporal localization tasks. Extensive experiments show that the proposed BSP is superior and complemen-tary to the existing action classification-based pre-training counterpart, and achieves new state-of-the-art performance on several temporal localization tasks. Please visit our web-site for more details https://frostinassiky.github.io/bsp. 1.

Introduction
Recently, the focus on video analysis has shifted be-yond trimmed video action classification to video tempo-ral localization. This is because in many real-world ap-plications, instead of short (e.g., few seconds long) video clips, long, untrimmed videos are often presented (e.g., from social media websites as YouTube, Instagram) with both non-interesting background and foreground contained e.g., a particular action of interest. This requires a video model
*Work done during an internship at Samsung AI Centre.
Figure 1. Pre-training datasets for different tasks. The well-established pre-training-then-fine-tuning paradigm for image and video classification model optimization is effective thanks to the availability of large related datasets (e.g., ImageNet and Kinetics).
For video temporal localization tasks however, existing datasets are either too small for model pre-training or less effective due to the lack of temporal boundary annotations. We solve this problem by introducing a novel boundary-sensitive pretext (BSP) task. to conduct temporal localization tasks. Examples of these tasks include temporal action localization [65, 5], video grounding [2, 44] and step localization [79].
As in most other visual recognition tasks, recent mod-els designed for video temporal localization are based on deep learning. As such, model pre-training is critical. In particular, a two-staged model training strategy is com-monly adopted [19, 56, 18, 4]: first, a video encoder is pre-trained on a large action classification dataset (e.g., Ki-netics [6]1, Sports-1M [28]), then, a temporal localization head is trained on the target small-scale temporal localiza-1Kinetics was created by trimming 10s clips around a frame manually labelled as containing the target action. Therefore the rest of the untrimmed video cannot be treated as background.
tion dataset leaving the video encoder fixed. Thus, there is a clear mismatch between the pre-training of the video en-coder and the target task. Ideally, model pre-training should be carried out on temporal boundary-sensitive tasks. How-ever, this is not possible due to the lack of large scale video datasets with temporal boundary annotation. This is be-cause temporal boundaries’ labeling is much more expen-sive and tedious than video-level class labeling due to the need for manually examining every single frame2.
In this paper, we investigate the under-studied yet crit-ical problem of model pre-training for temporal localiza-tion in videos. Due to the difficulties in collecting a large-scale video dataset with temporal boundary annotations, we propose to synthesize large-scale untrimmed videos with boundary annotations by transforming the existing trimmed video action classification datasets. Once the pre-training data problem is tackled, we focus on defining and evaluat-ing a number of pretext tasks capable of exploiting the par-ticularities of the synthesized data through self-supervision.
In particular, the first key challenge boils down to how to obtain large scale training video data with temporal bound-ary information in a scalable and cheap manner. To that end, we introduce a simple yet effective method for generating three types of temporal boundary at large scale using ex-isting action classification video data (e.g.Kinetics). More specifically, we generate artificial temporal boundaries cor-responding to video content changes by either stitching trimmed videos containing different classes, stitching two video instances of the same class, or by manipulating the speed of different parts of a video instance. The associated pretext task used to train the video model uses supervised classification learning, where the task is to distinguish be-tween the types of temporal boundaries as defined above.
We experimentally show that such task offers superior per-formance to other possible pretext tasks, such as regressing the temporal boundary location, and that combining the dif-ferent boundary types into a multi-class classification prob-lem is superior to all binary classification tasks in isolation.
The following contributions are made in this work: (I)
We investigate the problem of model pre-training for tem-poral localization tasks in videos, which is largely under-studied yet particularly significant to video analysis. (II) We propose a scalable video synthesis method that can gener-ate a large number of videos with temporal boundary infor-mation. This approach not only solves the key challenge of lacking large pre-training data, but also facilitates the design (III) Extensive experiments show of model pre-training. that temporal action localization, video grounding, and step localization tasks can significantly benefit from the pro-posed model pre-training, yielding compelling or new state-of-the-art performance on a number of benchmark datasets. 2Boundary annotations are 3.8× more expensive than class annota-tions [23, 75] 2.