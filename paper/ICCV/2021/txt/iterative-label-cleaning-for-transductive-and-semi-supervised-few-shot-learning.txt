Abstract
Few-shot learning amounts to learning representations and acquiring knowledge such that novel tasks may be solved with both supervision and data being limited. Improved performance is possible by transductive inference, where the entire test set is available concurrently, and semi-supervised learning, where more unlabeled data is available.
Focusing on these two settings, we introduce a new algo-rithm that leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iteratively improving the quality of pseudo-labels. Our solu-tion surpasses or matches the state of the art results on four benchmark datasets, namely miniImageNet, tieredImageNet,
CUB and CIFAR-FS, while being robust over feature space pre-processing and the quantity of available data. The publicly available source code can be found in https:
//github.com/MichalisLazarou/iLPC 1.

Introduction
Few-shot learning [61, 56] is challenging the deep learn-ing paradigm in that, not only supervision is limited, but data is limited too. Despite the initial promise of meta-learning [39, 12], transfer learning [10, 59] is becoming in-creasingly successful in decoupling representation learning from learning novel tasks on limited data. Semi-supervised learning [30, 5] is one of the dominant ways of dealing with limited supervision and indeed, its few-shot learning coun-terparts [50, 66] are miniature versions where both labeled and unlabeled data are limited proportionally, while repre-sentation learning may be decoupled. These methods are closer to transductive inference [36, 51], which was a pillar of semi-supervised learning before deep learning [8].
Predicting pseudo-labels on unlabeled data [30] is one of the oldest ideas in semi-supervised learning [54]. Graph-based methods, in particular label propagation [68, 67], are prominent in transductive inference and translate to inductive inference in deep learning exactly by predicting pseudo-support S 1 feature extraction 2 nearest neighbor graph 3 label propagation queries Q f 7 iteration 6 support set augmentation 5 label cleaning 4 class balancing
Figure 1. Overview of the proposed method. See text for details. labels [22]. However, with the representation being fixed, the quality of pseudo-labels is critical in few-shot learning [63, 29]. At the same time, in learning with noisy labels [3, 21, 57], it is common to clean labels based on the loss value statistics of a small-capacity classifier.
In this work, we leverage these ideas to improve trans-ductive and semi-supervised few-shot learning. As shown in Figure 1, focusing on transduction, a set of labeled sup-port examples S and unlabeled queries Q are given, rep-resented in a feature space by mapping f . By label prop-agation [67], we obtain a matrix that associates examples to classes. The submatrix corresponding to unlabeled ex-amples, P , is normalized over examples and classes using the Sinkhorn-Knopp algorithm [24], assuming a uniform distribution over classes. We extract pseudo-labels from P , which we clean following O2U-Net [21], keeping only one example per class. Finally, inspired by [26], we move these examples from Q to S and iterate until Q is empty. 2.