Abstract
Contrastive learning has been widely used to train transformer-based vision-language models for video-text alignment and multi-modal representation learning. This paper presents a new algorithm called Token-Aware
Cascade contrastive learning (TACo) that improves con-trastive learning using two novel techniques. The ﬁrst is the token-aware contrastive loss which is computed by taking into account the syntactic classes of words. This is mo-tivated by the observation that for a video-text pair, the content words in the text, such as nouns and verbs, are more likely to be aligned with the visual contents in the video than the function words. Second, a cascade sampling method is applied to generate a small set of hard nega-tive examples for efﬁcient loss estimation for multi-modal fusion layers. To validate the effectiveness of TACo, in our experiments we ﬁnetune pretrained models for a set of downstream tasks including text-video retrieval (YouCook2,
MSR-VTT and ActivityNet), video action step localization (CrossTask), video action segmentation (COIN). The results show that our models attain consistent improvements across different experimental settings over previous methods, set-ting new state-of-the-art on three public text-video retrieval benchmarks of YouCook2, MSR-VTT and ActivityNet. 1.

Introduction
Aligning or grounding language to videos is a challeng-ing topic in the context of vision-language (VL) research as it requires the model to understand contents, dynamics, and causality presented in videos [3]. Inspired by the suc-cess of BERT [10] in natural language processing, there is a growing interest in applying transformer-based multi-modal models for video-text alignment and representation learn-ing [40, 39, 59, 32, 14, 27]. These models are typically pretrained on large amounts of noisy video-text pairs using contrastive learning [34, 33], and then applied in a zero-shot manner or ﬁnetuned for various downstream tasks, such as text-video retrieval [51], video action step localiza-tion [60], video action segmentation [42], video question
Figure 1: The proposed token-aware cascade contrastive learning pipeline. We compute three contrastive losses: 1) sentence-level loss L1 over all negative examples; 2) token-level loss L2 on content words (noun, verb) over all nega-tive examples; 3) sentence-level loss L3 over hard negative examples sampled based on L1 and L2 online. answering [43, 26] and video captioning [57].
In this paper, we present a new variant of con-trastive learning, Token-Aware Cascade contrastive learn-ing (TACo) to improve the video-text alignment for both large-scale pretraining and downstream speciﬁc tasks. As the name indicates, TACo makes two modiﬁcations to the conventional contrastive learning used in video-language domain. The ﬁrst is the token-aware contrastive loss which is computed by taking into account the syntactic classes of words. This is motivated by the observation that, given a video and its corresponding text, content words, such as nouns and verbs, are more likely than function words to be aligned with (or grounded to) visual contents in the video. Conventional contrastive learning typically compute the loss after aggregating over all the words in the text and frames in the video (loss L1 or L3 in Fig. 1). In contrast, the token-aware contrastive loss is computed using only a subset of words whose syntactic classes belong to a pre-deﬁned set (e.g., nouns and verbs), which forces the ground-ing of individual words to the video (loss L2). For example, we pay particular attention to the words “add”, “tomatos”,
“pan” and “stir” in Fig. 1.
The second technique we introduce is a cascade sam-pling method to ﬁnd a small set of hard negative exam-ples for training the multi-modal fusion layers. Consider a batch of K video-text pairs. For each of the video-text pairs, the ideal case is that we use the remaining K − 1 negative videos or texts to compute the contrastive loss af-ter multi-modal fusion. However, the cost of computing the contrastive loss quickly becomes prohibitive when it is cou-pled with multi-modal fusion layers, considering its high complexity O(K 2 × L2) where L is total number of visual and textual tokens. A conventional way to address this is using random sampling to select a small subset of negative pairs. In this paper, instead of random sampling, we pro-pose a cascade sampling method as shown in the top-right of Fig. 1 to efﬁciently select a small set of hard negative examples on the ﬂy during training. It leverages the video-text alignment scores computed in L1 and L2 before multi-modal fusion layers, and helps to learn the multi-modal fu-sion layers more effectively without any extra overhead.
We perform a comprehensive empirical study to val-idate the effectiveness of TACo in both pretraining and dataset-speciﬁc scenarios. We apply TACo and different variants of contrastive losses to train or pretrain and ﬁne-tune on various downstream tasks including text-video re-trieval (YouCook2, MSR-VTT and ActivityNet) [57, 51, 12], video action step localization (CrossTask) [60] and ac-tion segmentation (COIN) [42]. Our results show that TACo improves the text-video retrieval performance over current state-of-the-art across three benchmarks. Furthermore, the learned multi-modal representation and video representa-tion can be effectively transferred to CrossTask and COIN, and achieve better or comparable performance to current state-of-the-art methods. 2.