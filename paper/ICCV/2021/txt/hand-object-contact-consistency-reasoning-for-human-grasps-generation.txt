Abstract
While predicting robot grasps with parallel jaw grippers have been well studied and widely applied in robot manipula-tion tasks, the study on natural human grasp generation with a multi-ﬁnger hand remains a very challenging problem. In this paper, we propose to generate human grasps given a 3D object in the world. Our key observation is that it is crucial to model the consistency between the hand contact points and object contact regions. That is, we encourage the prior hand contact points to be close to the object surface and the object common contact regions to be touched by the hand at the same time. Based on the hand-object contact consistency, we design novel objectives in training the human grasp gen-eration model and also a new self-supervised task which allows the grasp generation network to be adjusted even dur-ing test time. Our experiments show signiﬁcant improvement in human grasp generation over state-of-the-art approaches by a large margin. More interestingly, by optimizing the model during test time with the self-supervised task, it helps achieve larger gain on unseen and out-of-domain objects.
Project page: https://hwjiang1510.github.io/GraspTTA/. 1.

Introduction
Capturing hand-object interactions has been an active
ﬁeld of study [48, 20, 30, 13, 2, 4, 47, 42, 57] and it has wide applications in virtual reality [21, 55], human-computer interaction [51] and imitation learning in robotics [59, 49, 39]. In this paper, we study the interactions via generation:
*Equal contribution.
As shown in Fig. 1, given only a 3D object in the world coordinate, we generate the 3D human hand for grasping it.
Unlike predicting robot grasps with parallel jaw grippers [33, 56, 60, 5], predicting human grasps is substantially more difﬁcult because: (i) Human hands have a lot more degrees of freedom, which leads to much more complex contact; (ii) The generated grasp needs to be not only physically plausible but also presented in a natural way, consistent with how objects are usually grasped.
To synthesize physically plausible and natural grasp poses, recent works propose to use generative models [9, 25, 47] supervised by large-scale datasets [20, 19, 16] with grasp annotations and contact analysis on hands. Speciﬁcally, the large-scale dataset allows the model to generate realistic human grasps and the contact analysis encourages the hand contact points to be close with the object but without inter-penetration. While these methods put a lot of efforts into modeling the hand and its contact points, they ignore that the object itself also has more possible contact regions that need to be reached (see contact map in Fig. 1). In fact, recent work has studied the common contact regions on objects and trained neural networks to directly predict the contact map from the 3D object model [2, 4].
In this paper, we argue that it is critical for the hand contact points and object contact regions to reach mutual agreement and consistency for grasp generation. To achieve this, we propose to unify two separate models for both the hand grasp synthesis and object contact map estimation. We show that the consistency constraint between hand contact points and object contact map is not only useful for opti-mizing better grasps during training time by designing new
losses, but also provides a self-supervised task to adjust the grasp when testing on a novel object. We introduce the two components as follows.
First, we train a Conditional Variational Auto-Encoder [44] (CVAE) based network which takes the 3D object point clouds as inputs and predicts the hand grasp parameterized by a MANO model [41], namely GraspCVAE.
During training the GraspCVAE, we design two novel losses with one encouraging the hand to touch the object surface and another forcing the object contact regions touched by the ground truth hand close to the predicted hand. With these two consistent losses, we observe more realistic and physically plausible grasps.
Second, given the hand grasp pose and object point clouds as inputs, we train another network that predicts the contact map on the object. We name this model the ContactNet.
The key role of the ContactNet is to provide supervision to ﬁnetune GraspCVAE during test time when no ground truth is available. We design a self-supervised consistency task, which requires the hand contact points produced by the GraspCVAE to be consistent and overlapped with the object contact map predicted by the ContactNet. We use this self-supervised task to perform test-time adaptation which
ﬁnetunes the GraspCVAE to generate a better human grasp.
This adaptation approach can be applied on each single test instance. We emphasize that this procedure does not require any extra outside supervision and it can ﬂexibly adapt to different inputs by resuming to the model before adaptation.
We evaluate our approach on multiple datasets include
Obman [20], HO-3D [19] and FPHA [16] datasets. We show that by utilizing the novel objectives based on the contact con-sistency constraints in training time, we achieve signiﬁcant improvements on human grasps generation against state-of-the-art approaches. More interestingly, by optimizing with the proposed self-supervised task during test time, it gener-alizes and adapts our model to unseen and out-of-domain objects, leading to the large performance gain.
Our contributions of this paper include: (i) Novel hand-object contact consistency constraints for learning human grasp generation; (ii) A new self-supervised task based on the consistency constraints which allows the generation model to be adjusted even during test time; (iii) Signiﬁcant im-provement on grasp generation for both in-domain and out-of-domain objects. 2.