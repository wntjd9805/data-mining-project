Abstract
The application of light ﬁeld data in salient object de-tection is becoming increasingly popular recently. The difﬁ-culty lies in how to effectively fuse the features within the fo-cal stack and how to cooperate them with the feature of the all-focus image. Previous methods usually fuse focal stack features via convolution or ConvLSTM, which are both less effective and ill-posed. In this paper, we model the infor-mation fusion within focal stack via graph networks. They introduce powerful context propagation from neighbouring nodes and also avoid ill-posed implementations. On the one hand, we construct local graph connections thus avoiding prohibitive computational costs of traditional graph net-works. On the other hand, instead of processing the two kinds of data separately, we build a novel dual graph model to guide the focal stack fusion process using all-focus pat-terns. To handle the second difﬁculty, previous methods usu-ally implement one-shot fusion for focal stack and all-focus features, hence lacking a thorough exploration of their sup-plements. We introduce a reciprocative guidance scheme and enable mutual guidance between these two kinds of in-formation at multiple steps. As such, both kinds of features can be enhanced iteratively, ﬁnally beneﬁting the saliency prediction. Extensive experimental results show that the proposed models are all beneﬁcial and we achieve signif-icantly better results than state-of-the-art methods. 1.

Introduction
Salient object detection (SOD) methods can be catego-rized into RGB based ones, RGB-D based ones, and the recently proposed light ﬁeld based ones. By only based on static images, although RGB SOD methods [18, 26, 24, 54] have achieved excellent performance on many benchmark datasets, they still can not handle challenging and complex
*Equal contribution.
†Corresponding author.
Figure 1. (a) and (b) show the comparison of traditional dense graph models and our proposed local graph model. (c) illustrates the framework of our model. R(cid:13) means the reciprocative unit. scenes. This is because the appearance saliency cues con-veyed in RGB images are heavily constrained, especially when the foreground and background appearance are com-plex or similar. To solve this problem, depth information is introduced to provide supplementary cues in RGB-D SOD methods [3, 52, 31, 27]. However, it is not easy to obtain high-quality depth maps and many current RGB-D SOD benchmark datasets only have noisy depth maps. On the contrary, light ﬁeld data Hence, the light ﬁeld SOD prob-lem has much potential to explore.
Besides the focal stack images, light ﬁeld data also have an all-focus image that provides the context information.
Thus, light ﬁeld SOD has two key points, i.e., how to ef-fectively fuse multiple focal stack features and how to co-operate the focal stack cues with the all-focus information.
A straightforward way to solve the ﬁrst problem is to con-catenate focal stack features and use a convolution layer for fusion. Such a simple way can not sufﬁciently explore the complex interaction within different focal slices, hence may limit the model performance. It is also an ill-posed solu-tion since convolution requires a ﬁxed input number, hence many methods have to randomly pad the input images when
they are less than the pre-deﬁned number. Adopting ConvL-STM [43] is another popular solution, where the focal stack images are processed one by one in a pre-deﬁned sequential order using the memory mechanism. This also involves an ill-posed problem setting since there is no meaningful order among focal stack images. Furthermore, the usage of the sequential order may cause ConvLSTM to ignore the infor-mation of the focal slices that are input earlier. As for the second problem, most previous works [38] simply concate-nate or sum the focal stack feature with the all-focus fea-ture and then adopt convolutional fusion only once. Such a straightforward fusion method heavily limits the explo-ration of complex supplementary relations between these two kinds of information.
To solve the ﬁrst problem, adopting the powerful graph neural networks (GNNs) [14, 35] is a possible way. GNNs aggregate the contextual information from neighbouring nodes and propagate it to the target node, thus can achieve effective feature fusion. At the same time, it avoids the ill-posed implementation problem since the graph connection can be built ﬂexibly and does not depend on a sequential or-der. A straightforward way is to view each pixel location in the feature maps of the focal stack as a node and construct dense edge connections among all locations, as shown in
Figure 1(a). However, this is impractical since light ﬁeld
SOD requires large feature maps for focal slices to obtain
ﬁne-grained segmentation. Hence, building a densely con-nected graph involves prohibitive computational costs.
To this end, we propose to build local graphs to efﬁ-ciently aggregate contexts in different focal slices. We treat each image pixel location in the focal stack as nodes and build the graph only within local neighbouring nodes, as shown in Figure 1(b). As such, the context propagation within focal slices can be efﬁciently performed with dramat-ically reduced edge connections. One can further introduce multiscale local neighbours, hence incorporating larger con-text information with acceptable computational costs. Be-sides building a graph within the focal stack, we also build a focal-all graph to introduce external guidance from the all-focus feature for the fusion of focal features, thus resulting in a novel dual local graph (DLG) network.
To tackle the second key point in light ﬁeld SOD, we pro-pose a novel reciprocative guidance architecture, as shown in Figure 1(c). It introduces multi-step guidance between the all-focus image feature and the focal stack features. In each step, the former is ﬁrst used to guide the fusion of the latter, and then the fused feature is used to update the for-mer. We perform such a process in a reciprocative fashion, where mutual guidance can be conducted recurrently. Fi-nally, the two kinds of features can be improved with more discriminability, beneﬁting the ﬁnal SOD decision.
Our main contributions can be summarised as:
• We propose a new GNN model named dual local graph to enable effective context propagation in focal stack features under the guidance of the all-focus feature and also avoid high computational costs.
• We propose a novel reciprocative guidance scheme to make the focal stack and the all-focus features guide and promote each other at multiple steps, thus gradu-ally improving the saliency detection performance.
• Extensive experiments illustrate the effectiveness of our method. It surpasses other light ﬁeld methods by a large margin. Moreover, with much less training data, our method also shows competitive or better per-formance compared with RGB-D or RGB based SOD models. 2.