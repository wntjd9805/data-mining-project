Abstract
Object recognition has made great advances in the last decade, but predominately still relies on many high-quality training examples per object category. In contrast, learning new objects from only a few examples could enable many impactful applications from robotics to user personalization.
Most few-shot learning research, however, has been driven by benchmark datasets that lack the high variation that these applications will face when deployed in the real-world. To close this gap, we present the ORBIT dataset and benchmark, grounded in the real-world application of teachable object recognizers for people who are blind/low-vision. The dataset contains 3,822 videos of 486 objects recorded by people who are blind/low-vision on their mobile phones. The benchmark reflects a realistic, highly challenging recognition problem, providing a rich playground to drive research in robustness to few-shot, high-variation conditions. We set the bench-mark’s first state-of-the-art and show there is massive scope for further innovation, holding the potential to impact a broad range of real-world vision applications including tools for the blind/low-vision community. We release the dataset at https://doi.org/10.25383/city.14294597 and benchmark code at https://github.com/microsoft/ORBIT-Dataset. 1.

Introduction
Object recognition systems have made spectacular ad-vances in recent years [42, 47, 43, 37, 14, 30, 36] however, most systems still rely on training datasets with 100s to 1,000s of high-quality, labeled examples per object category.
These demands make training datasets expensive to collect, and limit their use to all but a few application areas.
Few-shot learning aims to reduce these demands by train-ing models to recognize completely novel objects from only a few examples [9, 49, 40, 2, 38, 11, 46]. This will enable recognition systems that can adapt in real-world, dynamic scenarios, from self-driving cars to applications where users provide the training examples themselves. Meta-learning algorithms which “learn to learn” [45, 9, 49, 11] hold partic-(a) Frames from clean videos (b) Frames from clutter videos
Figure 1: High-variation examples in the ORBIT dataset – a facemask, hairbrush, keys, and watering can. Full videos in the supplementary material. Further examples in Figure A.5. ular promise toward this goal with recent advances opening exciting possibilities for light-weight, adaptable recognition.
Most few-shot learning research, however, has been driven by datasets that lack the high variation — in number of examples per object and quality of those examples (fram-ing, blur, etc.; see Table 1) — that recognition systems will likely face when deployed in the real-world. Key datasets such as Omniglot [23, 49] and miniImageNet [49], for exam-ple, present highly structured benchmark tasks which assume a fixed number of objects and training examples per object.
Meta-Dataset [48], another key dataset, poses a more chal-lenging benchmark task of adapting to novel datasets given a small (random) number of training examples. Its constituent datasets [23, 17, 39, 26, 32, 50, 6], however, mirror the high-quality images of Omniglot and miniImageNet, leav-ing robustness to the noisy frames that would be streamed from a real-world system unaddressed. While these datasets have catalyzed research in few-shot learning, state-of-the-art performance is now relatively saturated and leaves reduced scope for algorithmic innovation [16, 4, 33].
To drive further innovation in few-shot learning for real-world impact, there is a strong need for datasets that capture the high variation inherent in real-world applications. We
motivate that both the dataset and benchmark task should be grounded in a potential real-world application to bring real-world recognition challenges to life in their entirety. An application area that neatly encapsulates a few-shot, high-variation scenario are teachable object recognisers (TORs) for people who are blind/low-vision [24, 18]. Here, a user can customize an object recognizer by capturing a small number of (high-variation) training examples of essential objects on their mobile phone. The recognizer is then trained (in deployment) on these examples such that it can recognize the user’s objects in novel scenarios. As a result, TORs capture a microcosm of highly challenging and realistic conditions that can be used to drive research in real-world recognition tasks, with the potential to impact a broad range of applications beyond just tools for the blind/low-vision community.
We introduce the ORBIT dataset [31], a collection of videos recorded by people who are blind/low-vision on their mobile phones, and an associated few-shot benchmark grounded in TORs. Both were designed in collaboration with a team of machine learning (ML), human-computer in-teraction, and accessibility researchers, and will enable the
ML community to 1) accelerate research in few-shot, high-variation object recognition, and 2) explore new research directions in few-shot video recognition. We intend both as a rich playground to drive research in robustness to chal-lenging, real-world conditions, a step beyond what curated few-shot datasets and structured benchmark tasks can offer, and to ultimately impact a broad range of real-world vision applications. In summary, our contributions are: 1. ORBIT benchmark dataset. The ORBIT bench-mark dataset [31] (Section 3) is a collection of 3822 videos of 486 objects recorded by 77 blind/low-vision people on their mobile phones and can be downloaded at https://doi.org/10.25383/city.14294597. Examples are shown in Figures 1 and A.5. Unlike existing datasets [39, 8, 26, 49, 48], ORBIT show objects in a wide range of real-istic conditions, including when objects are poorly framed, occluded by hands and other objects, blurred, and in a wide variation of backgrounds, lighting, and object orientations. 2. ORBIT teachable object recognition benchmark.
We formulate a few-shot benchmark on the ORBIT dataset (Section 4) that is grounded in TORs for people who are blind/low-vision. Contrasting existing few-shot (and other) works, the benchmark proposes a novel user-centric formu-lation which measures personalization to individual users.
It also incorporates metrics that reflect the potential compu-tational cost of real-world deployment on a mobile device.
These and the benchmark’s other metrics are specifically designed to drive innovation for realistic settings. 3. State-of-the-art (SOTA) on the ORBIT benchmark.
We implement 4 few-shot learning models that cover the main classes of approach in the field, extend them to videos, and establish the first SOTA on the ORBIT benchmark (Sec-tion 5). We also perform empirical studies showing that training on existing few-shot learning datasets is not suffi-cient for good performance on the ORBIT benchmark (Ta-ble 4) leaving significant scope for algorithmic innovation in few-shot techniques that can handle high-variation data.
Code for loading the dataset, computing bench-mark metrics, and running the baselines is available at https://github.com/microsoft/ORBIT-Dataset. 2.