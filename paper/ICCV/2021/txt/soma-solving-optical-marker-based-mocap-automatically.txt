Abstract
Marker-based optical motion capture (mocap) is the
“gold standard” method for acquiring accurate 3D human motion in computer vision, medicine, and graphics. The raw output of these systems are noisy and incomplete 3D points or short tracklets of points. To be useful, one must associate these points with corresponding markers on the captured subject; i.e. “labelling”. Given these labels, one can then “solve” for the 3D skeleton or body surface mesh.
Commercial auto-labeling tools require a specific calibra-tion procedure at capture time, which is not possible for archival data. Here we train a novel neural network called
SOMA, which takes raw mocap point clouds with varying numbers of points, labels them at scale without any calibra-tion data, independent of the capture technology, and re-quiring only minimal human intervention. Our key insight is that, while labeling point clouds is highly ambiguous, the 3D body provides strong constraints on the solution that can be exploited by a learning-based method. To enable learn-ing, we generate massive training sets of simulated noisy and ground truth mocap markers animated by 3D bodies from AMASS. SOMA exploits an architecture with stacked self-attention elements to learn the spatial structure of the 3D body and an optimal transport layer to constrain the assignment (labeling) problem while rejecting outliers. We extensively evaluate SOMA both quantitatively and quali-tatively. SOMA is more accurate and robust than existing state of the art research methods and can be applied where commercial systems cannot. We automatically label over 8 hours of archival mocap data across 4 different datasets captured using various technologies and output SMPL-X body models. The model and data is released for research purposes at https://soma.is.tue.mpg.de/. 1.

Introduction
Marker-based optical motion capture (mocap) systems record 2D infrared images of light reflected or emitted by a set of markers placed at key locations on the surface of a subject’s body. Subsequently, the mocap systems recover the precise position of the markers as a sequence of sparse and unordered points or short tracklets. Powered by years of commercial development, these systems offer high tem-poral and spatial accuracy. Richly varied mocap data from such systems is widely used to train machine learning meth-ods in action recognition, motion synthesis, human motion modeling, pose estimation, etc. Despite this, the largest ex-isting mocap dataset, AMASS [28], has about 45 hours of mocap, much smaller than video datasets used in the field.
Mocap data is limited since capturing and processing it is expensive. Despite its value, there are large amounts of archival mocap in the world that have never been labeled;
this is the “dark matter” of mocap. The problem is that, to solve for the 3D body, the raw mocap point cloud (MPC) must be “labeled”; that is, the points must be assigned to physical “marker” locations on the subject’s body. This is challenging because the MPC is noisy and sparse and the labeling problem is ambiguous. Existing commercial tools, e.g. [21, 27], offer partial automation, however none provide a complete solution to automatically handle vari-ations in marker layout, i.e. number of markers used and their rough placement on the body, variation in subject body shape and gender, and variation across capture technolo-gies namely active vs passive markers or brands of system.
These challenges typically preclude cost-effective labeling of archival data, and add to the cost of new captures by re-quiring manual cleanup.
Automating the mocap labeling problem has been ex-amined by the research community [14, 16, 19]. Existing methods focus on fixing the mistakes in already labeled markers through denoising [8, 19]. Recent work formulates the problem in a matching framework, directly predicting the label assignment matrix for a fixed number of markers
In short, the existing methods in a restricted setup [14]. are limited to a constrained range of motions [14], a single body shape [8, 16, 19], a certain capture scenario, a spe-cial marker layout, or require a subject-specific calibration sequence [14, 21, 27]. Other methods require high-quality real mocap marker data for training, effectively prohibiting their scalability to novel scenarios [8, 14].
To address these shortcomings we take a data-driven ap-proach and train a neural network end-to-end with self-attention components and an optimal transport layer to pre-dict a per-frame constrained inexact matching between mo-cap points and labels. Having enough “real” data for train-ing is not feasible, therefore we opt for synthetic data.
Given a marker layout, we generate synthetic mocap point clouds with realistic noise, and then train a layout-specific network that can cope with realistic variations across a whole mocap dataset. While previous works have exploited synthetic data [16, 19], they are limited in terms of body shapes, motions, marker layouts, and noise sources.
Even with a large synthetic corpus of MPC, labeling a cloud of sparse 3D points, containing outliers and missing data, is a highly ambiguous task. The key to the solution lies in the fact that the points are structured, as is their variation with articulated pose. Specifically, they are constrained by the shape and motion of the human body. Given sufficient training data, our attentional framework learns to exploit lo-cal context at different scales. Furthermore, if there were no noise, the mapping between labels and points would be one-to-one. We formulate these concepts as a unified train-ing objective enabling end-to-end model training. Specif-ically, our formulation exploits a transformer architecture to capture local and global contextual information using self-attention (Sec. 4.1). By generating synthetic mocap data with varying body shapes and poses, SOMA implic-itly learns the kinematic constraints of the underlying de-formable human body (Sec. 4.4). A one-to-one match be-tween 3D points and markers, subject to missing and spuri-ous data, is achieved by a special normalization technique (Sec. 4.2). To provide a common output framework, consis-tent with [28], we use MoSh [26, 28] as a post-processing step to fit SMPL-X [34] to the labeled points; this also helps deal with missing data caused by occlusion or dropped markers. The SOMA system is outlined in Fig. 3.
To generate training data, SOMA requires a rough marker layout that can be obtained by a single labeled frame, which requires minimal effort. Afterward, virtual markers are automatically placed on a SMPL-X body and animated by motions from AMASS [28].
In addition to common mocap noise models like occlusions [14, 16, 19], and ghost points [16, 19], we introduce novel terms to vary maker placement on the body surface and we copy noise from real marker data in AMASS (Sec. 4.4). We train
SOMA once for each mocap dataset and apart from the one layout frame, we do not require any labeled real data. After training, given a noisy MPC frame as input, SOMA predicts a distribution over labels of each point, including a null la-bel for ghost points.
We evaluate SOMA on several challenging datasets and find that we outperform the current state of the art numer-ically while being much more general. Additionally, we capture new MPC data using a Vicon mocap system and compare hand-labeled ground-truth to Sh¯ogun and SOMA output. SOMA performs similarly compared with the com-mercial system. Finally, we apply the method on archival mocap datasets: Mixamo [10], DanceDB [4], and a previ-ously unreleased portion of the CMU mocap dataset [11].
In summary, our main contributions are: (1) a novel neu-ral network architecture exploiting self-attention to process sparse deformable point cloud data; (2) a system that con-sumes mocap point clouds directly and outputs a distribu-tion over marker labels; (3) a novel synthetic mocap gener-ation pipeline that generalizes to real mocap datasets; (4) a robust solution that works with archival data, different mo-cap technologies, poor data quality, and varying subjects and motions; (5) 220 minutes of processed mocap data in
SMPL-X format, trained models, and code are released for research purposes. 2.