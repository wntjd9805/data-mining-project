Abstract
Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion represen-tation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representa-tion based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By convert-ing appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representa-tion from it. The proposed neural block, dubbed SELFY, can be easily inserted into neural architectures and trained end-to-end without additional supervision. With a suffi-cient volume of the neighborhood in space and time, it ef-fectively captures long-term interaction and fast motion in the video, leading to robust action recognition. Our exper-imental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementar-ity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 & V2, Diving-48, and FineGym, the pro-posed method achieves the state-of-the-art results. 1.

Introduction
Learning spatio-temporal dynamics is the key to video understanding. While extending standard convolution in space and time has been actively investigated for the pur-pose in recent years [1, 44, 46], the empirical results so far indicate that spatio-temporal convolution alone is not suf-ficient for grasping the whole picture; it often learns irrel-evant context bias rather than motion information [32, 33] and thus the additional use of optical flow turns out to boost the performance in most cases [1,29]. Motivated by this, re-*Equal contribution.
Figure 1: Spatio-temporal self-similarity (STSS) repre-sentation learning. STSS describes each position (query) by its similarities (STSS tensor) with its neighbors in space and time (neighborhood). It allows to take a generalized, far-sighted view on motion, i.e., both short-term and long-term, both forward and backward, as well as spatial self-motion. Our method learns to extract a rich motion repre-sentation from STSS without additional supervision. cent action recognition methods learn to extract explicit mo-tion, i.e., flow or correspondence, between feature maps of adjacent frames to improve the performance [22, 27]. But, is it essential to extract such an explicit form of flows or cor-respondences? How can we learn a richer and more robust form of motion information for videos in the wild?
In this paper, we propose to learn spatio-temporal self-similarity (STSS) representation for video understanding.
Self-similarity is a relational descriptor for an image that ef-fectively captures intra-structures by representing each local region as similarities to its spatial neighbors [37]. As illus-trated in Fig. 1, given a sequence of frames, i.e., a video, it
extends along time and thus represents each local region as similarities to its neighbors in space and time. By convert-ing appearance features into relational values, STSS enables a learner to better recognize structural patterns in space and time. For neighbors at the same frame it computes a spa-tial self-similarity map, while for neighbors at a different frame it extracts a motion likelihood map. Note that if we fix our attention to the similarity map to the very next frame within STSS and attempt to extract a single displacement vector to the most likely position at the frame, the problem reduces to optical flow, which is a limited type of motion information. In contrast, we leverage the whole volume of
STSS and let our model learn to extract a generalized mo-tion representation from it in an end-to-end manner. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.
We introduce a neural block for STSS representation, dubbed SELFY, that can be easily inserted into neural archi-tectures and learned end-to-end without additional supervi-sion. Our experimental analysis demonstrates its superior-ity over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolutions. On the standard benchmarks for action recog-nition, Something-Something V1&V2 [10], Diving-48 [28], and FineGym [36], the proposed method achieves the state-of-the-art results. 2.