Abstract
Several approaches have been proposed in recent liter-ature to alleviate the long-tail problem, mainly in object classification tasks. In this paper, we make the first large-scale study concerning the task of Long-Tail Visual Rela-tionship Recognition (LTVRR). LTVRR aims at improving the learning of structured visual relationships that come from the long-tail (e.g., “rabbit grazing on grass”). In this setup, the subject, relation, and object classes each follow a long-tail distribution. To begin our study and make a future benchmark for the community, we introduce two LTVRR-related benchmarks, dubbed VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets.
We use these benchmarks to study the performance of sev-eral state-of-the-art long-tail models on the LTVRR setup.
Lastly, we propose a visiolinguistic hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top of existing models and despite be-ing simple, our results show that they can remarkably im-prove the performance, especially on tail classes. Bench-marks, code, and models have been made available at: https://github.com/Vision-CAIR/LTVRR. 1.

Introduction
Most existing works in visual recognition assume that training data are abundant, with typically a few hundred to thousands of examples per class [3, 16, 33, 8, 9]. A more realistic setup, however, is to assume that classes follow a long-tail distribution, where most categories have only few examples. What makes the long-tail distribution more nat-ural is that it covers the spectrum of frequent classes, few-shot classes (classes rarely observed in the training set) and
*Equal contribution
†Work done while working at King Abdullah University of Science and
Technology (KAUST)
‡Corresponding Authors
Figure 1: Long-Tail Visual Relationship Recognition (LTVRR). In contrast to existing Visual Relationship Recog-nition (VRR) setups, where each of the subject (S), ob-jects(O), and relationships (R) have abundant examples, In
LTVRR, we focus on such rare relational events where S,R, and O also follow a long-tail distribution which we believe is more realistic and challenging even zero-shot classes (classes that do not appear in the training set). Few-shot and zero-shot learning has been sep-arately studied in [35, 30, 49] and [41, 41, 40], respectively.
LTVRR Several approaches have been developed to ad-vance Long-Tail Object Recognition (LTOR) [20, 22, 39, 36]. However, most of the metrics and evaluation setups in long-tail object recognition do not apply to the Visual
Relationship Recognition (VRR) literature, which is more complex and structured. The goal of the VRR task is to recognize the categories of two interacting objects and their relation, e.g., recognizing triplets like <dog, riding,
In contrast to most existing VRR horse> [23, 28, 47]. benchmarks, all object categories no matter their frequency contribute equally to evaluation metrics in LTOR, where the average per class accuracy is the common metric. In-spired by LTOR literature, we extend their long-tail setup to study visual relationship recognition. In our setup, dubbed
Long-Tail Visual Relationship Recognition (LTVRR), sub-jects, objects, and relationships follow a long-tail distribu-In this setup, this structured recognition tion; see Fig 1. task is more challenging as not only could the combina-tion (S, R, O) be rare, but so can any of the interacting subjects/objects (S/O) and/or the relation (R). An impor-tant distinction between our and previous works is that our focus is on much more long-tailed distributions than previ-ous methods. Most long-tail literature focuses on the range of class frequency that it is on a smaller scale than in our setup (between 5 and 5000 for [22], between 1 and 1000 for [23], and which is around a factor of 1000 between the most frequent and the least frequent classes). On the other hand, for our benchmarks, we use the following range of frequencies: For GQA-LT ( 1, 703 object classes and 310 relation classes), the most frequent object and relationship categories have 374, 282 and 1, 692, 068 examples, and the least frequent have 1 and 2 examples, respectively. This re-sults in factors of around 300, 000+ for objects and around 1.7 million for relations between the most frequent and least frequent classes. For VG8K-LT (5, 330 objects classes and 2000 relation classes), the most frequent object and rela-tionship categories have 196, 944 and 618, 687 examples, and the least frequent have 14 and 18 examples, respec-tively, which leads to factors of approximately 14, 000 for objects and 34, 000 for relations; see more details in Sec. 4
We also implement several state-of-the-art models [36, 20, 13, 22] targetted on long-tail object classification in our
LTVRR setup, which we believe is crucial for further work on this setup. Orthogonally, we also propose a novel aug-mentation technique, dubbed RelMix and a hubless regular-ization loss, introduced in section 3. Inspired from [38], in
RelMix, we augment the training data systematically using a combination of features to improve upon the tail perfor-mance. This effectively helps in augmenting more data for tail classes, hence balancing the head and tail distribution.
We also regularize the model by casting long-tail visual un-derstanding as a hubness problem and introduce a Visio-linguistic Hubless (VilHub) loss. The approach is inspired by hubness literature in Natural Language Processing (e.g.,
[10, 18]) but differs in (a) they use the hubness to improve word-level translation from one language to another, at the same time we model hubness in a visio-lingual task con-necting vision to language. (b) Our approach can correct learning representation that minimizes hubness from deep vision and language neural networks in an end-to-end way in contrast to only correcting bias parameters [10].
Contributions: (1) We adapt several state of the art approaches in long-tail classification to our setup and report the performances on two proposed benchmarks GQA-LT and VG8K-LT. Due to the large vocabulary size of objects and relationships in the
LTVRR setup, we also analyze the models based on their capacity to bring categories that are semantically similar to the ground-truth, higher in the rank of the model’s predictions according to wordNet [26], and word2vec [48].
We found this to be useful, especially when the vocabulary of predictions is large. (2) We propose a novel augmentation method, dubbed
RelMix, for the visual relationship recognition problem.
We empirically show that our augmentation method, while simple, effectively improves the performance across the whole class distribution with more focus on tail classes. (3) We propose to cast the long-tail visual understanding as a hubness problem, and introduce a Visio-linguistic
Hubless (VilHub) loss.We showed that VilHub loss can be simply integrated with some existing losses like Focal Loss (FL) [20] and Weighted Cross Entropy [20] to improve performance as an effective regularizer. 2.