Abstract
We introduce Task Switching Networks (TSNs), a task-conditioned architecture with a single uniﬁed en-coder/decoder for efﬁcient multi-task learning. Multiple tasks are performed by switching between them, perform-ing one task at a time. TSNs have a constant number of pa-rameters irrespective of the number of tasks. This scalable yet conceptually simple approach circumvents the overhead and intricacy of task-speciﬁc network components in ex-isting works.
In fact, we demonstrate for the ﬁrst time that multi-tasking can be performed with a single task-conditioned decoder. We achieve this by learning task-speciﬁc conditioning parameters through a jointly trained task embedding network, encouraging constructive interac-tion between tasks. Experiments validate the effectiveness of our approach, achieving state-of-the-art results on two challenging multi-task benchmarks, PASCAL-Context and
NYUD. Our analysis of the learned task embeddings fur-ther indicates a connection to task relationships studied in the recent literature. 1.

Introduction
The very concept of computer vision is to automatically perform the tasks that a human visual system can do. Even artiﬁcial neural networks (ANNs) were also designed as an inspiration from the biological nervous system, such as the human brain. As opposed to the most successful ANNs, the brain and its visual cortex can perform multiple tasks – such as object, parts, and boundary detection or depth and orientation prediction – without any difﬁculty. Be-ing able to perform multitude of such tasks has allowed humans to efﬁciently conduct complex activities.
In the very spirit, real-world applications like autonomous driving, healthcare, agriculture, manufacturing, cannot be addressed by merely seeking for the perfection on solving individual tasks. It goes without saying that a system capable of per-forming multiple tasks not only has potential of being ef-(a) Single-task (b) Multi-task (c) TC Multi-task (d) Our TSNs
Figure 1: Solutions for multi-task learning. (a) Every task is solved by training an individual network, i.e., us-ing an independent encoder-decoder pair for each task. (b)
General multi-task solutions are built on sharing the en-coder and maintaining separate decoders for each task. (c)
Task-conditional (TC) multi-task solutions [15, 24] are built on sharing partial parameters of the encoder (task-speciﬁc modules also exist), and using separate decoders for each task. (d) In the proposed Task Switching Networks (TSNs), all parameters of a single encoder-decoder pair are shared, and a small task embedding network C facilitates switching between different tasks. Best viewed in color.
ﬁcient in memory usage, computation, and learning speed, but it may also beneﬁt from complementary tasks.
To tackle multi-task learning (MTL), different solutions have been proposed. Encoder-based methods [18, 27, 23] focus on the encoder, by enhancing the representation capability of architectures so that both shared and task-speciﬁc information can be encoded, while decoder-based approaches [47, 44] explore techniques mainly on the de-coder part, to better reﬁne the encoder features for speciﬁc tasks. Optimization-based methods [6, 17, 37] explicitly target on task interference or negative transfer issue from optimization perspective, by re-weighting the loss or re-ordering the task learning. In general, these methods fol-low the structure as Fig. 1 (b). Recently, another direc-tion for MTL emerged, termed task-conditional (TC) multi-tasking [24, 15], shown in Fig. 1 (c). They perform sepa-rate pass within the MTL model and activate a set of task-speciﬁc modules for each task. The task-speciﬁc modules
are used to adapt the network for corresponding tasks. As this setting has many practical use cases [15, 24], our pro-posed TSNs also follow it and execute one task each time.
Despite that promising results are achieved, the existing methods [44, 24, 15] do not scale well with the number of tasks since they require a large number of task-speciﬁc parameters (modules). This can be seen in Fig. 1 (b) and (c), where task-speciﬁc decoders or modules (in the en-coder) scale with the number of tasks. Additionally, even though task-speciﬁc modules minimize adverse interactions amongst tasks, they also minimize positive interactions, i.e., inductive bias [5]. Motivated by these, we propose Task
Switching Networks (TSNs). TSNs share all parameters among all tasks and do not require any task-speciﬁc mod-ules (parameters). Hence, our network is simple and has constant size independent of the number of tasks, while still enabling task interactions. We argue that our motivation is also consistent with the widely accepted view in neurobiol-ogy, that the visual cortex does not have separate modules for different tasks [20, 26].
More speciﬁcally, our task-switching networks solve multiple tasks by switching between them – performing one task at a time, and follows a task-conditional single-encoder-single-decoder architecture. As shown in Fig. 1 (d), the task switching is accomplished by employing a small network to learn task-speciﬁc embeddings from task encodings, and the behaviour of the decoder is adapted by conditioning it on those embeddings. In practice, we con-dition only the decoder (U-Net), in a hope that the encoder learns the concept of ‘thought space’ [36] as it is forced to be task-agnostic. This way, the encoder features can also be reused to efﬁciently perform multiple tasks in series, which is not possible in encoder-based conditioning [24, 15].
Interestingly, the task embedding network offers some insight into the relationships between tasks. During train-ing, a latent embedding for each task is learned together with its corresponding mapping to the conditioning parame-ters in each decoder layer. Though there are still many open questions in the study of task relationships and multi-task learning [40], we observe that the structure of our task em-beddings resemble the task relationships reported in [48].
To summarize, in this work we study multi-task net-works without task-speciﬁc parameters, and investigate the behaviour with regards to efﬁciency, optimization, and ac-curacy. We state our key contributions as follows.
• We introduce Task-Switching Networks, an efﬁcient yet simple architecture for multi-task learning.
• We demonstrate that conditioning a single shared de-coder can outperform multi-decoder methods even on heterogeneous tasks such as segmentation and regres-sion.
• We adopt a small embedding network to learn task conditioning, facilitating optimization and offering in-sights into relationships between tasks. 2.