Abstract
Multi-source Domain Adaptation (MSDA) is more prac-tical but challenging than the conventional unsupervised domain adaptation due to the involvement of diverse mul-tiple data sources. Two fundamental challenges of MSDA are: (i) how to deal with the diversity in the multiple source domains and (ii) how to cope with the data shift between the target domain and the source domains. In this paper, to address the first challenge, we propose a theoretical-guaranteed approach to combine domain experts locally trained on its own source domain to achieve a combined multi-source teacher that globally predicts well on the mix-ture of source domains. To address the second challenge, we propose to bridge the gap between the target domain and the mixture of source domains in the latent space via a generator or feature extractor. Together with bridging the gap in the latent space, we train a student to mimic the pre-dictions of the teacher expert on both source and target ex-amples. In addition, our approach is guaranteed with rigor-ous theory offered insightful justifications of how each com-ponent influences the transferring performance. Extensive experiments conducted on three benchmark datasets show that our proposed method achieves state-of-the-art perfor-mances to the best of our knowledge. 1.

Introduction
Recent advances in deep learning have enjoyed great success in performing visual learning tasks under the col-lection of massive annotated data [26, 64, 50, 54, 3]. How-ever, directly transferring knowledge of a learned model, which is trained on a source domain, to a novel target domain can undesirably degrade its performance due to the existence of domain and label shifts [49]. To ad-dress these issues, a diverse range of approaches in do-main adaptation (DA) has been proposed from shallow do-main adaptation [45, 16, 5, 6] to deep domain adaptation
[13, 32, 51, 12, 55, 9, 29, 41, 40]. While the conventional
DA aims to transfer knowledge from a labeled source do-main to an unlabeled target domain, in many real-world contexts, labeled data are collected from multiple domains, for example, images taken under different conditions (e.g., weather, poses, lighting conditions, distinct backgrounds, and etc) [70]. This has arisen a very practical and use-ful setting for transfer learning named multi-source domain adaptation (MSDA) in which we need to transfer knowledge from multiple distinct source domains to a single unlabeled target domain.
For multi-source domain adaptation, there exist two fun-damental challenges: (i) how to deal with the diversity in the labeled source domains and (ii) how to cope with the domain shift between the target domain and the source do-mains. The first challenge makes it harder to train a single model that is expected to work well on multiple source do-mains due to the requirement to resolve diverge data com-plexity imposed on model training. To overcome this chal-lenge, inspired by [36, 23], we propose combining domain experts into a multi-source teacher by mixing the domain expert predictions using the coefficients learned by a do-main discriminator. Our rigorous theory demonstrates that the performance of this multi-source teacher expert predict-ing globally on the mixture source domains is at least bet-ter than that of the worst domain expert predicting locally on its domain (see Theorem 1). Therefore, if we can train qualified domain experts, their combination leads to another qualified expert with significantly broader coverage.
To address the second challenge, as suggested by The-orem 3, we employ a joint feature extractor that maps the target domain and the mixture of source domains into the same latent space with the help of adversarial learning. Fur-thermore, together with closing the divergence of the target domain and mixture of source domains on the latent space,
we train a target-domain student to imitate the multi-source teacher on both source and target examples while enforcing the clustering assumption [4] on the target-domain student to strengthen the student’s generalization ability.
• We propose an approach named Student-Teacher
Ensemble Multi-source Domain Adaptation (STEM) with theoretical guarantees for multi-source domain adaptation. Not only driving us in devising our STEM, the rigorous theory developed provides us an insight-ful understanding of how each model component really influences the transferring performance.
• We conduct extensive experiments on three bench-mark datasets including Digits-five, Office-Caltech10, and DomainNet. Experimental results show that our
STEM achieves state-of-the-art performances on those three benchmark datasets. More specifically, for
Digits-five and Office-Caltech10 datasets, our STEM wins the baselines on all pairs and surpasses the runner-up baselines by 3.2% and 1.5% on average, while for DomainNet dataset, our STEM wins the runner-up baseline on 5 out of 6 pairs and surpasses the runner-up baseline by 6.0% on average. 2.