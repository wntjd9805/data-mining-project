Abstract
Many real-world ML deployments face the challenge of training a rare category model with a small labeling bud-get. In these settings, there is often access to large amounts of unlabeled data, therefore it is attractive to consider semi-supervised or active learning approaches to reduce human labeling effort. However, prior approaches make two as-sumptions that do not often hold in practice; (a) one has ac-cess to a modest amount of labeled data to bootstrap learn-ing and (b) every image belongs to a common category of interest. In this paper, we consider the scenario where we start with as-little-as five labeled positives of a rare cate-gory and a large amount of unlabeled data of which 99.9% of it is negatives. We propose an active semi-supervised method for building accurate models in this challenging set-ting. Our method leverages two key ideas: (a) Utilize hu-man and machine effort where they are most effective; hu-man labels are used to identify “needle-in-a-haystack” pos-itives, while machine-generated pseudo-labels are used to identify negatives. (b) Adapt recently proposed representa-tion learning techniques for handling extremely imbalanced human labeled data to iteratively train models with noisy machine labeled data. We compare our approach with prior active learning and semi-supervised approaches, demon-strating significant improvements in accuracy per unit la-beling effort, particularly on a tight labeling budget. 1.

Introduction plication might need to recognize a particular type of ap-parel. Ecological monitoring requires recognition of rare animal species. A major challenge of building models for rare categories is acquiring training examples - precisely be-cause they are rare! Fortunately, in many real-world sce-narios one has access to a large amount of unlabeled data.
Naively labeling the unlabeled data is unlikely to find many rare examples without significant human effort. There-fore, a natural strategy is to interactively mine the data by combining [24, 27, 16, 31, 40, 34] active [32] and semi-supervised [6, 46, 36, 43] learning techniques. However, prior active and semi-supervised learning approaches as-sume access to a modest amount of labeled data and assume that every image belongs to a common class of interest (Fig-ure 1). In contrast, we are interested in building models for rare categories starting with only a few labeled positives (as little as 5) and where most (as much as 99.9%) of the unla-beled data is background. Simply put, our goal is to maxi-mize model accuracy given a small fixed amount of human labels (500–1000).
Typical active and semi-supervised approaches use an initial labeled set to train a model for identifying and la-beling relevant data. However, we have so few labeled pos-itives for each rare category that it is difficult to train a deep model using only the initial labeled set. Prior work in the rare category setting resorts to training simple linear mod-els using features from pre-trained deep networks [29, 11].
Unlike these approaches we show that it is feasible to im-prove deep features with a limited amount of labeled data using a combination of active and semi-supervised learning to address the following challenges:
Training image classification models for a single (or small number of) rare category is common in real-world settings. For instance, autonomous vehicle development requires recognizing rare entities, like construction vehi-cles, in the video logs of a large fleet. A shopping ap-1Stanford University 2Carnegie Mellon University 3Argo AI 4Google
Research
• Because humans can only label a small fraction of the full dataset, we make use of semi-supervised learning to pseudo-labels additional examples. Pseudo-labeling pos-itives works poorly because they are rare. We let humans label “hard” examples that may contain positives and hard negatives and let machines pseudo-label “easy” negatives. 1
Figure 1: Problem setup: Most active and semi-supervised learning methods focus on balanced datasets where 100’s-1000’s of labeled examples are available to initialize learning (top). We argue that in practical ML deployment, one often wishes to learn a model for a rare category (where 99.9% of the data is background) from a small number of exemplars (bottom).
• Training on large amounts of pseudo-labeled negatives leads to difficult, highly-imbalanced learning. We adapt background splitting [26], a recently proposed tech-nique for learning from highly imbalanced human la-beled datasets, for iterative semi-supervised learning with both human and machine labels.
• Iteratively learning (deep) features significantly improves accuracy but is computationally expensive and adds sig-nificant delays between model updates and human label-ing. To reduce overall computational costs and latency between model updates and human labeling, we up-date the features at a low frequency but train linear models on cached features at a high frequency to pick the samples to query humans on.
Figure 2 shows an overview of our approach, which syn-thesizes these key ideas into a human-in-loop system for building models for rare categories. Our system provides significant improvements in accuracy per unit labeling ef-fort compared to prior approaches, particularly on a tight labeling budget. 2.