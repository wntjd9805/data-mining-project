Abstract
In this paper, we propose a generalizable mixed-precision quantization (GMPQ) method for efﬁcient in-ference. Conventional methods require the consistency of datasets for bitwidth search and model deployment to guarantee the policy optimality, leading to heavy search cost on challenging largescale datasets in realistic appli-cations. On the contrary, our GMPQ searches the mixed-quantization policy that can be generalized to largescale datasets with only a small amount of data, so that the search cost is signiﬁcantly reduced without performance degradation. Speciﬁcally, we observe that locating net-work attribution correctly is general ability for accurate vi-sual analysis across different data distribution. Therefore, despite of pursuing higher model accuracy and complex-ity, we preserve attribution rank consistency between the quantized models and their full-precision counterparts vi-a efﬁcient capacity-aware attribution imitation for gener-alizable mixed-precision quantization strategy search. Ex-tensive experiments show that our method obtains com-petitive accuracy-complexity trade-off compared with the state-of-the-art mixed-precision networks in signiﬁcant-ly reduced search cost. The code is available at http-s://github.com/ZiweiWangTHU/GMPQ.git. 1.

Introduction
Deep neural networks have achieved the state-of-the art performance across a large number of vision tasks such as image classiﬁcation [15, 44, 18], object detection
[40, 28, 14], face recognition [6, 49, 29] and many other-s. However, the mobile devices with limited storage and computational resources are not capable of processing deep models due to the extremely high complexity. Therefore, it is desirable to design network compression strategy accord-ing to the hardware conﬁgurations.
∗Corresponding author
Figure 1. (a) Conventional methods require the consistency of datasets for bitwidth search and model deployment, while our GM-PQ searches the optimal quantization policy on small datasets and generalizes it to largescale datasets. (b) The attribution computed by Grad-cam for images from ImageNet (top row) and PASCAL
VOC (bottom row). Different from random quantization, the op-timal quantization policy keeps similar attribution with the full-precision counterparts regardless of datasets. ARD means the av-erage Attribution Rank Distance for the top-100 pixels with the highest attribution in the full-precision feature maps.
Recently, several network compression techniques have been proposed including pruning [27, 16, 33], quantiza-tion [59, 30, 51], efﬁcient architecture design [20, 17, 37]
and low-rank decomposition [7, 57, 26]. Among these approaches, quantization constrains the network weights and activations in limited bitwidth for memory saving and fast processing. In order to fully utilize the hardware re-sources, mixed-precision quantization [50, 9, 3] is present-ed to search the bitwidth in each layer so that the optimal accuracy-complexity trade-off is obtained. However, con-ventional mixed-precision quantization requires the consis-tency of datasets for bitwidth search and network deploy-ment to guarantee policy optimality, which causes signif-icant search burden for automated model compression on largescale datasets such as ImageNet [5]. For example, it usually takes several GPU days to acquire the expected quantization strategy for ResNet18 on ImageNet [50, 3].
In this paper, we present a GMPQ method to learn gen-eralizable mixed-precision quantization strategy via attribu-tion rank preservation for efﬁcient inference. Unlike ex-isting methods which requires the dataset consistency be-tween quantization policy search and model deploymen-t, our method enables the acquired quantization strategy to be generalizable across various datasets. The quanti-zation policy searched on small datasets achieves promis-ing performance on challenging largescale datasets, so that policy search cost is signiﬁcantly reduced. Figure 1(a) shows the difference between our GMPQ and convention-al mixed-precision networks. More speciﬁcally, we observe that correctly locating the network attribution beneﬁts vi-sual analysis for various input data distribution. Therefore, despite of considering model accuracy and complexity, we enforce the quantized networks to imitate the attribution of the full-precision counterparts. Instead of directly minimiz-ing the Euclidean distance between attribution of quantized and full-precision models, we preserve their attribution rank consistency so that the attribution of quantized networks can adaptively adjust the distribution without capacity insufﬁ-ciency. Figure 1(b) demonstrates the attribution computed by Grad-cam [42] for mixed-precision networks with opti-mal and random quantization policy and their full-precision counterparts, where the mixed-precision networks with the optimal bitwidth assignment acquire more consistent attri-bution rank with the full-precision model. Experimental re-sults show that our GMPQ obtains competitive accuracy-complexity trade-off on ImageNet and PASCAL VOC com-pared with the state-of-the-art mixed-precision quantization methods in only several GPU hours. 2.