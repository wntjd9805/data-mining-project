Abstract
Vision-and-Language Navigation (VLN) requires an agent to find a path to a remote location on the basis of natural-language instructions and a set of photo-realistic panoramas. Most existing methods take the words in the instructions and the discrete views of each panorama as the minimal unit of encoding. However, this requires a model to match different nouns (e.g., TV, table) against the same input view feature. In this work, we propose an object-informed sequential BERT to encode visual percep-tions and linguistic instructions at the same fine-grained level, namely objects and words. Our sequential BERT also enables the visual-textual clues to be interpreted in light of the temporal context, which is crucial to multi-round VLN tasks. Additionally, we enable the model to identify the relative direction (e.g., left/right/front/back) of each navigable location and the room type (e.g., bed-room, kitchen) of its current and final navigation goal, as such information is widely mentioned in instructions im-plying the desired next and final locations. We thus en-able the model to know-where the objects lie in the im-ages, and to know-where they stand in the scene. Exten-sive experiments demonstrate the effectiveness compared against several state-of-the-art methods on three indoor
VLN tasks: REVERIE, NDH, and R2R. Project repository: https://github.com/YuankaiQi/ORIST 1.

Introduction
Vision-and-Language Navigation (VLN) offers the ap-pealing prospect of more flexible interactions with robotic applications including domestic robots and personal assis-tants. One of the first VLN tasks to appear was Room-to-Room navigation (R2R) [4]. This task saw an agent ini-tialised at a random location within a simulated environ-*Corresponding author
Figure 1. Objects, rooms, and directions are important clues that can be inferred from visiolinguistic information. Our Object-and-Room Informed Sequential BERT (ORIST) is designed to learn to navigate by leveraging this information. ment rendered from real images, and required it to navi-gate to a remote goal location according to natural-language instructions, such as “Leave the bedroom, and enter the kitchen. Walk forward, and take a left at the couch. Stop in front of the window.” The actions available to the agent at each step are to investigate the current panorama, to move to a neighbouring navigable location/viewpoint, or to stop. An interactive version of the problem is introduced in [25, 33], while REVERIE [28] extends it to identifying remote ob-jects, and TOUCHDOWN [8] introduces outdoor environ-ments.
Numerous methods have been proposed to address in-door VLN tasks. Ma et al. [22] propose to learn textual-visual co-grounding to enhance the understanding of in-structions completed in the past and the instruction to be executed next. Heuristic search algorithms for exploration and back-tracking are introduced in [16, 23]. Qi et al. [27]
propose to disentangle object- and action-related instruc-tion chunks for more accurate visual-textual matching. An-other line of work exploits data augmentation to improve the generalisation ability in unseen environments. In [10], a speaker model is proposed to generate instructions for newly sampled trajectories, and in [32] visual information of seen scenarios is randomly dropped to mimic unseen sce-narios. Most recently, it has been empirically demonstrated that hard example sampling and auxiliary losses [11, 41] are helpful for navigating unseen environments.
Although significant advances have been made, these methods take words in instructions and each discrete view of a panorama as the minimal unit for encoding, which limits the ability to learn the relationships between lan-guage elements and fine-grained visual entities. This is be-cause the view feature, generally obtained by ResNet [13] which is trained for whole-image classification, mainly rep-resents one salient object but it needs to be matched to many different natural-language landmarks mentioned in crowd-sourced navigation instructions. Figure 1 shows an example where the coffee table, couch, and fireplace are used as nav-igation landmarks in different instructions, but they need to match to the same input view feature in most existing VLN methods.
Motivated by the success of BERT-like models (Bidirec-tional Encoder Representations from Transformers) [9, 21, 30] in joint textual and visual understanding, we propose an object-and-room informed sequential BERT to encode in-structions and visual perceptions at the same fine-grained level, namely words and object regions. This enables the model to better “know where” the objects referred to lie.
We also introduce temporal context into our BERT-based model, which allows the model to be aware of completed parts of instructions and seen environments, resulting in more accurate next action prediction.
Relative directions (i.e., left, right, front, and back) and room types (e.g., living room, bedroom) are important clues for VLN tasks as they provide strong directional guidance and semantic-visual information for action selection. To take advantage of such information we incorporate a direc-tion loss and two room-type losses into the proposed model.
These predict the relative direction of navigable viewpoints, the type of room that need to be reached next, as well as the type of room at the final goal location. The relative direction is predictable from each viewpoint’s orientation description (i.e., heading and elevation angles), and the room types are identifiable through the presence of specific objects, such as a couch indicating living room or a microwave indicat-ing kitchen. This provides the model with opportunity to
“know where” it is, and where it is going.
To demonstrate the generalisation ability of our model, we evaluate on three different indoor VLN tasks: remote object grounding (REVERIE) [28], visual dialog navigation (NDH) [33] and room-to-room navigation (R2R) [4]. Our method achieves state-of-the-art results on all of the listed tasks: 18.97 SPL (9.28 RGSPL) on the REVERIE task, 3.17
GP on the NDH task, and 52% SPL on the R2R task. 2.