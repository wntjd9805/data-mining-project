Abstract
We present a large-scale stereo RGB image object pose estimation dataset named the StereOBJ-1M dataset. The dataset is designed to address challenging cases such as ob-ject transparency, translucency, and specular reflection, in addition to the common challenges of occlusion, symmetry, and variations in illumination and environments. In order to collect data of sufficient scale for modern deep learning models, we propose a novel method for efficiently annotat-ing pose data in a multi-view fashion that allows data cap-turing in complex and flexible environments. Fully anno-tated with 6D object poses, our dataset contains over 396K frames and over 1.5M annotations of 18 objects recorded in 183 scenes constructed in 11 different environments. The 18 objects include 8 symmetric objects, 7 transparent objects, and 8 reflective objects. We benchmark two state-of-the-art pose estimation frameworks on StereOBJ-1M as baselines for future work. We also propose a novel object-level pose optimization method for computing 6D pose from keypoint predictions in multiple images. 1.

Introduction
Effectively leveraging 3D cues from visual data to infer the pose of an object is crucial for applications such as aug-mented reality (AR) and robotic manipulation. Compared to objects with opaque and Lambertian surfaces, estimating the pose of transparent and reflective objects is especially challenging. To leverage depth information from sensors, previous approaches have explored deep models that take
RGB-D maps as input [34, 35, 8, 28, 32]. Unfortunately, as the experiments in [17, 37, 29, 12] have shown, existing commercial depth-sensing methods, such as time-of-flight (ToF) or projected light sensors, failed to capture the depths of transparent or reflective surfaces. As a result, monocular
RGB-D maps cannot serve as a reliable input for object pose estimation models in these challenging scenarios. Based on this observation, we focus on using stereo RGB images as our input modality, allowing for object pose estimation on a wider range of objects, including transparent or highly re-flective objects.
Figure 1: StereOBJ-1M dataset. Upper: CAD models of the objects in the dataset. Lower: four data stereo image pair samples with bounding box annotations.
A major challenge in modern object pose estimation is that of acquiring a large-scale training dataset. To increase data size for training large-scale neural networks, previ-ous works have explored leveraging synthetically rendered
[33, 27, 10] or augmented images [35] with 3D mesh mod-els. However, photorealistic rendering is still challenging with only basic graphics rendering tools and limited exper-tise. Synthetic image datasets that are currently available typically introduce a very large domain gap. This is espe-cially true for transparent and reflective objects where vari-ations in illumination and background scenes are crucial but difficult to model.
To address the challenge of costly pose data acquisition, and to enable further training and evaluation of modern ob-ject pose estimation models, we introduce a novel method for capturing and labeling a large-scale dataset with high efficiency and quality. Our method is based on multi-view geometry to accurately localize fiducial markers, cameras, and object keypoints in the scene. We use a hand-held stereo camera to record video data. With the help of two other static cameras mounted to tripods, the positions of a
dataset data type stereo depth occlusion
FAT [33]
CAMERA [35]
YCB [1]
LINEMOD [9]
GraspNet-1Billion [4]
T-LESS [10] kPAM [21]
LabelFusion [22]
REAL275 [35]
TOD [17]
StereOBJ-1M (Ours) synthetic mixed reality real real real real real real real real real
✓
✗
✗
✗
✗
✗
✗
✗
✗
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✗
✓
✓
✓
✓
✓
✓
✓
✓
✗
✗
✓ transparent objects
✗
✗
✗
✗
✗
✗
✗
✗
✗
✓
✓ reflective objects
✗
✗
✗
✗
✗
✗
✓
✓
✗
✗
✓
# of frames 60,000 300,000 133,936 18,000 97,280 47,762 100,000 352,000 7,072 64,000 396,509
# of outdoor environments 0 0 0 0 0 0 0 0 0 0 3
# of scenes 3,075 30 92 15 190
--138 13 10 183
# of objects 21 42 21 15 88 30 91 12 42 20 18
# of annotations 205,359 4,350,656 613,917 15,784 970,000 47,762
-1,000,000 35,356 64,000 1,517,835
Table 1: Dataset Comparisons. Our StereOBJ-1M dataset is the only large-scale 6DoF object pose dataset that provides stereo
RGB as input modality, includes transparent and reflective objects and is captured in both indoor and outdoor environments.
In terms of capacity, our dataset is also the largest real-image dataset in size and the dataset with the most scene diversity. set of fiducial markers can be calculated on the fly, from which the pose of every recorded frame can be automati-cally computed. By annotating 2D object keypoints in just a few frames selected in a long recorded video, the 3D lo-cations of the keypoints can be computed by triangulation.
The 6D poses of objects can then be calculated by aligning 3D CAD models to the keypoints before being propagated to all other frames.
Using the procedure outlined above, we generate
StereOBJ-1M dataset, the first pose dataset with stereo
RGB as input modality with over 100K frames. It is also the largest 6D object pose dataset in history: it consists of 396,509 high-resolution stereo frames and over 1.5 million 6D pose annotations of 18 objects recorded in 183 indoor and outdoor scenes. The capacity of StereOBJ-1M is suffi-cient for training large-scale neural networks without ad-ditional synthetic images. The average labeling error of
StereOBJ-1M is 2.3mm which is the best annotation pre-cision among all public object pose datasets.
We implement two state-of-the-art methods [27, 17] as the baseline comparisons for 6D pose estimation using stereo on the StereOBJ-1M dataset. To handle 2D-3D cor-respondences predictions in two or more images, we pro-pose a novel object-level 6D pose optimization approach named Object Triangulation. Contrary to classic triangu-lation that optimizes the 3D location of a point, we directly optimize the 6D pose of an object from 2D keypoint lo-cations in multiple images. Experiment results show that
Object Triangulation consistently improves pose estimation over monocular input while classic triangulation can yield worse results. With Object Triangulation, the stereo vari-ants of both baseline methods significantly outperform their monocular counterparts on StereOBJ-1M, by at least 25% in ADD(-S) AUC and 14% in ADD(-S) accuracy, which highlights the importance of stereo modality in object pose estimation. We expect that StereOBJ-1M will serve as a common benchmark dataset for stereo RGB-based object pose estimation. 2.