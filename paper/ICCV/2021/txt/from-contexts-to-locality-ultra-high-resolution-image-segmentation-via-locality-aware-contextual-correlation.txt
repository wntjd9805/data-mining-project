Abstract
Ultra-high resolution image segmentation has raised in-creasing interests in recent years due to its realistic appli-cations. In this paper, we innovate the widely used high-resolution image segmentation pipeline, in which an ultra-high resolution image is partitioned into regular patches for local segmentation and then the local results are merged into a high-resolution semantic mask. In particular, we in-troduce a novel locality-aware contextual correlation based segmentation model to process local patches, where the relevance between local patch and its various contexts are jointly and complementarily utilized to handle the se-mantic regions with large variations. Additionally, we present a contextual semantics reﬁnement network that as-sociates the local segmentation result with its contextual semantics, and thus is endowed with the ability of reduc-ing boundary artifacts and reﬁning mask contours dur-ing the generation of ﬁnal high-resolution mask. Further-more, in comprehensive experiments, we demonstrate that our model outperforms other state-of-the-art methods in public benchmarks. Our released codes are available at https://github.com/liqiokkk/FCtL. 1.

Introduction
With the advance of photography and sensor technolo-gies, the accessibility to ultra-high resolution images (i.e., 2K, 4K, or even higher resolution images) has opened new horizons to the computer vision community. It will beneﬁts a wide range of imaging applications, e.g., urban planning and remote sensing based on high-resolution geospatial im-ages and high-resolution medical image analysis, and thus the demand for studying and analyzing such images has ur-gently increased in recent years.
In this paper, we aim at the speciﬁc task of semantic seg-mentation for ultra-high resolution geospatial images cap-tured from aerial view. The recent development of deep convolutional neural networks (CNNs) has given rise to remarkable progress of semantic segmentation techniques.
*Wenxi Liu and Yuanlong Yu are the corresponding authors.
Figure 1. For the task of ultra-high resolution image segmentation, the most common approach is to segment cropped local patches and then combine them into a high-resolution mask. To address the core problem on local segmentation quality, we propose a locality-aware contextual correlation based model that exploits rescaled various contexts (×1, ×2, ×3 large as local patch in the original image) to produce reﬁned results.
Yet, most CNN-based segmentation models target on full resolution images and perform pixel-level class prediction, which requires more computation resources comparing to image classiﬁcation and object detection. This hurdle be-comes signiﬁcant when the image resolution grows to be ultra high, leading to the pressing dilemma between mem-ory efﬁciency (even feasibility) and segmentation quality.
Particularly, in order to segment an ultra-high resolu-tion image, the prevailing practice is either to downsam-ple it to a smaller spatial dimension before performing seg-mentation, or to separately segment the partitioned patches and merge their results into a high-resolution one. These trivial practices sacriﬁce the segmentation quality for the model efﬁciency. Additionally, the recent attempts propose to utilize the well-pretrained segmentation models to ob-tain the coarse segmentation masks and another model to reﬁne the contours of the masks [5, 37]. However, these methods mainly focus on high-resolution natural images or daily photos concerning with large objects, while the high-resolution geospatial images are captured from aerial views covering a large ﬁeld of view, which may contain many ob-jects/regions with large contrast in scale and shape. Hence, it requires the segmentation model to be capable of cap-turing not only the semantics over large image regions but also the image details of different granularity. The recent work GLNet [4] proposes to incorporate the local and global information via a two-stream network that separately pro-cesses the downsampled global image and cropped local patches, as well as a feature sharing module that shares the concatenated local and global features in both streams.
Their method can achieve obvious improvements over ex-isting methods, which embodies the importance of contex-tual information for segmentation performance. Neverthe-less, their feature sharing scheme does not spatially asso-ciate local features with the global ones and thus does not well exploit their correlation, which makes their model too complex to optimize and their performance suboptimal.
To thoroughly utilize the rich information within ultra-high resolution geospatial images, we present an ultra-high resolution geospatial image segmentation model featuring with the locality-aware contextual correlation scheme. Sim-ilar to [4, 25], our framework is based on the widely used practice for high-resolution image segmentation, in which image patches are regularly cropped from the original im-age, then individually segmented, and ﬁnally their local re-sults are overlayingly merged. However, each local patch of the ultra-high resolution geospatial images often contain se-mantic regions with large contrast in sizes (e.g., house and forest), which challenges the local segmentation model. In-spired by prior practices (e.g. [4]), contextual information turns out effective to resolve this problem. But, unlike pre-vious methods, we propose that the semantics within local patches can be structually and complementarily associated and inferred by their contextual regions of different scales.
For instance, in Fig. 1, the contexts with varied coverage guide the model to the attentive regions relevant to the ob-jects of different granularity in the image (e.g., small or large building). Hence, we propose a locality-aware con-textual correlation based deep network model to exploit the correlation between local patch and its contextual regions.
In concrete, we ﬁrst present a locality-aware contextual cor-relation module to capture the positional relevance of local patch and context, which is enabling to attentively enhance the relevant features of local patch, i.e., locality-aware fea-tures. Then, we propose an adaptive context fusion scheme to balance and combine the locality-aware features associ-ated by various contexts. As shown in Fig. 1, the contexts can lead to different yet complementary locality-aware fea-tures, thus allow tolerance to misleading information in a single context. To do so, the corresponding spatial weight maps of different locality-aware features are predicted on-the-ﬂy to accomplish the complementary fusion.
Furthermore, to obtain the ﬁnal segmentation result of the ultra-high resolution image, the results of local patches will be put back together. Directly montaging local seg-mentation masks may cause boundary vanishing artifacts for adjacent patches, so the prior practice is to overlap ad-jacent patches partially and compute the average results for overlap regions. To some extent, this trivial approach can reduce the artifacts, yet cannot achieve the optimal results.
Therefore, we propose an effective contextual semantics re-ﬁnement network that utilizes the correlation of local mask and context mask to enhance the relevant semantic regions and thus adaptively reﬁne the local results without intro-ducing boundary vanishing artifacts. Besides, our proposed model can also leverage the contextual semantics to polish the contours of segmentation masks.
To evaluate our model, we conduct comprehensive ex-periments and demonstrate that our proposed model out-performs the state-of-the-art approaches on public ultra-high resolution arerial image datasets, DeepGlobe and Inria
Aerial. The main contributions of our paper are summarized as below:
• We present an ultra-high resolution image segmenta-tion framework based on a novel local segmentation model. It leverages the locality-aware contextual cor-relation and the adaptive feature fusion scheme, which associates and combines local-context information to strengthen local segmentation.
• We present a contextual semantics reﬁnement network that leverages the relevance of local segmentation and context mask to avoid boundary vanishing artifacts and reﬁne the local semantic mask.
• Our method achieves the state-of-the-art semantic seg-mentation performance in several public ultra-high res-olution geospatial image datasets. 2.