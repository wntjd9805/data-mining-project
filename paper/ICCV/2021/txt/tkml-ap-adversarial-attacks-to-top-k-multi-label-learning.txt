Abstract
Top-k multi-label learning, which returns the top-k pre-dicted labels from an input, has many practical applications such as image annotation, document analysis, and web search engine. However, the vulnerabilities of such algo-rithms with regards to dedicated adversarial perturbation attacks have not been extensively studied previously. In this work, we develop methods to create adversarial perturba-tions that can be used to attack top-k multi-label learning-based image annotation systems (TkML-AP). Our methods explicitly consider the top-k ranking relation and are based on novel loss functions. Experimental evaluations on large-scale benchmark datasets including PASCAL VOC and MS
COCO demonstrate the effectiveness of our methods in re-ducing the performance of state-of-the-art top-k multi-label learning methods, under both untargeted and targeted at-tacks. 1.

Introduction
The past decade has witnessed the tour de force of mod-ern deep neural networks (DNNs), which have significantly improved, or in some cases, revolutionized, the state-of-the-art performance of many computer vision problems.
Notwithstanding this tremendous success, the omnipotent
DNN models are surprisingly vulnerable to adversarial at-tacks [24, 6, 12].
In particular, inputs with specially de-signed perturbations, commonly known as adversarial ex-amples, can easily mislead a DNN model to make erroneous predictions. The vulnerabilities of DNN models to adver-sarial examples impede the safe adoptions of machine learn-ing systems in practical applications. It also motivates the explorations of algorithms generating adversarial examples
[3, 17, 14] as a means to analyze the vulnerabilities of DNN models and improve their security.
Most existing works on generating adversarial examples have been focused on the case of multi-class classification
[1, 24, 6, 19, 3, 17], where one instance can only be assigned to exactly one out of a set of mutually exclusive classes (labels). Because of the singleness of the labels, existing
Figure 1: Illustrative examples of the untargeted and targeted at-tacks to the top-3 multi-label image annotation for an image from the PASCAL VOC 2012 dataset. The green icons correspond to the ground truth labels. The red icons represent the targeted labels for attacking. The figure is better viewed in color. adversarial perturbation generation schemes for multi-class classification are based on the top-1 attack (i.e., C&W [3],
Deepfool [17]), only aiming to alter the top predicted label using the adversarial perturbation.
However, in many real-world applications such as im-age annotation, document categorization, and web search engines, it is more natural to solve the multi-label learning problem, where an instance is associated with a non-empty subset of labels. Furthermore, in these applications, the out-put of the system is usually a set of labels of a fixed size, corresponding to the top-k predicted labels. We term this as the top-k multi-label learning (TkML). The practical cases of TkML open more opportunities for attackers and leading to larger uncertainties for defenders. There are two com-mon settings that we will consider subsequently for TkML adversarial attacks. The untargeted attack aims to only re-place the top-k labels with a set of arbitrary k labels that are
not true classes of the untampered input. The targeted at-tack, on the other hand, aims to coerce the TkML classifier to use a specific set of k labels that are not true classes of the input as the top-k predictions.
In this work, we describe the first untargeted and targeted adversarial attack algorithms for TkML based on a contin-uous formulation of the ranking operation, which we term as TkML-AP. Specifically, we note that to perturb the pre-dictions of a TkML algorithm, it is sufficient to clear any ground-truth labels from the top-k set. There are many dif-ferent ways to achieve this, but we will focus on ones that enlist the “least actions”, i.e., perturbing the predicted labels with minimum changes to the original label rankings. For the untargeted attack, this means move the ground-truth la-bels out of the top-k predictions, and for the targeted attack, this means move the target labels to the top-k set. Fig.1 gives an illustrative explanation of the proposed idea.
Thus, the key challenge in generating adversarial exam-ples for TkML is to optimize perturbations that can lead to the change of top-k rankings of the predicted label. To this end, we introduce a reformulation of the top-k sum that lends itself to efficient numerical algorithms based on gra-dient descent methods. In particular, we provide loss func-tions for adversarial perturbations to TkML that are con-vex in terms of the individual prediction scores. This has a further advantage that even though the model may be non-linear, a convex loss function can encourage many equally effective local optima. Hence any adversarial perturbation that can lead the model to have the same loss value will have equal effects. We demonstrate the effectiveness of our method on attacking state-of-the-art TkML algorithms us-ing large scale benchmark datasets (PASCAL VOC 2012
[4] and MS COCO 2014 [11]). The main contributions of our work can be summarized as follows: 1. We present the first algorithms for untargeted and tar-geted adversarial attacks to the TkML problem. 2. Our method is based on a continuous reformulation of the non-differentiable ranking operation. The objec-tive function is convex in terms of the individual pre-diction scores, which is easier to optimize. 3. Numerical experiments on large-scale benchmark datasets confirm the effectiveness of our method in at-tacking state-of-the-art TkML algorithms. 2.