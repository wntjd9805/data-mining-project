Abstract
We address the problem of learning to segment actions from weakly-annotated videos, i.e., videos accompanied by transcripts (ordered list of actions). We propose a frame-work in which we model actions with a union of low-dimensional subspaces, learn the subspaces using tran-scripts and reﬁne video features that lend themselves to ac-tion subspaces. To do so, we design an architecture consist-ing of a Union-of-Subspaces Network, which is an ensemble of autoencoders, each modeling a low-dimensional action subspace and can capture variations of an action within and across videos. For learning, at each iteration, we gen-erate positive and negative soft alignment matrices using the segmentations from the previous iteration, which we use for discriminative training of our model. To regularize the learning, we introduce a constraint loss that prevents im-balanced segmentations and enforces relatively similar du-ration of each action across videos. To have a real-time in-ference, we develop a hierarchical segmentation framework that uses subset selection to ﬁnd representative transcripts and hierarchically align a test video with increasingly re-ﬁned representative transcripts. Our experiments on three datasets show that our method improves the state-of-the-art action segmentation and alignment, while speeding up the inference time by a factor of 4 to 13.1 1.

Introduction
Localization and classiﬁcation of human actions in long uncurated videos has been a major challenge in video under-standing [54, 10, 11, 27, 65, 69, 66, 59, 19]. While many methods have studied the problem in a fully-supervised set-ting using dense supervision [50, 57, 29, 33, 55, 67], gath-ering framewise annotations is costly and cannot scale to massive amounts of video data, which are available today.
As a result, there has been an increasing interest in meth-ods that can learn from weakly-annotated videos. In par-1Code is available at https://github.com/ZijiaLewisLu/
ICCV21-TASL. ticular, action transcripts, which refer to sequences of ac-tions appearing in videos without specifying their begin-ning and ending times, are less costly to gather and can also be obtained from video narrations or other meta data
[32, 1, 42]. This has motivated a variety of interesting ap-proaches that learn to localize and classify actions using transcripts [21, 2, 48, 49, 9, 73, 36, 4, 37].
Challenges. Despite tremendous advances, existing works on weakly-supervised action learning still face major chal-lenges.
In fact, a successful class of recent methods fo-cuses on alternating between segmentation of the training videos using transcripts and retraining models with the ob-tained segmentations [49, 36]. However, training a model with the one estimated segmentation could ignore and dis-courage other likely segmentations and propagate the initial segmentation errors.
Moreover, existing methods often ignore the underlying low-dimensional structures of videos.
In fact, it is well known that high-dimensional visual data, e.g., rigid and nonrigid motions or human actions, lie in low-dimensional subspaces [63, 12, 41, 3, 40, 38, 6]. Yet, leveraging such low-dimensional subspaces in the weakly-supervised set-ting has been mainly ignored, as the existing works work in the fully-supervised or fully-unsupervised regimes and can-not take advantage of weak supervision, e.g., transcripts.
On the other hand, inference on test videos that do not have transcripts is often extremely costly. This comes from the fact that existing methods require aligning the test video with every transcript in the training set to select the most likely transcript and the associated segmentation. This pre-vents methods from being applicable in real-time.
Paper Contributions.
In this paper, we address the problem of weakly-supervised action segmentation by de-veloping a Transcript-aware Action Subspace Learning (TASL) framework that models actions with a union of low-dimensional subspaces, learns the subspaces using weak su-pervision (transcripts) and reﬁnes video features that lend themselves to action subspaces. To do so, we design an ar-chitecture consisting of a feature learning module and a new
Figure 1: We propose a framework, referred to as Transcript-aware Action Subspace Learning (TASL), for weakly-supervised segmentation of videos.
The framework consists of a Union-of-Subspaces Network (USN), which learns to embed actions into discriminative low-dimensional subspaces, and an efﬁcient constrained video alignment algorithm that generates positive and negative soft alignments, which will be used for parameter learning.
Union-of-Subspaces Network (USN). USN is an ensemble of autoencoders, each modeling a low-dimensional action subspace, that captures variations of each action. As we show in the experiments, depending on the semantic sim-ilarity of actions (e.g., sharing verb or noun), the learned subspaces will be nearly orthogonal to each other in some directions (allowing discrimination), while intersecting in some other directions (capturing shared information).
For learning, we alternate between segmenting training videos using transcripts and learning models and features from segmentations. However, instead of learning a model to reproduce an obtained segmentation, we generate posi-tive and negative soft alignment matrices using the optimal segmentation, which we will use for discriminative learn-ing of subspaces. We introduce a constraint loss to prevent imbalanced segmentations and to enforce relatively similar duration of each action across videos.
To have real-time inference, we develop a hierarchical segmentation framework that uses subset selection to ﬁnd representative transcripts of training videos. We will hier-archically align a test video with increasingly reﬁned rep-resentative transcripts. Our experiments on three datasets show that our method improves the state of the art while speeding up inference by a factor of 4 to 13. 2.