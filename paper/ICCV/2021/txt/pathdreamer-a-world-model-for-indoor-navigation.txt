Abstract
People navigating in unfamiliar buildings take advan-tage of myriad visual, spatial and semantic cues to efﬁ-ciently achieve their navigation goals. Towards equipping computational agents with similar capabilities, we intro-duce Pathdreamer, a visual world model for agents navigat-ing in novel indoor environments. Given one or more previ-ous visual observations, Pathdreamer generates plausible high-resolution 360◦ visual observations (RGB, semantic segmentation and depth) for viewpoints that have not been visited, in buildings not seen during training. In regions of high uncertainty (e.g. predicting around corners, imagining the contents of an unseen room), Pathdreamer can predict diverse scenes, allowing an agent to sample multiple real-istic outcomes for a given trajectory. We demonstrate that
Pathdreamer encodes useful and accessible visual, spatial and semantic knowledge about human environments by us-ing it in the downstream task of Vision-and-Language Nav-igation (VLN). Speciﬁcally, we show that planning ahead with Pathdreamer brings about half the beneﬁt of looking ahead at actual observations from unobserved parts of the environment. We hope that Pathdreamer will help unlock model-based approaches to challenging embodied naviga-tion tasks such as navigating to speciﬁed objects and VLN. 1.

Introduction
World models [23], or models of environments [72], are an appealing way to represent an agent’s knowledge about its surroundings. An agent with a world model can predict its future by ‘imagining’ the consequences of a series of proposed actions. This capability can be used for sampling-based planning [16, 57], learning policies directly from the model (i.e., learning in a dream) [17, 23, 64, 25], and for counterfactual reasoning [6]. Model-based approaches such as these also typically improve the sample efﬁciency of deep reinforcement learning [72, 62]. However, world models that generate high-dimensional visual observations (i.e., im-ages) have typically been restricted to relatively simple en-vironments, such as Atari games [62] and tabletops [16].
Our goal is to develop a generic visual world model for agents navigating in indoor environments. Speciﬁcally,
Figure 1: Generating photorealistic 360◦ visual observa-tions from an imagined 6.3m trajectory in a previously un-seen building. Observations also include depth and segmen-tations (not shown here). given one or more previous observations and a proposed navigation action sequence, we aim to generate plausible high-resolution visual observations for viewpoints that have not been visited, and do so in buildings not seen during training. Beyond applications in video editing and content creation, solving this problem would unlock model-based methods for many embodied AI tasks, including navigating to objects [5], instruction-guided navigation [3, 66, 40] and dialog-guided navigation [74, 26]. For example, an agent asked to ﬁnd a certain type of object in a novel building, e.g. ‘ﬁnd a chair’, could perform mental simulations using the world model to identify navigation trajectories that are most likely to include chair observations – without moving.
Building such a model is challenging. It requires syn-thesizing completions of partially visible objects, using as few as one previous observation. This is akin to novel view synthesis from a single image [19, 80], but with potentially unbounded viewpoint changes. There is also the related but considerably more extreme challenge of predicting around corners. For example, as shown in Fig. 1, any future nav-igation trajectory passing the entrance of an unseen room requires the model to plausibly imagine the entire contents
of that room (we dub this the room reveal problem). This requires generalizing from the visual, spatial and seman-tic structure of previously explored environments—which in our case are photo-realistic 3D captures of real indoor spaces in the Matterport3D dataset [7]. A third problem is temporal consistency: predictions of unseen building re-gions should ideally be stochastic (capturing the full distri-bution of possible outcomes), but revisited regions should be rendered in a consistent manner to previous observations.
Towards this goal, we introduce Pathdreamer. Given one or more visual observations (consisting of RGB, depth and semantic segmentation for panoramas) of an indoor scene, Pathdreamer synthesizes high-resolution visual ob-servations (RGB, depth and semantic segmentations) along a speciﬁed trajectory through future viewpoints, using a hi-erarchical two-stage approach. Pathdreamer’s ﬁrst stage,
Structure Generator, generates depth and semantic segmen-tations. Inspired by video prediction [11], these outputs are conditioned on a latent noise tensor capturing the stochas-tic information about the next observation (such as the lay-out of an unseen room) that cannot be predicted determin-istically. The second stage’s Image Generator renders the depth and semantic segmentations as realistic RGB images using modiﬁed Multi-SPADE blocks [63, 51]. To maintain long-term consistency in the generated observations, both stages use back-projected 3D point cloud representations which are re-projected into image space for context [51].
Pathdreamer can generate plausible views for unseen in-door scenes under large viewpoint changes (see Figure 1), while also addressing the room reveal problem – in this case correctly hypothesizing that the unseen room revealed at po-sition 2 will most likely resemble a kitchen. Empirically, using the Matterport3D dataset [7] and 360◦ observations, we evaluate both stages of our model against prior work and reasonable baselines and ablations. We ﬁnd that the hierar-chical structure of the model is essential for predicting over large viewpoint changes, that maintaining both RGB and semantic context is required, and that prediction quality de-grades gradually when we evaluate with trajectory rollouts of up to 13m (with viewpoints 2.25m apart on average).
Encouraged by these results, we investigate whether
Pathdreamer’s RGB predictions can improve performance on a downstream task: Vision-and-Language Navigation (VLN), using the R2R dataset [3]. VLN requires agents to interpret and execute natural language navigation instruc-tions in a photorealistic 3D environment. A robust ﬁnding from previous research is that performance improves dra-matically when agents can look ahead at unobserved parts of the environment while following an instruction [50].
We show that replacing look-ahead observations with Path-dreamer predictions maintains around half of the gains, a
ﬁnding we expect to have signiﬁcant implications for VLN research. In summary, our main contributions include:
• Proposing the study of visual world models for generic indoor environments and deﬁning evaluation protocols and baselines for future work.
• Pathdreamer, a stochastic hierarchical visual world independent threads of model combining multiple, previous work on video prediction [11], semantic im-age synthesis [63] and video-to-video synthesis [51].
• Extensive experiments characterizing the performance of Pathdreamer and demonstrating improved results on the downstream VLN task [3]. 2.