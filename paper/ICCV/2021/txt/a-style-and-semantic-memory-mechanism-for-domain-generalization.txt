Abstract
Mainstream state-of-the-art domain generalization algo-rithms tend to prioritize the assumption on semantic in-variance across domains. Meanwhile, the inherent intra-domain style invariance is usually underappreciated and put on the shelf. In this paper, we reveal that leveraging intra-domain style invariance is also of pivotal importance in improving the efficiency of domain generalization. We verify that it is critical for the network to be informative on what domain features are invariant and shared among in-stances, so that the network sharpens its understanding and improves its semantic discriminative ability. Correspond-ingly, we also propose a novel “jury” mechanism, which is particularly effective in learning useful semantic feature commonalities among domains. Our complete model called
STEAM can be interpreted as a novel probabilistic graph-ical model, for which the implementation requires conve-nient constructions of two kinds of memory banks: semantic feature bank and style feature bank. Empirical results show that our proposed framework surpasses the state-of-the-art methods by clear margins. 1.

Introduction
Machine learning models are usually deployed in scenar-ios where the test data are unknown beforehand. This phe-nomenon can lead to dangerous consequences especially when the predictions are used for life-threatening occasions such as medical diagnosis. The prediction might be seri-ously erroneous due to the distributional gap between train-ing data and test data. It is therefore critical for machine learning algorithms to maintain safe and reliable predictions that generalize well across domains. The goal of domain generalization (DG) approaches [20, 21, 41, 55] is to solve this issue by leveraging labeled data from multiple training domains. However, we observed that mainstream state-of-the-art DG algorithms tend to only prioritize the semantic
∗This work was performed at JD AI Research. invariance assumption across domains, while the style in-variance within each domain is usually ignored. In this pa-per, we reveal that intra-domain style invariance is also of pivotal importance to improve DG approaches. Particularly, we propose a novel model to incorporate both intra-domain style invariance and inter-domain semantic invariance for
DG tasks. The proposed framework is called STEAM, which relies on a STyle and sEmAntic Memory mechanism to practically implement our proposed assumptions.
In contrast to existing DG works [7, 53, 55], STEAM further benefits from the hypothesis that instances from the same domain should share style information. The moti-vation is that the simple constraint helps efficiently disen-tangle the style feature, and therefore eases the search for true semantic feature with a reduced degree of freedom. To reach this goal, we resort to the recently prevailing self-supervised learning paradigm that makes our assumptions practically accessible. Our first objective is to achieve intra-domain style invariance by conveniently resorting to con-trastive loss. Given the invariance assumption, style fea-tures corresponding to each domain are discovered, and the network can further learn semantic features along the direc-tions mostly orthogonal to the domain styles. Since the se-mantic feature is considered as the true causal factor affect-ing the instance category, STEAM effectively helps reduce overfitting to the domain styles through the above mecha-nisms. Most importantly, we also force the network to re-spect the conventional semantic invariance among domains.
Specifically, we require that each pair of samples from the same class to compute a similarity score with all the seman-tic features in “memory”. These two samples need to reach a consensus on every such similarity score when sweeping through all the stored semantic features. We name this pro-cedure “jury” mechanism, and we elaborate the mathemati-cal net effect of such mechanism in the paper.
To summarize, our contributions in this paper include: 1.
We explore the assumption of instance level intra-domain style invariance and justify the empirical advantage by in-corporating such assumptions into the DG framework. 2.
We propose a “jury” mechanism, which efficiently learns
domain-agnostic semantic features beneficial for classifica-tion tasks. Such mechanism is capable of efficiently pre-venting semantic features from overfitting to domain styles. 3. We observe that the proposed STEAM not only general-izes well on DG benchmarks, but also can be conveniently modified for domain adaptation (DA) problems. 2.