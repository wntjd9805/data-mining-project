Abstract
Autonomous systems need to understand the semantics and geometry of their surroundings in order to comprehend and safely execute object-level task specifications. This pa-per proposes an expressive yet compact model for joint ob-ject pose and shape optimization, and an associated opti-mization algorithm to infer an object-level map from multi-view RGB-D camera observations. The model is expressive because it captures the identities, positions, orientations, and shapes of objects in the environment. It is compact be-cause it relies on a low-dimensional latent representation of implicit object shape, allowing onboard storage of large multi-category object maps. Different from other works that rely on a single object representation format, our approach has a bi-level object model that captures both the coarse level scale as well as the fine level shape details. Our ap-proach is evaluated on the large-scale real-world ScanNet dataset and compared against state-of-the-art methods. 1.

Introduction
Range sensors, such as RGB-D cameras and LIDARs, have become a primary data source for robot localization and mapping due to their increasing accuracy, affordabil-ity, and compactness. This has contributed to the devel-opment of RGB-D Simultaneous Localization And Map-ping (SLAM) [23, 33, 35, 46] and Structure from Motion (SfM) [2, 11, 43] approaches that provide accurate and ef-ficient ego-motion estimation and map reconstruction. The map representations used in RGB-D SLAM, however, are predominantly geometric, composed of point landmarks
[23, 45], surfels [47] or explicit (mesh) and implicit (signed distance field) surface representations [33, 39]. These geo-metric models do not provide semantic information such as the class, pose, shape, or affordances of objects in the scene.
Maps that combine geometric and semantic information are
We gratefully acknowledge support
W911NF-17-2-0181 and NSF RI IIS-2007141. from ARL DCIST CRA
Figure 1. Overview of ELLIPSDF: a) Ground-truth scene recon-struction from colored point clouds in ScanNet [13] scene 0087, where the RGB axes show the camera trajectory, b) Reconstructed object meshes in the world frame using the SDF model decoded from a latent code, and the optimized SIM(3) transformation rep-resenting object pose. useful and understandable for humans and allow specifi-cation of symbolic tasks, such as retrieval, object-directed navigation, grasping, and safety critical operation, in terms of object entities.
Recent works that focus on object-level localization and mapping include [37, 42, 32, 45, 29, 20], which utilize ob-jects as landmarks for localization and navigation [42, 3, 32, 45, 29, 20] or as functional entities for motion, part, and af-fordance identification [26, 38, 31, 27]. The memory and computational efficiency of the object representations used by semantic SLAM are vital for accommodating online con-struction, onboard storage, and multi-robot use of large se-mantic maps. On one hand, a parsimonious way for opti-mizing and storing object maps is needed to ensure online computation and low onboard memory use. On the other hand, it is desirable to preserve as many details about the object shapes, texture, and affordances as possible. Strik-ing the right balance between a faithful object reconstruc-tion and a compact object representation remains an open research problem.
This paper proposes ELLIPSDF, which is an expressive yet compact model of object pose and shape, and an asso-ciated optimization algorithm to infer an object-level map from multi-view RGB-D camera observations, as shown in
Fig. 1. ELLIPSDF is expressive because it captures the
identity, scale, position, orientation, and shape of objects in the environment. It is compact because it relies on a low-dimensional latent encoding of the signed distance function (SDF) to an object’s surface, allowing onboard storage of large multi-category object maps.
Shape representation using SDF predicted by an autode-coder network was proposed in DeepSDF [36] and Du-alSDF [18]. In this paper, we extend the SDF prediction network in prior works by proposing a bi-level object model with a shared latent representation. Object primitive shapes and SDF are predicted from a shared latent space. On the coarse-level, an ellipsoid is used as a primitive shape to constrain the overall shape scale. On the fine-level, an au-todecoder similar to DeepSDF is used to preserve the object shape details. To summarize, the main contribution of this work is the design of
• a bi-level object model with coarse and fine levels, en-abling joint optimization of object pose and shape. The coarse-level uses a primitive shape for robust pose and scale initialization, and the fine-level uses SDF resid-ual directly to allow accurate shape modeling. The wo levels are coupled via a shared latent space.
• a cost function to measure the mismatch between the bi-level object model and the segmented RGB-D ob-servations in the world frame. 2.