Abstract
End-to-end approaches to autonomous driving com-monly rely on expert demonstrations. Although humans are good drivers, they are not good coaches for end-to-end al-gorithms that demand dense on-policy supervision. On the contrary, automated experts that leverage privileged infor-mation can efficiently generate large scale on-policy and off-policy demonstrations. However, existing automated ex-perts for urban driving make heavy use of hand-crafted rules and perform suboptimally even on driving simula-tors, where ground-truth information is available. To ad-dress these issues, we train a reinforcement learning expert that maps bird’s-eye view images to continuous low-level actions. While setting a new performance upper-bound on
CARLA, our expert is also a better coach that provides in-formative supervision signals for imitation learning agents to learn from. Supervised by our reinforcement learning coach, a baseline end-to-end agent with monocular camera-input achieves expert-level performance. Our end-to-end agent achieves a 78% success rate while generalizing to a new town and new weather on the NoCrash-dense bench-mark and state-of-the-art performance on the more chal-lenging CARLA LeaderBoard. 1.

Introduction
Even though nowadays, most autonomous driving (AD) stacks [30, 48] use individual modules for perception, plan-ning and control, end-to-end approaches have been pro-posed since the 80’s [35] and the success of deep learning brought them back into the research spotlight [5, 50]. Nu-merous works have studied different network architectures for this task [3, 16, 52], yet most of these approaches use supervised learning with expert demonstrations, which is known to suffer from covariate shift [36, 40]. While data augmentation based on view synthesis [2, 5, 35] can par-tially alleviate this issue, in this paper, we tackle the prob-lem from the perspective of expert demonstrations.
Expert demonstrations are critical for end-to-end AD al-gorithms. While imitation learning (IL) methods directly
Figure 1: Roach: RL coach allows IL agents to benefit from dense and informative on-policy supervisions. mimic the experts’ behavior [3, 10], reinforcement learn-ing (RL) methods often use expert demonstrations to im-prove sample efficiency by pre-training part of the model via supervised learning [27, 47]. In general, expert demon-strations can be divided into two categories: (i) Off-policy, where the expert directly controls the system, and the state/observation distribution follows the expert. Off-policy data for AD includes, for example, public driving datasets
[6, 22, 51]. (ii) On-policy, where the system is controlled by the desired agent and the expert “labels” the data. In this case, the state/observation distribution follows the agent, but expert demonstrations are accessible. On-policy data is fundamental to alleviate covariate shift as it allows the agent to learn from its own mistakes, which the expert in the off-policy data does not exhibit. However, collecting ade-quate on-policy demonstrations from humans is non-trivial.
While trajectories and actions taken by the human expert can be directly recorded during off-policy data collection, labeling these targets given sensor measurements turns out to be a challenging task for humans. In practice, only sparse events like human interventions are recorded, which, due to the limited information it contains, is hard to use for training and better suited for RL [2, 23, 24] than for IL methods.
In this work we focus on automated experts, which in
contrast to human experts can generate large-scale datasets with dense labels regardless of whether they are on-policy or off-policy. To achieve expert-level performance, auto-mated experts may rely on exhaustive computations, expen-sive sensors or even ground truth information, so it is un-desirable to deploy them directly. Even though some IL methods do not require on-policy labeling, such as GAIL
[20] and inverse RL [1], these methods are not efficient in terms of on-policy interactions with the environment.
On the contrary, automated experts can reduce the ex-pensive on-policy interactions. This allows IL to success-fully apply automated experts to different aspects of AD.
As a real-world example, Pan et al. [34] demonstrated end-to-end off-road racing with a monocular camera by imitat-ing a model predictive control expert with access to expen-sive sensors. In the context of urban driving, [36] showed that a similar concept can be applied to the driving simula-tor CARLA [12]. Driving simulators are an ideal proving ground for such approaches since they are inherently safe and can provide ground truth states. However, there are two caveats. The first regards the “expert” in CARLA, com-monly referred to as the Autopilot (or the roaming agent).
The Autopilot has access to ground truth simulation states, but due to the use of hand-crafted rules, its driving skills are not comparable to a human expert’s. Secondly, the supervi-sion offered by most automated experts is not informative.
In fact, the IL problem can be seen as a knowledge transfer problem and just learning from expert actions is inefficient.
To tackle both drawbacks and motivated by the success of model-free RL in Atari games [18] and continuous con-trol [14], we propose Roach (RL coach), an RL expert that maps bird’s-eye view (BEV) images to continuous actions (Fig. 1 bottom). After training from scratch for 10M steps,
Roach sets the new performance upper-bound on CARLA by outperforming the Autopilot. We then train IL agents and investigate more effective training techniques when learn-ing from our Roach expert. Given that Roach uses a neural network policy, it serves as a better coach for IL agents also based on neural networks. Roach offers numerous informa-tive targets for IL agents to learn from, which go far be-yond deterministic action provided by other experts. Here we demonstrate the effectiveness of using action distribu-tions, value estimations and latent features as supervisions.
Fig. 1 shows the scheme of learning from on-policy su-pervisions labeled by Roach on CARLA. We also record off-policy data from Roach by using its output to drive the vehicle on CARLA. Leveraging 3D detection algorithms
[26, 49] and extra sensors to synthesize the BEV, Roach could also address the scarcity of on-policy supervisions in the real world. This is feasible because on the one hand,
BEV as a strong abstraction reduces the sim-to-real gap
[31], and on the other hand, on-policy labeling does not have to happen in real-time or even onboard. Hence 3D de-tection becomes easier given the complete sequences [37].
In summary, this paper presents Roach, an RL expert that sets a new performance upper-bound on CARLA. More-over, we demonstrate the state-of-the-art performance on both the CARLA LeaderBoard and the CARLA NoCrash benchmark using a single camera based end-to-end IL agent, which is supervised by Roach using our improved training scheme. Our repository is publically available at https://github.com/zhejz/carla-roach 2.