Abstract
The vast majority of modern consumer-grade cameras employ a rolling shutter mechanism, leading to image dis-tortions if the camera moves during image acquisition. In this paper, we present a novel deep network to solve the generic rolling shutter correction problem with two con-secutive frames. Our pipeline is symmetrically designed to predict the global shutter image corresponding to the intermediate time of these two frames, which is difﬁcult for existing methods because it corresponds to a camera pose that differs most from the two frames. First, two time-symmetric dense undistortion ﬂows are estimated by using well-established principles: pyramidal construction, warp-ing, and cost volume processing. Then, both rolling shut-ter images are warped into a common global shutter one in the feature space, respectively. Finally, a symmetric consis-tency constraint is constructed in the image decoder to ef-fectively aggregate the contextual cues of two rolling shutter images, thereby recovering the high-quality global shutter image. Extensive experiments with both synthetic and real data from public benchmarks demonstrate the superiority of our proposed approach over the state-of-the-art methods. 1.

Introduction
Popular low-budget commercial cameras are generally built upon CMOS sensors due to their low cost and sim-plicity in design. Most CMOS cameras employ a rolling shutter (RS) mechanism. Different from the global shutter (GS) camera that exposes all pixels at the same time, the
RS camera is exposed in a row-wise manner from top to bottom. Thus, the images and videos captured by a mov-ing RS camera will have the RS effect (e.g., stretch, wob-ble). We provide an illustration in Fig. 2 while assuming the camera has a frame time of τ . The high dynamic sampling characteristics of the RS mechanism are regarded as an ad-vantage rather than a disadvantage [1], but simply ignoring the RS effects in 3D geometric vision applications may lead to erroneous, undesirable and distorted results [5, 18, 3, 9].
*Corresponding author
To mitigate or eliminate the RS effect which is increasingly becoming a nuisance in photography, a well-known RS cor-rection problem has therefore attracted more and more at-tention [36, 38, 20]. This challenging problem aims at re-covering the GS image corresponding to the camera pose at a speciﬁc exposure time (e.g., 0 or τ in Fig. 2).
Existing works on RS correction generally fall into one of two categories: single-frame-based or multi-frame-based. It is an ill-posed problem in the case of single-frame-based RS correction, where additional prior assumptions need to be formulated explicitly [26, 17, 24, 23] or implic-itly [25, 38]. Following the previous studies [36, 37, 20, 8] in reducing the ill-posedness, we also focus on the well-posed generic RS correction problem from two consecutive frames of a video. In particular, since RS cameras are usu-ally time-synchronized with other sensors (e.g., GS camera,
IMU, etc.) in hardware by referring to the ﬁrst scanline time
[29, 13], we deal with a corresponding challenge of correct-ing the RS images to the GS image corresponding to the exposure time of the ﬁrst scanline of the second frame (i.e., the intermediate time τ of these two frames). This is of both theoretical interest and great practical importance.
To this end, classical two-frame-based methods [36, 37] heavily rely on speciﬁc RS motion models and require nontrivial iterative optimizations, which limits their use in time-constrained applications. Although the state-of-the-art learning-based method [20] has achieved promising results in recovering the GS image corresponding to the middle time 3τ 2 of the second frame, it is prone to fail to predict a plausible GS image at time τ , where many image texture details cannot be well restored (c.f . Fig. 1(d)). Since the
GS image at time 3τ 2 and the second RS image have close content, their asymmetric network only exploits the limited information of the second RS image, but ignores the con-textual aggregation of both RS frames.
Recently, various image-to-image translation problems have been tackled with the deep learning pipelines (e.g., image correction [20], optical ﬂow estimation [32], depth estimation [22], video deblurring [31], and image super-resolution [19], etc.). However, it still poses a challenge to learn to produce a satisfactory GS image corresponding to
(a) Original RS image 1 (b) Original RS image 2 (c) Ground truth GS image (d) Liu et al. [20] (e) Predicted only by RS 1 (f) Predicted only by RS 2 (g) Our corrected GS image
Figure 1. An example of RS correction from two consecutive frames. (a-b) Input two consecutive RS images; (c) Ground truth GS image corresponding to the middle time of two consecutive frames; (d) GS image predicted by the state-of-the-art method [20]; (e-g) The forward, backward, and target GS images predicted from only the ﬁrst RS image, only the second RS image, and the combined two RS images (our pipeline), respectively. The ﬁrst and second RS images have different contributions to the upper and lower regions of the corresponding
GS image which can be seen between yellow and blue boxes. Our proposed approach makes use of this property for RS correction. the intermediate time τ of two consecutive frames. Its com-plexities mainly lie in the following facts: 1) Different from the local neighbors that can provide sufﬁcient description in general image-to-image translation problem, a pixel of the target GS image may not be in the neighboring pixel of its corresponding RS images, depending on the type of motion, the 3D structure, and the scanline time; 2) We observe in
Fig. 1(e)&(f) that the ﬁrst and second RS images contribute greatly to the lower and upper parts of the corresponding time-centered GS image, respectively. This is expected be-cause they are closer in time and thus share more similar camera poses. Consequently, the centered exposure time τ corresponds to the most unique camera pose that deviates greatly from both RS frames, such that only the cues from a single RS image may result in the lack of details of the corrected GS image. Therefore, motivated by these two in-sights, we propose a novel symmetric undistortion network architecture to rectify the geometric inaccuracies induced by the RS effect from two consecutive RS images. The net-work can beneﬁt from the overall exploitation and aggre-gation of contextual information, where the time-centered
GS image can be reconstructed by using an adaptive fusion scheme, as illustrated in Fig. 1(g).
Our network takes two consecutive RS images as in-put and predicts the corrected GS image corresponding to the intermediate time of these two frames. To essentially exploit and merge contextual information across both RS images, our pipeline is symmetrically constructed. It con-sists of two main processes: a PWC (pyramid, warping, and context-aware cost volume)-based undistortion ﬂow es-timator and a time-centered GS image decoder. The suc-cess of the classic PWC-Net framework [32] in aggregating multi-scale context information inspires the construction of our undistortion ﬂow estimator, which is to estimate the pixel-wise undistortion ﬂows of the ﬁrst and second RS im-ages. Note that the context-aware cost volume we construct can effectively promote contextual consistency at different scales. Then, the pixel-wise undistortion ﬂows are used to warp the learned image features to their corresponding GS counterparts. Finally, we develop a time-centered GS image decoder to further align the contextual cues of their warped features and convert the aggregated feature representations to the target GS image in a robust way. Our symmetric undistortion network (SUNet) can be trained end-to-end and solely uses the ground truth GS image for supervision. Fur-thermore, it can also inpaint the occluded regions from the learned image priors, resulting in a high-quality GS image as shown in Fig. 1(g). Experimental results on two bench-mark datasets demonstrate that our approach is superior to the state-of-the-art methods in removing the RS artifacts.
In summary, our main contributions are:
• We propose an efﬁcient end-to-end symmetric RS undistortion network to solve the generic RS correc-tion problem with two consecutive frames.
• Our context-aware cost volume together with the sym-metric consistency constraint can aggregate the con-textual cues of two input RS images effectively.
• Extensive experiments show that our approach per-forms favorably against the state-of-the-art methods in both GS image restoration and inference efﬁciency. 2.