Abstract
Optical flow is inherently a 2D search problem, and thus the computational complexity grows quadratically with re-spect to the search window, making large displacements matching infeasible for high-resolution images. In this pa-per, we take inspiration from Transformers and propose a new method for high-resolution optical flow estimation with significantly less computation. Specifically, a 1D attention operation is first applied in the vertical direction of the tar-get image, and then a simple 1D correlation in the hori-zontal direction of the attended image is able to achieve 2D correspondence modeling effect. The directions of atten-tion and correlation can also be exchanged, resulting in two 3D cost volumes that are concatenated for optical flow es-timation. The novel 1D formulation empowers our method to scale to very high-resolution input images while main-taining competitive performance. Extensive experiments on
Sintel, KITTI and real-world 4K (2160 × 3840) resolution images demonstrated the effectiveness and superiority of our proposed method. Code and models are available at https://github.com/haofeixu/flow1d. 1.

Introduction
Optical flow estimation, a classic topic in computer vi-sion, is a fundamental building block of various real-world applications such as 3D reconstruction [22], video process-ing [20] and action recognition [31]. The recent advance-ment of deep learning enables directly optical flow learning with a neural network [18]. By further improving the archi-tectures and training strategies, deep learning based meth-ods [18, 34, 35, 43, 37] have demonstrated stronger perfor-mance and faster inference speed compared with traditional optimization based approaches [13, 46, 33, 29, 44, 3].
An essential component in deep learning based optical flow frameworks is cost volume [14], which is usually com-puted by the dot product operation (also known as correla-tion [10]) between two feature vectors. It stores the match-*Work primarily done while interning at MSRA mentored by JY
Figure 1: Optical flow factorization. We factorize the 2D optical flow with 1D attention and correlation in orthogonal directions to achieve large displacements search on high-resolution images. Specifically, for the correspondence (red point) of the blue point, we first perform a 1D vertical at-tention to propagate the information of the red point to the green point, which lies on the same row of the blue point.
Then a simple 1D correlation in the horizontal direction can be applied to build a horizontal cost volume. The vertical cost volume can be derived likewise with switched attention and correlation directions. ing costs between each pixel in the source image and its po-tential correspondence candidates in the target image. By explicitly constructing a cost volume layer that encodes the search space, the network learns to better reason about the relative pixel displacements, as demonstrated by the supe-rior performance of FlowNetC than FlowNetS that without such a layer [10, 18].
The original cost volume in FlowNetC [10] is con-structed in a single scale and it has difficulty in modeling large displacements due to the quadratic complexities with respect to the search window. PWC-Net [34] migrates this problem by constructing multiple partial cost volumes in a coarse-to-fine warping framework. However, coarse-to-fine methods tend to miss small objects since they might not be visible in the highly-downsampled coarse scales, and thus have little chance to be correctly estimated [29, 32, 3].
Moreover, warping might introduce artifacts in occlusion regions [24], which may potentially hinder the network to learn correct correspondences.
Current state-of-the-art optical flow method, RAFT [37], maintains a single-resolution feature map and gradually es-timates the flow updates in an iterative manner, eliminat-ing several limitations of previous coarse-to-fine frame-works. One key component in RAFT is a 4D cost vol-ume (H × W × H × W ) that is obtained by computing the correlations of all pairs. Thanks to such a large cost vol-ume, RAFT achieves striking performance on established benchmarks. Nevertheless, the 4D cost volume requirement makes it difficult to scale to very high-resolution inputs due to the quadratic complexity with respect to the image reso-lution. Although one can partially alleviate this problem by processing on downsampled images, some fine-grained de-tails, which might be critical for some scenarios (e.g., ball sports and self-driving cars), will be inevitably lost in such a process. Furthermore, with the popularity of consumer-level high-definition cameras, it is much easier than before to get access to high-resolution videos, which accordingly raises the demand to be able to process such high-resolution videos with high efficiency.
To this end, we propose a new cost volume construction method for high-resolution optical flow estimation. Our key idea is to factorize the 2D search to two 1D substitutes in the vertical and horizontal direction, respectively, such that we can use 1D correlations to construct compact cost volumes.
Intuitively, such 1D correlations are not sufficient for op-tical flow estimation which is inherently a 2D search prob-lem. However, as illustrated in Fig. 1, if we can propagate the information on the target image along the direction or-thogonal to the correlation direction, the computed cost vol-ume will contain meaningful correlation between the source pixel and its correspondence. This insight motivates us to design proper feature propagation and aggregation schemes for 1D correlation. Inspired by Transformers [38], we pro-pose to learn such propagation with the attention mecha-nism, where we first apply 1D self attention on source fea-ture (not shown in Fig. 1 for brevity), then 1D cross atten-tion between the source and target features (see Fig. 3).
Our 1D formulation yields two 3D cost volumes of size (H × W × W ) and (H × W × H), respectively, which are then concatenated for the subsequent optical flow regres-sion. This way, we reduce the complexity of all-pair corre-lation [37] from O(H×W×H×W ) to O(H×W×(H+W )), enabling our method to scale to very high-resolution in-puts with significant less computation. For example, our method consumes 6× less memory than RAFT on 1080p (1080 × 1920) videos. We also show flow results on real-world 4K (2160 × 3840) resolution images and our method can handle images more than 8K (4320 × 7680) resolution on a GPU with 32GB memory. Meanwhile, the evaluation on Sintel [4] and KITTI [26] shows that the accuracy of our method is only slightly worse than RAFT but outperforms other methods such as FlowNet2 [18] and PWC-Net [34].
Our contributions can be summarized as follows:
• We explore an innovative cost volume construction method which is fundamentally different from all ex-isting methods.
• We show that cost volumes constructed using 1D correlations, despite somewhat counter-intuitive, can achieve striking flow estimation accuracy comparable to the state of the art.
• Our method is slightly inferior compared to RAFT in terms of accuracy but enjoys significantly less mem-ory consumption, enabling us to process very high-resolution images (more than 8K resolution (4320 × 7680) in our experiment). 2.