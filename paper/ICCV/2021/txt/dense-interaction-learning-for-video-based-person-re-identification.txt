Abstract
Video-based person re-identification (re-ID) aims at matching the same person across video clips. Efficiently exploiting multi-scale fine-grained features while building the structural interaction among them is pivotal for its suc-cess. In this paper, we propose a hybrid framework, Dense
Interaction Learning (DenseIL), that takes the principal ad-vantages of both CNN-based and Attention-based architec-tures to tackle video-based person re-ID difficulties. Den-seIL contains a CNN encoder and a Dense Interaction (DI) decoder. The CNN encoder is responsible for efficiently ex-tracting discriminative spatial features while the DI decoder is designed to densely model spatial-temporal inherent in-teraction across frames. Different from previous works, we additionally let the DI decoder densely attends to interme-diate fine-grained CNN features and that naturally yields multi-grained spatial-temporal representation for each video clip. Moreover, we introduce Spatio-TEmporal Positional
Embedding (STEP-Emb) into the DI decoder to investigate the positional relation among the spatial-temporal inputs.
Our experiments consistently and significantly outperform all the state-of-the-art methods on multiple standard video-based person re-ID datasets. 1.

Introduction
Person re-identification (re-ID) tells whether a person-of-interest has been noticed in a different location by an-other camera. It is essential to many important surveillance applications such as tracking [70] and retrieval [89].
In recent years, significant progresses have been achieved in image-based person re-ID [42, 59, 63, 22], as well as the video-based one [87, 41, 80], due to the rapid development of Convolutional Neural Networks (CNN) [36].
The goal of image-based person re-ID is to match per-son images captured in different times and locations. To achieve this goal, recent methods are proposed to bet-ter dig appearance features while concurrently to main-tain the robustness with respect to body part misalign-(a) Left: different identities with similar appearance. Right: the same identity with misalignment or occlusion. (b) Our proposed Dense Interaction Learning (DenseIL).
Figure 1: (a): Two key challenges existed in video-based person re-ID. (b): To tackle these two challenges, we intro-duce a DenseIL framework to densely capture multi-grained spatial-temporal interaction with the guidance of global rela-tionship, where the preferences (Ï‰) are automatically learned by our proposed Dense Attention. ment [43, 54, 7, 91, 73, 63, 67, 33, 32, 22]. Nevertheless, it is still insufficient to model the discrete relationships among various misaligned body parts from a single image. This mo-tivates the exploration on video-based re-ID, which further takes adjacent frames of the captured person into consid-eration, and therefore the occluded or missed parts can be inferred. In this sense, the key point to video-based re-ID lies in designing an architecture that is suitable for temporal dynamics, such as leveraging optical flow [9], RNN [92] and 3D CNN [40].
However, the aforementioned methods neglect the im-portance of spatial-temporal interaction between body parts within intra- and inter-frame, which limits their effectivity.
As a result, the state-of-the-arts [80, 79] suggest that graph convolutional network [34] has the merit of modeling spatial-temporal dependencies and shows promising performance on video-based re-ID task. However, their models are built upon the coarse-grained representation while leaving the
fine-grained information implied in each frame not fully ex-ploited. As demonstrated in Figure 1a, when different identi-ties share similar appearance, depending on coarse-grained knowledge is not enough to distinguish the difference. In-stead, fine-grained information (such as a shoe, a shoulder bag, etc.) plays an enormous role in re-identification.
Inspired by this observation, in this paper, we present
Dense Interaction Learning (DenseIL) that not only builds spatial-temporal interaction between body parts but also densely exploits fine-grained cues (see Figure 1b). Basi-cally, DenseIL is composed of a CNN encoder and a Dense
Interaction (DI) decoder. The CNN encoder exerts its ad-vantage on efficiently encoding spatial context into discrimi-native features [37]. Our CNN encoder consists of several
CNN Blocks (e.g., Res-Block [21], Dense-Block [31], SE-Block [30], etc.) and therefore is capable of generating a set of hidden features from low-level/high-resolution to high-level/low-resolution.
Our DI decoder comprises stacked self-attention [66], feed-forward layer, layer normalization [1] and a newly pro-posed Dense Attention. Specifically, our decoder reuses the self-attention module explored in vanilla Transformer to deliberately model spatial-temporal inherent interaction across frames. After that, the subsequent Dense Attention module simultaneously attends to both the outputs of the self-attention module and the intermediate fine-grained CNN features of the CNN encoder with the guidance of global re-lationship (i.e., the last self-attention outputs). Thus, the de-coder can naturally generate a multi-grained spatial-temporal representation for each video clip.
In contrast to ResNet [21] and DenseNet [31] that asso-ciate features of preceding layers through summation or con-catenation, we create dense information flow between CNN and Attention mechanism with the proposed Dense Attention scheme. Intuitively, our Dense Attention is designed to better facilitate the coordination of CNN and Attention mechanism by extracting hybrid information from both the convolution and preceding self-attention features in a single attention function. Acting in this way, the model automatically and flexibly learns the preference on the fine-grained context (frame level, extracted by CNN) and global spatial-temporal interaction across frames (sequence level, modeled by self-attention) when re-identifying person-of-interest, which will be beneficial to scale-variations and misalignment problem, as a by-product of our model design. is
Moreover, intrinsically since the DI decoder permutation-invariant, we further propose Spatio-TEmporal
Positional Embedding (STEP-Emb) to explicitly enhance the chronological relation in intra- and inter-frame. By incorpo-rating STEP-Emb, our DI decoder is able to investigate the absolute or relative position of the spatial-temporal inputs.
Experiments show that our method significantly out-performs the state-of-the-arts on three video-based person re-ID datasets. In particular, we achieve the best perfor-mance of 87.0% and 97.1% mAP on large-scale MARS and
DukeMTMC-VideoReID datasets respectively. 2.