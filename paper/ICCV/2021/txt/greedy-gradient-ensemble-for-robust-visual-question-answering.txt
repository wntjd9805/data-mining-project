Abstract
Language bias is a critical issue in Visual Question An-swering (VQA), where models often exploit dataset bias-es for the final decision without considering the image in-formation. As a result, they suffer from performance drop on out-of-distribution data and inadequate visual explana-tion. Based on experimental analysis for existing robust
VQA methods, we stress the language bias in VQA that comes from two aspects, i.e., distribution bias and shortcut bias. We further propose a new de-bias framework, Greedy
Gradient Ensemble (GGE), which combines multiple biased models for unbiased base model learning. With the greedy strategy, GGE forces the biased models to over-fit the bi-ased data distribution in priority, thus makes the base mod-el pay more attention to examples that are hard to solve by biased models. The experiments demonstrate that our method makes better use of visual information and achieves state-of-the-art performance on diagnosing dataset VQA-CP without using extra annotations. 1.

Introduction
Visual Question Answering (VQA) is a challenging task that requires both language-aware reasoning and image un-derstanding. With advances in deep learning, neural net-works [37, 34, 6, 13, 18, 17, 19, 29] that model the correla-tions between vision and language have shown remarkable results on large-scale benchmark datasets [3, 15, 23, 20].
However, recent studies have demonstrated that most
VQA methods tend to rely on existing idiosyncratic biases in the datasets [15, 24, 43]. They often leverage superfi-cial correlations between questions and answers to train the model without considering exact vision information. For example, a model may blindly answer “tennis” for the ques-tion “What sports ...” just based on the most common textual
QA pairs in the train set. Unfortunately, models exploiting
∗Corresponding author. (a) Distribution Bias (b) Shortcut Bias
Figure 1. Two aspects of language bias in VQA. (a) Distribution
Bias: The answer distribution for certain question type is sig-(b) Shortcut Bias: The correct answers nificantly long-tailed. produced by the model may rely on the question-answer shortcut rather than proper visual grounding. statistical shortcuts during training often show poor gener-alization ability to out-of-domain data, and hardly provide proper visual evidence for a certain answer.
Currently, the prevailing solutions for this problem can be categorized into ensemble-based [36, 7, 10], grounding-based [39, 42, 22] and counterfactual-based [8]. Similar to re-weighting and re-sampling strategies in traditional long-tailed classification [45, 25, 16, 31], ensemble-based meth-ods re-weight the samples by the question-only branch.
Grounding-based models stress a better use of image in-formation according to human-annotated visual explana-tion [11, 21]. Newly proposed counterfactual-based meth-ods [8, 33] further combine these two lines of work and achieve better performance.
Nevertheless, it has been shown that existing methods have not fully leveraged both vision and language informa-tion. For example, Shrestha et al. [40] argue that improved accuracy in grounding-based methods [39, 42] does not ac-tually emerge from proper visual grounding but some un-known regularization effects. Similar to [40], we further analyse all the three categories of existing work by control experiments in Section 3.2. We found that language bias in
VQA is actually two-fold: (a) the statistical distribution gap between train and test, i.e., distribution bias shown in Fig-ure 1(a), and (b) the semantic correlation between specific
QA pairs, i.e., shortcut bias shown in Figure 1(b). Although long-tailed distribution in train set is usually considered to be one of the factors that increase shortcut bias, we exper-imentally demonstrate that they are actually two aspects of the language bias. Grounding supervision in [39] or ensem-ble regularization in [7, 10] does not necessarily force the model to focus on visual information as expected. To en-courage the model to pay attention to the images, we need to explicitly model bothbiases and reduce them step by step.
Inspired by our empirical findings, we propose Greedy
Gradient Ensemble (GGE), a new model-agnostic de-bias framework that ensembles biased models and the base mod-el like gradient descent in functional space. The key idea of our method is to make use of the over-fitting phenomenon in deep learning. The biased part of data is greedily over-fitted by biased features, as a result, the expected base model can be learned with more ideal data distribution and focus on examples that are hard to solve with biased models.
In the experiments, variants of GGE models are provided in ablation study, which demonstrates the generalization a-bility of our method and further supports our claim that dis-tribution bias and question shortcut bias are complementary in VQA. To verify if a model can really use visual informa-tion for the answer decision, we further study the language bias in VQA from a visual modelling perspective. Quantita-tive and qualitative evaluations show that GGE can provide better visual evidence accompanied with predictions.
The major contributions are:
• We provide analysis for the language bias in VQA task and decompose the language bias into distribution bias and shortcut bias.
• We propose a new model-agnostic de-bias framework
Greedy Gradient Ensemble (GGE), which sequentially ensembles biased models for robust VQA.
• On VQA-CP, our method makes better use of visual information and achieves state-of-the-art performance, with 17.34% gain against simple UpDn baseline with-out extra annotations. Code is available at https:
//github.com/GeraldHan/GGE. 2.