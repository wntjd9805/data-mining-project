Abstract
Despite the great progress of person re-identification (ReID) with the adoption of Convolutional Neural Net-works, current ReID models are opaque and only outputs a scalar distance between two persons. There are few meth-ods providing users semantically understandable explana-tions for why two persons are the same one or not.
In this paper, we propose a post-hoc method, named Attribute-guided Metric Distillation (AMD), to explain existing ReID models. This is the first method to explore attributes to answer: 1) what and where the attributes make two per-sons different, and 2) how much each attribute contributes to the difference.
In AMD, we design a pluggable inter-preter network for target models to generate quantitative contributions of attributes and visualize accurate attention maps of the most discriminative attributes. To achieve this goal, we propose a metric distillation loss by which the in-terpreter learns to decompose the distance of two persons into components of attributes with knowledge distilled from the target model. Moreover, we propose an attribute prior loss to make the interpreter generate attribute-guided at-tention maps and to eliminate biases caused by the imbal-anced distribution of attributes. This loss can guide the in-terpreter to focus on the exclusive and discriminative at-tributes rather than the large-area but common attributes of two persons. Comprehensive experiments show that the interpreter can generate effective and intuitive explanations for varied models and generalize well under cross-domain settings. As a by-product, the accuracy of target models can be further improved with our interpreter. 1 1.

Introduction
Person Re-identification (ReID), i.e., retrieval of the same person captured by multiple cameras, has attracted
*This work was done when Xiaodong Chen was an intern at JD AI
Research.
†Wu Liu is the corresponding author 1See the project on www.xiaodongchen.cn/AMD.github.io/
Figure 1. The motivation of attribute-guided metric distillation. (a)
Given a query, the ReID model returns a rank list of gallery im-ages based on pairwise metrics. (b) The learned Interpreter can visualize intuitive attention maps of attributes to tell users what at-tributes make two persons different, and generate contributions of attributes to reflect the impact of each attribute. (c) Refined results by re-weighted distances from Interpreter. (Best viewed in color.) tremendous attention from academia and industry [19, 30, 32, 34, 41, 42]. Although Convolutional Neural Networks (CNNs) have significantly improved the accuracy of per-son ReID, we still cannot completely trust the results pro-duced by black-box models, especially for critical scenar-ios [43]. Therefore, this paper is focused on the interpre-tation of CNN-based person ReID models which is crucial yet rarely studied.
In recent years, there has been a surge of work in discov-ering how a target CNN processes input images and makes predictions [10, 24, 46]. These methods usually visualize gradients or salient regions on feature maps w.r.t. the in-put image and its prediction [4, 8, 24, 25, 26, 46]. Partic-ularly, Chen et al. [5] proposed to explain neural networks semantically and quantitatively by decomposing the predic-tion made by CNNs into semantic concepts by knowledge distillation. However, these methods mainly consider clas-sification problems. They cannot be directly applied to per-son ReID, which is an open-set retrieval task and usually solved by metric learning [42, 45].
A CNN-based ReID system usually maps a query image and gallery images into a metric space, then outputs pair-wise distances by which a rank list of gallery images is re-turned, as shown in Figure 1 (a). Although Yang et al. [36] proposed Ranking Activation Maps which could visualize related regions of two persons, it still cannot semantically explain why they are similar or not. Attributes, e.g., colors and types of clothes, shoes, etc., are semantically under-standable for humans and have been exploited as mid-level features for person ReID [18], but there is no method us-ing attributes for explanations of person ReID. Therefore, we aim to learn an interpreter with the help of semantic attributes for answering two questions: 1) what attributes make two persons different, and 2) how much impact each attribute contributes to the difference, as shown in Figure 1 (b). In real applications, the interpreter not only can help users focus on the most discrepant attributes of two persons but also can assist developers to improve the accuracy of
ReID models, as shown in Figure 1 (c).
However, interpretation of ReID models with attributes faces unique challenges. Firstly, since the output of ReID models are distances of pairwise images, it is difficult to use class activation or gradients to visualize salient regions or disentangle semantics as classification [24, 46]. Moreover, persons in the wild can be described by various fine-grained and imbalanced attributes [7, 15, 18], which may bring bi-ases to ReID results as well as the explanations. For ex-ample, attributes with large areas, such as coats and pants, always overwhelm small but discriminative ones like hats and shoes. Furthermore, there are only weakly-annotated image-level or ID-level attribute labels without accurate bounding boxes or masks [18], which makes it hard to learn accurate locations and intuitive visualizations for attributes.
To this end, we propose a post-hoc method, named
Attribute-guided Metric Distillation, which explores se-mantic attributes towards explainable person ReID. Specif-ically, we design a pluggable interpreter network to utilize the knowledge from a target ReID model by metric distil-lation. The interpreter is grafted on the target ReID model and directly adopts the parameters of the first several CNN stages to exploit the low-level and mid-level features of the target model. The rest layers of the interpreter are equipped with an attribute decomposition head, by which the inter-preter can learn to generate a set of attribute-guided atten-tion maps (AAMs) for a pair of input person images. On the one hand, the generated AAMs can be directly used to visualize discriminative attributes for the image pair. On the other hand, the AAMs can be applied to the visual features of the image pair from the target model. By this means, their features and distance can be decomposed into attribute-guided components to quantify the contribution of each at-tribute to the overall distance. Thus, the interpreter not only can output the quantitative contributions of attributes to the overall distance of two persons but also can generate intu-itively visualizations of attributes for users.
To guide learning of the interpreter, we design two loss functions. One is a metric distillation loss which can guar-antee the consistency between two distance metrics: 1) the decomposed attribute-guided distances from the interpreter, and 2) the overall distance from the target model. The other is the attribute prior loss. It is designed based on the ob-servation that the difference between two persons mainly comes from the exclusive attributes rather than common ones. Thus, the attribute prior loss makes the interpreter pay more attention to exclusive but discriminative attributes of two persons with only weakly-labeled attributes of persons.
The contributions of this paper are three-fold:
• This is one of the first attempt toward explainable per-son ReID by attribute-guided metric distillation that can semantically and quantitatively explain the results of existing ReID models;
• We design a pluggable interpreter network with an at-tribute decomposition head to obtain contributions of attributes to the difference of two persons and generate intuitive visualizations for target ReID models;
• To guide the learning of the interpreter, the metric dis-tillation loss and attribute prior loss are proposed to guarantee consistency during metric distillation and prevent biases of attributes.
To show the effectiveness and compatibility of the inter-preter, we apply it to the state-of-the-art ReID models on different datasets with comprehensive experiments. As a by-product, the performance of the state-of-the-art models is further improved with our interpreter. 2.