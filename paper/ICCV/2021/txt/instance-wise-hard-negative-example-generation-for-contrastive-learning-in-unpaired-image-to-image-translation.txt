Abstract
Contrastive learning shows great potential in unpaired image-to-image translation, but sometimes the translated results are in poor quality and the contents are not pre-served consistently. In this paper, we uncover that the neg-ative examples play a critical role in the performance of contrastive learning for image translation. The negative ex-amples in previous methods are randomly sampled from the patches of different positions in the source image, which are not effective to push the positive examples close to the query examples. To address this issue, we present instance-wise hard Negative Example Generation for Contrastive learn-ing in Unpaired image-to-image Translation (NEGCUT).
Speciﬁcally, we train a generator to produce negative exam-ples online. The generator is novel from two perspectives: 1) it is instance-wise which means that the generated exam-ples are based on the input image, and 2) it can generate hard negative examples since it is trained with an adversar-ial loss. With the generator, the performance of unpaired image-to-image translation is signiﬁcantly improved. Ex-periments on three benchmark datasets demonstrate that the proposed NEGCUT framework achieves state-of-the-art performance compared to previous methods. 1.

Introduction
Image-to-image translation aims to transfer images from the source domain to the target domain with the content in-formation preserved, which is of signiﬁcant importance on various applications such as style transfer [13, 24, 29, 37], domain adaption [4, 19, 20, 33, 51] and image coloriza-tion [2, 48, 58, 60]. Due to the inconvenience of collect-ing paired training data, recent methods are usually based on the unpaired setting. In that case, cycle-consistency loss has been widely used to preserve the consistency between the source images and generated images, for instance, Cy-cleGAN [61], StarGAN [7], UNIT [34] and MUNIT [22].
*Corresponding author: Wengang Zhou and Houqiang Li.
Source Image
CUT Result
Source Image
NEGCUT Result
Generating (cid:125)
Neg
Pos
Query
Pos
Query
Sampling (cid:125)
Neg y c n e u q e r
F
Query-Neg Cosine Similarity
Figure 1. We visualize the generated images along with the distri-bution of cosine similarity between query and negative samples in
CUT [41] and our method. The blue histogram refers to the distri-bution in CUT while the orange histogram refers to the distribution in our method.
The recently proposed method CUT [41] introduces con-trastive learning in unpaired image-to-image translation and achieves better performance over methods [31, 34, 61] that use cycle-consistency loss.
In this paper, we aim to fur-ther improve the performance of contrastive learning for unpaired image-to-image translation. We uncover that the performance of contrastive learning relies heavily on the hardness of negative samples. As shown in Figure 1, the negative samples in the method [41] are randomly sampled from the patches of different positions in the image, which sometimes leads to the translated results in poor quality and the contents not preserved consistently. Also we calculate the cosine similarities between the query patches and neg-ative patches, and we can ﬁnd that their cosine similarities are around 0. In other words, these negative patches are not challenging enough to push the positive examples close to the query examples, which will result in the framework not taking full advantage of contrastive learning.
To address the above issue, we present instance-wise hard Negative Example Generation for Contrastive learning in Unpaired image-to-image Translation (NEGCUT) in this paper. More precisely, we propose a novel negative genera-tor to excavate hard negative examples. For a source image, we ﬁrst extract its features on different layers of image gen-erator encoder and embed them into feature vectors. Based on the embedded features from source images, the negative generator produces instance-wise negative examples related to the source image. Moreover, the negative samples should be diverse enough to push the query patch closer to the pos-itive patch. To this end, we add the noise as an extra input for the generator. However, the noise input can probably be ignored for the generator, thus the generator can gener-ate similar examples for different input noises. This is also called the mode collapse issue [45]. Inspired by the mode seeking loss in MSGAN [38], we introduce diversity loss to the generator to encourage the generator to produce diverse hard negative samples for different input noise.
To generate challenging negative samples for contrastive learning, the main idea is to train the negative generator against the encoder network in an adversarial manner. Two components in the framework, i.e., the encoder network and negative generator, are updated alternatively to play a min-max game. On one hand, the encoder network narrows the distance between query and positive samples against hard negative samples to minimize contrastive loss. On the other hand, the negative generator produces hard negative sam-ples close to the positive samples to maximize contrastive loss. Intuitively, the framework will reach an equilibrium where the encoder learns detailed and distinguishing rep-resentation to discriminate the positive samples from gen-In Figure 1, we visualize erated hard negative samples. the generated images along with the distribution of cosine similarity between the query and negative samples in the
CUT and NEGCUT. It is observed that the negative samples produced by negative generator are harder than those sam-pled in the method [41], which push the encoder network to learn distinguishing representation and ﬁnally results in
ﬁne-grained correspondence of structures and textures.
Our contributions are summarized as follows,
• We identify that instance-wise negative examples that increase hardness as training process play a critical role in the performance of contrastive learning for un-paired image-to-image translation.
• We propose a novel framework NEGCUT to mine instance-wise hard negative examples for contrastive learning in unpaired image-to-image translation.
• Extensive experiments on three benchmark datasets demonstrate the superiority of our method, which achieves new state-of-the-art performance. The gen-erated images of our method are of better visual per-formance with consistent detailed correspondence. 2.