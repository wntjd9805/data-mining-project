Abstract
Learning to model how the world changes as time elapses has proven a challenging problem for the computer vision community. We introduce a self-supervised approach to this problem that solves a multi-modal temporal cycle consistency objective jointly in vision and language. This objective requires a model to learn modality-agnostic func-tions to predict the future and past that undo each other when composed. We hypothesize that a model trained on this objective will discover long-term temporal dynamics in video. We verify this hypothesis by using the resultant visual representations and predictive models as-is to solve a variety of downstream tasks. Our method outperforms state-of-the-art self-supervised video prediction methods on future action anticipation, temporal image ordering, and arrow-of-time classiﬁcation tasks, without training on tar-get datasets or their labels. 1.

Introduction
Prediction is a central problem in computer vision which researchers have been grappling with since the early days of the ﬁeld [10, 12, 22, 29, 34, 39, 54]. Previous deep learn-ing methods have largely focused on predicting ﬁxed, small offsets into the future. To understand why this formulation is ﬂawed, consider Figure 1. This ﬁgure shows a frame (a) from a video at time t and three frames at times > t. Which of the three should be the output of a model that predicts the future? Option (d) is closest to the future that humans are likely to imagine. By predicting frames such as option (b), which occur in the immediate future [15, 16, 47, 53], we limit the scope of temporal transitions that can be learned by models and hurt downstream performance.
Motivated by this example, we identify three central challenges in training a model to predict the future. First, manually annotating videos with temporal relationships be-tween frames is prohibitively expensive, and ground truth may be difﬁcult to deﬁne. Therefore, models should be
∗ Work done as an intern at Google Research.
Figure 1. Predicting the future is challenging. Given a frame (a) at time t, previous work focuses on predicting frames at a ﬁxed off-set, such as (b). However, these frames are often either redundant or stochastic, motivating the prediction of non-immediate futures.
Predicting such a frame is highly non-trivial, as many are irrel-evant, such as (c). Aided by the textual information in narrated video, we can learn long-term temporal dynamics in video, and predict (d). We learn these dynamics by solving a multi-modal temporal cycle consistency problem. able to learn from large unlabeled datasets of in-the-wild ac-tion and discover transitions autonomously, to enable prac-tical applications. Second, modeling the complex long-term transitions in the real world requires learning high-level concepts, more naturally found in abstract latent representa-tions than raw pixels. Finally, the duration elapsed by tem-poral transitions can vary signiﬁcantly depending on con-text, and models must be able to make predictions at varied offsets into the future. To satisfy these desiderata, we intro-duce a new self-supervised training objective, Multi-Modal
Temporal Cycle Consistency (MMCC), and a model that learns a representation to solve it.
We show the MMCC objective in Figure 2. Starting from a sampled frame in a narrated video, our model learns to at-tend among all narration text to retrieve a relevant utterance.
Combining both modalities, the model learns a function to predict a latent future, attending over the entire video to re-trieve a future frame. This frame’s corresponding utterance is estimated, and a function to predict a past frame is learned in a similar way. The cycle constraint requires that the ﬁnal model prediction be equal to the starting frame.
MMCC addresses all three challenges discussed above.
In Figure 1, only (d) is a viable solution to our cycle formu-lation. Selecting (c) as a future would not allow the model to return to (a), since the two frames have no clear relation-ship. On the other hand, because the model does not know which modality its input comes from—and therefore must operate equally on vision and language—it is discouraged from selecting lower-level future frames such as (b), which likely do not accompany a predictable change in text.
We show that our model, trained end-to-end from scratch to solve the MMCC objective on the HowTo100M dataset
[37], captures long-term dynamics in its predictive model of the future, and can be used without further training to an-ticipate future actions, order image collections, and identify salient temporal relationships in long videos. It also learns representations of video and text that contain information relevant to modeling temporal dynamics, which we demon-strate to be crucial to the quality of prediction.
Our main contributions are:
• MMCC, a self-supervised multi-modal temporal cycle consistency objective that requires learning visual rep-resentations attuned to temporal dynamics, as well as long-term predictive models of the future and past.
• An attention-based model to solve this objective, which uses cross-modal and temporal cues to discover relationships through time in video.
• Since no previous self-supervised benchmarks exist in this area, a suite of qualitative and quantitative tasks to evaluate learned representations and predictive mod-els. Our model outperforms the self-supervised SOTA in video prediction on all tasks. 2.