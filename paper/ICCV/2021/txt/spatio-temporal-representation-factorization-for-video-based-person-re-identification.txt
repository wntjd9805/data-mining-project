Abstract
Despite much recent progress in video-based person re-identification (re-ID), the current state-of-the-art still suffers from common real-world challenges such as ap-pearance similarity among various people, occlusions, and frame misalignment.
To alleviate these problems, we propose Spatio-Temporal Representation Factorization (STRF), a flexible new computational unit that can be used in conjunction with most existing 3D convolutional neural network architectures for re-ID. The key innova-tions of STRF over prior work include explicit pathways for learning discriminative temporal and spatial features, with each component further factorized to capture com-plementary person-specific appearance and motion infor-mation. Specifically, temporal factorization comprises two branches, one each for static features (e.g., the color of clothes) that do not change much over time, and dynamic features (e.g., walking patterns) that change over time. Fur-ther, spatial factorization also comprises two branches to learn both global (coarse segments) as well as local (finer segments) appearance features, with the local features par-ticularly useful in cases of occlusion or spatial misalign-ment. These two factorization operations taken together re-sult in a modular architecture for our parameter-wise light
STRF unit that can be plugged in between any two 3D convolutional layers, resulting in an end-to-end learning framework. We empirically show that STRF improves per-formance of various existing baseline architectures while demonstrating new state-of-the-art results using standard person re-ID evaluation protocols on three benchmarks. 1.

Introduction
We consider the problem of video-based person re-IDentification (re-ID). Given a video tracklet of a person of interest, the task is to retrieve the closest match (which ide-ally should be the true match) among a gallery set of video
⋆ This work was done during Abhishek Aich’s internship with United
Imaging Intelligence. Corresponding author: Srikrishna Karanam.
Figure 1: Illustration of proposed concept and its efficacy. We present the intuition behind our proposed Spatio-Temporal Rep-resentation Factorization (STRF) module designed to overcome common real-world re-ID system issues, e.g., similar-appearance identities, occlusions and misaligned frame. By capturing tempo-rally static/dynamic and spatially coarse/fine information at dif-ferent layers of a 3D-CNN, STRF produces robust discrimina-tive representation to tackle these challenges as demonstrated here through attention maps of penultimate layer of feature extractor. tracklets. With numerous applications in security, surveil-lance, and forensics [2], this problem has seen a dramatic increase in interest and various methodologies in the vision community [7, 13, 28, 29, 32, 36, 50, 54].
While there has been admirable progress in image-based re-ID as evidenced by recent quantitative results [7], there are many challenges that still preclude the ubiquitous use of re-ID algorithms in real-world systems. One such issue is appearance similarity, where multiple people wear simi-lar looking clothes (e.g., large conferences or public events with a strict dress code). Other challenging issues include occlusions and frame misalignment that are a direct conse-quence of large crowd flow densities (e.g., in airports just after flight arrival) and inter-camera viewpoint disparities.
Having access to additional data, e.g., an extra temporal di-mension like videos instead of 2D images, can help alleviate some of these issues by leveraging spatio-temporal data.
Video-based re-ID has seen much recent work [4, 5, 14, 19, 27, 45, 46, 49] in part due to the availability of rele-vant large-scale video datasets [41, 52]. However, learn-ing a spatio-temporal representation that can alleviate the issues noted above still remains a challenge. While ad-vances in general 3D convolutional networks (3D-CNNs) provide reasonable baseline spatio-temporal features, ex-isting re-ID techniques typically rely on specialized archi-tectures [20, 45, 46, 49] that are inflexible to be used with these baseline models. Other lines of work are focused en-tirely on learning either temporal or spatial representations separately [4, 5, 19], overlooking the complementarity that both streams of information provide in challenging scenar-ios, e.g., distinguishing people wearing similar clothes.
To address the aforementioned issues, we present a flex-ible new computational unit called Spatio-Temporal Rep-resentation Factorization (STRF) module. Given a feature volume from a certain 3D convolutional layer in a baseline 3D-CNN model, STRF extracts complementary informa-tion along both spatial (h × w) and temporal (time, t) di-mensions. By design, the proposed STRF module can be in-serted in an existing 3D-CNN model after any convolutional layer, introducing only ∼0.15 million learnable parameters per unit (for instance, this results in only a ∼1.73% over-all parameter increase with I3D [3]), resulting in a flexible and parameter-wise economic framework that is end-to-end trainable. STRF comprises two modules, called temporal feature factorization module (FFM) and spatial feature fac-torization module, to process feature tensors. The design principles of these modules are motivated by certain obser-vations in video tracklets, which we discuss next.
The intuition behind STRF is demonstrated in Figure 1.
We begin with the factorization module in the temporal di-mension. First, the overall or “global” appearance of the person (e.g., color of clothes, skin, hair, etc) in a tracklet does not change (static) substantially over time. While one can argue these can change with illumination variations, we assume these variations are limited in a given camera view over a short period of time. Next, the walking patterns of a person may change over time, e.g., walking on a level surface vs. climbing stairs (dynamic). Consequently, there are two possible information factorization strategies when processing feature maps: low-frequency (static) sampling and high-frequency (dynamic) sampling. Low-frequency sampling of feature maps results in capturing the “slowly-moving” or approximately constant features, i.e., the ap-pearance information. On the other hand, high-frequency sampling of feature maps results in capturing information that is more dynamically varying, i.e., walking patterns
[31]. The temporal factorization module results in capturing static and dynamic features across time, which is especially helpful in identifying different individuals with similar ap-pearance (see last row video tracklet in Figure 1).
The spatial factorization module, on the other hand, does the same low-frequency (which we call “coarse”) and high-frequency (which we call “fine”) sampling and processing as above, but along the spatial h × w dimensions. This is motivated by commonly occurring real-world issues such as occlusions and frame misalignment. Under these scenarios, the spatial FFM’s high-frequency sampling and processing unit is able to capture more “details” of the person of in-terest as opposed to the other entities that are the causes of occlusion, or other background information in the case of misalignment. To understand this better, observe the atten-tion maps for top row video tracklet in Figure 1. The base-line model, without our proposed module, highlights mostly the bicycle regions in the feature maps, whereas by adding our module, the model is able to capture the person regions in the frames more comprehensively. Similarly, to cover cases where there are no occlusions or misalignment, the spatial FFM’s low-frequency sampling and processing unit become responsible for capturing more slowly-varying or spatially global appearance information. This results in the spatial factorization module to capture two separate streams of spatial information for robust representations.
To summarize, when multiple people in the gallery “look alike” (e.g., same clothes), features from our temporal fac-torization branch help disambiguate (i.e., people may look alike but walk differently). On the other hand, with occlu-sion/clutter, our idea is to rely on “local” features, which can be learned using our spatial branch. Our main contri-butions are as follows.
• We present a novel framework in video-based re-ID to learn discriminative 3D features by factorizing both temporal and spatial dimension of features into low-frequency (static/coarse) and high-frequency (dy-namic/fine) components to tackle misalignment, occlu-sion, and similar appearance problems.
• To realize these factorization, we propose a flexible train-able unit with negligible computational overhead, called
Spatio-Temporal Representation Factorization (STRF) module, that can be used in conjunction with any base-line 3D-CNN based re-ID architecture (see Figure 2).
• We conduct extensive experiments on multiple datasets to demonstrate how the proposed STRF module im-proves the performance of baseline architectures and also achieves state-of-the-art performance obtained by stan-dard re-ID evaluation protocols (see Table 2 and 3). 2.