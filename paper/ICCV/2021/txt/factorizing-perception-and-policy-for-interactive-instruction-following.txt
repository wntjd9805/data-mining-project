Abstract
Performing simple household tasks based on language directives is very natural to humans, yet it remains an open challenge for AI agents. The ‘interactive instruction follow-ing’ task attempts to make progress towards building agents that jointly navigate, interact, and reason in the environ-ment at every step. To address the multifaceted problem, we propose a model that factorizes the task into interactive perception and action policy streams with enhanced com-ponents and name it as MOCA, a Modular Object-Centric
Approach. We empirically validate that MOCA outperforms prior arts by signiﬁcant margins on the ALFRED bench-mark with improved generalization. 1.

Introduction
The prospect of having a robotic assistant that can carry out daily chores based on language directives is a dis-tant dream that has eluded the research community for decades. On recent progress in computer vision, natu-ral language processing and embodiment, several bench-marks have been developed to encourage research on var-ious components of such instruction following agents in-cluding navigation [2, 6, 8, 23], object interaction [30, 41], and interactive reasoning [11, 15] in visually rich 3D envi-ronments [5,22,33]. However, to move towards building re-alistic assistants, the agent should possess all these abilities.
Taking a step forward, we address the more holistic task of interactive instruction following [15, 30, 34, 41] which re-quires an agent to navigate through an environment, inter-act with objects, and complete long-horizon tasks, follow-ing natural language instructions with egocentric vision.
To accomplish a goal in the interactive instruction fol-lowing task, the agent should infer a sequence of actions and object interactions. While action prediction requires global semantic cues, object localisation needs a pixel-level
∗: equal contribution. §: work done while with GIST. †: corresponding author.
Figure 1: We divide interactive instruction following into percep-tion and policy. Each heat-map indicates where a stream focuses on in the given visual observation. While a single stream exploits the same features for pixel-level and global understanding and thus fails to interact with the object, our factorized approach handles perception and policy separately and interacts successfully. understanding of the environment, making them semanti-cally different tasks.
In addition, in neuroscience litera-ture [14], there is a human visual cortex model that has two pathways; the ventral stream (involved with object percep-tion) and the dorsal stream (involved with action control).
Inspired by them, we present a Modular Object-Centric
Approach (MOCA) to factorize interactive perception and action policy in separate streams in a uniﬁed end-to-end framework for building an interactive instruction following agent. Speciﬁcally, our agent has the action policy module (APM) which is responsible for sequential action prediction and the interactive perception module (IPM) that localises the objects to interact with.
Figure 1 shows that our two-stream model is more bene-ﬁcial than the single-stream one. The heat maps indicate the model’s visual attention. For the action of ‘picking up the candle,’ the proposed factorized model focuses on a candle in both the streams and results in a successful interaction.
In contrast, the single-stream model does not attend on the candle, implying the challenge to handle two different pre-dictions in a single stream.
In the IPM, we propose to reason about object classes
for better localisation and name it object-centric localisation (OCL). We further improve the localising ability in time by using the spatial relationship amongst the objects that are interacted with over consecutive time steps. For better grounding of visual features with textual instructions, we propose to use dynamic ﬁlters [20, 24] for its effectiveness of cross-modal embedding. We also show that these com-ponents are more effective when employed in a model that factorizes perception and policy.
We train our agent using imitation learning, speciﬁcally behavior cloning. When a trained agent’s path is blocked by immovable objects like walls, tables, kitchen counters, etc. at inference, however, it is likely to fail to escape such obstacles since the ground truth contains only perfect ex-pert trajectories that ﬁnish the task without any errors. To avoid such errors, we further propose an obstruction evasion mechanism in APM. Finally, we adopt data augmentation to address the sample insufﬁciency of imitation learning.
We empirically validate our proposed method on the re-cently proposed ALFRED benchmark [34] and observe that it signiﬁcantly outperforms prior works in the literature by large margins in all evaluation metrics.
We summarize our contributions as follows:
• We propose to factorize perception and policy for em-bodied interactive instruction following tasks.
• We also present an object-centric localisation and an obstruction evasion mechanism for the task.
• We show that this agent outperforms prior arts by large margins in all metrics.
• We present qualitative and quantitative analysis to demonstrate our method’s effectiveness. 2.