Abstract ments show that the proposed method achieves state-of-the-art performance on several challenging benchmarks.
Reconstructing a high-precision and high-fidelity 3D hu-man hand from a color image plays a central role in repli-cating a realistic virtual hand in human-computer interac-tion and virtual reality applications. The results of current methods are lacking in accuracy and fidelity due to various hand poses and severe occlusions. In this study, we propose an I2UV-HandNet model for accurate hand pose and shape estimation as well as 3D hand super-resolution reconstruc-tion. Specifically, we present the first UV-based 3D hand shape representation. To recover a 3D hand mesh from an
RGB image, we design an AffineNet to predict a UV posi-tion map from the input in an image-to-image translation fashion. To obtain a higher fidelity shape, we exploit an additional SRNet to transform the low-resolution UV map outputted by AffineNet into a high-resolution one. For the first time, we demonstrate the characterization capability of the UV-based hand shape representation. Our experi-1.

Introduction
Observing and understanding the human hand has been an important task in computer vision and human-computer interaction, with applications from gesture recognition to augmented reality (AR) and virtual reality (VR). Recently, we have witnessed significant progress in 3D hand pose and shape estimation [5, 7, 8, 10, 14, 36, 52, 49], driven by efforts in large-scale data collection and annotation
[46, 52, 53], coupled with the development of 3D repre-sentations and learning methods [6, 33]. This has led to re-markable advances in 3D hand understanding from a single-view color image.
Due to the lack of hand surface data, most of the ear-lier works study 3D pose estimation by estimating 3D joint location from a single image [3, 7, 20, 37, 52]. However, the sparse joints representation cannot meet the needs of
many applications such as interacting a virtual hand with an object in some immersive VR scenarios [19]. To bet-ter display the hand surface, previous approaches regress a parametric hand model (MANO) [34] with articulated and nonrigid deformations [5, 15, 26, 31, 48]. Although it is easy to use CNN to predict the MANO parameters from RGB input and use 3D annotations to supervise this regression process [17, 53], this high-dimensional nonlin-ear regression limits the accuracy of reconstructed hands.
Then regression-based methods introduce various interme-diate representations to guide the training process. In these methods, the 3D hand reconstruction is decomposed into two stages, that first regresses a set of intermediate repre-sentations such as 2D keypoints, masks, or 3D keypoints, then predicts the model parameters from these intermediate representations [48, 50]. The performance of these works largely depends on the design of these intermediate repre-sentations as well as the usage of reasonable supervision terms. More recently, [14, 27] remove the dependence of parametric model prior and directly regress the 3D coordi-nates of mesh vertices. Even though good performances are shown, the above methods, which estimate model parame-ters or vertices coordinate from high-dimensional encoded features, break the spatial relationship contained in the orig-inal pixel space.
[29] proposes to predict a 1D heatmap for each mesh vertex coordinate and achieves state-of-the-art performance, but it only preserves spatial information in feature transformation while its vertex-wise output is still a discrete 3D representation. Different to above 3D represen-tation and learning method, we propose to use UV position map [13] as the hand representation in this work.
Inspired by recent 3D body recovery methods that map a 3D mesh of the human body into a UV map representa-tion [1, 43], we propose to represent 3D hand surface in
UV space and train a neural network to predict 3D hand shape from a single RGB input. The usage of UV repre-sentation enables an efficient network to directly regress the hand surface, without relying on any model prior or inter-mediate representations. To properly predict the UV posi-tion map from the RGB input, we present AffineNet that ad-dresses the single-view 3D reconstruction issue in an image-to-image translation task. Traditional image-to-image con-version pipelines are designed for tasks (such as appear-ance conversion or semantic segmentation) with good spa-tial alignment between the input and the output [41, 51].
However, in our setup, the hand shape displayed by the UV position map is different from that in the input RGB image.
To address this problem, we propose a novel affine connec-tion module to align the encoded feature maps with the UV maps and then connect the aligned feature maps with the decoded feature maps. In AffineNet, hierarchical UV posi-tion maps and multi-level feature maps are employed, and multiple UV maps can be supervised at the training stage.
For 3D pose estimation, we obtain a set of 3D keypoints from the output hand mesh via a pre-defined mapping.
Another advantage of the UV-based representation is that the dense UV position map enables reconstructing a 3D sur-face with more vertices by sampling in the valid area of the UV position map. Motivated by this observation, we present a UV-based 3D hand super-resolution reconstruc-tion module named SRNet to realize high-fidelity 3D hand reconstruction from the coarse 3D hand shape. In order to make the best of the proposed hand UV position map repre-sentation, we restore high-fidelity hand shape by using a
CNN to map the low-resolution UV position map into a high-resolution one. However, there lacks of high-fidelity hand surface data to supervise the learning of SRNet. Thus, we construct a scan dataset called SuperHandScan to learn the SRNet. We transfer the high-quality 3D hand scan and the registered coarse MANO model to high/low-resolution
UV position maps, and then use those UV maps to train the
SRNet. Since the input of SRNet is a coarse hand mesh in UV-based representation, there is wide application scope for the SRNet, in other words, a well-trained SRNet can be used for mesh super-resolution reconstruction of any coarse hand mesh.
In summary, we present an I2UV-HandNet model which consists of an AffineNet for 3D hand pose and shape esti-mation and an SRNet for hand mesh super-resolution recon-struction. Overall, the main contributions of this paper are summarized as follows:
• To our best knowledge, we are the first to introduce UV map representation in 3D hand pose and shape estima-tion. Based on our novel representation, we propose an end-to-end network named AffineNet to predict hand mesh from a single color image.
• For the first time in hand reconstruction, we propose
SRNet, a hand mesh super-resolution reconstruction network to predict a high-fidelity hand mesh from a coarse hand mesh.
• Our method can predict accurate and high-fidelity hand meshes from RGB inputs. Experimental results show that our method surpasses other state-of-the-art methods on multiple challenging datasets. 2.