Abstract
The popular unsupervised video object segmentation methods fuse the RGB frame and optical ﬂow via a two-stream network. However, they cannot handle the distract-ing noises in each input modality, which may vastly de-teriorate the model performance. We propose to estab-lish the correspondence between the input modalities while suppressing the distracting signals via optimal structural matching. Given a video frame, we extract the dense lo-cal features from the RGB image and optical ﬂow, and treat them as two complex structured representations. The
Wasserstein distance is then employed to compute the glob-al optimal ﬂows to transport the features in one modality to the other, where the magnitude of each ﬂow measures the extent of the alignment between two local features. To plug the structural matching into a two-stream network for end-to-end training, we factorize the input cost matrix into small spatial blocks and design a differentiable long-short
Sinkhorn module consisting of a long-distant Sinkhorn lay-er and a short-distant Sinkhorn layer. We integrate the mod-ule into a dedicated two-stream network and dub our model
TransportNet. Our experiments show that aligning motion-appearance yields the state-of-the-art results on the popular video object segmentation datasets. 1.

Introduction
Video Object Segmentation (VOS) aims to track the moving objects with an accurate segmentation mask. It can be divided into two scenarios depending on whether the tar-get objects are indicated at the test time. One scenario is the Semi-supervised VOS (SVOS) [59], where a model is
Figure 1. Examples of distracting signals in the appearance and motion input in UVOS. Our proposed TransportNet can gener-ate accurate segmentation masks (column (d)) comparing to the method without Optimal Structural Matching (OSM) (column (e)). trained over a training set, and at the test time the model is provided with the ground-truth mask on the ﬁrst frame as prior to track the objects to be segmented in the subsequent frames. The other is called Unsupervised VOS (UVOS) or primary object segmentation [21]1, where no ground-truth mask is provided at the test time and there is no prior infor-mation about the target object. UVOS discovers the most salient, or primary, objects that move against a video’s back-ground2, and all objects that consistently appear throughout the video with predominant motion is deﬁned as one objec-t [36, 2] (e.g., the person and the motorbike in Figure 1 are deﬁned as one object). This is essentially different from the recently proposed task of Unsupervised Multi-object Seg-mentation [2], a variant of the conventional UVOS for seg-menting separate objects in DAVIS-17 dataset [39], or the task of Video Instance Segmentation [60].
We focus on the UVOS as it requires no user inter-actions. Since the target objects are unknown, the state-of-the-art UVOS methods rely on the motion cues (i.e.,
∗Corresponding author. This work is supported in part by Nation-al Key Research and Development Program of China under Grant No. 2018AAA0100400, in part by the NSFC (61876088, 61825601), in part by the 333 High-level Talents Cultivation Project of Jiangsu Province (BRA2020291). 1 Besides these two names, it is also called “zero-shot VOS” in the literature. 2 In UVOS, the moving objects that are more likely to be followed by hu-man gaze are referred to as “foreground” while the remaining regions (e.g.,
“people in crowds” or “still cars in the background”) are referred to as
“background” and not annotated as target objects in the ground truth [57].
optical ﬂow) to ﬁnd the primary objects to be segment-ed [51, 50, 19, 28, 27, 12]. A commonly-used architecture is a two-stream CNN, consisting of an appearance branch and a motion branch, which respectively takes the RGB frame and optical ﬂow as parallel inputs [19]. To establish the deep interactions between appearance and motion, vari-ous network variants are adapted from the two-stream CNN, fusing the motion and appearance signals via sophisticated cross-modality learning module [70, 51, 50].
Despite promising performance, the existing methods are not able to well handle the distracting signals which may signiﬁcantly deteriorate the model performance. In U-VOS, the distracting signals may originate from the RGB frame and/or from the optical ﬂow. To illustrate the former, the top two rows in Figure 1 shows a video frame with the motorbike as the only primary target foreground object to be segmented on this dataset. As seen, the car appearing as a static background object is a distracting object which would bring ambiguity to the VOS model. On the other hand, as illustrated in the bottom two rows in Figure 1, the distract-ing signals in the optical ﬂow are typically caused by the inaccurate ﬂow estimation generated by models trained on synthetic videos [9]. When applying such models in real videos, the domain gap can cause the ﬂow ﬁelds to con-tain signiﬁcant noises, especially when the foreground ob-ject is nearly static. Such noises can be further ampliﬁed when there are unexpected camera and/or background ob-ject movements in the video. Therefore, it is not reliable to blindly fuse the appearance and motion features, and there is a demand to establish the correspondence between the
ﬂow vectors and the object instances while suppressing the distracting signals in the input.
A natural solution to align the motion and appearance is to compare their local features. The challenges lie in that we have no supervision on local motion-appearance cor-respondence for training and not all local features in one modality can ﬁnd their counterparts in the other. We for-mulate the motion-appearance alignment as an instance of
Optimal Structure Matching (OSM), and aim to discover the discriminative cross-modality patterns while minimiz-ing the distracting noises in the input via structure learning.
Given a video frame, we extract the dense local features from the optical ﬂow and the RGB image, and obtain two complex structured representations, each consisting of a set of local building features. The Wasserstein distance [37] is then employed to compute the structural similarity be-tween the two structured representations. The Wasserstein distance has the form of the optimal transport problem [37] which can ﬁnd the global least-expensive ﬂows to transport the local features in one modality to the local features in the other, leading to the minimum structural distance. The mag-nitude of each ﬂow characterizes the degree of alignmen-t between two local features and can be used to establish the motion-appearance correspondence. Since the match-ing process is reconstructing one structure against the other, the distracting noises not compatible with the holistic struc-tural matching would end up with low magnitudes in their matching ﬂows and be naturally ﬁltered out.
To integrate the OSM into a two-stream CNN for end-to-end training, we design a differentiable neural network lay-er based on the Sinkhorn method [8], a solver used to opti-mize the Wasserstein distance. We notice that the Sinkhorn method involves computation/memory intensive matrix op-erations, hindering its applicability in VOS which typically requires high-resolution inputs and the stacking of multiple layers to ensure the performance. To this end, we propose a Factorized Sinkhorn method which factorizes a large in-put cost matrix into a number of small spatial blocks and performs structural matching within the long-distant and short-distant local blocks. The long-distant matching can well preserve the global structure information of the origi-nal complex representations while the short-distant match-ing can focus on the ﬁne-grained details. By doing so, we not only speed up the optimization by 27.5× but also im-prove the performance of UVOS by 3.7% in terms of mean
J on FBMS dataset [33]. The two matching operations are implemented as two differentiable network layers termed as
Long-distant SinkHorn (LSH) and Short-distant SinkHorn (SSH), which can be applied sequentially as a building block in a two-stream CNN for motion-appearance align-ment. We plug our Long-Short SinkHorh (LSSH) block into a network architecture designed for UVOS and dub our net-work TransportNet to emphasize its origin from the optimal transport problem. We conduct extensive experiments on three popular benchmark datasets, demonstrating that our
TransportNet yields the state-of-the-art performance.
Our contributions include: (1) a novel model exploit-ing the motion-appearance alignment for noise-tolerate U-VOS, (2) a unique OSM mechanism establishing the struc-tural correspondence between motion and appearance sig-nals while suppressing the noises, (3) a novel LSSH block to enable the structural matching in end-to-end training. 2.