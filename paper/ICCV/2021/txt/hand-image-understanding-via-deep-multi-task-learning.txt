Abstract
Analyzing and understanding hand information from multimedia materials like images or videos is important for many real world applications and remains active in re-search community. There are various works focusing on re-covering hand information from single image, however, they usually solve a single task, for example, hand mask segmen-tation, 2D/3D hand pose estimation, or hand mesh recon-struction and perform not well in challenging scenarios. To further improve the performance of these tasks, we propose a novel Hand Image Understanding (HIU) framework to ex-tract comprehensive information of the hand object from a single RGB image, by jointly considering the relationships between these tasks. To achieve this goal, a cascaded multi-task learning (MTL) backbone is designed to estimate the 2D heat maps, to learn the segmentation mask, and to gen-erate the intermediate 3D information encoding, followed by a coarse-to-fine learning paradigm and a self-supervised learning strategy. Qualitative experiments demonstrate that our approach can recover reasonable mesh representations even in challenging situations. Quantitatively, our method significantly outperforms the state-of-the-art approaches on various widely-used datasets, in terms of diverse evaluation metrics https://github.com/MandyMo/HIU-DMTL. 1.

Introduction
Hand image understanding (HIU) keeps active in both computer vision and graphics communities, aiming to re-cover the spatial configurations from RGB/depth images, including 2D/3D hand pose estimation, hand mask segmen-tation and hand mesh reconstruction, which have been em-ployed in various domains [23, 25, 30, 33, 49, 52]. Recov-ering the spatial configurations is still challenging, due to the inherent depth and scale ambiguity, diverse appearance variation, and complicated articulations. While a bunch of existing works have considered markerless HIU, most of whom require depth camera [37, 62, 51, 41, 26, 15,
∗corresponding author, zhangxiong@yy.com
Figure 1. Hand Image Understanding. The figure illustrates the fundamental idea of this work. We derive the 2D hand pose, hand mask, hand mesh (including 3D hand pose) representation simul-taneous from a monocular RGB image of the hand object in a coarse-to-fine manner. 67, 28, 32, 14, 22, 21] or synchronized multi-view im-ages [3, 18, 46, 51, 59] to deal with the aforementioned challenges. As a consequence, most of those methods are impractical in real-world situations where only monocular
RGB images are available.
For the monocular RGB scenario, the main obstacles re-side in three-folds. Firstly, the lack of high-quality large-scale datasets with precise annotations. Existing datasets are either synthesized using software [73, 19, 35], or labeled in a semi-automated manner [74, 27], or collected in a con-trolled experimental environment [66, 68, 50]. Secondly, the incapability of the current datasets makes the trained models not generalize well to various wild images, espe-cially under self occlusions and complex configurations, which may hinder its applications. Thirdly, contemporary approaches fail to exploit unlabeled images, which is more widely distributed than those with annotations.
The above obstacles motivate us to bring out two ques-tions: Can the existing multi-modality data be harnessed comprehensively to tackle the aforementioned difficulties?
Can the tremendous wild images without labels be exploited well enough to favor the HIU?
In this work, we demonstrate the answers to be yes, and the fundamental idea is illustrated in Figure 1. Specifically, an innovative multi-task learning (MTL) framework is de-signed to tackle the HIU problem, which follows the cas-cade coarse-to-fine design manner. Concretely, the frame-1
work consists of a backbone and several regressor heads corresponding to different tasks. The backbone aims to learn the various elementary representations from hand im-ages, including 2D pose estimation, hand mask segmenta-tion, and 3D part orientation field (POF) encoding. To re-construct the whole hand mesh efficiently, we exploit the generative hand model MANO [44], and employ the regres-sor heads to regress MANO’s parameters based on the se-mantic features of the elementary tasks. To efficiently fuse beneficial semantic-features among various tasks, we con-ceive the task attention module (TAM) to aggregate the se-mantic features across individual tasks and derive compact high-level representations by removing redundant features.
Note that the 3D hand joints can be achieved as a side out-put in MANO. With these designs together, one can obtain the 2D/3D hand pose, hand mask, and hand mesh from a
RGB image simultaneously.
It is clear that the whole framework can be trained with generic supervised learning by leveraging existing multi-modality datasets. The self-supervised learning strategies can be adopted by leveraging the implicit relationship con-straints maintained among the reasonable predictions from each task. For instance, the mask rendered from the hand mesh with proper camera parameters, shall match the one that is estimated by the backbone; the coordinates of the re-projected 2D hand pose shall be close to the integral of locations encoded in the heat-maps. The self-supervised learning make it possible to exploit enormous wild images, which can improve the accuracy of the framework, and en-hance the generalization capability. Additionally, consid-ering the absence of a large-scale hand dataset with well-labeled hand mask and 2D pose, we collect a high-quality dataset with manually labeled 2D hand pose and hand mask.
To summarize, our main contributions are as bellows:
• We design an innovative cascade multi task learning (MTL) framework, dubbed HIU-DMTL, for hand image understanding, which can exploit existing multi-modality hand datasets effectively.
• The self-supervised learning (SSL) was firstly introduced to alleviate the insufficient data problem for HIU, and the effectiveness of which has been verified through exten-sive experiments comprehensively.
• We propose a simple while effective task attention mod-ule (TAM), targeting to aggregate semantic features across various tasks, which proves to be instrumental for
MTL on the HIU tasks.
• Our HIU-DMTL framework outperforms contemporary hand mesh recovery approaches [19, 16, 4, 70, 72, 34], and demonstrates new state-of-the-art performances on widely used benchmarks [68, 73, 50, 74, 46], in terms of various evaluation metrics. 2.