Abstract
Efﬁcient action recognition has become crucial to extend the success of action recognition to many real-world appli-cations. Contrary to most existing methods, which mainly focus on selecting salient frames to reduce the computa-tion cost, we focus more on making the most of the se-lected frames. To this end, we employ two networks of different capabilities that operate in tandem to efﬁciently recognize actions. Given a video, the lighter network pro-cesses more frames while the heavier one only processes a few. In order to enable the effective interaction between the two, we propose dynamic knowledge propagation based on a cross-attention mechanism. This is the main compo-nent of our framework that is essentially a student-teacher architecture, but as the teacher model continues to inter-act with the student model during inference, we call it a dynamic student-teacher framework. Through extensive ex-periments, we demonstrate the effectiveness of each compo-nent of our framework. Our method outperforms competing state-of-the-art methods on two video datasets: ActivityNet-v1.3 and Mini-Kinetics.
Figure 1: mAP vs. GFLOPs curves on ActivityNet-v1.3 [5]:
The proposed dynamic student-teacher framework performs similar to or better than the recent state-of-the-art meth-ods [11, 22, 26, 44, 45] at a much lower computational cost.
More experimental results are available in Section 4. 1.

Introduction
Video action recognition is one of the fundamental prob-lems for video understanding. Consequently, several meth-ods have been proposed and signiﬁcant progress has been made over the last decade thanks to advances in deep learn-ing. Recent state-of-the-art methods mostly use 3D-CNN that takes as input a video clip of several frames [9, 13].
Many of these methods densely sample clips from each video and aggregate the activations to achieve excellent re-sults. However, they require high computational costs, mak-ing it challenging to apply to practical applications. In this paper, we are interested in the problem of efﬁcient video recognition, to achieve better performance and computa-*Qualcomm AI Research is an initiative of Qualcomm Tech-nologies, Inc. tional cost trade-off as shown in Figure 1.
Multiple approaches have been proposed for efﬁcient ac-tion recognition in recent years with focus on two aspects: (a) more efﬁcient CNNs and (b) salient frame/clip selec-tion. On the ﬁrst aspect, methods have been proposed to design efﬁcient versions of 3D-CNN [21, 29, 35, 36, 46], such as the temporally separable convolution that reduces the computational cost per clip. But the more successful way has been to simply switch to the lighter 2D-CNNs in-stead [8, 24], often in conjunction with RNN/LSTM mod-els [3, 11, 44]. Even with more efﬁcient networks, compu-tation would be high for longer videos if all the frames are processed. So, the second aspect of frame selection based on saliency complements the ﬁrst one and has led to most success too [3, 11, 22, 44]. These methods rely on learning a policy that decides how a particular frame should be pro-cessed/skipped [3, 11] and at what resolution [26, 30]. Such policy functions certainly boost efﬁciency, however these methods rely on the policy not to miss frames that are cru-cial for action recognition.
In this paper, we focus on making the most of the se-lected frames by minimizing the performance loss due to switching to a lighter network while keeping the efﬁciency high. To this end, we propose a knowledge propagation framework with two networks, one light and the other rela-tively heavy, that operate in tandem to efﬁciently recognize actions. The interaction between the two networks is not just limited to the training phase, but it is used for infer-ence also, dynamically adjusting to each test video, hence we call this method Dynamic Knowledge Propagation. We take inspiration from the self-attention technique [38] that describes a mapping between a query and key-value pairs.
Ours is a cross-attention mechanism instead, where the query comes from the light network and key-value pairs come from the heavy network. These two networks are used as student and teacher models in our framework, which we call Dynamic Student-Teacher architecture, as the student and teacher interact dynamically for each test video. The proposed framework lets the student process most of the sampled frames, while the teacher processes only a few of them. Then, with the dynamic knowledge propagation the student features are enhanced by the better quality features from the teacher model.
Our main contribution is the dynamic knowledge prop-agation mechanism. This enables interaction between the student and teacher model both during training and infer-ence and is the key component of the proposed framework.
Our second contribution is the dynamic student-teacher framework for efﬁcient video action recognition. The model combines light and heavy networks to reduce compu-tational costs without signiﬁcant performance degradation using dynamic knowledge propagation. Finally, through extensive experiments we demonstrate the effectiveness of each component of the proposed approach. We evaluate our method on two popular benchmarks, ActivityNet-v1.3 [5] and Mini-Kinetics [4], and improve state-of-the-art on both of them with 1.4× and 2.5× less GFLOPs, respectively. 2.