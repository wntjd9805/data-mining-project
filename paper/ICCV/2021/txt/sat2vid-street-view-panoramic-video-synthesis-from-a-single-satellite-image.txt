Abstract
We present a novel method for synthesizing both tem-porally and geometrically consistent street-view panoramic video from a single satellite image and camera trajectory.
Existing cross-view synthesis approaches focus on images, while video synthesis in such a case has not yet received enough attention. For geometrical and temporal consis-tency, our approach explicitly creates a 3D point cloud rep-resentation of the scene and maintains dense 3D-2D corre-spondences across frames that reflect the geometric scene configuration inferred from the satellite view. As for syn-thesis in the 3D space, we implement a cascaded network architecture with two hourglass modules to generate point-wise coarse and fine features from semantics and per-class latent vectors, followed by projection to frames and an up-sampling module to obtain the final realistic video. By leveraging computed correspondences, the produced street-view video frames adhere to the 3D geometric scene struc-ture and maintain temporal consistency. Qualitative and quantitative experiments demonstrate superior results com-pared to other state-of-the-art synthesis approaches that ei-ther lack temporal consistency or realistic appearance. To the best of our knowledge, our work is the first one to syn-thesize cross-view images to videos.. 1.

Introduction
Street-view images have been proven to be helpful for exploring remote places or for strategic ground planning in emergency or intelligence operations. They are useful for a variety of applications in virtual or mixed reality, realistic simulations and gaming, viewpoint interpolation, or cross-view matching. Nevertheless, their acquisition is rather ex-pensive, and regular updates to capture changes are required for some tasks. On the other hand, satellite images are reg-ularly captured, easier to obtain, have significantly better earth coverage, and are generally much more widely avail-able than street-view images. The generation of street views
*Corresponding author.
Please use Adobe Reader / KDE Okular to view animations, which are also available with higher resolution in our supplementary materials and arXiv (https://arxiv.org/abs/2012.06628).
Input Satellite RGB
Synthesized Video
Figure 1: Street-view panoramic video synthesis results of our method (animations). For a single satellite image and a given trajectory (indicated by â†‘ in the figure), we learn to synthesize a corresponding street-view panoramic video with both geometrical and temporal consistency. from given satellite or aerial images is thus an attractive and interesting alternative for the aforementioned applications.
While single street-view image generation from satellite images has recently been investigated [27, 20], these meth-ods are not suitable to create continuous view-point changes around a given location since they built upon random gen-erators and lack constraints on the correspondence between frame pixels. They are thus unable to synthesize temporally and geometrically consistent image sequences that are de-sired for a better visual experience.
In this paper, we approach the novel task to synthesize street-view panoramic video sequences as realistically as possible and as consistent as possible from a single satel-lite image and given viewing locations. To achieve this, in-stead of resorting to 2D generators like [27, 20] and generat-ing images individually, we propose to generate the whole scene in a 3D representation of point cloud, and establish the correspondence between these visible points and the 2D frame pixels. In this way, the projected views from the en-tire generated scene instance will be naturally consistent by design. In order to generate image frames as good as a sin-gle image, we design a two-stage 3D generator in a coarse-to-fine manner that exploits the characteristics of different
3D convolutional neural networks. Fig. 1 presents two ex-amples of our synthesized results, which well demonstrate the temporal consistency of our generated video.
Our major contributions can be summarized as follows. (1) We present the first work for satellite-to-ground video synthesis from a single satellite image with a trajectory. (2)
We propose a novel cross-view video synthesis method that ensures both spatial and temporal consistency by explicitly modeling a cross-frame correspondence using a 3D point cloud representation and building projective geometry con-straints into our network architecture. (3) Our method out-performs multiple baseline methods both qualitatively and quantitatively on a newly-constructed dataset for cross-view video synthesis that is expanded from the London panorama dataset [20]. The source code and pre-trained models will be made publicly available upon publication. 2.