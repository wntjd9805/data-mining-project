Abstract
NeRF synthesizes novel views of a scene with unprece-dented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP.
In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strat-egy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality. 1.

Introduction
Novel View Synthesis (NVS) addresses the problem of rendering a scene from unobserved viewpoints, given a number of RGB images and camera poses as input, e.g., for interactive exploration. Recently, NeRF [29] demonstrated state-of-the-art results on this problem using a neural radi-ance field representation for representing 3D scenes. NeRF produces geometrically consistent, high-quality novel views even when faced with challenges like thin structures, semi-transparent objects and reflections. Additionally, NeRF’s underlying representation requires only very little storage and thus can be easily streamed to users.
The biggest remaining drawbacks of NeRF are its long training and rendering times. While training can be sped up with a multi-GPU cluster, rendering must happen in real-time on the consumer’s device for interactive applications like virtual reality. This motivates us to focus on increasing
NeRF’s rendering speed in this paper.
NeRF represents the scene’s geometry and appearance with a Multi-Layer Perceptron (MLP). During volumetric rendering, this network is sampled hundreds of times for
Figure 1: KiloNeRF. Instead of representing the entire scene by a single, high-capacity MLP, we represent the scene by thousands of small MLPs. This allows us to render the scene above 2548x faster without loss in visual quality. millions of pixels. As the MLP used in NeRF is relatively deep and wide, this process is very slow. A natural idea is to decrease the depth and number of hidden units per layer in order to speed up the rendering process. However, without any further measures, a reduction in network size leads to an immediate loss in image quality due to the limited capacity for fitting complex scenes. We counteract this by using a large number of independent and small networks, and by letting each network represent only a fraction of the scene.
We find that training our KiloNeRF with thousands of networks, na¨ıvely from scratch leads to noticeable artifacts.
To overcome this problem, we first train a regular NeRF as teacher model. KiloNeRF is then trained such that its out-puts (density and color) match those of the teacher model for any position and view direction. Finally, KiloNeRF is fine-tuned on the original training images. Thanks to this three-stage training strategy, our model reaches the same
visual fidelity as the original NeRF model, while being able to synthesize novel views three orders of magnitude faster as illustrated in Fig. 1. Crucial for achieving our speedup is an adequate implementation of the concurrent evaluation of many MLPs. Towards this goal, we published our efficient implementation using PyTorch, MAGMA, Thrust and cus-tom CUDA kernels at https://github.com/creiser/kilonerf. 2.