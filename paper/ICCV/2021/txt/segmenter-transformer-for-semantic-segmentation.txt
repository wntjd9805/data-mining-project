Abstract
Image segmentation is often ambiguous at the level of individual image patches and requires contextual informa-tion to reach label consensus. In this paper we introduce
Segmenter, a transformer model for semantic segmentation.
In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision
Transformer (ViT) and extend it to semantic segmentation.
To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embed-dings with a point-wise linear decoder or a mask trans-former decoder. We leverage models pre-trained for im-age classification and show that we can fine-tune them on moderate sized datasets available for semantic segmenta-tion. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes. 1.

Introduction
Semantic segmentation is a challenging computer vi-sion problem with a wide range of applications includ-ing autonomous driving, robotics, augmented reality, im-age editing, medical imaging and many others [27, 28, 45].
The goal of semantic segmentation is to assign each im-age pixel to a category label corresponding to the under-lying object and to provide high-level image representa-tions for target tasks, e.g. detecting the boundaries of peo-ple and their clothes for virtual try-on applications [29].
*Equal contribution.
†Inria, ´Ecole normale sup´erieure, CNRS, PSL Research University, 75005 Paris, France.
Code: https://github.com/rstrudel/segmenter
Figure 1: Our approach for semantic segmentation is purely transformer based. It leverages the global image context at every layer of the model. Attention maps from the first Seg-menter layer are displayed for three 8 × 8 patches and high-light the early grouping of patches into semantically mean-ingful categories. The original image (top-left) is overlayed with segmentation masks produced by our method.
Despite much effort and large progress over recent years
[10, 22, 31, 37, 48, 65, 66], image segmentation remains a challenging problem due to rich intra-class variation, con-text variation and ambiguities originating from occlusions and low image resolution.
Recent approaches to semantic segmentation typically rely on convolutional encoder-decoder architectures where the encoder generates low-resolution image features and the decoder upsamples features to segmentation maps with per-pixel class scores. State-of-the-art methods deploy Fully
Convolutional Networks (FCN) [44] and achieve impres-sive results on challenging segmentation benchmarks [10, 23, 57, 58, 60, 64, 66]. These methods rely on learnable stacked convolutions that can capture semantically rich in-formation and have been highly successful in computer vi-sion. The local nature of convolutional filters, however,
limits the access to the global information in the image.
Meanwhile, such information is particularly important for segmentation where the labeling of local patches often de-pends on the global image context. To circumvent this issue,
DeepLab methods [8, 9, 10] introduce feature aggregation with dilated convolutions and spatial pyramid pooling. This allows to enlarge the receptive fields of convolutional net-works and to obtain multi-scale features. Following recent progresses in NLP [50], several segmentation methods ex-plore alternative aggregation schemes based on channel or spatial [22, 23, 61] attention and point-wise [66] attention to better capture contextual information. Such methods, how-ever, still rely on convolutional backbones and are, hence, biased towards local interactions. An extensive use of spe-cialised layers to remedy this bias [8, 10, 22, 58] suggests limitations of convolutional architectures for segmentation.
To overcome these limitations, we formulate the prob-lem of semantic segmentation as a sequence-to-sequence problem and use a transformer architecture [50] to leverage contextual information at every stage of the model. By de-sign, transformers can capture global interactions between elements of a scene and have no built-in inductive prior, see Figure 1. However, the modeling of global interac-tions comes at a quadratic cost which makes such meth-ods prohibitively expensive when applied to raw image pix-els [11]. Following the recent work on Vision Transform-ers (ViT) [19, 49], we split the image into patches and treat linear patch embeddings as input tokens for the trans-former encoder. The contextualized sequence of tokens pro-duced by the encoder is then upsampled by a transformer decoder to per-pixel class scores. For decoding, we con-sider either a simple point-wise linear mapping of patch embeddings to class scores or a transformer-based decod-ing scheme where learnable class embeddings are processed jointly with patch tokens to generate class masks. We con-duct an extensive study of transformers for segmentation by ablating model regularization, model size, input patch size and its trade-off between accuracy and performance.
Our Segmenter approach attains excellent results while re-maining simple, flexible and fast. In particular, when us-ing large models with small input patch size the best model reaches a mean IoU of 53.63% on the challenging ADE20K
[68] dataset, surpassing all previous state-of-the-art convo-lutional approaches by a large margin of 5.3%. Such im-provement partly stems from the global context captured by our method at every stage of the model as highlighted in
Figure 1.
In summary, our work provides the following four con-tributions: (i) We propose a novel approach to semantic seg-mentation based on the Vision Transformer (ViT) [19] that does not use convolutions, captures contextual information by design and outperforms FCN based approaches. (ii) We present a family of models with varying levels of resolu-tion which allows to trade-off between precision and run-time, ranging from state-of-the-art performance to models with fast inference and good performances. (iii) We pro-pose a transformer-based decoder generating class masks which outperforms our linear baseline and can be extended to perform more general image segmentation tasks. (iv) We demonstrate that our approach yields state-of-the-art results on both ADE20K [68] and Pascal Context [38] datasets and is competitive on Cityscapes [14]. 2.