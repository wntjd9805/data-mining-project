Abstract
Few-shot class incremental learning (FSCIL) aims to in-crementally add sets of novel classes to a well-trained base model in multiple training sessions with the restriction that only a few novel instances are available per class. While learning novel classes, FSCIL methods gradually forget base (old) class training and overfit to a few novel class samples. Existing approaches have addressed this problem by computing the class prototypes from the visual or seman-tic word vector domain. In this paper, we propose address-ing this problem using a mixture of subspaces. Subspaces define the cluster structure of the visual domain and help to describe the visual and semantic domain considering the overall distribution of the data. Additionally, we pro-pose to employ a variational autoencoder (VAE) to generate synthesized visual samples for augmenting pseudo-feature while learning novel classes incrementally. The combined effect of the mixture of subspaces and synthesized features reduces the forgetting and overfitting problem of FSCIL. Ex-tensive experiments on three image classification datasets show that our proposed method achieves competitive results compared to state-of-the-art methods. 1.

Introduction
In many practical applications, it is crucial for a model to classify novel objects, i.e., objects for which only a few in-stances are available during training. For example, this can occur when the distribution of the test data deviates from that experienced at training, or if the model faces new ob-jects from a class for which significantly less data was pro-vided during training. While the former, to some extent, can be addressed by various techniques such as domain adapta-*denotes equal contribution.
Figure 1: We cluster the training samples into a number of clus-ters, in this example, three. Then, we generate three corresponding subspaces, which are constructed from these three clusters. These subspaces are then used to project visual features and semantic vectors onto. The distance between the projected visual feature and the projected semantic vector is minimized according to a loss function during training in each subspace. In addition to project-ing real sample features, we also project synthesized features to the mixture of subspaces. The combined effect of the mixture of subspaces and synthesized features helps the network not overfit to few-shot data of novel classes and forget base class knowledge. tion, addressing the latter is usually studied under the Few-Shot Learning (FSL) paradigm [33, 10]. Generally, in a FSL framework, the goal is to classify samples into few-shot classes given only a training set of base categories. How-ever, some variations of the problem classify both base and novel class instances together in a generalized manner. In a more realistic scenario, all novel class instances may not be available at a time. It creates another branch of the problem, few-shot class incremental learning (FSCIL), where novel classes are added to the model incrementally over time, and in each incremental step, the model is tested based on both base and novel class instances. Because of this restriction,
FSCIL is the most complex form of FSL problem.
Initial results on FSCIL have been proposed in the litera-ture [35, 20, 4, 8]. We identify two critical challenges in this problem. (a) Catastrophic forgetting of base classes: Re-cent works observed a fascinating performance using word
vectors in the learning process (in addition to knowledge distillation techniques) to address the forgetting problem
[25, 8, 49]. The general motivation is that shared attributes (e.g., shape, color) between the base (e.g., horse, tiger) and novel class (e.g., zebra) help to understand novel classes better using a few examples and not to forget base classes.
Considering the subtle variations of such relationships be-tween closely and distantly related classes, [8] proposed to apply clustering on semantic vectors of the embedding space. However, being trained on noisy unsupervised texts, word vectors always estimate only a crude stereotype of any class name, not truly reflecting dataset specific visual inter-play between base and novel objects. In this paper, we ar-gue that the relation of visual and semantic vectors must be computed on an embedding space which has the knowledge of the entire dataset. (b) Overfitting to novel classes: To ad-dress the problem, traditional methods use class prototypes
[11, 27] and some memory [8] of base class instances. How-ever, because of fewer data available to train novel classes incrementally, it is not easy to escape from this problem.
Moreover, if any intermediate incremental step faces this problem, the impact propagates in future incremental trials.
In this paper, we argue that including synthesized features of novel classes can reduce this problem.
We endeavor to design an FSCIL approach that improves the classification performance while not suffering from the drawbacks of the methods mentioned in the above para-graph. Here, we also use semantic word vectors in the net-work pipeline. We apply clustering in image feature space instead of word embedding space while relating similar and dissimilar base classes of a novel class. Based on each clus-ter, we create a set of subspaces. The subspaces are con-structed in such a way as to best represent individual clus-ters of features formed by visually similar samples. Singu-lar Value Decomposition (SVD) is employed for this pur-pose, and by selecting a set of basis vectors with the greatest eigenvalues, we ensure that the signal in each visual feature cluster is well represented. Less prominent portions of the feature clusters are more likely noise than signal and will, hence, not be well represented in the subspace. Empirically, we observed that capturing information about how the data projects onto such subspaces leads to less forgetting of base classes and better alignment of features and semantics of classes. Next, at each incremental step, we utilize a varia-tional auto encoder (VAE) for producing high-quality syn-thesized features representing rich prior knowledge about novel classes. The generative model is trained using only available class instances, capable of generating and aug-menting novel class features using a few examples during each incremental session. Note that instead of the tradi-tional use of semantic word vectors in such a feature gen-eration process [15, 29, 39, 43, 14], here we use sampled features to generate more features. Considering the mixture of subspaces while relating base and novel classes and aug-menting synthesized features at each incremental session re-duces both catastrophic forgetting and overfitting problems during novel class training. Evaluating on MiniImageNet,
CUB200, and CIFAR100 cloud benchmark datasets, we consistently outperform many current state-of-the-art meth-ods.
In summary, the contributions of this work are: (1) a novel FSCIL framework that elegantly addresses both the catastrophic forgetting problem of base classes by using a mixture of subspaces and the overfitting problem of novel classes by using synthesized features. (2) a subspace com-putation strategy based on clustering in image feature space to relate base and novel classes more accurately for the FS-CIL problem. (3) state-of-the-art performance on MiniIma-geNet, CUB200, and CIFAR100 cloud benchmark datasets. 2.