Abstract
An important scenario for image quality assessment (IQA) is to evaluate image restoration (IR) algorithms. The state-of-the-art approaches adopt a full-reference paradigm that compares restored images with their corresponding pristine-quality images. However, pristine-quality images are usually unavailable in blind image restoration tasks and real-world scenarios.
In this paper, we propose a prac-tical solution named degraded-reference IQA (DR-IQA), which exploits the inputs of IR models, degraded images, as references. Specifically, we extract reference informa-tion from degraded images by distilling knowledge from pristine-quality images. The distillation is achieved through learning a reference space, where various degraded images are encouraged to share the same feature statistics with pristine-quality images. And the reference space is opti-mized to capture deep image priors that are useful for qual-ity assessment. Note that pristine-quality images are only used during training. Our work provides a powerful and dif-ferentiable metric for blind IRs, especially for GAN-based methods. Extensive experiments show that our results can even be close to the performance of full-reference settings. 1.

Introduction
Digital images may be subject to various quality degradations during processing, compression, transmission, etc. [36]. And image restoration (IR) algorithms are devel-oped to improve the quality of degraded images [18, 35, 38, 41, 43, 46, 47]. A consequential question is how to as-sess the quality of restored images and evaluate IR algo-rithms. The state-of-the-art IQA approaches adopt a full-reference paradigm (FR-IQA), which learns to compare re-stored images with their corresponding pristine-quality im-*This work was performed when Heliang Zheng was visiting Microsoft
Research as a research intern. Code and models are available at https:
//github.com/researchmm/CKDN.
Figure 1. (a) Pristine-quality images can provide strong refer-ence information for IQA. When pristine-quality images are un-available, (b) directly regressing restored images to quality scores causes a dramatic drop in performance. To this end, (c) we propose to extract reference information from degraded images and make such a solution effective. FR, NR, DR, S, and SRCC indicate full-reference, no-reference, degraded-reference, quality score, and
Spearman’s Rank order Correlation Coefficients [30], respectively. ages (as shown in Figure 1 (a)). FR-IQA algorithms have been widely adopted as IR evaluation metrics, e.g., PSNR,
SSIM [36], and LPIPS [45]. However, they cannot be ap-plied to blind image restoration tasks and real-world appli-cations, where pristine-quality images are unavailable. In this paper, we study the problem of evaluating IR models without relying on pristine-quality images.
To evaluate IR models without pristine-quality images, no-reference IQA (NR-IQA) methods provide a solution, that is, directly regressing restored images to quality scores, i.e., mean opinion scores (MOS) [2, 13, 20, 32]. How-ever, the absence of reference information makes the prob-lem much more challenging and causes a dramatic drop in performance, e.g., 0.1409 SRCC (Spearman’s Rank order
Correlation Coefficients [30]) drops. This motivates us to seek available reference information. We find that the in-puts of IR models (i.e., degraded images) are usually free to be obtained, and it has been verified that degraded im-ages also contain useful image priors like pristine-quality images for solving under-constrained problems in computer vision [25, 33]. To this end, we propose a new solution named degraded-reference IQA (DR-IQA). However, di-rectly replacing the pristine-quality references of existing
FR-IQA models [5] by degraded references causes 0.1239
SRCC drops, because the noises involved by various degra-dations make it difficult to mine and leverage reference in-formation from degraded images.
To leverage degraded images and improve the effective-ness of DR-IQA, we propose a Conditional Knowledge
Distillation Network (CKDN). As shown in Figure 2,
CKDN consists of three modules, i.e., a degradation-tolerant embedding module (DTE), a quality-sensitive em-bedding module (QSE), and a convolutional score predictor (CSP). The DTE is the key module, which aims to effec-tively extract reference information for IQA from degraded images. Specifically, it learns a reference space, where var-ious degraded images are optimized to share the same fea-ture statistics with pristine-quality images. Such space is learned in the condition of conducting quality assessments with the QSE and CSP, where the QSE learns discriminative features from restored images, and the CSP maps feature differences to human-annotated scores. Moreover, we find that pre-training such conditions can help to learn quality-sensitive features and further advance the optimization of the reference space.
In particular, we propose a relative score regression task that can enlarge the space of training data by creating data pairs.
We conduct extensive experiments to evaluate our pro-posed solution.
It is shown that our proposed CKDN enables DR-IQA to achieve comparable performance to the full-reference setting on various IR tasks, e.g., traditional/GAN-based super-resolution, de-noising, etc.
Moreover, we further study the influence of reference qual-ity and the performance of current IQA methods on evalu-ating different IR tasks. We draw three conclusive insights: 1) our CKDN works well with a large range of degrada-tion types, 2) reference images are extremely important for evaluating GAN-based images, and 3) current IQA methods can provide around 85% reliable judgments for evaluating
IR algorithms. Please refer to Section 5 for more details.
Our main contributions can be summarized as follows: 1)
To the best of our knowledge, we are the first to leverage degraded images for IQA. 2) We formulate this practical setting and propose an effective conditional knowledge dis-tillation network to solve this problem. 3) Extensive analy-ses show that degraded references are of great help for eval-uating GAN-based images. Overall, we believe our work would contribute to the IQA and the blind SR communities, and providing insights for evaluating GAN-based models. 2.