Abstract
The simultaneous recognition of multiple objects in one image remains a challenging task, spanning multiple events in the recognition field such as various object scales, incon-sistent appearances, and confused inter-class relationships.
Recent research efforts mainly resort to the statistic label co-occurrences and linguistic word embedding to enhance the unclear semantics. Different from these researches, in this paper, we propose a novel Transformer-based Dual
Relation learning framework, constructing complementary relationships by exploring two aspects of correlation, i.e., structural relation graph and semantic relation graph. The structural relation graph aims to capture long-range cor-relations from object context, by developing a cross-scale transformer-based architecture. The semantic graph dy-namically models the semantic meanings of image objects with explicit semantic-aware constraints. In addition, we also incorporate the learnt structural relationship into the semantic graph, constructing a joint relation graph for ro-bust representations. With the collaborative learning of these two effective relation graphs, our approach achieves new state-of-the-art on two popular multi-label recognition benchmarks, i.e. MS-COCO and VOC 2007 dataset. 1.

Introduction
Multi-label image recognition aims at assigning multi-ple labels for multiple objects presented in one natural im-age. As a fundamental task in computer vision, multi-label image recognition can serve as prerequisites for many ap-plications, such as weakly supervised localization and seg-mentation [12, 15, 44], attribute recognition [25, 22] , scene understanding [29] and recommendation systems [39, 19].
Benefiting from the development of deep learning tech-niques [17, 30], recent CNN-based architectures have made significant process in distinguishing multiple objects. But
*Works done while interning at Tencent Youtu Lab.
â€ Jia Li is the Corresponding author. URL: http://cvteam.net
Figure 1. Motivation of the proposed dual relations. a) Struc-tural relation provides long-term contextual relations for recog-nizing snowboard, while b) semantic relation builds dynamic cor-relations of co-occurred classes. These two relations jointly form a structural and semantic-aware image understanding. the accurate parsing of multi-label images still faces great challenges, including various object scales, inconsistent vi-sual appearances, and confused inter-class relationships.
One intuitive solution for discovering visual consistency is to enhance the feature representation with self-attention mechanisms [46, 34, 16]. For example, Wang et al. [34] propose to automatically discover the attentional regions with a recurrent neural network, introducing discriminative features for representation learning. Beyond these improve-ments, Guo et al. [16] propose the assumption for visual perception consistency of attention regions and then am-plify these regions by a visual consistency loss. Although the spatial representations of CNNs are strengthened by these techniques, the multi-label dependencies are not ex-plicitly modeled, which is crucial for the understanding of multi-label relationships.
To tackle this problem, recent ideas propose to learn the inter-class relationships based on the co-occurrences of multiple classes (e.g., the snowboard should be attached with higher confidence if person occurs). Pioneer works tend to model this relationship with Recurrent Neural Net-works (RNN) [32, 2, 40], while the co-occurred labels can be gradually refined in the sequential predictions. Inspired by the advanced improvements of Graph Convolutional
[20], tens of works [8, 5, 33, 41, 6, 3]
Network (GCN) propose to construct label-wise relationships based on the semantic meanings or statistical co-occurrences. For exam-ple, Chen et al. [8] propose to construct the graph model with the semantic word embeddings, forming a static label-wise relationship. However, this static relationship neglects the characteristics of each image, leading to negative opti-mization for objects with less-frequent co-occurrences. To solve this problem, some works [5, 41, 45] propose to con-struct dynamic graph based on the image-specific descrip-tors of high-level semantic features. Nevertheless, this mod-eling of multi-label relationship still shows its limitations: 1) the spatial interactions of contextual objects are not im-plicitly modeled in the label-wise relation, 2) the features of high-level semantics are somewhat unstable and do not re-flect specific semantic classes, 3) the representation of long-range context and various object scales are not considered.
To efficiently solve these deficiencies as well as the ma-jor challenges in multi-label image recognition, we pro-pose to model a joint structural and semantic relationship of multi-label objects in one image. As in Fig. 1, considering the co-occurrence of semantic labels in vanilla class-wise relation models [8], absent classes would also be halluci-nated (i.e., skis and skateboard). Beyond these demerits, the semantic meaning of one object should be not only de-cided by its intrinsic attributes but also the contextual in-formation. In Fig. 1 a), the appearance of snowboard is visually similar with skis and skateboard, and also shows high co-occurrence frequencies in Fig. 1 b). But human being can easily recognize it as a snowboard based on the long-range contextual information (snow) and even person appearances. Based on these investigations, we propose a collaborative framework with joint structural and semantic relational graphs in Fig. 1 c), which depict the position-wise and class-wise relationship respectively.
To construct the structural graph, we make the first at-tempt to introduce the Transformer architecture [31] into multi-label recognition. This new attempt greatly broadens the receptive capability of conventional CNNs and draws position-wise long-term dependencies for object contex-tual correlations (Fig. 1 a)). Starting from this novel de-sign, we further propose a cross-scale attention module to enhance the perceptive ability of various object scales.
For the construction of semantic graph, we aim at con-structing dynamic relation which is aware of object emer-gence and structural embedding. Different from previous works [5, 41] with implicit high-level embedding, the graph nodes in our paper are explicitly constructed with semantic-aware constraints, reflecting features of specific classes.
Beyond this explicit class-wise embedding, we incorpo-rate the learnt structural graph embedding into the seman-tic relation construction from two aspects: adjacent corre-lation construction and feature-wise complementary. These two mechanisms efficiently endow the semantic graph with the perception of structural information, generating robust graph relations. With the collaborative learning of pro-posed structural and semantic relation, our proposed ap-proach achieves state-of-the-art results on two most popular benchmarks, i.e., MS-COCO [23] and PASCAL VOC [13].
In summary, our contribution is three-fold: 1) We pro-pose a novel Transformer-based Dual Relation learning framework for multi-label image recognition tasks, which jointly models the structural and semantic information with
Transformer architectures. 2) A transformer-based struc-tural relation graph is constructed to incorporate long-term contextual information, building position-wise spatial re-3) A semantic rela-lationships across different scales. tion graph is constructed with explicit class-specific con-straints and structure-aware embedding, modeling the dy-namic class-wise dependencies. 2.