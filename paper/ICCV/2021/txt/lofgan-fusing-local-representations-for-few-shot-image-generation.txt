Abstract
Given only a few available images for a novel unseen category, few-shot image generation aims to generate more data for this category. Previous works attempt to glob-ally fuse these images by using adjustable weighted coef-ﬁcients. However, there is a serious semantic misalignment between different images from a global perspective, mak-ing these works suffer from poor generation quality and di-versity. To tackle this problem, we propose a novel Local-Fusion Generative Adversarial Network (LoFGAN) for few-shot image generation.
Instead of using these available images as a whole, we ﬁrst randomly divide them into a base image and several reference images. Next, LoFGAN matches local representations between the base and refer-ence images based on semantic similarities, and replaces the local features with the closest related local features. In this way, LoFGAN can produce more realistic and diverse images at a more ﬁne-grained level, and simultaneously enjoy the characteristic of semantic alignment. Further-more, a local reconstruction loss is also proposed, which can provide better training stability and generation quality.
We conduct extensive experiments on three datasets, which successfully demonstrates the effectiveness of our proposed method for few-shot image generation and downstream vi-sual applications with limited data. Code is available at https://github.com/edward3862/LoFGAN-pytorch. 1.

Introduction
As a representative deep generative model, generative adversarial networks (GANs) [7] have shown impressive results in various visual tasks in recent years. However, most GAN models still struggle with insufﬁcient training data [25]. Although many GAN-based few-shot learning algorithms have been presented recently, most of these al-gorithms are specially designed for discriminative tasks like
†Equal contribution, ∗Corresponding author. image classiﬁcation [20] and segmentation [21], rather than the pure image generation in a data-limited regime.
To this end, few-shot image generation has aroused in-creasing attention. The goal of few-shot image generation is to generate diverse images for a novel category, when a few available images of this category are given. In partic-ular, inspired by the episodic training mechanism [20], the generative model is generally trained on an auxiliary dataset with sufﬁcient labeled training categories and images. Af-ter that, given a few images from a new unseen category, the learned generative model is expected to generate diverse images for this speciﬁc category. Considering the disjoint label spaces between the seen auxiliary dataset and the un-seen test dataset, the generative model is hoped to obtain the generalization ability by learning from thousands of simu-lated few-shot image generation tasks.
Current few-shot generation approaches can be roughly transformation-based [2], i.e., divided into three types, optimization-based [5, 13], and fusion-based [8, 9].
Transformation-based methods apply intra-category trans-formation on one conditional image while optimization-based methods introduces a meta-learning paradigm [6, 16] to learn an initialization strategy for unconditional image generation tasks, both applicable to simple generation tasks.
Fusion-based methods (inspired by metric-based few-shot learning) deﬁne this problem as a conditional generation task. The generative model encodes several input images to a feature space and performs a fusion operation (instead of the comparison operation in metric-based few-shot classiﬁ-cation). The fused feature is then decoded back to a realistic image of the same category.
The essence of fusion-based few-shot generation is to implement a label-consistent mapping from a few con-ditional inputs to diverse outputs while simultaneously maintaining the image quality and diversity. Technically,
GMN [3] combines Matching Network [20] with VAE [11] by appending a decoder after the matching procedure. Due to the limited generation capacity of VAE, this method is only applicable to generate digits and simple visual pat-terns. To address this problem, MatchingGAN [8] re-places the VAE part with a generative adversarial network and achieves natural image generation for the ﬁrst time, but still struggles with complex natural images. Recently,
F2GAN [9] proposes a fuse-and-ﬁll strategy in the fusion procedure to enhance the generation ability. However, the above methods still suffer from a limited and imprecise generation space which is formulated by a strictly linear combination as well as a weighted image-level reconstruc-tion loss.
In other words, the images in the same cate-gory are linearly fused with interpolation coefﬁcients at the global feature map level. This may bring two problems.
First, when the input images are not semantically aligned, the fused feature map will be misaligned too, and globally adding them will produce aliasing artifacts in the output im-age. Second, simple global combination will also hurt the generation diversity because the relative position of each lo-cal semantic area is strictly ﬁxed during fusion.
To tackle the above problems, we propose a novel local-fusion approach to fusion-based few-shot image generation.
Given a handful of images, we randomly choose one of them as a base image and the others as reference images.
The base image deﬁnes a basis of the generation and the reference images act like a bank of many available local representations. We ﬁrst select local positions randomly in the base image. Then, since the input images come from the same category, we can ﬁnd semantically matched local representations for these selected positions in the bank. We fuse the matched local representations from different im-ages in a more ﬁne-grained level, and replace them back to the corresponding positions in the base image. The whole process is completed in a local fusion module at the feature level without additional parameters. Since the fusion opera-tion is performed in local areas rather than the whole feature map, the generated images will contain fewer artifacts.
In addition, we propose a new local reconstruction loss to better cooperate with the proposed local fusion module at the training stage. In previous fusion-based few-shot im-age generation methods, a global reconstruction loss is used to enforce the generated images to contain the information of the input images, which is implemented by minimizing the pixel-level distance between the generated image and a weighted sum of input images. However, adding the in-put images at each pixel position as a reconstruction target can not ensure semantically alignment because each image is unique in content with different structures. To this end, we consistently stand in a ‘local’ view to tackle this prob-lem. We enforce the generated images to be close to the input images in some local areas instead. We reproduce the above feature-level local fusion procedure at the image level to build a clearer image as the reconstruction target. We ﬁnd the proposed local reconstruction loss can further improve the generation quality for few-shot image generation.
Our contributions can be summarized as follows:
• We propose Local-Fusion Generative Adversarial Net-work (LoFGAN) for few-shot image generation, which can ﬂexibly match the semantically nearest local fea-tures to achieve better generation quality and diversity.
• We present a local fusion module along with a new local reconstruction loss to better train the network, which provides more reﬁned guidance for generation.
• We conduct comprehensive experiments on three datasets where our method achieves the state-of-the-art performance in few-shot image generation, demon-strating the effectiveness of our proposed method. 2.