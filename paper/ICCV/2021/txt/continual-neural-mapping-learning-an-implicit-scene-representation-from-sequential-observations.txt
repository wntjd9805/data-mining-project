Abstract 1.

Introduction
Recent advances have enabled a single neural network to serve as an implicit scene representation, establishing the mapping function between spatial coordinates and scene properties. In this paper, we make a further step towards continual learning of the implicit scene representation di-rectly from sequential observations, namely Continual Neu-ral Mapping. The proposed problem setting bridges the gap between batch-trained implicit neural representations and commonly used streaming data in robotics and vision com-munities. We introduce an experience replay approach to tackle an exemplary task of continual neural mapping: ap-proximating a continuous signed distance function (SDF) from sequential depth images as a scene geometry repre-sentation. We show for the first time that a single network can represent scene geometry over time continually without catastrophic forgetting, while achieving promising trade-offs between accuracy and efficiency.
Scene representations convert visual sensory data into compact forms. Recent trends [58, 34] show that the map-ping function y = f (x; θ) between the spatial coordinate x and the scene property y can serve as an implicit scene representation parameterized by a single neural network θ.
Such a new paradigm has drawn significant attention: the neural network defined in a continuous and differentiable function space can be trained to recover fine-grained details at scene scale with efficient memory consumption, which offers great benefits over alternatives.
However, batch training of the implicit neural represen-tation is impractical and inefficient when dealing with pos-sibly unending streams of data. To handle the sequential observations and obtain a globally consistent representa-tion over time, conventional approaches turn to a data fu-sion paradigm. A discretized scene representation is pre-defined in memory-inefficient parameter space and updated according to perceived observations at each time. The gap between the emerging neural representation paradigm and
the conventional data fusion paradigm addresses a critical issue: how we can learn an implicit neural representation continually from sequential observations?
In this paper, we introduce a novel problem setting of continual neural mapping. The central idea is to maintain a continually updated neural network at each time to approx-imate the mapping function f (·) within the environment.
Past observations (x1:t, y1:t) are marginalized out and sum-marized into compact neural network parameters θt during training. The neural network not only serves as a mem-ory of sequential data, but also makes predictions of scene properties within the entire environment. The prediction-updating fashion leads to a self-improved mapping function when constantly exploring the environment, which resem-bles human-like learning scenarios from a continual learn-ing perspective.
We instantiate the proposed continual neural mapping problem by tackling the SDF approximation from sequen-tial depth images. We propose an experience replay ap-proach that distills past experience to guide the predic-tion without catastrophic forgetting. Experimental results demonstrate that the proposed method outperforms batch re-training/fine-tuning baselines and obtains comparable re-sults against state-of-the-art approaches. The key contribu-tions of our work are summarized as follows:
- We are the first to address the problem of learning an implicit neural scene representation continually from se-quential data, namely continual neural mapping;
- We deal with the problem of SDF approximation from sequential data under the proposed continual neural map-ping setting, outperforming competitive approaches;
- We propose an experience replay method to learn scene geometry continually without catastrophic forgetting. The memory consumption and training time are orders of mag-nitude less than the batch re-training baseline. 2.