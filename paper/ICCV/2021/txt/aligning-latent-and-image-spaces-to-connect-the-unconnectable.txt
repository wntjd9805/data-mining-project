Abstract
In this work, we develop a method to generate inï¬nite high-resolution images with diverse and complex content.
It is based on a perfectly equivariant patch-wise genera-tor with synchronous interpolations in the image and latent spaces. Latent codes, when sampled, are positioned on the coordinate grid, and each pixel is computed from an inter-polation of the neighboring codes. We modify the AdaIN mechanism to work in such a setup and train a GAN model to generate images positioned between any two latent vec-tors. At test time, this allows for generating inï¬nitely large images of diverse scenes that transition naturally from one into another. Apart from that, we introduce LHQ: a new dataset of 90k high-resolution nature landscapes. We test the approach on LHQ, LSUN Tower and LSUN Bridge and outperform the baselines by at least 4 times in terms of qual-ity and diversity of the produced inï¬nite images. The project website is located at https://universome.github.io/alis. 1.

Introduction
Modern image generators are typically designed to syn-thesize pictures of some ï¬xed size and aspect ratio. The real world, however, continues outside the boundaries of any captured photograph, and so to match this behavior, several recent works develop architectures to produce in-ï¬nitely large images [29, 10, 57], or images that partially extrapolate outside their boundary [25, 43, 57].
Most of the prior work on inï¬nite image generation fo-cused on the synthesis of homogeneous texture-like patterns
[20, 2, 29] and did not explore the inï¬nite generation of complex scenes, like nature or city landscapes. The critical challenge of generating such images compared to texture synthesis is making the produced frames globally consis-tent with one another: when a scene spans across several frames, they should all be conditioned on some shared in-formation. To our knowledge, the existing works explored three ways to achieve this: 1) ï¬t a separate model per scene, so the shared information is encoded in the modelâ€™s weights (e.g., [29, 40, 53, 11]); 2) condition the whole generation process on a global latent vector [57, 25, 43]; and 3) predict spatial latent codes autoregressively [10].
The ï¬rst approach requires having a large-resolution photograph (like satellite images) and produces pictures whose variations in style and semantics are limited to the given imagery. The second solution can only perform some limited extrapolation since using a single global latent code
ğ‘¥
ğ‘¤
ğ‘™
= ((ğ‘‘ âˆ’ ğ›¿ âˆ’ ğ‘) â‹… ğ‘¤
ğ‘
+ (ğ›¿ + ğ‘) â‹… ğ‘¤
)/ğ‘‘
ğ›¿
ğ‘
ğ‘‘
ğ‘™
ğ‘¤
ğ‘
ğ‘¤
ğ‘Ÿ
ğ‘¤
ğ‘™
ğ‘
ğ‘¥
ğ‘¥
ğ‘¥
Figure 2: Illustration of our alignment procedure. We po-sition initial latent codes (anchors) on the 2D coordinate space and compute latent code wx for each position x as a linear interpolation between its two neighboring anchors.
ğ‘¥
ğ‘Ÿ cannot encompass the diversity of an inï¬nite scenery (as also conï¬rmed by our experiments). The third approach is the most recent and principled one, but the autoregres-sive inference is dramatically slow [15]: generating a single 2562 image with [10]â€™s method takes us 10 seconds on a single V100 GPU.
â‡ 
This work, like [10], also seeks to build a model with global consistency and diversity in its generation process.
However, in contrast to [10], we approach the problem from a different angle. Instead of slowly generating local latent codes autoregressively (to make them coherent with one another), we produce several global latent codes indepen-dently and train the generator to connect them.
We build on top of the recently proposed coordinate-based decoders that produce images based on pixelsâ€™ co-ordinate locations [28, 25, 43, 1, 5] and develop the above idea in the following way. Latent codes, when sampled, are positioned on the 2D coordinate space â€” they consti-tute the â€œanchorsâ€ of the generation process. Next, each image patch is computed independently from the rest of the image, and the latent vector used for its generation is pro-duced by linear interpolation between nearby anchors. If the distance d
R between the anchors is sufï¬ciently large, they cover large regions of space with the common global context and make the neighboring frames in these regions be semantically coherent. This idea of aligning latent and image spaces (ALIS) is illustrated in Figure 2. An impor-tant ingredient of our setup is the use of Fourier coordinate embeddings [42, 45], which provide crucial positional in-formation and help to model high-frequency details. We inherit their design from INR-GAN [43]. 2
Our model is GAN-based [14] and the generator is trained to produce plausible images from any position in space. This is in high contrast to existing coordinate-based approaches that generate samples only from [0, 1]2 coordi-nates area, which constitutes a single frame size. To make the model generalize to any position in space, we do not in-put global coordinates information. Instead, we input only its position relative to the neighboring anchors, which, in turn, could be located arbitrarily on the x-axis.
We utilize StyleGAN2 architecture [24] for our method and modify only its generator component. Originally, Style-GAN2 passes latent vectors into the decoder by modulating and demodulating convolutional weights â€” an adjustment of adaptive instance normalization layer (AdaIN) [18]. For our generator, we redesign AdaIN to make it work with the coordinate-based latent vectors and develop Spatially-Aligned AdaIN (SA-AdaIN), described in Sec 3. Our model is trained in a completely unsupervised way from a it never sees dataset of unrelated square image crops, i.e. full panorama images or even different parts of the same panorama during training. By training it to produce real-istic images located between arbitrary anchors describing different content (for example, mountains and a forest), it learns to connect unrelated scenes into a single panorama.
This task can be seen as learning to generate camera transi-tions between two viewpoints located at semantically very different locations.
We test our approach on several LSUN categories and
Landscapes HQ (LHQ): a new dataset consisting of 90k high-resolution nature landscape images that we introduce in this work. We outperform the existing baselines for all the datasets in terms of inï¬nite image quality by at least 4 times and at least 30% in generation speed. 2.