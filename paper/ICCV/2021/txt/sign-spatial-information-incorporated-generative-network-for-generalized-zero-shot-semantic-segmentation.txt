Abstract
Unlike conventional zero-shot classiﬁcation, zero-shot semantic segmentation predicts a class label at the pixel level instead of the image level. When solving zero-shot semantic segmentation problems, the need for pixel-level prediction with surrounding context motivates us to in-corporate spatial information using positional encoding.
We improve standard positional encoding by introducing the concept of Relative Positional Encoding, which inte-grates spatial information at the feature level and can handle arbitrary image sizes. Furthermore, while self-training is widely used in zero-shot semantic segmentation to generate pseudo-labels, we propose a new knowledge-distillation-inspired self-training strategy, namely Annealed
Self-Training, which can automatically assign different im-portance to pseudo-labels to improve performance. We sys-tematically study the proposed Relative Positional Encod-ing and Annealed Self-Training in a comprehensive exper-imental evaluation, and our empirical results conﬁrm the effectiveness of our method on three benchmark datasets. 1.

Introduction
Zero-shot learning (ZSL) solves the task of learning in the absence of training data of that task (e.g., recognizing unseen classes). It has been widely adopted in classic com-puter vision problems, such as classiﬁcation [1, 15, 2, 46, 48, 52, 58, 41, 54, 7, 38, 14, 34, 39, 19, 25, 51] and ob-ject detection [42, 3, 16, 11, 43, 4]. The key challenge in
ZSL based tasks is to make the underlying model capable of recognizing classes that had not been seen during train-ing. Earlier work focused on learning a joint embedding be-tween seen and unseen classes [1, 15, 2, 46, 48, 52, 58, 41].
In recent works, knowledge and generative-based methods have gained more prominence. Knowledge-based methods
[39, 19, 25, 51] use structured knowledge learned in another domain (e.g., natural language [36], knowledge graph[32], etc.) as constraints [19, 25] to transfer the learned informa-tion to unseen categories. Generative methods use attributes
[14, 7], natural language [44] or word embeddings [14, 9] as priors to generate synthetic features for unseen categories.
In this work, we investigate the zero-shot semantic seg-mentation problem, which is less studied than other zero-shot computer vision problems [53, 5, 26, 18]. Generative methods have been widely adopted in zero-shot semantic segmentation problem. Bucher et al. [5] proposed ZS3Net, in which they leverage Generative Moment Matching Net-works [31] to generate synthetic features for unseen cat-egories by using word2vec [35] embeddings and random noise as prior (Fig. 1(a)). The use of random noise prevents the model from collapsing, which occurs due to lack of fea-ture variety [59, 17]. Li et al. [30] extended ZS3Net [5] by adding a structural relation loss which constrains the gen-erated synthetic features to have a similar structure as the semantic word embedding space. Gu et al. [18] suggested using a context-aware normal distributed prior when gener-ating synthetic features instead of random noise (Fig. 1(b)).
We propose to incorporate spatial information to im-prove the performance of the zero-shot semantic segmen-tation problem. Our motivation arises from the assump-tion that knowing a pixel’s location may help semantic seg-mentation because it is a 2D prediction task. Incorporat-ing spatial information in computer vision problems has re-cently attracted the attention of the community. In image classiﬁcation, [12] slices the input image into nine patches and adds a positional vector for each patch to indicate the patches’ location. Other methods leverage [56] the rela-tive position of objects through a space-aware knowledge graph for object detection. Zhang et al. [57] counted the co-occurrence of features to learn spatial invariant representa-tion for semantic segmentation. However, to the best of our knowledge, spatial information has not been widely studied in previous zero-shot learning research. In this work, we propose to exploit spatial information by using Positional
Encoding [50], as shown in Fig. 1(c). Positional Encoding generates a positional vector that indicates the position of a pixel in the image. Previous work [12] divided the input im-ages into ﬁxed number of patches and appended positional
embeddings on the images. However, in our case, dividing the input image into small patches is incompatible because the semantic segmentation problem requires the entire im-age as input. We mitigate this shortcoming by incorporat-ing spatial information into the image features and propose
Relative Positional Encoding to handle varying input image sizes.
In the zero-shot learning problem, unlabeled samples, in-cluding unseen classes, are sometimes available. Trained models can annotate unlabeled samples to obtain additional training data [60] and ﬁne-tune the models with pseudo-annotated data. Such a training strategy is called self-training. In zero-shot semantic segmentation, self-training annotates pixel-level pseudo labels [5, 18]. To reduce the number of unreliable pseudo labels, [5] ranks the conﬁ-dence score (i.e. the probability of classes after softmax function) of pseudo labels and uses only the most conﬁ-dent 75% pseudo labels. [18] eliminates pseudo labels if the conﬁdence score is below a certain threshold.
This work proposes a knowledge distillation-inspired
[22] self-training strategy, namely Annealed Self-Training to generate better pseudo-annotations for self-(AST), training. Knowledge distillation is widely used in the form of teacher-student learning [22, 37], where the student net-work is trained on the soft labels from the teacher network in addition to (one-hot) hard labels since soft labels have more information due to high entropy [22]. In the zero-shot semantic segmentation problem, previous methods [5, 18] set a threshold to eliminate pseudo labels from the low con-ﬁdence scores, while assigning the same loss weights to the remaining pseudo labels. This is similar to using hard la-bels in teacher-student learning. However, it is hard to en-sure that the threshold can generalize for each sample, and again low conﬁdence pseudo labels may also contain some useful information. To avoid the shortcomings of setting a threshold and assigning the same loss weights to pseudo labels, in AST, we use pseudo-annotations of all unlabeled pixels while re-weighing their importance according to their conﬁdence score. We leverage the annealed softmax [22] function to normalize the pseudo labels’ weights and con-trol their relative importance by adjusting the annealed tem-perature in the softmax function.
We make the following contributions in this paper.
Firstly, we introduce the Spatial Information Module to in-corporate spatial information in semantic segmentation us-ing a novel Relative Positional Encoding (RPE) scheme.
Compared to previous work [12], RPE does not need patch-sliced input and can handle varying image sizes. Secondly, we propose a knowledge distillation-inspired self-training strategy, namely Annealed Self-Training (AST). AST gen-erates pseudo-annotations for unlabeled samples and ad-justs their importance during self-training with a tunable an-nealed temperature. Finally, we evaluate the performance 2D Positional Encoding
+
NN
NN
+
G
+
G
+
G
Image Deep 
Features
Synthetic 
Features
Semantic 
Latent Code
Gaussian 
Noise Prior
Contextual 
Prior
Spatial 
Prior
,
,
~"(0, 1) (a) (b) (c)
Figure 1. Latent code used in generating synthetic features con-sists of two parts: i) semantic word embeddings and ii) a normal distributed prior. The normal distributed prior can be: (a) Gaus-sian noise [5]; (b) Context-aware prior [18]; (c) Our context-aware and space-aware priors. on three benchmark datasets and conduct extensive ablation experiments to demonstrate the effectiveness of our method. 2.