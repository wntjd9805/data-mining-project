Abstract 1.

Introduction
Single image 3D photography enables viewers to view a still image from novel viewpoints. Recent approaches combine monocular depth networks with inpainting net-works to achieve compelling results. A drawback of these techniques is the use of hard depth layering, making them unable to model intricate appearance details such as thin hair-like structures. We present SLIDE, a modular and uniﬁed system for single image 3D photography that uses a simple yet effective soft layering strategy to better pre-serve appearance details in novel views.
In addition, we propose a novel depth-aware training strategy for our in-painting module, better suited for the 3D photography task.
The resulting SLIDE approach is modular, enabling the use of other components such as segmentation and mat-ting for improved layering. At the same time, SLIDE uses an efﬁcient layered depth formulation that only re-quires a single forward pass through the component net-works to produce high quality 3D photos. Extensive ex-perimental analysis on three view-synthesis datasets, in combination with user studies on in-the-wild image col-lections, demonstrate superior performance of our tech-nique in comparison to existing strong baselines while be-ing conceptually much simpler. Project page: https:
//varunjampani.github.io/slide
∗Equal Contribution.
Still images remain a popular choice for capturing, stor-ing, and sharing visual memories despite the advances in richer capturing technologies such as depth and video sens-ing. Recent advances [34, 39, 26, 31, 16, 17] show how such 2D images can be “brought to life” just by interac-tively changing the camera viewpoint, even without scene movement, thereby creating a more engaging 3D viewing experience. Following recent works, we use the term ‘Sin-gle image 3D photography’ to describe the process of con-verting a 2D image into a 3D viewing experience. Single image 3D photography is quite challenging, as it requires estimating scene geometry from a single image along with inferring the dissoccluded scene content when moving the camera around. Recent state-of-the-art techniques for this problem can be broadly classiﬁed into two approaches -modular systems [26, 31] and monolithic networks [34, 39].
Modular systems [31, 26, 16, 17] leverage state-of-the-art 2D networks such as single-image depth estimation, 2D inpainting, and instance segmentation. Given recent ad-vances in monocular depth estimation [29, 21, 20] and in-painting [40, 41] fueled by deep learning on large-scale 2D datasets, these modular approaches have been shown to work remarkably well on in-the-wild images. A key compo-nent of these modular approaches is decomposing the scene into a set of layers based on depth discontinuities. The scene is usually decomposed into a set of layers with hard discon-tinuities and thus can not model soft appearance effects such 1
as matting. See Figure 1 (right) for an example novel view synthesis result from 3D-Photo [31], which is a state-of-the-art single image 3D photography system.
In contrast, monolithic approaches [34, 39] attempt to learn end-to-end trainable networks using view synthesis losses on multi-view image datasets. These networks usu-ally take a single image as input and produce a 3D represen-tation of a scene, such as point clouds [39] or multi-plane images [34], from which one could interactively render the scene from different camera viewpoints. Since these net-works usually decompose the scene into a set of soft 3D layers [34] or directly generate 3D structures [39], they can model appearance effects such as matting. Despite being elegant, these networks usually perform poorly while infer-ring disoccluded content and have difﬁculty generalizing to scenes out of the training distribution, a considerable lim-itation given the difﬁculty in obtaining multi-view datasets on a wide range of scene types.
In this work, we propose a new 3D photography ap-proach that uses soft layering and depth-aware inpainting.
We refer to our approach as ‘SLIDE’ (Soft-Layering and
Inpainting that is Depth-aware). Our key technique is a simple yet effective soft layering scheme that can incorpo-rate intricate appearance effects. See Figure 1 for an exam-ple view synthesis result of SLIDE (Ours), where thin hair structures are preserved in novel views. In addition, we pro-pose an RGBD inpainting network that is trained in a novel depth-aware fashion resulting in higher quality view syn-thesis. The resulting SLIDE framework is modular, and al-lows easy incorporation of state-of-the-art components such as depth and segmentation networks. SLIDE uses a sim-ple two-layer decomposition of the scene and requires only a single forward pass through different components. This is in contrast to the state-of-the-art approaches [31, 26], which are modular and require several passes through some components networks. Moreover, all of the components in the SLIDE framework are differentiable and can be imple-mented using standard GPU layers in a deep learning tool-box, resulting in a uniﬁed system. This also brings our
SLIDE framework closer to single network approaches. We make the following contributions in this work:
• We propose a simple yet effective soft layering formula-tion that enables synthesizing intricate appearance details such as thin hair-like structures in novel views.
• We propose a novel depth-aware technique for training an inpainting network for the 3D photography task.
• The resulting SLIDE framework is both modular and uni-ﬁed with favorable properties such as only requiring a single forward computation with favorable runtime.
• Extensive experiments on four different datasets demon-strate the superior performance of SLIDE in terms of both quantitative metrics as well as from user studies. 2.