Abstract 1.

Introduction
Recent attempts to solve the problem of head reenact-ment using a single reference image have shown promising results. However, most of them either perform poorly in terms of photo-realism, or fail to meet the identity preser-vation problem, or do not fully transfer the driving pose and expression. We propose HeadGAN, a novel system that conditions synthesis on 3D face representations, which can be extracted from any driving video and adapted to the fa-cial geometry of any reference image, disentangling identity from expression. We further improve mouth movements, by utilising audio features as a complementary input. The 3D face representation enables HeadGAN to be further used as an efficient method for compression and reconstruction and a tool for expression and pose editing.
Visual data synthesis [44, 43], including talking head animation [45, 49, 48, 17, 33, 34, 23] are particularly ex-citing and thriving research areas, with countless applica-tions in editing, games, social media, VR, teleconference and virtual assistance. Over the past years, solutions were mainly given by the graphics community. For instance,
Face2Face [40] method performs face reenactment, by re-covering facial expressions from a driving video and over-writing them to the source frames. Some recent learning-based approaches [23, 25, 13] have sought to solve the prob-lem of full head reenactment, which aims to transfer not only the expression, but also the pose, from a driving per-son to the source identity. The shortcoming of such methods is their dependence on long video footage of the source, as they train person-specific models. At the same time, various
methods have been proposed for reenacting human heads under a few-shot setting [45, 43, 48, 17, 34, 14], where only a limited number of reference images are available, even a single one. Most state-of-the-art approaches use facial key-points to guide synthesis [49, 48, 43, 34], which usually leads to identity preservation problems during reenactment, as key-points encode appearance information. The problem becomes more prominent when the head geometry of the source differs from that of the person in the driving video.
In this paper we propose HeadGAN, a novel one-shot
GAN-based method for head animation and editing. We take a different approach from most existing few-shot meth-ods and use a 3D face representation similar to PNCC [52] to condition synthesis. We capitalise on prior knowledge of expression and identity disentanglement, enclosed within 3D Morphable Models (3DMMs) [3, 5, 4, 6]. Our decision to model faces with 3DMMs enables HeadGAN to oper-ate as: 1) a real-time reenactment system operating at âˆ¼ 20 fps, 2) an efficient method for facial video compression and reconstruction, 3) a facial expression editing method, 4) a novel view synthesis system, including face frontalisation.
Fig. 1 illustrates the tasks supported by our method. Apart from 3D faces, we optionally condition the generative pro-cess on speech features coming from the audio signal, en-abling our method to perform accurate mouth synthesis, as suggested by our automated lipreading experiment.
We perform extensive comparisons with state-of-the-art methods [45, 43, 48, 34, 31, 51] and report superior image quality and performance, in terms of standard GAN metrics
[19, 41], on the tasks of reconstruction, reenactment and frontalisation, even when compared to models [48] trained on the larger VoxCeleb2 [9] dataset. Lastly, we conduct an ablation study in order to demonstrate the contribution of each component of our system. 2.