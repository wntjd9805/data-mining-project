Abstract
We introduce the problem of perpetual view generation— long-range generation of novel views corresponding to an arbitrarily long camera trajectory given a single image. This is a challenging problem that goes far beyond the capabili-ties of current view synthesis methods, which quickly degen-erate when presented with large camera motions. Methods for video generation also have limited ability to produce long sequences and are often agnostic to scene geometry. We take a hybrid approach that integrates both geometry and image synthesis in an iterative ‘render, refine and repeat’ framework, allowing for long-range generation that cover large distances after hundreds of frames. Our approach can be trained from a set of monocular video sequences. We pro-pose a dataset of aerial footage of coastal scenes, and com-pare our method with recent view synthesis and conditional video generation baselines, showing that it can generate plausible scenes for much longer time horizons over large camera trajectories compared to existing methods.
Project page at https://infinite-nature.github.io. 1.

Introduction
Consider the input image of a coastline in Fig. 1. Imag-ine flying through this scene as a bird. Initially, we would see objects grow in our field of view as we approach them.
* indicates equal contribution
Beyond, we might find a wide ocean or new islands. At the shore, we might see cliffs or beaches, while inland there could be mountains or forests. As humans, we are adept at imagining a plausible world from a single picture, based on our own experience.
How can we emulate this ability on a computer? One ap-proach would be to attempt to generate an entire 3D planet with high-resolution detail from a single image. However, this would be extremely expensive and far beyond the cur-rent state of the art. So, we pose the more tractable problem of perpetual view generation: given a single image of scene, the task is to synthesize a video corresponding to an arbi-trary camera trajectory. Solving this problem can have ap-plications in content creation, novel photo interactions, and methods that use learned world models like model-based reinforcement learning.
Perpetual view generation, though simple to state, is an extremely challenging task. As the viewpoint moves, we must extrapolate new content in unseen regions and also synthesize new detail in existing regions that are now closer to the camera. Two active areas of research, video synthesis and view synthesis, both fail to scale to this problem for different reasons.
Recent video synthesis methods apply developments in image synthesis [20] to the temporal domain, or rely on recurrent models [10]. But they can generate only limited numbers of novel frames (e.g., 25 [41] or 48 frames [9]).
Additionally, such methods often neglect an important el-ement of the video’s structure—they model neither scene geometry nor camera movement. In contrast, many view synthesis methods do take advantage of geometry to synthe-size high-quality novel views. However, these approaches can only operate within a limited range of camera motion.
As shown in Figure 6, once the camera moves outside this range, such methods fail catastrophically.
We propose a hybrid framework that takes advantage of both geometry and image synthesis techniques to ad-dress these challenges. We use disparity maps to represent a scene’s geometry, and decompose the perpetual view gen-eration task into the framework of render-refine-and-repeat.
First, we render the current frame from a new viewpoint, using disparity to ensure that scene content moves in a geo-metrically correct manner. Then, we refine the resulting image and geometry. This step adds detail and synthesizes new content in areas that require inpainting or outpainting.
Because we refine both the image and disparity, the whole process can be repeated in an recurrent manner, allowing for perpetual generation with arbitrary trajectories.
To train our system, we curated a large dataset of drone footage of nature and coastal scenes from over 700 videos, spanning 2 million frames. We run a structure from motion pipeline to recover 3D camera trajectories, and refer to this as the Aerial Coastline Imagery Dataset (ACID). Our trained model can generate sequences of hundreds of frames while maintaining the aesthetic feel of an aerial coastal video, even though after just a few frames, the camera has moved beyond the limits of the scene depicted in the initial view.
Our experiments show that our novel render-refine-repeat framework, with propagation of geometry via disparity maps, is key to tackling this problem. Compared to recent view synthesis and video generation baselines, our approach can produce plausible frames for much longer time horizons.
This work represents a significant step towards perpetual view generation, though it has limitations such as a lack of global consistency in the hallucinated world. We believe our method and dataset will lead to further advances in genera-tive methods for large-scale scenes. 2.