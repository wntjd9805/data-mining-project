Abstract
Recent works on implicit neural representations have shown promising results for multi-view surface reconstruc-tion. However, most approaches are limited to relatively simple geometries and usually require clean object masks for reconstructing complex and concave objects.
In this work, we introduce a novel neural surface reconstruction framework that leverages the knowledge of stereo match-ing and feature consistency to optimize the implicit surface representation. More speciﬁcally, we apply a signed dis-tance ﬁeld (SDF) and a surface light ﬁeld to represent the scene geometry and appearance respectively. The SDF is directly supervised by geometry from stereo matching, and is reﬁned by optimizing the multi-view feature consistency and the ﬁdelity of rendered images. Our method is able to improve the robustness of geometry estimation and support reconstruction of complex scene topologies. Extensive ex-periments have been conducted on DTU, EPFL and Tanks and Temples datasets. Compared to previous state-of-the-art methods, our method achieves better mesh reconstruc-tion in wide open scenes without masks as input. 1.

Introduction
Surface reconstruction from calibrated multi-view im-ages is one of the key problems in 3D computer vision.
Traditionally, surface reconstruction can be divided into two substeps: 1) depth maps and point clouds are reconstructed from images via multi-view stereo (MVS) algorithms [2, 8, 41, 37, 44]; 2) a surface, usually represented as a trian-gular mesh, is extracted from dense points by maximizing the conformity to points [20, 17, 26]. Optionally, a sur-face reﬁnement step can be applied to recover geometry de-tails through multi-view photo-consistency [34, 42, 5, 4, 7].
While this pipeline has been proven to be effective and ro-bust in various scenarios, the reconstructed geometry may be suboptimal due to the accumulated loss in representa-tion conversions from images to points, then to mesh. For example, errors introduced in point cloud reconstruction would pass to the surface reconstruction, causing wrong
Mesh
Rendered Image
Figure 1. Our reconstructions on Family and Horse of the Tanks and Temples dataset. The proposed MVSDF is able to reconstruct correct mesh topologies with ﬁne details for highly texture-less and reﬂective surfaces. mesh topology and difﬁcult to be recovered. Although re-cent learning-based MVS [15, 13, 45, 32, 46] and mesh ex-traction [31, 28, 39, 33] methods have been proposed to boost the reconstruction quality of each substep indepen-dently, it is still desirable to reconstruct the optimal surface from images in an end-to-end manner.
Alternatively, recent works on neural representations show that mesh surface can be directly constructed from images through implicit representations and differentiable rendering [30, 24, 36, 25, 49, 39, 29]. Surface geometry and color information of the scene are usually represented as im-plicit functions, which are directly modeled by multi-layer perceptrons (MLPs) in the network and optimized through differentiable rendering. The triangle mesh can be extracted from the implicit ﬁeld via the Marching Cube algorithm
[26, 28]. Compared with classical meshing pipelines, these methods are able to reconstruct the scene geometry in an end-to-end manner and generate synthesized images at the
same time. However, as all scene parameters are jointly optimized at the same time, geometry is only a by-product of the entire differential rendering pipeline, and ambigui-ties exist in geometry and appearance[52]. To mitigate the problem, implicit differentiable renderer (IDR) [49] applies manually labeled object masks as input, but it is not feasi-ble for a large number of images and is sometimes not well deﬁned for real-world image inputs.
In this paper, we present MVSDF, a novel neural sur-face reconstruction framework that combines implicit neu-ral surface estimation with recent advanced MVS networks.
On the one hand, we follow the implicit differentiable renderer[49] to represent the surface as zero level set of a signed distance ﬁeld (SDF) and the appearance as a surface light ﬁeld, which are jointly optimized through render loss.
On the other hand, we introduce deep image features and depth maps from learning-based MVS [46, 51, 50] to assist the implicit SDF estimation. The SDF is supervised by in-ferred depth values from the MVS network, and is further reﬁned by maximizing the multi-view feature consistency at the surface points of the SDF. We ﬁnd that the surface topol-ogy can be greatly improved with the guidance from MVS depth maps, and our method can be applied to complex ge-ometries even without input object masks. Also, compared to render loss in IDR, the multi-view feature consistency imposes a photometric constraint at an early stage of the dif-ferentiable rendering pipeline, which signiﬁcantly improves the geometry accuracy and helps to preserve high-ﬁdelity details in ﬁnal reconstructions.
Our method has been evaluated on DTU [14], EPFL
[40] and Tanks and Temples [18] datasets. We compare our method with classical meshing pipelines and recent differ-entiable rendering based networks on both mesh reconstruc-tion and view synthesis quality. Both quantitative and quali-tative results demonstrate that our method is able to recover complex geometries even without object masks as input. 2.