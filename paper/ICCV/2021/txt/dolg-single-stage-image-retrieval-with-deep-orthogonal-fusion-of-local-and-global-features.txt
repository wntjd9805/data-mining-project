Abstract
Image Retrieval is a fundamental task of obtaining im-ages similar to the query one from a database. A com-mon image retrieval practice is to firstly retrieve candi-date images via similarity search using global image fea-tures and then re-rank the candidates by leveraging their local features. Previous learning-based studies mainly fo-cus on either global or local image representation learning to tackle the retrieval task. In this paper, we abandon the two-stage paradigm and seek to design an effective single-stage solution by integrating local and global information inside images into compact image representations. Specif-ically, we propose a Deep Orthogonal Local and Global (DOLG) information fusion framework for end-to-end im-age retrieval. It attentively extracts representative local in-formation with multi-atrous convolutions and self-attention at first. Components orthogonal to the global image rep-resentation are then extracted from the local information.
At last, the orthogonal components are concatenated with the global representation as a complementary, and then ag-gregation is performed to generate the final representation.
The whole framework is end-to-end differentiable and can be trained with image-level labels. Extensive experimental results validate the effectiveness of our solution and show that our model achieves state-of-the-art image retrieval per-formances on Revisited Oxford and Paris datasets. 1 1.

Introduction
Image retrieval is an important task in computer vision, and its main purpose is to find out the images from a large-scale database that are similar to a query one.
It is extensively studied by designing various handcrafted fea-tures [25, 6, 50]. Owing to the development of deep learn-* Equal contribution. † Corresponding authors. 1Codes: PaddlePaddle Implementation.
Figure 1: Illustration of current two-stage and our single-stage image retrieval. Previous methods (a) firstly obtain candidates similar to the query from the database via global deep representation, and then local descriptors are extracted for leveraged re-ranking. Our method (b) aggregates global and local features via an orthogonal fusion to generate the final compact descriptor, and then single-shot similarity search is performed. ing technologies, great progress has been achieved recently
[1, 29, 37, 9]. Representations (also named as descriptors) of images, which are used to encode image contents and measure their similarities, play a central role in this task. In the literature of learning-based solutions, two types of im-age representations are widely explored. One is global fea-ture [4, 3, 44, 1] which serves as high-level semantic image signature and the other one is local feature [5, 36, 29, 18] which can comprise discriminative geometry information about specific image regions. Generally, the global feature can be learned to be invariant to viewpoint and illumination, while local features are more sensitive to local geometry and textures. Therefore, previous state-of-the-art solutions
[38, 29, 9] always work in a two-stage paradigm. As shown in Figure 1(a), candidates are retrieved via global feature with high recall, and then re-ranking is performed with lo-cal features to further improve precision.
In this paper, we also concentrate on the field of image retrieval with deep networks. Though state-of-the-art per-formance has been achieved by previous two-stage solu-tions, they need to rank images twice, and the second re-ranking stage is conducted using the expensive RANSAC
[13] or AMSK [42] for spatial verification with local fea-tures. More importantly, errors exist inevitably in both stages. Two-stage solutions would suffer from error accu-mulation which can be a bottleneck for further performance improvement. To alleviate these problems, we abandon the two-stage framework and attempt to find an effective uni-fied single-stage image retrieval solution, which is shown in Figure 1(b). Previous wisdom has implied that global features and local features are two complementary and es-sential elements for image retrieval. Intuitively, integrating local features and global features into a compact descriptor can achieve our goal. A satisfying local and global fusion scheme can take advantage of both types of features to mu-tually boost each other for single-stage retrieval. Besides, error accumulation can be avoided. Therefore, we techni-cally answer how to design an effective global and local fu-sion mechanism for end-to-end single-stage image retrieval.
Specifically, we proposed a Deep Orthogonal Local and
Global feature fusion model (DOLG). It consists of a lo-cal and a global branch for learning two types of features jointly and an orthogonal fusion module to combine them.
In detail, the local components orthogonal to the global feature are decomposed from the local features. Subse-quently, the orthogonal components are concatenated with the global feature as a complementary part. Finally, it is ag-gregated into a compact descriptor. With our orthogonal fu-sion, the most critical local information can be extracted and redundant components to the global information are elimi-nated, such that local and global components can be mu-tually reinforced to produce final representative descriptor with objective-oriented training. To enhance local feature learning, inspired by lessons from prior research, the lo-cal branch is equipped with multi-atrous convolutions [10] and self-attention [29] mechanisms to attentively extract representative local features. We think alike FP-Net [31] in terms of orthogonal feature space learning, but DOLG aims at complementary fusion of features in orthogonal spaces. Extensive experiments on Revisited Oxford and
Pairs [32] show the effectiveness of our framework. DOLG also achieves state-of-the-art performance on both datasets.
To summarize, our main contributions are as follows:
• We propose to retrieve images in a single-stage paradigm with a novel orthogonal global and local feature fusion framework, which can generate a com-pact representative image descriptor and is end-to-end learnable.
• In order to attentively extract discriminative local fea-tures, a module with multi-atrous convolution layers followed by a self-attention module is designed for im-proving our local branch.
• Extensive experiments are conducted and comprehen-sive analysis is provided to validate the effectiveness of our solution. Our single-stage method significantly outperforms previous two-stage state-of-the-art ones. 2.