Abstract
Rotation augmentations generally improve a model’s in-variance/equivariance to rotation - except in object detec-tion. In object detection the shape is not known, therefore rotation creates a label ambiguity. We show that the de-facto method for bounding box label rotation, the Largest
Box Method, creates very large labels, leading to poor per-formance and in many cases worse performance than us-ing no rotation at all. We propose a new method of rota-tion augmentation that can be implemented in a few lines of code. First, we create a differentiable approximation of label accuracy and show that axis-aligning the bounding box around an ellipse is optimal. We then introduce Ro-tation Uncertainty (RU) Loss, allowing the model to adapt to the uncertainty of the labels. On five different datasets (including COCO, PascalVOC, and Transparent Object Bin
Picking), this approach improves the rotational invariance of both one-stage and two-stage architectures when mea-sured with AP, AP50, and AP75. 1.

Introduction
It is desirable for object detectors to work when scenes are rotated. But there is a problem: methods like Convo-lutional Neural Networks (CNNs) may be scale and trans-lation invariant but CNNs are not rotation invariant [15].
To overcome this problem, the training dataset can be ex-panded to include data at new rotation angles. This is known as rotation augmentation. In object detection, rotation aug-mentation can be abstracted as follows: given an original bounding box and any desired angle of rotation, how should we determine the axis-aligned rotated bounding box label?
If the shape of the object is known, this is quite simple: we rotate the object shape and re-compute the bounding box.
However in the case of object detection, the shape is un-known.
The prevailing folk wisdom in the community is to se-lect a label large enough to completely overlap the ro-In studying this problem, we find that tated box [45].
Figure 1: A method is proposed to properly rotate a bounding box for rotation augmentation. The previ-ous solution of largest box is an overestimate of the per-fect bounding box for a rotated scene. See table for how choice of rotation augmentation affects object detection per-formance. this method may hurt performance, and on COCO [22], we find that every other prior we tried is better. Yet somehow this “Largest Box” method is very prevalent both in academia and in large scale object detection code-bases [43, 4, 45, 37, 28, 26, 5, 7, 1, 17, 6, 40].
Indeed, recent analysis has found that largest box is only robust to about < 3◦ [16] of rotation.
In this paper, we propose an advance on the largest box solution that achieves significantly better performance on five object detection datasets; while retaining the simplicity of a few lines of code implementation. 1.1. Contributions
In a nutshell, our solution has two aspects. First, we de-rive an elliptical shape prior from first principles to deter-mine the rotated box label. We compare it to many other
novel priors and show this is optimal. Second, we introduce a novel Rotation Uncertainty (RU) Loss function, which allows the network to adapt the labels at higher rotations us-ing priors from lower rotations based on label certainty. We demonstrate the effectiveness of this solution by both im-proving performance datasets where rotation is important such as Pascal VOC [14] and Transparent Object Bin Pick-ing [18] and generalizing to novel test time rotations on MS
COCO [22] (Figure 1). 1.2. Scope
Rotation data augmentation in object detection is not new. This paper is not about finding the best overall way to use a rotation data augmentation. For that - a brute force search or papers like AutoAugment [45] might be better examples. This paper focuses solely on methods to perform a rotation augmentation on axis-aligned bounding boxes. When implemented, these proposed modifications boil down to a few lines of code and leave little reason to use the current Largest Box method. 2.