Abstract
State of the art (SOTA) few-shot learning (FSL) methods suffer signiﬁcant performance drop in the presence of do-main differences between source and target datasets. The strong discrimination ability on the source dataset does not necessarily translate to high classiﬁcation accuracy on the target dataset. In this work, we address this cross-domain few-shot learning (CDFSL) problem by boosting the gen-eralization capability of the model. Speciﬁcally, we teach the model to capture broader variations of the feature dis-tributions with a novel noise-enhanced supervised autoen-coder (NSAE). NSAE trains the model by jointly recon-structing inputs and predicting the labels of inputs as well as their reconstructed pairs. Theoretical analysis based on intra-class correlation (ICC) shows that the feature em-beddings learned from NSAE have stronger discrimination and generalization abilities in the target domain. We also take advantage of NSAE structure and propose a two-step
ﬁne-tuning procedure that achieves better adaption and im-proves classiﬁcation performance in the target domain. Ex-tensive experiments and ablation studies are conducted to demonstrate the effectiveness of the proposed method. Ex-perimental results show that our proposed method consis-tently outperforms SOTA methods under various conditions. 1.

Introduction
After years of development, deep learning methods have achieved remarkable success on visual classiﬁcation tasks
[17, 37, 21, 30]. The outstanding performance, however, heavily relies on large-scale labeled datasets [5]. Mean-while, although some large-scale public datasets, e.g. Ima-geNet [9], have made it possible to achieve better than hu-man performance on common objects recognition, practical applications of visual classiﬁcation systems usually target at categories whose samples are very difﬁcult to collect, e.g.
∗Equal contribution with alphabetical order. †Work done as an intern in Noah’s Ark Lab. t e
N e g a m
I
-i n i m
B
U
C feature  extractor  feature  extractor  (1) training on source domain feature  extractor  feature  extractor  (a) (b) (c)
? (d) (2) testing on target domain
Figure 1: Motivation illustration. Visualization of feature embeddings by a less-generalized feature extractor fa and a well-generalized feature extractor fb cross source and target domains. medical images. The scarcity of data limits the generaliza-tion of current vision systems. Therefore, it is essential to learn to generalize to novel classes with a limited number of labeled samples available in each class. Cross-domain few-shot learning (CDFSL) is proposed to recognize instances of novel categories in the target domain with few labeled samples. Different from general few-shot learning(FSL) where large-scale source dataset and few-shot novel dataset are from the same domain, target dataset and source dataset under CDFSL setting come from different domains, i.e. the marginal distributions of features of images in two domains are quite different [49].
Much work has been done to solve FSL problem and ob-tained promising results [41, 14, 35, 36, 11, 33, 43]. How-[6, 16] show that the state-of-the-art (SOTA) meta-ever, learning based FSL methods fail to generalize well and per-form poorly under CDFSL setting. It is therefore of great
importance to improve the generalization capability of the model and address the domain shift issue from source to tar-get domains. [39] proposes to add a feature-transformation layer to simulate various distributions of image features in training. However, this method requires access to a great amount of data from multiple domains during training. [47] combines the FSL learning objective and the domain adap-tation objective, while their basic assumption that source and target domain have identical label sets limits its applica-tion. [16] experimentally shows that the traditional transfer learning methods can outperform meta-learning FSL meth-ods by a large margin on the benchmark. In these methods, a feature extractor is pre-trained on the source dataset and then ﬁne-tuned on the target dataset with only a few labeled samples. Following this thread,
[26] proposes to regular-ize the eigenvalues of the image features to avoid negative knowledge transfer.
In this work, our observation is that generalization capa-bility plays a vital role for representation learning in cross-domain settings. As the feature distributions of different domains are distinct, a competent feature extractor on the source domain does not necessarily lead to good perfor-mance on the target domain. It may overﬁt to the source domain and fail to generalize in the target domain. Fig. 1(a) shows an example of a less-generalized feature extractor fa that ﬁts the source dataset very well and achieves high performance in downstream classiﬁcation task. When the model is transferred to a different target domain, as shown in Fig. 1(c), the corresponding feature embeddings of differ-ent classes may become less discriminative or even insepa-rable. On the other hand, a less perfect feature extractor fb on the source domain (Fig. 1(b)), may have stronger gen-eralization capability and obtain more discriminative fea-ture embeddings in the target domain (Fig. 1(d)). Under this intuition, we focus on boosting the generalization ca-pability of the transfer learning based methods, and inves-tigate a multi-task learning scheme that shows the potential to improve generalization performance in [22]. Speciﬁcally, we propose a novel noise-enhanced supervised autoencoder (NSAE) that takes more than classiﬁcation tasks and learns the feature space in discriminative and generative manners.
We take advantage of the NSAE structure in the following aspects. First of all, it is shown in [22] that a supervised autoencoder can signiﬁcantly improve model generalization capability. We develop the model to jointly predict the la-bels of inputs and reconstruct the inputs. Secondly, moti-vated by the observation that “the addition of noise to the input data of a neural network during training can, in some circumstances, lead to signiﬁcant improvements in general-ization performance” [31, 2, 1], we consider reconstructed images as noisy inputs and feed them back to the system.
The joint classiﬁcations based on reconstructed and origi-nal images further improve the generalization capability and avoid the necessity of designing a mechanism to add hand-crafted noises. Thirdly, we develop a two-step ﬁne-tuning procedure to better adapt model to the target domain. Be-fore tuning with the supervised classiﬁcation method, we
ﬁrst tune model on the target domain in an unsupervised manner by learning to reconstruct images in novel classes.
Furthermore, theoretical analysis based on inter-class corre-lation (ICC) suggests that our intuition in Fig. 1 holds statis-tically in CDFSL settings. Last but not the least, we claim that our proposed method can be easily added to existing transfer learning based methods to boost their performance.
Our major contributions are summarized as follows:
• To the best of our knowledge, our work is the ﬁrst work that proposes to use supervised autoencoder frame-work to boost the model generalization capability un-der few-shot learning settings.
• We propose to take reconstructed images from autoen-coder as noisy inputs and let the model further predict their labels, which proves to further enhance the model generalization capability. The two-step ﬁne-tuning procedure that does reconstruction in novel classes bet-ter adapts model to the target domain.
• Extensive experiments across multiple benchmark datasets, various backbone architectures, and differ-ent loss function combinations demonstrate the efﬁ-cacy and robustness of our proposed framework under cross-domain few-shot learning setting. 2.