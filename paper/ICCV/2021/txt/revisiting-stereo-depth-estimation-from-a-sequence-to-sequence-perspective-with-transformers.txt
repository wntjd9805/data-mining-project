Abstract
Stereo depth estimation relies on optimal correspon-dence matching between pixels on epipolar lines in the left and right images to infer depth. In this work, we re-visit the problem from a sequence-to-sequence correspon-dence perspective to replace cost volume construction with dense pixel matching using position information and atten-tion. This approach, named STereo TRansformer (STTR), has several advantages: It 1) relaxes the limitation of a fixed disparity range, 2) identifies occluded regions and pro-vides confidence estimates, and 3) imposes uniqueness con-straints during the matching process. We report promis-ing results on both synthetic and real-world datasets and demonstrate that STTR generalizes across different do-mains, even without fine-tuning. 1.

Introduction
Stereo depth estimation is of substantial interest since it enables the reconstruction of 3D information. To this end, corresponding pixels are matched between the left and right camera image; the difference in corresponding pixel loca-tion, i. e. the disparity, can then be used to infer depth and reconstruct the 3D scene. Recent deep learning-based ap-proaches to stereo depth estimation have shown promising results but several challenges remain.
One such challenge relates to the use of a limited dispar-ity range. Disparity values can, in theory, range from zero to the image width depending on the resolution/baseline of the cameras, and their proximity to the physical objects.
However, many of the best performing approaches are con-strained to a manually pre-specified disparity range (typi-cally a maximum of 192 px) [21]. These methods rely on
“cost volumes” in which matching costs are computed for multiple candidate matches and a final predicted disparity value is computed as the aggregated sum. This self-imposed disparity range is necessary to enable memory-feasible im-plementations of these methods but is not flexible to prop-erties of the physical scene and/or the camera setup. In ap-plications such as autonomous driving and endoscopic in-tervention, it is important to recognize close objects irre-spective of camera setup (with disparity values potentially larger than 192) to avoid collisions, suggesting the need to relax the fixed disparity range assumption.
Geometric properties and constraints such as occlusion and matching uniqueness, which led to the success of non-learning based approaches such as [18], are also often miss-ing from learning-based approaches. For stereo depth es-timation, occluded regions do not have a valid disparity.
Prior algorithms generally infer disparities for occluded re-gions via a piece-wise smoothness assumption, which may not always be valid. Providing a confidence estimate to-gether with the disparity value would be advantageous for down-stream analysis, such as for registration or scene un-derstanding algorithms, to enable weighting or rejection of occluded and low-confidence estimates. However, most prior approaches do not provide such information. More-over, pixels in one image should not be matched to multi-ple pixels in the other image (up to image resolution) since they correspond to the same location in the physical scene
[28]. Although this constraint can be clearly useful to re-solve ambiguity, most existing learning-based approaches do not impose it.
The aforementioned problems largely arise from short-comings of the contemporary view of stereo matching which attempts to construct a cost volume. Approaches that consider disparity estimation from a sequence-to-sequence matching perspective along epipolar lines can avoid these challenges. Such methods are not new, to our knowledge, the first attempt using dynamic programming was proposed in 1985 [28], where intra- and inter-epipolar line informa-tion is used together with a uniqueness constraint. How-(a) Network overview (c) MPI Sintel (b) Scene Flow (d) KITTI 2015
Figure 1. (a) STTR estimates disparity by first extracting features from stereo images using a shared feature extractor. The extracted feature descriptors are then used by a Transformer for dense self- and cross-attention computation, yielding a raw disparity estimate. A context adjustment layer further refines the disparity with information across epipolar lines conditioned on the left image for cross epipolar line optimality. (b-f) Inference of STTR trained only on synthetic Scene Flow dataset. Top row shows the left images. Bottom row shows predicted disparities. The color map used to visualize disparity is relative to the image width and is shown on the right. Black color indicates occlusion. Best viewed in color. (e) Middlebury 2014 (f) SCARED ever, it only used similarities between pixel intensities as matching criteria, which is inadequate beyond local match-ing, and thus restricted its performance. Recent advances in attention-based networks that capture long-range asso-ciations between feature descriptors prompt us to revisit this perspective. We take advantage of the recent Trans-former architecture [36] proposed for language processing and recent advances in feature matching [31], and present a new end-to-end-trained stereo depth estimation network named STereo TRansformer (STTR). The main advantage of STTR is that it computes pixel-wise correlation densely and does not construct a fixed-disparity cost volume. There-fore, STTR can mitigate the drawbacks of most contem-porary approaches that were detailed above with little to no compromises in performance. We report competitive performance on synthetic and real image benchmarks and demonstrate that STTR trained only on synthetic images also generalizes well to other domains without refinement.
We make the following technical advances to enable the realization of STTR: – Instead of pixel-wise intensity correlation in traditional stereo depth estimation methods like [28], we adopt a
Transformer with alternating self- and cross-attentions combined with the optimal transport theory previously demonstrated in sparse feature matching [31]. This de-sign allows us to match pixels explicitly and densely while imposing a uniqueness constraint. – We provide a relative pixel distance encoding to the feature descriptor and use a customized attention mechanism to define discriminative features during the matching process. This helps resolve ambiguity during the matching process. – We devise a memory-feasible implementation of
STTR which enables the training of the proposed model on conventional hardware. For seamless dis-tribution and reproducibility, our code is available on-line1 and only uses existing PyTorch functions [29]. 2.