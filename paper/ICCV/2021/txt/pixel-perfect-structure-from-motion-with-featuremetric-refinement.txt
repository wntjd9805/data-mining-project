Abstract
Finding local features that are repeatable across multiple views is a cornerstone of sparse 3D reconstruction. The clas-sical image matching paradigm detects keypoints per-image once and for all, which can yield poorly-localized features and propagate large errors to the ﬁnal geometry. In this pa-per, we reﬁne two key steps of structure-from-motion by a direct alignment of low-level image information from multi-ple views: we ﬁrst adjust the initial keypoint locations prior to any geometric estimation, and subsequently reﬁne points and camera poses as a post-processing. This reﬁnement is robust to large detection noise and appearance changes, as it optimizes a featuremetric error based on dense features predicted by a neural network. This signiﬁcantly improves the accuracy of camera poses and scene geometry for a wide range of keypoint detectors, challenging viewing conditions, and off-the-shelf deep features. Our system easily scales to large image collections, enabling pixel-perfect crowd-sourced localization at scale. Our code is publicly avail-able at github.com/cvg/pixel-perfect-sfm as an add-on to the popular SfM software COLMAP. 1.

Introduction
Mapping the world is an important requirement for spatial intelligence applications in augmented reality or robotics.
Tasks like visual localization or path planning can bene-ﬁt from accurate sparse or dense 3D reconstructions of the environment. These can be built from images using
Structure-from-Motion (SfM), which associates observa-tions across views to estimate camera parameters and 3D scene geometry. Sparse reconstruction based on matching local image features [10, 20, 22, 32, 48, 54, 56, 62] is the most common due to its scalability and its robustness to appearance changes introduced by varying devices, view-points, and temporal conditions found in crowdsourced sce-narios [2, 28, 33, 39, 45, 47, 55].
SfM assumes that sparse interest points [10, 20, 22, 32, 48, 56, 59, 81, 88] can be reliably detected across views. It typi-*indicates equal contributions
Figure 1: From sparse to dense. We improve the accu-racy of sparse Structure-from-Motion by reﬁning 2D key-points, camera poses, and 3D points using the direct align-ment of deep features. This featuremetric optimization lever-ages dense image information but can scale to scenes with thousands of images. Such reﬁnement results in subpixel-accurate reconstructions, even in challenging conditions. cally selects such points for each image independently and relies on these initial detections for the remainder of the reconstruction process. However, detecting keypoints from a single view is inherently inaccurate due to appearance changes and discrete image sampling [30]. The advent of convolutional neural network (CNNs) for detection has mag-niﬁed this issue, as they generally do not retain local image information and instead favor global context.
Multi-view geometric optimization with bundle adjust-ment [4, 40, 79] is commonly used to reﬁne cameras and points using reprojection errors. Dusmanu et al. [23] pro-posed to reﬁne keypoint locations prior to SfM via an analo-gous geometric cost constrained with local optical ﬂow. This can improve SfM, but has limited accuracy and scalability.
In this work, we argue that local image information is valuable throughout the SfM process to improve its accu-racy. We adjust both keypoints and bundles, before and after reconstruction, by direct image alignment [18, 25, 49] in a learned feature space. Exploiting this locally-dense informa-Differently, dense matching [13, 46, 58, 71, 74, 78, 80] considers all pixels in each image, resulting in denser and more accurate correspondences. It has been successful for constrained settings like optical ﬂow [38, 73] or stereo depth estimation [86], but is not suitable for large-scale SfM due to its high computational cost due to many redundant corre-spondences. Several recent works [44, 57, 75, 92] improve the matching efﬁciency by ﬁrst matching coarsely and subse-quently reﬁning correspondences using a local search. This is however limited to image pairs and thus cannot create point tracks required by SfM.
Our work combines the best of both paradigms by leverag-ing dense local information to reﬁne sparse observations. It is inherently amenable to SfM as it can optimize all locations over multiple views in a track simultaneously.
Subpixel estimation is a well-studied problem in correspon-dence search. Common approaches either upsample the input images or ﬁt polynomials or Gaussian distributions to local image neighborhoods [27,34,37,48,66]. With the widespread interest in CNNs for local features, solutions tailored to 2D heatmaps have been recently developed, such as learning ﬁne local sub-heatmaps [36] or estimating subpixel corrections with regression [14, 77] or the soft-argmax [52, 89]. Cleaner heatmaps can also arise from aggregating predictions over multiple virtual views using data augmentation [20].
Detections or local afﬁne frames can be combined across multiple views with known poses in a least-squares geo-metric optimization [24, 79]. Dusmanu et al. [23] instead reﬁne keypoints solely based on tentative matches, without assuming known geometry. This geometric formulation ex-hibits remarkable robustness, but is based on a local optical
ﬂow whose estimation for each correspondence is expen-sive and approximate. We unify both keypoint and bundle optimizations into a joint framework that optimizes a fea-turemetric cost, resulting in more accurate geometries and a more efﬁcient keypoint reﬁnement.
Direct alignment optimizes differences in pixel intensities by implicitly deﬁning correspondences through the motion and geometry. It therefore does not suffer from geometric noise and is naturally subpixel accurate via image interpola-tion. Direct photometric optimization has been successfully applied to optical ﬂow [8,49], visual odometry [18,25,26,42],
SLAM [5, 69], multi-view stereo (MVS) [19, 21, 87], and pose reﬁnement [70]. It generally fails for moderate displace-ments or appearances changes, and is thus not suitable for large-baseline SfM. One notable work by Woodford & Ros-ten [84] reﬁnes dense SfM+MVS models with a robust image normalization. It focuses on dense mapping with accurate initial poses and moderate appearance changes. Georgel et al. [29] instead estimate more accurate relative poses by el-egantly combining photometric and geometric costs. They show that dense information can improve sparse estimation
Figure 2: Reﬁnement pipeline. Our reﬁnement works on top of any SfM pipeline that is based on local features. We perform a two-stage adjustment of keypoints and bundles.
The approach ﬁrst reﬁnes the 2D keypoints only from tenta-tive matches by optimizing a direct cost over dense feature maps. The second stage operates after SfM and reﬁnes 3D points and poses with a similar featuremetric cost. tion is signiﬁcantly more accurate than geometric optimiza-tion, while deep, high-dimensional features extracted by a
CNN ensure wider convergence in challenging conditions.
This formulation elegantly combines globally-discriminative sparse matching with locally-accurate dense details. It is applicable to both incremental [67, 72] and global [9, 12, 51]
SfM irrespective of the types of sparse or dense features.
We validate our approach in experiments evaluating the accuracy of both 3D structure and camera poses in vari-ous conditions. We demonstrate drastic improvements for multiple hand-crafted and learned local features using off-the-shelf CNNs. The resulting system produces accurate reconstructions and scales well to large scenes with thou-sands of images. In the context of visual localization, it can, in addition to providing a more accurate map, also reﬁne poses of single query images with minimal overhead.
For the beneﬁt of the research community, we will release our code as an extension to COLMAP [67, 68] and to the popular localization toolbox hloc [60, 61]. We believe that our featuremetric reﬁnement can signiﬁcantly improve the accuracy of existing datasets [64] and push the community towards sub-pixel accurate localization at large scale. 2.