Abstract
Inspired by the complementarity between conventional frame-based and bio-inspired event-based cameras, we pro-pose a multi-modal based approach to fuse visual cues from the frame- and event-domain to enhance the single object tracking performance, especially in degraded conditions (e.g., scenes with high dynamic range, low light, and fast-motion objects). The proposed approach can effectively and adaptively combine meaningful information from both do-mains. Our approach’s effectiveness is enforced by a novel designed cross-domain attention schemes, which can effec-tively enhance features based on self- and cross-domain at-tention schemes; The adaptiveness is guarded by a specially designed weighting scheme, which can adaptively balance the contribution of the two domains. To exploit event-based visual cues in single-object tracking, we construct a large-scale frame-event-based dataset, which we subsequently employ to train a novel frame-event fusion based model. Ex-tensive experiments show that the proposed approach out-performs state-of-the-art frame-based tracking methods by at least 10.4% and 11.9% in terms of representative success rate and precision rate, respectively. Besides, the effective-ness of each key component of our approach is evidenced by our thorough ablation study. 1.

Introduction
Recently, convolutional neural networks (CNNs) based approaches show promising performance in object tracking tasks [4, 6, 10, 12, 14, 18, 20, 30, 37, 46, 48, 49]. These approaches mainly use conventional frame-based cameras as sensing devices since they can effectively measure ab-solute light intensity and provide a rich representation of a scene. However, conventional frame-based sensors have limited frame rates (i.e., (cid:20) 120 FPS) and dynamic range (i.e., (cid:20) 60 dB). Thus, they do not work robustly in degraded conditions. Figure 1 (a) and (b) show two examples of de-graded conditions, high dynamic range, and fast-moving object, respectively. Under both conditions, we hardly see (cid:3) Joint ﬁrst authors. yBaocai Yin (ybc@dlut.edu.cn) and Bo Dong (bo.dong@sri.com) are the corresponding authors.
Figure 1. Limitations of conventional frame-based and bio-inspired event-based cameras. (a) and (b) show the limitation of a frame-based camera under HDR and fast-moving object, respec-tively. (d) shows an event-based camera’s asynchronous output of the scene shown in (c), sparse and no texture information. the moving objects. Thus, obtaining meaningful visual cues of the objects is challenging. By contrast, an event-based camera, a bio-inspired sensor, offers high temporal resolu-tion (up to 1MHz), high dynamic range (up to 140 dB), and low energy consumption [7]. Nevertheless, it cannot mea-sure absolute light intensity and thus texture cues (as shown in Figure 1 (d)). Both sensors are, therefore, complemen-tary. The unique complementarity triggers us to propose a multi-modal sensor fusion-based approach to improve the tracking performance in degraded conditions, which lever-ages the advantages of both the frame- and event-domain.
Yet, event-based cameras measure light intensity changes and output events asynchronously. It differs sig-niﬁcantly from conventional frame-based cameras, which represent scenes with synchronous frames. Besides, CNNs-based approaches are not designed to digest asynchronous inputs. Therefore, combining asynchronous events and syn-chronous images remains challenging. To address the chal-lenge, we propose a simple yet effective event aggregation approach to discretize the time domain of asynchronous events. Each of the discretized time slices can be accumu-lated to a conventional frame, thus can be easily processed by a CNNs-based model. Our experimental results show the proposed aggregation method outperforms other commonly used event accumulation approaches [9, 26, 32, 40, 50]. An-other critical challenge, similar to other multi-modal fusion-based approaches [2, 8, 27, 29, 33, 41], is grasping mean-ingful cues from both domains effectively regardless the di-versity of scenes. In doing so, we introduce a novel cross-domain feature integrator, which leverages self- and cross-domain attention schemes to fuse visual cues from both the event- and frame-domain effectively and adaptively. The ef-fectiveness is enforced by a novel designed feature enhance-ment module, which enhances its own domain’s feature based on both domains’ attentions. Our approach’s adap-tivity is held by a specially designed weighting scheme to balance the contributions of the two domains. Based on the two domains’ reliabilities, the weighting scheme adaptively regulates the two domains’ contributions. We extensively validate our multi-modal fusion-based method and demon-strate that our model outperforms state-of-the-art frame-based methods by a signiﬁcant margin, at least 10.4% and 11.9% in terms of representative success rate and precision rate, respectively.
To exploit event-based visual cues in single object track-ing and enable more future research on multi-modal learn-ing with asynchronous events, we construct a large-scale single-object tracking dataset, FE108, which contains 108 sequences with a total length of 1.5 hours. FE108 provides ground truth annotations on both the frame- and event-domain. The annotation frequency is up to 40Hz and 240Hz for the frame and event domains, respectively. To the best of our knowledge, FE108 is the largest event-frame-based dataset for single object tracking, which also offers the highest annotation frequency in the event domain.
To sum up, our contributions are as follows: (cid:15) We introduce a novel cross-domain feature integrator, which can effectively and adaptively fuse the visual cues provided from both the frame and event domains. (cid:15) We construct a large-scale frame-event-based dataset for single object tracking. The dataset covers wide chal-lenging scenes and degraded conditions. (cid:15) Our extensively experimental results show our ap-proach outperforms other state-of-the-art methods by a sig-niﬁcant margin. Our ablation study evidences the effective-ness of the novel designed attention-based schemes. 2.