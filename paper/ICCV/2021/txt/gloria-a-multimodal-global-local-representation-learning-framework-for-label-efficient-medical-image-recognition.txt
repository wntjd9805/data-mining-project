Abstract
In recent years, the growing utilization of medical imag-ing is placing an increasing burden on radiologists. Deep learning provides a promising solution for automatic medi-cal image analysis and clinical decision support. However, large-scale manually labeled datasets required for training deep neural networks are difficult and expensive to obtain for medical images. The purpose of this work is to develop label-efficient multimodal medical imaging representations by leveraging radiology reports. We propose an attention-based framework for learning global and local represen-tations by contrasting image sub-regions and words in the paired report.
In addition, we propose methods to lever-age the learned representations for various downstream medical image recognition tasks with limited labels. Our results demonstrate high-performance and label-efficiency for image-text retrieval, classification (finetuning and zeros-shot settings), and segmentation on different datasets. 1.

Introduction
Advancements in medical imaging technologies have revolutionized healthcare practices and improved patient outcome. However, the growing number of imaging studies in recent years places an ever-increasing burden on radiol-ogists, impacting the quality and speed of clinical decision making. While deep learning and computer vision provide a promising solution for automating medical image anal-ysis, annotating medical imaging datasets requires domain expertise and is cost-prohibitive at scale. Therefore, the task of building effective medical imaging models is hindered by the lack of large-scale manually labeled datasets.
To address this problem, a natural solution is to lever-age the corresponding medical reports that contain detailed
*Equal Contribution
Correspondence: mschuang@stanford.edu
Emails: {mschuang,liyues,mlungren,syyeung}@stanford.edu
Code: https://github.com/marshuang80/gloria
Figure 1: Our multimodal global-local representation learning framework (GLoRIA) extracts features through the image and text encoders, and learns global and localized representations by con-trasting attention-weighted image sub-regions and words in the re-ports. The learned global-local representations are utilized to ob-tain label-efficient models for various downstream tasks including image-text retrieval, classification (fine-tuning and zero-shot set-tings) and segmentation. descriptions of the medical conditions observed by radi-ologists. Several recent works utilize these medical re-ports to provide supervision signals and learn multimodal representations by maximising mutual information between the global representations of the paired image and report
[13, 3, 41, 40]. However, pathology usually occupies only small proportions of the medical image, making it diffi-cult to effectively represent these subtle yet crucial visual cues using global representations alone. This motivates a need for learning localized features to capture fine-grained semantics in the image in addition to global representa-tions. While the idea of learning local representations has been explored in several other contexts for natural images
[7, 27, 25, 4], including image-text retrieval and text-to-image generation, these works typically require pre-trained object detection models to extract localized image features, which are not readily available for medical images.
In this work, we focus on jointly learning global and local representations for medical images using the cor-Specifically, we intro-responding radiology reports. duce GLoRIA: a framework for learning Global-Local
Representations for Images using Attenion mechanism by contrasting image sub-regions and words in the paired re-port. Instead of relying on pretrained object detectors, we learn attention weights that emphasize significant image sub-regions for a particular word to create context-aware local image representations (Fig. 1). Due to the lengthy na-ture of medical reports, we introduce a self-attention-based image-text joint representation learning model, which is ca-pable of multi-sentence reasoning. Furthermore, we pro-pose a token aggregation strategy to handle abbreviations and typos common in medical reports.
We demonstrate the generalizability of our learned repre-sentations for data-efficient image-text retrieval, classifica-tion and segmentation. We conduct experiments and evalu-ate our methods on three different datasets: CheXpert [16],
RSNA Pneumonia [32] and SIIM Pneumothorax. Utiliz-ing both global and local representations for image-text re-trieval is non-trivial due to the difficulty in incorporating multiple representations for each image-text pair. There-fore, we introduce a similarity aggregation strategy to lever-age signals from both global and local representations for retrieval. Furthermore, our localized image representations are generated using attention weights that rely on words to provide context. Thus, to leverage localized representations for classification, we generate possible textual descriptions of the severity, sub-type and location for each medical con-dition category. This allows us to frame the image classifi-cation task by measuring the image-text similarity and en-ables zero-shot classification using the learned global-local representations. Finally, experimental results on various tasks and datasets show that our GLoRIA achieves good performance with limited labels and consistently outper-forms other methods in previous works.
Our contribution can be summarized as follows: (1) We propose GLoRIA: a framework for jointly learning mul-timodal global and local representations of medical im-ages by contrasting attention weighted image regions with words in the paired reports and (2) we demonstrate the label-efficiency of our framework by evaluating the learned multimodal global-local representations on image-text re-trieval, classification (finetune and zero-shot) and segmen-tation tasks with limited labels. 2.