Abstract
Co-occurrent visual pattern makes aggregating contex-tual information a common paradigm to enhance the pixel representation for semantic image segmentation. The ex-isting approaches focus on modeling the context from the perspective of the whole image, i.e., aggregating the image-level contextual information. Despite impressive, these methods weaken the significance of the pixel representa-tions of the same category, i.e., the semantic-level con-textual information. To address this, this paper proposes to augment the pixel representations by aggregating the image-level and semantic-level contextual information, re-spectively. First, an image-level context module is designed to capture the contextual information for each pixel in the whole image. Second, we aggregate the representations of the same category for each pixel where the category re-gions are learned under the supervision of the ground-truth segmentation. Third, we compute the similarities between each pixel representation and the image-level contextual information, the semantic-level contextual information, re-spectively. At last, a pixel representation is augmented by weighted aggregating both the image-level contextual infor-mation and the semantic-level contextual information with the similarities as the weights. Integrating the image-level and semantic-level context allows this paper to report state-of-the-art accuracy on four benchmarks, i.e., ADE20K, LIP,
COCOStuff and Cityscapes 1. 1.

Introduction
Semantic image segmentation, which assigns per-pixel predictions of object categories for the given image, is a fun-damental problem in computer vision. This task is excep-tionally significant to tons of real-world applications, e.g., automatic driving and robot sensing. Recent developments
*Corresponding author. 1Our code will be available at https://github.com/SegmentationBLWX
/sssegmentation.
Figure 1. Main idea of integrating image-level and semantic-level context. The blue region on the upper branch denotes for semantic-level context and the purple area on the lower branch stands for image-level context. of deep neural networks [16, 24] encourage the emergence of a series of works [4, 5, 7, 22, 30, 35], where FCN is the cornerstone of these works. Its encoder-decoder structure, which reduces the spatial dimension to extract features and then leverages upsampling to recover spatial extent, shows numerous improvements in semantic segmentation. Based on this, recent researches mainly focus on two issues to fur-ther boost the segmentation performance. One is how to improve the encoder structure so that the models can ex-tract more robust representation for each pixel [25, 29, 34].
The other is how to model the context so that the network can enhance the representation capability of each pixel by encoding the contextual information into the original fea-ture representations [4, 11, 14, 17, 30, 35], which is also the interest of this paper.
Co-occurrent visual pattern inspires the emergence of a series of works about modeling context. These approaches can be roughly divided into two types, i.e., multi-scale con-text modeling and relational context modeling. For multi-scale context modeling, Deeplab [4] introduces the atrous spatial pyramid pooling (ASPP) so that it can leverage vari-ous dilation convolutions to capture the multi-scale contex-tual information. PSPNet [35] proposes to utilize pyramid spatial pooling to aggregate the multi-scale contextual in-formation. For relational context modeling, Wang et al. [26] first revisit the traditional local means [1], and then design
a non-local block to weighted aggregate the contextual in-formation in the whole image. Zhu et al. propose an asym-metric pyramid non-local block to decrease the compution and GPU memory consumption of the standard non-local module. Apart from this, ACFNet [32] and OCRNet [30] first group the pixels into a set of regions, and then aug-ment the pixel representations by weighted aggregating the region representations where the weights are determined by the relations between the pixels and regions. Though im-pressive, these solutions only focus on aggregating the con-textual information from the perspective of the whole image (i.e., image-level contextual information), while discard the significance of the pixel representations of the same cate-gory. Accordingly, they all suffer from the same issue that the contextual information of each pixel is unevenly cap-tured from the category region the pixel belongs to and the regions of other categories. For instance, the pixels in the boundaries or the regions of objects of small scales tend to capture much more contextual information from the regions of other objects. Since the label of a pixel is the category of the object that the pixel belongs to, too much contextual information from other objects may cause the network to mislabel these pixels as other categories.
To alleviate the problem above, this paper proposes to augment the pixel representations by aggregating the image-level and semantic-level contextual information, re-spectively. As illustrated in Figure 1, the image-level con-text stands for all the pixels in the input image and the semantic-level context denotes for the pixels in the same category region. Based on this definition, an image-level context module (ILCM) is first designed to capture the con-textual information from the whole image and thereby, we can obtain the image-level contextual information. Then, a novel semantic-level context module (SLCM) is pro-posed to aggregate representations of the same category for each pixel (i.e., the semantic-level contextual information), where the category regions are learned under the supervi-sion of the ground-truth segmentation. Next, the similar-ities between the pixel representation and the image-level contextual information, the semantic-level contextual infor-mation are calculated. At last, the pixel representations are augmented by weighted aggregating the image-level con-textual information and the semantic-level contextual infor-mation, where the weights are determined by the calculated similarities. On the whole, our major contributions are sum-marized as follows:
• To the best of our knowledge, this paper first explores improving the pixel representations by aggregating the image-level contextual information and semantic-level contextual information, respectively.
• This paper designs a simple yet effective image-level context module (ILCM) and a novel semantic-level context module (SLCM) to capture the contextual in-formation from the perspective of the whole image and the category region, respectively. Experimental results demonstrate the effectiveness of our method.
• A general architecture framework named ISNet is pro-posed in this paper, which reveals how to leverage
ILCM and SLCM to consistently boost the perfor-mance of semantic image segmentation. The proposed framework allows this paper to achieve state-of-the-art accuracy on four segmentation benchmarks, i.e.,
ADE20K, LIP, COCOStuff and Cityscapes. 2.