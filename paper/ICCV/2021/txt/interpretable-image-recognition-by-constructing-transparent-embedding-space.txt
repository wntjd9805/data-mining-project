Abstract
Humans usually explain their reasoning (e.g. classiﬁca-tion) by dissecting the image and pointing out the evidence
Inspired from these parts to the concepts in their minds. by this cognitive process, several part-level interpretable neural network architectures have been proposed to explain the predictions. However, they suffer from the complex data structure and confusing the effect of the individual part to output category. In this work, an interpretable image recog-nition deep network is designed by introducing a plug-in transparent embedding space (TesNet) to bridge the high-level input patches (e.g. CNN feature maps) and the out-put categories. This plug-in embedding space is spanned by transparent basis concepts which are constructed on the Grassmann manifold. These basis concepts are en-forced to be category-aware and within-category concepts are orthogonal to each other, which makes sure the embed-ding space is disentangled. Meanwhile, each basis concept can be traced back to the particular image patches, thus they are transparent and friendly to explain the reasoning process. By comparing with state-of-the-art interpretable methods, TesNet is much more beneﬁcial to classiﬁcation tasks, esp. providing better interpretability on predictions and improve the ﬁnal accuracy. The code is available at https://github.com/JackeyWang96/TesNet. 1.

Introduction
Convolutional neural networks(CNNs) [20, 19, 29, 12, 14] have achieved surpassing performance in many visual tasks, such as image recognition and detection. However, besides the extraordinary discrimination power, CNNs and their corresponding results are still hard to explain, which severely limits their applications such as self-driving cars, diagnosis of cancer and etc. Recently, more and more in-terpretable methods have been proposed on CNNs, in order to open the black box of neural networks. Among them, an
*Corresponding authors.
Figure 1: Image of a Yellow-headed Blackbird and humans usually explain their reasoning process through some parts of the image which looks like some learned concepts of the
Yellow-headed Blackbird to classify the bird’s species. intuitive strategy is to visualize the feature representation hidden inside a CNN, but there is still a vast gap between network visualization and semantic interpretations for neu-ral networks.
Considering the examples in Figure 1, how would you identify the bird image as a Yellow-headed Blackbird and not a Cape Glossy Starling? Maybe you ﬁnd that the bird’s head, legs, and feathers look like those concepts of Yellow-headed Blackbird rather than Cape Glossy Starling. In other words, you may gather the evidence in your mind and make a ﬁnal decision. Speciﬁcally, humans usually explain their reasoning process by dissecting the image into object parts and pointing out the evidence from these identiﬁed parts to the concepts stored in his / her mind. Therefore, for the in-telligent machine, it is an emergent issue to determine the object parts and construct the concepts in order to imple-ment interpretable image classiﬁcation.
To enforce the CNN-based classiﬁer with interpretabil-ity, the basis concepts are introduced as a plug-in compo-nent in CNN architecture [1]. From the cognitive point of view [26], the interpretable concepts should cover the fol-lowing characteristics: 1) Informative: the input data can be efﬁciently represented in the space spanned by the ba-sis concepts, and its essential information is preserved in the new representation, 2) Diversity: each data point is re-lated to only a few non-overlapping basis concepts, and the points belonging to one category are related to the similar the basis concepts subset of concepts, 3) Discriminative: are class-aware so that the categories can be separated well in the space of concepts.
For grasping the concepts, researchers take advantage of the high-level features in deep neural networks (e.g.,
CNN), on which the auto-encoding [1] or prototype learn-ing [5, 15, 23] are operated. These existing methods can explicitly represent the inputs with basis concepts, i.e., they met the ﬁrst requirement. Among them, some prior distribu-tion such as U-shaped Beta distribution is adopted to limit the number of concepts to which the high-level features are related [15]. However, they suffer from the situation that the basis concepts may be entangled, which is not friendly to isolate the effect of individual concept to the input repre-sentation and the output category, and further destroys the classiﬁcation performance.
In this study, thus, we focus on constructing the basis concepts simultaneously containing the above three charac-teristics. First, each category has its own basis concepts, and the corresponding concept subsets of different categories are as much different as possible. Second, a good map-ping is built to provide a bridge between high-level features and basis concepts. Third, for the input image, the basis concepts are helpful to compute the ﬁnal prediction score along all categories. To implement this, a Grassmann man-ifold is introduced to construct basis concepts. As shown in Figure 2, for each category, the subset of corresponding basis concepts is taken as a point on the Grassmann man-ifold. In this case, the basis concepts of one category are orthogonal to each other. Meanwhile, the class-aware con-cept subsets are part away from each other by constraining their projection metric. The above two constraints make sure that the basis concepts are disentangled. To improve the transparency of leaned basis concepts, the prototypical high-level patches of the original images are extracted to represent the concept.
In this work, an interpretable network architecture is designed by introducing a plug-in transparent embedding space (TesNet) which is spanned by transparent basis con-cepts which are constructed on the Grassmann manifold.
These basis concepts are enforced to be category-aware and within-category concepts are orthogonal to each other, which makes sure the embedding space is disentangled.
In order to demonstrate the model efﬁciency, we evaluate our model on two case studies, i.e bird species identiﬁca-tion and car model identiﬁcation. Extensive experiments demonstrate the broad applicability of our model on differ-ent CNN architectures. To investigate the proposed basis concept construction strategy, a series of ablation studies
Figure 2: The illustration of constructing transparent em-bedding space on Grassmann Manifold. The space is spanned by category-aware transparent basis concepts.
Each basis concept is demonstrated by its most related pro-totypical high-level patches. have been conducted. As expected, our model achieves the state-of-the-art performance in terms of accuracy compared with interpretable methods providing the same level of in-terpretability.
The contribution of our work can be summarized as: 1)
An interpretable neural network with transparent basis con-cepts (TesNet) is proposed to explain the prediction results semantically and quantitatively. 2) The basis concepts are category-aware and disentangled, which leverages discrim-inability and interpretability. 3) The latent space spanned by the basis concepts has the ability to represent the input and preserve the essential information. 4) The proposed trans-parent basis concepts construction layer is a generic solu-tion and can be taken as a plug-in component for various
CNN architectures. 2.