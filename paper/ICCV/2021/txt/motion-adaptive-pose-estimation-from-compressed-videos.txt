Abstract
Human pose estimation from videos has many real-world applications. Existing methods focus on applying models with a uniform computation proﬁle on fully de-coded frames, ignoring the freely-available motion signals and motion-compensation residuals from the compressed stream. A novel model, called Motion Adaptive Pose Net is proposed to exploit the compressed streams to efﬁciently de-code pose sequences from videos. The model incorporates a
Motion Compensated ConvLSTM to propagate the spatially aligned features, along with an adaptive gate to dynami-cally determine if the computationally expensive features should be extracted from fully decoded frames to compen-sate the motion-warped features, solely based on the resid-ual errors. Leveraging the informative yet readily available signals from compressed streams, we propagate the latent features through our Motion Adaptive Pose Net efﬁciently
Our model outperforms the state-of-the-art models in pose-estimation accuracy on two widely used datasets with only around half of the computation complexity. 1.

Introduction
Human pose estimation has drawn increasing amount of attentions over the years [1, 27, 38, 22, 45, 34, 41, 25]. It has a wild range of applications in action recognition, hu-man computer interactions, AR/VR and robotics. Over the years, there have been growing interests in the pose estima-tion from videos, in which human dynamics has been faith-fully captured compared to still images. In applications like intelligent surveillance camera analysis or imitation learn-ing for robots, thousands of hours videos need to be ana-lyzed by the deep models, which draws attention to more efﬁcient approaches [23, 49, 7] to process the frames.
Directly adopting the state-of-the-art models to perform pose estimation on each frame is sub-optimal as it not only
*Corresponding author.
Figure 1. We introduce the Motion Adaptive Pose Net, which ex-ploit the usage of the free of charge yet valuable motion vectors and motion-compensation residuals from compressed streams to dramatically boost the efﬁciency of the video based pose estima-tion models. Exploiting the readily available motion and residual information stored in the compressed streams, we obtain state of the art performance with about half of the computation. ignores the valuable temporal dynamics embedded across consecutive frames but also results in huge amount of re-dundant computations. Recent approaches [19, 20, 41, 25] adopt temporal modules to model the temporal dynamics frame by frame, e.g. Recurrent Neural Net [19], LSTM models [20, 41] and temporal convolution models [25], etc.
However, features from each frame are still extracted in-dependently without considering the natural coherence be-tween neighboring frames.
On the other hand, video compression techniques relies heavily on the temporal coherence to drastically reduce the size of the videos. The modern standard [30, 29] split the entire videos into group of pictures (GOP) and only fraction of frames are encoded in its complete form. For the remain-ing frames, only sparse motion vectors (MV) and residual errors (R) are stored. The SOTA pose machines operate on fully decoded frames while constantly ignore the free of charge yet valuable motion ﬁeld encoded in the compressed streams.
Motivated by the tremendous amount of space savings
brought by the modern video codec, we explore the us-age of the compressed streams for efﬁcient pose estima-tion from videos. The encoded motion ﬁeld between frames provide valuable insights for how the pose changes across frames while the residual errors provides a direct measure-ment for the quality of the motion vectors used for compres-sion. Considering the unique properties of these two cost-free representations, we propose an adaptive pose machine that dynamically switch between the light MV-warped fea-tures from a ConvLSTM and the accurate features extracted from decoded frames, based on the residual errors. When-ever the model determines motion warped features are reli-able, the computationally heavy feature extraction stage is skipped, which offers drastic computation savings.
We validate our models on two widely used datasets and showcase that with the valuable intermediate representa-tions during decoding, we could develop 1) efﬁcient opera-tions that provide relatively reliable features for pose infer-ence as well as 2) fast veriﬁcation mechanism on the reli-ability of the morphed features. As the result, our Motion
Adaptive Pose Net outperform the previous SOTA models in terms of both the efﬁciency and the accuracy. In sum-mary, our contributions are as followed:
- We exploit the internal motion signals and residual er-rors in compressed videos for pose estimation, which are free of cost yet preserve valuable motion information.
- A dynamic model is developed to efﬁciently utilize the compressed signals, which drastically reduce the computa-tion costs compared to SOTA models.
- We evaluate our proposed model on two widely used datasets: Penn Action [48] and Sub-JHMDB [15]. We out-performs existing methods in both accuracy and efﬁciency. 2.