Abstract
The recently developed vision transformer (ViT) has achieved promising results on image classification com-pared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature rep-resentations in transformer models for image classification.
To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of dif-ferent sizes to produce stronger image features. Our ap-proach processes small-patch and large-patch tokens with two separate branches of different computational complex-ity and these tokens are then fused purely by attention multi-ple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the
ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT. 1.

Introduction
The novel transformer architecture [36] has led to a big leap forward in capabilities for sequence-to-sequence mod-eling in NLP tasks [10]. The great success of transform-ers in NLP has sparked particular interest from the vision community in understanding whether transformers can be a strong competitor against the dominant Convolutional Neu-ral Network based architectures (CNNs) in vision tasks such as ResNet [15] and EfficientNet [34]. Previous re-Figure 1: Improvement of our proposed approach over
DeiT [35] and ViT [11]. The circle size is proportional to the model size. All models are trained on ImageNet1K from scratch. The results of ViT are referenced from [45]. search efforts on transformers in vision have, until very re-cently, been largely focused on combining CNNs with self-attention [3, 48, 31, 32]. While these hybrid approaches achieve promising performance, they have limited scala-bility in computation compared to purely attention-based transformers. Vision Transformer (ViT) [11], which uses a sequence of embedded image patches as input to a standard transformer, is the first kind of convolution-free transform-ers that demonstrate comparable performance to CNN mod-els. However, ViT requires very large datasets such as Im-ageNet21K [9] and JFT300M [33] for training. DeiT [35] subsequently shows that data augmentation and model reg-ularization can enable training of high-performance ViT models with fewer data. Since then, ViT has instantly in-spired several attempts to improve its efficiency and effec-tiveness from different aspects [35, 45, 14, 38, 19].
Along the same line of research on building stronger vision transformers, in this work, we study how to learn multi-scale feature representations in transformer models for image recognition. Multi-scale feature representations
have proven beneficial for many vision tasks [5, 4, 22, 21, 25, 24, 7], but such potential benefit for vision transform-ers remains to be validated. Motivated by the effective-ness of multi-branch CNN architectures such as Big-Little
Net [5] and Octave convolutions [6], we propose a dual-branch transformer to combine image patches (i.e. tokens in a transformer) of different sizes to produce stronger visual features for image classification. Our approach processes small and large patch tokens with two separate branches of different computational complexities and these tokens are fused together multiple times to complement each other.
Our main focus of this work is to develop feature fusion methods that are appropriate for vision transformers, which has not been addressed to the best of our knowledge. We do so by an efficient cross-attention module, in which each transformer branch creates a non-patch token as an agent to exchange information with the other branch by attention.
This allows for linear-time generation of the attention map in fusion instead of quadratic time otherwise. With some proper architectural adjustments in computational loads of each branch, our proposed approach outperforms DeiT [35] by a large margin of 2% with a small to moderate increase in FLOPs and model parameters (See Figure 1).
The main contributions of our work are as follows:
• We propose a novel dual-branch vision transformer to extract multi-scale feature representations for image classification. Moreover, we develop a simple yet ef-fective token fusion scheme based on cross-attention, which is linear in both computation and memory to combine features at different scales.
• Our approach performs better than or on par with sev-eral concurrent works based on ViT [11], and demon-strates comparable results with EfficientNet [34] with regards to accuracy, throughput and model parameters. 2.