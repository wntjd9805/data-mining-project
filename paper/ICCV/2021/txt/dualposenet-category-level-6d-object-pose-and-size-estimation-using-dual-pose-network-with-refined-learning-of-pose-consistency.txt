Abstract
Category-level 6D object pose and size estimation is to predict full pose configurations of rotation, translation, and size for object instances observed in single, arbitrary views of cluttered scenes. In this paper, we propose a new method of Dual Pose Network with refined learning of pose consis-tency for this task, shortened as DualPoseNet. DualPoseNet stacks two parallel pose decoders on top of a shared pose encoder, where the implicit decoder predicts object poses with a working mechanism different from that of the ex-plicit one; they thus impose complementary supervision on the training of pose encoder. We construct the encoder based on spherical convolutions, and design a module of
Spherical Fusion wherein for a better embedding of pose-sensitive features from the appearance and shape observa-tions. Given no testing CAD models, it is the novel intro-duction of the implicit decoder that enables the refined pose prediction during testing, by enforcing the predicted pose consistency between the two decoders using a self-adaptive loss term. Thorough experiments on benchmarks of both category- and instance-level object pose datasets confirm efficacy of our designs. DualPoseNet outperforms existing methods with a large margin in the regime of high precision.
Our code is released publicly at https://github. com/Gorilla-Lab-SCUT/DualPoseNet. 1.

Introduction
Object detection in a 3D Euclidean space is demanded in many practical applications, such as augmented reality, robotic manipulation, and self-driving car. The field has been developing rapidly with the availability of benchmark datasets (e.g., KITTI [10] and SUN RGB-D [25]), where carefully annotated 3D bounding boxes enclosing object in-stances of interest are prepared, which specify 7 degrees of
*Corresponding author freedom (7DoF) for the objects, including translation, size, and yaw angle around the gravity axis. This 7DoF setting of 3D object detection is aligned with common scenarios where the majority of object instances stand upright in the 3D space. However, 7DoF detection cannot precisely lo-cate objects when the objects lean in the 3D space, where the most compact bounding boxes can only be determined given full pose configurations, i.e., with the additional two angles of rotation. Pose predictions of full configurations are important in safety-critical scenarios, e.g., autonomous driving, where the most precise and compact localization of objects enables better perception and decision making.
This task of pose prediction of full configurations (i.e., 6D pose and size) is formally introduced in [30] as category-level 6D object pose and size estimation of novel instances from single, arbitrary views of RGB-D observa-It is closely related to category-level amodal 3D tions. object detection [22, 31, 36, 24, 35, 21] (i.e., the above 7DoF setting) and instance-level 6D object pose estimation
[12, 8, 14, 16, 32, 26, 20, 17, 29, 18]. Compared with them, the focused task in the present paper is more challenging due to learning and prediction in the full rotation space of
SO(3); more specifically, (1) the task is more involved in terms of both defining the category-level canonical poses (cf. Section 3 for definition of canonical poses) and align-ing object instances with large intra-category shape varia-tions [15, 4], (2) deep learning precise rotations arguably requires learning rotation-equivariant shape features, which is less studied compared with the 2D counterpart of learn-ing translation-invariant image features, and (3) compared with instance-level 6D pose estimation, due to the lack of testing CAD models, the focused task cannot leverage the privileged 3D shapes to directly refine pose predictions, as done in [2, 32, 29, 18].
In this work, we propose a novel method for category-level 6D object pose and size estimation, which can partially address the second and third challenges mentioned above.
Our method constructs two parallel pose decoders on top of a shared pose encoder; the two decoders predict poses with different working mechanisms, and the encoder is designed to learn pose-sensitive shape features. A refined learning that enforces the predicted pose consistency between the two decoders is activated during testing to further improve the prediction. We term our method as Dual Pose Network with refined learning of pose consistency, shortened as Du-alPoseNet. Fig. 1 gives an illustration.
For an observed RGB-D scene, DualPoseNet first em-ploys an off-the-shelf model of instance segmentation (e.g.,
MaskRCNN [11]) in images to segment out the objects of interest. It then feeds each masked RGB-D region into the encoder. To learn pose-sensitive shape features, we con-struct our encoder based on spherical convolutions [9, 7], which provably learn deep features of object surface shapes with the property of rotation equivariance on SO(3).
In this work, we design a novel module of Spherical Fusion to support a better embedding from the appearance and shape features of the input RGB-D region. With the learned pose-sensitive features, the two parallel decoders either make a pose prediction explicitly, or implicitly do so by reconstruct-ing the input (partial) point cloud in its canonical pose; while the first pose prediction can be directly used as the result of DualPoseNet, the result is further refined during testing by fine-tuning the encoder using a self-adaptive loss term that enforces the pose consistency. The use of im-plicit decoder in DualPoseNet has two benefits that poten-tially improve the pose prediction: (1) it provides an aux-iliary supervision on the training of pose encoder, and (2) it is the key to enable the refinement given no testing CAD models. We conduct thorough experiments on the bench-mark category-level object pose datasets of CAMERA25 and REAL275 [30], and also apply our DualPoseNet to the instance-level ones of YCB-Video [3] and LineMOD
[13]. Ablation studies confirm the efficacy of our novel de-signs. DualPoseNet outperforms existing methods in terms of more precise pose. Our technical contributions are sum-marized as follows:
• We propose a new method of Dual Pose Network for category-level 6D object pose and size estimation. Du-alPoseNet stacks two parallel pose decoders on top of a shared pose encoder, where the implicit one predicts poses with a working mechanism different from that of the explicit one; the two decoders thus impose comple-mentary supervision on training of the pose encoder.
• In spite of the lack of testing CAD models, the use of implicit decoder in DualPoseNet enables a refined pose prediction during testing, by enforcing the pre-dicted pose consistency between the two decoders us-ing a self-adaptive loss term. This further improves the results of DualPoseNet.
• We construct the encoder of DualPoseNet based on spherical convolutions to learn pose-sensitive shape features, and design a module of Spherical Fusion wherein, which is empirically shown to learn a bet-ter embedding from the appearance and shape features from the input RGB-D regions. 2.