Abstract
We present a novel semi-supervised semantic segmenta-tion method which jointly achieves two desiderata of seg-mentation model regularities: the label-space consistency property between image augmentations and the feature-space contrastive property among different pixels. We lever-age the pixel-level ℓ2 loss and the pixel contrastive loss for the two purposes respectively. To address the compu-tational efficiency issue and the false negative noise issue involved in the pixel contrastive loss, we further introduce and investigate several negative sampling techniques. Ex-tensive experiments demonstrate the state-of-the-art perfor-mance of our method (PC2Seg) with the DeepLab-v3+ ar-chitecture, in several challenging semi-supervised settings derived from the VOC, Cityscapes, and COCO datasets. 1.

Introduction
Modern deep learning based solutions [3,4,45] to seman-tic segmentation typically require large-scale pixel-wise an-notated datasets [11, 15, 32] (i.e., the high-data regime).
When the deep models are trained with limited labeled data (i.e., the low-data regime), however, their performance drops drastically due to over-fitting. The performance drop in the low-data regime and the high annotation cost associated with dense pixel labeling have motivated the community to study semi-supervised segmentation methods
[17,26,37,56,57]. In the semi-supervised setting, a model is trained with the help of an additional large-scale unlabeled dataset. A successful semi-supervised method is useful in practice – only a portion of the production data needs to be densely annotated to deliver a satisfactory model.
As shown in Fig. 1, our key insight is that there are two desired regularity properties which a semi-supervised seg-mentation model should possess. The first one is the consis-tency property in the label space. The output segmentation mask of the model should be invariant (or equivariant) to
*Work partly done during an internship at X, The Moonshot Factory.
Fig. 1: Two desired properties of a semi-supervised seg-mentation model and our corresponding techniques to si-multaneously achieve such properties: (1) Consistency in the label space – the predicted mask should be invariant to augmentations. We thus leverage the pixel-wise ℓ2 loss be-tween two views of the same unlabeled image. (2) Con-trastiveness in the feature space – the features should be able to distinguish similar pixel pairs from dissimilar pairs.
To this end, we introduce the pixel contrastive loss that pulls positive pixel pairs closer and pushes negative pairs away. the transformations of the input. When an input image goes through two different color augmentations, the segmenta-tion mask should stay the same, since the object semantics and locations are unchanged. The second property is the contrastive property in the feature space. By “contrastive,” we mean that the intermediate features of the model should have the discriminative power to group visually similar pix-els together while distinguishing them from visually dis-similar pixels. Such a contrastive property will enable the model to classify pixels into correct semantic categories.
State-of-the-art semi-supervised segmentation methods, in fact, benefit from using the aforementioned label-space consistency property on the unlabeled images, in the forms of invariance under data augmentations [17, 57], invari-ance under feature perturbations [37], consistency of dif-ferent network branches [26], and model self-consistency
[56]. However, the explicit application of the feature-space
contrastive property and the possibility of the joint label-consistent and feature-contrastive regularization have been largely under-explored.
Based on our insight, this paper explores how to lever-age and simultaneously enforce the consistency property in the label space and the contrastive property in the feature space, leading to a novel pixel contrastive-consistent semi-supervised segmentation (PC2Seg) method. For the label consistency property, we introduce a simple pixel-wise ℓ2 consistency loss between the outputs of weakly and strongly augmented views of the same image. For the feature con-trastive property, we extend the popular InfoNCE-based contrastive loss [5, 19] and make significant modifications for its use at the pixel level on an intermediate feature map.
More concretely, the pixel contrastive loss in semantic segmentation faces the unique technical challenges of high computational cost and harmful false negative examples, compared with the standard image-level contrastive learn-ing. The potentially high computation is due to the need of contrasting a large number of pixels. The harm of false neg-ative examples (i.e., when examples of the same class as the anchor are mistakenly chosen as the contrasting examples) has been noted in image-level contrastive learning [10, 23].
This problem becomes particularly severe in segmentation, where accurate per-pixel predictions are required compared with image classification. To overcome these issues, we develop simple-yet-effective negative sampling strategies.
For each positive pair of pixels, we sample only a moderate number of negative pixels from the mini-batch for efficiency and avoid false negative pixels by a simple cross-image and pseudo-label weighting heuristic.
Our contributions are three-fold: 1. We propose the first framework that leverages both pixel-consistency (in the label space) and pixel-contrastive (in the feature space) properties for semi-supervised seman-tic segmentation. We show that these two properties are complementary and their synergy is important. 2. We generalize the existing image-level contrastive learn-ing to pixel-level. To overcome the computational cost and false negative difficulties inherent in segmentation tasks, we propose a novel negative sampling technique and investigate its four variants. 3. We demonstrate state-of-the-art performance on multi-ple widely-used benchmarks. This is achieved relatively easily by leveraging the proposed framework together with standard loss functions and data augmentations, without sacrificing efficiency compared to other semi-supervised methods. 2.