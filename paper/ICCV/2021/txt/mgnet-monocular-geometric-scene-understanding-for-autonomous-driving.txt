Abstract
We introduce MGNet, a multi-task framework for monoc-ular geometric scene understanding. We define monocular geometric scene understanding as the combination of two known tasks: Panoptic segmentation and self-supervised monocular depth estimation. Panoptic segmentation cap-tures the full scene not only semantically, but also on an instance basis. Self-supervised monocular depth estimation uses geometric constraints derived from the camera mea-surement model in order to measure depth from monocular video sequences only. To the best of our knowledge, we are the first to propose the combination of these two tasks in one single model. Our model is designed with focus on low latency to provide fast inference in real-time on a sin-gle consumer-grade GPU. During deployment, our model produces dense 3D point clouds with instance aware se-mantic labels from single high-resolution camera images.
We evaluate our model on two popular autonomous driv-ing benchmarks, i.e., Cityscapes and KITTI, and show com-petitive performance among other real-time capable meth-ods. Source code is available at https://github. com/markusschoen/MGNet. 1.

Introduction
Scene understanding is an essential component for au-tonomous driving perception systems, since it provides necessary information for higher-level functions such as multi-object-tracking or behavior planning. Recent ad-vances in semantic segmentation [5, 42, 61, 67, 81, 85] and instance segmentation [22, 25, 39, 69, 70] based on deep neural networks show outstanding results, while fast mod-els [9,13,26,51,78] focus on optimizing the speed-accuracy trade-off for usage in latency-critical applications, e.g., au-tonomous driving. New tasks emerge regularly, bringing perception systems a step forward towards full scene under-standing, but also increasing task complexity.
One of these tasks is panoptic segmentation [32], a com-bination of semantic and instance segmentation. Semantic segmentation focuses on stuff classes, i.e., amorph regions (a) Input image (b) Panoptic segmentation (c) Monocular depth estimation (d) 3D point cloud
Figure 1: Example prediction of our model, (a) the input image which is fed to the network, (b) the panoptic pre-diction on top of the input image, (c) the monocular depth estimation and (d) the final 3D point cloud generated by the network in real-time. like sky or road, while instance segmentation focuses on thing classes, i.e., countable objects like cars, bicycles or pedestrians. Panoptic segmentation handles both stuff and thing classes, providing not only unique class label for each pixel in the image, but also instance IDs for countable ob-jects. Panoptic segmentation is an important step towards scene understanding in autonomous vehicles, since it pro-vides not only the object masks, but also interesting amorph regions like drivable road space or sidewalks. However, the vast majority [3, 7, 12, 31, 36, 38, 40, 46, 52–54, 58, 65, 66, 72, 73, 76] of panoptic segmentation methods focus on high quality rather than inference speed, making them unsuit-able for on-board integration into an autonomous vehicle perception system. Only few methods [9, 13, 26, 51] have been proposed in the low-latency regime. Additionally, the camera-based measurement in the two-dimensional image plane limits the capabilities of panoptic segmentation mod-els in autonomous vehicles. The 2D pixel location is not sufficient for higher-level systems such as behavior plan-ning to reason about the current environment. Rather, a 3D representation is required.
Monocular depth estimation [11] tackles this problem by predicting per-pixel depth estimates from single camera im-ages. Using this depth information, pixels from the image can be projected into 3D space. Camera-based depth esti-mation is an ill-posed problem due to the camera measure-ment model, making it a very challenging task to solve.
Furthermore, accurate depth annotations are hard to ac-quire. Stereo cameras can provide depth information, but require an accurate calibration between the two cameras.
Additionally, resulting depth images are noisy, inaccurate at farther distances, and have a lot of missing regions where the stereo matching failed. In contrast, lidar sensors pro-vide distance measurements with high accuracy. The mea-surements can be projected into the image plane and used as a label, but this again requires an accurate calibration and synchronization between the sensors. The generated depth images are much more accurate, but very sparse.
Therefore, recent methods follow a self-supervised train-ing strategy using either stereo images [14, 16], video se-quences [2, 19, 20, 45, 64, 75, 82–84, 86], or both [17, 37, 43] during training. The training objective is formulated as an image synthesis problem based on geometric constraints.
Combining multiple tasks can be resource demanding.
The naive approach of training a separate network for each task can reach hardware limitations quickly in low resource environments. Hence, multi-task learning [1] emerged to combine multiple tasks in a single network to reduce latency and memory requirements on the target hardware. While joint task training can potentially boost single-task perfor-mance, it brings its own difficulties, e.g., loss balancing and conflicting gradients.
In this work, we introduce the task of monocular geo-metric scene understanding, a combination of panoptic seg-mentation and self-supervised monocular depth estimation.
We propose a multi-task framework, which we call MGNet, to tackle this new task with focus on latency. MGNet combines the ideas of state-of-the-art methods Panoptic
Deeplab [7] and Monodepth2 [17] with a lightweight net-work architecture. The self-supervised monocular depth estimation formulation requires only video sequences dur-ing training, releasing us from the need of hard to acquire ground truth data. Hence, our model can be trained using data from a single camera. We propose an improved version of the Dense Geometrical Constrains Module (DGC), intro-duced in [75], using our panoptic prediction for scale-aware depth estimation. Similar to [3], we generate pseudo labels for video sequence frames, which reduces the number of annotated frames necessary for panoptic segmentation. The multi-task setting implicitly constrains the model to learn a unified representation for both tasks and reduces the overall latency. We use homoscedastic uncertainty weighting, in-troduced in [29], but adopt a novel weighting scheme, com-bining fixed and learnable task weights, to improve multi-task performance. During deployment, our model produces 3D point clouds with instance aware class labels using a single camera image as input. Figure 1 shows an exam-ple prediction of our model. We evaluate our method on
Cityscapes [8] and KITTI [15] and are able to outperform previous approaches in terms of latency, while maintain-ing competitive accuracy. Specifically, on Cityscapes, we achieve 55.7 PQ and 8.3 RMSE with 30 FPS on full reso-lution 1024 × 2048 pixel images. On KITTI, we achieve 3.761 RMSE with 82 FPS on 384 × 1280 pixel images. 2.