Abstract
Predicting accurate future trajectories of multiple agents is essential for autonomous systems but is challenging due to the complex interaction between agents and the uncer-tainty in each agent’s future behavior. Forecasting multi-agent trajectories requires modeling two key dimensions: (1) time dimension, where we model the inﬂuence of past agent states over future states; (2) social dimension, where we model how the state of each agent affects others. Most prior methods model these two dimensions separately, e.g.,
ﬁrst using a temporal model to summarize features over time for each agent independently and then modeling the interaction of the summarized features with a social model.
This approach is suboptimal since independent feature en-coding over either the time or social dimension can result in a loss of information. Instead, we would prefer a method that allows an agent’s state at one time to directly affect another agent’s state at a future time. To this end, we propose a new Transformer, termed AgentFormer, that si-multaneously models the time and social dimensions. The model leverages a sequence representation of multi-agent trajectories by ﬂattening trajectory features across time and agents. Since standard attention operations disregard the agent identity of each element in the sequence, AgentFormer uses a novel agent-aware attention mechanism that pre-serves agent identities by attending to elements of the same agent differently than elements of other agents. Based on
AgentFormer, we propose a stochastic multi-agent trajec-tory prediction model that can attend to features of any agent at any previous timestep when inferring an agent’s future position. The latent intent of all agents is also jointly modeled, allowing the stochasticity in one agent’s behavior to affect other agents. Extensive experiments show that our method signiﬁcantly improves the state of the art on well-established pedestrian and autonomous driving datasets. 1.

Introduction
The safe planning of autonomous systems such as self-driving vehicles requires forecasting accurate future trajec-Figure 1. Different from standard approaches that model multi-agent trajectories in the time and social dimensions separately, our
AgentFormer allows for joint modeling of the time and social di-mensions while preserving time and agent information. tories of surrounding agents (e.g., pedestrians, vehicles).
However, multi-agent trajectory forecasting is challenging since the social interaction between agents, i.e., behavioral inﬂuence of an agent on others, is a complex process. The problem is further complicated by the uncertainty of each agent’s future behavior, i.e., each agent has its latent intent unobserved by the system (e.g., turning left or right) that governs its future trajectory and in turn affects other agents.
Therefore, a good multi-agent trajectory forecasting method should effectively model (1) the complex social interaction between agents and (2) the latent intent of each agent’s fu-ture behavior and its social inﬂuence on other agents.
Multi-agent social interaction modeling involves two key dimensions as illustrated in Fig. 1 (Top): (1) time dimen-sion, where we model how past agent states (positions and velocities) inﬂuence future agent states; (2) social dimen-sion, where we model how each agent’s state affects the
state of other agents. Most prior multi-agent trajectory forecasting methods model these two dimensions separately (see Fig. 1 (Middle)). Approaches like [25, 1, 15] ﬁrst use temporal models (e.g., LSTMs [17] or Transformers [46]) to summarize trajectory features over time for each agent inde-pendently and then input the summarized temporal features to social models (e.g., graph neural networks [23]) to cap-ture social interaction between agents. Alternatively, meth-ods like [44, 18] ﬁrst use social models to produce social features for each agent at each independent timestep and then apply temporal models over the social features. In this work, we argue that modeling the time and social dimen-sions separately can be suboptimal since the independent feature encoding over either the time or social dimension is not informed by features across the other dimension, and the encoded features may not contain the necessary infor-mation for modeling the other dimension. termed AgentFormer,
To tackle this problem, we propose a new Transformer model, that simultaneously learns representations from both the time and social dimensions.
AgentFormer allows an agent’s state at one time to af-fect another agent’s state at a future time directly instead of through intermediate features encoded over one dimen-sion. As Transformers require sequences as input, we leverage a sequence representation of multi-agent trajecto-ries by ﬂattening trajectory features across time and agents (see Fig. 1 (Bottom)). However, directly applying standard
Transformers to these multi-agent sequences will result in a loss of time and agent information since standard atten-tion operations discard the timestep and agent identity as-sociated with each element in the sequence. We solve the loss of time information using a time encoder that appends a timestamp feature to each element. However, the loss of agent identity is a more complicated problem: unlike time, there is no innate ordering between agents, and assigning an agent index-based encoding will break the required permu-tation invariance of agents and create artiﬁcial dependencies on agent indices in the model. Instead, we propose a novel agent-aware attention mechanism to preserve agent infor-mation. Speciﬁcally, agent-aware attention generates two sets of keys and queries via different linear transformations; one set of keys and queries is used to compute inter-agent attention (agent to agent) while the other set is designated for intra-agent attention (agent to itself). This design al-lows agent-aware attention to attend to elements of the same agent differently than elements of other agents, thus keep-ing the notion of agent identity. Agent-aware attention can be implemented efﬁciently via masked operations. Further-more, AgentFormer can also encode rule-based connectiv-ity between agents (e.g., based on distance) by masking out the attention weights between unconnected agents.
Based on AgentFormer, which allows us to model social interaction effectively, we propose a multi-agent trajectory prediction framework that also models the social inﬂuence of each agent’s future trajectory on other agents. The prob-abilistic formulation of the model follows the conditional variational autoencoder (CVAE [21]) where we model the generative future trajectory distribution conditioned on con-text (e.g., past trajectories, semantic maps). We introduce a latent code for each agent to represent its latent intent. To model the social inﬂuence of each agent’s future behavior (governed by latent intent) on other agents, the latent codes of all agents are jointly inferred from the future trajectories of all agents during training, and they are also jointly used by a trajectory decoder to output socially-aware multi-agent future trajectories. Thanks to AgentFormer, the trajectory decoder can attend to features of any agent at any previ-ous timestep when inferring an agent’s future position. To improve the diversity of sampled trajectories and avoid sim-ilar samples caused by random sampling, we further adopt a multi-agent trajectory sampler that can generate diverse and plausible multi-agent trajectories by mapping context to various conﬁgurations of all agents’ latent codes.
We evaluate our method on well-established pedestrian datasets, ETH [37] and UCY [28], and an autonomous driv-ing dataset, nuScenes [3]. On ETH/UCY and nuScenes, we outperform state-of-the-art multi-agent prediction methods with substantial performance improvement (41% and 42%).
We further conduct extensive ablation studies to show the superiority of AgentFormer over various combinations of social and temporal models. We also demonstrate the efﬁ-cacy of agent-aware attention against agent encoding.
To summarize, the main contributions of this paper are: (1) We propose a new Transformer that simultaneously models the time and social dimensions of multi-agent tra-jectories with a sequence representation. (2) We propose a novel agent-aware attention mechanism that preserves the agent identity of each element in the multi-agent trajectory sequence. (3) We present a multi-agent forecasting frame-work that models the latent intent of all agents jointly to produce socially-plausible future trajectories. (4) Our ap-proach signiﬁcantly improves the state of the art on well-established pedestrian and autonomous driving datasets. 2.