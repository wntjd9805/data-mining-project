Abstract 3D perception of object shapes from RGB image in-put is fundamental towards semantic scene understand-ing, grounding image-based perception in our spatially 3-dimensional real-world environments. To achieve a map-ping between image views of objects and 3D shapes, we leverage CAD model priors from existing large-scale databases, and propose a novel approach towards con-structing a joint embedding space between 2D images and 3D CAD models in a patch-wise fashion – establishing cor-respondences between patches of an image view of an object and patches of CAD geometry. This enables part similarity reasoning for retrieving similar CADs to a new image view without exact matches in the database. Our patch embed-ding provides more robust CAD retrieval for shape estima-tion in our end-to-end estimation of CAD model shape and pose for detected objects in a single input image. Experi-ments on in-the-wild, complex imagery from ScanNet show that our approach is more robust than state of the art in real-world scenarios without any exact CAD matches. 1.

Introduction
Fundamental to many visual perception tasks is an un-derstanding of the decomposition of an observed scene into its constituent objects, and the semantic meaning of these objects – including class categorization, segmentation, and structural 3D understanding. In recent years, advances in 2D object recognition and localization have achieved im-pressive success in image-based understanding, even from only single image input [22, 18, 46, 21]. Such recognition and perception constrained to the image domain unfortu-nately remains limited towards understanding 3D attributes 1
such as shape and structure, which are not only fundamen-tal towards a comprehensive, human-like understanding of objects in a scene, but towards many applications such as autonomous exploration and interaction of an environment.
To address 3D perception from a single RGB image, we have recently seen several methods proposed taking a gen-erative approach towards reconstructing the observed ob-jects’ geometry [19, 42, 14]. These methods show promis-ing results in attaining 3D understanding of objects in com-plex scene imagery, but take a low-level approach towards geometric reconstruction, constructing voxel-by-voxel or vertex-by-vertex, often resulting in noisy or oversmoothed geometry, or geometry not representing a valid object in-stance (e.g., missing a leg on a chair). In contrast, several approaches have taken a CAD-based prior for representing the 3D structure of the objects seen in an RGB or RGB-D observation, by retrieving and aligning CAD models from a database similar to the observed objects [41, 33, 2, 30].
This CAD prior enables representation of each object with a clean, complete 3D mesh known to represent valid instances of objects and able to be stored compactly for potential downstream applications. Unfortunately, such a retrieval-based approach tends to struggle with generalization, in par-ticular when a new observed image of an object does not exactly match any CAD model in the dataset.
We observe that in these challenging scenarios, various part similarities can be leveraged to ﬁnd a similar shape.
Thus, we propose Patch2CAD, which constructs a joint em-bedding space between images and CAD models based on encoding mid-level geometric relations by establishing sim-ilarity of patches of images to patches of object geome-try. These correspondences can be aggregated into CAD prediction by majority voting. This enables CAD retrieval based on the predominant part similarities, enabling im-proved generalizability for CAD retrieval to reconstruct the shapes of objects seen in an image.
To achieve a 3D understanding of object structure from a single RGB image, we ﬁrst detect object locations in 2D, then construct our patch-based image-CAD embedding space enabling voting for retrieval of a similar CAD model, and predict the pose of that CAD in the image. Patch2CAD is trained end-to-end to comprehensively establish an effec-tive image-CAD embedding.
Our main contribution is a patch-based learning of a joint embedding space between the two very different domains of 2D images and 3D CAD models, which establishes more robust, part-level-based correspondences (see Figure 1). We demonstrate that this patch-wise embedding enables mean-ingful CAD retrievals for image observations not just in the top nearest neighbor, but for top-k retrieval. As a result, we achieve more effective association of CAD shapes to im-ages observations of objects with no exact CAD matches in a candidate database, as is typically the case for real-world scenarios. We demonstrate Patch2CAD’s effective 3D shape perception on both ScanNet [11] and Pix3D [52] datasets. In particular, on the complex, in-the-wild images from ScanNet [11], Patch2CAD exhibits notable advantage to its patch-based approach, outperforming state of the art by 1.9 Mesh AP (22% relative improvement). 2.