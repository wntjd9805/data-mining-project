Abstract
Human-Object Interaction (HOI) detection is a funda-mental visual task aiming at localizing and recognizing in-teractions between humans and objects. Existing works fo-cus on the visual and linguistic features of the humans and objects. However, they do not capitalise on the high-level and semantic relationships present in the image, which pro-vides crucial contextual and detailed relational knowledge for HOI inference. We propose a novel method to exploit this information, through the scene graph, for the Human-Object Interaction (SG2HOI) detection task. Our method,
SG2HOI, incorporates the SG information in two ways: (1) we embed a scene graph into a global context clue, serving as the scene-specific environmental context; and (2) we build a relation-aware message-passing module to gather relationships from objects’ neighborhood and trans-fer them into interactions. Empirical evaluation shows that our SG2HOI method outperforms the state-of-the-art meth-ods on two benchmark HOI datasets: V-COCO and HICO-DET. Code will be available at https://github.com/ ht014/SG2HOI. 1.

Introduction
Recently, Human-Object Interaction (HOI) detection [5, 14] aims at detecting the types of interactions of human-object pairs. It has gained increasing attention in the com-puter vision community as it has a wide range of practi-cal applications, e.g., action recognition [11] and Human-Computer Interaction (CHI) [8]. Formally, the goal of HOI is to detect and localize all the interaction triples in an im-age, i.e. <human, interaction, object>. HOI is a challeng-ing problem—an image typically contains multiple humans and objects in a complex scene, while the majority of all the human-object pairs are non-relation. Therefore, some works [25, 28, 27] that are solely based on visual features cannot learn good discriminative patterns for HOIs.
∗Corresponding author.
Figure 1. An illustration of our Scene Graph to Human-Object In-teraction method, where we consider the scene graph between ob-jects as external knowledge to facilitate the prediction of HOIs in two ways: scene graph embedding (green arrow) described in
Sec. 3.1 and relation-aware message passing (red arrow) described in Sec. 3.2.
It is intuitive to turn to external knowledge, e.g., the well-known knowledge graph ConceptNet [23], to miti-gate the limitation of visual appearance features. How-ever, as such knowledge graphs are often general-purpose, most of retrieved results are redundant. Thus, such external knowledge may not provide sufficiently informative cues for HOIs. Instead, we turn to another closely-related task, i.e. Scene Graph Generation (SGG), to generate a tiny rela-tion (knowledge) graph for each image, serving as the ex-ternal knowledge to make up the visual cues.
SGG [17, 18, 29] and HOI [5, 28] both aim at identify-ing spatial and other types of relations between objects in an image. Figure 1 illustrates a scene graph and the HOI graph of the same image. There are two main differences between the two tasks: (1) in SGG, subjects can be of any type (humans, cars, etc.), while in HOI they are fixed as hu-mans, which results in more edges in the SG; and (2) the predicates of HOI only consist of interaction verbs, while in SGG many types of relations may exist, including loca-tive prepositions (e.g., on) and semantic actions (e.g., play with). Simply speaking, an SG is a general relation graph whereas an HOI graph is a human-focused subject graph, which can be considered a subgraph extracted from the SG.
Thus, we believe that the scene graph can provide more de-tailed cues for HOI detection and improve the performance of an HOI model. Specifically, incorporating the SG in the
HOI task has two benefits: (1) the SG puts each object and human in a relation graph, which can provide contextual cues and benefit the scene understanding; and (2) informa-tion contained in relationships in the SG can be explicitly or implicitly transferred to more accurately identify the cor-rect interactions. Therefore, our motivation is to develop a decoding method that transfers the knowledge encoded in the scene graph to the HOI graph. To this end, we decode an SG from two aspects: the global scene-level and regional relation-level.
For the scene-level, our goal is to extract the scene-specific cues from the detailed scene graph, because many previous works [16, 25, 27] have demonstrated that the in-teractions are scene-biased, that is, some interactions are highly correlated to specific scenes. Taking Figure 1 as an example, the image is a restaurant scene from the visual ap-pearance, and it is more likely that the interaction is about eating. Therefore, accurate scene cues can benefit HOI recognition. Many existing works [16, 25] have employed the global visual appearance as the scene clues. However, due to the coarseness of the appearance feature, their perfor-mance does not gain a significant improvement. Instead, we treat a scene graph as the scene-specific contextual cue and propose two components to embed it: scene graph layout encoding and attention-based relation fusion.
For the relation-level, we observe that the relations in the scene graph can explicitly or implicitly transfer to interac-tions. For instance, as shown in Figure 1, since the three relation triples: <hand, of, man 1>, <hand, hold, fork>, and <man 1, near, table> simultaneously take place, we are more likely to infer that man 1’s interaction is eat at.
Besides, based on the relation <man 2, near, man 1 >, we could hypothesise that both of them probably have the sim-ilar interaction. Furthermore, based on his visual features, we could infer that the man is also eating at the table. Thus, the knowledge of the exact relations between object pairs makes the inference more certain. To this end, we develop a relation-aware message passing module to reason on the
SG by gathering relation information from inter- and intra-class neighbors and refine their features.
In summary, our contribution is three-fold:
• We propose a novel Scene Graph to Human-Object In-teraction (SG2HOI) detection network to bridge the gap between the two tasks. To the best of our knowl-edge, we are the first to utilize scene graphs for HOI detection.
• We design two components to decode the SG: scene graph embedding and relation-aware message-passing, to learn the environmental context and transfer SG re-lations to HOI interactions, respectively.
• We evaluate our approach on two popular HOI detec-tion benchmark datasets: V-COCO and HICO-DET, in terms of a wide range of evaluation metrics. Our eval-uation shows that SG2HOI method outperforms state-of-the-art models on both two datasets. 2.