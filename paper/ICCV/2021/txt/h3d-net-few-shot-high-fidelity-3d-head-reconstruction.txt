Abstract
Recent learning approaches that implicitly represent sur-face geometry using coordinate-based neural representa-tions have shown impressive results in the problem of multi-view 3D reconstruction. The effectiveness of these tech-niques is, however, subject to the availability of a large number (several tens) of input views of the scene, and com-In this paper, we putationally demanding optimizations. tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that en-ables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modelling the geometry and another estimat-ing the surface radiance, using implicit differentiable ren-dering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By do-ing this, we achieve high-fidelity head reconstructions, in-cluding hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Mor-phable Models methods in the few-shot scenario, and non-parametric methods when large sets of views are available. 1.

Introduction
Recent learning based methods have shown impressive results in reconstructing 3D shapes from 2D images. These approaches can be roughly split into two main categories: model-based [3, 11, 25, 35, 36, 37, 44, 45, 49, 52] and model-free [9, 10, 16, 18, 21, 31, 33, 34, 39, 50, 51, 53]. The former incorporate prior knowledge using 3D Morphable
Models (3DMMs) to limit the space of feasible solutions, making these approaches well suited for few-shot and one-shot shape estimation. However, most model-based meth-ods produce shapes that usually lack geometric detail and cannot handle arbitrary topology changes.
On the other hand, model-free approaches based on dis-crete representations like voxels, meshes or point-clouds, have the flexibility to represent a wider spectrum of shapes, although at the cost of being computationally tractable only for small resolutions or being restricted to fixed topolo-gies. These limitations have been overcome by neural im-plicit representations [8, 23, 24, 27, 32, 38, 41, 56], which can represent both geometry and appearance as a contin-uum, encoded in the weights of a neural network. [26, 54] have shown the success of such representations in learning detail-rich 3D geometry directly from images, with no 3D ground truth supervision. Unfortunately, the performance of these methods is currently conditioned to the availabil-ity of a large number of input views, which leads to a time consuming inference.
In this work we introduce H3D-Net, a hybrid scheme that combines the strengths of model-based and model-free representations by incorporating prior knowledge into neu-ral implicit models for category-specific multi-view recon-struction. We apply this approach to the problem of few-shot full head reconstruction.
In order to build the prior, we first use several thousands of raw incomplete scans to learn a space of Signed Distance Functions (SDF) repre-senting 3D head shapes [27]. At inference, this learnt shape prior is used to initalize and guide the optimization of an
Implicit Differentiable Renderer (IDR) [54] that, given a po-tentially reduced number of input images, estimates the full head geometry. The use of the learned prior enables faster convergence during optimization and prevents it from being trapped into local minima, yielding 3D shape estimates that capture fine details of the face, head and hair from just three input images (see Figure 1).
We exhaustively evaluate our approach on a mid-resolution Multiview-Stereo (MVS) public dataset [29] and on a high-resolution dataset we collected with a structured-light scanner, consisting of 10 3D full-head scans. The results show that we consistently outperform current state-of-the-art, both in a few-shot setting and when many input views are available. Importantly, the use of the prior also makes our approach very efficient, achieving competitive results in terms of accuracy about 20Ã— faster than IDR [54].
Our key contributions can be summarized as follows:
- We introduce a method for reconstructing high quality full heads in 3D from small sets of in-the-wild images.
- Our method is the first to use implicit functions for reconstructing 3D humans heads from multiple images and also to rival parametric and non-parametric models in 3D accuracy at the same time.
- We devise a guided optimization approach to introduce a probabilistic shape prior into neural implicit models.
- We collect and release a new dataset1 containing high-resolution 3D full head scans, images, masks and camera poses for evaluation purposes, which we dub
H3DS. 2.