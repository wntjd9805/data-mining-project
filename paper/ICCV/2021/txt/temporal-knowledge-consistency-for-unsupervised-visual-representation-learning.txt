Abstract
The instance discrimination paradigm has become dom-inant in unsupervised learning. It always adopts a teacher-student framework, in which the teacher provides embed-ded knowledge as a supervision signal for the student. The student learns meaningful representations by enforcing in-stance spatial consistency with the views from the teacher.
However, the outputs of the teacher can vary dramatically on the same instance during different training stages, in-troducing unexpected noise and leading to catastrophic for-getting caused by inconsistent objectives. In this paper, we first integrate instance temporal consistency into current instance discrimination paradigms, and propose a novel and strong algorithm named Temporal Knowledge Consis-tency (TKC). Specifically, our TKC dynamically ensembles the knowledge of temporal teachers and adaptively selects useful information according to its importance to learning instance temporal consistency. Experimental result shows that TKC can learn better visual representations on both
ResNet and AlexNet on linear evaluation protocol while transfer well to downstream tasks. All experiments suggest the good effectiveness and generalization of our method.
Code will be made available.
Figure 1. Mainstream unsupervised methods adopt the teacher-student framework, where the teacher is an EMA ensemble of pre-vious student encoders. This figure illustrates the proportion of previous students in the teacher with respect to training steps. The red curve shows that the EMA teacher ensembles the previous en-coders by a predesigned factor α, where only alomst encoders in the very close steps are ensembled. Our TKC (the green curve) reuses the early models and adaptively learns the importance ω for each of them, thus leads to temporal consistent representations. 1.

Introduction
The rise of Deep Convolutional Neural Networks (DCNN) [23, 28, 46] has led to significant success in com-puter vision benchmarks [8, 13, 33]. The excellent perfor-mance of supervised DCNN always relies on a large quan-tity of manually labeled data, which is costly to collect
[18, 51]. Unsupervised representation learning has been attracted more and more interest, for it can learn a good
*Equal Contribution.
†Corresponding author.
This paper is supported by the National Key R&D Plan of the Ministry of Science and Technology (Project No.2020AAA0104400). representation without human annotations. These methods are generally to manually design a pretext task to learn rep-resentations, such as image in-painting [42], colorization
[9, 60, 30, 31], rotate predicting [18, 6, 14] and cluster-ing [2, 62, 4]. All these pretext tasks are based on spe-cific domain knowledge, which has poor generation on var-ious downstream tasks. Recently, instance discrimination
[51, 21, 5, 19, 36] paradigm has led to remarkable progress in unsupervised representation learning and even surpasses the supervised pre-training on extensive downstream tasks
[36, 21].
The instance discrimination paradigm treats each sam-ple itself as its own category and trains the CNN to sep-arate all the different samples from each other. The cur-rent paradigm can be formulated as a teacher-student frame-work enforcing the instance spatial consistency of two net-works, which are the student network and the EMA teacher network [7, 19, 5]. The instance spatial consistency con-strains the similarity of different spatial views from the same instance, and its ultimate goal is to learn instance-discriminative and spatial-invariant representations. One of the key points in these instance discrimination works is the EMA teacher. For instance, MoCo [21] uses the EMA teacher to output consistent negative samples for the stu-dent; BYOL [19] trains a student to mimic the representa-tions from the EMA teacher; SimCLR [5] maintains a real-time EMA teacher of the student. 1:
However, we argue that the current EMA teacher is sub-optimal as illustrated in Fig. (1) the EMA teacher only ensembles the rare knowledge of recent encoders by a handcraft proportion, which means that it only concentrates on instance spatial consistency while the instance tempo-ral consistency is ignored. As a consequence, the outputs of the same sample can vary dramatically among different training stages, which can introduce unexpected noise and finally lead to catastrophic forgetting [34, 61]. (2) The EMA manner can’t leverage the importance of different encoders.
It assumes that the outputs of later models are largely more important than the earlier ones, despite that the benefits of previous epochs have been observed in previous works
[29, 61].
Specifically,
In this paper, we integrate instance temporal consistency into the instance discrimination paradigm and propose a novel and strong algorithm, namely Temporal Knowledge
Consistency(TKC), which contains the temporal teacher and the knowledge transformer. temporal teacher supplies instance temporal consistency via intro-ducing the temporal knowledge from previous models. And the knowledge transformer dynamically learns the impor-tance of different temporal teachers, then adaptively ensem-bles the useful information according to their importance, to generate instance temporal consistency objective. In ad-dition, we provide a computation-economical implementa-tion, which can provide temporal knowledge without pre-serving multiple previous models.
Our experimental results on different tasks and bench-marks have demonstrated that TKC can learn a better vi-sual representation with excellent transferability and scala-bility. Concretely, we achieve state-of-the-art performance on ResNet and AlexNet backbones on linear evaluation pro-tocol. Moreover, we evaluate representations learned by
TKC on many downstream tasks and architectures. All re-sults suggest the effectiveness of TKC. Overall, the main contributions in this work include:
• We are the first to integrate instance temporal consis-tency into the current EMA teacher in the instance dis-crimination paradigm.
• We propose a novel and strong algorithm, named Tem-poral Knowledge Consistency (TKC), which can dy-namically ensemble the knowledge from different tem-poral teachers.
• Extensive experiments are conducted on several benchmarks and architectures, which shows the supe-rior performance on mainstream benchmarks and the scalability of TKC. 2.