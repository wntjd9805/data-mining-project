Abstract
We present a graph-convolution-reinforced transformer, named Mesh Graphormer, for 3D human pose and mesh reconstruction from a single image. Recently both trans-formers and graph convolutional neural networks (GC-NNs) have shown promising progress in human mesh re-construction. Transformer-based approaches are effective in modeling non-local interactions among 3D mesh ver-tices and body joints, whereas GCNNs are good at ex-ploiting neighborhood vertex interactions based on a pre-specified mesh topology.
In this paper, we study how to combine graph convolutions and self-attentions in a trans-former to model both local and global interactions. Ex-perimental results show that our proposed method, Mesh
Graphormer, significantly outperforms the previous state-of-the-art methods on multiple benchmarks, including Hu-man3.6M, 3DPW, and FreiHAND datasets. Code and pre-trained models are available at https://github. com/microsoft/MeshGraphormer. 1.

Introduction 3D human pose and mesh reconstruction from a single image is a popular research topic as it offers a wide range of applications for human-computer interactions. However, due to the complex body articulation, it is a challenging task.
Recently, Transformers and Graph Convolutional Neu-ral Networks (GCNNs) have shown promising advances in human mesh reconstruction. For example, recent stud-ies [20, 5] have suggested using GCNNs to directly regress 3D positions of mesh vertices by taking into account local interactions among neighbor vertices. In a recent study [22], a transformer encoder was used with self-attention to cap-ture global interactions among body joints and mesh ver-tices, which lead to further improvement.
However, as discussed in the literature [41, 40, 8, 41, 3],
Transformers and Convolution Neural Networks (CNN) each have their own limitations. Transformers are good at modeling long-range dependencies on the input tokens, but
Figure 1: Summary. We study how to combine self-attentions and graph convolutions in a transformer for hu-man mesh reconstruction. The proposed Graphormer out-performs existing graph convolution networks [5, 20] and transformer-based method [22] by a clear margin. The num-bers are the reconstruction error (PA-MPJPE) in the unit of millimeter. The lower the better. they are less efficient at capturing fine-grained local infor-mation. Convolution layers, on the other hand, are useful for extracting local features, but many layers are required to capture global context. In natural language processing and speech recognition, Conformer [8] is a recently pro-posed technique that leverages the complementarity of self-attention and convolutions to learn representations. This motivates us to combine self-attention and graph convolu-tion for the 3D reconstruction of human mesh (Figure 1).
We present a graph-convolution-reinforced transformer called Mesh Graphformer for reconstructing human pose and mesh from a single image. We inject graph convolu-tions into transformer blocks to improve the local interac-tions among neighboring vertices and joints.
In order to leverage the power of graph convolutions, Graphormer is free to attend to all image grid features that contain more detailed local information and are helpful in refining the 3D coordinate prediction. Consequently, Graphormer and im-age grid features are mutually reinforced to achieve better performance in human pose and mesh reconstruction.
Extensive experiments show that the proposed Mesh
Graphormer models both local and global interactions ef-fectively and clearly outperforms previous state-of-the-art
Instead of regressing the parametric coefficients, non-parametric approaches [5, 26, 20] regress vertices directly from an image. Among the previous studies, the Graph
Convolutional Neural Network (GCNN) [17, 5, 20] is one of the most popular options as it is able to model the local interactions between neighboring vertices based on a given adjacency matrix [32, 5, 20, 38]. However, it is less effi-cient to capture global interactions between the vertices and body joints. To overcome this limitation, transformer-based methods [22] use a self-attention mechanism to freely at-tend vertices and body joints in the mesh and thereby en-code non-local relationships of a human mesh. However, it is less convenient to model local interactions than GNN-based methods [5, 20].
Among the works mentioned above, METRO [22] is the most relevant study to our proposed method. The main difference between METRO and our proposed model is that we are designing a graph-convolution-reinforced trans-former encoder for reconstructing human mesh. In addition, we add image grid features as input tokens to the trans-former and allow the joints and mesh vertices to attend to grid features.
Transformer architecture is developing rapidly for differ-ent applications [6, 35, 33, 15, 10]. One important direction is to improve the expressiveness of a transformer network for better context modeling. Recent studies [40, 8] show that the combination of convolution and self-attention in a transformer encoder is helpful to improve representation learning. However, previous work has mainly focused on language modeling and speech recognition. When it comes to complex data structures such as 3D human mesh, this re-mains an open problem.
To address these challenges, we investigate how to inject graph convolutions into the transformer encoder blocks to better model both local and global interactions among 3D mesh vertices and body joints. 3. Graphormer Encoder
Figure 2 shows the architecture of our proposed
Graphormer encoder. It consists of a stack of N = 4 iden-tical blocks. Each block consists of five sub-modules, in-cluding a Layer Norm, a Multi-Head Self-Attention mod-ule, a Graph Residual Block, a second Layer Norm, and a
Multi-Layer Perceptron (MLP) at the end. Our Graphormer encoder has a similar architecture to the traditional trans-former encoder [35], but we introduce graph convolution into the network to model fine-grained local interactions.
In the following sections, we describe Multi-Head Self-Attention (MHSA) module and Graph Residual Block.
Figure 2: Architecture of a Graphormer Encoder. We pro-pose a graph-convolution-reinforced transformer encoder to capture both global and local interactions for 3D human mesh reconstruction. The encoder consists of a stack of
N = 4 identical blocks. methods for reconstructing human mesh in several datasets.
Additionally, we offer ablation studies on various model design options to incorporate graph convolutions and self-attention into a transformer encoder.
The main contributions of this paper include
• We present a graph-convolution-reinforced trans-former called Mesh Graphormer to model both local and global interactions for the 3D reconstruction of hu-man pose and mesh.
• Mesh Graphormer allows joints and mesh vertices to freely attend to image grid features to refine the 3D coordinate prediction.
• Mesh Graphormer outperforms previous state-of-the-art methods on Human3.6M, 3DPW, and FreiHAND datasets. 2.