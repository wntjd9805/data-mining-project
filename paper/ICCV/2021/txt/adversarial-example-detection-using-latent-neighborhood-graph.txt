Abstract 1.

Introduction
Detection of adversarial examples with high accuracy is critical for the security of deployed deep neural network-based models. We present the ﬁrst graph-based adversar-ial detection method that constructs a Latent Neighborhood
Graph (LNG) around an input example to determine if the input example is adversarial. Given an input example, se-lected reference adversarial and benign examples (repre-sented as LNG nodes in Figure 1) are used to capture the local manifold in the vicinity of the input example. The LNG node connectivity parameters are optimized jointly with the parameters of a graph attention network in an end-to-end manner to determine the optimal graph topology for ad-versarial example detection. The graph attention network is used to determine if the LNG is derived from an adver-sarial or benign input example. Experimental evaluations on CIFAR-10, STL-10, and ImageNet datasets, using six adversarial attack methods, demonstrate that the proposed method outperforms state-of-the-art adversarial detection methods in white-box and gray-box settings. The proposed method is able to successfully detect adversarial examples crafted with small perturbations using unseen attacks.
Deep learning techniques are being widely used in var-ious domains including computer vision [6, 13, 10, 24], natural language processing [12, 43], and speech recogni-tion [16, 37]. However, an extensive line of research has shown that an attacker can manipulate the prediction of a deep learning-based classiﬁcation system by adding a small perturbation to deep learning model inputs, intermediate embeddings [19, 11, 8], or by inducing distribution shifts
[26, 41]. These results highlight a major security issue for deep neural network-based prediction systems, especially the ones deployed in critical applications such as access control and user authentication [49].
To address this security concern, a variety of defense mechanisms have been proposed. These defense mecha-nisms can be broadly categorized into two categories. The proactive approaches, e.g. adversarial training [7, 14] and robustness-driven regularization [42], explicitly considers the presence of known adversarial attack methods to train a model, which increases model robustness to adversarial perturbation. However, in order to use this approach, ex-isting models need to be re-trained, which can be costly.
In contrast, the reactive approach requires no re-training of    
existing models; instead, it builds a detector to ﬁlter adver-sarial examples in the test environment, and thus becomes a viable solution for already deployed systems. In addition, detection-based defense mechanisms can also help to iden-tify security-compromised input sources.
A key ﬁnding of recent state-of-the-art detection meth-ods [47, 38] is that there is a signiﬁcant correlation between the legitimacy of an input example and its neighborhood in-formation in the learned embedding space. For instance, the
Deep k-Nearest Neighbors (DkNN) [47] detector computes the embedding of nearest neighbors of the input example at each layer of the network, and subsequently uses both the embedding and the class labels of the nearest neigh-bors to determine if the input is adversarial.
Inspired by this insight, we propose a method to leverage dynamically constructed neighborhood graphs for detecting adversarial examples. We introduce Latent Neighborhood Graph – a general structure encoding not only the neighbors of the in-put, but also the relation between them – to represent the neighborhood of the input. Compared to DkNN, the bene-ﬁts of our solution are three folds: (i) LNG covers multi-hop neighbors which characterizes the local manifolds of the in-put example, while DkNN only describes the manifold of the input example, (ii) LNG aggregates neighborhood in-formation adaptively based on the connectivity learned on the embedding space which encodes much richer informa-tion than the class labels employed in DkNN, (iii) LNG in-corporates both adversarial and benign neighbors in detec-tion while DkNN only utilizes benign neighbors due to the measurement of consistency of the neighborhood labels at each layer of the network. In addition to information en-coding, existing detectors are also limited by the computa-tion cost. PeerNet [21], a graph-based convolutional net-work claimed to be robust to adversarial attacks, relies on pixel-wise neighborhood retrieval based on the intermedi-ate 2D feature maps of a deep neural network, which in-creases the computation burden at test time. To overcome the aforementioned limitations, our approach purely relies on the embeddings at the ﬁnal hidden layer of a deep neural network. We show that a combination of graph attention net and our novel LNG representation sufﬁce to achieve state-of-the-art adversarial example detection performance.
In the proposed method, input example is used as a cen-tral node to construct a latent graph connected with samples curated from a reference dataset (see Figure 1). The graph describes the local manifold patterns for both the input ex-ample and its immediate benign and adversarial neighbors for adversarial detection. Both the nodes and the linkage of the graph are estimated on-the-ﬂy, and we train the graph constructor and discriminator in an end-to-end manner. Ex-perimental evaluations on three benchmark datasets show that the proposed approach yields state-of-the-art adversar-ial example detection performance against various known and unknown adversarial attacks, while maintaining high performance (more than 80%) against best-efforts white-box attack conﬁguration.
The contributions of this work are as follows:
• We present the ﬁrst work that poses adversarial example detection as a graph classiﬁcation problem. Our method efﬁciently constructs a latent neighborhood graph using reference examples for adversarial example detection.
• The proposed method estimates the latent neighborhood graph’s adjacency matrix on-the-ﬂy based on the dis-tances of neighborhood examples, and adaptively aggre-gates the information from both benign and adversarial neighbors for adversarial example detection.
• State-of-the-art gray-box and white-box detection perfor-mance on adversarial examples generated using known and unseen adversarial example generation methods. 2.