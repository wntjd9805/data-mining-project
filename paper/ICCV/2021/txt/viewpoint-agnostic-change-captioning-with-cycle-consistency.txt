Abstract
Change captioning is the task of identifying the change and describing it with a concise caption. Despite recent advancements, ﬁltering out insigniﬁcant changes still re-mains as a challenge. Namely, images from different cam-era perspectives can cause issues; a mere change in view-point should be disregarded while still capturing the actual changes. In order to tackle this problem, we present a new
Viewpoint-Agnostic change captioning network with Cycle
Consistency (VACC) that requires only one image each for the before and after scene, without depending on any other information. We achieve this by devising a new difference encoder module which can encode viewpoint information and model the difference more effectively. In addition, we propose a cycle consistency module that can potentially im-prove the performance of any change captioning networks in general by matching the composite feature of the gen-erated caption and before image with the after image fea-ture. We evaluate the performance of our proposed model across three datasets for change captioning, including a novel dataset we introduce here that contains images with changes under extreme viewpoint shifts. Through our ex-periments, we show the excellence of our method with re-spect to the CIDEr, BLEU-4, METEOR and SPICE scores.
Moreover, we demonstrate that attaching our proposed cy-cle consistency module yields a performance boost for ex-isting change captioning networks, even with varying image encoding mechanisms. 1.

Introduction
With an endless stream of data in real world, it is piv-otal to develop automated systems that assist human to quickly grasp the essence of the data. Consider, for ex-ample, data streams from a myriad of surveillance cameras scattered around highways. It is highly labor-intensive and almost implausible to monitor them all without automation
†Corresponding author.
Figure 1. An example of viewpoint-agnostic change captioning.
Compared to the top image, both images on the bottom are ac-quired from severely different viewpoints. Only the bottom left image contains an actual change, where the color of the brown box changes. Therefore, a meaningful caption should be gener-ated only for this image (“the brown box turned yellow”), while the caption for the bottom right should indicate there is no change. due to the sheer amount of data and the ﬂashing rate of change. Change detection has received much attention as one solution to this problem, along with other usages in various ﬁelds such as medical imaging and satellite imag-ing [21, 36]. Recent advancements even allow generating a short descriptive sentence that summarizes the detected changes, often referred to as change captioning [10, 19, 28].
Despite such improvements in change detection and cap-tioning, one of the most challenging aspects remains un-solved: identifying only the relevant semantic changes [26].
As shown in Figure 1, a picture of the same scene from an-other perspective is the epitome of an irrelevant change. Re-turning to the previous highway monitoring example, this can be of grave importance when aggregating data acquired
from multiple cameras, as their viewpoints all differ. Col-lecting information from various data sources can easily happen in everyday life, especially with the prevalence of smartphones these days. Hence, the caption should only indicate what really changed while ignoring the viewpoint shift.
Although the change in perspective has been addressed in previous change captioning works, they utilize datasets with relatively small viewpoint changes [19, 28] or make use of other information [24, 25] such as depth images, point clouds, and/or ground truth camera position to com-pensate for greater disparities in perspectives. On the other hand, our goal is to perform change captioning using only a pair of images in any viewpoints with no additional infor-mation.
To this end, we propose a new model that can pinpoint the semantic changes in the scene even under extreme view-point shifts. We further improve the captioning quality by devising a cycle consistency module that builds a compos-ite feature of the generated caption and the before image to match it with the encoded after image. Using the CLEVR engine [11], we build a synthetic dataset that simulates ex-treme viewpoint shifts to gauge the robustness of networks to perspective changes and the ability to isolate only the rel-evant differences. Finally, our contributions in this work can be summarized as follows: 1. We propose a new network for change captioning that is robust to viewpoint changes. Speciﬁcally, we tackle the problem of change captioning between pictures with extreme viewpoint shifts without relying on any extra data (i.e., using only one before image and one after image). To the best of our knowledge, our work is the ﬁrst attempt to solve this problem under such limited conditions. 2. The technical novelties of our new network for change captioning are two-fold. First, we devise a new differ-ence encoder that captures the change from a pair of images while being robust to the viewpoint difference.
Second, we present a cycle consistency module that as-sesses the quality of the resultant caption by creating a composite feature of the caption and before image fea-ture and matching it with the after image feature. The module is generalizable and can be attached to other models to improve their performance. 3. We introduce CLEVR-DC created using the CLEVR engine [11] as a novel dataset for change caption-ing with extreme viewpoint shifts. We perform ex-periments on CLEVR-DC and two existing datasets,
CLEVR-Change [19] and Spot-the-Diff [10], on all of which our method mostly outperforms multiple state-of-the-art methods. 2.