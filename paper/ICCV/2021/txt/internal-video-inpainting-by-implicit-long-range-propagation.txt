Abstract
We propose a novel framework for video inpainting by adopting an internal learning strategy. Unlike previous methods that use optical ﬂow for cross-frame context prop-agation to inpaint unknown regions, we show that this can be achieved implicitly by ﬁtting a convolutional neural net-work to known regions. Moreover, to handle challenging se-quences with ambiguous backgrounds or long-term occlu-sion, we design two regularization terms to preserve high-frequency details and long-term temporal consistency. Ex-tensive experiments on the DAVIS dataset demonstrate that the proposed method achieves state-of-the-art inpainting
∗Equal contribution quality quantitatively and qualitatively. We further extend the proposed method to another challenging task: learn-ing to remove an object from a video giving a single object mask in only one frame in a 4K video. Our source code is available at https://tengfei-wang.github.io/
Implicit-Internal-Video-Inpainting/. 1.

Introduction
Video inpainting is the problem of ﬁlling in missing re-gions in a video sequence with both spatial and temporal consistency. Video inpainting is beneﬁcial for video edit-ing, such as removing watermarks and unwanted objects.
With the explosion of multimedia content in daily life, there
are growing needs for inpainting sequences from multiple domains and real-world high-resolution videos. It is also expected to alleviate the human workload of labor-intensive mask labeling for semi-automatic object removal.
The video inpainting task is still unsolved yet because the existing approaches cannot consistently produce visu-ally pleasing inpainted videos with long-range consistency.
Most traditional methods [25, 13, 12, 37] adopt patch-based optimization strategies. These methods have limited ability to capture complex motion or synthesize new content. Re-cent ﬂow-guided methods [11, 39] propagate context infor-mation with the optical ﬂow to achieve temporally consis-tent results. However, obtaining accurate optical ﬂow in the missing region is non-trivial, especially when there is con-stantly blocked regions or the motion is complicated. Re-cent deep models [35, 15, 18, 27, 6, 20, 42] trained on large video datasets achieve more promising performance. How-ever, the dataset collection process is time-consuming and labor-intensive, and these methods may suffer from perfor-mance drop when test videos are in different domains from training videos. Most recently, Zhang et al. [43] propose an internal learning approach to video inpainting, which avoids the domain gap problem as the model training is completed on the test video. While internal video inpainting is a promising direction, their approach sometimes gener-ates incorrect or inconsistent results as this approach still depends on the externally-trained optical ﬂow estimation to propagate context information. Therefore, we propose a new internal learning method for video inpainting that can overcome the aforementioned issues with implicit long-range propagation, as shown in Fig. 1.
Instead of propagating information across frames via ex-plicit correspondences like optical ﬂow, we show that the in-formation propagation process can be implicitly addressed by the intrinsic properties of natural videos and convolu-tional neural networks. We will analyze these properties in detail and focus on handling two special challenging cases by imposing regularization. In the end, we manage to restore the missing region with cross-frame correlation and ensure temporal consistency by enforcing gradient con-straints. We train a convolutional neural network on the test video so that the trained model can propagate the informa-tion on known pixels (not in masks) to the whole video.
We ﬁrst evaluate the proposed method on the DAVIS dataset. The proposed approach achieves state-of-the-art performance both quantitatively and qualitatively, and the results of the user study indicate that our method is mostly preferred. We also apply our method to different video do-mains such as autonomous driving scenes, old ﬁlms, and animations, and obtain promising results, as illustrated in
Fig. 1 (c). In addition, our formulation possesses great ﬂex-ibility to extend to more challenging settings: 1) A video with the mask on a single frame. We adopt a similar train-ing strategy by switching the input and output in the above formulation to propagate masks. 2) Super high-resolution image sequences, such as a video with 4K resolution. We design a progressive learning scheme, and the ﬁner scale demands an additional prior, which is the output from the coarse-scale. We show examples of the extended method in
Fig. 1 (b) and Fig. 1 (d).
Our contribution can be summarized as follows:
• We propose an internal video inpainting method, which implicitly propagates information from the known regions to the unknown parts. Our method achieves state-of-the-art performance on DAVIS and can be applied to videos in various domains.
• We design two regularization terms to address the am-biguity and deﬁciency problems for challenging video sequences. The anti-ambiguity term beneﬁts the de-tails generation, and the gradient regularization term reduces temporal inconsistency.
• To the best of our knowledge, our approach is the ﬁrst deep internal learning model that demonstrates the fea-sibility of removing the objects in a 4K video with only a single frame mask. 2.