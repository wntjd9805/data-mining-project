Abstract
This paper presents a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer, which sig-nificantly enhances the ViT of
[12] for encoding high-resolution images using two techniques. The first is the multi-scale model structure, which provides image encod-ings at multiple scales with manageable computational cost. The second is the attention mechanism of Vision Long-former, which is a variant of Longformer [3], originally developed for natural language processing, and achieves a linear complexity w.r.t. the number of input tokens. A com-prehensive empirical study shows that the new ViT signifi-cantly outperforms several strong baselines, including the existing ViT models and their ResNet counterparts, and the
Pyramid Vision Transformer from a concurrent work [47], on a range of vision tasks, including image classification, object detection, and segmentation. The models and source code are released at https://github.com/microsoft/vision-longformer. 1.

Introduction
Vision Transformer (ViT) [12] has shown promising re-sults on image classification tasks for its strong capabil-ity of long range context modeling. But its quadratic in-crease of both computational and memory complexity hin-ders its application on many vision tasks that require high-resolution feature maps computed on high-resolution im-ages1, like object detection [34, 24], segmentation [27, 6], and human pose estimation [49, 37]. Vision-language tasks, like VQA, image captioning, and image-text retrieval, also benefit from high-resolution feature maps [16, 53], which are extracted with pre-trained CNN models. Developing a
† indicates equal contributions. 1In this paper, encoding a high-resolution image means generating high-resolution feature maps for high-resolution images. vision Transformer that can process high-resolution feature maps is a critical step toward the goal of unifying the model architecture of vision and language modalities and improv-ing multi-modal representation learning. In this paper, we propose a new vision Transformer architecture Multi-Scale
Vision Longformer, which significantly enhances the base-line ViT [12] for encoding high-resolution images using two techniques: (1) the multi-scale model structure, and (2) the attention mechanism of Vision Longformer.
Models with multi-scale (pyramid, hierarchical) struc-ture provide a comprehensive encoding of an image at multiple scales, while keeping the computation and mem-ory complexity manageable. Deep convolutional networks are born with such multi-scale structure, which however is not true for the conventional ViT architecture. To obtain a multi-scale vision Transformer, we stack multiple (e.g., four) vision Transformers (ViT stages) sequentially. The first ViT stage operates on a high-resolution feature map but has a small hidden dimension. As we go to later ViT stages, the feature map resolution reduces while the hidden dimension increases. The resolution reduction is achieved by performing patching embedding at each ViT stage. In our experiments, we find that with the same number of model parameters and the same model FLOPs, the multi-scale ViT achieves a significantly better accuracy than the vanilla ViT on image classification task. The results show that the multi-scale structure not only improves the compu-tation and memory efficiency, but also boosts the classifi-cation performance. The proposed multi-scale ViT has the same network structure as conventional (multi-scale) CNN models such as ResNet [14], and can serve as a replace-and-plug-in choice for almost all ResNet applications. In this paper, we demonstrate this plausible property in image classification, object detection and instance segmentation.
The multi-scale structure alone is not sufficient to scale up ViT to process high-resolution images and feature maps, due to the quadratic increase of the computation and mem-ory complexity with respect to the number of tokens in the 1
self-attention layers. Compared to natural language tasks where data is 1-D, this problem is more severe in vision tasks where the increase in complexity is quartic (fourth or-der) with the increase of image resolution. For example, the computational complexity of a 4× higher resolution multi-head self attention (MSA) layer (hidden dimension reduced by 4, i.e., 4H × 4W × D 4 ) equals to that of 64 layers in the original size (i.e., H × W × D). To address this challenge, we develop a 2-D version of Longformer[3], called Vision
Longformer, to achieve a linear complexity w.r.t. the num-ber of tokens (quadratic w.r.t. resolution). Our experiments show that compared to the baseline ViT, Vision Longformer shows no performance drop while significantly reduces the computational and memory cost in encoding images. The result indicates that the “local attention + global memory” structure in Vision Longformer is a desirable inductive bias for vision Transformers. We also compare Vision Long-former with other efficient attention mechanisms. The re-sult again validates its superior performance on both image classification and object detection tasks.
The main contributions of this paper are two-fold: (1)
We propose a new vision Transformer that uses the multi-scale model structure and the attention mechanism of 2-D
Longformer for efficient high-resolution image encoding. (2) We perform a comprehensive empirical study to show that the proposed ViT significantly outperforms previous
ViT models, their ResNet counterparts, and ViTs with sev-eral other efficient attention mechanisms, on image classifi-cation, object detection and segmentation tasks. 2.