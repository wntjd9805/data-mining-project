Abstract
While deep learning succeeds in a wide range of tasks, it highly depends on the massive collection of annotated data which is expensive and time-consuming. To lower the cost of data annotation, active learning has been proposed to in-teractively query an oracle to annotate a small proportion of informative samples in an unlabeled dataset.
Inspired by the fact that the samples with higher loss are usually more informative to the model than the samples with lower loss, in this paper we present a novel deep active learn-ing approach that queries the oracle for data annotation when the unlabeled sample is believed to incorporate high loss. The core of our approach is a measurement Temporal
Output Discrepancy (TOD) that estimates the sample loss by evaluating the discrepancy of outputs given by models at different optimization steps. Our theoretical investiga-tion shows that TOD lower-bounds the accumulated sample loss thus it can be used to select informative unlabeled sam-ples. On basis of TOD, we further develop an effective un-labeled data sampling strategy as well as an unsupervised learning criterion that enhances model performance by in-corporating the unlabeled data. Due to the simplicity of
TOD, our active learning approach is efﬁcient, ﬂexible, and task-agnostic. Extensive experimental results demonstrate that our approach achieves superior performances than the state-of-the-art active learning methods on image classiﬁ-cation and semantic segmentation tasks. 1.

Introduction
Large-scale annotated datasets are indispensable and critical to the success of modern deep learning models.
Since the annotated data is often highly expensive to ob-tain, learning techniques including unsupervised learning
[6], semi-supervised learning [59], and weakly supervised learning [44] have been widely explored to alleviate the
In this paper1 we focus on active learning [4] dilemma. which aims to selectively annotate unlabeled data with lim-1Code is available at https://github.com/siyuhuang/TOD ited budgets while resulting in high performance models.
In existing literature of active learning, two main-stream approaches have been studied, namely the diversity-aware approach and the uncertainty-aware approach. The diversity-aware approach [15] aims to pick out diverse sam-ples to represent the distribution of a dataset. It works well on low-dimensional data and classiﬁer with a small num-ber of classes [40]. The uncertainty-aware approach [52] aims to pick out the most uncertain samples based on the current model. However, the uncertainty heuristics, such as distance to decision boundary [2] and entropy of poste-rior probabilities [41], are often task-speciﬁc and need to be speciﬁcally designed for individual tasks such as image classiﬁcation [19], object detection [50], and semantic seg-mentation [8].
In this paper, we consider that the samples with higher loss would be more informative than the ones with lower loss. Speciﬁcally in supervised learning settings, when samples are correctly labeled, the averaged loss function over all samples should be gradually minimized during the learning procedure. Moreover, in every iteration the train-ing model would backward propagated error according to the loss of every sample [28], while the sample with high loss usually brings informative updates to the parameters of the training model [16]. In this work, we generalize these evidences to active learning problems and propose a sim-ple yet effective loss estimator Temporal Output Discrep-ancy (TOD), which could measure the potential loss of a sample only relied on the training model, when the ground-truth label of the sample is not available. Speciﬁcally, TOD computes the discrepancy of outputs given by models at dif-ferent optimization steps, and a higher discrepancy corre-sponds to a higher sample loss. Our theoretical investiga-tion shows that TOD well measures the sample loss.
On basis of TOD, we propose a deep active learning framework that leverages a novel unlabeled data sampling strategy for data annotation in conjunction with a semi-supervised training scheme to boost the task model perfor-mance with unlabeled data. Speciﬁcally, the active learning procedure can be split into a sequence of training cycles starting with a small number of labeled samples. By the
end of every training cycle, our data sampling strategy esti-mates Cyclic Output Discrepancy (COD), which is a variant of TOD, for every sample in the unlabeled pool and selects the unlabeled samples with the largest COD for data annota-tion. The newly-annotated samples are added to the labeled pool for model training in the next cycles. Furthermore, with the aid of the unlabeled samples, we augment the task learning objective with a regularization term derived from
TOD, so as to improve the performance of active learning in a semi-supervised manner.
Compared with the existing deep active learning algo-rithms, our approach is more efﬁcient, more ﬂexible, and easier to implement, since it does not introduce extra learn-able models such as the loss prediction module [54] or the adversarial network [43, 57] for uncertainty estimation. In the experiments, our active learning approach shows supe-rior performances in comparison with the state-of-the-art baselines on various image classiﬁcation and semantic seg-mentation datasets. Further ablation studies demonstrate that our proposed TOD can well estimate the sample loss and beneﬁt both the active data sampling and the task model learning.
The contributions of this paper are summarized as fol-lows. 1. This paper proposes a simple yet effective loss mea-sure TOD. Both theoretical and empirical studies vali-date the efﬁcacy of TOD. 2. This paper presents a novel deep active learning frame-work by incorporating TOD into an active sampling strategy and a semi-supervised learning scheme. 3. Extensive active learning experiments on image classi-ﬁcation and semantic segmentation tasks evaluate the effectiveness of the proposed methods. 2.