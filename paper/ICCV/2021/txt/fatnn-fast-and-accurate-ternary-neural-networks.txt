Abstract
Ternary Neural Networks (TNNs) have received much at-tention due to being potentially orders of magnitude faster in inference, as well as more power efficient, than full-precision counterparts. However, 2 bits are required to encode the ternary representation with only 3 quantization levels leveraged. As a result, conventional TNNs have simi-lar memory consumption and speed compared with the stan-dard 2-bit models, but have worse representational capabil-ity. Moreover, there is still a significant gap in accuracy be-tween TNNs and full-precision networks, hampering their deployment to real applications. To tackle these two chal-lenges, in this work, we first show that, under some mild constraints, computational complexity of the ternary inner product can be reduced by 2×. Second, to mitigate the per-formance gap, we elaborately design an implementation-dependent ternary quantization algorithm. The proposed framework is termed Fast and Accurate Ternary Neural
Networks (FATNN). Experiments on image classification demonstrate that our FATNN surpasses the state-of-the-arts by a significant margin in accuracy. More importantly, speedup evaluation compared with various precision is ana-lyzed on several platforms, which serves as a strong bench-mark for further research. Source code and models are available at: https://github.com/MonashAI/QTool 1.

Introduction
Equipped with high-performance computing and large-scale datasets, deep convolution neural networks (DCNN) have become a cornerstone for most computer vision tasks. However, a significant obstacle for deploying DCNN algorithms to mobile/embedded edge devices with lim-ited computing resources is the ever growing computation complexity—in order to achieve good accuracy, the models are becoming very heavy. To tackle this problem, much re-search effort has been spent on model compression. Repre-sentative methods include model quantization [49, 47], net-*PC and BZ contributed equally. Part of this work was done when all authors were with The University of Adelaide. CS is the corresponding author, email: chunhua@me.com work pruning [23, 51] and neural architecture search for lightweight models [52, 27].
In this paper, we focus on model quantization, which reduces the model complexity by representing a network with low-precision weights and activations.
Network quantization aims to map the continuous in-put values within a quantization interval to the correspond-ing quantization level, and a low-precision quantized value is assigned accordingly. TNNs in which both the activa-tions and weights are quantized to ternary, are particularly of interest because most of the calculations can be realized with bit operations, thus completely eliminating multipli-cations. However, there exists two limitations for conven-tional TNNs. The first limitation is the inefficient imple-mentation of TNNs. Specifically, the ternary representation of {−1, 0, 1} needs 2 bits to encode with one state wasted.
As a result, with the conventional bitwise implementation of quantized networks [47, 44], the complexity of ternary in-ner product is the same with the standard 2-bit counterparts.
Another limitation is the considerable accuracy drop com-pared with the full-precision counterparts due to the much more compact capacity.
To handle these drawbacks, we introduce a new frame-work, termed FATNN, where we co-design the underlying ternary implementation of computation and the quantization algorithm. In terms of implementation, we fully leverage the property of the ternary representation to design a series of bit operations to accomplish the ternary inner product with improved efficiency. In particular, FATNN reduces the computational complexity of TNNs by 2×, which solves the existing efficiency bottleneck. In contrast to previous works, nearly no arithmetic operations exist in the proposed implementation. Also, FATNN works efficiently on almost all kinds of devices (such as CPU, GPU, DSP, FPGA and
ASIC) with basic bit operation instructions available. Fur-thermore, we design the compatible ternary quantization al-gorithm in accordance to the mild constraints derived from the underlying implementation. Early works with learned quantizers either propose to learn the quantized values [44] or seek to learn the quantization intervals [20, 13]. How-ever, most of them assume the uniform quantizer step size, which might still be non-optimal on optimizing network
performance. To make the low-precision discrete values sufficiently fit the statistics of the data distribution, we pro-pose to parameterize the step size of each quantization level and optimize them with the approximate gradient. Besides, we suggest calibrating the distribution between the skip connection and the low-precision branch to further improve the performance. The overall approach is usable for quan-tizing both activations and weights, and works with exist-ing methods for back-propagation and stochastic gradient descent.
Our main contributions are summarized as follows:
• We propose a ternary quantization pipeline, in which we co-design the underlying implementation and the quantization algorithm. To our best knowledge, the proposed FATNN is the first solution being applied on general platforms specific to TNNs while previous ac-celeration methods only target on the dedicated hard-ware (FPGA or ASIC).
• We devise a fast ternary inner product implementa-tion which reduces the complexity of TNNs by 2× while keeping the bit-operation-compatible merit. We then design a highly accurate ternary quantization al-gorithm in accordance with the constraints imposed by the implementation.
• We evaluate the execution speed of FATNN and make comparison with other bit configurations on various platforms. Moreover, experiments on image classifi-cation task demonstrate the superior performance of our FATNN over a few competitive state-of-the-art ap-proaches. 2.