Abstract
Membership inference attacks (MIA) try to detect if data samples were used to train a neural network model, e.g. to detect copyright abuses. We show that models with higher dimensional input and output are more vulnerable to MIA, and address in more detail models for image translation and semantic segmentation, including medical image seg-mentation. We show that reconstruction-errors can lead to very effective MIA attacks as they are indicative of memo-rization. Unfortunately, reconstruction error alone is less effective at discriminating between non-predictable images used in training and easy to predict images that were never seen before. To overcome this, we propose using a novel predictability error that can be computed for each sample, and its computation does not require a training set. Our membership error, obtained by subtracting the predictability error from the reconstruction error, is shown to achieve high
MIA accuracy on an extensive number of benchmarks. 1 1.

Introduction
Deep neural networks have been widely adopted in vari-ous computer vision tasks, e.g. image classiﬁcation, semantic segmentation, image translation and generation, etc. Due to the high sample-complexity of such models, they require large amounts of training data. However, collection and an-notation of many training samples is an expensive and labor intensive process. In many domains, such as medical imag-ing, publicly available training data are particularly scarce due to privacy concerns. In such settings, a common solution is training the model privately and then providing black-box access to the trained model. However, even black-box access may leak sensitive information about the training data.
Membership inference attacks (MIA) are one way to detect such leakage. Given access to a data sample, an attacker attempts to ﬁnd whether or not the sample was used in the training process.
Due to memorization in deep neural networks, prediction conﬁdence tends to be higher for images used in training.
This difference in prediction conﬁdence helps MIA methods 1Our source code is available at GitHub: https://bit.ly/3k0UE6P to successfully determine which images were used for train-ing. Therefore, in addition to detecting information leakage,
MIA also provide insights on the degree of memorization in the victim model.
MIA has previously been applied to a variety of neural net-work tasks including: classiﬁcation, generative adversarial models, and segmentation. The accuracy achieved by MIA can vary greatly as a function of different properties of the at-tempted tasks. Our empirical results highlight two properties that make tasks more vulnerable to MIA attacks: i) Uncer-tainty: tasks where there is more uncertainty in the prediction of the output given an input are more susceptible to MIA. ii)
Output dimensionality: tasks with higher-dimensional out-puts are more vulnerable to MIA. These apparently simple properties can explain non-intuitive observations, for exam-ple, that reconstruction-based MIA on CycleGAN leads to very low accuracy. As the training images should be recon-structed back after being translated to the other domain and back, there is no uncertainty of the output given the input.
As the desired output is fully speciﬁed by the input (i.e. they are identical), the reconstruction task is easy leading to low
MIA accuracy.
Motivated by the above ﬁndings, we focus our attention on two tasks that exhibit these properties: supervised image translation and semantic segmentation, including medical image segmentation. We begin by evaluating a simple-to-implement but effective MIA that uses pixel-wise reconstruc-tion error between the model output and ground truth. This approach exploits memorization of the training data in the victim model, resulting in lower reconstruction error on im-ages used for training. However, we observe that for some sample images the ground truth result can be easily predicted, and for others it is harder to predict. Reconstruction error alone is therefore less accurate at discriminating between hard to predict samples used in training and easy samples not seen before. To overcome this limitation, we propose a novel predictability error which is computed for each query input image and its ground truth output.
Our predictability error uses the accuracy of a linear pre-dictor computed over the given query image, predicting pixel values from deep features of the input image. The linear pre-dictor serves as a simple approximation of the task attempted
Model
Dataset
Reconstruction Membership
Pix2pix
Pix2pix
Pix2pix
Pix2pixHD
Pix2pixHD
Pix2pixHD
SPADE
SPADE
UperNet50
UperNet101
HRNetV2
Inf-Net
PraNet
Facades
Maps2sat
Cityscapes
Facades
Maps2sat
Cityscapes
Cityscapes
ADE20K
ADE20K
ADE20K
ADE20K
COVID19
Polyp
Error 93.62% 84.22% 77.74% 98.92% 95.74% 96.04% 99.75% 85.31% 96.80% 95.74% 83.67% 97.16% 96.03%
Error 97.59% 85.65% 83.23% 99.95% 99.42% 99.09% 100% 89.79% 98.09% 96.94% 85.92% 99.01% 96.38%
Table 1. Membership attack ROCAUC using our (i) reconstruction error Lrec and (ii) membership error Lmem. Using the member-ship error, which subtracts the image predictability error from the reconstruction error, substantially improves performance. by the victim model, providing an indication of the ease of predicting the output image from the input. The reconstruc-tion error, together with the predictability error, helps to discriminate between two factors of variation in the recon-struction error: (i) The ”intrinsic” difﬁculty of the generation task for each image, based on its predictability error, and (ii) The boost in accuracy due to memorization of the train-ing images. Deﬁning a membership error that subtracts the predictability error from the reconstruction error is shown empirically to achieve high success rates in MIA.
Differently from other MIA approaches, we do not as-sume the existence of a large number of in-distribution data samples for training a shadow model - but rather operate on a single sample, using only a single query to the victim model.
Our method is demonstrated to be effective over strong base-lines on an extensive number of benchmarks, taken from image translation, semantic segmentation, and medical im-age segmentation. We discuss possible defenses against MIA and show their ineffectiveness against our method.
Our main contributions are: 1. Highlighting two key properties of tasks that are highly vulnerable to MIA. 2. Presenting the ﬁrst MIA on image translation models. 3. Proposing a general single-sample, self-supervised, im-age predictability error for MIA. 2.