Abstract
Image completion has made tremendous progress with convolutional neural networks (CNNs), because of their powerful texture modeling capacity. However, due to some local inductive prior, spatial-inherent properties (e.g., invariant kernels), CNNs do not perform well in under-standing global structures or naturally support pluralis-tic completion. Recently, transformers demonstrate their power in modeling the long-term relationship and gener-ating diverse results, but their computation complexity is quadratic to input length, thus hampering the application in processing high-resolution images. This paper brings the best of both worlds to pluralistic image completion: ap-pearance prior reconstruction with transformer and texture replenishment with CNN. The former transformer recovers pluralistic coherent structures together with some coarse textures, while the latter CNN enhances the local texture de-tails of coarse priors guided by the high-resolution masked images. The proposed method vastly outperforms state-of-the-art methods in terms of three aspects: 1) large per-formance boost on image ﬁdelity even compared to deter-ministic completion methods; 2) better diversity and higher
ﬁdelity for pluralistic completion; 3) exceptional general-*Corresponding author. ization ability on large masks and generic dataset, like Im-ageNet. Code and pre-trained models have been publicly released at https://github.com/raywzy/ICT. 1.

Introduction
Image completion (a.k.a. image inpainting), which aims to ﬁll the missing parts of images with visually realistic and semantically appropriate contents, has been a longstand-It is ing and critical problem in computer vision areas. widely used in a broad range of applications, such as object removal [2], photo restoration [34, 35], image manipula-tion [23, 15], and image re-targeting [6]. To solve this chal-lenging task, traditional methods like PatchMatch [2] usu-ally search for similar patches within the image and paste them into the missing regions, but they require appropriate information to be contained in the input image, e.g., similar structures or patches, which is often difﬁcult to satisfy.
In recent years, CNN-based solutions [22, 14, 18, 27, 36, 21] started to dominate this ﬁeld. By training on large-scale datasets in a self-supervised way, CNNs have shown their strength in learning rich texture patterns, and ﬁlls the missing regions with such learned patterns. Moreover, CNN models are computationally efﬁcient considering the sparse connectivity of convolutions. Nonetheless, they share some
inherent limitations: 1) The local inductive priors of con-volution operation make modeling the global structures of an image difﬁcult; 2) CNN ﬁlters are spatial-invariant, i.e. the same convolution kernel operates on the features across all positions, by that the duplicated patterns or blurry arti-facts frequently appear in the masked regions. On the other hand, CNN models are inherently deterministic. To achieve diverse completion outputs, recent frameworks [41, 39] rely on optimizing the variational lower bound of instance like-lihood. Nonetheless, extra distribution assumption would inevitably hurt the quality of generated contents [40].
Transformer, as well-explored architectures in language tasks, is on-the-rise in many computer vision tasks. Com-pared to CNN models, it abandons the baked-in local in-ductive prior and is designed to support the long-term inter-action via the dense attention module [33]. Some prelim-inary works [5] also demonstrate its capacity in modeling the structural relationships for natural image synthesis. An-other advantage of using a transformer for synthesis is that it naturally supports pluralistic outputs by directly optimizing the underlying data distribution. However, the transformer also has its own deﬁciency. Due to quadratically increased computational complexity with input length, it struggles in high-resolution image synthesis or processing. Moreover, most existing transformer-based generative models [26, 5] works in an auto-regressive manner, i.e., synthesize pixels in a ﬁxed order, like the raster-scan order, thus hampering its application in the image completion task where the miss-ing regions are often with arbitrary shapes and sizes.
In this paper, we propose a new high-ﬁdelity pluralis-tic image completion method by bringing the best of both worlds: the global structural understanding ability of trans-formers and the efﬁcient local texture reﬁnement ability of
CNN models. To achieve this, we decouple image comple-tion into two steps: pluralistic appearance priors reconstruc-tion with a transformer to recover the coherent image struc-tures, and low-resolution upsampling with CNN to replen-ish ﬁne textures. Speciﬁcally, given an input image with missing regions, we ﬁrst leverage the transformer to sample low-resolution completion results, i.e. appearance priors.
Then, guided by the appearance priors and the available pix-els of the input image, another upsampling CNN model is utilized to render high-ﬁdelity textures for missing regions meanwhile ensuring coherence with neighboring pixels. In particular, unlike previous auto-regressive methods [5, 32], in order to make the transformer model capable of com-pleting the missing regions by considering all the available contexts, we optimize the log-likelihood objective of miss-ing pixels based on the bi-directional conditions, which is inspired by the masked language model like BERT [9].
To demonstrate the superiority, we compare our method with state-of-the-art deterministic [25, 20, 37] and pluralis-tic [41] image completion approaches on multiple datasets.
Our method makes signiﬁcant progress from three aspects: 1) Compared with previous deterministic completion meth-ods, our method establishes a new state of the art and out-performs theirs on a variety of metrics by a large margin; 2) Compared with previous pluralistic completion methods, our method further enhances the results diversity, mean-while achieving higher completion ﬁdelity; 3) Thanks to the strong structure modeling capacity of transformers, our method generalizes much better in completing extremely large missing region and large-scale generic datasets (e.g.,
ImageNet) as shown in Figure. 1. Remarkably, the FID score on ImageNet is improved by 41.2 at most compared with the state-of-the-art method PIC [41]. 2.