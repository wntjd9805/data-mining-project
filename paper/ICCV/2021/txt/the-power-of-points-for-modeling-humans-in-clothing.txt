Abstract
Currently it requires an artist to create 3D human avatars with realistic clothing that can move naturally. De-spite progress on 3D scanning and modeling of human bod-ies, there is still no technology that can easily turn a static scan into an animatable avatar. Automating the creation of such avatars would enable many applications in games, so-cial networking, animation, and AR/VR to name a few. The key problem is one of representation. Standard 3D meshes are widely used in modeling the minimally-clothed body but do not readily capture the complex topology of cloth-ing. Recent interest has shifted to implicit surface mod-els for this task but they are computationally heavy and lack compatibility with existing 3D tools. What is needed is a 3D representation that can capture varied topology at high resolution and that can be learned from data. We ar-gue that this representation has been with us all along — the point cloud. Point clouds have properties of both im-plicit and explicit representations that we exploit to model 3D garment geometry on a human body. We train a neu-ral network with a novel local clothing geometric feature to represent the shape of different outfits. The network is trained from 3D point clouds of many types of clothing, on many bodies, in many poses, and learns to model pose-dependent clothing deformations. The geometry feature can be optimized to fit a previously unseen scan of a person in clothing, enabling the scan to be reposed realistically.
Our model demonstrates superior quantitative and qualita-tive results in both multi-outfit modeling and unseen outfit animation. The code is available for research purposes at https://qianlim.github.io/POP. 1.

Introduction
Animatable clothed human avatars are required in many applications for 3D content generation. To create avatars with naturally-deforming clothing, existing solutions either involve heavy artist work or require 4D scans for train-ing machine-learning models [55]. These solutions are ex-pensive and often impractical. Instead, can we turn a sin-gle static 3D scan — which can be acquired at low cost today even with hand-held devices — into an animatable avatar? Currently, no existing technology is able to do this and produce realistic clothing deformations. Given a static scan, traditional automatic rigging-and-skinning methods
[3, 18, 33] can be used to animate it, but are unable to produce pose-dependent clothing deformations. Physics-based simulations can produce realistic deformations, but require “reverse-engineering” a simulation-ready clothing mesh from the given scan. This involves expert knowledge and is not fully automatic.
Taking a data-driven approach, the goal would be to learn a model that can produce reasonable pose-dependent cloth-ing deformation across different outfit types and styles and can generalize to unseen outfits. However, despite the re-cent progress in modeling clothed human body shape de-formations [22, 31, 36, 37, 44, 55], most existing models are outfit-specific and thus cannot generalize to unseen out-fits. To date, no such cross-garment model exists, due to several technical challenges.
The first challenge lies in the choice of 3D shape repre-sentation. To handle outfits of different types and styles at once, the shape representation must handle changing topol-ogy, capture high-frequency details, be fast at inference time, and be easy to render. Classical triangle meshes ex-cel at rendering efficiency but are fundamentally limited by their fixed topology. The implicit surface representation is topologically flexible, but is in general computationally heavy and lacks compatibility with existing graphics tools.
Because point clouds are an explicit representation, they are efficient to render, but they can also be viewed as implic-itly representing a surface. Thus they are flexible in topol-ogy and, as the resolution of the point cloud increases, they can capture geometric details. While point clouds are not commonly applied to representing clothing, they are widely used to represent rigid objects and many methods exist to process them efficiently with neural networks [1, 17, 32]. In this work, we show that the seemingly old-fashioned point cloud is, in fact, a powerful representation for modeling clothed humans.
In recent work, SCALE [36] demonstrates that a point cloud, grouped into local patches, can be exploited to repre-sent clothed humans with various clothing styles, includ-ing those with thin structures and open surfaces. How-the patch-based formulation in SCALE often suf-ever, fers from artifacts such as gaps between patches.
In this work, we propose a new shape representation of dense point clouds. For simplicity, we avoid using patches, which have been widely used in recent point cloud shape representa-tions [4, 15, 16, 21, 36], and show that patches are not nec-essary. Instead, we introduce smooth local point features on a 2D manifold that regularize the points and enable arbitrar-ily dense up-sampling during inference.
Another challenging aspect of cross-outfit modeling con-cerns how outfits of different types and styles can be encoded in a single, unified, model.
In most existing outfit-specific models, the model parameters (typically the weights of a trained shape decoder network) need to repre-sent both the intrinsic, pose-independent shape of a clothed person, and how this shape deforms as a function of the in-put pose. To factor the problem, we propose to isolate the intrinsic shape from the shape decoder by explicitly condi-tioning it with a geometric feature tensor. The geometric feature tensor is learned in an auto-decoding fashion [43], with a constraint that a consistent intrinsic shape is shared across all examples of the same outfit. Consequently, the shape decoder can focus on modeling the pose-dependent effects and can leverage common deformation properties across outfits. At inference time, the geometric feature ten-sor can be optimized to fit to a scan of a clothed body with a previously unseen outfit, making it possible for the shape decoder to predict pose-dependent deformation of it based on the learned clothing deformation properties.
These ideas lead to POP: our dense point cloud model that produces pose-dependent clothing geometry across dif-ferent outfits and demonstrating the Power of Points for modeling shapes of humans in clothing. POP is evaluated on both captured and synthetic datasets, showing state-of-the-art performance on clothing modeling and generaliza-tion to unseen outfits.
In summary, our contributions are: (1) a novel dense point cloud shape representation with fine-grained local fea-tures that produces state-of-the-art detailed clothing shapes with various clothing styles; (2) a novel geometry feature tensor that enables cross-garment modeling and general-ization to unseen outfits; (3) an application of animating a static scan with reasonable pose-dependent deformations.
The model and code are available for research purposes at https://qianlim.github.io/POP. 2.