Abstract
With the development of 3D scanning technologies, 3D vision tasks have become a popular research area. Ow-ing to the large amount of data acquired by sensors, un-supervised learning is essential for understanding and uti-lizing point clouds without an expensive annotation pro-cess.
In this paper, we propose a novel framework and an effective auto-encoder architecture named “PSG-Net” for reconstruction-based learning of point clouds. Unlike existing studies that used fixed or random 2D points, our framework generates input-dependent point-wise features for the latent point set. PSG-Net uses the encoded input to produce point-wise features through the seed generation module and extracts richer features in multiple stages with gradually increasing resolution by applying the seed fea-ture propagation module progressively. We prove the effec-tiveness of PSG-Net experimentally; PSG-Net shows state-of-the-art performances in point cloud reconstruction and unsupervised classification, and achieves comparable per-formance to counterpart methods in supervised completion. 1.

Introduction
Deep neural networks, especially convolutional neural networks (CNNs), have achieved success in the perfor-mance of various computer vision tasks [10, 14, 13]. Re-cently, research centered on 2D space has been expanded to 3D space following the development of 3D scanning tech-niques. 3D sensors such as LiDAR and RGB-D cameras acquire data in form of point clouds, so it is essential to effectively recognize this type of data in robotics and au-tonomous driving applications. Point clouds lie in 3D space, so they incur significantly higher labeling costs for a spe-cific vision task when compared to 2D data. Therefore, the need for effective unsupervised learning techniques for point clouds is highly emphasized.
For unsupervised learning, auto-encoder structures based on CNNs have been widely used to deal with 2D im-age data. Although CNNs show superior ability in learn-Figure 1. Examples of the 3D point cloud reconstruction, unsuper-vised classification, point cloud completion results with PSG-Net.
Our network successfully performs each task from the input point cloud. ing general features from 2D images, it is difficult to apply
CNNs to point clouds because of the irregularity of the data format. Thus, network architectures that are specifically de-signed for point cloud recognition must be used as the en-coder of the auto-encoder. In previous studies [5, 34, 24], point cloud classification networks such as PointNet [16] and PointNet++ [17] have been used as encoders, and some other works have utilized graph layers [30]. Encoders ex-tract a global feature representation called a codeword vec-tor, which becomes the input to the decoder.
For the decoder architecture, a popular approach is the concept of “folding” a 2D plane into a 3D object surface be-cause the number of the output point clouds may not be de-termined. During the folding operation, the latent point set sampled from the 2D plane is transformed into 3D points, thus achieving point cloud generation. This approach was first proposed in FoldingNet [30] and has been mathemati-cally established in several studies [5, 34]. In FoldingNet, a pre-defined set of fixed grid points in 2D space was used as the latent point set to the decoder along with the output of the encoder. Then, the folding operation was applied to transform this 2D point set into 3D points. In AtlasNet [5] and 3D Point Capsule Network (3D-PointCapsNet) [34],
multi-patch approaches were used for point cloud recon-struction. These works used randomly sampled points from the uniform distribution in a fixed area of the 2D plane.
Throughout this paper, the latent point set will be referred to as seed.
One limitation of these methods is that the output point cloud is generated from an arbitrary 2D plane. Fixed grid points [30] or randomly sampled points [5, 34] were used as inputs to the decoder. Considering that an arbitrary 2D plane might not have enough capacity to model a complex 3D surface, AtlasNet and 3D-PointCapsNet utilize multiple patches to improve performance. However, the number of decoders increases with the number of patches, which leads to significant computational costs. Therefore, it is neces-sary to fundamentally change the sampling process in the 2D plane, rather than simply use more patches.
In this paper, we propose a novel framework for reconstruction-based learning. The main idea is to generate the seed from the function of the input point cloud. Previ-ous methods used fixed or randomly sampled 2D points as seeds, which can be a substantial constraint to the decoder.
We revisit the problem to explain why the generated seed helps our decoder to generate various 3D shapes. We im-plement our framework by “PSG-Net”, which is far more effective and superior to existing methods. The seed is gen-erated in multiple stages through the seed generation mod-ule. Then, the seed feature propagation module processes the generated seed and codeword vector to produce the out-put shape. In addition, we introduce a progressive approach to enrich the information of the feature for the output point cloud. This is achieved by generating a gradually increasing resolution of the seed and interpolated feature maps. The re-sult from the last seed feature propagation module is trans-formed into 3D point cloud coordinates through the point generation layers.
Our network achieves performances comparable to the state-of-the-art methods in various unsupervised tasks such as point cloud reconstruction and unsupervised classifica-tion, and achieves the best performance among counterpart methods in supervised point cloud completion. Further-more, our method can be used in combination with other existing methods such as the multi-patch approach, which is expected to result in enhanced performance. The main contributions of this study are as follows. 1. We propose a novel framework for reconstruction-based learning of point clouds that uses the generated input-dependent point-wise features as seed, instead of using simple 2D points. To implement this frame-work, we incorporate seed generation module (SGM) and seed feature propagation module (SFPM) in an ef-ficient auto-encoder architecture called PSG-Net. 2. We analyze two proposed modules and demonstrate the superiority of our model by experimentally prov-ing the analysis. 3. We show that the performance of PSG-Net is compara-ble to state-of-the-art methods in various 3D tasks such as point cloud reconstruction, unsupervised classifica-tion and supervised point cloud completion. 2.