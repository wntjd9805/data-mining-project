Abstract
The data poisoning attack has raised serious security concerns on the safety of deep neural networks, since it can lead to neural backdoor that misclassiﬁes certain inputs crafted by an attacker. In particular, the sample-targeted
It targets at one or backdoor attack is a new challenge. a few speciﬁc samples, called target samples, to misclas-sify them to a target class. Without a trigger planted in the backdoor model, the existing backdoor detection schemes fail to detect the sample-targeted backdoor as they depend on reverse-engineering the trigger or strong features of the trigger.
In this paper, we propose a novel scheme to de-tect and mitigate sample-targeted backdoor attacks. We discover and demonstrate a unique property of the sample-targeted backdoor, which forces a boundary change such that small “pockets” are formed around the target sam-ple. Based on this observation, we propose a novel defense mechanism to pinpoint a malicious pocket by “wrapping” them into a tight convex hull in the feature space. We de-sign an effective algorithm to search for such a convex hull and remove the backdoor by ﬁne-tuning the model using the identiﬁed malicious samples with the corrected label ac-cording to the convex hull. The experiments show that the proposed approach is highly efﬁcient for detecting and mit-igating a wide range of sample-targeted backdoor attacks. 1.

Introduction
Deep neural networks (DNNs) play a critical role in a wide range of applications, such as image classiﬁcation [6], facial recognition [23] and autonomous driving [37]. De-spite these advances, DNNs are data-driven, depending on the size and quality of the training data, and computation resource for model training. They are also empirical, re-quiring extensive expertise to design a good model archi-tecture. Therefore, it is often infeasible for general users to train their own models on a large scale. Instead, users typically outsource model training to third parties known as Machine Learning as a Service (MLaaS) [25] or reuse a public model from an online model zoo storage website, e.g., Caffe Model Zoo [15] or Tensorﬂow Model Zoo [1].
However, this raises a fundamental question: can we trust a model provided by someone else? DNNs are com-monly considered as black-boxes and lack interpretability and transparency to humans. Moreover, it is not feasi-ble to test their behavior exhaustively. These properties can be exploited by attackers to plant a backdoor to the model provided to the user. This can be done through stealthily injecting poisoned data into the training dataset by an attacker, when the model trainer collects training data from the web, which is in fact one type of data poison at-tack [17, 28, 38, 2]. Alternatively, the model trainer it-self can change the training data to intentionally plant a backdoor. The backdoor attack can be largely categorized into two types, sample-targeted and trigger-based, depend-ing on if a predeﬁned trigger is adopted to activate the backdoor. In trigger-based backdoor attacks, a backdoor is planted during training using a “trigger” stamped on sam-ples [9, 21, 36, 26], which is a predeﬁned special pattern such as a small white block as illustrated in Figure 1(a).
After training, the backdoor model behaves normally with clean samples but misclassiﬁes an input into the target cat-egory if the trigger is embedded in the input sample.
In contrast, instead of adopting a predeﬁned trigger, the sample-targeted backdoor attack targets at one or a few spe-ciﬁc samples, called as target samples, to misclassify them to a target class. The most straightforward method of inject-ing a sample-targeted backdoor is to simply ﬂip the label of the target sample (see Figure 1(d), which is an image of a car but labeled as “cat”). Such a sample is included in the training set to create a sample-targeted backdoor [34]. In the Feature Collision attack [28] and its variations [38, 2], the attacker perturbs a small number of samples in the target class (e.g., with the label of “cat”) without changing their la-bels, to minimize their feature distance to the target sample
of detecting sample-targeted backdoors. Our approach is motivated by the observation that the sample-targeted back-door leads to small “pockets” around the target samples on the decision boundary, thus misclassifying them to the target category. Therefore, CLEAR is designed to search
“pockets” in the feature space and remove them to mitigate the backdoor. Our contributions are summarized as follows.
• We discover and demonstrate a unique feature of sample-targeted attacks: they force a boundary change of the original benign model such that small “pockets” are formed around the target sample.
• We propose a novel defense mechanism to pinpoint a malicious pocket by “wrapping” them into a tight con-vex hull in the feature space. To achieve this, we de-sign an effective algorithm to search for such a convex hull. The malicious samples identiﬁed by the algo-rithm are then utilized to remove the backdoor by ﬁne-tuning the model. Those samples have been shown critical for backdoor mitigation.
• Third, we evaluate our approach by conducting exten-sive experiments against four state-of-the-art single-target/multi-target sample-targeted backdoor attacks
[34, 28, 38, 2] across multiple datasets on multiple widely used model architectures. To the best of our knowledge, our work is the ﬁrst to successfully detect and mitigate sample-targeted backdoors. 2.