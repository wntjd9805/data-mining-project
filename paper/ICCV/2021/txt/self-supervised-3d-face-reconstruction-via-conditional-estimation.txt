Abstract
We present a conditional estimation (CEST) framework to learn 3D facial parameters from 2D single-view images by self-supervised training from videos. CEST is based on the process of analysis by synthesis, where the 3D facial parameters (shape, reflectance, viewpoint, and illumina-tion) are estimated from the face image, and then recom-bined to reconstruct the 2D face image. In order to learn semantically meaningful 3D facial parameters without ex-plicit access to their labels, CEST couples the estimation of different 3D facial parameters by taking their statisti-cal dependency into account. Specifically, the estimation of any 3D facial parameter is not only conditioned on the given image, but also on the facial parameters that have already been derived. Moreover, the reflectance symmetry and consistency among the video frames are adopted to im-prove the disentanglement of facial parameters. Together with a novel strategy for incorporating the reflectance sym-metry and consistency, CEST can be efficiently trained with in-the-wild video clips. Both qualitative and quantitative experiments demonstrate the effectiveness of CEST. 1.

Introduction
Reconstructing 3D faces from single-view 2D images has been a longstanding problem in computer vision. The common approach represents the 3D face as a combination of its shape, as represented by the 3D coordinates of a num-ber of points on its surface called vertices, and its texture, as represented by the reflectances of red, green and blue at these vertices [4]. The problem then becomes learning a regression model between the 2D images, and vertices and their reflectances.
The regression itself may be learned using training data where both, the 2D images and the corresponding 3D pa-rameters are available. However, these data are scarce, and even the ones that are available generally only have shape information [8, 47, 46]; the ones that do have other pa-rameters are usually captured in a controlled environment
[22] or are synthetic [33], which is not representative of real-world images. Consequently, there is great interest in self-supervised learning methods, which learn the regres-sion model from natural in-the-wild 2D images or videos,
Figure 1: Conventional 3D face reconstruction and our CEST framework.
The dotted lines separate the modules used for inference of the 3D param-eters from those used for training with self-supervision. without explicit access to 3D training data [39, 41].
The problem is complicated by the fact that the actual image formation depends not only on the shape and texture of the face, but also the illumination (the intensity and di-rection of the incident light), and other factors such as the viewpoint (incorporating the orientation of the face and the position of the camera), etc. Thus, the learned regression model must also account for these factors. To this end, the general approach is one where shape, reflectance, illumina-tion and viewpoint parameters are all extracted from the 2D image. The regression model that extracts these facial pa-rameters are learned through self-supervision: the extracted facial parameters are recombined to render the original 2D image, and the model parameters are learned to minimize the reconstruction error.
The solution, however, remains ambiguous because a 2D image may be obtained from different combinations of shape, texture, illumination and viewpoint. To ensure that the self-supervision provides meaningful disentanglement, the manner in which the facial parameters are recombined to reconstruct the 2D image are based on the actual physics of image formation [39, 41, 33]. To further reduce poten-tial ambiguities, regularizations are necessary. Reflectance symmetry has been proposed as a regularizer [42, 38, 45], wherein the reflectance of a face image and its mirror reflec-tion are assumed to be identical. Smoothness has also been employed to regularize the shape and reflectance [41, 38].
Additional regularization may be obtained by considering correspondences between multiple images of the same face
[18, 37], particularly when they are obtained under near identical conditions such as the sequence of images from a video. The approach in [37] has considered reflectance con-sistency, where reflectances of all image frames in a video clip are assumed to be similar.
In all of these prior works, the target parameters, namely the shape, reflectance, illumination and viewpoint parame-ters are all individually estimated, without considering their direct influences on one another, although they are jointly
In effect, at inference time they assume that optimized. the estimate of, e.g. the reflectance, is conditionally in-dependent of the estimated shape or viewpoint, given the original 2D image. The coupling among the four is only considered during (self-supervised) training, where they must all combine to faithfully recreate the input 2D image
[11, 14, 29, 42, 37]. This is illustrated in Fig. 1(a).
In reality, 2D images are reduced-dimensional projec-tions, and thus imperfect representations of the full three-dimensional structure of the face, and the aspects of re-flectance and illumination imprinted in them are not in-dependent of the underlying shape of the object or the viewpoint they were captured from. Therefore, the cap-tured 2D image represents a joint interaction among view-point, shape, reflectance and illumination. Consequently, the statistical estimates of any of these four factors may not, in fact, be truly conditionally independent of one another given only the 2D image (although, given the entire 3D model they might have been). Thus, modelling all of these variables as being conditionally independent effectively represents a lost opportunity since, by predicting them in-dividually, the constraints they impose on one another are ignored. Optimization-based approaches [17, 18, 35] do at-tempt to capture the dependence by iteratively estimating shape and reflectance from one another. However, these methods require correspondence information of the image sequence in a video and suffer from costly inference.
In this paper, we propose a novel learning-based frame-work based on conditional estimation (CEST). CEST ex-plicitly considers the statistical dependency of the various 3D facial parameters (shape, viewpoint, reflectance and il-lumination) upon one another, when derived from single 2D image. The specific form of the dependencies adopted in this paper is shown in Fig. 1(b). We note that the CEST framework is very general and allows us to consider any other dependency structures. Our paper serves as one of the many potential choices that work well in practice. To this end, we present a specific, and intuitive, solution in CEST, where the viewpoint, facial shape, facial reflectance, and il-lumination are predicted sequentially and conditionally. In this context, the prediction of facial shape is conditioned on the input image and the derived viewpoint; the prediction of facial reflectance is conditioned on the input image, derived viewpoint and facial shape; and so forth.
As before, learning remains self-supervised, through comparison of re-rendered 2D images obtained with the es-timated 3D face parameters to the original images. As ad-ditional regularizers, we also employ reflectance symmetry constraints [42, 38, 45], and reflectance consistency con-straints (across frames in a short video clip) [37]. These are included in the form of cross-frame reconstruction er-ror terms, the number of which increases quadratically with the number of video frames considered together for self-supervision. To address the dramatically increased number of reconstruction terms, we propose a stochastic optimiza-tion strategy to improve training efficiency.
We present ablation studies and comparisons to state-of-the-art methods [39, 42, 37] to evaluate CEST. We show that CEST produces better reflectance and structured illu-mination, leading to more realistic rendered faces with fine facial details, compared to all other tested methods. It also achieves better shape estimation accuracy on AFLW2000-3D [49] and MICC [1] datasets than current state-of-the-art self-supervised and fully supervised approaches. Overall, our contributions can be summarized as follows:
• We propose CEST, a conditional estimation framework for 3D face reconstruction that explicitly considers the statistical dependencies among 3D face parameters.
• We propose a specific design for the decomposition of conditional estimation, where the viewpoint, shape, re-flectance, and the illumination are derived sequentially.
• We propose a stochastic optimization strategy to effi-ciently incorporate reflectance symmetry and consistency constraints into CEST. As the number of video frames increase, the computational complexity of CEST is in-creased linearly, rather than quadratically. 2.