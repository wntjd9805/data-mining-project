Abstract
While deep neural networks have achieved remarkable success in various computer vision tasks, they often fail to generalize to new domains and subtle variations of input images. Several defenses have been proposed to improve the robustness against these variations. However, current defenses can only withstand the speciﬁc attack used in train-ing, and the models often remain vulnerable to other input variations. Moreover, these methods often degrade perfor-mance of the model on clean images and do not generalize to out-of-domain samples. In this paper we present Gener-ative Adversarial Training, an approach to simultaneously improve the model’s generalization to the test set and out-of-domain samples as well as its robustness to unseen ad-versarial attacks. Instead of altering a low-level pre-deﬁned aspect of images, we generate a spectrum of low-level, mid-level and high-level changes using generative models with a disentangled latent space. Adversarial training with these examples enable the model to withstand a wide range of attacks by observing a variety of input alterations during training. We show that our approach not only improves per-formance of the model on clean images and out-of-domain samples but also makes it robust against unforeseen attacks and outperforms prior work. We validate effectiveness of our method by demonstrating results on various tasks such as classiﬁcation, segmentation and object detection. 1.

Introduction
Deep neural networks have shown promising generaliza-tion to in-domain samples. However, they are vulnerable to slight alterations of input images and have limited general-ization to new domains. Several defenses have been pro-posed for improving the models’ robustness against input variations. However, these models only provide robustness against a narrow range of threat models used in training, and they have poor generalization to unseen attacks. We hypothesize that this is due to the fact that they only con-sider a small subset of realistic examples on or near the manifold of natural images. For instance, additive pertur-bations leave high-level semantic aspects of images intact.
Therefore, models trained against these examples do not provide robustness to high-level input variations. In order for a model to become robust to all realistic variations of in-puts, it needs to see a diverse set of samples during training.
However, most of the existing works only alter a low-level aspect of images such as color [18, 24, 4], spatial [13, 36, 1], pose [2, 42], and others. Even if we consider the union of several types of attacks, the model can still be vulnerable to other input variations that are not contained in any of the constituent threat models. To provide robustness against un-foreseen attacks, we propose adversarial training against a range of low-level to high-level variations of inputs. We leverage generative models with disentangled latent repre-sentations to systematically build diverse and realistic ex-amples without leaving the manifold of natural images. We show that our approach improves generalization and robust-ness of the model to unseen variations of input images with-out training against any of them.
We build upon state-of-the-art generative models which disentangle factors of variation in images. We create ﬁne and coarse-grained adversarial changes by manipulating various latent variables at different resolutions. Loss of the target network is used to guide the generation process. The pre-trained generative model constrains the search space for our adversarial examples to realistic images, thereby re-vealing the target model’s vulnerability in the natural im-age space. We verify that we do not deviate from the space of realistic images with a user study as well as a t-SNE plot comparing distributions of real and adversarial images. As a result, we observe that including these examples in training the model enhances its accuracy on clean images as well as out-of-domain samples. Moreover, since the model has seen a variety of low-level and high-level alterations of images, it becomes robust to a wide range of adversarial examples in-cluding recoloring, spatial transformations, perceptual and additive perturbations.
Our contributions can be summarized as follows:
• We present Generative Adversarial Training (GAT) to simultaneously improve the model’s generalization to clean and out-of-domain samples and its robustness to unforeseen attacks. Our approach is based on adversar-ial training with ﬁne-grained unrestricted adversarial examples in which the attacker controls which aspects of the image to manipulate, resulting in a diverse set of realistic, on-the-manifold examples.
• We evaluate GAT against a diverse set of adversarial attacks: recoloring, spatial transformations, percep-tual and additive perturbations, and demonstrate that it achieves state-of-the-art robustness against these at-tacks without training against any of them.
• We extend our approach to semantic segmentation and object detection tasks, and propose the ﬁrst method for generating unrestricted adversarial examples for seg-mentation and detection. Training with our examples improves both robustness and generalization of the model. 2.