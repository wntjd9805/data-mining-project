Abstract
We present a novel approach for extracting human pose features from human action videos. The goal is to let the pose features capture only the poses of the action while being invariant to other factors, including video back-grounds, the video subjects’ anthropometric characteristics and viewpoints. Such human pose features facilitate the comparison of pose similarity and can be used for down-stream tasks, such as human action video alignment and pose retrieval. The key to our approach is to ﬁrst normal-ize the poses in the video frames by mapping the poses onto a pre-deﬁned 3D skeleton to not only disentangle subject physical features, such as bone lengths and ratios, but also to unify global orientations of the poses. Then the normal-ized poses are mapped to a pose embedding space of high-level features, learned via unsupervised metric learning. We evaluate the effectiveness of our normalized features both qualitatively by visualizations, and quantitatively by a video alignment task on the Human3.6M dataset and an action recognition task on the Penn Action dataset. 1.

Introduction
Video alignment aims to ﬁnd dense temporal correspon-dences between a pair of videos. Finding alignments be-tween two natural human action videos is especially chal-lenging because the two videos to be aligned can have large variations in many factors, such as scales and orientations of the video subjects, camera viewpoints, action speeds and orientations, etc. A feature that is robust to these variations is desirable in ﬁnding the alignments.
A common approach to the problem of human ac-tion video alignment is to ﬁrst estimate 2D or 3D hu-man poses from two input videos, and then ﬁnd the align-ments by matching with features extracted from joint posi-tions [48, 11], so as to reduce certain interference in video backgrounds and subjects’ clothing. However, human poses still contain large variations in scale, bone length ratios, orientations, etc. Since existing 3D pose estimation meth-ods [28, 37] recover 3D poses in camera coordinate sys-tems, the joint positions relative to a root joint are depen-dent on viewpoints (as illustrated by a toy example in the supplementary material). Global orientation normalization by Procrustes alignment is hard to be applied to in-the-wild videos when the ground-truth 3D poses are not available.
Besides viewpoint, the joint positions computed by exist-ing 3D pose estimation methods are also dependent on the video subjects’ anthropometric characteristics, such as bone lengths and ratios. Such anthropometric variation would cause a difference in distance measurements (e.g., L2 dis-tance after Procrustes alignment) even when the subjects in the videos perform exactly the same poses, as illustrated in
Figure 1.
Given the above limitations of using joint position-based pose representations for video alignment, an important ob-servation is that pose similarity is better described by joint angular representations than relative joint positions. The poses of two subjects performing an identical pose should have the same joint angles or joint rotations, but could pro-duce a difference in joint positions due to the difference in relative bone lengths, as illustrated in Figure 1. In addition, relative joint angles or rotations of physically connected joints are consistent among cameras and invariant to view-points. Thus the key to extract subject- and scene-invariant features for comparison is to extract features with respect to joint angular representations rather than joint position-based representations.
At ﬁrst sight, a straightforward solution might be to com-pute joint angles from joint positions, and use raw joint angles [10] or their aggregations [35, 52] as features for matching. However, the joint angle features suffer from in-formation loss by dropping the skeleton’s relational context, which has a proven signiﬁcance in capturing pose discrim-ination [8, 40]. Joint rotations also have limitations in that either directly regressing 3D joint rotations from 2D poses, or computing joint rotations from 3D poses by inverse kine-matics (IK) is an ill-posed problem, where multiple possi-ble sets of joint rotations can be mapped to the same set of joint positions [17, 53]. Even though existing works have attempted to add kinematic constraints to reduce the IK am-biguity [17, 46], it is still impractical to compare pose sim-ilarities directly in the joint rotation space using the joint rotations computed from joint positions [57].
To address the limitations in position-based and angular-based pose representations, we propose to use a normalized human pose, an intermediate pose representation that re-ﬂects the pose information with respect to joint rotations, and is parameterized by joint positions to preserve the re-lational context of body conﬁgurations, as shown in Fig-ure 1(c). This normalized pose representation is enlight-ened by the recent works that use joint rotations as pose parameterizations for motion reconstruction [45] and pose sequence generation [53, 38]. They incorporate a determin-istic forward kinematics (FK) layer in neural networks to convert the joint rotations into joint positions to avoid the joint rotation ambiguity problem in IK. FK recursively ro-tates the bones in a skeleton from a root joint to the leaf joints according to the joint rotations, resulting in joint posi-tions that can be supervised by ground-truth joint positions.
We adopt an FK layer to perform pose normalization. Our normalized pose representation retains the joint rotations of the subjects in video frames, such that it captures the pose information and is invariant to all other factors related to the original scene and subject in the video; and is param-eterized by the joint positions of a pre-deﬁned skeleton to reduce ambiguity in comparing pose similarities.
Figure 2. The pipeline of our proposed method. (a) Pose nor-malization: the 2D pose in each video frame is mapped onto a 3D condition skeleton; (b) pose embedding: the 3D condition skeleton poses are mapped to a pose embedding space.
We design a neural network that learns to normalize human poses in videos. Speciﬁcally, the pose normaliza-tion network takes in 2D poses and estimate joint rotations, which are then applied by FK on a pre-deﬁned 3D skeleton with uniﬁed ﬁxed bone lengths (called condition skeleton as in [53]). The joint rotations of the subjects’ poses in videos are thus converted into the joint positions of the condition skeleton to normalize the poses. In this way, the difference in joint positions of the condition skeleton is caused only by the difference in joint rotations. Since the normalized poses are not paired with ground-truth poses for training, our network adopted a cycle consistency training strategy (Section 3.2). With joint rotations, the poses can also be easily uniﬁed to the same global orientation by specifying the root joint rotation. Finally, the pose features are learned from the normalized 3D poses by metric learning. The re-sulting pose features are high-level human pose represen-tations and can be directly compared by the Euclidean dis-tance.
In this paper we mainly focus on the video alignment task, but the proposed feature could also be used for other pose similarity tasks, such as pose retrieval, action detec-tion, etc. Experiments show that our proposed normalized pose is robust to variations in viewpoint and subjects’ an-thropometry. Pose features learned from the normalized poses have shown proven performances on a dense corre-spondence task on the Human3.6M dataset, and an action recognition task on the Penn Action dataset. 2.