Abstract
The paper studies planar surface reconstruction of in-door scenes from two views with unknown camera poses.
While prior approaches have successfully created object-centric reconstructions of many scenes, they fail to exploit other structures, such as planes, which are typically the dominant components of indoor scenes. In this paper, we re-construct planar surfaces from multiple views, while jointly estimating camera pose. Our experiments demonstrate that our method is able to advance the state of the art of re-construction from sparse views, on challenging scenes from
Matterport3D. 1.

Introduction
Consider the two photos in Figure 1, as humans, we can infer that they were taken from the same scene: there is a chair on one side, a bedside table on the other, and a large glass wall and floor in the middle. We perceive the scene correctly despite the fact that they are sparse views [37]: they were taken from very different camera poses, and very little of the scene structure within them overlaps. There are also challenges in grouping: while each photo might, at first glance, seem to contain its own glass wall and floor, they are each “slices” of the same planar objects. Despite these chal-lenges, humans readily understand spaces like these from only a few ordinary photos, such as when they share collec-tions of photos from the same event or look for housing.
Yet this setting poses challenges for today’s computer vision methods. Traditional tools from multi-view geom-etry [18, 1] largely rely on correspondence for reconstruc-tion and are fundamentally limited to the small part of the scene that directly overlaps, even when the camera pose is known. Learning-based single view 3D [63, 28], offers ways of reconstructing each image, but produces two messy piles of partial reconstructions due to the unknown view-point change. In Figure 1, the floor is fragmented across both views and the back of the chair is present in only one view. While humans can associate these pieces and infer their relative positions to produce a coherent reconstruction, it is not trivial for today’s reconstruction algorithms.
We believe the ease with which humans solve the two unknown camera reconstruction problem, coupled with the difficulty it poses for computers marks it as an important task on the path to human-level 3D perception.
Indeed, it poses challenges to existing work in deep learning for multiview reconstruction, which typically requires known camera poses [23, 31] as opposed to unknown poses, many views [21] as opposed to two, additional depth information at test time [65, 62] as opposed to RGB images, or works only on synthetic data [37]. Typically the extra information used is fundamental to the algorithm (e.g., using poses for triangulation) and cannot be removed to produce a method that works in the two unknown view reconstruction settings.
We propose a learning-based approach that constructs a coherent 3D reconstruction from two views with an un-known relationship. Our insight, supported empirically, is that progress can be made by jointly tackling three related challenges: per-view reconstruction, inter-view correspon-dence, and inter-view 6DOF (rotation and translation) pose.
Throughout, we use plane segments as our representation since they have simple parameters, are often good approxi-mations [14], and there is a strong line of work for estimat-ing them [29, 28] or related properties [13, 9] from images.
Our method, described in Section 3, combines a deep neural network architecture and an optimization problem to jointly estimate planes, their relationships, and cam-era transformations. Our architecture builds on PlaneR-CNN [28] to produce, per-input, plane segments and param-eters, per-plane embeddings for correspondences, as well as a probability distribution over relative cameras. This infor-mation is used in a discrete-continuous optimization prob-lem (along with optional point features) to produce a co-herent reconstruction across views. This reasoning across views enables our approach to, for instance in Figure 1, produce a single floor rather than a set of inconsistent floor fragments, and jointly infer the distance from the images to the scene boundaries as well as the relative camera pose.
We validate our approach on realistic renderings from the Matterport3D [4] dataset using pairs with limited over-lap (average 53◦ rotation, 2.3m translation, 21% overlap).
We report experimental results in Section 4 for three tasks: producing a single coherent reconstruction from the two views, matching planes across views, and estimating the full 6DOF relative camera pose. We compare extensively with a variety of baselines (e.g., adding an independent net-work to estimate relative pose followed by fusion of the two scene layouts) and ablations that test the contributions of our method and design choices. Our results demonstrate the value of joint consideration of the interrelated problems: our approach substantially outperforms the fusion of exist-ing approaches to the independent problems of camera pose estimation and scene structure estimation. 2.