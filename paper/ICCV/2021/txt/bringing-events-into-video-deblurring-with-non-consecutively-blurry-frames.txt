Abstract
Recently, video deblurring has attracted considerable research attention, and several works suggest that events at high time rate can beneﬁt deblurring. Existing video deblurring methods assume consecutively blurry frames, while neglecting the fact that sharp frames usually appear
In this paper, we develop a princi-nearby blurry frame. pled framework D2Nets for video deblurring to exploit non-consecutively blurry frames, and propose a ﬂexible event fu-sion module (EFM) to bridge the gap between event-driven and video deblurring. In D2Nets, we propose to ﬁrst de-tect nearest sharp frames (NSFs) using a bidirectional LST-M detector, and then perform deblurring guided by NSFs.
Furthermore, the proposed EFM is ﬂexible to be incorpo-rated into D2Nets, in which events can be leveraged to no-tably boost the deblurring performance. EFM can also be easily incorporated into existing deblurring networks, mak-ing event-driven deblurring task beneﬁt from state-of-the-art deblurring methods. On synthetic and real-world blurry datasets, our methods achieve better results than competing methods, and EFM not only beneﬁts D2Nets but also signif-icantly improves the competing deblurring networks. 1.

Introduction
Videos have played the crucial role in computer vision
ﬁeld, and blur is commonly inevitable due to the movement of camera or moving objects in the capturing scene. To rem-edy the adverse effects of blur, video deblurring has drawn considerable research attention in many applications, e.g.,
SLAM [12], 3D reconstruction [29] and tracking [36]. In recent years, event camera [3,22], a novel sensor for record-ing intensity changes of the capturing scene at microsecond level, has been developed, and events with high time rate are also suggested to facilitate deblurring [8, 25].
Both video deblurring [7, 9, 19] and event-driven deblur-∗Corresponding author: rendongweihit@gmail.com
Blurry Frame
DMPHN [37]
Corresponding Events
STFAN [39]
CDVD-TSP [19]
D2Nets*
Figure 1. Deblurring results by our D2Nets*, state-of-the-art im-age deblurring DMPHN [37] and video deblurring STFAN [39] and CDVD-TSP [19]. ring [8, 20, 24, 25, 25, 33] have achieved unprecedented pro-gresses, but they still have limitations. On the one hand, existing video deblurring networks are usually based on the assumption of consecutively blurry frames in a video, and design CNN-based [4, 11, 19, 30, 35, 38] and RNN-based
[7, 18, 32] architectures, among which encoder-decoder ar-chitecture is the most popular choice to act as the the basic backbone. However, it is a common fact that blur does not consecutively occur in videos, i.e., some frames in a blur-ry video are extremely sharp and clean [27]. These sharp frames actually can be exploited to facilitate the restoration of blurry frames, but they are indistinguishably processed in existing video deblurring methods, also adversely yielding sharp textures lost. On the other hand, event-driven restora-tion methods heavily rely on the employment of events, where various architectures such as BHA [20], CIE [28] and EDMD [8] are designed. In these methods, the modules for exploiting events are not easy to cooperate with existing image and video deblurring methods, thus restraining the development of principled framework for video deblurring
and event-driven deblurring.
In this paper, we ﬁrst develop a principled framework (i.e., Detect&Deblur Netwotks, D2Nets) to leverage non-consecutively blurry frames, and then propose an event fu-sion module (EFM) to bridge the gap between event-driven and video deblurring. First, our D2Nets consists of three steps: (i) We propose to distinguish sharp frames and blurry frames using a bidirectional LSTM (BiLSTM) [6] as shown in Fig. 2, based on which two nearest sharp frames (NS-Fs) can be found for a blurry frame in the front and rear directions. BiLSTM can take as input either frames or their corresponding events. (ii) As shown in Fig. 3, blurry frames can then be restored to reconstruct latent sharp video frames using an encoder-decoder deblurring backbone, where NS-Fs are employed to guide the deblurring of a blurry frame instead of its neighboring frames. (iii) We further suggest to enhance the temporal consistency of restored video by a post-processing step, which is also beneﬁcial to possibly surviving blurry frames due to detection errors by BiLSTM.
Second, the proposed EFM simultaneously exploits ben-eﬁts from events and adjacent frames, and can be incorpo-rated into the latent space of encoder-decoder architecture.
EFM can then be incorporated into both steps of blurry frames restoration and temporal consistency enhancement in D2Nets, bridging the gap between event-driven and video deblurring, resulting in D2Nets*. Moreover, our EFM can be incorporated into existing state-of-the-art deblurring net-works, e.g., DMPHN [37], STFAN [39] and CDVD-TSP
[19], making event-driven deblurring can beneﬁt from these state-of-the-art image and video deblurring methods. As shown in Fig. 1, existing video deblurring methods can-not fully remove severe blur, while our D2Nets* is able to restore more visually plausible deblurring result.
Experiments have been conducted on two benchmark datasets, including GoPro dataset [17] and Blur-DVS dataset [8] captured by DAVIS240C camera [3]. By exploit-ing NSFs, sharp textures from NSFs can better facilitate re-constructing latent clean frames, leading to notable gains by our D2Net over state-of-the-art deblurring methods. The proposed EFM not only beneﬁts our D2Nets, but also sig-niﬁcantly improves competing methods when collaborating with events to tackle video deblurring.
The contributions of this work are three-fold:
• A principled deblurring framework D2Nets is devel-oped to exploit nearest sharp frames when restoring blurry frames in a non-consecutively blurry video.
• An event fusion module EFM is proposed to better u-tilize beneﬁcial information from events to facilitate deblurring.
• Our EFM has also been incorporated into existing im-age and video deblurring methods for tackling event-driven video deblurring. Extensive experiments are
Figure 2. The architecture of BiLSTM detector for distinguish-ing sharp frames from blurry frames. The BiLSTM detector takes 5 adjacent frames as input, which can be either video frame se-quence Bi−2, · · · , Bi, · · · , Bi+2 or their corresponding events
Ei−2, · · · , Ei, · · · , Ei+2. conducted to validate the effectiveness of D2Nets and
EFM for synthetic and real-world blurry videos. 2.