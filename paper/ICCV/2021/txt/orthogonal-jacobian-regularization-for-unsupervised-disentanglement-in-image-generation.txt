Abstract
Unsupervised disentanglement learning is a crucial is-sue for understanding and exploiting deep generative mod-els. Recently, SeFa tries to find latent disentangled direc-tions by performing SVD on the first projection of a pre-trained GAN. However, it is only applied to the first layer and works in a post-processing way. Hessian Penalty mini-mizes the off-diagonal entries of the output’s Hessian ma-trix to facilitate disentanglement, and can be applied to multi-layers. However, it constrains each entry of output independently, making it not sufficient in disentangling the latent directions (e.g., shape, size, rotation, etc.) of spa-tially correlated variations. In this paper, we propose a sim-ple Orthogonal Jacobian Regularization (OroJaR) to en-courage deep generative model to learn disentangled rep-resentations. It simply encourages the variation of output caused by perturbations on different latent dimensions to be orthogonal, and the Jacobian with respect to the in-put is calculated to represent this variation. We show that our OroJaR also encourages the output’s Hessian matrix to be diagonal in an indirect manner.
In contrast to the
Hessian Penalty, our OroJaR constrains the output in a holistic way, making it very effective in disentangling la-tent dimensions corresponding to spatially correlated vari-ations. Quantitative and qualitative experimental results show that our method is effective in disentangled and con-trollable image generation, and performs favorably against the state-of-the-art methods. Our code is available at https://github.com/csyxwei/OroJaR. 1.

Introduction
In a disentangled representation, each dimension corre-sponds to the change in one factor of variation (FOV), while being independent to changes in other factors [3]. Learn-ing disentangled representations from a given dataset is a
*This work was done when Yuxiang Wei was a research intern at TAL
Figure 1: Examples of orthonormal directions learned by our method in BigGAN conditioned to synthesize ImageNet
Golden Retrievers or Churches. Moving across a row, we move a latent code along a single linear direction in z-space. major challenge in artificial intelligence, and can be benefi-cial to many computer vision tasks, such as domain adapta-tion [33, 45], controllable image generation [32, 38, 41, 48], and image manipulation [37].
In the recent few years, unsupervised disentanglement learning has attracted intensive attention, owing to its im-portance in understanding generative models [32, 38] and extensive applications in various vision tasks [37, 45].
Based on two representative generative models, i.e. Vari-ational Autoencoder (VAE) [26] and Generative Adversar-ial Networks (GAN) [12], many disentanglement methods
[6, 7, 11, 15, 17, 25, 32, 38, 48] have been proposed. VAE-based methods, such as β-VAE [15], FactorVAE [25], β-TCVAE [6], etc., attain disentanglement mainly by enforc-ing the independence in the latent variables. However, their disentanglement performance and the visual quality of generated images remain quite limited. With the progress in Generative Adversarial Networks (GAN) [12], many
GAN-based disentanglement methods have been proposed
[7,32,38,48]. SeFa [38] learns the disentangled latent direc-tions by directly decomposing the weight of the first fully-connected layer of a pre-trained GAN. However, it is only applied to the first layer of the generator model and works in a post-processing way, which limits the performance of dis-1
entanglement. Hessian Penalty [32] encourages to learn a disentangled representation by minimizing the off-diagonal entries of the output’s Hessian matrix with respect to its in-put. However, it uses a max function to extend the reg-ularization from scalar-valued functions to vector-valued functions, yet treats each entry of the output independently, making it not sufficient in disentangling the latent directions (e.g., shape, size, rotation, etc.) corresponding to spatially correlated variations.
Inspired by Hessian Penalty [32] and SeFa [38], we pro-pose a simple regularization term to encourage the gen-erative model to learn disentangled representations. Our method is based on a straightforward intuition: when per-turbing a single dimension of the network input, we would like the change in the output to be independent (and also uncorrelated) with those caused by the other input dimen-sions. To this end, the output’s Jacobian matrix is calcu-lated to represent the change caused by the latent input.
To encourage the changes caused by different latent dimen-sions to be uncorrelated, we simply constrain the Jacobian vector of each dimension to be orthogonal. In contrast to
Hessian Penalty, we constrain the change in a holistic way, thereby making it very competitive in disentangling latent dimensions corresponding to spatially correlated variations.
We call this regularization term as Orthogonal Jacobian
Regularization (OroJaR). In Sec. 3.4, we show that our
OroJaR also constrains the Hessian matrix to be diagonal in an indirect way. On the other hand, our OroJaR can be treated as an end-to-end generalization of SeFa on multi-ple layers, which is also beneficial to the disentanglement performance. In practice, due to the fact that computing the
Jacobian matrices during training is time consuming, we ap-proximate it via a first-order finite difference approximation to accelerate training.
Experiments show that our OroJaR performs favorably against the state-of-the-art methods [32, 38, 48] for unsu-pervised disentanglement learning on three datasets (i.e.,
Edges+Shoes [46], CLEVR [32], and Dsrpites [29]). More-over, our OroJaR can be used to explore directions of mean-ingful variation in the latent space of pre-trained generators.
From Fig. 1, our method is effective in finding the disentan-gled latent directions (e.g., rotation, zoom and color, etc.) in
BigGAN pre-trained on ImageNet.
The contributions of this work can be summarized as:
• We present a simple Orthogonal Jacobian Regulariza-tion (OroJaR) to encourage the deep generative model to learn better disentangled representations.
• OroJaR can be applied to multiple layers of the gener-ator, constrains the output in a holistic way, and indi-rectly encourages the Hessian matrix to be diagonal.
• Extensive experiments show the effectiveness of our proposed method in learning and exploring disentan-gled representations, especially those corresponding to spatially correlated variations. 2.