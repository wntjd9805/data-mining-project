Abstract
While CNN-based models have made remarkable progress on human pose estimation, what spatial depen-dencies they capture to localize keypoints remains un-In this work, we propose a model called Trans-clear.
Pose, which introduces Transformer for human pose esti-mation. The attention layers built in Transformer enable our model to capture long-range relationships efficiently and also can reveal what dependencies the predicted key-points rely on. To predict keypoint heatmaps, the last at-tention layer acts as an aggregator, which collects contri-butions from image clues and forms maximum positions of keypoints. Such a heatmap-based localization approach via
Transformer conforms to the principle of Activation Maxi-mization [19]. And the revealed dependencies are image-specific and fine-grained, which also can provide evidence of how the model handles special cases, e.g., occlusion.
The experiments show that TransPose achieves 75.8 AP and 75.0 AP on COCO validation and test-dev sets, while being more lightweight and faster than mainstream CNN archi-tectures. The TransPose model also transfers very well on
MPII benchmark, achieving superior performance on the test set when fine-tuned with small training costs. Code and pre-trained models are publicly available1. 1.

Introduction
Deep convolutional neural networks have achieved im-pressive performances in the field of human pose estima-tion. DeepPose [56] is the early classic method, directly regressing the numerical coordinate locations of keypoints.
Afterwards, fully convolutional networks like [60, 36, 38, 63, 12, 40, 61, 51] have become the mainstream by pre-dicting keypoints heatmaps, which implicitly learn spatial dependencies between body parts. Yet, most prior works take deep CNN as a powerful black box predictor and focus on improving the network structure, what exactly happens inside the models or how they capture the spatial relation-ships between body parts remains unclear. However, from
*Corresponding author. 1https://github.com/yangsenius/TransPose
Figure 1. A schematic diagram of TransPose. Below: The infer-ence pipeline. Above: Dependency areas for each predicted key-point location. In this example, the person’s left-ankle is occluded by a dog. Which exact image clues the model uses to infer the occluded joint? The attention map (red box) gives fine-grained evidence beyond intuition: such a pose estimator highly relies on the image clues around the left ankle, left upper leg, and joints on the right leg to estimate the location of occluded left ankle. the scientific and practical standpoints, the interpretability of the model can aid practitioners the ability to understand how the model associates structural variables to reach the final predictions and how a pose estimator handles various input images. It also can help model developers for debug-ging, decision-making, and further improving the design.
For existing pose estimators, some issues make it chal-(1) Deep-lenging to figure out their decision processes. ness. The CNN-based models, such as [60, 38, 61, 51], are usually very deep non-linear models that hinder the in-terpretation of the function of each layer. (2) Implicit re-lationships. The global spatial relationships between body parts are implicitly encoded within the neuron activations and the weights of CNNs. It is not easy to decouple such relationships from large amounts of weights and activations in neural networks. And solely visualizing the intermediate features with a large number of channels (e.g. 256, 512 in
SimpleBaseline architecture [61]) provides little meaning-tion layer in Transformer specially acts as an aggregator, which collects different contributions from all image loca-tions by attention scores and finally forms the maximum positions in the heatmaps. This type of keypoint localiza-tion approach via Transformer establishes a connection with the interpretability of Activation Maximization [19, 49] and extends it to the localization task. The resulting attention scores can indicate what concrete image clues significantly contribute to the predicted locations. With such evidence, we can further analyze the behaviors of the model by exam-ining the influence of different experimental variables. In summary, our contributions are as follow:
• We introduce Transformer for human pose estimation to predict heatmap-based keypoints positions, which can efficiently capture the spatial relationships be-tween human body parts.
• We demonstrate that our keypoint localization ap-proach based on Transformer conforms to the inter-pretability of Activation Maximization [19, 49]. Qual-itative analysis reveals the dependencies beyond intu-ition, which are image-specific and fine-grained.
• TransPose models achieve competitive performances against state-of-the-arts CNN-based models via fewer parameters and faster speeds. TransPose achieves 75.8
AP and 75.0 AP on COCO validation set and test-dev set, with 73% fewer parameters and 1.4× faster than
HRNet-W48.
In addition, our model transfers very well on MPII benchmark. 2.