Abstract
Training a neural network model for recognizing mul-tiple labels associated with an image, including identify-ing unseen labels, is challenging, especially for images that portray numerous semantically diverse labels. As challeng-ing as this task is, it is an essential task to tackle since it represents many real-world cases, such as image retrieval of natural images. We argue that using a single embed-ding vector to represent an image, as commonly practiced, is not sufficient to rank both relevant seen and unseen la-bels accurately. This study introduces an end-to-end model training for multi-label zero-shot learning that supports the semantic diversity of the images and labels. We propose to use an embedding matrix having principal embedding vectors trained using a tailored loss function. In addition, during training, we suggest up-weighting in the loss func-tion image samples presenting higher semantic diversity to encourage the diversity of the embedding matrix. Exten-sive experiments show that our proposed method improves the zero-shot model’s quality in tag-based image retrieval achieving SoTA results on several common datasets (NUS-Wide, COCO, Open Images). 1.

Introduction
Identifying all the relevant labels that describe the ob-jects or scene in an image is an essential task in computer vision real-world applications. With the ongoing increase of photos stored online comes a growing need for better image tagging and tag-based retrieval for various use cases such as search, organization, or data collection. Recent datasets in this field made progress in this direction by including a large number of classes with annotations of their presence in each image. Yet, annotating a large number of classes for many images, each with high semantic diversity, can be very time-consuming and practically infeasible for real-world applications. Although current conventional multi-label classification methods can deal with a large number of classes, they are still limited by the annotated (seen) set of
Figure 1: Our model extracts a set of principal embedding vectors used as a transformation matrix Ai where each row sets a ranking principal direction for labels in the word vec-tor space based on their relevance. By using multiple di-rections, it can deal with multiple diverse image semantic concepts. In addition, we propose that images with larger semantic diversity (top images) should be up-weighted dur-ing training compared to ones with lower semantic diversity (bottom images). labels provided with the datasets.
On the other hand, Zero-shot (ZS) learning for multi-label classification adds the ability to recognize labels from additional categories that do not exist during training (un-seen). This is usually done by transferring knowledge be-tween the seen and unseen labels.
In most cases, a text model [5, 32, 36] is used to transfer this knowledge using word vectors. Then, a visual model is trained, learning a transformation between the visual space and the word vec-tor label space.
Most studies on ZS classification focus on the single la-bel problem, i.e., recognizing a single unseen label in each image [1, 11, 14, 25, 29, 34, 50, 57, 53, 51, 52]. However, a single label per image does not provide a full description
of it and usually is not practical for real-world applications.
Other studies tried to tackle the ZS multi-label classification problem.
[35] trained a multi-label classifier on the seen classes and linearly combined word vectors of these classes using the prediction scores to represent an image. Based on that representative vector, the ranking of unseen labels was done by computing similarities to their word vectors. [58] trained a network to output per-image a single principal di-rection that ranks relevant labels higher than non-relevant ones. However, this method faces difficulties with images that include multiple labels with high semantic diversity. In these cases, the extracted principal direction needs to be ro-bust to high variability in the relevant labels’ word vectors.
For example, the classes ”dog” and ”rice” are quite dif-ferent, but might still be present in the same image. As we grow with the number of annotated classes in images, the probable semantic diversity in each image grows. This high semantic diversity problem requires special treatment, which is hard to achieve using a single principal direction.
Several works had approached the problem of high se-mantic diversity of the labels in an image using a pre-trained object detector and learning to select bounding boxes of seen or unseen labels [3, 37, 38, 39]. Yet, these approaches require annotated bounding boxes as ground truth, making it not scalable for a large number of labels. Alternatively,
[21] used attention techniques to estimate the relevant re-gions based on a pre-trained model’s features. However, this usually requires a large pre-trained model to get rich regional features (VGG-19) and a complex loss function to be tuned.
In this paper, we propose a method that aims to prop-erly cope and leverage the semantic diversity of the labels in each image, by allowing multiple principal directions, constructed as a transformation matrix in the loss function.
Also, sample images with larger semantic diversity are up-weighted in the loss function as these images are considered hard examples. As a result, our model learns to extract a per-image transformation designed to handle the image la-bel diversity challenges (Figure 1). We believe that by doing so, we learn a model that is better suited for understanding and recognizing multiple seen and unseen labels in an im-age.
We further show how we achieve results that are on par or better than SotA while keeping a relatively simple end-to-end training scheme using our suggested loss function.
The main contributions presented in this study include:
• A loss function tailored to the problem of ZS multi-label classification.
• We show that up-weighting samples with higher se-mantic diversity further improves the model general-ization.
• An efficient end-to-end training scheme for ZS models is proposed, reaching SotA results in tag-based image retrieval while keeping high-performance for image tagging on several datasets (NUS-Wide, Open-Images, and MS-COCO) with a smaller number of model pa-rameters compared to other methods. 2.