Abstract
Domain adaptation is critical for success when con-fronting with the lack of annotations in a new domain. As the huge time consumption of labeling process on 3D point cloud, domain adaptation for 3D semantic segmentation is of great expectation. With the rise of multi-modal datasets, large amount of 2D images are accessible besides 3D point clouds. In light of this, we propose to further leverage 2D data for 3D domain adaptation by intra and inter domain cross modal learning. As for intra-domain cross modal learning, most existing works sample the dense 2D pixel-wise features into the same size with sparse 3D point-wise features, resulting in the abandon of numerous useful 2D features. To address this problem, we propose Dynamic sparse-to-dense Cross Modal Learning (DsCML) to in-crease the sufﬁciency of multi-modality information interac-tion for domain adaptation. For inter-domain cross modal learning, we further advance Cross Modal Adversarial
Learning (CMAL) on 2D and 3D data which contains dif-ferent semantic content aiming to promote high-level modal complementarity. We evaluate our model under various multi-modality domain adaptation settings including day-to-night, country-to-country and dataset-to-dataset, brings large improvements over both uni-modal and multi-modal domain adaptation methods on all settings. Code is avail-able at https://github.com/leolyj/DsCML 1.

Introduction real-world applications, 3D semantic segmentation is a challenging task with plenty of such as particular robotics, autonomous driving and virtual reality. Like other tasks of scene perception, 3D semantic segmentation also faces the challenge of domain shift. For instance, training on one country and testing on another and in different times
*Corresponding Author: Yinjie Lei (yinjie@scu.edu.cn) 2D Image (H,W,3) 3D Point Cloud (N,3) 2D Network
Projection 3D Network 2D Prediction Map (H,W,C) 3D Prediction Map (1,N,C) sampling
Learning 
Loss
Figure 1. The common strategy of feature processing for 2D-3D cross modal learning. The 2D dense feature map with dense pixel-wise features are sampled to sparse features with the same size of 3D point features. As a result, such sparse-to-sparse feature matching only leverages quite limited 2D features and might cause insufﬁcient 2D-3D information interaction. Speciﬁcally, H and W are the height and width of 2D image respectively. N denotes the number of points in point cloud. C is the number of categories for semantic segmentation. of the day may lead to signiﬁcantly dreadful performance.
Plenty of domain adaptation methods are proposed to ad-dress such domain shift on the task of 2D semantic segmen-tation [32, 18, 17, 34, 25, 45] but rarely on 3D [44].
In recent works for multi-modality datasets creation, re-searchers often incidentally capture 2D images as counter-part when executing the data collection of 3D point clouds.
In light of this, Jaritz et al. [22] proposes a cross modal learning method to address domain adaptation for 3D se-mantic segmentation. Through the complementary advan-tages between 2D and 3D data, the multi-modality domain
adaptation can bring large improvements over uni-modal adaptation methods. And the 2D images are leveraged with-out labels which means no additional human efforts in label-ing work.
While this work [22] already explored multi-modality in domain adaptation, as shown in Fig. 1, it executes the learning between 2D and 3D only towards the matched fea-tures by projection from 3D points to 2D image. Multi-tudes of mismatched features which also contain useful in-formation are discarded. We consider whether 2D and 3D features can be more sufﬁciently utilized in cross modal learning. This is challenging as the two inputs are hetero-geneous and contain different number of elements. Com-pared to the sparse 3D point clouds, pixels in 2D image are dense and with compact layout. Even if all features of 3D point cloud well matched with the corresponding 2D pixel-wise features, there can be still lots of 2D features are mismatched. That is why numerous works of other percep-tion tasks [37, 9, 24, 6, 21, 26, 39] also capitalize on multi-modality in the same way with [22], i.e., sparse-to-sparse feature matching.
To address such limitation, we propose a strategy namely
Dynamic sparse-to-dense Cross Modal Learning (DsCML) where the sparse point cloud features and dense pixel fea-tures can sufﬁciently interact with each other. Speciﬁcally, the proposed DsCML is inspired from the fact that in 2D semantic segmentation, the neighboring pixels are mostly classiﬁed to a same category. And all the same-categorized pixel features should be sampled to exchange information with the corresponding 3D point-wise feature. For each 3D point-wise feature, DsCML can dynamically capture the re-lated multiple 2D pixel-wise features with same category.
This introduces much richer context information of texture and color in 2D image which is complementary to space in-formation of 3D point cloud. Additionally, a novel sparse-to-dense learning loss is proposed to support the learning of multi-modality where 2D and 3D features differ by orders of magnitude. This DsCML is applied on source and target domain alternately which is the key to domain adaptation.
The method mentioned above is adopted in an intra-domain manner where the 2D and 3D data contain same semantic content. In this paper, we further explore inter-domain cross modal learning for high-level semantic inter-action of multi-modality data with different semantic con-tent. Speciﬁcally, we introduce cross modal learning to common adversarial strategy by adding discriminator for identiﬁcation between 2D and 3D features. We coin our method Cross Modal Adversarial Learning (CMAL). It en-ables the mutual learning between 2D and 3D as well as the alignment of feature distribution from different domains, which is the other key to domain adaptation.
The main contributions of this paper are summarized as follows:
• To the best of our knowledge, this is the ﬁrst work to explore cross modal learning in both intra and inter do-main for the semantic segmentation problem.
• As for intra-domain cross modal learning, we propose a module named DsCML to establish sufﬁcient re-lationships of multi-modality features, i.e., sparse-to-dense feature matching.
• As for inter-domain cross modal learning, we pro-pose a method named CMAL to achieve both high-level cross modal interaction and cross domain feature alignment.
• The proposed method is evaluated on various real-to-real adaptation settings (i.e., day-to-night, country-to-country and dataset-to-dataset), obtaining state-of-the-art segmentation performance with both uni-modal and multi-modal methods. 2.