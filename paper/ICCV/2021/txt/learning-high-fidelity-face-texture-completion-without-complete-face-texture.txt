Abstract 1.

Introduction
For face texture completion, previous methods typically use some complete textures captured by multiview imaging systems or 3D scanners for supervised learning. This paper deals with a new challenging problem – learning to com-plete invisible texture in a single face image without using any complete texture. We simply leverage a large corpus of face images of different subjects (e. g., FFHQ) to train a texture completion model in an unsupervised manner. To achieve this, we propose DSD-GAN, a novel deep neural network based method that applies two discriminators in
UV map space and image space. These two discriminators work in a complementary manner to learn both facial struc-tures and texture details. We show that their combination is essential to obtain high-fidelity results. Despite the network never sees any complete facial appearance, it is able to gen-erate compelling full textures from single images.
Human face analysis and digitization are among the most popular topics in computer vision and graphics. Most face photos we take do not represent a full view of a face due to occlusion by the face itself or other objects. In fact, self-occlusion is ubiquitous for face images in the wild as shown by the examples in Fig. 1, leading to invisible texture con-tent. The task of face texture completion is to infer the in-visible face content and recover full-face appearance. It has a wide variety of applications ranging from 3D avatar cre-ation [14, 20], 3D morphable model construction [2, 31], and face image manipulation [13, 36], to high-level vision tasks such as face recognition [13, 6]. This work is devoted to texture1 completion learning using deep neural networks for single face images.
However, learning face texture completion is not 1Following [6], we use texture to refer to a facial appearance on an image, not an albedo or intrinsic image.
straightforward due to the difficulty of collecting labeled training data. For single images, obtaining complete tex-ture by manually labeling or painting is not a viable op-tion. Using multi-view images to obtain high-resolution and high-quality textures is also not a trivial task, which ne-cessitates sophisticated face image capture and processing pipelines. Previous works often use some special devices placed in controlled environments for training data capture, such as a multi-view DSLR capturing system or a 3D scan-ning system [4, 37, 6, 20]. Most datasets from these works are not publicly available. Note that most of them capture face albedo which is different from the texture we consider in this paper, but the requirements for face scanning, regis-tration, and stitching are similar for obtaining both.
To step aside from the effort for labeled data collection, we propose using a large collection of high-resolution face photos captured in unconstrained settings to train a face tex-ture completion model. Although this would eliminate the need for complete texture acquisition, it poses new chal-lenges for the learning task, since, for each input face im-age, there’s no image of the same person that can be used for supervision.
To this end, we propose a novel generative adversarial learning method called Dual-Space-Discriminator Genera-tive Adversarial Network (DSD-GAN) to learn face texture completion in an unsupervised fashion. To make full use of the partially visible face appearance, we apply two dis-criminators designed differently in the UV texture space and image space. Our key observation is that the former is more suitable to learn texture details, whereas the latter is more important to learn facial structure.
In the UV texture space, a discriminator takes small lo-cal patches as input to focus on detailed textural patterns.
Real and fake patches are obtained on one image accord-ing to their visibility. Since the discriminator focuses on local patches, it may get stuck into local minima and ig-nore global textural consistency. Therefore, we make the lo-cal discriminator conditioned on the patch coordinates and train it to regress these coordinates. On the other hand, we employ a differentiable mesh renderer [10] to convert the generated UV texture into the image space and apply a dis-criminator taking the full image as input. The goal here is to capture the general face structure, core semantic compo-nents, and overall color gradient caused by different illumi-nation conditions. The raw face photos are used as real sam-ples, which naturally provide reliable training signals. To make the discriminator focus more on missing regions and avoid confusing it when missing regions are small, we ap-ply spatially-varying labels for more effective training. We show that our dual space discriminators are very effective for the unsupervised learning task and the trained generator can produce high-fidelity face texture completion results. 2.