Abstract
Dark environment becomes a challenge for computer vision algorithms owing to insufficient photons and un-desirable noise. To enhance object detection in a dark environment, we propose a novel multitask auto encod-ing transformation (MAET) model which is able to ex-plore the intrinsic pattern behind illumination translation. the MAET learns the in-In a self-supervision manner, trinsic visual structure by encoding and decoding the re-alistic illumination-degrading transformation considering the physical noise model and image signal processing (ISP). Based on this representation, we achieve the ob-ject detection task by decoding the bounding box coor-dinates and classes. To avoid the over-entanglement of two tasks, our MAET disentangles the object and degrad-ing features by imposing an orthogonal tangent regular-ity. This forms a parametric manifold along which multi-task predictions can be geometrically formulated by max-imizing the orthogonality between the tangents along the outputs of respective tasks. Our framework can be im-plemented based on the mainstream object detection ar-chitecture and directly trained end-to-end using normal target detection datasets, such as VOC and COCO. We have achieved the state-of-the-art performance using syn-thetic and real-world datasets. Codes will be released at https://github.com/cuiziteng/MAET. 1.

Introduction
Low-illumination environment poses significant chal-lenges in computer vision. Computational photography community has proposed many human-vision-oriented al-gorithms to recover normal-lit images [26, 46, 28, 27, 4, 20, 51, 12, 8]. Unfortunately, the restored image does not nec-essarily benefit the high-level visual understanding tasks.
As the enhancement/restoration approaches are optimized for human visual perception, they may generate artifacts
*Corresponding author.
Figure 1. Detection and enhancement results of image taken by
Sony DSC-RX100M7 camera at night with 0.1s exposure time and 3200 ISO. (a) is detection result on the original image by MAET (YOLOv3) and (b), (c), (d) are the enhanced images by Lv et al.
[28], Zhang et al.
[8] respectively, on which
YOLOv3 failed to make detection.
[51], Guo et al. (see Fig. 1 for an example), which are misleading for conse-quent vision tasks. Another line of research focuses on the robustness of specific high-level vision algorithms. They either train models on a large volume of real-world data
[31, 25, 48] or rely on carefully designed task-related fea-tures [29, 17].
However, existing methods suffer from two major incon-sistencies: target inconsistency and data inconsistency (in the existing research). Target inconsistency refers to the fact that most methods focus on their own target, either human vision or machine vision. Each line follows their routes sep-arately without benefiting each other under a general frame-work.
In the meantime, data inconsistency complicates the as-sumption that the training data should resemble the one used
improves the detection of dark objects using MAET regu-larity, it may also risk overfitting the object-level represen-tation into self-supervisory imaging signals. To this end, we propose to disentangle the object detection and transfor-mation decoding tasks by imposing an orthogonal tangent regularity. It assumes that the multivariate outputs of above two tasks form a parametric manifold, and disentangling the multitask outputs along the manifold can be geometrically formulated by maximizing the orthogonality among the tan-gents along the output of different tasks. The framework can be directly trained end-to-end using standard target de-tection datasets, such as COCO [23] and VOC [6], and make it detect low-light images. Although we consider YOLOv3
[39] for illustration, the proposed MAET is a general frame-work that can be easily applied to other mainstream object detectors, e.g., [40, 22, 54].
Our contributions to this study are as follows:
• By exploring physical noise models of sensors and the
ISP pipeline, we leverage a novel MAET framework to encode the intrinsic structure, which can decode low-light-degrading transformation. Then, we perform the object detection by decoding bounding box coordi-nates and categories based on this robust representa-tion. Our MAET framework is compatible with main-stream object architectures.
• Moreover, we present the disentangling of multitask outputs to avoid the overfitting of the learned object-detection features into the self-supervisory degrading parameters. This can be naturally performed from a geometric perspective by maximizing the orthogonal-ity along the tangents corresponding to the output of different tasks.
• Based on comprehensive evaluation and compared with other methods, our method shows superior perfor-mance pertaining to low-light object detection tasks. 2.