Abstract
In recent years, deep learning moves video-based Con-tinuous Sign Language Recognition (CSLR) significantly forward. Currently, a typical network combination for
CSLR includes a visual module, which focuses on spatial and short-temporal information, followed by a contextual module, which focuses on long-temporal information, and the Connectionist Temporal Classification (CTC) loss is adopted to train the network. However, due to the limita-tion of chain rules in back-propagation, the visual module is hard to adjust for seeking optimized visual features. As a result, it enforces that the contextual module focuses on con-textual information optimization only rather than balancing efficient visual and contextual information. In this paper, we propose a Self-Mutual Knowledge Distillation (SMKD) method, which enforces the visual and contextual modules to focus on short-term and long-term information and en-hances the discriminative power of both modules simultane-ously. Specifically, the visual and contextual modules share the weights of their corresponding classifiers, and train with
CTC loss simultaneously. Moreover, the spike phenomenon widely exists with CTC loss. Although it can help us choose a few of the key frames of a gloss, it does drop other frames in a gloss and makes the visual feature saturation in the early stage. A gloss segmentation is developed to relieve the spike phenomenon and decrease saturation in the vi-sual module. We conduct experiments on two CSLR bench-marks: PHOENIX14 and PHOENIX14-T. Experimental re-sults demonstrate the effectiveness of the SMKD. 1.

Introduction
As spoken language in speaking-hearing person’s daily conversation, sign language plays the most important role for hearing-impaired person’s communication. Sign lan-guage is used by millions of people all over the world.
Different to spoken language, sign language conveys mean-ing by manual elements (e.g., hand configuration) and non-Figure 1. Overview of the proposed SMKD method. With the help of the shared-classifier, the visual and contextual modules are attempted to align the features at gloss level. This makes the two modules focus more on spatial-temporal information. To explore the short-term temporal information, a gloss segmentation is pro-posed into the visual module. manual elements (e.g., facial expressions) [24], and has its own vocabulary and grammar. These characters make speaking-hearing person hard to understand sign language.
Automatic Sign Language Recognition (SLR) provides a bridge to overcome this gap.
Different to the video-based isolated SLR, which rec-ognizes a gloss-wise clip into its corresponding gloss (i.e., written words that represent signs), video-based Continuous
Sign Language Recognition (CSLR) is a much more com-plicated task that aims to translate a sign language video into its corresponding sign gloss sequence. Due to the enor-mous cost of creating frame-level annotations, most CSLR dataset only has sentence-level annotations, and researchers often treat video-based CSLR as a weakly supervised prob-lem [6, 3]. This further increases the difficulty of the task.
To address these problems, some recent works [6, 26] adopt a deep network to deal with the video-based CSLR. The net-work consists of a visual module to extract the short-term spatial-temporal information of the input sequence and fol-lowed by a contextual module to encode the long-term con-textual information. To train the designed network, the Con-[9] loss is used nectionist Temporal Classification (CTC)
to search the alignment between extracted features and the corresponding labeling.
However, end-to-end training makes the visual module hard to learn effective visual features as the penalty is hard to conduct from the contextual module [26, 34]. This makes the contextual module tends to over fit on the contextual information like sequential order of sign action instead of seeking optimize visual information [3]. Representation power of visual module isn’t explored enough. Meanwhile, limited scale of the datasets makes network’s performance decrease quickly on test set. To exploit the visual module’s potential, some works [6, 35] propose to learn explicit vi-sual features by optimizing the visual module directly with an auxiliary task, and these enhancements on the visual fea-tures optimization improve the generalization ability of the whole network. However, it is not a good choice to train vi-sual module independently as that will lose the cooperation between visual and contextual modules as shown in [22].
In this study, we aim to enforce the contextual module to focus more on the visual information and strengthen the dis-criminative capability of the visual module to ensure power-ful visual features. To achieve this, we propose a knowledge distillation method named Self-Mutual Knowledge Distil-lation (SMKD) that lets the visual module and contextual module share the weights of their corresponding classifiers and perform the CTC training simultaneously. The SMKD is inspired by two facts: 1) the CTC loss can be viewed as an iterative softmax loss, as shown in [19]; 2) according to [27], the classifier weight vector can be treated as a proto-type of their respective class, and they can be used to guide the feature learning of the network with softmax loss. Based on these two facts, the weights of the visual and contextual modules are shared initially to enforce them to produce fea-tures as consistent as possible. With the feature alignment at the gloss level, the discriminative power of the visual fea-tures is enhanced, and the contextual module is enforced to focus more on the visual feature sequence.
In addition, as CTC loss will bring the spike phe-nomenon [19, 8], which causes only a few key frames con-tribute to final result and makes the visual module lose its discriminative power for the other frames. To explore the short-term temporal information which is depressed dur-ing CTC constrained training, we further propose to add a gloss segmentation into the visual module, where the pseudo gloss segment label is produced by a proposed Gloss
Segment Boundary Assignment (GSBA) algorithm. Bene-fiting from the above mechanism, the weight vector of each gloss can feed more spatial-temporal information into the contextual module for better generalization capability. An overview of our proposed method is shown in Fig. 1.
Notably, we will decouple the weight matrix sharing be-tween the two modules during the final training stage to relax the constraints on the contextual module, and make it focuses on long-term temporal information. We conduct extensive experiments on two CSLR benchmarks to demon-strate the effectiveness of the SMKD. In summary, the ma-jor contributions of our work are as follows:
• A SMKD method is proposed to enforce the visual and contextual module to focus more on spatial-temporal information, and it also can strengthen the discrimina-tive power of both modules simultaneously.
• A gloss segmentation is developed to relief the spike phenomenon caused by CTC constraint and decrease saturation in visual module during the model training. 2.