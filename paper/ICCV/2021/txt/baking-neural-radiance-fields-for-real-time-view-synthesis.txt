Abstract
Neural volumetric representations such as Neural Ra-diance Fields (NeRF) have emerged as a compelling tech-nique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF’s computa-tional requirements are prohibitive for real-time applica-tions: rendering views from a trained NeRF requires query-ing a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. “bake”) it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF’s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF’s ability to render ﬁne geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen cap-tures are shown in our video. 1.

Introduction
The task of view synthesis — using observed images to recover a 3D scene representation that can render the scene from novel unobserved viewpoints — has recently seen dramatic progress as a result of using neural volumet-In particular, Neural Radiance Fields ric representations. (NeRF) [31] are able to render photorealistic novel views with ﬁne geometric details and realistic view-dependent ap-pearance by representing a scene as a continuous volumetric function, parameterized by a multilayer perceptron (MLP) that maps from a continuous 3D position to the volume density and view-dependent emitted radiance at that loca-tion. Unfortunately, NeRF’s rendering procedure is quite slow: rendering a ray requires querying an MLP hundreds of times, such that rendering a frame at 800 × 800 resolu-tion takes roughly a minute on a modern GPU. This pre-vents NeRF from being used for interactive view synthesis
Figure 1: Our method “bakes” NeRF’s continuous neural volumetric scene representation into a discrete Sparse Neu-ral Radiance Grid (SNeRG) for real-time rendering on com-modity hardware (∼ 65 frames per second on a MacBook
Pro in the example shown here). Our method is more than two orders of magnitude faster than prior work for acceler-ating NeRF’s rendering procedure and more than an order of magnitude faster than the next-fastest alternative (Neural
Volumes) while achieving substantially higher-quality. applications such as virtual and augmented reality, or even simply inspecting a recovered 3D model in a web browser.
We address the problem of rendering a trained NeRF in real-time, see Figure 1. Our approach accelerates render-ing by three orders of magnitude, achieving 12 milliseconds per frame on a single GPU. We precompute and store (i.e.
“bake”) a trained NeRF into a sparse 3D voxel grid data structure, which we call a Sparse Neural Radiance Grid (SNeRG). Each active voxel in a SNeRG contains opac-ity, diffuse color, and a learned feature vector that encodes
view-dependent effects. To render this representation, we
ﬁrst accumulate the diffuse colors and feature vectors along each ray. Next, we pass the accumulated feature vector through a lightweight MLP to produce a view-dependent residual that is added to the accumulated diffuse color.
We introduce two key modiﬁcations to NeRF to effec-tively bake it into this sparse voxel representation: 1) we design a “deferred” NeRF architecture that represents view-dependent effects with an MLP that only runs once per pixel (instead of once per 3D sample as done in NeRF), and 2) we regularize NeRF’s predicted opacity ﬁeld during training to encourage sparsity, which improves both the storage cost and rendering time for the resulting SNeRG.
We demonstrate that our approach is able to increase the rendering speed of NeRF so that frames can be rendered in real-time, while retaining NeRF’s ability to represent ﬁne geometric details and convincing view-dependent effects.
Furthermore, our representation is compact, and requires less than 90 MB on average to represent a scene. 2.