Abstract
Unsupervised pretraining has achieved great success and many recent works have shown unsupervised pretrain-ing can achieve comparable or even slightly better transfer performance than supervised pretraining on downstream target datasets. But in this paper, we find this conclusion may not hold when the target dataset has very few labeled samples for finetuning, i.e., few-label transfer. We analyze the possible reason from the clustering perspective: 1) The clustering quality of target samples is of great importance to few-label transfer; 2) Though contrastive learning is es-sential to learn how to cluster, its clustering quality is still inferior to supervised pretraining due to lack of label super-vision. Based on the analysis, we interestingly discover that only involving some unlabeled target domain into the un-supervised pretraining can improve the clustering quality, subsequently reducing the transfer performance gap with supervised pretraining. This finding also motivates us to propose a new progressive few-label transfer algorithm for real applications, which aims to maximize the transfer per-formance under a limited annotation budget. To support our analysis and proposed method, we conduct extensive exper-iments on nine different target datasets. Experimental re-sults show our proposed method can significantly boost the few-label transfer performance of unsupervised pretraining. 1.

Introduction
Model pretraining plays a key role for deep transfer learning. By pretraining the model on a large auxiliary source dataset and then fine-tuning on the small-scale tar-get dataset, it can achieve better performance than the train-from-scratch counterpart. The recent work BiT [25] has shown that supervised pretraining on large scale source dataset can achieve very strong transfer performance. De-spite the great success of supervised pretraining, a large amount of labeled source data is required. Recently, unsu-pervised pretraining [20, 7, 18, 6, 8, 19] has achieved great
*Equal contribution, † Dongdong Chen is the corresponding author
Figure 1: t-SNE visualization of features on Pet [30] by using dif-ferent models: (a) unsupervised pretrained model, (b)supervised pretrained model, (c) target-aware unsupervised pretrained(TUP) model, (d) finetuned TUP model by using a few labeled samples. progress. By directly pretraining on the larger-scale unla-beled data (e.g., ImageNet), many state-of-the-art (SOTA) unsupervised learning works [7, 18, 19, 6] demonstrate that unsupervised pretraining can achieve comparable or even slightly better transfer performance than supervised pre-training on many downstream target datasets.
In this paper, we ask the question “does unsuper-vised pretraining really achieves comparable transfer per-formance as supervised pretraining?”. And we empirically find the answer is “no” when the downstream target dataset has limited label samples for finetuning, i.e., “few-label transfer”. We seek to investigate the underlying reason from the clustering perspective. We hypothesize that the clustering of target samples in the feature space is of great importance for few-label transfer and unsupervised pre-training has worse clustering quality than supervised pre-training. Intuitively, if the pretrained representation has a very good clustering in the target space, it will only need very few labels to learn a good classifier boundary. To ver-ify our hypothesis, we compare the clustering quality of un-supervised and supervised pretrained models on the target dataset in Figure 1 (a) (b). Obviously, the target samples are better clustered by using the supervised pretrained models.
The following analysis (Table 2) will also show the positive correlation between the clustering quality and the few-label transfer performance.
To understand why unsupervised pretraining has inferior clustering quality, we follow the work [34] to analyze the widely used contrastive loss. Specifically, the contrastive loss can be decomposed into two terms: an alignment term that encourages two samples of a positive pair should be as close as possible, and a uniformity term that encour-ages the learned representation to uniformly distribute on the unit hypersphere. With the alignment term, by using strong augmentation during training, the sub-space of simi-lar images will overlap and be pulled closer. In other words, contrastive learning is trying to cluster the pretraining unla-beled data, but it encourages the learned representation to distribute in the whole space. Therefore, if the target data has a large domain gap with the source data, their feature representations will scatter in the whole space and hard to cluster. By contrast, supervised pretraining does not encour-age the learned representation to be uniformly distributed and the label supervision also provides stronger alignment force across different images. So the learned representation is more compact and better clustered even for the same tar-get domain.
Based on the above analysis, we discover that only in-volving some unlabeled target data into the unsupervised pretraining process (“target-aware”unsupervised pretrain-ing, or TUP) can significantly improve its clustering qual-ity (Figure 1 (c)), thus subsequently reducing the perfor-mance gap with supervised pretraining. This finding is very interesting and useful in real application scenarios where some small-scale unlabeled data is easy to obtain. On the other hand, considering data annotation is often conducted after unlabeled data collection, we further study the ques-tion that “can we leverage the clustering property to max-imize the target performance under a limited annotation budget”. And we propose a simple progressive few-label transfer algorithm for practical usage. Specifically, given the pretrained representation, we first conduct the cluster-ing on the unlabeled target data to find the most representa-tive samples to annotate, and then use the annotated samples to finetune the pretrained model. The finetuned model can further improve the clustering quality (Figure 1 (d)), thus making data annotation and model finetuning form an ac-tive co-evolution loop.
To demonstrate our finding and the proposed method, extensive experiments are conducted on nine different tar-get datasets. The experimental results demonstrate that the proposed method can significantly improve the few-label transfer performance for unsupervised pretraining, and even outperform supervised pretraining. For example, when each target dataset has 10 labeled samples per category, our proposed TUP can boost the average transfer perfor-mance of unsupervised pretraining from 67.49% to 74.15%, slightly better than supervised pretraining 73.27%. By fur-ther equipping our progressive transfer strategy, the transfer performance can increase to 76.69% under the same anno-tation budget. To summarize, our contributions are three-fold: 1) We are the first that points out the few-label transfer gap between unsupervised pretraining and supervised pre-training, which is not studied in the research field yet; 2)
We analyzed the possible underlying reasons and discover a simple and effective strategy for real applications where some small-scale unlabeled data can be collected; 3) We further propose a progressive few-label transfer strategy to boost the performance under the limited annotation budget. 2.