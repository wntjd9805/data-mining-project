Abstract
With recent developments of convolutional neural net-works, deep learning for 3D point clouds has shown sig-niﬁcant progress in various 3D scene understanding tasks, e.g., object recognition, semantic segmentation. In a safety-critical environment, it is however not well understood how such deep learning models are vulnerable to adversarial ex-amples.
In this work, we explore adversarial attacks for point cloud-based neural networks. We propose a uniﬁed formulation for adversarial point cloud generation that can generalise two different attack strategies. Our method gen-erates adversarial examples by attacking the classiﬁcation ability of point cloud-based networks while considering the perceptibility of the examples and ensuring the minimal level of point manipulations. Experimental results show that our method achieves the state-of-the-art performance with higher than 89% and 90% of attack success rate on synthetic and real-world data respectively, while manipu-lating only about 4% of the total points. 1.

Introduction
Deep learning has shown great potentials in solving a wide spectrum of computer vision tasks.
In life-crucial appli-cations, one concern is that deep neural networks can be vulnerable to adversarial examples, a special kind of inputs that can fool the networks to make undesirable predictions.
Several adversarial attack techniques have been proposed to generate such examples. In contrast, adversarial defense methods have been developed to detect and neutralise ad-versarial examples. Therefore, understanding how adver-sarial attacks and defenses operate is of great importance to make deep learning techniques more reliable and robust.
With the growing popularity of low-cost 3D sensors and light-ﬁeld cameras, the community has started investigating the vulnerability of deep networks on 3D data, especially 3D point clouds [40, 19, 20, 43, 36]. However, existing works focus on common scenarios, such as generating ad-versarial point clouds by perturbing points. While such ap-proaches have high attack success rates, the perturbations
Figure 1: Left: input point cloud (1,024 points) classiﬁed correctly by PointNet [27] and 15 selected points (in green).
Right: adversarial point cloud misclassiﬁed by PointNet and perturbed locations (in red). are not imperceptible and can be identiﬁed easily by outlier detection or noise removal algorithms. In addition, existing adversarial attack methods do not perform optimally since all points in a point cloud are involved in the manipulation.
In this work, we study 3D adversarial attack in a more extreme but practical setting: how to generate an adversar-ial point cloud with minimum number of points perturbed from an original point cloud while maintaining the percep-tibility of the original point cloud (see Figure 1). To address this problem, we propose a new formulation for adversarial point cloud generation that can be adapted to different at-tack strategies. The novelty of our work lies in both the research problem and proposed solution. Speciﬁcally, min-imal 3D point cloud attack is an unexplored problem. We are also the ﬁrst to propose a formulation that (i) considers both the perceptibilty and optimality of adversarial samples, and (ii) generalises both point perturbation and addition in a uniﬁed framework. In summary, our contributions include:
• A new technique to generate minimal 3D adversarial point clouds;
• A uniﬁed formulation that generalises two adversarial point cloud generation strategies: point perturbation and point addition;
• A vulnerability analysis on the relation between the per-turbed points found by our method and the concept of
critical points in PointNet [27];
• A benchmark of adversarial attacks on both synthetic and real-world 3D point clouds, which shows our method achieves consistent performance over both domains. 2.