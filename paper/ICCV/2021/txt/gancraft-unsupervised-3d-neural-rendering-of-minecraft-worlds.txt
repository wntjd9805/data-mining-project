Abstract
We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic im-ages for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adver-sarial training. This stands in contrast to prior work on neu-ral rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent ap-pearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Ex-perimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photoreal-istic 3D block world synthesis. The project website is avail-able at https://nvlabs.github.io/GANcraft/. 1.

Introduction
Imagine a world where every Minecrafter is a 3D painter!
Advances in 2D image-to-image translation [3, 22, 49] have enabled users to paint photorealistic images by draw-ing simple sketches similar to those created in Microsoft
Paint. Despite these innovations, creating a realistic 3D scene remains a painstaking task, out of the reach of most people. It requires years of expertise, professional software, a library of digital assets, and a lot of development time.
In contrast, building 3D worlds with blocks, say physical
LEGOs or their digital counterpart, is so easy and intuitive that even a toddler can do it. Wouldn’t it be great if we could build a simple 3D world made of blocks representing various materials (like Fig. 1 (insets)), feed it to an algorithm, and receive a realistic looking 3D world featuring tall green trees, ice-capped mountains, and the blue sea (like Fig. 1)? With such a method, we could perform world-to-world translation to convert the worlds of our imagination to reality. Needless to say, such an ability would have many applications, from entertainment and education, to rapid prototyping for artists.
In this paper, we propose GANcraft, a method that pro-duces realistic renderings of semantically-labeled 3D block worlds, such as those from Minecraft (www.minecraft. net). Minecraft, the best-selling video game of all time with over 200 million copies sold and over 120 million monthly users [2], is a sandbox video game in which a user can ex-plore a procedurally-generated 3D world made up of blocks arranged on a regular grid, while modifying and building structures with blocks. Minecraft provides blocks repre-senting various building materials—grass, dirt, water, sand, snow, etc. Each block is assigned a simple texture, and the game is known for its distinctive cartoonish look. While one might discount Minecraft as a simple game with simple mechanics, Minecraft is, in fact, a very popular 3D content
Representative method consistent? worlds? 3D view Across W/o paired instead of Minecraft segmentations.
CycleGAN [71], MUNIT [21] pix2pix [22], SPADE [49] wc-vid2vid [36]
NeRF [39], NSVF [31]
GANcraft (ours) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:51) data? (cid:51) (cid:55) (cid:55) (cid:55) (cid:51)
Table 1: Given a Minecraft world, our goal is to train a neural renderer that can convert any camera trajectory in the Minecraft world to a sequence of view-consistent im-ages in the real world, at test time. The training needs to be achieved without paired Minecraft–real data as it does not exist. Among prior work, only unsupervised image-to-image translation methods such as CycleGAN [71] and
MUNIT [21] can work in this setting. However, they do not generate 3D view-consistent outputs. Neural radiance ﬁeld-based methods like NeRF [39] and NSVF [31] are suited for novel view synthesis. They cannot handle the Minecraft–real domain gap. All other prior works require paired training data unavailable in our setting. GANcraft, our proposed method, can generate 3D view-consistent Minecraft-to-real synthesis results without paired Minecraft–real training data. creation tool. Minecrafters have faithfully recreated large cities and famous landmarks including the Eiffel Tower!
The block world representations are intuitive to manipulate and this makes it well-suited as the medium for our world-to-world translation task. We focus on generating natural landscapes, which was also studied in several prior work in image-to-image translation [3, 49].
At ﬁrst glance, generating a 3D photorealistic world from a semantic block world seems to be a task of translating a sequence of projected 2D segmentation maps of the 3D block world, and is a direct application of image-to-image translation . This approach, however, immediately runs into several serious issues. First, obtaining paired ground truth training data of the 3D block world, segmentation labels, and corresponding real images is extremely costly if not impossible. Second, existing image-to-image translation models [21, 49, 61, 71] do not generate consistent views [36].
Each image is translated independent of the others.
While the recent world-consistent vid2vid work [36] overcomes the issue of view-consistency, it requires paired ground truth 3D training data. Even the most recent neural rendering approaches based on neural radiance ﬁelds such as NeRF [39], NSVF [31], and NeRF-W [37], require real images of a scene and associated camera parameters, and are best suited for view interpolation. As there is no paired 3D and ground truth real image data, as summarized in Table 1, none of the existing techniques can be used to solve this new task. This requires us to employ ad hoc adaptations to make our problem setting as similar to these methods’ require-ments as possible, e.g. training them on real segmentations
In the absence of ground truth data, we propose a frame-work to train our model using pseudo-ground truth photoreal-istic images for sampled camera views. Our framework uses ideas from image-to-image translation and improves upon work in 3D view synthesis to produce view-consistent pho-torealistic renderings of input Minecraft worlds as shown in
Fig. 1. Although we demonstrate our results using Minecraft, our method works with other 3D block world representations, such as voxels. We chose Minecraft because it is a popular platform available to a wide audience.
Our key contributions include:
• The novel task of producing view-consistent photore-alistic renderings of user-created 3D semantic block worlds, or world-to-world translation, a 3D extension of image-to-image translation.
• A framework for training neural renderers in the ab-sence of ground truth data. This is enabled by using pseudo-ground truth images generated by a pretrained image synthesis model (Section 3.1).
• A new neural rendering network architecture trained with adversarial losses (Section 3.2), that extends recent work in 2D and 3D neural rendering [20, 31, 37, 39, 44] to produce state-of-the-art results which can be conditioned on a style image (Section 4). 2.