Abstract
Driving requires interacting with road agents and pre-dicting their future behaviour in order to navigate safely.
We present FIERY: a probabilistic future prediction model in bird’s-eye view from monocular cameras. Our model pre-dicts future instance segmentation and motion of dynamic agents that can be transformed into non-parametric future trajectories. Our approach combines the perception, sen-sor fusion and prediction components of a traditional au-tonomous driving stack by estimating bird’s-eye-view pre-diction directly from surround RGB monocular camera in-puts. FIERY learns to model the inherent stochastic nature of the future solely from camera driving data in an end-to-end manner, without relying on HD maps, and predicts mul-timodal future trajectories. We show that our model outper-forms previous prediction baselines on the NuScenes and
Lyft datasets. The code and trained models are available at https://github.com/wayveai/fiery. 1.

Introduction
Prediction of future states is a key challenge in many autonomous decision making systems. This is particularly true for motion planning in highly dynamic environments: for example in autonomous driving where the motion of other road users and pedestrians has a substantial influence on the success of motion planning [10]. Estimating the mo-tion and future poses of these road users enables motion planning algorithms to better resolve multimodal outcomes where the optimal action may be ambiguous knowing only the current state of the world.
Autonomous driving is inherently a geometric problem, where the goal is to navigate a vehicle safely and correctly through 3D space. As such, an orthographic bird’s-eye view (BEV) perspective is commonly used for motion planning and prediction based on LiDAR sensing [38, 49]. Recent advances in camera-based perception have rivalled LiDAR-based perception [48], and we anticipate that this will also be possible for wider monocular vision tasks, including pre-diction. Building a perception and prediction system based on cameras would enable a leaner, cheaper and higher reso-lution visual recognition system over LiDAR sensing.
Most of the work in camera-based prediction to date has either been performed directly in the perspective view coor-dinate frame [1, 23], or using simplified BEV raster rep-resentations of the scene [28, 12, 10] generated by HD-mapping systems such as [29, 16]. We wish to build predic-tive models that operate in an orthographic bird’s-eye view frame (due to the benefits for planning and control [34]), though without relying on auxiliary systems to generate a
BEV raster representation of the scene.
A key theme in robust perception systems for au-tonomous vehicles has been the concept of early sensor fusion, generating 3D object detections directly from im-age and LiDAR data rather than seeking to merge the pre-dicted outputs of independent object detectors on each sen-sor input. Learning a task jointly from multiple sources of sensory data as in [50], rather than a staged pipeline, has been demonstrated to offer improvement to perception performance in tasks such as object detection. We seek similar benefits in joining perception and sensor fusion to prediction by estimating bird’s-eye-view prediction directly from surround RGB monocular camera inputs, rather than a multi-stage discrete pipeline of tasks.
Lastly, traditional autonomous driving stacks [13] tackle future prediction by extrapolating the current behaviour of dynamic agents, without taking into account possible inter-actions. They rely on HD maps and use road connectivity to generate a set of future trajectories. Instead, FIERY learns to predict future motion of road agents directly from cam-era driving data in an end-to-end manner, without relying on HD maps. It can reason about the probabilistic nature of the future, and predicts multimodal future trajectories (see blog post and Figure 1).
To summarise the main contributions of this paper:
Figure 1: Multimodal future predictions by our bird’s-eye view network. Top two rows: RGB camera inputs. The predicted instance segmentations are projected to the ground plane in the images. We also visualise the mean future trajectory of dynamic agents as transparent paths. Bottom row: future instance prediction in bird’s-eye view in a 100m × 100m capture size around the ego-vehicle, which is indicated by a black rectangle in the center. 1. We present the first future prediction model in bird’s-eye view from monocular camera videos. Our frame-work explicitly reasons about multi-agent dynamics by predicting future instance segmentation and motion in bird’s-eye view. 2. Our probabilistic model predicts plausible and multi-modal futures of the dynamic environment. 3. We demonstrate quantitative benchmarks for future dynamic scene segmentation, and show that our learned prediction outperforms previous prediction baselines for autonomous driving on the NuScenes [5] and Lyft [25] datasets.
2.