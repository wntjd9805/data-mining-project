Abstract 1.

Introduction
We present the ﬁrst method capable of photorealistically reconstructing deformable scenes using photos/videos cap-tured casually from mobile phones. Our approach augments neural radiance ﬁelds (NeRF) by optimizing an additional continuous volumetric deformation ﬁeld that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation ﬁelds are prone to local min-ima, and propose a coarse-to-ﬁne optimization method for coordinate-based models that allows for more robust opti-mization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation ﬁeld that further improves robustness. We show that our method can turn casually captured selﬁe photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub “nerﬁes.” We evalu-ate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high ﬁdelity.
*Work done while the author was an intern at Google. 1
High quality 3D human scanning has come a long way – but the best results currently require a specialized lab with many synchronized lights and cameras, e.g., [14, 15, 18].
What if you could capture a photorealistic model of yourself (or someone else) just by waving your mobile phone camera?
Such a capability would dramatically increase accessibility and applications of 3D modeling technology.
Modeling people with hand-held cameras is especially challenging due both to 1) nonrigidity – our inability to stay perfectly still, and 2) challenging materials like hair, glasses, and earrings that violate assumptions used in most recon-struction methods. In this paper we introduce an approach to address both of these challenges, by generalizing Neural
Radiance Fields (NeRF) [32] to model shape deformations.
Our technique recovers high ﬁdelity 3D reconstructions from short videos, providing free-viewpoint visualizations while accurately capturing hair, glasses, and other complex, view-dependent materials, as shown in Figure 1. A special case of particular interest is capturing a 3D self-portrait – we call such casual 3D selﬁe reconstructions nerﬁes.
Rather than represent shape explicitly, NeRF [32] uses a neural network to encode color and density as a function of location and viewing angle, and generates novel views using volume rendering. Their approach produces 3D visualiza-tions of unprecedented quality, faithfully representing thin
structures, semi-transparent materials, and view-dependent effects. To model non-rigidly deforming scenes, we gen-eralize NeRF by introducing an additional component: A canonical NeRF model serves as a template for all the ob-servations, supplemented by a deformation ﬁeld for each observation that warps 3D points in the frame of reference of an observation into the frame of reference of the canonical model. We represent this deformation ﬁeld as a multi-layer perceptron (MLP), similar to the radiance ﬁeld in NeRF.
This deformation ﬁeld is conditioned on a per-image learned latent code, allowing it to vary between observations.
Without constraints, the deformation ﬁelds are prone to distortions and over-ﬁtting. We employ a similar approach to the elastic energy formulations that have seen success for mesh ﬁtting [7, 12, 45, 46]. However, our volumetric deformation ﬁeld formulation greatly simpliﬁes such regu-larization, because we can easily compute the Jacobian of the deformation ﬁeld through automatic differentiation, and directly regularize its singular values.
To robustly optimize the deformation ﬁeld, we propose a novel coarse-to-ﬁne optimization scheme that modulates the components of the input positional encoding of the de-formation ﬁeld network by frequency. By zeroing out the high frequencies at the start of optimization, the network is limited to learn smooth deformations, which are later reﬁned as higher frequencies are introduced into the optimization.
For evaluation, we capture image sequences from a rig of two synchronized, rigidly attached, calibrated cameras, and use the reconstruction from one camera to predict views from the other. We plan to release the code and data.
In summary, our contributions are: 1(cid:13) an extension to
NeRF to handle non-rigidly deforming objects that optimizes a deformation ﬁeld per observation; 2(cid:13) rigidity priors suit-able for deformation ﬁelds deﬁned by neural networks; 3(cid:13) a coarse-to-ﬁne regularization approach that modulates the ca-pacity of the deformation ﬁeld to model high frequencies dur-ing optimization; 4(cid:13) a system to reconstruct free-viewpoint selﬁes from casual mobile phone captures. 2.