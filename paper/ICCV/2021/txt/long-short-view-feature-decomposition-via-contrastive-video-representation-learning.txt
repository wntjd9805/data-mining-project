Abstract
Self-supervised video representation methods typically focus on the representation of temporal attributes in videos.
However, the role of stationary versus non-stationary at-tributes is less explored: Stationary features, which remain similar throughout the video, enable the prediction of video-level action classes. Non-stationary features, which repre-sent temporally varying attributes, are more beneﬁcial for downstream tasks involving more ﬁne-grained temporal un-derstanding, such as action segmentation. We argue that a single representation to capture both types of features is sub-optimal, and propose to decompose the representation space into stationary and non-stationary features via con-trastive learning from long and short views, i.e. long video sequences and their shorter sub-sequences. Stationary fea-tures are shared between the short and long views, while non-stationary features aggregate the short views to match the corresponding long view. To empirically verify our ap-proach, we demonstrate that our stationary features work particularly well on an action recognition downstream task, while our non-stationary features perform better on action segmentation. Furthermore, we analyse the learned rep-resentations and ﬁnd that stationary features capture more temporally stable, static attributes, while non-stationary features encompass more temporally varying ones. 1.

Introduction
Learning rich video representations is a key challenge for general video understanding. An ideal representation extracts useful information that beneﬁts numerous down-stream tasks such as action recognition, retrieval and ac-tion segmentation. Learning such representations in a su-pervised setting is inherently biased towards static fea-tures [24]. However, in order to solve more complex down-† Work done during an internship at the Bosch Center for Artiﬁcial
Intelligence.
Figure 1. Video attributes can be divided into two sections: Sta-tionary features (shown in yellow), that are shared between long and short views, and non-stationary features (shown in green), that aggregate the short views to match the corresponding long view. stream tasks, that require temporal attributes of videos such as temporal action segmentation, we need a more diverse set of features. As a remedy, we train our network to represent stationary and non-stationary features. To get an intuition, let’s consider the following example: a video of a bartender mixing a cocktail in Figure 1 is stationary in one sense – we see a bartender in a bar with liquor bottles in the background – but non-stationary in another: different steps in preparing a cocktail are shown, such as holding the bottle, pouring the left bottle and pouring the right bottle. Here, the station-ary attributes of the video enable us to predict the overall action class, i.e. mixing cocktails. On the other hand, the non-stationary attributes enable more ﬁne-grained temporal distinctions, e.g. predicting when different steps occur in the video. Ideally, both types of attributes should be repre-sented by video models.
Learning representations in a supervised setting, usually involves pretraining on large-scale labeled datasets such as
Kinetics [17] with video-level annotations, inhibiting strong static biases [24]. As a result they capture mainly the sta-tionary features, but largely ignore non-stationary features as the stationary features are sufﬁcient for action recogni-tion. Self-supervised leaning provides a promising direc-tion to address this shortcoming. As the supervisory signals arise from the underlying structure of the data, it has the po-tential to extract more descriptive features. Several previous self-supervised methods aim to capture temporal features in videos by designing a temporal pretext task, e.g. predicting temporal transformations [15] or video speed [2]. These methods are not explicitly encouraged to capture stationary and non-stationary features. In contrast, we explicitly de-compose the representation space into stationary and non-stationary features, enabling us to solve a more diverse set of downstream tasks, including action recognition and tem-poral action segmentation.
Following the recent trend in self-supervised learning, our proposed method ﬁts in the contrastive learning frame-work. The supervisory signal that leads us to distinguish between the stationary and non-stationary features emerges from long and short views of a given video. Naively apply-ing contrastive learning to long and short views results in a set of features that represent both long and short views sim-ilarly. We argue that this assumption is only valid for a sub-set of features, which we call stationary. The other subset, which we call non-stationary features, includes a set of fea-tures that aggregate from short to long views, i.e. combining attributes of several short views gives us the attributes of the long view, see Figure 1. Therefore, imposing a naive simi-larity between the long and short views is prone to ignoring the non-stationary features, which are crucial for different downstream tasks. Accordingly, we divide the ﬁnal features into two disjoint subsets, which are later used in two sepa-rate contrastive losses. For the stationary features of a given long view, we provide a positive pair through the stationary features of a corresponding short view. Whereas an aggre-gated form of the non-stationary features of all correspond-ing short views forms a positive pair for the non-stationary features of the long view.
We validate our argument above empirically and train our method on the Kinetics dataset [17] without using any labels. We evaluate the model on several downstream tasks and datasets and analyse the learned representations; we highlight the main results here. We observe that station-ary features are more beneﬁcial for action classiﬁcation, speciﬁcally on datasets with a high static bias [24] such as
UCF101 [33]. On the other hand, non-stationary features achieve higher performance on downstream tasks involving more temporal aspects, such as temporal action segmenta-tion on the Breakfast dataset [19]. We show that our non-stationary features perform substantially better than the sta-tionary features on this dataset, supporting our hypothesis on the design choice. To the best of our knowledge, we are the ﬁrst to conduct this evaluation for video representation learning. Our proposed method outperforms a strong con-trastive learning baseline using data augmentations by sig-niﬁcant margins on all three datasets and achieves state-of-the-art retrieval results on HMDB51 [20]. Furthermore, we analyse the learned representations and ﬁnd that stationary features capture attributes, that remain similar over time and can be detected in a few frames, while non-stationary fea-tures encompass more temporally varying attributes, which are revealed when observing more frames. 2.