Abstract
We address the problem of network quantization, that is, reducing bit-widths of weights and/or activations to lighten network architectures. Quantization methods use a rounding function to map full-precision values to the nearest quan-tized ones, but this operation is not differentiable. There are mainly two approaches to training quantized networks with gradient-based optimizers. First, a straight-through estimator (STE) replaces the zero derivative of the rounding with that of an identity function, which causes a gradient mismatch problem. Second, soft quantizers approximate the rounding with continuous functions at training time, and exploit the rounding for quantization at test time. This allevi-ates the gradient mismatch, but causes a quantizer gap prob-lem. We alleviate both problems in a uniﬁed framework. To this end, we introduce a novel quantizer, dubbed a distance-aware quantizer (DAQ), that mainly consists of a distance-aware soft rounding (DASR) and a temperature controller.
To alleviate the gradient mismatch problem, DASR approx-imates the discrete rounding with the kernel soft argmax, which is based on our insight that the quantization can be formulated as a distance-based assignment problem between full-precision values and quantized ones. The controller adjusts the temperature parameter in DASR adaptively ac-cording to the input, addressing the quantizer gap problem.
Experimental results on standard benchmarks show that
DAQ outperforms the state of the art signiﬁcantly for various bit-widths without bells and whistles. 1.

Introduction
Convolutional neural networks (CNNs) have made signif-icant progress in the ﬁeld of computer vision, such as image recognition [27, 48], object detection [2, 43], and semantic segmentation [7, 34]. Deeper [15, 46] and wider [45] CNNs, however, require lots of parameters and FLOPs, making it difﬁcult to deploy modern network architectures on edge devices (e.g., mobile phones, televisions, or drones). Re-cent works focus on compressing networks to lighten the
*Corresponding author
Figure 1: The discretizer takes a full-precision input, and then assigns it to the nearest quantized value, e.g., q1 in this example.
We interpret the assignment process of a discretizer as follows: It
ﬁrst computes the distances between the full-precision input and quantized values, q1 and q2, and then applies an argmin operator over the distances to choose the quantized value. Since this operator is non-differentiable, the quantized network cannot be trained end-to-end with gradient-based optimizers. (Best viewed in color.) network architectures. Pruning [14] and distillation [16] are representative techniques for network compression. The pruning removes redundant weights in a network, and the distillation encourages a compact network to have features similar to the ones obtained from a large network. The net-works compressed by these techniques still exploit ﬂoating-point computations, indicating that they are not suitable for edge devices favoring ﬁxed-point operations for power efﬁ-ciency. Network quantization [42] is an alternative approach that converts full-precision weights and/or activations into low-precision ones, enabling a ﬁxed-point inference, while reducing memory and computational cost.
Quantization methods typically use a staircase function as a quantizer, where it normalizes a full-precision value within a quantization interval, and assigns the normalized one to the nearest quantized value using a discretizer (i.e., a rounding function) [11, 12, 22]. Since the derivative of the rounding is zero at almost everywhere, gradient-based optimizers could not be used to train quantized networks. To address this, the straight-through estimator (STE) [3] replaces the derivative of the rounding with that of identity or hard tanh functions for backward propagation. This, however, causes a gradient mismatch between forward and backward passes at train-ing time, making the training process noisy and degrading
the quantization performance at test time [11, 31, 47]. In-stead of using the STE, recent methods use soft quantizers, which approximate the discrete rounding with sigmoid [47] or tanh [12] functions, for both forward and backward passes, alleviating the gradient mismatch problem, while maintain-ing differentiability at training time. These approaches, on the other hand, use the discrete quantizer at inference time.
That is, they exploit different quantizers (soft and discrete ones) at training and test time, resulting in a quantizer gap problem [36, 47]. The quantizer gap might be relieved by raising a temperature parameter in the sigmoid function grad-ually [47], such that the soft quantizer will be transformed to the discrete one eventually at training time, but this causes an unstable gradient ﬂow.
We introduce in this paper a distance-aware quantizer (DAQ) that alleviates the gradient mismatch and quantizer gap problems in a uniﬁed framework. Our approach builds upon the insight that the discretizer (i.e., rounding) chooses the nearest quantized value by ﬁrst computing the distances between a full-precision input and quantized values, and then applying an argmin operator over the distances w.r.t the quantized values (Fig. 1). Motivated by this, we propose a distance-aware soft rounding (DASR) that approximates the discrete rounding accurately using a kernel soft argmax [28], while maintaining differentiability, alleviating the gradient mismatch problem. We also introduce a temperature con-troller that adjusts a temperature parameter in DASR adap-tively depending on the distances between the full-precision input and quantized values. This imposes DASR to have the same output as the discrete rounding, addressing the quantizer gap problem. We apply our DAQ to quantize weights and/or activations for various network architectures, and achieve state-of-the-art results on standard benchmarks, clearly demonstrating the effectiveness of our approach. To our knowledge, it is the ﬁrst approach to alleviating both gradient mismatch and quantizer gap problems jointly. We summarize the main contributions of this paper as follows:
• We propose a novel differentiable approximation of the discrete rounding function, dubbed DASR, allowing to train quantization networks end-to-end, while alleviating the gradient mismatch problem.
• We introduce a temperature controller, which adjusts the temperature parameter in DASR adaptively, to address the quantizer gap problem.
• We set a new state of the art on standard benchmarks, and provide an extensive analysis of our approach, demonstrat-ing the effectiveness of DAQ. 2.