Abstract
Geometric feature extraction is a crucial component of point cloud registration pipelines. Recent work has demon-strated how supervised learning can be leveraged to learn better and more compact 3D features. However, those ap-proaches’ reliance on ground-truth annotation limits their scalability. We propose BYOC: a self-supervised approach that learns visual and geometric features from RGB-D video without relying on ground-truth pose or correspon-dence. Our key observation is that randomly-initialized
CNNs readily provide us with good correspondences; al-lowing us to bootstrap the learning of both visual and ge-ometric features. Our approach combines classic ideas from point cloud registration with more recent representa-tion learning approaches. We evaluate our approach on in-door scene datasets and find that our method outperforms traditional and learned descriptors, while being competitive with current state-of-the-art supervised approaches. 1.

Introduction
One’s ability to align two views of the same scene is closely intertwined with their ability to identify correspond-ing points between the two views. The duality between correspondence estimation and point cloud registration has long been recognized and serves as the basis for many ap-proaches in both problems . Given an accurate registration of a scene, one can easily extract correspondences between the two views. Conversely, given point correspondences, one can easily register two views of a scene. Can we lever-age this cycle to jointly learn both correspondence estima-tion and point cloud registration from scratch?
At the core of this cycle is the ability to generate good feature descriptors for points in the scene. The prevail-ing approach to 3D feature learning relies on preregistered scenes to sample ground-truth correspondences for the su-pervised training of a feature encoder. This is done by sampling positive and negative feature pairs and applying triplet [12, 32, 35, 57] or contrastive [3, 12, 56] losses.
While very successful, these approaches require us to have
Figure 1. BYOC estimates visual correspondences and uses them to train a visual and a geometric encoder on RGB-D video frames.
At test time, it can successfully register raw point clouds. already registered the raw depth or RGB-D scans to gener-ate the training data. This limits this approach to data that can be successfully registered with automated approaches like COLMAP [46]. Ideally, we would leverage the success of supervised approaches without relying on ground-truth correspondence labels.
To this end, we propose Bootstrap Your Own Correspon-dences (BYOC): a self-supervised end-to-end approach that learns point cloud registration by leveraging pseudo-correspondence labels. Our approach extracts pseudo-correspondences using the features of a randomly initial-ized feature encoder. We use the sampled correspondences to register the point clouds and apply losses based on the quality of the registration to train the feature encoders. This allows us to slowly bootstrap1 the feature learning process and learn from RGB-D scans without relying on any pose or correspondence supervision. 1We use bootstrap in its idiomatic rather than its statistical sense.
This approach works well for registering RGB-D frames, but it is less effective for raw point clouds. This is pri-marily due to the fact that randomly initialized 2D CNNs produce more distinctive features than current point cloud encoders, as shown in Fig. 3. We leverage this observation and propose bootstrapping the geometric feature learning using visual correspondences. We do this by using the es-timated visual correspondences, as opposed to ground-truth correspondences [3, 12, 32, 35, 56, 57], to train the geomet-ric encoder. We train the geometric encoder by adapting
SimSiam [8], a non-contrastive self-supervised approach, for 3D representation learning. Unlike typical contrastive self-supervised approaches, SimSiam allows us to train the model using only positive pairs without requiring negative sampling or momentum encoders.
Our work draws inspiration from two sources: iterative closest point algorithm (ICP) [4, 9, 61] and self-supervised learning with pseudo-labels [7, 26, 34]. While seemingly different, the same intuition lies at the core of both lines of work. ICP is a registration algorithm that assumes that the closest points between two point clouds correspond to each other. Through iterative refinement and resampling, it can register roughly aligned point clouds. Meanwhile, self-supervised learning with pseudo-labels learns to predict pseudo-labels in the form of the current top prediction [34], feature clusters [7], or even a previous prediction [26].
Through redefining the labels over time, the model can pro-gressively learn better representations. Both rely on the ob-servation that pseudo-labels in a well-structured space (i.e., similar entities already lie close to each other) can provide a valuable learning signal. This is particularly relevant for learning due to the observation that CNNs, even when ran-domly initialized, are good feature extractors [42, 51].
We evaluate our approach on two indoor scene datasets:
ScanNet [13] and 3D Match [60]. Despite the simplicity of our approach, it outperforms hand-crafted features as well as several supervised baselines, while being compet-itive with current state-of-the-art supervised approaches.
In summary, we propose a self-supervised approach that uses sampled correspondences from randomly-initialized feature encoders to learn point-wise features for point cloud registration (Sec. 3.1). We further demonstrate how vi-sual correspondences can further improve geometric feature learning (Sec. 3.2). We demonstrate the efficacy of this ap-proach on point cloud registration (Sec. 4.1) and correspon-dence estimation (Sec. 4.2). 2.