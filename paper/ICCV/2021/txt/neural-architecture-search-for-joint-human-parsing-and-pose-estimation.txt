Abstract
Human parsing and pose estimation are crucial for the understanding of human behaviors. Since these tasks are closely related, employing one unified model to perform two tasks simultaneously allows them to benefit from each other.
However, since human parsing is a pixel-wise classification process while pose estimation is usually a regression task, it is non-trivial to extract discriminative features for both tasks while modeling their correlation in the joint learning fashion. Recent studies have shown that Neural Architec-ture Search (NAS) has the ability to allocate efficient fea-ture connections for specific tasks automatically. With the spirit of NAS, we propose to search for an efficient network architecture (NPPNet) to tackle two tasks at the same time.
On the one hand, to extract task-specific features for the two tasks and lay the foundation for the further searching of feature interaction, we propose to search their encoder-decoder architectures, respectively. On the other hand, to ensure two tasks fully communicate with each other, we propose to embed NAS units in both multi-scale feature in-teraction and high-level feature fusion to establish optimal connections between two tasks. Experimental results on both parsing and pose estimation benchmark datasets have demonstrated that the searched model achieves state-of-the-art performances on both tasks. 1 1.

Introduction
Human parsing and pose estimation are two key tasks for analyzing human behaviors. Human parsing aims to seg-ment the human body into different semantic regions, while pose estimation is to locate the keypoints of the human body and analyze structural information. Visually speaking, these two tasks are closely related. On the one hand, keypoints are often included within the different semantic regions, indi-cates that the semantic information from human parsing can help locate these points for accurate pose estimation. On the
†This work is done when Yuhang Huang is an intern at AI Research of JD.com.
* Wu Liu is the corresponding author. 1The code will be public at https://github.com/GuHuangAI/NPP.
Input
JPPNet [21] MuLA [31]
Ours
GT
Figure 1. The illustration of our motivation. As we can see, manu-ally designed frameworks [21,31] can not improve pose estimation and human parsing simultaneously. There is an obvious inconsis-tency between the two tasks, which suggests it is challenging for manually designed modules to extract task-specific features and enhance their correlation, e.g., in MuLA [31], some keypoints are not surrounded by the corresponding semantic parts. This moti-vates us to search for a better network architecture that allows two tasks to fully benefit from each other. other hand, the group of keypoints naturally possesses rich structural information, which can guide the generation of semantic parts.
With the advanced learning ability of the deep neural network, it becomes a general practice to deploy convo-lutional neural networks (CNN) to tackle these two tasks
[28, 40]. The majority of existing models share a similar basic encoder-decoder architecture with different learning objectives. For human parsing, they [26, 36, 44] aim at mapping the up-sampled output to pixel-wise annotations, while the ground-truth of pose estimation [6, 30, 42] cor-responds to the heatmaps of sparse keypoints. Given the correlation between two tasks, there are recent develop-ments [21, 31, 41, 46] that attempt to perform joint infer-ence via neural networks from the multi-task learning per-spective. These models conduct the representation learning through one shared [21] or two separate encoder-decoder structure [31,46], and design the hand-crafted modules [31] to interact high-level features extracted for two tasks.
However, designing the suitable network architecture and optimal feature interaction for joint learning is chal-lenging. On the one hand, though the two tasks are visu-ally related, they still possess their own characteristics. The general solutions for pose estimation focus on aggregat-ing information into small joint areas while human parsing needs to explore the pixel-wise context information. There-F H pos aux
F H pos
F Lpos
F Lpar
F H par
F H edg
F H pos aux
F H pos
F H par
F H edg
Outpos 2
Outpos 1
Outpar 1
Outpar 2
Figure 2. The overall framework of NPPNet. The input image passes through two task-specific encoder-decoder structures to extract two sets of features with multiple scales, each set absorbs information from the other one in the multi-scale feature interaction structure. The two encoder-decoder structures output two sets of high-level features including main and auxiliary features and generate the initial prediction.
Then the high-level feature fusion structure fuses these features and commits the final prediction. The entire network architecture is searchable. fore, it is challenging to extract discriminative features for both tasks when tackling them simultaneously. On the other hand, it is difficult to model the correlation between the two tasks. Existing works [31, 46] address this issue by manu-ally designing the fusion modules to interact the high-level features from the two branches. Yet it is rather rigid and ignores the diverse intermediate features with multi-scale information, considering different levels of features require the more refined interaction. As shown in Fig. 1, existing frameworks for joint learning cannot obtain consistent re-sults for both tasks, which motivates us to design a more efficient network architecture.
Recent studies [10, 18, 20, 24] have shown that Neural
Architecture Search (NAS) is able to flexibly search for ef-ficient architectures for various vision tasks [13,45], includ-ing parsing [23, 29] and pose estimation [2]. Different from them, in this paper, we propose to employ search for the joint learning framework to tackle both tasks. We name our searched model as NPPNet. As shown in Fig. 2, on the one hand, to extract task-specific features for both tasks and lay the foundation for the further searching of feature inter-action, we propose to search for their encoder-decoder ar-chitectures, respectively. On the other hand, to ensure both intermediate and high-level features of two tasks interact with each other, we propose to embed NAS units in both multi-scale feature interaction and high-level feature fusion to establish optimal connections between two tasks.
In this paper, we design three types of search spaces tailed for joint human parsing and pose estimation. By ap-plying the different search spaces during feature extraction, interaction, and fusion, NPPNet improves the performance of both tasks simultaneously. In summary, the main contri-butions of this paper as follows:
• We propose an end-to-end network NPPNet entirely searched by NAS, which conducts human parsing and pose estimation simultaneously. To the best of our knowledge, this is one of the first attempts towards us-ing NAS to tackle two tasks simultaneously.
• To extract the discriminative features for both tasks and allow them to benefit from each other, we de-sign three searching spaces for constructing the task-specific encoder-decoder structure, the multi-scale fea-ture interaction, and the high-level feature fusion.
• Extensive experiments are conducted on the LIP and the extended PASCAL-Person-Part datasets. Results show that our proposed NPPNet achieves state-of-the-art performances on both tasks. 2.