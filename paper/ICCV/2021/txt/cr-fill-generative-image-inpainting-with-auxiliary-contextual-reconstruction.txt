Abstract
Recent deep generative inpainting methods use attention layers to allow the generator to explicitly borrow feature patches from the known region to complete a missing re-gion. Due to the lack of supervision signals for the corre-spondence between missing regions and known regions, it may fail to ﬁnd proper reference features, which often leads to artifacts in the results. Also, it computes pair-wise simi-larity across the entire feature map during inference bring-ing a signiﬁcant computational overhead. To address this issue, we propose to teach such patch-borrowing behavior to an attention-free generator by joint training of an aux-iliary contextual reconstruction task, which encourages the generated output to be plausible even when reconstructed by surrounding regions. The auxiliary branch can be seen as a learnable loss function, i.e. named as contextual recon-struction (CR) loss, where query-reference feature similar-ity and reference-based reconstructor are jointly optimized with the inpainting generator. The auxiliary branch ( i.e. CR loss) is required only during training, and only the inpaint-ing generator is required during the inference. Experimen-tal results demonstrate that the proposed inpainting model compares favourably against the state-of-the-art in terms of quantitative and visual performance. Code is available at https://github.com/zengxianyu/crfill. 1.

Introduction
Image inpainting is a task of predicting missing regions in images. It is an important problem in computer vision and can be used in many applications, e.g. image restora-tion, compositing, manipulation, re-targeting, and image-based rendering [2, 10, 19]. Traditional methods such as [4, 9, 2] borrow example patches from known regions or external dataset and paste them into the missing regions.
They cannot hallucinate novel image contents for challeng-ing cases involving complex, non-repetitive structures. Re-cent research efforts have shifted the attention to data-driven deep CNN-based approaches [20, 6, 28, 14, 29].
For inpainting, an unlimited amount of paired training data can be automatically generated simply by corrupt-ing images deliberately and using the original images be-fore corruption as the ground-truths. By training on large datasets, deep network-based methods have shown promis-ing results for inpainting complex scenes. An important challenge in inpainting is that there are many plausible an-swers for ﬁlling in a missing region in natural images, and this ambiguity often leads to blurry or distorted structures.
To overcome this issue, recent methods [29, 16, 30, 31, 23] try to reduce the uncertainty by explicitly assigning a known region as a reference for ﬁlling a missing region. This can be implemented as a patch-borrowing operation, e.g. con-textual attention (CA) module [28], which copies feature patches from the known reference region and pastes them into the missing region.
However, there is no direct supervision on feature sim-ilarity nor the information of patch correspondences in the
CA module, thus sometimes inappropriate patches are cho-sen with higher weights, which leads to artifacts in the con-sequent inpainting results. Consider the image shown in
Fig. 1 (a) with missing pixels. Fig. 1 (b) shows the refer-ence patches selected by CA layer with the largest weight.
For visual analysis, we copy each reference patch to the cor-responding location in the missing region. (c) shows the in-painting results of DeepFillv2 [29]. By comparing Fig. 1 (b) and Fig. 1 (c), we can see the artifacts are caused by the incorrect reference regions found by the CA module. More-over, a patch-borrowing operation in the CA layer requires computing the similarity of every pair of patches in the fea-ture map, which is computationally expensive especially for high-resolution images.
To address these issues, we consider to avoid using the explicit patch-borrowing in the inpainting generator (to make the generator efﬁcient and robust to borrowing incor-rect reference patches) while retaining and encouraging the patch/feature copy-pasting behaviors through jointly train-ing of contextual reconstruction (to make the result realis-tic). The idea is partially motivated by Generative Adver-sarial Networks where its generator and discriminator are trained jointly with losses on both, while only the generator is used during testing.
Speciﬁcally, we propose to attach an auxiliary contextual reconstruction branch to the inpainting network as a new training loss. The auxiliary branch can be regarded as a con-textual reconstruction loss (CR loss) which encourages the generated output to be plausible even when reconstructed by surrounding (contextual) regions/features. CR loss not only encourages the individual features of generated miss-ing regions reconstructable from features of the surround-ing regions but also the complete contextually reconstructed composite visually plausible, in analogy to a jigsaw puzzle.
Through extensive experiments, we validate that a vanilla attention-free CNN trained with CR loss can learn to inherit the patch-borrowing behavior which was needed to be explicitly enforced with attention layers by previous approaches, as shown in Fig. 1. Moreover, since CR loss is required only during training, there is no computational overhead brought to the inpainting model during inference, making the model much more efﬁcient for testing.
We summarize the contributions of this paper as follow:
• A new learnable, auxiliary contextual reconstruction branch/loss (CR loss) to encourage the generator net-work to borrow appropriate known regions as refer-ences for ﬁlling in a missing region.
• An efﬁcient and robust attention-free inpainting gener-ator trained jointly with the traditional inpainting loss and the auxiliary contextual reconstruction loss while allowing efﬁcient inference for testing.
• Extensive experiments showing the effectiveness of the
CR loss and favorable performances over the state-of-the-art methods. 2.