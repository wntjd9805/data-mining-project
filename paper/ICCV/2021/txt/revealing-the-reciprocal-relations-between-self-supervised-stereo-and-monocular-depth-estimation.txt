Abstract
Current self-supervised depth estimation algorithms mainly focus on either stereo or monocular only, neglect-ing the reciprocal relations between them. In this paper, we propose a simple yet effective framework to improve both stereo and monocular depth estimation by leveraging the underlying complementary knowledge of the two tasks. Our approach consists of three stages. In the ﬁrst stage, the pro-posed stereo matching network termed StereoNet is trained on image pairs in a self-supervised manner. Second, we introduce an occlusion-aware distillation (OA Distillation) module, which leverages the predicted depths from Stere-oNet in non-occluded regions to train our monocular depth estimation network named SingleNet. At last, we design an occlusion-aware fusion module (OA Fusion), which gener-ates more reliable depths by fusing estimated depths from
StereoNet and SingleNet given the occlusion map. Fur-thermore, we also take the fused depths as pseudo labels to supervise StereoNet in turn, which brings StereoNet’s performance to a new height. Extensive experiments on
KITTI dataset demonstrate the effectiveness of our proposed framework. We achieve new SOTA performance on both stereo and monocular depth estimation tasks.
Figure 1. Characteristics of stereo and monocular models. In the upper part, we paste a car ‘instance’ to both left image Il and right image Ir, respectively. Dl(stereo) and Dl(mono) are left dispar-ity maps generated from the stereo and monocular models, where the brighter color means bigger disparity.
In the lower left, we move the car in left image a distance to the right, and the estimated disparity of the car becomes larger, as shown in D(cid:48) l(stereo). In the lower right, we shrink the car, and see the corresponding dis-parity becomes smaller, as shown in D(cid:48)(cid:48) l(mono). 1.

Introduction
Depth estimation from either stereo image pairs or monocular images is a fundamental problem in computer vision. It has been extensively studied due to its wide ap-plications in robotic manipulation[37], augmented reality
[35, 27] and autonomous driving [26, 43]. Current super-vised depth estimation methods [4, 13], though tremendous progress has been achieved, require costly dense ground-truth data for training. Alternatively, self-supervised meth-ods are getting increasing attention in recent years[10, 40, 3], which only requires stereo or monocular raw images.
Recent SOTA self-supervised methods mainly focus on one of monocular or stereo depth estimation problems, ne-glecting the reciprocal relations between them. On the one hand, stereo matching approaches aim at learning structural information by comparing the similarity of local left and right patches to obtain the optimal disparity and seeking a globally smooth disparity map. Thus for left boundaries and occlusions where only a single view can be seen, unsuper-vised stereo matching methods often fail to learn reliable depth. On the other hand, monocular depth estimation is an inherently ill-posed problem and it mainly relies on the ap-pearance or semantic knowledge inside the features. Thus it is robust to occluded regions. As shown in Fig. 1, we con-duct the dummy experiments to elaborate the observation.
To fully exploit the complementary knowledge of the two tasks, in this paper, we design a simple but effective framework to integrate the advantages of both stereo and monocular depth estimation networks. Generally, we train a stereo depth estimation network, named StereoNet in a self-supervised manner. Due to the invisible characteristic, estimated depth in occluded regions are not reliable. Thus an occlusion-aware distillation strategy is adopted to extract the visible estimated depths from StereoNet. Different from methods [46, 11] that adopt stereo images for novel view synthesis by a left-right depth consistency term, we propose a monocular depth estimation framework named SingleNet, under the supervision of distilled depth from StereoNet and observe a considerable improvement. The gain can be at-tributed to two main reasons. First, StereoNet learns more reliable depth given the stereo structural knowledge in vis-ible regions than SingleNet under the same self-supervised training. Second, our occlusion-aware distillation strategy only adopts non-occluded depths as supervision to guide the SingleNet to learn semantic information. Furthermore, not only StereoNet can help to train SingleNet, but also
SingleNet can be leveraged to improve StereoNet in turn.
Even though StereoNet is generally more accurate than Sin-gleNet, we observe that SingleNet still performs better than
StereoNet on occluded pixels. Especially, along the bound-ary region of objects, SingleNet tends to preserve sharp edge across object borders while bleeding artifacts are obvi-ous for StereoNet. Inspired by this observation, we propose an occlusion-aware fusion strategy, which fuses estimated depth maps from both StereoNet and SingleNet given the occlusion map. The fused depth map gives full play to its strength of the structure-based StereoNet and appearance-based SingleNet. A further hint can be conducted by adopt-ing the fused depth as pseudo-labels for supervision to train
StereoNet in turn to further improve the performance of self-supervised StereNet.
In summary, the main contributions of this work are listed below in threefold:
• We propose a simple yet effective framework to boost the performance of self-supervised stereo and monoc-ular depth estimation by mining task-speciﬁc strengths and revealing the reciprocal relations of the two tasks.
• We put forward a novel occlusion-aware distillation strategy for training monocular depth estimation net-works as well as an effective occlusion-aware fusion strategy that combines the advantages of the structure-based stereo depth estimation and the appearance-based monocular depth estimation.
• Extensive experiments on the KITTI benchmark shows that our method establishes new SOTA performances on both stereo and monocular depth estimation tasks. 2.