Abstract
Vision-language Navigation (VLN) tasks require an agent to navigate step-by-step while perceiving the visual observations and comprehending a natural language in-struction. Large data bias, which is caused by the dis-parity ratio between the small data scale and large navi-gation space, makes the VLN task challenging. Previous works have proposed various data augmentation methods to reduce data bias. However, these works do not explicitly reduce the data bias across different house scenes. There-fore, the agent would overfit to the seen scenes and achieve poor navigation performance in the unseen scenes. To tackle this problem, we propose the Random Environmen-tal Mixup (REM) method, which generates cross-connected house scenes as augmented data via mixuping environment.
Specifically, we first select key viewpoints according to the room connection graph for each scene. Then, we cross-connect the key views of different scenes to construct aug-mented scenes. Finally, we generate augmented instruction-path pairs in the cross-connected scenes. The experimental results on benchmark datasets demonstrate that our aug-mentation data via REM help the agent reduce its perfor-mance gap between the seen and unseen environment and improve the overall performance, making our model the best existing approach on the standard VLN benchmark. 1.

Introduction
Recently, there is a surge of research interests in Vision-Language Navigation (VLN) [4] tasks, in which an agent learns to navigate by following a natural language instruc-tion. The agent begins at a random point and goes towards
*Equal Contribution
†Corresponding author
Figure 1. REM mixes up two scenes and generates data triplets (environment, path, instruction). We divide the two scenes and recombine them to construct a new cross-connected scene, and re-construct the corresponding paths and instructions. a goal via actively exploring the environments. Before the navigation starts, the agent receives a language instruction.
At every step, the agent can get the surrounding visual infor-mation. The key to this task is to perceive the visual scene and comprehend natural language instructions sequentially and make actions step-by-step.
Recent advances made by deep learning works in the domains of feature extraction [19, 3, 40, 43], attention [3, 15, 35] and multi-modal grounding [5, 34, 50] facilitate the agent to understand the environment. Moreover, many rein-forcement learning works [41, 26, 48] help the agent to ob-tain a robust navigation policy. Benefited from these works, previous attempts in the field of Vision-Language Naviga-tion have made great progress in improving the ability to perceive the vision and language inputs [17, 16, 56, 55], and learning a robust navigation policy [62, 18, 23]. How-ever, the VLN task still contains large bias due to the dis-parity ratio between small data scale and large navigation space, which impacts the generalization ability of naviga-tion. Although the mostly widely used dataset, Room-to-room dataset [4], contains only 22K instruction-path pairs, the actual possible navigation path space increases expo-nentially along with the path length. Thus, the learned nav-igation policy can easily overfit to the seen scenes and is hard to generalize to the unseen scenes.
Previous works have proposed various data augmenta-tion methods in an attempt to reduce data bias. Fried et al. propose a speaker-follower framework [16] to generate more data pairs in order to reduce the data bias of the data samples. Tan et al. [51] propose an environmental dropout method to augment the vision features in environments; thereby reducing the vision bias inside a house scene. How-ever, these methods focus on intra-scene data augmentation and fail to explicitly reduce the data bias across different house scenes.
Accordingly, in this paper, we propose to reduce the do-main gap across different house scenes by means of scene-wise data augmentation.
If an agent sees different house scenes during a navigation process, it will be less likely to overfit to a part of the scene textures or room structures. In-spired by this motivation, we propose our method, named
Random Environmental Mixup (REM), to improve the gen-eralization ability of a navigation agent. REM breaks up two scenes and the corresponding paths, followed by re-combining them to obtain a cross-connected scene between the two scenes. The REM method provides more general-ized data, which helps reduce the generalization error, so that the agent’s navigation ability in the seen and unseen scenes can be improved.
The REM method comprises three steps. First, REM se-lects the key vertexes in the room connection graph accord-ing to the betweenness centrality [8]. Second, REM splits the scenes by the key vertexes and cross-connect them to generate new augmented scenes. We propose an orienta-tion alignment approach to solve the feature mismatch prob-lem. Third, REM splits trajectories and instructions into sub-trajectories and sub-instructions by their context, then cross-connects them to generate augmented training data.
An overview of the REM method is presented in Fig. 1.
The experimental results on benchmark datasets demon-strate that REM can significantly reduce the performance gap between seen and unseen environments, which dramat-ically improves the overall navigation performance. Our ab-lation study shows that the proposed augmentation method outperforms other augmentation methods at the same aug-mentation data scales. Our final model obtains 59.1% in
Success weighted by Path Length (SPL) [2], which is 2.4% higher than the previous state-of-the-art result; accordingly, our method becomes the new state-of-the-art method on the standard VLN benchmark. 2.