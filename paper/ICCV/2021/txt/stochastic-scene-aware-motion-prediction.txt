Abstract
A long-standing goal in computer vision is to capture, model, and realistically synthesize human behavior. Specif-ically, by learning from data, our goal is to enable vir-tual humans to navigate within cluttered indoor scenes and naturally interact with objects. Such embodied behavior has applications in virtual reality, computer games, and robotics, while synthesized behavior can be used as train-ing data. The problem is challenging because real human motion is diverse and adapts to the scene. For example, a person can sit or lie on a sofa in many places and with vary-ing styles. We must model this diversity to synthesize vir-tual humans that realistically perform human-scene inter-actions. We present a novel data-driven, stochastic motion synthesis method that models different styles of perform-ing a given action with a target object. Our Scene-Aware
Motion Prediction method (SAMP) generalizes to target ob-jects of various geometries while enabling the character to navigate in cluttered scenes. To train SAMP, we collected
MoCap data covering various sitting, lying down, walking, and running styles. We demonstrate SAMP on complex in-door scenes and achieve superior performance than exist-ing solutions. Code and data are available for research at https://samp.is.tue.mpg.de. 1.

Introduction
The computer vision community has made substantial progress on 3D scene understanding and on capturing 3D human motion, but less work has focused on synthesizing 3D people in 3D scenes. The advances in these two sub-ﬁelds, however, have provided tools for, and have created interest in, embodied agents for virtual worlds (e.g. [35, 42, 55, 56]) and in placing humans into scenes (e.g. [6, 21]).
Creating virtual humans that move and act like real people, however, is challenging and requires tackling many smaller but difﬁcult problems such as perception of unseen environ-ments, plausible human motion modeling, and embodied in-teraction with complex scenes. While advances have been made in human locomotion modeling [23, 32] thanks to the availability of large scale datasets [7, 33, 38, 45, 50], realis-tically synthesizing virtual humans moving and interacting with 3D scenes, remains largely unsolved.
Imagine instructing a virtual human to “sit on a couch” in a cluttered scene, as illustrated in Fig. 1. To achieve this goal, the character needs to perform a series of com-plex actions. First, it should navigate through the scene to reach the target object while avoiding collisions with other objects in the scene. Next, the character needs to choose a contact point on the couch that will result in a plausi-ble sitting action facing the right direction. Finally, if the character performs this action multiple times, there should be natural variations in the motion, mimicking real-world human-scene interactions; e.g., sitting on different parts of the couch with different styles such as with crossed legs, arms in different poses, etc. Achieving these goals re-quires a system to jointly reason about the scene geome-try, smoothly transition between cyclic (e.g., walking) and acyclic (e.g., sitting) motions, and to model the diversity of human-scene interactions.
To this end, we propose SAMP for Scene-Aware Mo-tion Prediction. SAMP is a stochastic model that takes a 3D scene as input, samples valid interaction goals, and gen-erates goal-conditioned and scene-aware motion sequences of a character depicting realistic dynamic character-scene interactions. At the core of SAMP is a novel autoregres-sive conditional variational autoencoder (cVAE) called Mo-tionNet. Given a target object and an action, MotionNet samples a random latent vector at each frame to condition the next pose both on the previous pose of the character as well as the random vector. This enables MotionNet to model a wide range of styles while performing the target action. Given the geometry of the target object, SAMP fur-ther uses another novel neural network called GoalNet to generate multiple plausible contact points and orientations on the target object (e.g., different positions and sitting ori-entations on the cushions of a sofa). This component en-ables SAMP to generalize across objects with diverse ge-ometry. Finally, to ensure the character avoids obstacles while reaching the goal in a cluttered scene, we use an ex-plicit path planning algorithm (A* search) to pre-compute an obstacle-free path between the starting location of the character and the goal. This piecewise linear path consists of multiple way-points, which SAMP treats as intermediate goals to drive the character around the scene. SAMP runs in real-time at 30 fps. To the best of our knowledge, these individual components make SAMP the ﬁrst system that ad-dresses the problem of generating diverse dynamic motion sequences that depict realistic human-scene interactions in cluttered environments.
Training SAMP requires a dataset of rich and diverse character scene interactions. Existing large-scale MoCap datasets are largely dominated by locomotion and the few interaction examples lack diversity. Additionally, tradi-tional MoCap focuses on the body and rarely captures the scene. Hence, we capture a new dataset covering various human-scene interactions with multiple objects.
In each motion sequence, we track both the body motion and the object using a high resolution optical marker MoCap sys-tem. The dataset is available for research purposes.
Our contributions are: (1) A novel stochastic model for synthesizing varied goal-driven character-scene interac-tions in real-time. (2) A new method for modeling plausi-ble action-dependent goal locations and orientations of the body given the target object geometry. (3) Incorporating explicit path planning into a variational motion synthesis network enabling navigation in cluttered scenes. (4) A new
MoCap dataset with diverse human-scene interactions. 2.