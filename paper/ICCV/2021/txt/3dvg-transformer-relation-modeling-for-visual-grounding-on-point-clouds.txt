Abstract
Visual grounding on 3D point clouds is an emerging vi-sion and language task that benefits various applications in understanding the 3D visual world. By formulating this task as a grounding-by-detection problem, lots of re-cent works focus on how to exploit more powerful detec-tors and comprehensive language features, but (1) how to model complex relations for generating context-aware ob-ject proposals and (2) how to leverage proposal relations to distinguish the true target object from similar proposals are not fully studied yet. Inspired by the well-known trans-former architecture, we propose a relation-aware visual grounding method on 3D point clouds, named as 3DVG-Transformer, to fully utilize the contextual clues for relation-enhanced proposal generation and cross-modal proposal disambiguation, which are enabled by a newly designed coordinate-guided contextual aggregation (CCA) module in the object proposal generation stage, and a multiplex attention (MA) module in the cross-modal feature fusion stage. We validate that our 3DVG-Transformer outper-forms the state-of-the-art methods by a large margin, on two point cloud-based visual grounding datasets, ScanRe-fer and Nr3D/Sr3D from ReferIt3D, especially for complex scenarios containing multiple objects of the same category. 1.

Introduction
As one emerging 3D visual understanding task, visual grounding on point clouds, also called as referring 3D ob-ject localization, aims to locate the desired objects or re-gions in input point cloud from the given textual descrip-tions. The visual grounding technologies would signif-icantly benefit various real-life applications such as au-tonomous robots, AR/VR, etc. Even though much progress has been made in visual grounding on 2D images [1, 2, 3, 4, 5], it is still a challenging task to design a reliable point-based visual grounding scheme that can well understand the
* First two authors contributed equally.
† Corresponding author: Lu Sheng. relations in complex 3D scenes and distinguish the propos-als of the target object from other similar proposals.
Recently, Chen et al. [6] and Achlioptas et al. [7] pro-posed to tackle visual grounding on 3D point clouds by for-mulating it as a grounding-by-detection problem, together with two newly developed datasets (i.e., ScanRefer [6] and
ReferIt3D [7]). To be specific, they first use the state-of-the-art (SOTA) 3D object detector [8] or the ground-truth (GT) bounding boxes to generate object proposals, whose fea-tures are then fused with the language features from the lin-guistic query to predict the most confident proposals. Since then, several variants, such as TGNN [9] and InstanceRe-fer [10], were proposed to leverage instance segmenta-tion [11] and specially designed linguistic features for bet-ter localization and fine-grained matching between the two modalities. However, these methods still suffer from some critical issues: (1) how to model complex relations (e.g. rel-ative spatial locations) within each point cloud, (2) how to exploit various relations among proposals to distinguish the true target object from similar proposals with the aid of tex-tual descriptions. Thus the recent methods [6, 7, 9, 10] usu-ally fail to localize the target object when the input scenes contain multiple objects from the same category. Moreover, due to the relatively small scales of recent visual ground-ing datasets, the existing methods also suffer from the over-fitting problem, which also prevents these methods from learning a generalizable visual grounding model.
To this end, we propose a relation-aware visual ground-ing method on 3D point clouds, named as 3DVG-Transformer. While our method follows the ground-by-detection strategy from ScanRefer [6], we additionally ex-ploit various relations among proposals at both the object proposal generation stage and the cross-modal fusion stage, based on the powerful relation modeling capability by the well-known transformer architecture [12]. To be specific, in the object proposal generation stage, after producing the cluster centers and features as the initial object proposals, we propose a coordinate-guided contextual aggregation (CCA) module, which stacks a set of coordinate-guided transformer layers to extract multi-level context-aware rep-resentations from both neighboring proposals and the back-ground. Within each transformer layer, we add a new block-wise sparse spatial proximity matrix to the attention ma-trix at each multi-head attention module, so as to explic-itly describe relative spatial locations from proposals in each query proposal’s vicinity. At the cross-modal fusion stage, the word features extracted from the language encod-ing module and the proposal features from the selected pro-posals are fused with a multiplex attention (MA) module.
The multiplex attention module consists of a stack of in-terlaced self-attention and cross-attention blocks, where the self-attention block enhances contextual relationships be-tween proposals and the cross-attention block passes mes-sages from the word features to the proposal features. This module distinguishes the true grounding results from other proposals with the aid of comprehensive contextual knowl-edge within the point cloud and across visual and linguistic domains. The output from the cross-modal fusion module is directly fed into a feed-forward network (FFN) to predict the object confidence score for each proposal. Moreover, we optionally employ a pair of feature augmentation strate-gies for both modalities, (i.e., proposal copy & paste, and word erase), which also benefit the training process.
The contribution of this work is three-fold: (1) A simple and strong visual grounding framework (referred to as 3DVG-Transformer) specifically designed for point clouds, which comprehensively models various relations for relation-enhanced proposal generation and cross-modal proposal disambiguation. (2) A new coordinate-guided con-textual aggregation module for extracting multi-level con-textual features within point clouds, and a multiplex at-tention module for disambiguating the grounding results.
Both modules are inspired by the transformer architec-ture [12]. (3) The state-of-the-art visual grounding perfor-mance on the ScanRefer dataset [6] and Nr3D/Sr3D from the ReferIt3D dataset [7]. Our method significantly outper-forms the baselines [6, 7, 9, 10] on complex scenes with multiple objects from the same category. 2.