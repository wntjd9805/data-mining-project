Abstract
Explainable artificial intelligence has been gaining at-tention in the past few years. However, most existing meth-ods are based on gradients or intermediate features, which are not directly involved in the decision-making process of the classifier.
In this paper, we propose a slot attention-based classifier called SCOUTER for transparent yet ac-curate classification. Two major differences from other attention-based methods include: (a) SCOUTER’s expla-nation is involved in the final confidence for each category, offering more intuitive interpretation, and (b) all the cat-egories have their corresponding positive or negative ex-planation, which tells “why the image is of a certain cate-gory” or “why the image is not of a certain category.” We design a new loss tailored for SCOUTER that controls the model’s behavior to switch between positive and negative explanations, as well as the size of explanatory regions. Ex-perimental results show that SCOUTER can give better vi-sual explanations in terms of various metrics while keeping good accuracy on small and medium-sized datasets. Code is available1. 1.

Introduction
It is of great significance to know how deep models make predictions, especially for the fields like medical diagno-sis, where potential risks exist when black-box models are adopted. Explainable artificial intelligence (XAI), which can give a close look into models’ inference process, there-fore has gained lots of attention. 1https://github.com/wbw520/scouter
The most popular paradigm in XAI is attributive expla-nation, which gives the contribution of pixels or regions to the final prediction [27, 7, 24, 26]. One natural question that arises here is how these regions contribute to the decision.
For a better view of this, let gl(v) = w⊤ l v + bl denotes a fully-connected (FC) classifier for category l, where wl and bl are trainable vector and scalar, respectively. Training this classifier may be interpreted as a process to find from the training samples a combination of discriminative patterns sli with corresponding weight γi, i.e., (cid:33) (cid:32) gl(v) = (cid:88)
γis⊤ li v + bl. (1) i
In general, these patterns can include positive and negative ones. Given v of an image of l, a positive pattern gives s⊤ li v > 0. A negative pattern, in contrast, gives s⊤ li v < 0 for v of any category other than l, which means that the presence of pattern described by sli is a support of not being category l. Therefore, set Sl of all (linearly independent) patterns for l can be the union of sets S + l of all positive and negative patterns. l and S −
Differentiation of positive/negative patterns gives use-ful information on the decision. Figure 1(top) shows an
MNIST image for example. One of positive patterns that makes the image being 7 can be the acute angle formed by white line segments that appears around the top-right cor-ner, as in the second image. Meanwhile, the sixth image shows that the presence of the horizontal line is the support not being 1. A more practical application [2] in medical image analysis also points out the importance of visualizing positive/negative patterns. Nevertheless of the obvious ben-efit, recent mainstream methods like [44, 27, 23, 10] have not extensively studied this differentiation.
Positive and negative patterns lead to two interesting questions: (i) Can we provide positive explanation and neg-ative explanation that visually show support regions in the image that correspond to positive and negative patterns? (ii)
As the combination of patterns to be learned is rather arbi-trary and any combination is possible as long as it is dis-criminative; can we provide preference on the combination in order to leverage prior knowledge on the task in training?
In this paper, we re-formulate explainable AI with an ex-plainable classifier, coined SCOUTER (Slot-based COnfig-Urable and Transparent classifiER), which tries to find ei-ther positive or negative patterns in images. This approach is similar to the attention-based approach (e.g. [17, 35]) rather than the post-hoc approaches [44, 27, 26]. Our newly proposed explainable slot attention (xSlot) module is the main building block of SCOUTER. This module is built on top of the recently-emerged slot attention [19], which offers an object-centric approach for image representation. The xSlot module identifies the spatial support of either positive or negative patterns for each category in the image, which is directly used as the confidence value of that category; the commonly-used FC classifier is no longer necessary. The xSlot module can also be used to visualize the support as shown in Fig. 1. SCOUTER is also characterized by its configurability over patterns to be learned, i.e., the choice of positive or negative pattern and the desirable size of the pattern, which can incorporate the prior knowledge on the task. The controllable size of explanation can be beneficial for some applications, e.g., disease diagnosis in medicine, defect recognition in manufacturing, etc.
Contribution Our transparent classifier, SCOUTER, ex-plicitly models positive and negative patterns with a dedi-cated loss, allowing to set preference over the spatial size of patterns to be learned. We experimentally show that
SCOUTER successfully learns both positive and negative patterns and visualize their support in the given image as the explanation, achieving state-of-the-art results in several commonly-used metrics like IAUC/DAUC [23]. Our case study in medicine also highlights the importance of both types of explanations as well as controlling the area size of explanatory regions. 2.