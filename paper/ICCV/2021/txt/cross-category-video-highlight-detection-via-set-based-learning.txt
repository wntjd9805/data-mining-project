Abstract (cid:75)(cid:76)(cid:74)(cid:75)(cid:79)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:86)(cid:72)(cid:74)(cid:80)(cid:72)(cid:81)(cid:87) (cid:81)(cid:82)(cid:81)(cid:16)(cid:75)(cid:76)(cid:74)(cid:75)(cid:79)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:86)(cid:72)(cid:74)(cid:80)(cid:72)(cid:81)(cid:87) (cid:88)(cid:81)(cid:79)(cid:68)(cid:69)(cid:72)(cid:79)(cid:72)(cid:71)(cid:3)(cid:86)(cid:72)(cid:74)(cid:80)(cid:72)(cid:81)(cid:87)
Autonomous highlight detection is crucial for enhanc-ing the efﬁciency of video browsing on social media plat-forms.
To attain this goal in a data-driven way, one may often face the situation where highlight annotations are not available on the target video category used in practice, while the supervision on another video category (named as source video category) is achievable.
In such a situation, one can derive an effective highlight detec-tor on target video category by transferring the highlight knowledge acquired from source video category to the tar-get one. We call this problem cross-category video high-light detection, which has been rarely studied in previous works. For tackling such practical problem, we propose a
Dual-Learner-based Video Highlight Detection (DL-VHD) framework. Under this framework, we ﬁrst design a Set-based Learning module (SL-module) to improve the con-ventional pair-based learning by assessing the highlight ex-tent of a video segment under a broader context. Based on such learning manner, we introduce two different learners to acquire the basic distinction of target category videos and the characteristics of highlight moments on source video category, respectively. These two types of highlight knowledge are further consolidated via knowledge distilla-tion. Extensive experiments on three benchmark datasets demonstrate the superiority of the proposed SL-module, and the DL-VHD method outperforms ﬁve typical Unsu-pervised Domain Adaptation (UDA) algorithms on various cross-category highlight detection tasks. Our code is avail-able at https://github.com/ChrisAllenMing/
Cross_Category_Video_Highlight. 1.

Introduction
In current days, people show growing interests in sharing the videos recording their daily life on the social media plat-forms like YouTube and Instagram. Among all these videos, the well-edited ones that summarize the highlights of spe-*Corresponding author: Bingbing Ni. (cid:54)(cid:82)(cid:88)(cid:85)(cid:70)(cid:72)(cid:3)(cid:70)(cid:68)(cid:87)(cid:72)(cid:74)(cid:82)(cid:85)(cid:92)(cid:3)(cid:11)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:68)(cid:81)(cid:81)(cid:82)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:12)(cid:29) surfing (cid:55)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:3)(cid:70)(cid:68)(cid:87)(cid:72)(cid:74)(cid:82)(cid:85)(cid:92)(cid:3)(cid:11)(cid:88)(cid:86)(cid:72)(cid:3)(cid:76)(cid:81)(cid:3)(cid:83)(cid:85)(cid:68)(cid:70)(cid:87)(cid:76)(cid:70)(cid:72)(cid:12)(cid:29)(cid:3)skiing
Figure 1. The situation in which the target video category used in practice lacks supervision, while another video category, i.e. the source one, possesses annotation. ciﬁc events are apparently more attractive to the audience.
However, in most cases, the original video of a real-world event contains many contents unrelated to its gist, and it is an onerous and time-consuming task to pick out the high-light parts of the video manually. Therefore, in order to en-hance the efﬁciency of video content reﬁnement, it is desir-able to develop a machine learning model for autonomous video highlight detection.
To endow a model with the capability of identifying the highlight segments within a video, existing works have ex-plored various ways of supervision, including the explicit highlight annotations [10, 49, 15], the frequent occurrence of speciﬁc video segments [23, 48, 21], the duration of a video [41], etc. These approaches generally focused on training a highlight detector for a speciﬁc video category (e.g. surﬁng, skiing, parkour, etc.), while the transferability of a highlight detection model across different video cate-gories has been less studied in previous works.
As a matter of fact, in practical applications, one can face the situation where supervisory signal is lacked on the tar-get video category intended to be used in practice, while the supervision on another video category is available, just as shown in Fig. 1. Under such situation, we consider the problem of Cross-category Video Highlight Detection. The setting of this problem is analogous to that of Unsupervised
Domain Adaptation (UDA) [20] in which one seeks to adapt
the knowledge learned from the labeled source domain (the source video category with supervision) to the unlabeled target domain (the unsupervised target video category).
In addition, for optimizing the highlight detector, most of existing methods [10, 49, 15, 23, 41, 14] followed the philosophy of pair-based learning, i.e. comparing a positive sample (e.g. a highlight video segment or a segment bag containing highlights) with a negative one, and, after train-ing, the former is expected to rank higher than the latter.
Nevertheless, such learning manner might not fully exploit the contextual information spanning among different video segments. For example, in a soccer match, the moment of a player’s dribbling the ball is more attractive than the one of the players’ entering the pitch, and both of them are less exciting than the moment of a goal. These relationships can hardly be captured by a single segment pair, which makes the highlight prediction of a pair-learning-based model po-tentially imprecise in the span of a whole video.
Motivated by the facts above, in this work, we propose a
Dual-Learner-based Video Highlight Detection (DL-VHD) framework to address the Cross-category Video Highlight
Detection problem. Under this framework, we ﬁrst devise a Set-based Learning module (SL-module) to improve the conventional pair-based learning manner for highlight de-tection. In a nutshell, this module learns to regress the high-light score distribution over a set of segments from the same video, in which a Transformer encoder [37] is employed to model the interrelationship among various video segments.
Based on this learning mechanism, we further introduce two different learners to capture two types of knowledge about highlight moments. In speciﬁc, the coarse-grained learner gains the basic concepts about what distinguishes the videos of target category from other ones, and the ﬁne-grained learner acquires the precise highlight notions on source videos. These two kinds of knowledge are further integrated by distilling each of them into the other learner, and such integrated knowledge forms the more complete concepts about the highlight moments on target video cate-gory. In practice, the SL-module can be individually applied to derive an effective highlight detector when the segment-level annotation is available on the target video category, while, when such annotation is unobtainable, we can resort to the DL-VHD method for highlight knowledge transfer.
Our contributions can be summarized as follows:
• To the best of our knowledge, this work is the ﬁrst at-tempt at cross-category video highlight detection, in which we utilize a dual-learner-based scheme to trans-fer the concepts about highlight moments across dif-ferent video categories.
• We propose a novel set-based learning mechanism which is able to identify whether a video segment is highlight or not under a broader context.
• Under the category-speciﬁc setting, we verify the su-perior performance of the SL-module over previous methods. For cross-category highlight detection, the
DL-VHD model substantially surpasses existing UDA algorithms and performs comparably with the super-vised model trained on target video category. 2.