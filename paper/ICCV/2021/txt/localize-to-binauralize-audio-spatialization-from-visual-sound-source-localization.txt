Abstract
Videos with binaural audios provide immersive viewing experience by enabling 3D sound sensation. Recent works attempt to generate binaural audio in a multimodal learning framework using large quantities of videos with accompa-nying binaural audio. In contrast, we attempt a more chal-lenging problem – synthesizing binaural audios for a video with monaural audio in a weakly semi-supervised setting.
Our key idea is that any down-stream task that can be solved only using binaural audios can be used to provide proxy supervision for binaural audio generation, thereby reduc-ing the reliance on explicit supervision. In this work, as a proxy-task for weak supervision, we use Sound Source Lo-calization with only audio. We design a two-stage architec-ture called Localize-to-Binauralize Network (L2BNet). The
ﬁrst stage of L2BNet is a Stereo Generation (SG) network employed to generate two-stream audio from monaural au-dio using visual frame information as guidance. In the sec-ond stage, an Audio Localization (AL) network is designed to use the synthesized two-stream audio to localize sound sources in visual frames. The entire network is trained end-to-end so that the AL network provides necessary supervi-sion for the SG network. We experimentally show that our weakly-supervised framework generates two-stream audio containing binaural cues. Through user study, we further validate that our proposed approach generates binaural-quality audio using as little as 10% of explicit binaural su-pervision data for the SG network. 1.

Introduction
The perception of movement is primarily guided by rich visual cues in animals. This can be attributed to its evolu-tionary advantages over other modalities like sound. How-ever, audio contains localization information that can be ex-ploited, albeit not as richly as the visual modality. For ex-ample, consider a situation where you are talking to a friend on the sidewalk. The road is busy. Even if you are not fac-(a) (b)
Figure 1. (a) Binaural cues enable us to localize different instru-ments in live musical performances even with our eyes closed. (b)
We use saliency maps to model localization information and use it to convert monaural audio to binaural audio. ing the road, you are aware of vehicles zooming past. If you listen carefully, you can also ﬁgure out the direction of movement. Fig. 1(a) illustrates a similar phenomenon for live musical performances.
According to [18], there are two major factors that al-low us to spatialize using just the sounds we hear, namely,
Interaural Time Difference (ITD) and Interaural Level Dif-ference(ILD). ITD is the time difference between the same sound reaching each ear, and ILD is the amplitude differ-ence between them. In the commonly used recording se-tups, a monoaural track is recorded using a single micro-phone. It aggregates sound signals from various sources and consequently loses such difference information. Binaural recordings use two microphones embedded inside a dummy head-model with realistic ear pinna to model closely the sound that would have fallen on a person’s eardrums had they been present on site and preserves the localization in-formation.
Despite being introduced many years ago, binaural audio recordings are relatively scarce, especially for videos. This can be attributed primarily to the fact that special equip-ment with multiple microphones is required for recording
it. Devices with multiple microphones are expensive and creating custom recording hardware, like the dummy-head mentioned earlier, is both expensive and involved. As such, there is a need to develop computational approaches that can make binaural audio more accessible as it can have a far-reaching impact. When accompanied by binaural audio tracks, videos provide a more immersive 3D experience for the viewers by enabling sound spatialization. Such expe-riences are of high interest to AR/VR enthusiasts and au-diophiles. For people with visual impairments, being able to localize using the binaural audio track would enrich the experience of listening to movies and could possibly con-tribute towards making the cinematic experience more in-clusive.
Recent works [5, 31, 14], attempt this problem of gen-erating stereo audio in a self-supervised learning setup us-ing videos with accompanying stereo audio. Another recent work [32], has attempted to utilize the abundantly available videos with monaural audio to enhance the quality of stereo sound generated. However, both these works require signif-icant amounts of video recordings with binaural audio and creating such datasets is cumbersome.
As part of this work, we investigate how well we can lo-calize an object based only on information extracted from binaural audio. We subsequently attempt to leverage this localization capability to solve the inverse problem of gen-erating stereo tracks from monaural audio with minimal su-pervision. Through this work, we seek to establish that the efﬁcient and commonly available object localization net-works can be used to provide weak supervision for stereo-audio generation task while reducing the amount of binau-ral recordings needed for self-supervision. Fig. 1(b) gives an overview of our main idea.
Building on the idea that the complementary nature and natural synchronization of the video and audio stream are sufﬁcient for stereo-sound generation given real binaural audio supervision, we try to minimize the amount of bin-aural recordings needed for the task. To this end, we solve the proxy-downstream task of sound source localization to provide the necessary weak supervision for binaural audio generation. This choice was made based on the observation that stereo-audio alone can be successfully used to localize a sound-making object in an accompanying video [4] while using only monaural audio fails. As such, if the audio being generated is able to localize well, we can conclude that it be-longs to the subset of audios that contain localization infor-mation. However, we cannot be sure that this mapping be-tween stereo audio and localization is unique. To force the network to generate only real binaural audio and not spuri-ous solutions, we experiment by providing small amounts of explicit supervision to the network. Results show that as little as 10% of the total binaural recordings are enough to generate binaural-quality audios. The key novelties of our work can be listed as below-• We propose an end to end model to convert a monau-ral audio accompanying a video to stereo audio using localization as the weak supervision.
• We also show that guiding the stereo audio generation task using localization helps reduce the amount of bin-aural recordings needed for learning to 10% of that needed by the present state of the art. 2.