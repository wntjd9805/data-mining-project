Abstract
This paper tackles the task of category-level pose esti-mation for garments. With a near infinite degree of free-dom, a garment’s full configuration (i.e., poses) is often described by the per-vertex 3D locations of its entire 3D surface. However, garments are also commonly subject to extreme cases of self-occlusion, especially when folded or crumpled, making it challenging to perceive their full 3D surface. To address these challenges, we propose Garment-Nets, where the key idea is to formulate the deformable ob-ject pose estimation problem as a shape completion task in the canonical space. This canonical space is defined across garments instances within a category, therefore, specifies the shared category-level pose. By mapping the observed partial surface to the canonical space and completing it in this space, the output representation describes the gar-ment’s full configuration using a complete 3D mesh with the per-vertex canonical coordinate label. To properly han-dle the thin 3D structure presented on garments, we pro-posed a novel 3D shape representation using the general-ized winding number field. Experiments demonstrate that
GarmentNets is able to generalize to unseen garment in-stances and achieve significantly better performance com-pared to alternative approaches. Code and data can be found in https://garmentnets.cs.columbia.edu. 1.

Introduction
Garments are one of the most common objects in our life, yet they pose a set of unique properties that make them incredibly difficult for machines to perceive and interact:
• Infinite degree of freedom (DoF): in contrast to rigid objects whose pose can be fully specified as a low-dimensional vector, a piece of garment has near infinite
DoF, i.e., to fully specify its configuration (i.e., pose), we need to describe positions of all 3D points on the gar-ment surface. This issue is compounded when we con-sider category-level generalization, where there are infi-nite poses can be considered as the “canonical” for differ-ent garments instances.
Figure 1. Category-level Pose Estimation for Garments. The key idea of GarmentNets is to formulate the garment pose estima-tion problem as a shape completion task in the canonical space.
This canonical space is defined across garment instances within a category, therefore, specifies the shared category-level pose. The output describes the garment’s full configuration using a complete 3D mesh with per-vertex canonical coordinate label. crumpled. This property makes it particular challenging and sometimes ambiguous to perceive their full configu-ration from partial visual observation.
• Thin structure: garments often consist of thin 3D geo-metric structures that are not water-tight. This unique geometric property makes them ill-suited for typical 3D shape representations designed for solid rigid objects (e.g., occupancy grid or signed distance functions).
Due to these challenges, prior works on garments or cloth perception often build on simplifying assumptions such as full visibility [19], known instance-level mesh
[16, 24, 5], known physics or full state information in the initial observation [24], where the problem is reduced to an instance-level tracking task. As a result, these algorithms cannot generalize to new garment instances that are not ob-served during training.
• Severe self-occlusion: garments are often subject to ex-treme cases of self-occlusion, especially when folded or
To address these challenges, we propose GarmentNets, an end-to-end neural network that estimates the full config-uration of a garment from a partial observation. The algo-rithm highlights following key ideas:
To handle the infinite DoF and enable category-level generalization, we define a normalized coordinate space for each garment category using a canonical human pose.
This representation allows the algorithm to learn seman-tically meaningful correspondences between garment in-stances with different styles, shapes, or configurations.
To handle self-occlusions, the algorithm explicitly per-forms shape completion under its canonical pose, which al-lows the algorithm to fully specify the garment configura-tion even when the observed surface is incomplete.
To handle thin structures, we propose a novel 3D shape representation using winding number field (WNF) [12].
This representation allows the algorithm to accurately rep-resent thin cloth structures with strong gradient on the sur-face but continuous and smooth elsewhere, providing a more meaningful signal for the network to learn better geo-metric features.
We study the garments perception task in the context of robot manipulation, which is more challenging than the on-body garments perception (i.e., the garment being worn by people) since the number of possible configurations is much larger and the potential self-occlusion is more severe. How-ever, this setup also allows us to leverage simple robot inter-actions to reduce the possible configuration space. For ex-ample, we allow the robot to first lifts a crumpled garment with a random pick point and allows the gravity force to naturally pulls the cloth into a stable pose. The system then takes four RGB-D images of the cloth by rotating the grip-per. This task formulation potentially allows our perception algorithm to be used in a realistic robot manipulation task.
To the best of our knowledge, we are the first to en-able category-level full configuration estimation of gar-ments from partial observations. Our experiments demon-strate that the trained model is able to generalize to novel garment instances as well as real world images. 2.