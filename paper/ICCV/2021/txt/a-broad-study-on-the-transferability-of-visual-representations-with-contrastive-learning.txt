Abstract
Tremendous progress has been made in visual repre-sentation learning, notably with the recent success of self-supervised contrastive learning methods. Supervised con-trastive learning has also been shown to outperform its cross-entropy counterparts by leveraging labels for choos-ing where to contrast. However, there has been little work to explore the transfer capability of contrastive learning to a different domain.
In this paper, we conduct a compre-hensive study on the transferability of learned representa-tions of different contrastive approaches for linear evalua-tion, full-network transfer, and few-shot recognition on 12 downstream datasets from different domains, and object de-tection tasks on MSCOCO and VOC0712. The results show that the contrastive approaches learn representations that are easily transferable to a different downstream task. We further observe that the joint objective of self-supervised contrastive loss with cross-entropy/supervised-contrastive loss leads to better transferability of these models over their supervised counterparts. Our analysis reveals that the rep-resentations learned from the contrastive approaches con-tain more low/mid-level semantics than cross-entropy mod-els, which enables them to quickly adapt to a new task. Our codes and models will be publicly available to facilitate fu-ture research on transferability of visual representations. 1 1.

Introduction
Self-supervised learning is an important research area whose goal is to learn superior data representations with-out any labelled supervision. Recently, self-supervised contrastive learning has shown promising results in image classiﬁcation tasks [21, 7, 4].
In the contrastive learning paradigm, a model is trained to recognize different augmen-tations of the same image (commonly referred as positives) while discriminating them from other random images (re-*This work was done while the author was an intern at IBM. 1https://github.com/asrafulashiq/transfer broad (a) Linear evaluation (b) Few-shot classiﬁcation
Figure 1: Average top-1 accuracy of different models on the downstream datasets. (a) Linear evaluation with a ﬁxed feature extractor and (b) 5-way 5-shot few-shot classiﬁcation.
In both cases, we observe that contrastive pretrained models achieve su-perior performance compared to cross-entropy pretrained mod-els. Adding a self-supervised contrastive loss (SelfSupCon) improves the performance for both supervised cross-entropy and supervised contrastive pretrained models. We argue that incorpo-rating a self-supervised contrastive loss (SelfSupCon) increases the variability within the same-class features and makes the mod-els learn both high-level semantics and low-level cues. ferred as negatives) in the dataset. The promising perfor-mance of self-supervised contrastive learning led to the idea of leveraging label information in the contrastive learning paradigm. To this end, Khosla et al. [30] proposed a su-pervised contrastive learning framework that achieves better
ImageNet accuracy than the standard cross-entropy model.
Representations learned from contrastive learning have been shown to perform better than supervised cross-entropy models in various downstream tasks, particularly the object detection task [21, 7, 30, 45, 54]. Despite recent progress, it is unclear why contrastive representations transfer bet-ter to other tasks, since most prior work focuses on in-domain evaluation, particularly ImageNet classiﬁcation ac-In this paper, our goal is to understand the un-curacy. derlying mechanism of the superior transferability of con-trastive learning. Towards this end, we conduct a com-prehensive study regarding transfer learning of contrastive approaches on downstream image classiﬁcation, few-shot evaluation, and object detection. We rigorously bench-mark ﬁve methods with different training objective losses: cross-entropy, self-supervised contrastive, supervised con-trastive, joint cross-entropy/self-supervised contrastive, and joint supervised/self-supervised contrastive.
We ﬁrst compare the transfer performance of different
ImageNet pretrained models on a collection of 12 down-stream datasets from various domains. We ﬁnd that con-trastive methods perform much better than the supervised cross-entropy models, particularly in ﬁxed feature transfer learning; however, the performance gap becomes smaller after full-network ﬁne-tuning. We observe similar trends on other downstream tasks, including few-shot recogni-tion and object detection and instance segmentation on the
VOC0712 [15] and MS COCO [36] datasets. In particular, our results indicate that the joint objective of self-supervised contrastive loss and supervised cross-entropy/contrastive loss consistently outperforms the standard trained coun-terparts in different downstream tasks. Figure 1 shows the average top-1 accuracy of the different ImageNet pre-trained methods we studied on the downstream datasets, for both ﬁxed-feature linear evaluation and few-shot classiﬁca-tion. Both the self-supervised contrastive model (denoted
SelfSupCon) and supervised contrastive model (denoted
SupCon) perform better than the cross-entropy model (de-noted CE). Moreover, the combination of cross-entropy and self-supervised contrastive (denoted CE+SelfSupCon) performs better than cross-entropy or self-supervised con-trastive alone. The same goes for the combination of self-supervised contrastive and supervised contrastive (denoted
SupCon+SelfSupCon).
We next investigate why contrastive approaches show su-perior transferability by analyzing the similarity between hidden representations, intra-class separation, and robust-ness to image corruption. We ﬁnd that contrastive ap-proaches learn more low-level and mid-level information that can be easily adapted to a different domain than the supervised cross-entropy models, which mostly learns high-level semantics in the penultimate layers. Zhao et al. [54] hypothesized that one of the limiting factors of supervised cross-entropy models is the objective of minimizing intra-class variation. Our analysis also suggests that a model should have sufﬁcient intra-class variation in the source do-main to better transfer the learned representations to a dif-ferent domain. Most standard supervised loss functions aim to increase inter-class distance and decrease intra-class vari-ation, which might be harmful for transferability of features.
We infer that contrastive approaches have larger within-class separation than the standard cross-entropy models, which could be one of the factors underlying their superior transferability. We also analyze the robustness and calibra-tion of different models, and ﬁnd that contrastive losses are more robust to different image corruptions and predict well-calibrated class probabilities that are more representative of true correctness likelihoods than cross-entropy models. Our key contributions in this work are as follows:
• We benchmark ﬁve methods including cross-entropy, self-supervised contrastive, supervised contrastive, and their combinations on downstream image classiﬁ-cation, object detection, and few-shot recognition. All results show a similar trend that contrastive learning extracts better features for transfer learning.
• We show that combining supervised loss with self-supervised contrastive loss improves transfer learning performance. Speciﬁcally, learned representation from the joint objective of self-supervised contrastive and supervised contrastive loss signiﬁcantly outperforms the model trained with cross-entropy by 5.63% un-der linear evaluation protocol and 3.46% in few-shot recognition (5-shot) on the 12 downstream datasets, 1.37% AP50 under object detection on VOC0712, and 0.8% on MS COCO. The improvement of the joint
⇠ objective over supervised contrastive model is small but consistent across all downstream tasks. The joint objective of cross-entropy and self-supervised con-trastive loss also consistently performs better than the models trained with the individual objectives.
• We apply Centered Kernel Alignment (CKA) [33] and show that contrastive models contain more low-level and mid-level information in the penultimate lay-ers than standard cross-entropy models. Furthermore, our analysis suggests that the contrastive models have higher intra-class variation than the standard cross-entropy models, even if the network is not explicitly trained to increase intra-class distance. 2.