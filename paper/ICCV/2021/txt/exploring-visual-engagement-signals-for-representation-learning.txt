Abstract
Visual engagement in social media platforms com-prises interactions with photo posts including comments, shares, and likes.
In this paper, we leverage such Visual
Engagement clues as supervisory signals for representa-tion learning. However, learning from engagement signals is non-trivial as it is not clear how to bridge the gap be-tween low-level visual information and high-level social in-teractions. We present VisE, a weakly supervised learning approach, which maps social images to pseudo labels de-rived by clustered engagement signals. We then study how models trained in this way beneﬁt subjective downstream computer vision tasks such as emotion recognition or polit-ical bias detection. Through extensive studies, we empiri-cally demonstrate the effectiveness of VisE across a diverse set of classiﬁcation tasks beyond the scope of conventional recognition 1. 1.

Introduction
People post photos on social media to invite engagement and seek connections. A photo of a cute dog can resonate with other dog lovers and trigger reactions such as the “like” or “love” button or comments including “what an adorable dog” and “look at those blue eyes!” The widely available in-teractions with the photos posted on social media, which we call visual engagement, contain rich semantic descriptions (“dog”, “blue eyes”) and are far less expensive to obtain than manual annotations in standard computer vision tasks, including coarse and ﬁne-grained class labels [11, 80, 95], bounding boxes [51, 23], and image captions [8].
More importantly, visual engagement, including com-ments, replies, likes, and shares, provides emotional and cultural context that goes beyond the image content therein.
For example, the image in Fig. 1 could be described in a standard captioning task as “a dog sits next to a stuffed an-imal.” The social media audience of this post may react to the cuteness of the dog, comment on the torn stuffed an-imal with whimsical responses, or initiate a conversation.
*Equal contribution. 1Project page: https://github.com/KMnP/vise
Object Classification:               Dog
Scene:                                        Indoor
Fine-grained Classiﬁcation:    Eskie Mix
Objects Detection:  dog stuffed animal
Image Captioning: 
A dog sits next to a stuffed animal.
Visual Engagement:
• Does he know how cute he looks?
• He looks so innocent
• Who me?
• !!!!❤❤
• Haha. They call them canine  teeth for a reason.
• Not destroyed, just well loved.
• LOL!
Figure 1. Visual engagement vs. other common supervisory sig-nals. Given the same image, visual engagement provide semanti-cally and contextually richer information than conventional recog-nition and captioning tasks.
The resulting textual descriptions depart from the exactly what it says on the tin approach of standard image caption-ing tasks and express private states [65, 82]: opinions, emo-tions, and speculations, for example. We argue that visual engagement can also serve as supervisory signals for rep-resentation learning and transfer well to subjective down-stream computer vision tasks like emotion recognition or political bias classiﬁcation.
Motivated by this observation, we propose to learn im-age representations from semantically and contextually rich
Visual Engagement signals (VisE). We hypothesize that such learned representations, as a byproduct of mapping image content to human reactions, are able to infer private states expressed by images. This is beneﬁcial and could serve as a nice addition to current computer vision research which in general focuses on the objectively present factual information from images (e.g., “this is a dog” vs. “what a cute dog!”).
Open-world visual engagement contains dense subjec-tivity clues, but is inherently noisy in nature. How to prop-erly leverage such signals for representation learning is a challenging, open question. Inspired by recent studies on feature learning from proxy tasks [19, 3, 84], we cluster each type of visual engagement and obtain cluster assign-ment indices for all the responses associated with a training image. These cluster assignments are used as supervisory
cues. We then train a network from scratch to map images to cluster assignments in a multi-task fashion for represen-tation learning, where each task is to predict the cluster in-dex for that type of response.
In this paper, we consider two forms of human responses: (1) comments and (2) re-actions. In the former case, we conduct the clustering on representations encoded by a textual model. Unlike most existing multi-modal methods that perform pre-training for both language and vision modules [12, 66] with hundreds of millions of parameters, we simply use an off-the-shelf encoder to embed comments, which is computationally ef-ﬁcient. We then evaluate representations learned from en-gagement signals on downstream tasks.
Our main contribution is to demonstrate that social me-dia engagement can provide supervision for learning image representations that beneﬁt subjective downstream tasks.
To this end, we explore VisE pre-trained on 250 million publicly available social media posts. Through extensive experiments, we show that in three downstream tasks re-lated to private states detection, the learned VisE mod-els can outperform the ImageNet-supervised counterpart by a substantial margin in some cases. These results high-light that VisE broadens the current representation learning paradigms, thereby narrowing the gap between machine and human intelligence. 2.