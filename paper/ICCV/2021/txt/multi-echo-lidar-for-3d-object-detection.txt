Abstract
LiDAR sensors can be used to obtain a wide range of measurement signals other than a simple 3D point cloud, and those signals can be leveraged to improve perception tasks like 3D object detection. A single laser pulse can be partially reﬂected by multiple objects along its path, result-ing in multiple measurements called echoes. Multi-echo measurement can provide information about object con-tours and semi-transparent surfaces which can be used to better identify and locate objects. LiDAR can also measure surface reﬂectance (intensity of laser pulse return), as well as ambient light of the scene (sunlight reﬂected by objects).
These signals are already available in commercial LiDAR devices but have not been used in most LiDAR-based de-tection models. We present a 3D object detection model which leverages the full spectrum of measurement signals provided by LiDAR. First, we propose a multi-signal fusion (MSF) module to combine (1) the reﬂectance and ambient features extracted with a 2D CNN, and (2) point cloud fea-tures extracted using a 3D graph neural network (GNN).
Second, we propose a multi-echo aggregation (MEA) mod-ule to combine the information encoded in different sets of echo points. Compared with traditional single echo point cloud methods, our proposed Multi-Signal LiDAR Detector (MSLiD) extracts richer context information from a wider range of sensing measurements and achieves more accurate 3D object detection. Experiments show that by incorporat-ing the multi-modality of LiDAR, our method outperforms the state-of-the-art by up to relatively 9.1%. 1.

Introduction
LiDAR is a powerful sensor that has the ability to cap-ture a wide range of measurements for perception tasks in-cluding object detection. The most commonly used LiDAR measurement type is a set of 3D points (a point cloud) and their reﬂectance values, which provides accurate 3D shape information of objects in the scene. State-of-the-art ob-ject detection methods have made great breakthroughs by leveraging 3D point cloud data. However, despite such suc-cess, there are several types of LiDAR measurements that
Figure 1. Illustration of multi-signal measurements from Li-DAR sensor. Each photon detector on the sensor collects a group of signals and forms an ”Echo Group”, which is converted into a 2D LiDAR image and a multi-echo point cloud representation. are largely ignored in modern-day LiDAR perception algo-rithms. In the following, we describe three unique features of the LiDAR sensor, which are available in standard Li-DAR sensors, but surprisingly have rarely been used in pub-lished LiDAR-based object detection algorithms. We show that by leveraging these features, one can greatly improve 3D object detection performance.
The ﬁrst important feature of LiDAR is its ability to ob-tain multiple return signals with a single laser pulse, called echoes. LiDAR is a time-of-ﬂight measurement system which measures the time it takes for a laser pulse to hit an object and return to the sensor. More speciﬁcally, the laser emits a short pulse, and a photodetector timestamps the arrival of photons reﬂected back by object surfaces. It is possible for a photodetector to acquire multiple return sig-nals (echoes) if the laser is partially reﬂected by multiple objects along its path of propagation. We call the multi-ple returned signals generated from the same laser beam an ‘echo group.’ Points in the same echo group lie on one line in 3D space, and they are typically ordered according to their signal strength. In addition to the direct beneﬁt of increasing the number of points available, multiple echoes also imply that high-order echo points are likely on the con-tour of an object (objects obstruct only a part of the laser) or on a semi-transparent surface (a portion of the laser propa-gates through the surface). In either case, we hypothesize that echoes encode meaningful features that can help locate
or classify an object.
Overall, our contributions can be summarized as follows:
The second important feature of LiDAR is the ability to capture ambient scene illumination. The photodetector of the LiDAR continuously captures infrared (IR) light and therefore is capturing IR images of the scene (typically re-ﬂected sunlight) between laser pulses. Although this infor-mation is typically ignored in most LiDAR-based percep-tion algorithms, a LiDAR can be used to capture an image of the scene using the IR spectrum. Ambient measurements can be processed as a 2D image and can be used to extract texture information about the objects in the scene.
The third important feature of LiDAR is the ability to capture surface reﬂectance. LiDAR captures laser signal re-turns, so each point will have a corresponding reﬂectance value which measures the strength of the detected laser pulse. Reﬂectance also encodes material properties of ob-jects useful for detection and classiﬁcation. Unlike the am-bient signal, different points inside the same echo group will have different reﬂectance values, resulting in multiple re-ﬂectance values which we call multi-echo reﬂectance.
We propose a multi-signal LiDAR-based 3D object de-tector (MSLiD). First, to better leverage the dense tex-ture and surface properties encoded in the ambient and re-ﬂectance signals, we re-organize them as a dense 2D repre-sentation, called the ‘LiDAR Image.’ Then, in order to com-bine the dense 2D image with the sparse 3D point cloud, we propose a multi-signal fusion (MSF) module which in-corporates a 2D CNN branch and a 3D GNN branch. The
MSF module aims at fusing 2D visual information with 3D positional information by sending pixels and class features from the 2D branch to point-wise features learned in the 3D branch. Furthermore, In order to extract and combine in-formation encoded in different echo groups, we propose a multi-echo aggregation (MEA) module. To resolve the im-balance between the number of points in different echos,
The MEA module reassigns multi-echo points into two sets – ‘penetrable’ and ‘impenetrable’ sets according to whether the object reﬂects partial laser signal. Aggregating the fea-tures learned from two new sets of points provides richer context information of objects and leads to better location estimation. By cascading the MSF and MEA modules, the proposed system combines dense visual information from ambient/reﬂectance and sparse geometric information from the point cloud, while also extract richer context features by aggregating multiple echoes. By leveraging multi-signal Li-DAR measurements other than a single point cloud, MSLiD learns a more discriminative object representation which leads to accurate object localization and classiﬁcation.
We collect one real-world and one synthetic dataset with multiple LiDAR measurements, including ambient signal, multi-echo point cloud, and reﬂectance signals. Experi-ments on two datasets demonstrate that our method outper-forms state-of-the-art single-echo methods by up to 9.1%. 1. MSLiD is the ﬁrst to propose a 3D detection frame-work that properly leverages ambient illumination, multiple echos of point clouds and reﬂectance signals for LiDAR sensor. Our method shows improvement over prior methods using single-echo point cloud with reﬂectance intensity. 2. We propose a multi-signal fusion module to effectively combine dense visual information from ambient and reﬂectance signals with sparse 3D positional informa-tion from point cloud. 3. We propose a multi-echo aggregation module to form a richer context representation of objects from multi-ple groups of echoes, resulting in more accurate object localization and classiﬁcation. 2.