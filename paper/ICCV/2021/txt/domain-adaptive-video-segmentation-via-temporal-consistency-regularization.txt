Abstract
Video semantic segmentation is an essential task for the analysis and understanding of videos. Recent efforts largely focus on supervised video segmentation by learning from fully annotated data, but the learnt models often experi-ence clear performance drop while applied to videos of a different domain. This paper presents DA-VSN, a do-main adaptive video segmentation network that addresses domain gaps in videos by temporal consistency regulariza-tion (TCR) for consecutive frames of target-domain videos.
DA-VSN consists of two novel and complementary designs.
The ﬁrst is cross-domain TCR that guides the prediction of target frames to have similar temporal consistency as that of source frames (learnt from annotated source data) via adversarial learning. The second is intra-domain TCR that guides unconﬁdent predictions of target frames to have sim-ilar temporal consistency as conﬁdent predictions of target frames. Extensive experiments demonstrate the superiority of our proposed domain adaptive video segmentation net-work which outperforms multiple baselines consistently by large margins. e c r u o
S t e g r a
T
) s r u
O ( t e g r a
T
Previous Frame
Current Frame
Figure 1. Temporal consistency helps in domain adaptive video segmentation: A video segmentation model trained in a Source domain often experiences clear performance drop while applied to videos of a Target domain. We employ temporal consistency, the inherent and universal nature of videos, as a constraint to regu-larize inter-domain and intra-domain adaptation for optimal video segmentation in target domain as in Target (ours). 1.

Introduction
Video semantic segmentation aims to assign pixel-wise semantic labels to video frames, and it has been attract-ing increasing attention as one essential task in video anal-ysis and understanding [19, 53, 15, 45, 60]. With the advance of deep neural networks (DNNs), several stud-ies have been conducted in recent years with very impres-sive video segmentation performance [65, 40, 20, 33, 37, 38, 26, 47]. However, most existing works require large amounts of densely annotated training videos which entail a prohibitively expensive and time-consuming annotation process [3, 14]. One approach to alleviate the data an-notation constraint is to resort to self-annotated synthetic videos that are collected with computer-generated virtual scenes [62, 24], but models trained with the synthesized
*Corresponding author. data often experience clear performance drops while ap-plied to videos of natural scenes largely due to the domain shift as illustrated in Fig. 1.
Domain adaptive video segmentation is largely neglected in the literature despite its great values in both research and practical applications. It could be addressed from two approaches by leveraging existing research. The ﬁrst ap-proach is domain adaptive image segmentation [80, 69, 79, 58, 21, 74] which could treat each video frame inde-pendently to achieve domain adaptive video segmentation.
However, domain adaptive image segmentation does not consider temporal information in videos which is very im-portant in video semantic segmentation. The second ap-proach is semi-supervised video segmentation [56, 78, 5] that exploits sparsely annotated video frames for segment-ing unannotated frames of the same video. However, semi-supervised video segmentation was designed for consecu- 
tive video frames of the same domain and does not work well in domain adaptive video segmentation which usu-ally involves clear domain shifts and un-consecutive video frames of different sources.
In this work, we design a domain adaptive video segmen-tation network (DA-VSN) that introduces temporal con-sistency regularization (TCR) to bridge the gaps between videos of different domains. The design is based on the ob-servation that video segmentation model trained in a source domain tends to produce temporally consistent predictions over source-domain data but temporally inconsistent pre-dictions over target-domain data (due to domain shifts) as illustrated in Fig. 1. We designed two complementary regu-larization modules in DA-VSN, namely, cross-domain TCR (C-TCR) and intra-domain TCR (I-TCR). C-TCR employs adversarial learning to minimize the discrepancy of tempo-ral consistency between source and target domains. Specif-ically, it guides target-domain predictions to have similar temporal consistency of source-domain predictions which usually has decent quality by learning from fully-annotated source-domain data.
I-TCR instead works from a differ-ent perspective by guiding unconﬁdent target-domain pre-dictions to have similar temporal consistency as conﬁdent target-domain predictions. In I-TCR, we leverage entropy to measure the prediction conﬁdence which works effec-tively across multiple datasets.
The contributions of this work can be summarized in three major aspects. First, we proposed a new framework that introduces temporal consistency regularization (TCR) to address domain shifts in domain adaptive video segmen-tation. To the best of our knowledge, this is the ﬁrst work that tackles the challenge of unsupervised domain adapta-tion in video semantic segmentation. Second, we designed inter-domain TCR and intra-domain TCR that improve do-main adaptive video segmentation greatly by minimizing the discrepancy of temporal consistency across different do-mains and different video frames in target domain, respec-tively. Third, extensive experiments over two challenging synthetic-to-real benchmarks (VIPER [62] → Cityscapes-Seq [14] and SYNTHIA-Seq [63] → Cityscapes-Seq) show that the proposed DA-VSN achieves superior domain adap-tive video segmentation as compared with multiple base-lines. 2.