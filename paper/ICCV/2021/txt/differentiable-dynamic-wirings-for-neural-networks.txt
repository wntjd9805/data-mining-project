Abstract
A standard practice of deploying deep neural networks is to apply the same architecture to all the input instances.
However, a fixed architecture may not be suitable for dif-ferent data with high diversity. To boost the model capac-ity, existing methods usually employ larger convolutional kernels or deeper network layers, which incurs prohibitive computational costs. In this paper, we address this issue by proposing Differentiable Dynamic Wirings (DDW), which learns the instance-aware connectivity that creates differ-ent wiring patterns for different instances. 1) Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. 2) We generate edge weights by a learnable module, Router, and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. 3) Instead of using the same path of the network, DDW aggregates fea-tures dynamically in each node, which allows the network to have more representation power.
To facilitate effective training, we further represent the network connectivity of each sample as an adjacency ma-trix. The matrix is updated to aggregate features in the for-ward pass, cached in the memory, and used for gradient computing in the backward pass. We validate the effective-ness of our approach with several mainstream architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Ex-tensive experiments are performed on ImageNet classifica-tion and COCO object detection, which demonstrates the effectiveness and generalization ability of our approach. 1.

Introduction
Deep neural networks have driven a shift from feature engineering to feature learning. The great progress largely comes from well-designed networks with increasing capac-ity of models [10, 41, 13, 34]. To achieve the superior performance, a useful practice is to add more layers [33] or expand the size of existing convolutions (kernel width, number of channels) [14, 34, 21]. Meantime, the computa-tional cost significantly increases, hindering the deployment of these models in realistic scenarios.
Instead of adding much more computational burden, we prefer adding input-dependent modules to networks, increasing the model ca-pacity by accommodating the data variance.
Several existing work attempt to augment the input-dependent modules into network. For example, Squeeze-and-Excitation network (SENet) [12] learns to scale the ac-tivations in the channel dimension conditionally on the in-put. Conditionally Parameterized Convolution (CondConv)
[43] uses over-parameterization weights and generates in-dividual convolutional kernels for each sample. GaterNet
[4] adopts a gate network to extract features and generate sparse binary masks for selecting filters in the backbone net-work based upon inputs. All these methods focus on the adjustment of the micro structure of neural networks, using a data-dependent module to influence the feature represen-tation at the same level. Recall the deep neural network to mammalian brain mechanism in biology [26], the neurons are linked by synapses and responsible for sensing different information, the synapses are activated to varying degrees when the neurons perceive external information. Such a phenomenon inspires us to design a network where different samples activate different network paths.
In this paper, we learn to optimize the connectivity of neural networks based upon inputs.
Instead of using stacked-style or hand-designed manners, we allow a more flexible selection for wiring patterns. Specifically, we re-formulate the network into a directed acyclic graph, where nodes represent the convolution block while edges indi-cate connections. Different from randomly wired neural networks [42] that generate random graphs as connectiv-ity using predefined generators, we rewire the graph as a complete graph so that all nodes establish connections with each other. Such a setting allows more possible connections and makes the task of finding the most suitable connectiv-ity for each sample equivalent to finding the optimal sub-graph in the complete graph. In the graph, each node aggre-gates features from the preceding nodes, performs feature
transformation (e.g. convolution, normalization, and non-linear operations), and distributes the transformed features to the succeeding nodes. The output of the last node in the topological order is employed as the representation through the graph. To adjust the contribution of different nodes to the feature representation, we further assign weights to the edges in the graph. The weights are generated dynamically for each input via an extra module (denoted as router) along with each node. During the inference, only crucial connec-tions are maintained, which creates different paths for dif-ferent instances. As the connectivity for each sample is gen-erated through non-linear functions determined by routers, our method can enable the networks to have more represen-tation power than the static network.
We dub our proposed framework as the Differentiable
Dynamic Wirings (DDW). It doesn’t increase the depth or width of the network, while only introduces an extra neg-ligible cost to compute the edge weights and aggregate the features. To facilitate the training, we represent the network connection of each sample as an adjacent matrix and design a buffer mechanism to cache the matrices of a sample batch during training. With the buffer mechanism, we can conve-niently aggregate the feature maps in the forward pass and compute the gradient in the backward pass by looking up the adjacent matrices. In summary, Differentiable Dynamic
Wirings (DDW) has three appealing properties:
• We investigate and introduce the dynamic wirings based upon inputs to exploit the model capacity of neural networks. Without bells and whistles, sim-ply replacing static connectivity with dynamic one in many networks achieves solid improvements with only a slight increase of (∼ 1%) parameters and (∼ 2%) computational cost (see Table 1).
• DDW is easy and memory-efficient to train. The pa-rameters of networks and routers can be optimized in a differentiable manner. We also design a buffer mech-anism to conveniently access the network connectiv-ity, aggregate the feature maps in the forward pass and compute the gradient in the backward pass.
• We show that DDW not only improves the perfor-mance for human-designed networks (e.g. Mobiel-NetV2, ResNet, ResNeXt) but also boosts the perfor-mance for automatically searched architectures (e.g.
RegNet). It demonstrates good generalization ability on ImageNet classification (see Table 1) and COCO object detection (see Table 2) tasks. 2.