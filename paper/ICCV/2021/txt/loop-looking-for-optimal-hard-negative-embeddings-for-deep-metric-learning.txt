Abstract
Deep metric learning has been effectively used to learn distance metrics for different visual tasks like image re-In order to aid the training pro-trieval, clustering, etc. cess, existing methods either use a hard mining strategy to extract the most informative samples or seek to gener-ate hard synthetics using an additional network. Such ap-proaches face different challenges and can lead to biased embeddings in the former case, and (i) harder optimization (ii) slower training speed (iii) higher model complexity in the latter case. In order to overcome these challenges, we propose a novel approach that looks for optimal hard nega-tives (LoOp) in the embedding space, taking full advantage of each tuple by calculating the minimum distance between a pair of positives and a pair of negatives. Unlike mining-based methods, our approach considers the entire space be-tween pairs of embeddings to calculate the optimal hard negatives. Extensive experiments combining our approach and representative metric learning losses reveal a signiﬁ-cant boost in performance on three benchmark datasets1. 1.

Introduction
Deep metric learning tries to learn an embedding space, where closeness between embeddings encodes the level of semantic similarities between the data samples. This is done by leveraging deep neural networks to learn the map-ping between the data samples and the embedding space and enforcing the embeddings belonging to the same class to lie close while pushing those belonging to different classes further apart. Deep metric learning-based methods have achieved state-of-the-art (SOTA) results for several tasks, like face recognition [2, 24, 20, 8], re-identiﬁcation
[35, 19, 41, 30], image retrieval [31, 7], etc.
To train the deep networks, several loss functions with desirable properties have been formulated, which enable the
*Equal contribution 1Code available at https://github.com/puneesh00/LoOp
Figure 1: Illustration of the proposed approach. Given two pairs of points in the embedding space, x1, x2 from class A, and y1, y2 from class B, our method ﬁnds the optimal hard negatives by calculating the minimum distance between the curves joining these points. neural network to learn the mapping from the data space to the embedding space. Conventional methods, such as con-trastive loss [2, 9] which takes tuples with two samples and triplet loss [24, 33] which takes three sample tuples, con-sider the similarity between few samples. In contrast, meth-ods like lifted structure loss [25] aim to exploit all the sam-ples present in a batch to learn more informative represen-tations. Other methods [28, 21] that utilize several samples in the loss have also been proposed.
However, even when considering the whole batch, not all the samples are able to contribute to the loss term. This is because many of them already satisfy the constraints present in the loss. As a result, these samples are not suf-ﬁciently informative and lead to low gradient values. To overcome this problem, the idea of using hard samples has
been considered, i.e. positive samples which lie far away and negative samples which lie closer. Several works on hard negative mining [21, 11, 13, 37, 24] have been pro-posed. Mining-based strategies generally look for the most informative samples in the dataset and are prone to learning biased mappings, which do not generalize well to the entire dataset. On the other hand, hard negative generation-based methods [5, 40, 39] utilize an additional sub-network, ei-ther an autoencoder or a generative adversarial network [6] as the generator. This can result in harder optimization [1] and an increase in training time and computation.
To address the limitations of existing hard negative min-ing and generation methods, we propose a novel approach, which looks for optimal hard negative samples (LoOp) in the embedding space. This is done by ﬁnding points that minimize the distance between the curves joining a pair of positives and a pair of negatives, as shown in Fig 1. The curve joining a pair of points belonging to the same class lies in the region belonging to that class. Hence, ﬁnding the minimum distance with another curve, which is rep-resentative of some other class, allows us to consider the most informative pair of samples for computing the loss, as described in Section 3.1. Unlike mining-based meth-ods, our approach does not neglect any samples, and unlike generation-based methods, it does not increase the training complexity or optimization difﬁculty. It can be easily in-tegrated with various metric learning-based losses. We in-clude several experimental results in Section 4 to demon-strate the efﬁcacy of our approach.
Contributions We propose a novel approach, LoOp, which ﬁnds optimal hard negatives in the embedding space, and maximizes the contribution of each tuple in computing the pair-based metric learning loss. Our approach presents the solution to a general problem, namely ﬁnding the min-imum distance between two bounded curves, which can be useful in other applications as well. Generating hard nega-tives in the embedding space, our approach utilizes all train-ing samples and does not rely on a subset of samples like
It also avoids the computational mining-based methods. load and training complexity introduced by generation-based methods. We also explore the optimality of LoOp with a gradient-based theoretical analysis of the loss func-tion. Our approach generalizes well to various metric learn-ing losses, and it can be easily combined without any in-crease in optimization difﬁculty or additional parameters.
It outperforms state-of-the-art mining and generation-based methods on three benchmark datasets, Cars196 [17], CUB-200-2011 [29], and Stanford Online Products [25]. 2.