Abstract
The inverted index is one of the most commonly used structures for non-exhaustive nearest neighbor search on large-scale datasets. It allows a signiﬁcant factor of accel-eration by a reduced number of distance computations with only a small fraction of the database. In particular, the in-verted index enables the product quantization (PQ) to learn their codewords in the residual vector space. The quantiza-tion error of the PQ can be substantially improved in such combination since the residual vector space is much more quantization-friendly thanks to their compact distribution compared to the original data. In this paper, we ﬁrst raise an unremarked but crucial question; why the inverted in-dex and the product quantizer are optimized separately even though they are closely related? For instance, changes on the inverted index distort the whole residual vector space.
To address the raised question, we suggest a joint opti-mization of the coarse and ﬁne quantizers by substituting the original objective of the coarse quantizer to end-to-end quantization distortion. Moreover, our method is generic and applicable to different combinations of coarse and ﬁne quantizers such as inverted multi-index and optimized PQ.
Figure 1: This ﬁgure describes difference between coarse center updates of conventional inverted index and those of our proposed method. While the conventional inverted in-dex considers the distortion of the coarse centers only, the proposed method also considers the quantization error of the ﬁne quantizer in the coarse center updates. 1.

Introduction
For decades, approximate nearest neighbor search has been a vital problem in various ﬁelds including computer vision. It is especially difﬁcult with high-dimensional and large-scale datasets because of its impractical requirements for computational costs and memory overhead. To address such difﬁculty, efﬁcient indexing and compact data repre-sentation techniques have been highlighted.
The Product Quantization (PQ) [16] and its varia-tions [11, 19, 3, 29, 15, 8, 23, 21] has been recognized as the most popular and successful solution, since they pro-vide signiﬁcant factors of data compression rate and efﬁ-ciency in distance estimation. Speciﬁcally, the PQ divides the high-dimensional vector into M disjoint sub-vectors and
*Corresponding author quantizes those sub-vectors into K sub-codewords indepen-In this way, the PQ utilizes M K representations dently. with a small memory footprint. Moreover, it is empirically shown that the PQ-based techniques provide superior search quality than parallel research directed toward the same goal, the binary hashing methods [2, 27, 14, 28, 13, 9]
The inverted index provides a non-exhaustive search by a simple shortlisting mechanism based on the data cluster-ing. A practical scalable search system is implemented by the following procedure; given a query, the inverted index collects the shortlist which is a small fraction of the whole data, and the PQ re-ranks the candidates according to the estimated distances with the compact codes. Behind this visible synergy effect between them towards high scalabil-ity in terms of both speed and memory, the inverted index transforms the data into quantization-friendly residual vec-tors suitable for the PQ. Since the residual vector has a much more compact distribution compared to its original data, the
PQ beneﬁts from it to further reduce the quantization distor-tion.
In this paper, we raise a new question about the afore-mentioned tight relationship between the coarse (inverted index) and ﬁne (product) quantizers; why they are learned separately? For instance, a change in the coarse quantizer signiﬁcantly affects the distribution of residual vectors. As a result, we suggest that coarse and ﬁne quantizers should be jointly and collaboratively optimized. To this end, we pro-pose a joint optimization of the coarse and ﬁne quantizers by substituting the objective of the coarse quantizer to end-to-end quantization distortion. Finally, the designed scheme is orthogonal to the conventional inverted index techniques without any additional time and memory overhead at encod-ing, decoding, and query time. The experimental validation with various indexing and encoding techniques on several benchmarks are available in Sec. 5.
Our main contributions are summarized as follows:
• We highlight the necessity of joint optimization of the coarse and ﬁne quantizers.
• We propose a joint optimization of coarse and ﬁne quantizers by replacing the objective of coarse quan-tizer into the distortion of the ﬁne quantizer.
• Our proposed method is orthogonal to the conventional inverted index techniques without any additional time overhead at encoding, decoding, and query time.
• Experimental results show that our method achieves state-of-the-art performances over the ﬁve large-scale
ANN datasets. 2.