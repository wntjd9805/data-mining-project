Abstract
Measuring concept generalization, i.e., the extent to which models trained on a set of (seen) visual concepts can be leveraged to recognize a new set of (unseen) concepts, is a popular way of evaluating visual representations, especially in a self-supervised learning framework. Nonetheless, the choice of unseen concepts for such an evaluation is usually made arbitrarily, and independently from the seen concepts used to train representations, thus ignoring any semantic re-lationships between the two. In this paper, we argue that the semantic relationships between seen and unseen concepts affect generalization performance and propose ImageNet-CoG,1 a novel benchmark on the ImageNet-21K (IN-21K) dataset that enables measuring concept generalization in a principled way. Our benchmark leverages expert knowl-edge that comes from WordNet in order to define a sequence of unseen IN-21K concept sets that are semantically more and more distant from the ImageNet-1K (IN-1K) subset, a ubiquitous training set. This allows us to benchmark visual representations learned on IN-1K out-of-the box. We con-duct a large-scale study encompassing 31 convolution and transformer-based models and show how different architec-tures, levels of supervision, regularization techniques and use of web data impact the concept generalization perfor-mance.
*UGA, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France 1https://europe.naverlabs.com/cog-benchmark
1.

Introduction
There has been an increasing effort to tackle the need for manually-annotated large-scale data in deep models via transfer learning, i.e., by transferring representations learned on resourceful datasets and tasks to problems where anno-tations are scarce. Prior work has achieved this in various ways, such as, imitating knowledge transfer in low-data regimes [60], exploiting unlabeled data in a self- [22] or weakly- [37] supervised manner.
The quality of the learned visual representations for trans-fer learning is usually determined by checking whether they are useful for, i.e., generalize to, a wide range of downstream vision tasks. Thus, it is imperative to quantify this gener-alization, which has several facets, such as generalization to different input distributions (e.g., from synthetic images to natural ones), to new tasks (e.g., from image classifica-tion to object detection), or to different semantic concepts (e.g., across different object categories or scene labels). Al-though the first two facets have received much attention recently [18, 20], we observe that a more principled analysis is needed for the last one.
As also noted by [12, 67], the effectiveness of knowledge transfer between two tasks is closely related to the seman-tic similarity between the concepts considered in each task.
However, assessing this relatedness is not straightforward, as the semantic extent of a concept may depend on the task itself. In practice, models consider an exhaustive list of downstream tasks that cover a wide range of concepts [7, 29] in order to test their transfer learning capabilities. Previ-ous attempts discussing this issue have been limited to in-tuition [67, 75]. We still know little about the impact of the semantic relationship between the concepts seen during training visual representations and those seen during their evaluation (seen and unseen concepts, respectively).
In this paper, we study the generalization capabilities of visual representations across concepts that exist in a large, popular, and broad ontology, the subset of WordNet [41] used to build ImageNet-21K [11] (IN-21K), while keeping all the other generalization facets fixed. Starting from a set of seen concepts, the concepts from the popular ImageNet-1K [47] (IN-1K) dataset, we leverage semantic similarity metrics based on this ontology crafted by experts to measure the semantic distance between IN-1K and every unseen con-cept (i.e., any concept from IN-21K that is not in IN-1K).
We rank unseen concepts with respect to their distance to
IN-1K and define a sequence of five, IN-1K-sized concept generalization levels, each consisting of a distinct set of un-seen concepts with increasing semantic distance to the seen ones. This results in a large-scale benchmark that consists of five thousand concepts, that we refer to as the ImageNet
Concept Generalization benchmark, or ImageNet-CoG in short. The benchmark construction process is illustrated in
Fig. 1.
Given a model trained on IN-1K, the evaluation protocol for ImageNet-CoG consists of two phases: it first extracts features for images of IN-1K and of the five concept gen-eralization levels, and then learns individual classifiers, for each level, using a varying amount of samples per concept.
By defining the set of seen concepts for our benchmark to be IN-1K classes, we are able to evaluate models trained on
IN-1K out-of-the box. We therefore use publicly available pretrained models and analyse a large number of popular models under the prism of concept generalization. Our con-tributions are as follows.
• We propose a systematic way to study concept general-ization, by defining a set of seen concepts along with sets of unseen concepts that are semantically more and more distant from the seen ones.
• We design ImageNet-CoG, a large-scale benchmark, which embodies this systematic way. It is designed to evaluate models pretrained on IN-1K out-of-the-box and draws unseen concepts from the rest of the IN-21K dataset.
We measure concept generalization performance on five,
IN-1K-sized levels, by learning classifiers with a few or all the training images from the unseen concepts.
• We conduct a large-scale study benchmarking 31 state-of-the-art visual representation learning approaches on
ImageNet-CoG and analyse how different architectures, levels of supervision, regularization techniques and addi-tional web data impact the concept generalization perfor-mance, uncovering several interesting insights. 2.