Abstract
Existing differentiable neural architecture search ap-proaches simply assume the architectural distribution on each edge is independent of each other, which conflicts with the intrinsic properties of architecture.
In this paper, we view the architectural distribution as the latent representa-tion of specific data points. Then we propose Variational In-formation Maximization Neural Architecture Search (VIM-NAS) to leverage a simple yet effective convolutional neu-ral network to model the latent representation, and opti-mize for a tractable variational lower bound to the mutual information between the data points and the latent repre-sentations. VIM-NAS automatically learns a nearly one-hot distribution from a continuous distribution with extremely fast convergence speed, e.g., converging with one epoch.
Experimental results demonstrate VIM-NAS achieves state-of-the-art performance on various search spaces, including
DARTS search space, NAS-Bench-1shot1, NAS-Bench-201, and simplified search spaces S1-S4. Specifically, VIM-NAS achieves a top-1 error rate of 2.45% and 15.80% within 10 minutes on CIFAR-10 and CIFAR-100, respectively, and a top-1 error rate of 24.0% when transferred to ImageNet. 1.

Introduction
With the development of deep learning, various neu-ral architectures are needed for specific tasks. Given a specific dataset, neural architecture search (NAS) frees re-searchers from cumbersome architecture design by explor-ing the search space automatically to search for the opti-mal network architecture. Since different datasets have their own preference for the architecture, for instance, biomedi-cal images favor U-Net alike architectures with fully convo-lutional operations and symmetrical structure. NAS can be deemed as searching for the preference of a given dataset.
*Corresponding author: Wenrui Dai. †Equal contribution.
Thus, the architectural parameters can be viewed as the la-tent representation of specific data points.
In the sense of methodology, NAS can be divided into three parts, i.e., search space, search strategy, and perfor-mance estimation strategy [17, 38]. One-shot approaches
[3, 4, 34] have been developed as a promising alternative to reduce the search time by finding the best sub-network in a super network through parameter sharing. Gradient-based approaches [51, 29, 37, 41], aka Differentiable NAS, further treat the super network as the whole search space, intro-duce architectural parameters, and obtain the sub-network by optimizing the architectural parameters and super net-work weights in a differentiable manner.
Despite the remarkable performance, existing differen-tiable NAS approaches make improper assumptions that the predefined architectural distribution on each edge is inde-pendent of each other, conflicting with intrinsic properties of architecture. Specifically, SNAS [41] and FBNet [39] utilize the Concrete distribution [22, 30] to approximate the discrete categorical distribution on architectural parameters.
DATA [51] performs multiple sampling with replacement from the same Concrete distribution to expand the search space. SI-VDNAS [37] introduces Gaussian noise for vari-ational dropout and samples the super network through the learned dropout rate.
Isotropic Gaussian noise in SI-VDNAS as well as the factorizable distribution in SNAS,
FBNet and DATA ignores the dependencies between archi-tectural parameters.
In addition to the above improper assumptions, the search cost for differentiable NAS approaches is also pro-hibitive due to the numerous search spaces and tasks. Two-stage approaches [47, 36] decouple the model training and searching process for amortizing the training cost, but are still restricted by numerous search spaces. Therefore, an efficient search strategy with fast convergence speed is de-sirable for differentiable NAS.
In this paper, we propose a novel search strategy, namely
Variational Information Maximization Neural Architecture
Search (VIM-NAS), to maximize the mutual information
between the data points and the latent architectural rep-resentations. VIM-NAS leverages a simple yet effective convolutional neural network to model the latent architec-tural representation, and optimizes for a tractable varia-tional lower bound to the mutual information. Our contri-butions are summarized below.
• We propose a novel perspective that architectural dis-tribution can be deemed as the latent representation of a given dataset in NAS.
• We leverage a simple yet effective architectural neural network to model the dependencies among architec-tural distribution.
• We propose a novel search strategy to maximize the variational lower bound to the mutual information be-tween the data points and the latent architectural rep-resentations.
Experimentally, VIM-NAS exhibits currently the fastest convergence speed within one epoch and learns a nearly one-hot distribution from a continuous distribution. Specif-ically, VIM-NAS achieves the state-of-the-art performance on DARTS search space with a top-1 error rate of 2.45% and 15.80% within 10 minutes on CIFAR-10/100 and a top-1 error rate of 24.0% when transferred to ImageNet. VIM-NAS also achieves the state-of-the-art performance on other search spaces, including NAS-Bench-1shot1, NAS-Bench-201 and simplified search spaces S1-S4. 2.