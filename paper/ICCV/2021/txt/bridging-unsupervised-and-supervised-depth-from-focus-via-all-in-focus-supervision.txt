Abstract
Depth estimation is a long-lasting yet important task in computer vision. Most of the previous works try to estimate depth from input images and assume images are all-in-focus (AiF), which is less common in real-world applications. On the other hand, a few works take defocus blur into account and consider it as another cue for depth estimation. In this paper, we propose a method to estimate not only a depth map but an AiF image from a set of images with different focus positions (known as a focal stack). We design a shared architecture to exploit the relationship between depth and
AiF estimation. As a result, the proposed method can be trained either supervisedly with ground truth depth, or un-supervisedly with AiF images as supervisory signals. We show in various experiments that our method outperforms the state-of-the-art methods both quantitatively and qual-itatively, and also has higher efficiency in inference time. 1.

Introduction
Depth estimation has been one of the most fundamental computer vision problems in decades. Many downstream tasks, such as augmented reality (AR), virtual reality (VR) and autonomous driving, highly rely on this research topic.
Recently, it also enabled an increasing number of applica-tions for smartphone photography, such as depth-of-field adjustment, background substitution, and changing focus after the picture is taken.
Consequently, depth sensing has become a fundamental component for capturing devices. Active depth sensing solu-tions such as Time-of-Flight (ToF) and structured light are often expensive and power-consuming due to the need for specialized hardware. Passive techniques, such as binocular or multi-view stereo, are more cost and power-efficient but prone to errors in textureless regions.
Deep learning based stereo matching methods tackle this problem in a data-driven way by learning depth estimation directly from input images. However, they require a large amount of high-quality paired training data, which are time-consuming and expensive to acquire. They also suffer when the training data are imperfect: synthesized and unrealistic input images, or inaccurately registered depth maps.
Some unsupervised learning approaches [9, 42] were pro-posed to address this problem. They usually use image reconstruction loss and consistency loss without the need for ground truth depth data. They can also mitigate domain gaps by training directly with real-world stereo images without corresponding registered depth maps.
Another relatively under-explored cue for depth estima-tion is defocus blur. The task of depth-from-focus (or de-focus) aims to estimate the depth of a scene from a focal stack, i.e., images taken at different focal positions by the same camera. This allows consumer auto-focus monocular cameras to estimate depth without additional hardware.
Conventional optimization based depth-from-focus ap-proaches [37, 36, 26] estimate the level of sharpness for each pixel and often suffer from textureless objects or aper-ture problems. Deep learning techniques [11, 23] help to overcome these issues but need ground truth depth data for supervised learning. It’s difficult and time-consuming to retrieve focal stacks with registered depth maps, let alone the imperfect depth data obtained by hardware solutions such as
ToF sensors [11]. One could synthesize defocus blur on a synthetic dataset with synthetic depth maps [23]. However, it is still questionable whether the thin lens synthesis model could represent real-world optics well.
In this paper, we propose a novel method to estimate depth and an all-in-focus (AiF) image jointly from an input focal stack. We exploit the relationship between these two tasks and design a shared common network. Moreover, the proposed network can be trained either supervisedly with ground truth depth maps or unsupervisedly with only ground truth AiF images. Compared to high-quality labeled depth, acquiring AiF images is relatively easier because AiF images can be captured with smaller apertures along with longer
exposures. However, collecting the corresponding focal stack of an AiF image might be difficult because of the focus breathing phenomenon, where a camera’s field of view changes as the lens moves. To address this problem, we use synthetic data without such effects during training, and apply a calibration process on real data during testing.
Our contribution is three-fold:
• When trained supervisedly, our method outperforms the state-of-the-art methods in various comparisons, while our method also runs faster.
• To our knowledge, the proposed method is the first that can learn depth estimation unsupervisedly from only AiF images and performs favorably against the state-of-the-art methods.
• Domain gaps can be mitigated by our method with test-time optimization on real-world data, especially when ground truth depth data are not available. 2.