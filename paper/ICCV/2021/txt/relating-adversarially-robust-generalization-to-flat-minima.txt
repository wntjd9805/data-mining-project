Abstract
Adversarial training (AT) has become the de-facto stan-dard to obtain models robust against adversarial exam-ples. However, AT exhibits severe robust overﬁtting: cross-entropy loss on adversarial examples, so-called robust loss, decreases continuously on training examples, while even-tually increasing on test examples. In practice, this leads to poor robust generalization, i.e., adversarial robustness does not generalize well to new examples.
In this paper, we study the relationship between robust generalization and
ﬂatness of the robust loss landscape in weight space, i.e., whether robust loss changes signiﬁcantly when perturbing weights. To this end, we propose average- and worst-case metrics to measure ﬂatness in the robust loss landscape and show a correlation between good robust generaliza-tion and ﬂatness. For example, throughout training, ﬂat-ness reduces signiﬁcantly during overﬁtting such that early stopping effectively ﬁnds ﬂatter minima in the robust loss landscape. Similarly, AT variants achieving higher adver-sarial robustness also correspond to ﬂatter minima. This holds for many popular choices, e.g., AT-AWP, TRADES,
MART, AT with self-supervision or additional unlabeled ex-amples, as well as simple regularization techniques, e.g.,
AutoAugment, weight decay or label noise. For fair com-parison across these approaches, our ﬂatness measures are speciﬁcally designed to be scale-invariant and we conduct extensive experiments to validate our ﬁndings. 1.

Introduction
In order to obtain robustness against adversarial exam-ples [56], adversarial training (AT) [37] augments train-ing with adversarial examples that are generated on-the-ﬂy. While many different variants have been proposed,
AT is known to require more training data [29, 49], gen-erally leading to generalization problems [17]. In fact, ro-bust overﬁtting [46] has been identiﬁed as the main problem in AT: adversarial robustness on test examples eventually starts to decrease, while robustness on training examples
Figure 1: Robust Generalization and Flatness: Robust loss (RLoss, i.e., cross-lower is more robust, y-axis), entropy loss on PGD adversarial examples [37], against our average-case ﬂatness measure of RLoss in weight space (lower is “ﬂatter”, x-axis). Popular AT variants improving adversarial robustness on CIFAR10, e.g., TRADES [72],
AT-AWP [62], MART [60] or AT with self-supervision
[22]/unlabeled examples [7], also correspond to ﬂatter min-ima. Vice-versa, regularization explicitly improving ﬂat-ness, e.g., Entropy-SGD [8], weight decay or weight clip-ping [52], also improve robustness. Across all models, there is a clear relationship between good robust generaliza-,(cid:7) Our models, without early tion and ﬂatness in RLoss.
• stopping. (cid:78) RobustBench [10] models with early stopping. continues to increase (cf. Fig. 2). This is typically observed as increasing robust loss (RLoss) or robust test error (RErr), i.e., (cross-entropy) loss and test error on adversarial exam-ples. As a result, the robust generalization gap, i.e., the difference between test and training robustness, tends to be very large. In [46], early stopping is used as a simple and effective strategy to avoid robust overﬁtting. However, de-spite recent work tackling robust overﬁtting [51, 62, 25], it remains an open and poorly understood problem.
In “clean” generalization (i.e., on natural examples), overﬁtting is well-studied and commonly tied to ﬂatness of the loss landscape in weight space, both visually [34] and
Figure 2: Robust Overﬁtting: Robust (cross-entropy) loss (RLoss) and robust error (RErr) over epochs (normalized by 150 epochs) for AT, using a ResNet-18 on CIFAR10 (cf. Sec. 4), to illustrate robust overﬁtting. Left: Training
RLoss (light blue) reduces continuously throughout train-ing, while test RLoss (dark blue) eventually increases again.
We also highlight that robust overﬁtting is not limited to in-correctly classiﬁed examples (green), but also affects cor-rectly classiﬁed ones (rose). Right: Similar behavior, but less pronounced, can be observed considering RErr. We also show RErr obtained through early stopping (red). empirically [41, 28, 27]. In general, the optimal weights on test examples do not coincide with the minimum found on training examples. Flatness ensures that the loss does not increase signiﬁcantly in a neighborhood around the found minimum. Therefore, ﬂatness leads to good generalization because the loss on test examples does not increase sig-niﬁcantly (i.e., small generalization gap, cf. Fig. 3, right).
[34] showed that visually ﬂatter minima correspond to bet-ter generalization.
[41] and [28] formalize this idea by measuring the change in loss within a local neighborhood around the minimum considering random [41] or “adversar-ial” weight perturbations [28]. These measures are shown to be effective in predicting generalization in a recent large-scale empirical study [27] and explicitly encouraging ﬂat-ness during training has been shown to be successful in practice [74, 9, 35, 8, 26].
Recently, [62] applied the idea of ﬂat minima to AT: through adversarial weight perturbations, AT is regularized to ﬁnd ﬂatter minima of the robust loss landscape. This re-duces the impact of robust overﬁtting and improves robust generalization, but does not avoid robust overﬁtting. As re-sult, early stopping is still necessary. Furthermore, ﬂatness is only assessed visually and it remains unclear whether ﬂat-ness does actually improve in these adversarial weight di-rections. Similarly, [18] shows that weight averaging [26] can improve robust generalization, indicating that ﬂatness might be beneﬁcial in general. This raises the question whether other “tricks” [42, 18], e.g., different activation functions [51] or label smoothing [55], or approaches such as AT with self-supervision [22]/unlabeled examples [7] are successful because of ﬁnding ﬂatter minima.
Contributions: In this paper, we study whether ﬂat-ness of the robust loss (RLoss) in weight space im-proves robust generalization. To this end, we propose
Figure 3: Measuring Flatness. Left: Illustration of mea-suring ﬂatness in a random (i.e., average-case, blue) di-rection by computing the difference between RLoss ˜ af-L ter perturbing weights (i.e., w + ν) and the “reference” given a local neighborhood Bξ(w) around the
RLoss
In practice, we average found weights w, see Sec. 3.3. across/take the worst of several random/adversarial direc-tions. Right: Large changes in RLoss around the “sharp” minimum causes poor generalization from training (black) to test examples (red).
L both average- and worst-case ﬂatness measures for the ro-bust case, thereby addressing challenges such as scale-invariance [14], estimation of RLoss on top or jointly with weight perturbations, and the discrepancy between RLoss and RErr. We show that robust generalization generally improves alongside ﬂatness and vice-versa: Fig. 1 plots
RLoss (lower is more robust, y-axis) against our average-case ﬂatness in RLoss (lower is ﬂatter, x-axis), showing a clear relationship. In contrast to [62], not providing empir-ical ﬂatness measures, our results show that this relation-ship is stronger for average-case ﬂatness. This trend covers a wide range of AT variants on CIFAR10, e.g., AT-AWP
[62], TRADES [72], MART [60], AT with self-supervision
[22] or additional unlabeled examples [7, 2], as well as var-ious regularization schemes, including AutoAugment [12], label smoothing [55] and noise or weight clipping [52]. Fur-thermore, we consider hyper-parameters, e.g., learning rate schedule, weight decay, batch size, or different activation functions [15, 39, 21], and methods explicitly improving
ﬂatness, e.g., Entropy-SGD [8] or weight averaging [26]. 2.