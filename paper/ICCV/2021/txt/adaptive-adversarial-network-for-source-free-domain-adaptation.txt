Abstract
Unsupervised Domain Adaptation solves knowledge transfer along with the coexistence of well-annotated source the domain and unlabeled target instances. However, source domain in many practical applications is not al-ways accessible due to data privacy or the insufficient mem-ory storage for small devices. This scenario defined as
Source-free Domain Adaptation only allows accessing the well-trained source model for target learning. To address the challenge of source data unavailability, we develop an
Adaptive Adversarial Network (A2Net) including three components. Specifically, the first one named Adaptive Ad-versarial Inference seeks a target-specific classifier to ad-vance the recognition of samples which the provided source-specific classifier difficultly identifies. Then, the Contrastive
Category-wise Matching module exploits the positive rela-tion of every two target images to enforce the compactness of subspace for each category. Thirdly, Self-Supervised Ro-tation facilitates the model to learn additional semantics from target images by themselves. Extensive experiments on the popular cross-domain benchmarks verify the effec-tiveness of our proposed model on solving adaptation task without any source data. 1.

Introduction
Recent years witness great promising achievements from the exploration of deep neural network (DNN) in the prac-tical scenarios, i.e., image classification, segmentation and detection [20, 47, 24, 48]. However, DNN model easily suf-fers from severe performance degradation when evaluated on test data (target domain) lying in different distribution from the training instances (source domain). Such discrep-ancy termed as domain shift [9, 36] results from the vary-ing environments or devices [28] and various image styles
[49]. To tackle the challenge, unsupervised domain adap-tation (UDA) attracts increasing attention and achieves en-couraging results by using deep learning architecture.
Conventional UDA assumes the cross-domain data is
Figure 1. Schematic diagram of high-level source (black) and tar-get (red) feature distributions from the trained source model. The target samples can be divided into two subsets: source-similar and source-dissimilar sets. Square and circle represent two different categories. A2Net adaptively learns a new classifier (dashed) based on the frozen classifier (solid) trained in source domain. available during model training, so that it effectively mea-sures and eliminates the domain discrepancy [7, 39, 42].
Based on this assumption, the mainstream solutions of UDA are roughly divided into two paradigms. One branch at-tempts to transform source and target data into the high-level feature space with the consistent statistical moments to achieve the alignment of their feature distributions [14, 26, 41, 3, 16]. As for the representative work maximum mean discrepancy (MMD), the learned cross-domain fea-tures are enforced to share the identical first-order moment.
The other branch devotes more efforts to the deployment of adversarial framework [27, 33, 2]. The core strategy is exploiting feature generator to deceive the domain discrim-inator so that it fails to recognize which domain the fea-ture comes from. Despite the successes of these methods, it is not hard to observe that they heavily depend on the co-existence of source and target data. However, abundant application scenarios cannot always meet the basic assump-tion of UDA due to data privacy and memory constraint of small devices. For instance, the training benchmark of Im-ageNet [6] contains 14 million images occupying hundreds gigabytes storage, which is a huge burden for small-storage equipment. Moreover, many industries such as hospitals are
restricted to share their sensitive data with external sites.
The conflict between the practical demand and UDA set-ting motivates the novel research direction named Source-free Domain Adaptation where we are only provided with the well-trained source model instead of well-annotated source data to achieve adaptation to target data. Recently, a few research efforts [21, 19] start exploring this new sce-nario on cross-domain classification task by assuming that the source classifier contains sufficient knowledge. Thus, they both attempt to directly adjust target features to adapt the source classifier. Among them, SHOT [21], as a sim-ple yet efficient method, freezes the source classifier and integrates pseudo-label supervision and entropy minimiza-tion [46] to shorten the distance between target features and source classification boundary. Similarly, MA [19] first considers source classifier as an anchor to guide the gener-ation of new target samples closer to the source domain and then adopts adversarial strategy to achieve domain align-ment.
In addition, SoFA [43] adopts self-supervised re-construction to extract more discriminative knowledge from target images themselves to improve the classification abil-ity of model. However, when the data in source domain is imbalanced or insufficient, the above methods with frozen classifier becomes vulnerable due to the lower generaliza-tion of source classifier. It is difficult for these approaches to move abundant target features with large variance into the small source classification boundary. For example, as illus-trated in Figure 1, the source classifier (solid line) trained on the imbalanced data where circle class has only a few data points. Restricted by the frozen classifier, this, unfortu-nately, leads to a bad classification performance in source-dissimilar set. From another perspective, we post a ques-tion: “Can we seek a novel target-specific classifier during model optimization and adapt it to the target features?”.
Along with such a question, we propose a novel Adap-tive Adversarial Network (A2Net) to address the Source-Free Domain Adaptation. To achieve flexible adjustment for classifier and preserve the original source knowledge, our work firstly introduces a novel target classifier and then exploits dual-classifier design to achieve adversar-ial domain-level alignment and contrastive category-wise matching (CCM). Concretely, according to the predictions of source and target classifiers, we adaptively divide target samples into two categories: source-similar set and source-dissimilar one. By building such an adversarial relation be-tween dual-classifier and feature generator, A2Net grad-ually eliminates the significant difference across source-similar and source-dissimilar sets and remedies the defect of the frozen source classifier by updating the target classifier.
To further learn discriminative features, our work considers the relation of paired samples consisting two any target im-ages as three levels: positive, uncertain and negative pairs, and develops contrastive category-wise matching over all positive pairs to intensify their association. The main con-tributions of our work are summarized as three folds:
• First, the proposed A2Net integrates a new flex-ible classifier to be available for optimization and the frozen source classifier to form the dual-classifier architecture which we use to adaptively distinguish source-similar target samples from source-dissimilar ones and achieve alignment across them.
• Second, A2Net learns robust and discriminative fea-tures in a self-supervised learning manner. Specifi-cally, the contrastive category-wise matching module relies on source knowledge to explore the association of the paired target features and enforce the positive relation to achieve category-wise alignment.
• Finally, we further enhance the model to learn ad-ditional semantics through a self-supervised rotation.
Experimental results on three benchmarks fully verify the effectiveness of A2Net for source-free scenario. 2.