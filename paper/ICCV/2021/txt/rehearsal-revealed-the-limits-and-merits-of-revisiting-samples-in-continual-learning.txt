Abstract
Learning from non-stationary data streams and over-coming catastrophic forgetting still poses a serious chal-lenge for machine learning research. Rather than aiming to improve state-of-the-art, in this work we provide insight into the limits and merits of rehearsal, one of continual learn-ing’s most established methods. We hypothesize that models trained sequentially with rehearsal tend to stay in the same low-loss region after a task has finished, but are at risk of overfitting on its sample memory, hence harming general-ization. We provide both conceptual and strong empirical evidence on three benchmarks for both behaviors, bringing novel insights into the dynamics of rehearsal and continual learning in general. Finally, we interpret important contin-ual learning works in the light of our findings, allowing for a deeper understanding of their successes.1 1.

Introduction
Recent advances of neural networks have shown promis-ing results by surpassing human capabilities in a wide range of tasks [36, 29, 35]. However, these tasks are typically highly confined and remain static after deployment. This stems from a major limitation in neural network optimiza-tion, namely the assumption of independent and identically distributed (iid) training and testing distributions. When the iid assumption is not satisfied during learning, neural networks are prone to catastrophic forgetting [12], causing them to completely forget previously acquired knowledge.
Continual or lifelong learning strives to overcome this static nature of neural networks with a wide range of mech-anisms [10], among which rehearsal has shown promising results [22, 28, 7, 9]. Rehearsal aims to approximate the observed input distributions over time and later resamples from this approximation to avoid forgetting. Although there are various ways to use the input distribution approximation
*Authors contributed equally. 1Code: https://github.com/Mattdl/RehearsalRevealed
Figure 1: Illustration of our findings, visualized as the loss values in parameter space. When using rehearsal to train task 2 (T 2) after training task 1 (T 1), i.e. from w1 to w2, the learning trajectory will first move towards a finetuned minimum w2,F T but deflects at a high-loss ridge of T 1’s low-loss region. The crux is w2 ending up in the striped area where low loss for T 1 memory is in contrast to the higher loss observed for T 1 test data. Additionally, we show em-pirically that linear low loss paths (a) and (c) exist between w1 and w2, and between two models trained with different rehearsal memories w2 and w′ 2. (Section 2), this study focuses and refers to rehearsal in its most direct form, i.e. by sampling the input distribution in a limited rehearsal memory from which samples are revisited in later training batches.
Despite the wide use of rehearsal, due to its simplicity and effectiveness, fundamental analysis of why it works and what its limitations are is lacking in literature. Fur-thermore, we believe that insights into its internal workings might deepen our understanding of the catastrophic forget-ting phenomenon in general. In this work, we make an ini-tial attempt from the perspective of loss landscapes.
Fundamental open questions. Motivated by recent ad-vances in continual learning literature, we define two fun-damental open questions. Early work in rehearsal raised concerns about overfitting to the rehearsal memory as a consequence of repeated optimization on this limited set of data [22]. Following work [7] confirms this as the replay memory becomes perfectly memorized by the model, but also finds rehearsal to remain effective in terms of gener-alization. This leads to two open questions. First, ”Why does rehearsal work even though overfitting on the rehearsal memory occurs?”. Second, ”How does overfitting on the re-hearsal memory influence generalization?”.
Motivation. To formalize these inquiries for this study, we formulate two main hypotheses motivated by advances in prior work. Firstly, recent work empirically finds continual learning minima of individual tasks to be linearly connected through a low-loss path with the multitask solution, when starting from the same initialization [25]. Multitask learn-ing simultaneously learns multiple tasks, which rehearsal ultimately aims to approximate in the continual learning regime. Therefore, we hypothesize the rehearsal solution resides in the same low-loss region as the original task and the multitask solutions. Note that we refer the same low-loss region and not just any region with similar loss. Sec-ondly, large models are able to completely memorize small sets of data, such as rehearsal memories, without any gen-eralization capabilities [40]. Therefore we hypothesize that overfitting does harm generalization after all.
Contributions. Our two hypotheses are formalized as: 1. Hypothesis 1: Rehearsal is effective as its solution tends to reside in the same low-loss region as the task minimum from which learning is initiated. 2. Hypothesis 2: Rehearsal is suboptimal as it tends to overfit on the rehearsal memory, consequently harming generalization.
Section 4 provides extensive empirical evidence on MNIST,
CIFAR, and Mini-Imagenet data sequences to test both hy-potheses. Our findings can be summarized as follows: 1. The results in Section 4.1 unanimously support Hy-pothesis 1. Even after learning longer sequences of up to 5 tasks with rehearsal, the new minimum is found in the same low-loss region as for the first task. This suggests the existence of overlapping low-loss regions between the task’s input distribution and its approxi-mation by the rehearsal memory. 2. As suggested in [22, 7], Section 4.2 confirms overfit-ting on the rehearsal memory. However, this overfit-ting by itself is insufficient to explain harming gen-eralization, as following Hypothesis 1 there exists an overlapping low-loss region with the one of the task’s input distribution. Our empirical evidence finds clues that internal rehearsal dynamics draw the learning tra-jectory near a high-loss ridge of the rehearsal mem-ory. Additionally, near the high-loss ridge the approx-imation by the rehearsal memory for the task’s input distribution loss deteriorates. Therefore, we can con-firm the suboptimality of rehearsal in Hypothesis 2, but generalization is harmed by the combination of overfit-ting, internal rehearsal dynamics, and the low-quality approximation of the rehearsal memory near its high-loss ridges.
Furthermore, Section 4.3 provides additional evidence with a simple heuristic to withdraw from the rehearsal mem-ory high-loss ridge, showing promising results compared to standard rehearsal. Additionally, Section 4.4 concep-tually analyses the internal rehearsal dynamics w.r.t. our results. We believe these findings bring new insights into ultimately understanding catastrophic forgetting and pro-posed methods in literature. In Section 5 we discuss recent salient works in the light of our findings, namely GEM [22],
MIR [2], and GDumb [27]. 2.