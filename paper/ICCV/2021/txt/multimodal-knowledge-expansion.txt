Abstract
The popularity of multimodal sensors and the accessi-bility of the Internet have brought us a massive amount of unlabeled multimodal data. Since existing datasets and well-trained models are primarily unimodal, the modal-ity gap between a unimodal network and unlabeled multi-modal data poses an interesting problem: how to transfer a pre-trained unimodal network to perform the same task with extra unlabeled multimodal data? In this work, we propose multimodal knowledge expansion (MKE), a knowl-edge distillation-based framework to effectively utilize mul-timodal data without requiring labels. Opposite to tradi-tional knowledge distillation, where the student is designed to be lightweight and inferior to the teacher, we observe that a multimodal student model consistently rectifies pseudo la-bels and generalizes better than its teacher. Extensive ex-periments on four tasks and different modalities verify this finding. Furthermore, we connect the mechanism of MKE to semi-supervised learning and offer both empirical and theoretical explanations to understand the expansion capa-bility of a multimodal student. 1 1.

Introduction
Deep neural networks and supervised learning have made outstanding achievements in fields like computer vi-sion [16, 21, 33] and computer audition [17, 47]. With the popularity of multimodal data collection devices (e.g.,
RGB-D cameras and video cameras) and the accessibility of the Internet, a large amount of unlabeled multimodal data has become available. A couple of examples are shown in
Figure 1: (a) A unimodal dataset has been previously anno-tated for the data collected by an old robot; after a hardware upgrade with an additional sensor, the roboticist has access
*Corresponding to hangzhao@mail.tsinghua.edu.cn 1Code is available at: https://github.com/zihuixue/MKE
Figure 1: The popularity of multimodal data collection devices and the Internet engenders a large amount of unlabeled multimodal data. We show two examples above: (a) after a hardware upgrade, lots of unannotated multimodal data are collected by the new sen-sor suite; (b) large-scale unlabeled videos can be easily obtained from the Internet. to some new unlabeled multimodal data. (b) Internet videos are abundant and easily accessible. While there are exist-ing unimodal datasets and models for tasks such as image recognition, we further want to perform the same task on unlabeled videos. A natural question arises: how to transfer a unimodal network to the unlabeled multimodal data?
One naive solution is to directly apply the unimodal net-work for inference using the corresponding modality of un-labeled data. However, it overlooks information described by the other modalities. While learning with multimodal data has the advantage of facilitating information fusion and inducing more robust models compared with only using one modality, developing a multimodal network with supervised learning requires tremendous human labeling efforts.
In this work, we propose multimodal knowledge expan-sion (MKE), a knowledge distillation-based framework, to make the best use of unlabeled multimodal data. MKE en-ables a multimodal network to learn on the unlabeled data with minimum human labor (i.e., no annotation of the mul-2.