Abstract
The video-based action recognition task has been ex-In this paper, we study tensively studied in recent years. the structural vulnerability of deep learning-based action recognition models against the adversarial attack using the one frame attack that adds an inconspicuous perturbation to only a single frame of a given video clip. Our analysis shows that the models are highly vulnerable against the one frame attack due to their structural properties. Experiments demonstrate high fooling rates and inconspicuous charac-teristics of the attack. Furthermore, we show that strong universal one frame perturbations can be obtained under various scenarios. Our work raises the serious issue of ad-versarial vulnerability of the state-of-the-art action recog-nition models in various perspectives. 1.

Introduction
Human action recognition using videos has been exten-sively studied in recent years thanks to the development of deep network-based algorithms based on the abundance of computational resources and data [6]. From a network de-sign perspective, the distinguished main research topic of action recognition is how to model temporal information residing in video clips. Concerning this, various attempts have been made, such as utilizing the long short-term mem-ory (LSTM) module [3] or the optical flow [1], but re-cently, 3D convolutional neural network (CNN)-based ac-tion recognition models are widely used. To improve the performance and efficiency of 3D CNN-based action recog-nition models, various mechanisms in the temporal dimen-sion have been proposed, such as frame selection [4] and convolutional operations [1, 22, 20].
Many researchers have found the vulnerability of deep learning-based algorithms against so-called adversarial at-tacks, which add an inconspicuous perturbation to input data to mislead a target model to produce wrong output.
It has been reported that many state-of-the-art deep image
Figure 1: Overall illustration of one frame attack targeting a vulnerable frame. With only one attacked frame, the target action recognition model wrongly classifies a given video in
Kinetics-400 [8] as “peeling apples” instead of “arranging flowers.” classification methods are highly vulnerable to the adversar-ial attacks [16] and can raise severe security concerns [5].
On the other hand, there are not many studies on the vulner-ability of video-based deep action recognition systems.
We argue that the ways of modeling temporal informa-tion in deep models for action recognition have significant impact on the vulnerability of the models, which we de-note as structural vulnerability. Although there exist a few attempts for adversarial attacks on action recognition sys-tems [23, 14], they do not sufficiently consider the structural vulnerability of recent action recognition models. As a re-sult, they do not fully satisfy two criteria that a successful adversarial attack should meet: 1) achieving a high fool-ing rate and 2) keeping adversarial perturbation invisible to conceal that the video clip is attacked. Wei et al. [23] pro-posed an attack method of perturbing only a few frames of a video clip, in order to reduce computational resources and achieve inconspicuousness. However, this method targets only LSTM-based models and relies on the particular prop-erty of LSTM, i.e., temporal propagation of information.
Thus, it does not achieve a high fooling rate on the latest
CNN-based action recognition models (see Table 1). On
the other hand, Pony et al. [14] proposed an attack method to find a sequence of flickering perturbation that changes the overall color of a given video clip over time. However, it is known that artifacts changing in the temporal dimen-sion are more detectable than spatial artifacts by human ob-servers [13, 24]. Therefore, this adversarial attack does not sufficiently satisfy the second criterion, inconspicuousness (see Figure 7).
In this paper, we discover the structural vulnerability of recent CNN-based deep action recognition models, which has not been explored previously to the best of our knowl-edge. Using this vulnerability, we also show that perturba-tion in just a single vulnerable frame of a video clip can sig-nificantly degrade the accuracy of deep action recognition models, as illustrated in Figure 1. The attacked frame is shown only for 33 or 40 milliseconds when the target video clip has 30 or 25 frames per second (FPS), which is hardly perceivable to human observers. The main contributions of this work can be summarized as follows.
• We investigate the vulnerability caused by the struc-tural property of deep models using three state-of-the-art CNN-based deep action recognition models and ex-amine what factors of these models make them highly vulnerable against adversarial attacks. We show that the efforts to efficiently model temporal information induce the vulnerability issue.
• We show the possibility of so-called one frame attack on action recognition models in a white-box attack sce-nario. When only one vulnerable frame found by our analysis is perturbed with a gradient-based adversarial attack method, this perturbation can easily defeat deep learning-based action recognition systems. This one frame attack can fool the state-of-the-art video-based action recognition models with fooling rates of almost 100%. In addition, this adversarial attack is inconspic-uous, which is demonstrated via a subjective experi-ment.
• We further explore video-agnostic universal perturba-tion based on the one frame attack. We show that the universal perturbation, which is found from a small number of videos, can affect other input video clips with high fooling rates. Besides, the one frame attack can be effectively applied to time-invariant scenarios where the perturbation is added to the input video clip with an unknown temporal offset. 2.