Abstract
While multi-step adversarial training is widely popular as an effective defense method against strong adversarial attacks, its computational cost is notoriously expensive, com-pared to standard training. Several single-step adversarial training methods have been proposed to mitigate the above-mentioned overhead cost; however, their performance is not sufficiently reliable depending on the optimization setting.
To overcome such limitations, we deviate from the existing input-space-based adversarial training regime and propose a single-step latent adversarial training method (SLAT), which leverages the gradients of latent representation as the latent adversarial perturbation. We demonstrate that the ℓ1 norm of feature gradients is implicitly regularized through the adopted latent perturbation, thereby recovering local linearity and ensuring reliable performance, compared to the existing single-step adversarial training methods. Be-cause latent perturbation is based on the gradients of the latent representations which can be obtained for free in the process of input gradients computation, the proposed method costs roughly the same time as the fast gradient sign method.
Experiment results demonstrate that the proposed method, despite its structural simplicity, outperforms state-of-the-art accelerated adversarial training methods. 1.

Introduction
Although several studies have suggested the use of deep learning methods to solve challenging tasks, adversarial vul-nerability [30] is one of the remaining major challenges while employing deep learning to safety-critical applications.
Adversarial training (AT) approaches aim to mitigate the problem by training the model on generated adversarial ex-amples, i.e. the sample corrupted with human-imperceptible noise which can fool the state-of-the-art deep neural net-works. Although PGD AT [16] is one of the most effective training methods, it consumes a considerable training time because it relies on multiple projected gradient descent steps to generate the adversaries. The AT based on Fast Gradient
Sign Method (FGSM; [10]) reduces the training time; how-ever, recent works [16, 32, 31] have identified the FGSM’s vulnerability to the sophisticated adversaries.
The trade-off between adversarial robustness and compu-tational cost has facilitated the development of accelerated and trustworthy AT methods. Shafahi et al. [25] significantly reduced the computational burden by presenting a free AT method that updates both model parameters and adversarial perturbation through a single shared backward propagation.
Wong et al. [38] proposed a fast adversarial training based on the discovery that a slight modification in the FGSM train-ing method such as random initialization allows it to achieve an adversarial robustness on par with PGD AT. They also discovered the catastrophic overfitting problem of FGSM
AT, wherein the model suddenly loses its robustness during training within an epoch.
Although substantial technical advances have been made with regard to the above-mentioned methods, recent works have reported that such approaches are not sufficiently re-liable. Andriushchenko et al. [2] demonstrated that fast adversarial training still suffers from the catastrophic over-fitting, owing to the deteriorated local linearity of neural networks. Kim et al. [14] found that the fast adversarial training suddenly loses its robustness and eventually col-lapses when a simple multi-step learning rate schedule is used. Li et al. [15] reported that although fast adversar-ial training may recover quickly, it still temporally exhibits catastrophic overfitting.
This remaining problem in AT motivates us to explore novel ways to improve the reliability of single-step AT, with-out bearing a considerable training time. In this study, we demonstrate that the single-step latent adversarial training (SLAT) with the latent adversarial perturbation operates more effectively and reliably compared to the other single-step adversarial training variants. While many of existing adversarial training methods require multiple gradient com-putations which is inevitably time-consuming, we exploit the gradients of latent representations from multiple layers in parallel for the synergistic generation of adversary. Note that the gradients of latent representations can be obtained
(a) Existing approaches (b) SLAT
Figure 1: Visual illustration of the conceptual difference between existing and proposed approach. (a) FGSM may fail to generate the appropriate adversary because it approximates the solution of inner maximization problem with a single gradient step. While PGD-based AT may generate relatively more desirable adversary, it takes multiple iterations per sample to solve the inner maximization which is computationally expensive. Uniform random initialization [38] contributes to improving the performance of FGSM; however, the success of such initialization is not fundamentally justified. (b) Our proposed method mitigates the suggested problems by introducing latent adversarial perturbations in parallel. for free in the process of computation of input gradients.
To the best of our knowledge, our work is the first to reduce the computational cost of AT by deviating from the input-space based frameworks and injecting latent adversar-ial perturbations. The contribution of this study is summa-rized as follows. First, we propose that the local linearity of neural networks can be regularized without demanding cost of training time by adopting latent adversarial perturbation, unlike the gradient alignment (GA) regularization [2], which is three times slower compared to FGSM training. In partic-ular, we demonstrates that the ℓ1 norm of feature gradients is implicitly regularized by introducing latent adversarial perturbation, which closes the gap between the loss function of neural networks and its first-order approximation. As the latent adversarial perturbation is adopted across multi-ple latent layers, the synergistic regularization effect can be expected. Second, we demonstrate that SLAT outperforms the state-of-the-art accelerated adversarial training methods, while achieving performance comparable to PGD AT. 2.