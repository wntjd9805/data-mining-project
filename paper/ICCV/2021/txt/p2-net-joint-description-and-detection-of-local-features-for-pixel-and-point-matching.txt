Abstract
Accurately describing and detecting 2D and 3D key-points is crucial to establishing correspondences across im-ages and point clouds. Despite a plethora of learning-based 2D or 3D local feature descriptors and detectors having been proposed, the derivation of a shared descrip-tor and joint keypoint detector that directly matches pix-els and points remains under-explored by the community.
This work takes the initiative to establish ﬁne-grained cor-respondences between 2D images and 3D point clouds.
In order to directly match pixels and points, a dual fully-convolutional framework is presented that maps 2D and 3D inputs into a shared latent representation space to simul-taneously describe and detect keypoints. Furthermore, an ultra-wide reception mechanism and a novel loss function are designed to mitigate the intrinsic information variations between pixel and point local regions. Extensive experimen-tal results demonstrate that our framework shows competi-tive performance in ﬁne-grained matching between images and point clouds and achieves state-of-the-art results for the task of indoor visual localization. Our source code is avail-able at https://github.com/BingCS/P2-Net. 1.

Introduction
Establishing accurate pixel- and point- level matches across images and point clouds, respectively, is a funda-mental computer vision task that is crucial for a multitude of applications, such as Simultaneous Localization And
Mapping [34], Structure-from-Motion [44], pose estimation
[35], 3D reconstruction [25], and visual localization [42].
A typical pipeline of most methods is to ﬁrst recover the 3D structure given an image sequence [24, 41], and subse-quently perform matching between pixels and points based on 2D to 3D reprojected features. Such features will be ho-mogeneous as points in the reconstructed 3D model inherit
Figure 1: Examples of 2D-3D matches obtained by the
P2-Net. The proposed method can directly establish corre-spondences across images and point clouds by the jointly learned feature description and detection. the descriptors from corresponding pixels of the image se-quence. However, this two-step procedure requires accu-rate 3D reconstruction, which is not always feasible to be achieved, e.g., under challenging illumination or large view-point changes. More critically, this approach treats RGB images as ﬁrst-class citizens, and discounts the equivalence of sensors capable of directly capturing 3D point clouds, e.g., LIDAR, imaging RADAR and depth cameras. These factors motivate us to consider a uniﬁed approach to pixel and point matching, where an open question can be posed: how to directly establish correspondences between pixels in 2D images and points in 3D point clouds, and vice-versa?
This is inherently challenging as 2D images capture scene appearance, whereas 3D point clouds encode structure.
To this end, we formulate a new task of direct 2D pixel and 3D point matching (cf. Fig. 1) without any auxiliary steps (e.g., reconstruction). This task is undoubtedly chal-lenging for existing conventional and learning-based ap-proaches, which fail to bridge the gap between 2D and 3D representations as separately extracted 2D and 3D local fea-tures are distinct and do not share a common embedding.
Some recent works [20, 39] attempt to associate descriptors from different domains by mapping 2D and 3D inputs onto a shared latent space. However, they construct patch-wise descriptors, leading to coarse-grained matching results only.
Even if ﬁne-grained and accurate descriptors can be suc-cessfully obtained, direct pixel and point correspondences are still very difﬁcult to establish. First, 2D and 3D key-points are extracted based on distinct strategies - what leads to a good match in 2D (e.g., ﬂat, visually distinct area such as a poster), does not necessarily correspond to what makes a strong match in 3D (e.g., a poorly illuminated corner of the room). Additionally, because of the sparsity of point clouds, the local feature for a point can be mapped to (or from) many pixel features taken from pixels that are spa-tially close to the point, increasing the matching ambiguity.
Second, due to the large discrepancy between 2D and 3D data property and inﬂexible optimization manner, existing descriptor loss formulations [18, 31, 2] for either 2D or 3D local feature description do not guarantee convergence in this new context. Moreover, their detector designs only fo-cus on penalizing the confounding descriptors from a safe region, incurring sub-optimal matching results in practice.
To tackle all these challenges, we propose a dual fully-convolutional framework, named Pixel and Point Network (P2-Net), which is able to simultaneously achieve feature description and detection between 2D and 3D views. Fur-thermore, an ultra-wide reception mechanism is equipped when extracting descriptors to tackle the intrinsic informa-tion variations between pixel and point local regions. To optimize the network, we then design P2-Loss, consisting of two components: 1) a circle-guided descriptor loss in combination with a full sampling strategy, allowing to ro-bustly learn distinctive descriptors by optimizing positive and negative matches in a self-paced manner; 2) a batch-hard detector loss, which additionally seeks for the re-peatability of detections by encouraging the difference be-tween the positive and globally hardest negative matches.
Overall, our contributions are as follows: 1. We propose a joint learning framework with an ultra-wide reception mechanism for simultaneous 2D and 3D local features description and detection to achieve direct pixel and point matching. 2. We design a novel loss, composed of a circle-guided descriptor loss and a batch-hard detector loss, to ro-bustly learn distinctive descriptors whilst explicitly guiding accurate detections for both pixels and points. 3. We conduct extensive experiments and ablation stud-ies, demonstrating the practicability of the proposed framework and the generalization ability of the new loss, and providing the intuition behind our choices.
To the best of our knowledge, this is the ﬁrst joint learn-ing framework to handle 2D and 3D local features descrip-tion and detection for direct pixel and point matching. 2.