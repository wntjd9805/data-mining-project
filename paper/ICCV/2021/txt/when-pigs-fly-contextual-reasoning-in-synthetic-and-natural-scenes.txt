Abstract e.g., an object
Context is of fundamental importance to both human in the air is and machine vision; more likely to be an airplane than a pig.
The rich notion of context incorporates several aspects including physics rules, statistical co-occurrences, and relative object sizes, among others. While previous work has focused on crowd-sourced out-of-context photographs from the web to study scene context, controlling the nature and extent of contextual violations has been a daunting task.
Here we introduce a diverse, synthetic Out-of-Context (OCD) with fine-grained control over scene
Dataset context.
By leveraging a 3D simulation engine, we systematically control the gravity, object co-occurrences and relative sizes across 36 object categories in a virtual household environment. We conducted a series of experiments to gain insights into the impact of contextual cues on both human and machine vision using OCD.
We conducted psychophysics experiments to establish a human benchmark for out-of-context recognition, and then compared it with state-of-the-art computer vision models to quantify the gap between the two.
We propose a context-aware recognition transformer model, fusing object and contextual information via multi-head attention. Our model captures useful information for contextual reasoning, enabling human-level performance and better robustness in out-of-context conditions compared to baseline models across OCD and other out-of-context datasets. source code and data are publicly available at https://github.com/kreimanlab/
WhenPigsFlyContext
All
1.

Introduction
A coffee mug is usually a small object (Fig.1a), which does not fly on its own (Fig.1c) and can often be found on a table (Fig.1a) but not on a chair (Fig.1d). Such contextual cues have a pronounced impact on the object recognition capabilities of both humans [39], and computer vision models [34, 7, 25, 22]. Neural networks learn co-occurrence statistics between an object’s appearance and its label, but also between the object’s context and its label
[11, 30, 2]. Therefore, it is not surprising that recognition models fail to recognize objects in unfamiliar contexts
[29]. Despite the fundamental role of context in visual recognition, it remains unclear what contextual cues should be integrated with object information and how.
Two challenges have hindered progress in the study of the role of contextual cues: (1) context has usually been treated as a monolithic concept and (2) large-scale, internet-scraped datasets like ImageNet [9] or COCO [21] are highly uncontrolled. To address these challenges, we present a methodology to systematically study the effects of an object’s context on recognition by leveraging a
Unity-based 3D simulation engine for image generation
[19], and manipulating 3D objects in a virtual home environment [27]. The ability to rigorously control every aspect of the scene enables us to systematically violate contextual rules and assess their impact on recognition.
We focus on three fundamental aspects of context: (1) gravity - objects without physical support, (2) object co-occurrences - unlikely object combinations, and (3) relative size - changes to the size of target objects relative to the background. As a critical benchmark, we conducted psychophysics experiments to measure human performance and compare it with state-of-the-art computer vision models.
We propose a new context-aware architecture, which can incorporate object and contextual information to achieve higher object recognition accuracy given proper context and robustness to out-of-context situations. Our Context-aware
Recognition Transformer Network (CRTNet) uses two separate streams to process the object and its context independently before integrating them via multi-head attention in transformer decoder modules. Across multiple datasets, the CRTNet model surpasses other state-of-the-art computational models in normal context and classifies objects robustly despite large contextual variations, much like humans do.
Our contributions in this paper are three-fold. Firstly, we introduce a challenging new dataset in- and out-of-context object recognition that allows fine-grained control over context violations including gravity, object co-occurrences and relative object sizes (out-of-context dataset, OCD). Secondly, we conduct psychophysics in-experiments to establish a human benchmark for for a new context-aware computer vision models. recognition and compare it with and out-of-context
Finally, state-of-the-art we propose for object recognition, which combines object and scene information to reason about context and generalizes images. We release the entire well dataset, the generation of for additional images and the source code for CRTNet at https://github.com/kreimanlab/WhenPigsFlyContext. to out-of-context including our architecture tools 2.