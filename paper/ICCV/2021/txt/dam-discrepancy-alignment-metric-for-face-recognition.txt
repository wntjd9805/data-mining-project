Abstract
The ﬁeld of face recognition (FR) has witnessed remark-able progress with the surge of deep learning. The effec-tive loss functions play an important role for FR. In this pa-per, we observe that a majority of loss functions, including the widespread triplet loss and softmax-based cross-entropy loss, embed inter-class (negative) similarity sn and intra-class (positive) similarity sp into similarity pairs and opti-mize to reduce (sn − sp) in the training process. However, in the veriﬁcation process, existing metrics directly take the absolute similarity between two features as the conﬁdence of belonging to the same identity, which inevitably causes a gap between the training and veriﬁcation process. To bridge the gap, we propose a new metric called Discrep-ancy Alignment Metric (DAM) for veriﬁcation, which in-troduces the Local Inter-class Discrepancy (LID) for each face image to normalize the absolute similarity score. To estimate the LID of each face image in the veriﬁcation pro-cess, we propose two types of LID Estimation (LIDE) meth-ods, which are reference-based and learning-based estima-tion methods, respectively. The proposed DAM is plug-and-play and can be easily applied to the most existing meth-ods. Extensive experiments on multiple popular face recog-nition benchmark datasets demonstrate the effectiveness of our proposed method. 1.

Introduction
Face recognition based on deep learning has been well investigated for decades [31, 35, 42]. Most of the progress depends on large-scale training data [9, 50, 16], deep neural network architectures [37, 10, 11], and effective loss func-tion designs [26, 6, 41, 39, 53, 27, 4, 25, 13, 7]. Despite many efforts, most prior works use the sample-to-sample absolute similarity metric during inference. The identities are determined by directly thresholding cosine or L2 dis-*Equal contribution
†Corresponding author tance for each face image pair. It is inconsistent with the training process, which optimizes the relative margin be-tween the intra-class and the inter-class similarities. (a) (b)
Figure 1: The similarity distributions on different evaluation metrics of pairs from different IDs. The green histogram represents the positive pairs, and the red histogram repre-sents the negative pairs. (a): Distribution of cosine simi-larity metric. ID1 and ID2 have the same margin between positive and negative pairs, whereas the overall similarity of pairs of ID2 is less than ID1, which leads to judge a large number of positive pairs of ID2 as false negatives. (b):
Distribution of our proposed Discrepancy Alignment Met-ric (DAM). The scores of positive pairs of ID2 are calibrated to a higher level.
Speciﬁcally, the popular softmax-based loss functions (e.g., ArcFace [5], CosFace [41]) or metric learning based loss functions (e.g., Triplet loss [27]) seek to reduce sn − sp or sn − sp + m as the optimization target [32], where sp is the intra-class (positive) similarity, sn refers to inter-class (negative) similarity and m is the margin term to enhance the discriminative ability. Therefore, the rela-tive score sp − sn indicates the optimization degree for each face image during the training process. It works well for common close-set classiﬁcation, which maintains cate-gory weights of the classiﬁer by relative probabilities, i.e., cpred = arg max i=1. However, in open-set face
{ ezi (cid:80) ezj }C i
recognition1, the absolute cosine similarity between the pair of features s = (cid:104)f 1, f 2(cid:105) is considered as the probability of having the same identity (ID), which causes the gap be-tween the training and veriﬁcation process. The zi and zj are the logits predicted by the classiﬁer, f 1 and f 2 are the face embeddings extracted from the neural network, and cpred is the predicted class label from C classes.
A snapshot of a typical example is also shown in Fig. 1a, where two IDs have the same margin between the positive pairs and the negative pairs (i.e., ∆1 = ∆2). It means that they are optimized to the same degree during the training process. In contrast, the overall absolute cosine similarities of pairs of ID2 are less than ID1. When applying the op-timized model in practice, a large number of positive pairs of ID2 are judged as false negatives. In addition, the long-tail or non-uniform distributions usually exist in the train-ing data of real-world face recognition. Therefore, the op-timized model is biased to different domains [1], which ex-acerbates the gap. Consequently, a more accurate metric, which is more consistent with the existing loss functions, is needed to compensate for the gap between the training and inference for face recognition.
Motivated by the above analysis, we propose a new met-ric called Discrepancy Alignment Metric (DAM), which aims to bridge the gap between the training and veriﬁcation process for face recognition. First, we analyze the afore-mentioned gap and introduce the DAM, which incorporates the Local Inter-class Discrepancy (LID) of each feature to normalize the absolute similarity score, and is more con-sistent with the current popular loss functions. Then, we introduce two types of Local Inter-class Discrepancy Es-timation (LIDE) methods, which are the reference-based
LIDE and learning-based LIDE methods, respectively. In our LIDE, we propose to randomly sample from the training set or employ GAN to generate a set of images from diverse identities to build the anchor image set. For the reference-based LIDE method, the neighbors from the anchor image set are searched in the feature space to estimate the LID for each face image, which is ﬂexible without tuning the op-timized models. For the learning-based LIDE method, we directly leverage a learnable regression module to regress the LID for each face image, which avoids the need of an anchor image set during the veriﬁcation process.
The contributions of our paper are as follows: 1) We are the ﬁrst to investigate the gap between the train-ing and veriﬁcation process of face recognition, and propose a new metric called Discrepancy Alignment
Metric, which is plug-and-play and can be readily in-tegrated into existing face recognition methods. 2) The Local Inter-class Discrepancy (LID) of each fea-1In this paper, “open-set face recognition” and “face recognition” can be used interchangeably. ture is incorporated into the new metric to normal-ize the similarity, and two types of Local Inter-class
Discrepancy Estimation (LIDE) methods are intro-duced, including the reference-based and learning-based LIDE methods. 3) Extensive experiments on multiple benchmark datasets show that our proposed DAM signiﬁcantly improves the performance of face recognition. 2.