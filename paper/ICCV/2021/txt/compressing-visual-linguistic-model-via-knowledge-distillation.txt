Abstract
Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small
VL model.
In this paper, we study knowledge distillation (KD) to effectively compress a transformer based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention dis-tributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Stu-dent’s detector while the features are from Teacher’s own object detector. With aligned network inputs, the adapted
Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block, and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly im-proves the performance of small VL models on image cap-It reaches tioning and visual question answering tasks. 120.8 in CIDEr score on COCO captioning, an improve-ment of 5.1 over its non-distilled counterpart; and an accu-racy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effective-ness of VL distillation in both pre-training and fine-tuning stages. 1.

Introduction
There have been exciting progress in visual linguis-tic (VL) pre-training to learn omni-representation mod-els [44, 60, 9, 64, 83, 42] which could benefit a number of downstream tasks (i.e. , image captioning, VQA, image retrieval, etc.). The success can largely be attributed to the self-attention-based [68] transformer architecture, e.g. ,
BERT [14], which is effective in learning from image-text https://asu-active-perception-group.github.io/
DistillVLM
Figure 1: Overview of our proposed VL distillation schema. The
VL model typically contains a region feature extraction module and a multi-modal transformer module. To have an aligned input, we adapt the Teacher VL model based on the region proposals from Student’s region feature extractor. The VL distillation is then performed in both the pre-training stage and the fine-tuning stage. pairs at scale. So far, much of the work has focused on large models that suffer from high latency and large mem-ory footprints at the time of inference, which limits their deployment to resource constrained edge devices for real-world applications.
As one of the effective techniques to compress large models, knowledge distillation (KD) [25, 6] was proposed by injecting the knowledge from a strong Teacher model into a smaller Student model without losing too much gen-eralization power. Typically, the knowledge is transferred though mimicking the output logit [25, 57, 17], reducing the divergence of feature maps [80, 27, 78], or learning the intermediate layer representations [36, 1], etc.
In recent years, KD has been proven effective in com-pressing language models. For instance, Kim et al. [35] adopt KD for sequential model compression. In the trans-former based language model, DistillBERT [57] reduces the size of the BERT-base model by 40% using a cosine embedding loss on the basis of hidden embedding in the transformer block, and a soft-target probability loss. Tiny-BERT [34], MobileBERT [63] and MiniLM [72] further highlight the importance of minimizing the self-attention
distributions across Teacher and Student networks. In par-ticular, [10] visually shows that attention maps in BERT capture substantial linguistic knowledge and syntactic re-lations that provide critical information during the distilla-tion [34].
Heretofore, these advances have not been carried over to
VL model compression. We identify the major challenges that prevent us from applying these techniques directly to
VL distillation: Most existing VLP works [83, 42] use pre-trained object detector (e.g. , Faster-RCNN [54]) to extract regional features as visual tokens then feed them into the multi-modal transformer network for VL pre-training. A smaller VL model usually uses a lightweight detector for faster inference (e.g. , EfficientNet [65] based detector is adopted in [71] as visual feature extractor) that may be dif-ferent from Teacher’s detector. The object proposals from the two different detectors are usually very different, and there is no easy way to obtain the semantic correspondence between the two sets of object proposals.
It is therefore unable to align the attention distributions or hidden embed-dings between Student and Teacher.
To address the aforementioned challenges, we propose a set of strategies to enable distillation of VL models. First, instead of using object proposals from two different de-tectors, we use the same set of object proposals, obtained from Student’s lightweight detector for the visual token extraction of both Teacher and Student (as shown in Fig-ure 2). This ensures the semantic correspondence between the Teacher and Student’s visual tokens. Second, we use a loss term to have the Student to mimic the Teacher’s self-attention distribution at the last transformer layer. Third,
We further distill the knowledge from the outputs of the transformer layers (i.e. , the hidden embeddings). We find that simply learning from the layer-wise Teacher embed-ding does not provide adequate supervision for the distil-lation. Hence, we use a noise contrastive loss to align the token embeddings by contrasting them with randomly sam-pled negative embeddings that are held in a sample queue.
Figure 1 gives an overview of our proposed VL distillation schema, where VL distillation is applied for both the pre-training and fine-tuning stages.
In order to examine the effectiveness of our VL distillation, we choose the same compact transformer architecture used in [72, 71], and the lightweight object detector as in [71], but leverages knowl-edge distillation techniques to facilitate the training of the small VL model (dubbed as DistillVLM ). We show that our
DistillVLM achieves a comparable performance to a large
VL model, and clearly outperforms its non-distilled coun-terpart [71].
To summarize our contributions:
• For the first time, we propose VL distillation, a leverages knowledge distillation to technique that facilitate training of smaller VL models.
• Compared to non-distilled VL model pre-training, VL distillation offers a significant boosting in performance for VL tasks such as image captioning and visual question answering: DistillVLM achieves 120.8 in
CIDEr score on COCO captioning [43] and 69.8 in accuracy on VQA [20] tasks, which are 5.1 more or 0.8 higher than the VL pre-training baselines.
• We provide extensive ablations of DistillVLM , and systematically analyze the effect of various KD strate-gies. This provides insights for future research on VL model distillation. 2.