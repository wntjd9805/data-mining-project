Abstract
Existing RGB-D saliency detection models do not explic-itly encourage RGB and depth to achieve effective multi-modal learning. In this paper, we introduce a novel multi-stage cascaded learning framework via mutual informa-tion minimization to explicitly model the multi-modal in-formation between RGB image and depth data. Speciﬁ-cally, we ﬁrst map the feature of each mode to a lower dimensional feature vector, and adopt mutual information minimization as a regularizer to reduce the redundancy be-tween appearance features from RGB and geometric fea-tures from depth. We then perform multi-stage cascaded learning to impose the mutual information minimization constraint at every stage of the network. Extensive exper-iments on benchmark RGB-D saliency datasets illustrate the effectiveness of our framework. Further, to prosper the development of this ﬁeld, we contribute the largest (7× larger than NJU2K) COME15K dataset, which contains 15,625 image pairs with high quality polygon-/scribble-/object-/instance-/rank-level annotations. Based on these rich labels, we additionally construct four new benchmarks with strong baselines and observe some interesting phenom-ena, which can motivate future model design. Source code and dataset are available at https://github.com/
JingZhang617/cascaded_rgbd_sod. 1.

Introduction
Saliency detection models are trained to discover the re-gions of an image that attract human attention. Conven-tionally, saliency detection is performed mostly on RGB images only [37, 44, 22, 43, 36]. With the availability of depth data as shown in Table 1, RGB-D saliency detection
[46, 35, 51, 53] attracts great attention. The extra depth data provides real-world geometric information, which is useful for scenarios when the foreground shares similar appear-ance to the background. Further, the robustness of depth (cid:63) Deng-Ping Fan (dengpfan@gmail.com) is the corresponding author.
Work was done while Jing Zhang was an IIAI intern mentored by Deng-Ping Fan.
Image
Depth
GT
BBSNet
Ours
Figure 1. Comparison of saliency prediction of a state-of-the-art
RGB-D saliency detection model, e.g. BBSNet [12], with ours. sensors (e.g. Microsoft Kinect) against lighting changes can also beneﬁt the saliency detection task.
As RGB and depth data capture different information about the same scene, existing RGB-D saliency detection models [35, 1, 3, 2, 53, 49, 30, 12, 19, 33, 51] focus on mod-eling the complementary information of the RGB image and depth data implicitly by using different fusion strate-gies. Three main fusion strategies have been widely studied: early fusion [38, 46], late fusion [41, 16, 36] and cross-level fusion [35, 1, 3, 2, 53, 49, 30, 12, 19, 33, 51, 25]. Although performance improvement can be achieved with effective fusion strategies, there are no constraints on the network design that force it to learn complementary information be-tween the two modalities, and we cannot explicitly evaluate the contribution of depth data in those models [52].
As a multi-modal learning task, a trained model should maximize the joint entropy of different modalities within the network capacity. Maximizing the joint entropy is also equal to the minimization of mutual information, which prevents a network from focusing on redundant informa-tion. To explicitly model the complementary information between the RGB image and depth data, we introduce a multi-stage cascaded learning framework via mutual infor-mation minimization. Speciﬁcally, we introduce mutual in-formation minimization as regularizer (as shown in Fig. 2) to achieve two main beneﬁts: 1) explicitly modeling the re-dundancy between appearance features and geometric fea-tures; 2) effectively fusing appearance features and geomet-ric features with the mutual information minimization con-straint. The produced saliency maps in Fig. 1 illustrate ef-fectiveness of our solution.
Furthermore, we ﬁnd that there is no large-scale RGB-D saliency detection training set. In Table 1 we compare the widely used RGB-D saliency datasets, in terms of the size, types of data, the sources of depth data, and their roles (for training “Tr” or for testing “Te”) in RGB-D saliency detec-tion. We note that the conventional training set for RGB-D saliency detection is a combination of samples from the
NJU2K [21] dataset and NLPR [34], which includes only 2,200 image pairs in total. Although another 800 training images from the DUT dataset [35] can serve as the third part of the training set, the total number of training images is 3,000, which is not big enough, and may lead to biased model. In addition, we observe that there are similar back-grounds in the existing RGB-D saliency training set, e.g. more than 10% of the training dataset comes from the same scene with similar illumination conditions. The lack of di-versity in the dataset may render models of poor generaliza-tion ability. At the same time, we also notice that the largest testing set [11] contains only 1,000 image pairs, which may not be enough to fully evaluate the overall performance of the deep RGB-D saliency detection models.
To provide an RGB-D saliency detection dataset for ro-bust model training, and a sufﬁcient size of testing data for model evaluation, we contribute the largest RGB-D saliency detection dataset, relabeled from Holo50K dataset [18], with 8,025 image pairs for training and 7,600 image pairs for testing. We provide not only binary annotations, but also annotations for stereoscopic saliency detection, scrib-ble and polygon annotations for weakly supervised RGB-D saliency detection, instance-level RGB-D saliency anno-tations and RGB-D saliency ranking. Moreover, we con-tribute 5,000 unlabeled training images for semi-supervised or self-supervised RGB-D saliency detection.
Our main contributions are: 1) We design a multi-stage cascaded learning framework via mutual information minimization for RGB-D saliency detection to “explicitly” model redundancy between the RGB image and depth data. 2) The mutual information minimization regularizer can be easily extend to other multi-modal learning pipelines to model the redundancy of multiple modalities. 3) We con-tribute the largest RGB-D saliency detection dataset, with a 15,625 labeled set and a 5,000 unlabeled set to achieve fully-/weakly-/un-supervised RGB-D saliency detection. 4)
We present new benchmarks for RGB-D saliency detection, and introduce baseline models for stereoscopic and weakly supervised RGB-D saliency detection. 2.