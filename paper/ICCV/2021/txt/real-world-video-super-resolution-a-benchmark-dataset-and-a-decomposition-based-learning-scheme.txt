Abstract
Video super-resolution (VSR) aims to improve the spa-tial resolution of low-resolution (LR) videos. Existing
VSR methods are mostly trained and evaluated on syn-thetic datasets, where the LR videos are uniformly down-sampled from their high-resolution (HR) counterparts by some simple operators (e.g., bicubic downsampling). Such simple synthetic degradation models, however, cannot well describe the complex degradation processes in real-world videos, and thus the trained VSR models become ineffec-tive in real-world applications. As an attempt to bridge the gap, we build a real-world video super-resolution (Re-alVSR) dataset by capturing paired LR-HR video sequences using the multi-camera system of iPhone 11 Pro Max. Since the LR-HR video pairs are captured by two separate cam-eras, there are inevitably certain misalignment and lumi-nance/color differences between them. To more robustly train the VSR model and recover more details from the LR inputs, we convert the LR-HR videos into YCbCr space and decompose the luminance channel into a Laplacian pyra-mid, and then apply different loss functions to different com-ponents. Experiments validate that VSR models trained on our RealVSR dataset demonstrate better visual quality than those trained on synthetic datasets under real-world set-tings. They also exhibit good generalization capability in cross-camera tests. The dataset and code can be found at https://github.com/IanYeung/RealVSR. 1.

Introduction
Super-resolution (SR) [5] is a classical yet challenging task in image/video processing and computer vision, aiming at reconstructing high-resolution (HR) images/videos from their low-resolution (LR) counterparts. There are two ma-jor research branches in the field of SR: single image super-resolution (SISR) [9] and video super-resolution (VSR) [1].
*Equal contribution.
â€ Corresponding author. This work is supported by the Hong Kong RGC
RIF grant (R5001-18).
LR
Vimeo-90k
RealVSR+Lv1 RealVSR+Lv2
Figure 1. Video super-resolution results on a real-world video (captured by the iPhone 11 Pro Max) by EDVR [27] trained on the synthetic Vimeo-90k datatset [31] and our RealVSR dataset.
While SISR mainly exploits the spatial redundancy within an image, VSR utilizes both spatial and temporal redundan-cies to reconstruct the HR video. With the increasing pop-ularity of mobile imaging devices and rapid development of communication technology, VSR is attracting more and more attention for its great potentials in HR video genera-tion and enhancement.
The recent progress in VSR research largely attributes to the rapid development of deep convolutional neural net-works (CNNs) [3, 31, 14, 20, 25, 27, 11], which set new state-of-the-arts on several benchmarking VSR datasets
[31, 23]. Those datasets, however, are mostly synthetic ones because it was difficult to collect real-world LR-HR video pairs. Specifically, the LR videos are obtained by uniformly downsampling their HR counterparts using some simple operators, e.g., bicubic downsampling or direct downsam-pling after Gaussian smoothing. Unfortunately, such simple degradation models could not faithfully describe the com-plex degradation processes in real-world LR videos. As a
result, VSR models trained on such synthetic datasets be-come much less effective when applied in real-world appli-cations. An example is shown in Fig. 1, where we can see that the VSR model trained on the widely used Vimeo-90k datatset [31] is less effective in super-resolving on a real-world video captured by the iPhone 11 Pro Max.
In order to remedy the above mentioned problem, it is highly desired that we can have a VSR dataset of paired
LR-HR sequences which are more consistent with the real-world degradations. Constructing such a paired dataset used to be very difficult since it requires capturing accu-rately aligned LR-HR sequences of the same dynamic scene simultaneously. Fortunately, the multi-camera system of iPhone 11 Pro series enables us to move one large step to-wards this goal. As shown in Fig. 2, there are three sep-arate cameras of different focal lengths available in iPhone 11 Pro series. Utilizing the double taking function provided by the DoubleTake app, we are able to capture two approxi-mately synchronized sequences using two of the three cam-eras. Some image registration algorithms [4] can then be employed to align the LR-HR video sequence pairs. Fig. 2 also shows an example of the LR-HR pairs before and after registration. In this way, a real-world VSR dataset, namely
RealVSR, is constructed by capturing various indoor and outdoor scenes under different illuminations. RealVSR pro-vides a worthy benchmark for training and evaluating VSR algorithms for real-world degradations.
Due to the constraints in dual camera capturing, there exists certain misalignment and luminance/color differ-ences between the LR-HR sequences even after registration.
Therefore, directly training a CNN to map the LR sequence to the HR sequence with simple losses is not a very suit-able strategy. To alleviate the influence of color difference, we disentangle the luminance and color by transforming the
RGB videos into YCbCr space, and focus on the reconstruc-tion of video details such as edges and textures. On the color channels, we adopt a gradient weighted loss [30], in-tending to pay more attention to color edge reconstruction.
To address the problem of small misalignment and lumi-nance difference in Y channel, we decompose Y channels of predicted and targeted frames into Laplacian pyramids, and apply different losses on low-frequency and high-frequency components. As shown in Fig. 1, the VSR model trained on our dataset with the proposed learning strategy reproduces much better video details with less artifacts.
The contributions of this work are twofold. First, a Re-alVSR dataset (the first of its kind to the best of our knowl-edge) is constructed to mitigate the limitations of synthetic
VSR datasets and provides a new benchmark for training and evaluating real-world VSR algorithms. Second, we pro-pose a specific training strategy on RealVSR to learn VSR model with focus on detail reconstruction. Extensive ex-periments are conducted to validate the proposed RealVSR dataset and training strategy. Although the RealVSR dataset is built with iPhone 11 Pro Max, the VSR models trained on it also exhibit good generalization capability to videos cap-tured by other mobile phone cameras. 2.