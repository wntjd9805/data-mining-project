Abstract
We study self-supervised video representation learning, which is a challenging task due to 1) lack of labels for ex-plicit supervision; 2) unstructured and noisy visual infor-mation. Existing methods mainly use contrastive loss with video clips as the instances and learn visual representation by discriminating instances from each other, but they need a careful treatment of negative pairs by either relying on large batch sizes, memory banks, extra modalities or cus-tomized mining strategies, which inevitably includes noisy data. In this paper, we observe that the consistency between positive samples is the key to learn robust video represen-tation. Specifically, we propose two tasks to learn appear-ance and speed consistency, respectively. The appearance consistency task aims to maximize the similarity between two clips of the same video with different playback speeds.
The speed consistency task aims to maximize the similarity between two clips with the same playback speed but dif-ferent appearance information. We show that optimizing the two tasks jointly consistently improves the performance on downstream tasks, e.g., action recognition and video re-trieval. Remarkably, for action recognition on the UCF-101 dataset, we achieve 90.8% accuracy without using any ex-tra modalities or negative pairs for unsupervised pretrain-ing, which outperforms the ImageNet supervised pretrained model. Codes and models will be available. 1.

Introduction
By 2022, almost 79% of the world’s mobile data traffic will be video. With the rise of cameras on smartphones,
*Co-first authorship. This work was done when Deng Huang was a research intern at Baidu VIS.
†Corresponding author.
Figure 1: An illustration of appearance-speed consis-tency. Video clips (a) and (b) come from the same video, and the appearance is consistent with different playback speeds. On the other hand, by using the same frame inter-val, we can sample the clip (c) from different videos but with the same playback speed as clip (b). We train the model to map clips to appearance and speed embedding space while maintaining the consistency. The appearance-based retrieval strategy reduces the conflict between these two objectives. recording videos has never been easier. Video analysis has become one of the most active research topics [36, 35, 37].
However, the generation of high-quality video data requires a massive human annotation effort (e.g., Kinetics-400 [20],
Youtube-8M [1]), which is expensive, time-consuming, and hard to scale up. In contrast, millions of unlabeled videos are freely available on the Internet, e.g., YouTube. Thus, learning meaningful representations from unlabeled videos is crucial for video analysis.
Self-supervised learning makes it possible to exploit a variety of labels that come with the data for free.
In-stead of collecting manual labels, the proper learning ob-jectives are set to obtain supervision from the unlabeled data themselves. These objectives, also known as pre-text tasks, roughly fall into three categories: 1) predict-ing specific transformations (e.g., rotation angle [19], play-back speed [2], and order [39]) of video clips; 2) genera-tive dense prediction, e.g., future frame prediction [13]; and 3) instance discrimination, e.g., CVRL [27] and Pace [34].
Among these methods, the playback speed prediction task has attracted more attention because 1) we can easily train a model with speed labels generated automatically from the video inputs, and 2) models will focus on the moving ob-jects to perceive the playback speed [34]. Thus, models are encouraged to learn representative motion features.
While promising results have been achieved, exist-ing methods suffer from two limitations.
First, some of the approaches rely on pre-computed motion informa-tion (e.g., optical flow [15, 33]), which is computationally heavy, particularly when the dataset is scaled up. Second, while negative samples play important roles in instance dis-crimination tasks, it is hard to maintain their quality and quantity. Moreover, same-class negative samples can be harmful [4] to the representations used in downstream tasks.
In this work, we explore the consistency between both appearance and speed of video clips from the same and dif-ferent instances and eliminate the need for negatives that are detrimental in some cases. To this end, we propose two new pretext tasks, namely, Appearance Consistency Perception (ACP) and Speed Consistency Perception (SCP). Specif-ically, for the ACP task, we sample two clips from the same video with independent data augmentations and en-courage the representations of the two clips to be close enough in feature space. Models cannot finish this task by learning low-level information, i.e., color and rotation.
Instead, models tend to learn appearance features such as background scenes and the texture of objects because these features are consistent along a video. For the SCP task, we sample two clips from two different videos with the same playback speed. Representations of these two clips are pulled closer in the feature space. Since the appearance varies from instance to instance, speed can be the crucial clue to finish this task.
Moreover, to enrich the positive samples and integrate the ACP and SCP tasks, we propose an appearance-based video retrieval strategy, which is based on the observation that appearance features in the ACP task achieve a decent accuracy (45% top-1) in the video retrieval task. Thus, we collect the video with the same speed and similar appear-ance for the SCP task and make it more compatible with the
ACP task. This strategy further improves the performance of downstream tasks with negligible computational cost.
To summarize, our contributions are as follows:
• We propose the ACP and SCP tasks for unsupervised video representation learning. In this sense, negative samples no longer affect the quality of learned repre-sentations, making the training more robust.
• We propose the appearance-based feature retrieval strategy to select the more effective positive sample for speed consistency perception. In this way, we can bridge the gap between two pretext tasks.
• We verify the effectiveness of our method for learn-ing meaningful video representations on two down-stream tasks, namely, action recognition and retrieval, on the UCF-101 [28] and HMDB51 [22] datasets. In all cases, we demonstrate state-of-the-art performance over other self-supervised methods, while our method is easier to apply in practice because we do not have to maintain the collection of negative samples. 2.