Abstract 1.

Introduction
We propose a novel weakly supervised approach for 3D semantic segmentation on volumetric images. Unlike most existing methods that require voxel-wise densely labeled training data, our weakly-supervised CIVA-Net is the ﬁrst model that only needs image-level class labels as guid-ance to learn accurate volumetric segmentation. Our model learns from cross-image co-occurrence for integral region generation, and explores inter-voxel afﬁnity relations to predict segmentation with accurate boundaries. We em-pirically validate our model on both simulated and real cryo-ET datasets. Our experiments show that CIVA-Net achieves comparable performance to the state-of-the-art models trained with stronger supervision. 1Note that we use UCSF Chimera for 3D cryo-ET image visualiza-tion. For raw 3D images, the software will choose a threshold based on expert knowledge and denote the foreground voxels by light color, and noise/background voxels by black color. For binary segmentation masks, it will denote the foreground voxels by light color, and background voxels by black color.
* Corresponding Author
Recently, there has been an increasing interest in seman-tic segmentation for 3D images [66, 11, 50, 42]. 3D se-mantic segmentation methods that rely on point-wise an-notations have been successfully developed and achieved promising performance [11, 50, 42]. However, the full segmentation methods are generally data-hungry. To al-leviate the time and labor-intensive data annotation pro-cess, weakly-supervised methods have been widely devel-oped for two popular 3D data representations: point clouds
[56, 53, 41, 40] and meshes [47, 7]. As the dominant 3D representation for biomedical images, voxel grids have not
ﬁgured prominently in these developments, especially in the area that uses image-level class labels as supervision for full semantic segmentation. Existing weakly-supervised volu-metric segmentation approaches still highly rely on the su-pervision of 2D slices [9, 13], bounding boxes [57, 62] or sparse point annotations [43].
In this paper, we introduce a weakly supervised learn-ing approach using image-level labels for 3D volumetric segmentation, with the focus on cryo-electron tomography (cryo-ET). In recent years, cryo-ET emerges as a revolu-tionary in situ 3D structural biology imaging technique for
studying macromolecular complexes and virus structures in single cells [10]. Cryo-ET captures the 3D native struc-ture and spatial distribution of all macromolecular com-plexes and other subcellular components without disrupt-ing the cell [30]. During the COVID-19 pandemic, cryo-ET serves as a powerful imaging technique to study the structures of individual viruses and their interaction with host cells [24, 36]. Nevertheless, cryo-ET data is heav-ily affected by a low signal-to-noise ratio (SNR) due to the complex cytoplasm environment and missing wedge ef-fects. Moreover, the cryo-ET based COVID-19 analysis is greatly impeded by the lack of ground truth data for model training. The ground truth masks of cryo-ET tomograms are generally obtained by template matching or human an-notation. Template matching takes about 81 days to obtain the ground truth masks of one structure on one tomogram using one CPU core. If we use human annotation, annotat-ing all structures on one tomogram takes about a month by a structural biology expert. To help the timely understanding of the virus infection, accurate semantic segmentation for 3D structures needs to be performed with fewer annotation efforts required.
Therefore, we propose a weakly-supervised 3D volumet-ric segmentation method based on image-level class labels.
In our setting, image-level labels only indicate the classes that appeared in our input samples. Consider the example in Figure 1, there are three main challenges regarding se-mantic segmentation on cryo-ET images with image-level supervision. First, the cryo-ET images suffer from severe imaging limits such as noise and missing wedge effects (See Figure 3). Such limits greatly impede robust and ac-curate 3D semantic segmentation. Second, most of the ad-vanced weakly supervised semantic segmentation (WSSS) methods on 2D images are based on class activation maps (CAM). However, the CAMs can only cover the most dis-criminative area of the object and sometimes can incorrectly activate background regions, which can be summarized as under-activation and over-activation problems. The model thus cannot predict segmentation with accurate class bound-aries. Third, the volumetric segmentation problem would be more challenging in 3D images due to the complex spatial structures, where semantic segmentation requires accurate boundary prediction.
To overcome the aforementioned challenges, we present a novel framework that utilizes both cross-image consen-sus and inter-voxel afﬁnity relations. To address the under-activation and over-activation issues brought by CAM, we utilize the cross-image consensus among the same image group (i.e. images with the same class labels) to generate more consistent and integral object regions. This design provides high-quality supervision for the segmentation net-work. To detect accurate segmentation boundaries of com-plex 3D structures with only image-level labels available, we utilize the ﬁne-grained inter-voxel afﬁnity relations for the training of the segmentation network. Our framework can yield robust segmentation as it utilizes both cross-image and inter-voxel relations. To the best of our knowledge, we are the ﬁrst to propose a 3D volumetric semantic segmen-tation model based on image-level supervision. To summa-rize, the contributions of this paper are three-fold:
• We propose a cross-image co-occurrence learning module to tackle the challenges brought by CAM and imaging limits.
• We propose an inter-voxel afﬁnity learning module to predict segmentation with accurate boundaries of com-plex 3D structures with only image-level class labels available.
• Our experiments show that our method, namely CIVA-Net, achieves comparable performance to state-of-the-art models trained with stronger supervision. 2.