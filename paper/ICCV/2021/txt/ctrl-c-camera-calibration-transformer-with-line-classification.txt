Abstract
Single image camera calibration is the task of estimat-ing the camera parameters from a single input image, such as the vanishing points, focal length, and horizon line. In this work, we propose Camera calibration TRansformer with Line-Classification (CTRL-C), an end-to-end neural network-based approach to single image camera calibra-tion, which directly estimates the camera parameters from an image and a set of line segments. Our network adopts the transformer architecture to capture the global struc-ture of an image with multi-modal inputs in an end-to-end manner. We also propose an auxiliary task of line clas-sification to train the network to extract the global geo-metric information from lines effectively. Our experiments demonstrate that CTRL-C outperforms the previous state-of-the-art methods on the Google Street View and SUN360 benchmark datasets. Code is available at https:// github. com/ jwlee-vcl/ CTRL-C. 1.

Introduction
Single image camera calibration is a task of inferring intrinsic and extrinsic camera parameters by analyzing the distortion in an input image caused by the perspective pro-jection. It is a key problem in various computer vision applications, including image rotation correction [11, 28], photo upright adjustment [21, 5], metrology [8, 42], visual aesthetics assessment [3, 28], object composition for aug-mented reality [15, 18, 19], and so on.
Single view geometry is highly related to projective ge-ometric cues such as vanishing points (VPs) and the hori-zon line, which are the intersections of the world parallel lines and planes, respectively [13, 27]. Hence, for single image camera calibration, a classical approach is first to detect line segments in an input image and then find inlier line segments that account for VPs and the horizon line us-ing RANSAC or other sampling strategies [29, 34, 21, 31].
However, this approach relies solely on lines and may de-grade when inlier lines are falsely detected.
*Corresponding author: junho@kookmin.ac.kr
Figure 1. Overview of CTRL-C. From a given image (a), CTRL-C predicts camera parameters including the zenith, FoV, and hori-zon line (b) by taking multi-modal cues; semantic ones from (a) and geometric ones from detected line segments in (c). While di-rectly regressing the camera parameters, CTRL-C classifies line segments into vertical and horizontal convergence lines, as in (d).
This auxiliary line classification task assists the network to have a better understanding of the geometric structure in the image and thus helps improve the accuracy of camera parameter prediction.
Several deep learning-based approaches have recently been proposed to overcome such limitations [37, 16, 38].
These methods directly infer the camera parameters from an input image using semantic cues learned by deep neu-ral networks. While they achieve more accurate calibration results than classical methods, their networks need to learn geometric structures from an image without any explicit su-pervision, limiting their performance. Recently, a few deep learning-based methods [41, 22] leveraging geometric and semantic cues have been proposed and achieved superior re-sults. Nonetheless, their approaches to leveraging lines are still limited to conducting post-processing [41] or building multiple networks that are trained separately [22]. More-over, all the existing neural network-based approaches rely
on convolutional neural networks (CNNs), which are less effective in capturing long-term dependencies over an im-age, and consequently, global characteristics of an image such as the camera parameters.
To address the aforementioned issues, we propose to leverage transformers [35, 9], which have been recently adopted in multiple vision tasks [12, 20]. We observe that transformers are particularly suitable for the following goals related to single image camera calibrations: i) exploiting both geometric and semantic cues, and ii) effectively learn-ing their relationships and global contextual information of an image. As transformers treat any types of input data as a sequence of tokens, both the semantic and geometric cues can be easily incorporated into a single end-to-end network.
Also, thanks to the attention mechanism, transformers can readily capture long-term dependencies across local image contexts and line segments. Most importantly, we also em-pirically demonstrate that an auxiliary task using the out-puts of the transformers can even improve the performances by facilitating the interactions between these cues.
To this end, we propose a novel neural network named
Camera calibration TRansformer with Line-Classification,
CTRL-C in short, whose pipeline is illustrated in Fig. 1. Our
CTRL-C takes both an image and line segments as input and regresses the camera parameters based on the transformer encode-decoder architecture. The input image is first fed to a ResNet [14] and converted to a set of features of local patches. The transformer encoder then processes the image features with positional encoding to generate our semantic tokens. The line segments, extracted from the input image using the LSD algorithm [36], are also mapped to geomet-ric tokens. The subsequent transformer decoder aggregates both semantic and geometric tokens along with the queries for the camera parameters — zenith VP, horizon line, and field of view (FoV) — and learns the relationships across them. As an auxiliary task, the line segments are classified into convergence lines to either the zenith or horizontal VPs, which affects to improve the performance of camera param-eter regression.
Our experimental results on the Google Street View [22] and SUN360 [39] datasets show that CTRL-C outperforms previous single image camera calibration methods [31, 37, 16, 38, 22] in multiple evaluation criteria. Even without the line classification task, our baseline transformer architec-ture already achieves competitive performance compared to the previous state-of-the-art (SotA) methods. Adopting the line classification task improves the performance, re-ducing the error of the up direction, pitch, roll, and FoV angles. Especially, CTRL-C increases the AUCs of the hori-zon line estimation with significant margins, from 83.12% to 87.29% (4.17% gap) for the Google Street View test set and from 80.85% to 85.45% (4.6% gap) for the SUN360 test set, compared to the results of the previous SotA methods. 2.