Abstract
In this paper, we propose a Frequency-Aware Spatiotem-poral Transformer (FAST) for video inpainting detection, which aims to simultaneously mine the traces of video in-painting from spatial, temporal, and frequency domains.
Unlike existing deep video inpainting detection methods that usually rely on hand-designed attention modules and memory mechanism, our proposed FAST have innate global self-attention mechanisms to capture the long-range rela-tions. While existing video inpainting methods usually ex-ploit the spatial and temporal connections in a video, our method employs a spatiotemporal transformer framework to detect the spatial connections between patches and tem-poral dependency between frames. As the inpainted videos usually lack high frequency details, our proposed FAST syn-chronously exploits the frequency domain information with a speciﬁcally designed decoder. Extensive experimental re-sults demonstrate that our approach achieves very compet-itive performance and generalizes well. 1.

Introduction
Video inpainting has attracted much attention over the past years [18, 17, 35, 23, 6], which is a task of repairing the missing or corrupter regions in a video sequence with visually plausible pixels. Video inpainting has been widely used as a video editing technique in multiple applications such as video completion and virtual reality. However, the increasing progress and rapid development of video inpaint-ing also result in enticing malicious attackers to forge video sequences to release some fake news, aiming to mislead the direction of public opinion. Recently, as a consequence of advances in deep learning, a variety of studies [15, 24] have shown spectacular progress in video inpainting, which enables editing the special area of a video, e.g. removing objects that could be key evidence. Inpainted videos have
∗ Corresponding author
Figure 1. Compared with the ﬁrst video inpainting detection method VDINet [39], we can observe that our FAST approach can preserve more detailed information of prediction masks when guaranteeing the temporal consistency. become more and more difﬁcult to be distinguished even by eyes in pace with the remarkable success in video inpainting methods. Furthermore, the misuse of video inpainting tech-niques may pose potential threats and cause legal issues in society. Therefore, there is a justiﬁed demand for effective video inpainting detection methods, which attempt to detect whether the videos presented are pristine or inpainted.
There have been a variety of studies about video inpaint-ing [2, 15, 23, 17], which fall into two main classes: patch-based methods and learning-based methods. However, there is an important problem with the above two methods – the video inpainting turns out to obtain corresponding pixels from similar regions or frames, or learn related distributions from similar scenes. Therefore, these methods inevitably leave clues and artifacts such as the inconsistency between regional pixels, sharp change at the edge of the regions and the blurred area caused by the failure to acquire complete distribution. Accordingly, more recent approaches have been developed for inpainting detection, but most existing inpainting detection methods are frame-level based on the single input image. Further, [39] ﬁrst proposed the LSTM-based framework combining both RGB image and ELA in-formation to extract both spatial and temporal features for video inpainting detection. In general, there still leaves a clear gap in the experimental performance.
In this paper, we propose to learn frequency-aware spa-tiotemporal transformers for video inpainting detection.
Actually, one of the most important things in video in-painting detection is to discover the associations between patches and frames. The exiting methods usually employ attention models and memory mechanism, where the di-rect and hard combination will lead to inconsistent pre-diction results. Motivated by this, we construct the spa-tiotemporal transformer including encoder and decoder to capture the spatial and temporal artifacts using multihead self-attention mechanism. Furthermore, we incorporate the frequency-aware features as the auxiliary supervised infor-mation into the prediction process so that the upsampling operation for prediction masks is regularized to improve the generalizability. Afterwards, we optimize the FAST frame-work guided by the hybrid loss function which is directly related to the evaluation metric and diminish the effect of class imbalance existing in the datasets. Finally, we eval-uated our framework both in-domain and cross-domain to investigate both the performance and generalization.
To summarize, the contributions of our paper are three-fold: (i) We ﬁrst introduce the transformer-based frame-work for video inpainting detection which can explore the spatial and temporal information inside the inpainted videos. (ii) We present the frequency-aware features and augment the extracted features with frequency domain in-formation to ﬁnd tamper artifacts and manipulation clues well-hidden in the RGB frames. (iii) Experimental re-sults on the Davis Video Inpainting dataset and Free-form
Video Inpainting dataset show that our proposed frame-work achieves very competitive performance even when confronted with unseen approaches. 2.