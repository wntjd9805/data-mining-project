Abstract
Group activity recognition is a crucial yet challenging problem, whose core lies in fully exploring spatial-temporal interactions among individuals and generating reasonable group representations. However, previous methods either model spatial and temporal information separately, or di-rectly aggregate individual features to form group features.
To address these issues, we propose a novel group activ-ity recognition network termed GroupFormer. It captures spatial-temporal contextual information jointly to augment the individual and group representations effectively with a clustered spatial-temporal transformer. Specifically, our
GroupFormer has three appealing advantages: (1) A tailor-modified Transformer, Clustered Spatial-Temporal Trans-former, is proposed to enhance the individual representa-tion and group representation. (2) It models the spatial and temporal dependencies integrally and utilizes decoders to build the bridge between the spatial and temporal informa-tion. (3) A clustered attention mechanism is utilized to dy-namically divide individuals into multiple clusters for bet-ter learning activity-aware semantic representations. More-over, experimental results show that the proposed frame-work outperforms state-of-the-art methods on the Volleyball dataset and Collective Activity dataset. Code is available at https://github.com/xueyee/GroupFormer 1.

Introduction
Group activity recognition is a critical studied problem due to its wide applications in surveillance systems, video analysis, and social behaviors analysis. Different from conventional action recognition, group activity recognition concentrates on understanding the scene of multiple indi-viduals. The intuitive tactic to recognize group activity is to model relevant relations between individuals and infer their
∗ indicates equal contribution.
† denotes corresponding author.
Figure 1. Examples of a clip centered at the annotated frame. The actors with ‘spiking’, ‘blocking’, ‘digging’ actions perform tem-porally, but they may perform strong spatial-temporal dependen-cies, which shows the importance of considering spatial and tem-poral interactions for reasoning about the ‘Left Spiking’ activity. collective activity. Nevertheless, exploiting individual re-lations for inferring collective activity is very challenging, especially due to the complicated variations in both spatial and temporal transition in untrimmed scenarios.
Sorts of efforts have been dedicated to capturing rela-tion context in videos for the sake of group activities in-ferring. Earlier methods [6, 25, 31, 37, 34, 15] utilized re-current neural network (RNN) to model the dynamics of the individuals, which require a large amount of computa-tional cost. More recent works [20, 47, 18, 33, 23] applied attention-based methods to model the individual relations for inferring group activity. [47] built relational graphs and considered the actor interactions in several frames.
[33] captured spatial and temporal self-attention respectively, which are added and utilized to reinforce the mean-field
[20] introduced the standard Transformer en-CRF [50]. coder as the feature extractor to selectively exploit the spa-tial actor relations without considering the temporal dynam-ical information. [23] proposed a relation learning network to model and distill the group-relevant actions and activities by two agents.
However, the aforementioned methods confront two
challenges that remain to be addressed: 1) build a bridge to model spatial-temporal contextual information integrally and 2) group individuals based on their inter-connected re-lations for better inferring the global activity context. In the former case, few of the previous methods completely con-sider the spatial and temporal dependencies in a joint model, while diverse time-series information has strong spatial de-pendencies, as illustrated in Figure 1. Therefore, captur-ing spatial-temporal dependencies jointly is critical for rea-In the latter case, full-soning about the group activity. connected relations introduced by prior methods [47, 20] is suboptimal since interference information of irrelevant individuals is introduced.
Intuitively, not all individuals’ relations in the multi-person scenario perform key impacts for the inferring of group activity. As shown in Figure 1, in the volleyball scenario, the interactions between actors with ‘spiking’ and ‘blocking’ are considerably higher than the relations between actors with ‘spiking’ and ‘standing’, which contribute more for group activity inferring. In other words, the group activity is usually determined by a critical group of individuals with underlying closet relations.
In this paper, we propose an end-to-end trainable frame-work termed GroupFormer, which utilizes a tailor-modified
Transformer to model individual and group representation for group activity recognition. Firstly, we develop a Group
Representation Generator to generate an initial group repre-sentation by merging the individual context and scene-wide context. Multiple stacked Spatial-Temporal Transformers (STT) are then deployed to augment and refine both the in-dividual and group representation. Specifically, we adopt encoders to embed the spatial and temporal features and ap-ply decoders in a cross manner to build a bridge to model the spatial and temporal contextual information integrally. And a decoder is employed to contextualize the individual rep-resentation for augmenting group representation. Besides, in contrast to the existing full-attention manner in Trans-former, our STT is further enhanced by a clustered atten-tion mechanism, referred to as Clustered Spatial-Temporal
Transformer (CSTT), to model inter-group relations and intra-group relations. More specifically, we dynamically di-vide all individuals into C clusters, where individuals in the same cluster usually have relevant semantic informa-tion. By performing information propagation within each cluster, we can generate compact action features of individ-uals. Our inter-group attention is to fully model the relations among clusters for facilitating the group activity-aware rep-resentation learning. Finally, experimental results show that the proposed network outperforms state-of-the-art methods on the widely adopted Volleyball and Collective Activity datasets. In short, the contributions of this work can be sum-marized as three folds:
• We propose a new group activity recognition frame-work, termed GroupFormer, which takes advantage of query-key mechanism to model spatial-temporal con-text jointly for group activity inferring.
• A clustered attention mechanism is introduced to as-sign individuals into groups and build inter- and intra-group relations to enrich the global activity context.
• We perform extensive experiments on the widely adopted Volleyball and Collective datasets. The results show that our GroupFormer outperforms the state-of-the-art methods by a significant margin. 2.