Abstract
The presence of objects that are confusingly similar to the tracked target, poses a fundamental challenge in appearance-based visual tracking. Such distractor objects are easily misclassified as the target itself, leading to even-tual tracking failure. While most methods strive to suppress distractors through more powerful appearance models, we take an alternative approach.
To tackle the problem of
We propose to keep track of distractor objects in or-der to continue tracking the target. To this end, we intro-duce a learned association network, allowing us to prop-agate the identities of all target candidates from frame-to-frame. lacking ground-truth correspondences between distractor objects in vi-sual tracking, we propose a training strategy that com-bines partial annotations with self-supervision. We con-duct comprehensive experimental validation and anal-ysis of our approach on several challenging datasets.
Our tracker sets a new state-of-the-art on six bench-marks, achieving an AUC score of 67.1% on LaSOT [21] and a +5.8% absolute gain on the OxUvA long-term dataset [41]. The code and trained models are available at https://github.com/visionml/pytracking 1.

Introduction
Generic visual object tracking is one of the fundamen-tal problems in computer vision. The task involves esti-mating the state of the target object in every frame of a video sequence, given only the initial target location. Most prior research has been devoted to the development of ro-bust appearance models, used for locating the target object in each frame. The two currently dominating paradigms are Siamese networks [2, 35, 34] and discriminative appear-ance modules [3, 13]. While the former employs a template matching in a learned feature space, the latter constructs an appearance model through a discriminative learning for-mulation. Although these approaches have demonstrated promising performance in recent years, they are effectively limited by the quality and discriminative power of the ap-pearance model.
Figure 1. Visualization of the proposed target candidate associa-tion network used for tracking. For each target candidate (
) we extract a set of features such as score, position and appearance in order to associate candidates across frames. The proposed target association network then allows to associate these candidates (
)
) of the with the detected distractors ( (cid:32) previous frame. Lines connecting circles represent associations. (cid:32)
) and the target object ( (cid:32) (cid:32)
As one of the most challenging factors, co-occurrence of distractor objects similar in appearance to the target is a common problem in real-world tracking applications [4, 56, 46]. Appearance-based models struggle to identify the sought target in such cases, often leading to tracking failure.
Moreover, the target object may undergo a drastic appear-ance change over time, further complicating the discrimi-nation between target and distractor objects. In certain sce-narios, e.g., as visualized in Fig. 1, it is even virtually im-possible to unambiguously identify the target solely based on appearance information. Such circumstances can only be addressed by leveraging other cues during tracking, for instance the spatial relations between objects. We therefore set out to address problematic distractors by exploring such alternative cues.
We propose to actively keep track of distractor objects in order to ensure more robust target identification. To this end, we introduce a target candidate association net-work, that matches distractor objects as well as the target across frames. Our approach consists of a base appearance tracker, from which we extract target candidates in each frame. Each candidate is encoded with a set of distinctive features, consisting of the target classifier score, location, and appearance. The encodings of all candidates are jointly
processed by a graph-based candidate embedding network.
From the resulting embeddings, we compute the association scores between all candidates in subsequent frames, allow-ing us to keep track of the target and distractor objects over time. In addition, we estimate a target detection confidence, used to increase the robustness of the target classifier.
While associating target candidates over time provides a powerful cue, learning such a matching network requires tackling a few key challenges. In particular, generic visual object tracking datasets only provide annotations of one ob-ject in each frame, i.e., the target. As a result, there is a lack of ground-truth annotations for associating distractors across frames. Moreover, the definition of a distractor is not universal and depends on the properties of the employed ap-pearance model. We address these challenges by introduc-ing an approach that allows our candidate matching network to learn from real tracker output. Our approach exploits the single target annotations in existing tracking datasets in combination with a self-supervised strategy. Furthermore, we actively mine the training dataset in order to retrieve rare and challenging cases, where the use of distractor associa-tion is important, in order to learn a more effective model.
Contributions:
In summary, our contributions are as fol-lows: (i) We propose a method for target candidate associ-ation based on a learnable candidate matching network. (ii)
We develop an online object association method in order to propagate distractors and the target over time and intro-duce a sample confidence score to update the target clas-sifier more effectively during inference. (iii) We tackle the challenges with incomplete annotation by employing partial supervision, self-supervised learning, and sample-mining to (iv) We perform compre-train our association network. hensive experiments and ablative analyses by integrating our approach into the tracker SuperDiMP [11, 16, 3]. The resulting tracker KeepTrack sets a new state-of-the-art on six tracking datasets, obtaining an AUC of 67.1% on La-SOT [21] and 69.7% on UAV123 [37]. 2.