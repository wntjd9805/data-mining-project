Abstract
Multi-label image recognition is a challenging computer vision task of practical use. Progresses in this area, how-ever, are often characterized by complicated methods, heavy computations, and lack of intuitive explanations. To ef-fectively capture different spatial regions occupied by ob-jects from different categories, we propose an embarrass-ingly simple module, named class-specific residual atten-tion (CSRA). CSRA generates class-specific features for ev-ery category by proposing a simple spatial attention score, and then combines it with the class-agnostic average pool-ing feature. CSRA achieves state-of-the-art results on multi-label recognition, and at the same time is much simpler than them. Furthermore, with only 4 lines of code, CSRA also leads to consistent improvement across many diverse pretrained models and datasets without any extra training.
CSRA is both easy to implement and light in computations, which also enjoys intuitive explanations and visualizations. 1.

Introduction
Convolutional neural networks (CNNs) have dominated many computer vision tasks, especially in image classi-fication. However, although many network architectures have been proposed for single-label classification, e.g.,
VGG [27], ResNet [15], EfficientNet [29] and VIT [7], the progress in multi-label recognition remains modest. In multi-label tasks, the objects’ locations and sizes vary a lot and it is difficult to learn a single deep representation that fits all of them.
Recent studies in multi-label recognition mainly focus on three aspects: semantic relations among labels, object proposals, and attention mechanisms. To explore seman-tic relations, Bayesian network [14, 18], Recurrent Neural
Network (RNN) [32, 30] and Graph Convolutional Network
*This research was partly supported by the National Natural Science
Foundation of China under Grant 61772256 and 61921006.
# x: feature tensor, output of CNN backbone
# x’s size: (B, d, H, W)
# y_raw: by applying classifier (’FC’) to ’x’
# y_raw’s size: (B, C, HxW)
# C: number of classes y_raw = FC(x).flatten(2) y_avg = torch.mean(y_raw, dim=2) y_max = torch.max(y_raw, dim=2)[0] score = y_avg + Lambda * y_max
Figure 1. A simple modification in the testing stage using PyTorch, in which Lambda (or λ) is a hyperparameter that combines global average and max pooling scores. When Lambda (λ) is 0, score equals y_avg, which is the score of the baseline model. (GCN) [4, 3] have been adopted, but they suffer from high computational cost or manually defined adjacency matrices.
Proposal based methods [33, 31, 19] spend too much time in processing the object proposals. Although attention models are end-to-end and relatively simple, for multi-label clas-sification they resort to complicated spatial attention mod-els [26, 39, 11], which are difficult to optimize, implement, or interpret. Instead, we propose an embarrassingly simple and easy to train class-specific residual attention (CSRA) module to fully utilize the spatial attention for each object class separately, and achieves superior accuracy. The CSRA module has negligible computational cost, too.
Our motivation came from Fig. 1, in which only 4 lines of code consistently leads to improvement of multi-label recognition, across many diverse pretrained models and datasets, even without any extra training, as detailed in Ta-ble 1. The only change is to add a global max-pooling on top of the usual global average pooling, but the improve-ment is consistent. Its advantage is also verified on Ima-geNet, a single-label recognition task.
In this paper, we show that this operation, the max pool-ing among different spatial regions for every class, is in fact a class-specific attention operation, which can be further viewed as a residual component of the class-agnostic global average pooling. Hence, we generalize it to propose a sim-ple class-specific residual attention module (CSRA), and has achieved state-of-the-art performance on four multi-Table 1. An embarrassingly simple, almost zero-cost, and training-free improvement to a diverse set of existing models on 3 multi-label recognition datasets and 1 single-label recognition dataset.
Datasets & resolutions
Models
Baseline mAP/acc
Varying
λ mAP / acc mAP / acc difference
Fixed
λ mAP / acc mAP / acc difference
MS-COCO 448×448
VOC2007 448×448
WIDER 224×224
ImageNet
MobileNet [25]
EfficientNet [29]
VIT-B16-224 [7]
VIT-L16-224 [7]
ResNet-cut [15]
GCN [4]
MobileNet [25]
ResNet-cut [15]
GCN [4]
MobileNet [25]
VIT-B16-224 [7]
VIT-L16-224 [7]
ResNet-50 [15]
ResNet-101 [15]
VIT-B16-224 [7]
VIT-B16-384 [7] 71.8 75.6 79.0 80.4 82.4 83.0 89.6 93.9 94.0 72.1 86.3 87.7 75.6 77.1 80.5 83.5 0.2 0.1 0.8 0.4 0.05 0.2 0.2 0.05 0.2 0.1 0.3 0.2 0.03 0.02 0.2 0.3 73.0 76.1 79.7 80.6 82.6 83.2 90.3 94.0 94.1 72.3 86.4 87.8 75.7 77.2 80.7 83.6 1.2 ↑ 0.5 ↑ 0.7 ↑ 0.2 ↑ 0.2 ↑ 0.2 ↑ 0.7 ↑ 0.1 ↑ 0.1 ↑ 0.2 ↑ 0.1 ↑ 0.1 ↑ 0.1 ↑ 0.1 ↑ 0.2 ↑ 0.1 ↑ 0.2 0.2 0.2 0.2 0.02 0.2 0.2 0.02 0.2 0.2 0.2 0.2 0.02 0.02 0.2 0.2 73.0 76.1 79.3 80.6 82.5 83.2 90.3 94.0 94.1 72.2 86.4 87.8 75.7 77.2 80.7 83.6 1.2 ↑ 0.5 ↑ 0.3 ↑ 0.2 ↑ 0.1 ↑ 0.2 ↑ 0.7 ↑ 0.1 ↑ 0.1 ↑ 0.1 ↑ 0.1 ↑ 0.1 ↑ 0.1 ↑ 0.1 ↑ 0.2 ↑ 0.1 ↑ label datasets, namely VOC2007 [9], VOC2012 [10], MS-COCO [20] and WIDER-Attribute [19]. Furthermore, the proposed CSRA has an intuitive explanation on how spatial attention is integrated into it.
Our contributions can be summarized as:
• An extremely simple but effective method to improve pretrained models without any further training;
• A simple and effective CSRA module that achieves ex-cellent results on four multi-label recognition datasets;
• An intuitive interpretation of the proposed attention module (plus visualizations). 2.