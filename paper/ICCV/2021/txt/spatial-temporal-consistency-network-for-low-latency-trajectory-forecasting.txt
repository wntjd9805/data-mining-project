Abstract
Trajectory forecasting is a crucial step for autonomous vehicles and mobile robots in order to navigate and in-teract safely.
In order to handle the spatial interactions between objects, graph-based approaches have been pro-posed. These methods, however, model motion on a frame-to-frame basis and do not provide a strong temporal model.
To overcome this limitation, we propose a compact model called Spatial-Temporal Consistency Network (STC-Net). In
STC-Net, dilated temporal convolutions are introduced to model long-range dependencies along each trajectory for better temporal modeling while graph convolutions are em-ployed to model the spatial interaction among different tra-jectories. Furthermore, we propose a feature-wise convolu-tion to generate the predicted trajectories in one pass and re-fine the forecast trajectories together with the reconstructed observed trajectories. We demonstrate that STC-Net gen-erates spatially and temporally consistent trajectories and outperforms other graph-based methods. Since STC-Net re-quires only 0.7k parameters and forecasts the future with a latency of only 1.3ms, it advances the state-of-the-art and satisfies the requirements for realistic applications. 1.

Introduction
Intelligent agents like autonomous vehicles and mobile robots navigate and interact in spaces that are shared with humans. The safety of the humans is therefore of the high-est priority. To this end, agents are required to understand and forecast the trajectories of surrounding pedestrians or vehicles such that they can make the right decisions and for instance navigate safely and smoothly through a crowd. Tra-jectory forecasting, however, is challenging due to the com-plex behavior and interactions of humans in crowds. Fur-thermore, the available resources will always be a limiting factor as the agents are equipped with energy-efficient hard-ware. Hence, there is a need for compact forecasting mod-els with a very low latency.
In practice, a low latency is required for most applications since there is otherwise not enough time to adjust the motion and any millisecond can save lives. (a) Social-STGCNN [24] (b) Ours
Figure 1: State-of-the-art graph-based approaches like [24] do not model the temporal motion of the trajectories well, which results in shaky and unrealistic trajectories (dashed yellow) as shown in (a). In this figure, red denotes the ob-served trajectory and blue the ground-truth future trajectory.
Our approach addresses this issue and forecasts spatially and temporally consistent trajectories (b).
Over the last years, several approaches have been pro-posed for trajectory forecasting. RNN-based methods [1, 20, 36] utilize a recurrent model to model the trajectory of each pedestrian in the scene and their interactions are taken into account by a pooling operation. GAN-based meth-ods [13, 17, 29] enable RNN-based methods to produce multiple socially plausible trajectories. Graph-based meth-ods [15, 34, 24] model spatial relations as a graph and uti-lize graph convolutions. These methods, however, have in common that they use recurrent neural networks to generate the future trajectories, which results in a relatively high la-tency. To address this issue, Social-STGCNN [24] proposed a CNN for time-extrapolation. As a result, Social-STGCNN achieves a very low latency of 2ms. This, however, comes at the cost that the forecast trajectories are noisy and not tem-porally consistent as shown in Fig. 1a.
In this work, we therefore address this issue and present an approach that generates spatially and temporally consis-tent trajectories with a latency of less than 2ms. To achieve this, the proposed Spatial-Temporal Consistency Network (STC-Net), which is shown in Fig. 2, utilizes graph convolu-tions for spatial modeling and dilated temporal convolutions for temporal modeling. Our network generates trajectories that are consistent over the past and the future. This con-sistency is implicitly enforced by reconstructing the past, jointly refining the reconstructed past and forecast future, and computing the loss over the entire trajectory. In order
Figure 2: The proposed spatial-temporal consistency network takes observed trajectories (solid curves) of ùëÅ objects as input and forecasts the future trajectories (dashed curves). It combines graph convolutions (GC) for modeling interactions of trajec-tories in close proximity and dilated temporal convolutions (DTCs) for capturing temporal relations over the entire trajectories.
The feature-wise convolution (FWC) forecasts the features in one pass and the final refinement ensures the consistency of the reconstructed and forecast part of a trajectory. to achieve a very low latency, we do not use any recurrent layers, but propose feature-wise convolutions that generate the future trajectories with variable length in one pass. The proposed network design yields very compact networks with only 0.7k parameters and 1.3ms latency.
We thoroughly evaluate the approach on three datasets with six different scenes and analyze its generalization abil-ity by performing a cross-dataset experiment and evaluating the approach for different temporal sampling rates. The pro-posed approach outperforms other graph-based methods in terms of accuracy, model size, and latency. It is also im-portant to note that most approaches cannot keep up with the frame-rate, i.e., the forecasting cannot be performed un-til the next frame is captured. This means that these meth-ods are not fast enough for real-world applications. Even for a very slow frame-rate of 2.5 frames per second, only
[13, 25, 24] are able to process the data fast enough. Com-pared to these methods, our approach is not only faster, but it is also by far more accurate. 2.