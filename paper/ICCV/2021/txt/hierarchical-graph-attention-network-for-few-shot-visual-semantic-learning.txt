Abstract
Deep learning has made tremendous success in com-puter vision, natural language processing and even visual-semantic learning, which requires a huge amount of la-beled training data. Nevertheless, the goal of human-level intelligence is to enable a model to quickly obtain an in-depth understanding given a small number of sam-ples, especially with heterogeneity in the multi-modal sce-narios such as visual question answering and image cap-tioning. In this paper, we study the few-shot visual-semantic learning and present the Hierarchical Graph ATtention net-work (HGAT). This two-stage network models the intra- and inter-modal relationships with limited image-text samples.
The main contributions of HGAT can be summarized as follows: 1) it sheds light on tackling few-shot multi-modal learning problems, which focuses primarily, but not exclu-sively on visual and semantic modalities, through better exploitation of the intra-relationship of each modality and an attention-based co-learning framework between modal-ities using a hierarchical graph-based architecture; 2) it achieves superior performance on both visual question an-swering and image captioning in the few-shot setting; 3) it can be easily extended to the semi-supervised setting where image-text samples are partially unlabeled. We show via extensive experiments that HGAT delivers state-of-the-art performance on three widely-used benchmarks of two visual-semantic learning tasks. 1.

Introduction
Recently, signiﬁcant progress has been made to various applications in single modality, such as object detection and machine translation, thanks to the advancing deep learning technologies [15] including convolutional [26] and recur-rent [18] neural networks. However, in order for an artiﬁ-cial intelligence (AI) system to understand the real world around us, it requires not only the ability to memorize the rich information contained in a single modality, such as vi-sual signals (i.e., images and videos) and natural language
*Corresponding author (i.e., captions and questions), but also a joint comprehen-sion in multiple modalities. This, in general, is called multi-modal learning [5], and one typical example is the visual-semantic learning. For instance, a smart cooking robot in the kitchen is expected to make delicious dishes through un-derstanding the recipe instructions as well as detecting and selecting the right ingredients on the table. The robot can hardly perform this task without the ability of either reading the texts or seeing the objects.
Lots of methods have attempted to address the multi-learning problems through dealing with visual-modal semantic learning tasks, such as visual question answer-ing [32, 56, 54] and image captioning [10, 49, 12]. While these models are capable when massive human-annotated data and extensive training time are available, a real AI sys-tem should be able to quickly deliver an in-depth under-standing using a small number of learning samples. The ability of solving visual-semantic tasks with limited sam-ples, termed as few-shot visual-semantic learning, turns out to be challenging and critical for human-level intelligence.
Currently, for general few-shot learning problems, meta-learning [29, 36, 52] has become a standard methodol-ogy. Based on it, a few extensions have been recently made for few-shot visual-semantic learning. Fast Parameter
Adaptation for Image-Text Modeling (FPAIT) [11] directly applied Model-Agnostic Meta-Learning (MAML) [13], a well-known meta-learning algorithm, to the few-shot visual question answering and image captioning. Analogously, an-other work [46] adopted a question answering model with two meta-learning techniques, prototypical networks [42] and meta networks [35]. Nonetheless, these attempts left much to be desired in terms of their scope and performance.
Firstly and fundamentally, all these methods merely applied existing meta-learning algorithms without explicitly consid-ering the multi-modal nature, to which we paid careful at-tention in this work. For example, Teney et al. [46] obtained their model input through a simple element-wise produc-tion between visual and semantic representations. Addi-tionally, neither of them deal with the cases where labels are partially unlabeled, which is categorized as the semi-supervised learning setting. As labeling data can be ex-pensive or even infeasible, semi-supervised learning is very
Figure 1. Image-text samples for image captioning (left) and visual question answering (right). common and of great value in practice, and it becomes more severe when together with the few-shot setting.
For visual-semantic learning involving multiple modal-ities, especially the cases with limited and partially unla-beled image-text samples, it is vital to fully exploit the potential visual-semantic relationships, such as the intra-relationship of each modality (i.e., intra-modal relation-ships) and the inter-relationship between different modal-ities (i.e., inter-modal relationships). While the intra-modal learning have been examined meticulously such as Multi-modal DBMs [43], the inter-modal learning leaves more space to explore and endows the model capacities to atten-tively capture complementary information.
We take two examples in Figure 1 for illustration. For the left-hand side image captioning example, both images con-tain female tennis players with white outﬁt. Correspond-ingly, the two ground-truth captions share the words “fe-male”, “white”, and “tennis”. In this example, even some words in the query caption are missing, the potential inter-modal relationship and the information captured from the visual modality can be used to supplement and strengthen the semantic modality and complete the caption. For the right-hand side visual question answering example, both the left two images contain a computer on the desk, quite differ-ent from the third image. Therefore, the predicted answer of the middle image is likely to be fooled (e.g., computer) by the visual similarity solely. Only if inter-relationship is captured through exploiting modal mutual interactions, the right visual clue can be distinguished from the distractors with the help of the semantic information and lead to the correct answer (i.e., white). While models based on Graph
Neural Networks (GNNs) [16, 41] have been used to cap-ture relational structures [48, 14, 22] in few-shot learning, they are incapable of jointly and subtly exploiting the intra-and inter-modal relationship for few-shot visual-semantic learning.
In this paper, to deal with the few-shot visual-semantic learning tasks, we propose the Hierarchical Graph
ATtention network (HGAT). This two-stage network is able to model the intra-modal relationships and the inter-modal relationships with a few image-text samples and can be
In the ﬁrst stage, extended to a semi-supervised setting. visual-speciﬁc and semantic-speciﬁc GNNs are leveraged to model the intra-relationship of images and texts (i.e., visual-speciﬁc relationships and semantic-speciﬁc relationships), respectively. To model the inter-relationship between the visual and semantic modalities, an attention-based co-learning framework is presented to guide the node feature update of these GNNs. In the second stage, relation-aware
GNNs are used to predict the result of the query sample by jointly learning visual representations, semantic represen-tations, visual-speciﬁc relationships and semantic-speciﬁc relationships. We perform extensive experiments on three widely-used benchmarks, Toronto COCO-QA [40], Visual
Genome-QA [25] and COCO-FITB [11], which showed that HGAT is a strong and effective model customized for few-shot visual-semantic learning.
The superiority of our proposed method can be sum-marized as follows: First, it sheds light on tackling few-shot multi-modal learning problems, especially for few-shot visual-semantic learning, a fairly new but critical set-ting for human-level intelligence, through taking advantage of the intra- and inter-modal relationships. Second, com-pared with FPAIT and several few-shot learning methods, it delivers state-of-the-art performance in terms of accu-racy on both visual question answering and image caption-ing in the few-shot setting.
In addition, several ablation experiments show the beneﬁts of modeling of the visual-speciﬁc and semantic-speciﬁc relationships, the attention-based co-learning framework and the hierarchical graph-based architecture. Finally, it can be easily extended to the semi-supervised setting and delivers better performance compared with the other two graph-based methods. 2.