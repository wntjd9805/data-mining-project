Abstract
This paper aims to explain adversarial attacks in terms of how adversarial perturbations contribute to the attack-ing task. We estimate attributions of different image re-gions to the decrease of the attacking cost based on the
Shapley value. We deﬁne and quantify interactions among adversarial perturbation pixels, and decompose the en-tire perturbation map into relatively independent pertur-bation components. The decomposition of the perturba-tion map shows that adversarially-trained DNNs have more perturbation components in the foreground than normally-trained DNNs. Moreover, compared to the normally-trained
DNN, the adversarially-trained DNN have more compo-nents which mainly decrease the score of the true category.
Above analyses provide new insights into the understanding of adversarial attacks. 1.

Introduction
Deep neural networks (DNNs) have shown promise in various tasks, such as image classiﬁcation [36] and speech recognition [14]. Adversarial robustness of DNNs has re-ceived increasing attention in recent years. Previous stud-ies mainly focused on attacking algorithms [47, 3, 27], the detection of adversarial examples for the adversarial de-fense [28, 11, 25], and adversarial training to learn DNNs robust to adversarial attacks [13, 27].
Unlike previous studies of designing more powerful at-tacks or learning more robust DNNs, in this research, we aim to explain the signal-processing behavior behind the adversarial attack, i.e. how pixel-wise perturbations coop-erate with each other to achieve the attack. We develop new methods to explain adversarial attacks from the following perspectives. 1. Given an input image, the regional attribution to the adversarial attack is computed to diagnose the impor-tance of each image region to the decrease of the attacking
*Equal contribution
†Quanshi Zhang is the corresponding author. He is with the John
Hopcroft Center and the MoE Key Lab of Artiﬁcial Intelligence, AI In-stitute, at the Shanghai Jiao Tong University, China.
Figure 1. (a) Regional attributions to the adversarial attack. Re-gions with high attributions are important for the decrease of the attacking cost. (b2) Perturbation pixels A and B interact with each other and form a curve to conduct the adversarial attack; (b3) the entire perturbation can be decomposed into several components.
Perturbation pixels within each component have strong interac-tions, whereas perturbation pixels between different components have relatively weak interactions. cost, i.e. the Lp norm of the adversarial perturbation. As
Fig. 1 (a2) shows, regions of the bird’s head and neck have high attributions to the adversarial attack. If these two re-gions are not allowed to be perturbed, then magnitudes of adversarial perturbations in other regions would be signif-icantly increased for attacking. In this way, the attacking cost may increase signiﬁcantly.
The regional attribution (importance) provides a new perspective to understand adversarial attacks. We compute such regional attributions as Shapley values w.r.t. the at-tacking cost. 2. Pixel-wise interactions & perturbation compo-nents in the adversarial attack: Given a perturbation map of the input image, we further deﬁne and quantify inter-actions among pixel-wise perturbations in the perturbation map, termed perturbation pixels. I.e. we aim to explore how perturbation pixels cooperate to achieve the attack. Accord-ing to [45], the adversarial power of a single pixel mainly depends on the context around the pixel, rather than rely on each perturbation pixel independently. For instance, in
Fig. 1 (b2), perturbation pixels A and B do not directly con-tribute to the attack. Instead, they interact with each other to form a curve to fool the DNN.
The interaction among perturbation pixels can be deﬁned based on the game theory. Given a DNN g trained for clas-siﬁcation and an adversarial image x(cid:48) = x + δ ∈ Rn, y = g(x(cid:48)) ∈ R denotes the scalar output of the DNN (or one dimension of the vectorized network output). Let φi denote the importance (attribution) of the i-th perturbation pixel of
δ w.r.t. the output y, which is implemented as the Shapley value. The attribution values of all perturbation pixels sat-isfy g(x(cid:48)) − g(x) = (cid:80)n i=1 φi. Let φS denote the overall im-portance of all perturbation pixels in S, when perturbation pixels in S collaborate with each other. Then, the interac-tion is deﬁned as the change of the importance of S, when we ignore the collaboration between perturbation pixels and simply sum up the importance of each individual-working perturbation pixel in S, i.e. φS − (cid:80) i∈S φi quantiﬁes the in-teraction. If φS − (cid:80) i∈S φi > 0, it indicates that perturbation pixels in S cooperate with each other, and exhibit positive interactions. If φS − (cid:80) i∈S φi < 0, it indicates that perturba-tion pixels in S conﬂict with each other, and exhibit negative interactions.
Furthermore, based on the pixel-wise interactions among perturbation pixels, as Fig. 1 (b3) shows, we can decompose the effect of the adversarial attack into several perturbation components, which provides a new perspective to analyze how perturbation pixels cooperate with each other. To this end, we develop a method to extract groups of perturbation pixels with strong interactions as perturbation components.
Perturbation pixels in the same component have strong in-teractions with each other. Whereas, perturbation pixels in different components have relatively weak interactions.
Using the Shapely value for explanation and its ad-vantages: We deﬁne the regional attribution and inter-actions among perturbation pixels based on the Shapley value [39]. Though explanation methods in previous stud-ies, such as Grad-CAM [38] and GBP [44], can measure the importance of input elements, the Shapley value is proved to be the unique attribution satisfying four desirable prop-erties, i.e. the linearity property, the dummy property, the symmetry property, and the efﬁciency property [29]. The four properties can be considered the solid theoretic sup-port for the Shapley value. The scientiﬁc rigor of the Shap-ley value makes the attribution analysis and the interaction deﬁned on the Shapley value more trustworthy than other explanation methods. Please see Section A in supplemen-tary materials for more discussion.
We have analyzed regional attributions and pixel-wise interactions on different DNNs. The analysis of regional attributions has demonstrated that important image regions for the L2 attack and those for the L∞ attack were simi-lar, although L2 perturbations and L∞ perturbations were signiﬁcantly dissimilar. Furthermore, our research has pro-vided new insights into adversarial perturbations by inves-tigating the property of perturbation components.
• Our research has provided a game-theoretic view to ex-plain and verify the phenomenon that adversarailly-trained
DNNs mainly focus on foreground. For adversarially-trained DNNs, adversarial perturbations are more likely to interact with each other on the foreground.
• Moreover, the adversarial-trained DNN usually had more components punishing the true category and less compo-nents encouraging incorrect categories than the normally-trained DNN.
In fact, our research group led by Dr. Quanshi Zhang has proposed game-theoretic interactions, including inter-actions of different orders [57] and multivariate interac-tions [59]. As a basic metric, the interaction can be used to learn baseline values of Shapley values [35] and to explain signal processing in trained DNNs from different perspec-tives. For example, we have used interactions to build up a tree to explain the hierarchical interactions between words in NLP models [56] and to explain the generalization power of DNNs [58]. The interaction can also explain adversarial transferability [53] and adversarial robustness [34]. As an extension of the system of game-theoretic interactions, in this study, we interpret attributions and interactions of ad-versarial attacks.
However, the computational cost of the Shapley value is NP-hard, which makes the decomposition of perturba-tion components is also NP-hard. Thus, we develop an ef-ﬁcient approximation approach to the decomposition prob-lem. Our method has been applied to DNNs with various architectures for different tasks. Preliminary experiments have demonstrated the effectiveness of our method.
Contributions: This study provides new perspectives to understand adversarial attacks, which includes the regional attribution to the adversarial attack and the extraction of per-turbation components. We have applied our methods to var-ious benchmark DNNs and datasets. Experimental results provide an insightful understanding of adversarial attacks. 2.