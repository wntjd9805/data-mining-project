Abstract
Weakly supervised object localization (WSOL) is a challenging problem when given image category labels but requires to learn object localization models. Optimizing a convolutional neural network (CNN) for classiﬁcation tends to activate local discriminative regions while ignoring complete object extent, causing the partial activation issue.
In this paper, we argue that partial activation is caused by the intrinsic characteristics of CNN, where the convolution operations produce local receptive ﬁelds and experience difﬁculty to capture long-range feature dependency among pixels. We introduce the token semantic coupled attention map (TS-CAM) to take full advantage of the self-attention mechanism in visual transformer for long-range dependency extraction. TS-CAM ﬁrst splits an image into a sequence of patch tokens for spatial embedding, which produce attention maps of long-range visual dependency to avoid partial activation. TS-CAM then re-allocates category-related semantics for patch tokens, enabling each of them to be aware of object categories. TS-CAM ﬁnally couples the patch tokens with the semantic-agnostic attention map to achieve semantic-aware localization. Experiments on the ILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM counterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance. Code is available at https://github.com/vasgaowei/TS-CAM 1.

Introduction
Weakly supervised learning refers to methods that utilize training data with incomplete annotations to learn recogni-tion models. Weakly supervised object localization (WSOL)
*Corresponding authors
Figure 1. Comparison of weakly supervised object localization results. (a) Input Image. (b) Class Activation Map (CAM). (c)
TransAttention: Transformer-based Attention. (d) TS-CAM. Object localization boxes are in red. (Best viewed in color) solely requires the image-level annotations indicating the presence or absence of a class of objects in images to learn localization models [23, 24, 28, 46]. WSOL has attracted increasing attention as it can leverage the rich Web images with tags to learn object-level models [46].
As the cornerstone of WSOL [7], the Class Activation
Mapping (CAM) [55] utilizes the activation map from the last convolution layer to generate semantic-aware localiza-tion maps for object bounding-box estimation. However,
CAM suffers from severe underestimation of object regions because the discriminative regions activated through the classiﬁcation models are often much smaller than objects’ actual extent [2]. Local discriminative regions are capable of minimizing image classiﬁcation loss, but experience
Figure 2. Comparison of the mechanisms of (a) CNN-based CAM, (b) Transformer-based Attention and (c) the proposed TS-CAM.
The CNN-based CAM method is limited by the small-range feature dependency and the transformer-based attention is limited by the semantic-anostic issue. TS-CAM is able to produce semantic coupled attention maps for complete object localization. (Best viewed in color) difﬁculty for accurate object localization [46], Fig. 1(b).
Much effort has been made to solve this problem by proposing various regularizations [50, 51, 46, 20, 21, 6], divergent activation [33, 46, 48] or adversarial training [8, 21, 43, 50, 48, 33]. However, there is very little work to pay attention to fundamentally solving the inherent defects of
CNN’s local representation, Fig. 2(a). Capturing the long-range feature dependency, which can be interpreted as the semantic correlation between features in different spatial locations, is critical for WSOL.
Recently, visual transformer has been introduced to the computer vision area. Visual transformer [10] constructs a sequence of tokens by splitting an input image into patches with positional embedding and applying cascaded transformer blocks to extract visual representation. Thanks to the self-attention mechanism and Multilayer Perceptron (MLP) structure, visual transformers can learn complex spatial transforms and reﬂect long-range semantic corre-lations adaptively, which is crucial for localizing full object extent, Fig. 1(d). Nevertheless, visual transformer cannot be directly mitigated to WSOL for the following two reasons: (1) When using patch embeddings, the spatial topology of the input image is destroyed, which hinders the generation of activation maps for object localization. (2) The attention maps of visual transformers are semantic-agnostic (not distinguishable to object classes) and are not competent to semantic-aware localization, Fig. 2(b).
In this study, we propose the token semantic coupled at-tention map (TS-CAM), making the ﬁrst attempt for weakly supervised object localization with visual transformer. TS-CAM introduces a semantic coupling structure with two network branches, Fig. 2(c), one performs semantic re-allocation using the patch tokens and the other generates semantic-agnostic attention map upon the class tokens.
Semantic re-allocation, with class-patch semantic activation, enables the patch tokens to be aware of object categories.
The semantic-agnostic attention map aims to capture long-distance feature dependency between patch tokens by taking the advantages of the cascaded self-attention modules in transformer. TS-CAM ﬁnally couples the semantic-aware maps with the semantic-agnostic attention map for object localization, Fig. 2(c).
The contributions of this work are as follows:
• We propose the token semantic coupled attention map (TS-CAM), as the ﬁrst solid baseline for WSOL using visual transformer by leveraging the long-range feature dependency.
• We propose the semantic coupling module to combine the semantic-aware tokens with the semantic-agnostic attention map, providing a feasible way to leverage
both semantics and positioning information extracted by visual transformer for object localization.
• TS-CAM achieves a substantial improvement over pre-vious methods on two challenging WSOL benchmarks, fully exploiting the long-range feature dependency in the visual transformer. 2.