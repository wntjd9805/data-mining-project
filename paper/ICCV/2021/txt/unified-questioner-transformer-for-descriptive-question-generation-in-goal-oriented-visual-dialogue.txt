Abstract
Building an interactive artificial intelligence that can ask questions about the real world is one of the biggest
In partic-challenges for vision and language problems. ular, goal-oriented visual dialogue, where the aim of the agent is to seek information by asking questions during a turn-taking dialogue, has been gaining scholarly atten-tion recently. While several existing models based on the
GuessWhat?! dataset [10] have been proposed, the Ques-tioner typically asks simple category-based questions or ab-solute spatial questions. This might be problematic for com-plex scenes where the objects share attributes, or in cases where descriptive questions are required to distinguish ob-jects. In this paper, we propose a novel Questioner architec-ture, called Unified Questioner Transformer (UniQer), for descriptive question generation with referring expressions.
In addition, we build a goal-oriented visual dialogue task called CLEVR Ask. It synthesizes complex scenes that re-quire the Questioner to generate descriptive questions. We train our model with two variants of CLEVR Ask datasets.
The results of the quantitative and qualitative evaluations show that UniQer outperforms the baseline. 1.

Introduction
Information seeking through interaction is one of the most vital abilities for artificial intelligence. This is partic-ularly true in the human-agent interaction scenario [24, 17].
For example, task-oriented agents need to understand what the users are thinking, i.e., beliefs, preferences, and inten-tions, in order to correctly interpret their instructions [14, 39]. In most cases, such information is not provided prior to the interaction, so the agents have to elicit it by asking questions on the fly.
To build effective information-seeking agents, several studies in the vision and language community have tackled
*Corresponding Author: shoya@ailab.ics.keio.ac.jp
†Equally Contributed
Figure 1: UniQer and CLEVR Ask task. UniQer unifies the
Question Generator (QGen), which generates yes-no ques-tions, and the Guesser, which guesses an Oracle’s reference object, into a single transformer encoder-decoder architec-ture. The Object Targeting Module is introduced to assign target objects which are to be set apart from distracter ob-jects in the QGen. the goal-oriented visual dialogue task [10, 28]. This task consists of two agents, called a Questioner and an Oracle, and the goal is to train the Questioner to guess the Ora-cle’s reference object by asking yes-no questions during a turn-taking dialogue. The Oracle then needs to provide an answer given the question and the target object.
In goal-oriented visual dialogue, deciding which objects and how to address will depend on the complexity of the presented image. For example, in simple scenes, where each object has different attributes and thus is easy to dis-tinguish, the Questioner only needs to ask category-based questions such as “Is it a car?”. In a more complex scene, on the other hand, the Questioner needs to ask descriptive questions with referring expressions [7, 31] to narrow down
the candidates, such as “Is it behind a tree?” or “Is it a red car?”. In this paper, we particularly focus on building an agent that can facilitate such descriptive questions in goal-oriented visual dialogue.
Several models based on GuessWhat?! [10] have been published, most of which leveraged reinforcement learn-ing aimed at maximizing the success reward by generating a word token as an action [28, 35, 37, 26, 27, 2, 22, 1].
However, the Questioners in these models typically gener-ate only simple category-based questions, such as “Is it a person?” or “Is it a computer?”, or absolute spatial ques-tions, such as “Is it in front?” or “Is it on the left side of the image?”, which is not effective when a large number of similar-looking candidates is presented in the same place.
In response to the above issues, we propose Unified
Questioner Transformer (UniQer) and a task called CLEVR
Ask for descriptive question generation in goal-oriented vi-sual dialogue (Fig. 1). In UniQer, the question generator (QGen) and the Guesser are unified into a single transformer architecture. By utilizing such architecture, both the QGen and the Guesser can make use of the same object features, and the Guesser can consider object relations more effec-tively thanks to the self-attention structure. We also intro-duce an object-targeting module, inspired by the notion of target and distracter [8], which will decide on the objects that are to be addressed in the question. With this struc-ture, the QGen can focus on generating questions that will address the target from other objects in a supervised man-ner. Our task, CLEVR Ask, includes complex scenes in which similar-looking objects are presented; therefore, the
Questioner needs to develop the ability to ask descriptive questions rather than simple ones. The experimental results show that our model’s performance exceeds that of the base-line in terms of task success rate by around 20%. Besides, the extensive ablation studies conducted show the structural advantages of our model.
To summarize, our contributions are three-fold:
• We proposed a novel unified transformer architecture for the Questioner in goal-oriented visual dialogue. To the best of our knowledge, this is the first study that introduces a transformer architecture to goal-oriented visual dialogue.
• To address the limitations of GuessWhat?! dataset, we constructed a novel goal-oriented visual dialogue task, namely CLEVR Ask, which requires the Questioner to ask descriptive questions.
• We evaluated UniQer with two variants of CLEVR Ask datasets and found that our model outperformed the baseline and was able to ask descriptive questions with referring expressions, given complex scenes where the objects were hard to distinguish. 2.