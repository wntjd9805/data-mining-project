Abstract
Label distributions in real-world are oftentimes long-tailed and imbalanced, resulting in biased models towards dominant labels. While long-tailed recognition has been extensively studied for image classiﬁcation tasks, limited ef-fort has been made for the video domain. In this paper, we introduce VideoLT, a large-scale long-tailed video recog-nition dataset, as a step toward real-world video recog-nition. VideoLT contains 256,218 untrimmed videos, an-notated into 1,004 classes with a long-tailed distribution.
Through extensive studies, we demonstrate that state-of-the-art methods used for long-tailed image recognition do not perform well in the video domain due to the additional temporal dimension in videos. This motivates us to pro-pose FrameStack, a simple yet effective method for long-tailed video recognition.
In particular, FrameStack per-forms sampling at the frame-level in order to balance class distributions, and the sampling ratio is dynamically deter-mined using knowledge derived from the network during training. Experimental results demonstrate that FrameS-tack can improve classiﬁcation performance without sacri-ﬁcing the overall accuracy. Code and dataset are available at: https://github.com/17Skye17/VideoLT. 1.

Introduction
Deep neural networks have achieved astounding success in a wide range of computer vision tasks like image classiﬁ-cation [18, 19, 39, 41], object detection [14, 28, 34, 35], etc.
Training these networks requires carefully curated datasets like ImageNet and COCO, where object classes are uni-formly distributed. However, real-world data often have a long tail of categories with very few training samples, pos-ing signiﬁcant challenges for network training. This results in biased models that perform exceptionally well on head classes (categories with a large number of training samples)
∗ Equal contribution.
† Corresponding author.
Figure 1. Long-tailed video recognition. General video recog-nition methods are overﬁtted on head classes, while long-tailed video recognition focuses on the performance of both head and tail classes, especially on tail classes. (Blue box is the head class region, red box is the region of medium and tail classes.) but poorly on tail classes that contain a limited number of samples (see Figure 1).
Recently, there is a growing interest in learning from long-tailed data for image tasks [4, 10, 22, 29, 40, 42, 47, 49, 53]. Two popular directions to balance class distribu-tions are re-sampling and re-weighting. Re-sampling [8, 11, 16, 22, 53] methods up-sample tail classes and down-sample head classes to acquire a balanced data distribution from the original data. On the other hand, re-weighting methods [4, 10, 27, 40, 47, 52] focus on designing weights to balance the loss functions of head and tail classes. While extensive studies have been done for long-tailed image clas-siﬁcation tasks, limited effort has been made for video clas-siﬁcation.
While it is appealing to directly generalize these methods from images to videos, it is also challenging since for clas-siﬁcation tasks, videos are usually weakly labeled—only 1
a single label is provided for a video sequence and only a small number of frames correspond to that label. This makes it difﬁcult to apply off-the-shelf re-weighting and re-sampling techniques since not all snippets 1 contain in-formative clues—some snippets directly relate to the target class while others might consist of background frames. As a result, using a ﬁxed weight/sampling strategy for all snip-pets to balance label distributions is problematic. For long-tailed video recognition, we argue that balancing the distri-bution between head and tail classes should be performed at the frame-level rather than at the video (sample)-level— more frames in videos from tail classes should be sampled for training and vice versa. More importantly, frame sam-pling should be dynamic based on the conﬁdence of neural networks for different categories during the training course.
This helps preventing overﬁtting for head classes and un-derﬁtting for tail classes.
To this end, we introduce FrameStack, a simple yet effec-tive approach for long-tailed video classiﬁcation. FrameS-tack operates on video features and can be plugged into state-of-the-art video recognition models with minimal surgery. More speciﬁcally, given a top-notch classiﬁcation model which preserves the time dimension of input snip-pets, we ﬁrst compute a sequence of features as inputs of
FrameStack, i.e., for a input video with T frames, we obtain
T feature representations. To mitigate the long tail problem, we deﬁne a temporal sampling ratio to select different num-ber of frames from each video conditioned on the recogni-tion performance of the model for target classes. If the net-work is able to offer decent performance for the category to be classiﬁed, we then use fewer frames for videos in this class. On the contrary, we select more frames from a video if the network is uncertain about its class of interest. We in-stantiate the ratio using running average precision (AP) of each category computed on training data. The intuition is that AP is a dataset-wise metric, providing valuable infor-mation about the performance of the model on each cate-gory and it is dynamic during training as a direct indicator of progress achieved so far. Consequently, we can adaptively under-sample classes with high AP to prevent over-ﬁtting and up-sample those with low AP.
However, this results in samples with different time di-mensions and such variable-length inputs are not parallel-friendly for current training pipelines. Motivated by re-cent data-augmentation techniques which blend two sam-ples [43, 50] as virtual inputs, FrameStack performs tem-poral sampling on a pair of input videos and then concate-nates re-sampled frame features to form a new feature rep-resentation, which has the same temporal dimension as its inputs. The resulting features can then be readily used for
ﬁnal recognition. We also adjust the the corresponding la-1 We use “snippet” to denote a stack of frames sampled from a video clip, which are typically used as inputs for video networks. bels conditioned on the temporal sampling ratio.
Moreover, we also collect a large-scale long tailed video recognition dataset, VideoLT, which consists of 256,218 videos with an average duration of 192 seconds. These videos are manually labeled into 1,004 classes to cover a wide range of daily activities. Our VideoLT have 47 head classes (#videos > 500), 617 medium classes (100 <
#videos <= 500) and 340 tail classes (#videos <= 100), which naturally has a long tail of categories.
Our contributions are summarized as follows:
• We collect a new large-scale long-tailed video recogni-tion dataset, VideoLT, which contains 256,218 videos that are manually annotated into 1,004 classes. To the best of our knowledge, this is the ﬁrst “untrimmed” video recognition dataset which contains more than 1,000 manually deﬁned classes.
• We propose FrameStack, a simple yet effective method for long-tailed video recognition. FrameStack uses a temporal sampling ratio derived from knowledge learned by networks to dynamically determine how many frames should be sampled.
• We conduct extensive experiments using popular long-tailed methods that are designed for image classiﬁ-cation tasks, including re-weighting, re-sampling and data augmentation. We demonstrate that the existing long-tailed image methods are not suitable for long-tailed video recognition. By contrast, our FrameS-tack combines a pair of videos for classiﬁcation, and achieves better performance compared to alternative methods. The dataset, code and results can be found at: https://github.com/17Skye17/VideoLT. 2.