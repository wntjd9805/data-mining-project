Abstract
Existing blind image super-resolution (SR) methods mostly assume blur kernels are spatially invariant across the whole image. However, such an assumption is rarely applicable for real images whose blur kernels are usually spatially variant due to factors such as object motion and out-of-focus. Hence, existing blind SR methods would in-evitably give rise to poor performance in real applications.
To address this issue, this paper proposes a mutual affine network (MANet) for spatially variant kernel estimation.
Specifically, MANet has two distinctive features. First, it has a moderate receptive field so as to keep the locality of degradation. Second, it involves a new mutual affine convo-lution (MAConv) layer that enhances feature expressiveness without increasing receptive field, model size and computa-tion burden. This is made possible through exploiting chan-nel interdependence, which applies each channel split with an affine transformation module whose input are the rest channel splits. Extensive experiments on synthetic and real images show that the proposed MANet not only performs favorably for both spatially variant and invariant kernel es-timation, but also leads to state-of-the-art blind SR perfor-mance when combined with non-blind SR methods. 1.

Introduction
Single image super-resolution (SR), with the aim of re-constructing the high-resolution (HR) image from a low-resolution (LR) image, is a classical vision problem. Re-cently, convolutional neural networks (CNNs) [42, 40, 39, 7, 5, 21, 15, 44, 24, 23] have been widely used in SR. How-ever, most of these SR methods assume the blur kernel is ideal and fixed (usually a bicubic kernel), and thus dete-riorate seriously if the real kernel deviates from the ideal one [46, 3, 53, 14]. Therefore, dealing with unknown blur kernels, i.e., blind SR, is becoming a hot topic.
*Corresponding author.
Figure 1: Kernel estimation results of the proposed MANet on
“img017” in Urban100 [19] for scale factor 4. The shown image is the SR image, whose corresponding HR image was blurred by a spatially invariant kernel as shown in the top right green rectangle.
While existing blind SR methods [14, 3, 54, 53, 32, 25] have achieved remarkable performance, they assume blur kernels are spatially invariant and only estimate a single ker-nel for the whole image, giving rise to two inherent prob-lems. First, real-world blur kernels are typically spatially variant. Due to different environmental factors like object motion and depth difference, as well as non-ideal imaging such as out-of-focus and camera shake [37, 2], blur ker-nels at different locations of the image tend to be different.
Second, estimating a single kernel for the whole image is susceptible to the adverse effects of flat patches, even un-der the spatially invariant assumption. For a natural image, some patches contain edges or corners that are discrimina-tive for kernel estimation (e.g., the pillars in Fig. 1), while some other patches are rather flat (e.g., the blue sky) and are less discriminative since they correspond to various in-distinguishable but correct blur kernels that all result in the same LR patch. Therefore, estimating spatially variant ker-nels is more reasonable for blind SR.
The main challenge of spatially variant kernel estima-tion lies in the locality of degradation. A blur kernel only has impacts on a local image patch of the same size, e.g., 21 × 21, which becomes even smaller after downsampling (e.g., about 5 × 5 when scale factor is 4). Furthermore, uti-lizing pixels outside of the impacted patch may be detrimen-tal when nearby kernels are different, as shown in Fig. 4(b).
Therefore, an ideal kernel estimation model should estimate kernel from the impacted image patch. This is very chal-lenging due to the ill-posedness of the problem. An ap-pealing option is CNN, which has shown great promise for ill-posed problems [8, 52, 50]. However, most of existing networks have very large receptive fields, making them un-suitable for kernel estimation.
To tackle the problem, we propose the Mutual Affine
Network (MANet) that has a moderate receptive field. More specifically, MANet consists of feature extraction and ker-nel reconstruction module. The first module uses several residual blocks, along with downsampler layer, upsampler layer and skip connections, to extract image feature from the LR image input, while the second module reconstructs kernels for every HR image pixel from the feature. In partic-ular, we propose the mutual affine convolution (MAConv) layer for the residual block, in order to exploit channel in-terdependence without increasing network receptive field. It splits a feature along the channel dimension, and then trans-forms each split by the affine transformation module whose parameters are learned from the rest splits. After that, each split is fed into a convolution layer and then concatenated as the MAConv layer output.
The main contributions of this work are as follows:
• We propose a kernel estimation framework named
MANet. With a moderate receptive field (i.e., 22 × 22), it estimates kernels from tiny LR image patches. The mini-mum patch from which it can accurately estimate a kernel is of size 9 × 9.
• We propose the mutual affine convolution layer to en-hance feature expressiveness by exploiting channel inter-dependence without increasing network receptive field, making it suitable for feature extraction of blur kernels.
It also reduces model parameters and computation cost by about 30% compared with plain convolution layer.
• Compared with existing methods, MANet performs favourably for both spatially variant and invariant ker-nel estimation, leading to state-of-the-art blind SR perfor-mance when combined with non-blind SR models. It also shows good properties in dealing with different kinds of patches, e.g., estimating kernels accurately from non-flat patches and producing fixed kernels for flat patches. 2.