Abstract
Q: What does the woman do after hold bucket?                 A: Dump bucket
Relational reasoning is at the heart of video question an-swering. However, existing approaches suffer from several common limitations: (1) they only focus on either object-level or frame-level relational reasoning, and fail to inte-grate the both; and (2) they neglect to leverage semantic knowledge for relational reasoning. In this work, we pro-pose a Hierarchical VisuAl-Semantic RelatIonal Reasoning (HAIR) framework to address these limitations. Speciﬁ-cally, we present a novel graph memory mechanism to per-form relational reasoning, and further develop two types of graph memory: a) visual graph memory that leverages visual information of video for relational reasoning; b) se-mantic graph memory that is speciﬁcally designed to explic-itly leverage semantic knowledge contained in the classes and attributes of video objects, and perform relational rea-soning in the semantic space. Taking advantage of both graph memory mechanisms, we build a hierarchical frame-work to enable visual-semantic relational reasoning from object level to frame level. Experiments on four challeng-ing benchmark datasets show that the proposed framework leads to state-of-the-art performance, with fewer parame-ters and faster inference speed. Besides, our approach also shows superior performance on other video+language task. 1.

Introduction
Video Question Answering (VideoQA), an emerging task that requires machines to answer questions about videos in a natural language form, has recently drawn in-creasing interests from researchers. The task is particu-larly challenging, as it requires ﬁne-grained understand-ing of video content involving various complex relations such as object-object relation, frame-frame relation etc.
Thus, relational reasoning plays an important role in solv-∗Corresponding author. (a) woman hold  bucket 
... woman dump  bucket  woman dump bucket  after hold bucket 
... (b)
...
...
Write
...
Read
...
Write
...
Read
GNN
Memory Network
Graph Memory
Figure 1. (a) Hierarchical relational reasoning. Humans perform object-level ﬁrst and then frame-level relational reasoning for un-derstanding the whole video content. (b) A concise comparison of vanilla GNN, memory network and our graph memory. ing VideoQA problem. Recent works [9, 12, 14, 28, 20, 43] have introduced memory networks [44, 35], attention mech-anisms [46] or Graph Convolutional Networks (GCNs) [22] for relational reasoning in VideoQA. Although achieving promising results, these existing approaches suffer from two common limitations.
First, current approaches for VideoQA only focus on either object-level [14] or frame-level relational reason-ing [9, 12, 26, 51, 20], and do not integrate the both in a hierarchical manner. Given a video clip and an associ-ated question, as shown in Figure 1(a), a typical reasoning process for human is that we ﬁrst recognize relevant ob-jects and their interaction in each video frame (e.g. woman hold bucket, woman dump bucket), and then correlate these frames to understand a sequence of actions and their tempo-ral relationship (e.g. woman dump bucket after hold bucket).
Finally, the correct answer can be naturally derived based on the understanding of video content. Such a process of relational reasoning is conducted in a hierarchical way, i.e., from object level to frame level. It is desired to endow the machines with the same characteristic as human. However,
none of current approaches have attempted to explicitly per-form hierarchical relational reasoning. These approaches may miss the modeling of some crucial relations that are necessary for answering questions correctly.
Second, current approaches for VideoQA only con-sider visual information for relational reasoning, and neglect the reasoning in the semantic space. In [26, 20, 28], the proposed approaches perform relational reasoning over video frame features extracted by CNN. Huang et al.
[14] and Jin et al. [19] exploited object-level visual infor-mation using RCNN. These methods neglect to leverage se-mantic knowledge for relational reasoning, possibly leading to the misunderstanding of visual content due to the inher-ent semantic gap. Compared to visual information, seman-tic knowledge (e.g. the attributes and classes of multiple objects) provides more explicit and richer cues to beneﬁt the reasoning, which has been demonstrated in the image recognition domain [29, 7].
In this work, in an effort to address the aforementioned limitations, we put forward a Hierarchical VisuAl-Semantic
RelatIonal Reasoning (HAIR) framework, which jointly performs visual and semantic relational reasoning in a hi-erarchical structure (Figure 2). The core component of the framework is the graph memory mechanism, inspired by graph neural network (GNN) [40] and memory network
[44]. The GNN can pass message among nodes, which is a natural choice to perform relational reasoning. While the memory network is able to gradually distill query-related information through read and write operations. Here, we marry GNN with memory network to inherit the advan-tages of the both, enabling more efﬁcient relational rea-soning. A concise comparison of vanilla GNN, memory network and our graph memory is shown in Figure 1(b).
Moreover, we develop two types of graph memory mech-anisms: a) visual graph memory, which exploits visual in-formation of video for relational reasoning, and gradually learns query-related relation-aware visual representation; b) semantic graph memory, where we represent object classes and attributes as nodes and build edges to encode common-sense semantic relationships. It explicitly leverages seman-tic knowledge to facilitate relational reasoning. The two graph memory mechanisms work cooperatively and inter-act with each other via learnable visual-to-semantic and semantic-to-visual node mapping. Finally, taking advan-tage of the proposed graph memory mechanisms, we build a hierarchical structure, from object to frame level, thus en-abling hierarchical visual-semantic relational reasoning.
In summary, the contributions of this work are three-fold: (1) We present graph memory, a novel relational rea-soning mechanism. Furthermore, we develop visual graph memory and semantic graph memory to reason over dif-ferent types of information. (2) We propose a hierarchical visual-semantic relational reasoning (HAIR) framework to integrate object-level and frame-level relational reasoning in a hierarchical manner. (3) Experimental results show that our framework achieves state-of-the-art performance on four datasets for VideoQA, with fewer parameters and faster inference speed. Our approach also shows superior performance on other video+language tasks, e.g., language-based temporal grounding. 2.