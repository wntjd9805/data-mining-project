Abstract
We present a comprehensive framework for egocentric interaction recognition using markerless 3D annotations of two hands manipulating objects. To this end, we propose a method to create a unified dataset for egocentric 3D inter-action recognition. Our method produces annotations of the 3D pose of two hands and the 6D pose of the manipulated objects, along with their interaction labels for each frame.
Our dataset, called H2O (2 Hands and Objects), provides synchronized multi-view RGB-D images, interaction labels, object classes, ground-truth 3D poses for left & right hands, 6D object poses, ground-truth camera poses, object meshes and scene point clouds. To the best of our knowledge, this is the first benchmark that enables the study of first-person actions with the use of the pose of both left and right hands manipulating objects and presents an unprecedented level of detail for egocentric 3D interaction recognition. We fur-ther propose the method to predict interaction classes by estimating the 3D pose of two hands and the 6D pose of the manipulated objects, jointly from RGB images. Our method models both inter- and intra-dependencies between both hands and objects by learning the topology of a graph con-volutional network that predicts interactions. We show that our method facilitated by this dataset establishes a strong baseline for joint hand-object pose estimation and achieves state-of-the-art accuracy for first person interaction recog-nition. 1.

Introduction
In recent years, there has been tremendous progress in video understanding and action recognition. Current algo-rithms can reliably recognize the action the subject is per-forming in many unconstrained settings from third person viewpoints [9, 22, 23, 24, 71, 89]. Although action recogni-tion from first-person views has many applications in aug-mented reality, robotics and surveillance, it trails behind the progress in third person views, mostly due to the lack of large and diverse egocentric datasets. From an egocentric viewpoint, action recognition is mostly about understand-ing hand & object interactions. A unified understanding of
*Work performed while at Microsoft.
Project Page: https://www.taeinkwon.com/projects/h2o
Figure 1: Two hands manipulating objects for first person in-teraction recognition. We propose a dataset providing rich anno-tations for 3D poses of left & right hands, 6D object poses, camera poses, object meshes and scene point clouds, along with their asso-ciated interaction labels. We leverage our dataset to propose novel methods for 3D interaction recognition. the positions and movements of hands and the manipulated objects is crucial for recognizing egocentric interactions.
However, existing first-person interaction datasets mostly provide only 2D features (e.g. bounding boxes, hand seg-mentation) without reasoning in 3D about the motions of hands and the manipulated objects. In this work, we pro-pose, for the first time, a unified dataset for first person interaction recognition with markerless 3D annotations of two hands manipulating objects, as depicted in Fig. 1. We collect a richly annotated dataset including synchronized
RGB-D images, camera poses, right & left hand poses, ob-ject poses, object meshes, scene point clouds and action la-bels, which provides an unprecedented level of detail for un-derstanding 3D hand-object interactions. With the help of our dataset, we present the first method to estimate jointly the 3D pose of two hands and objects from a color image.
We further propose to learn interdependencies within and across hand and object poses using an adaptive graph con-volutional network for 3D interaction recognition.
Jointly capturing hands in action and the manipulated objects in 3D is a challenging problem due to reciprocal occlusions. The problem is more challenging from first person viewpoints due to the unique challenges brought by egocentric vision such as fast camera motion, large occlu-sion, background clutter [49] and most importantly, lack of datasets. Recent works have proposed datasets that success-fully addressed some of these challenges. Sridhar et al. [73] have presented one of the earliest datasets for hand-object interactions, in which a single hand manipulates a cuboid object. Pioneering works by [26, 32, 34] have further pro-posed datasets that include 3D annotations for object ma-nipulation scenarios of a single hand.
Most of these works, however, are limited by differ-ent factors. They mainly focus on single hand manipu-lation scenarios [26, 32, 34]. While single hand manip-ulation is relevant for some scenarios, most of the time, hand-object interaction involves two hands manipulating an object. Using only 2D annotations, [79, 80] presented datasets for hand-hand and hand-object interactions. The intricate nature of hand-object interactions, however, re-quires 3D reasoning rather than 2D to better resolve mutual occlusions. In the context of hand-object interactions, early work mostly tackles the problem of joint estimation of 3D hand and object poses, without reasoning about the actions.
While precise 3D position data for hands and objects is cru-cial for many applications in robotics and graphics, the sole knowledge of the pose lacks semantic meaning about the ac-tions of the subject. To that end, [26] released an egocentric action dataset including 3D annotations of hands and ob-jects; however, the data is captured with an intrusive motion capture system. Although motion capture datasets [26, 75] can provide large amounts of training samples with accurate 3D annotations, they can only be captured in controlled set-tings and have visible markers on the images that bias pose prediction in color images. Synthetic datasets [34] could provide an alternative to them, however, the existing ones cannot yet reach the realism that is needed to generalize to real images and are only for single-image scenarios that lack temporal context crucial for recognizing interactions.
Our method aims at tackling these limitations exhibited by prior work. To this end, we propose an approach for cre-ating a unified dataset for egocentric 3D interaction recog-nition that includes markerless annotations of the 3D pose of two hands and the 6D pose of the manipulated objects, along with their associated action labels for each frame of a large number of recordings that include 571,645 synchro-nized RGB-D frames.
In addition, we propose the first method to jointly predict the 3D pose of two hands and 6D pose of the manipulated objects using only RGB images and present a novel 3D interaction recognition approach that learns the interdependencies between hand and object poses by a topology-aware graph convolutional network.
Our contributions can be listed as follows:
• We present the first unified dataset for egocentric in-teraction recognition with markerless 3D annotations of two hands and the 6D pose of manipulated objects. Our dataset, which we call H2O, standing for 2 hands and objects, provides rich ground-truth annotations for 3D hand-object poses & shapes, action labels, camera poses, scene point clouds and object meshes that enable us to produce comprehensive egocentric scene interpretations.
• We propose a semi-automatic pipeline to curate a hand-object interaction dataset with action labels and the poses of two interacting hands as well as the objects in contact, using a practical multi-camera system with diverse back-grounds. We demonstrate the fidelity and accuracy of our annotations by detailed verifications.
• We introduce a unified approach to recognize hand-object interactions from RGB images that simultane-ously predicts, for the first time, the 3D pose of two in-teracting hands and the 6D pose of manipulated objects, along with action and object classes.
• Leveraging our dataset, we propose a novel method for 3D interaction recognition that learns the interdependen-cies between two hands and objects with a topology-aware graph convolutional network. To this end, we parameterize both hand and object poses as individual graphs and combine them in a single multi-graph archi-tecture. We then learn the interdependencies and connec-tions between different graph entities with an adaptive architecture and compute the topology of the multi-graph structure for recognizing 3D hand-object interactions.
We demonstrate that using the pose predictions facili-tated by our dataset, we achieve better overall performance for recognizing interactions outperforming the state-of-the-art [9, 17, 23]. We further provide baselines for hand & object pose estimation and interaction recognition to enable further benchmarking on this dataset. We will make our dataset and annotations publicly available upon acceptance. 2.