Abstract
Existing salient object detection (SOD) models usually focus on either backbone feature extractors or saliency heads, ignoring their relations. A powerful backbone could still achieve sub-optimal performance with a weak saliency head and vice versa. Moreover, the balance between model performance and inference latency poses a great challenge to model design, especially when considering different de-ployment scenarios. Considering all components in an integral neural architecture search (iNAS) space, we pro-pose a flexible device-aware search scheme that only trains the SOD model once and quickly finds high-performance but low-latency models on multiple devices. An evolu-tion search with latency-group sampling (LGS) is proposed to explore the entire latency area of our enlarged search space. Models searched by iNAS achieve similar perfor-mance with SOTA methods but reduce the 3.8×, 3.3×, 2.6×, 1.9× latency on Huawei Nova6 SE, Intel Core CPU, the
Jetson Nano, and Nvidia Titan Xp. The code is released at https://mmcheng.net/inas/. 1.

Introduction
Salient object detection (SOD) aims to segment the most attractive objects in images [1, 59]. Served as a pre-processing step, SOD is required by many downstream applications, i.e., image editing [8], image retrieval [22], visual tracking [24], and video object segmentation [20].
These applications often require the SOD model to be de-ployed with low inference latency on multiple devices, i.e., GPUs, CPUs, mobile phones, and embedded devices.
Each device has unique properties. For instance, GPUs are good at massively parallel computing [43] while the embed-ded devices are energy-friendly at the cost of a low com-puting budget [27]. Thus, different deployment scenarios require quite different designs of SOD models.
State-of-the-art (SOTA) SOD methods mostly design handcraft saliency heads [36, 44, 47, 78, 81] to aggre-gate multi-level features from the pre-trained backbone,
*M.M. Cheng (cmm@nankai.edu.cn) is the corresponding author. 95 94 93
F x a
M 92 91 90 0 3.8× higher speed d e e p s e r h h i g 4 . 8 ×
Ours
CSNet [16]
U2Net [46]
PoolNet-R [36]
EGNet-R [78]
DSS [25]
MINet-R [44]
BASNet [47]
R3Net [10]
CPD-R [64]
NLDF [42]
ITSD-R [81] 1 2 3 4
Huawei Nova6 SE speed (FPS), batch-size = 1
Figure 1. Mobile latency and performance comparison between our iNAS and recent state-of-the-art SOD models. e.g., VGG [51], ResNet [23] and Res2Net [13]. The pro-hibitive inference latency often prevents them from been ap-plied on other devices except for GPUs. On the other hand, handcraft low-latency SOD models designed for resource-constrained scenarios [16,46] suffer from large performance drop. It causes heavy workloads to manually design SOD models for different devices because of the dilemma be-tween model performance and inference latency. Therefore, we aim at a device-aware search scheme to quickly find suit-able low-latency SOD models on multiple devices.
There are several obstacles to achieve low-latency SOD models on different devices, as shown in Fig. 2. Firstly, the relative latency of operators varies among different de-vices due to different parallel computation abilities, IO bot-tlenecks, and implementations. Transfer the SOD model de-signed for one device to another would result in sub-optimal latency and performance. Secondly, conventional hand-craft SOD models design more powerful saliency heads [36, 44, 47, 81] or more efficient backbones [16, 46], while ig-noring their relations. Similarly, most neural architecture search (NAS) methods focus on the backbone for the clas-sification task [35, 53] or incorporate a fixed segmentation head [33, 34] while ignoring the backbone and head rela-tionship. We observe that a powerful backbone achieves
Embedded
Device
Mobile Phone
Device-Aware
Search
Design Space
GPU
Human
Designed
Backbone
Head
Deployment
Devices
Figure 2. iNAS unifies backbone and head design into an integral design space and specializes low-latency SOD models for different devices. sub-optimal efficiency with a weak saliency head and vice versa. These obstacles prevent the community from de-signing device-aware low-latency SOD models either with handcraft or NAS schemes.
To deal with these problems, we propose a device-aware search scheme with an integral search space to train the model once and quickly find high-performance but low-latency SOD models on multiple devices. Specifically, we propose an integral search space for SOD models that holis-tically consider the backbone and saliency head. To meet multi-scale requirements of SOD models while avoiding the latency increased by multi-branch structures, we construct a searchable multi-scale unit (SMSU). The SMSU supports searchable parallel convolutions with different kernel sizes, and reparameterizes searched multi-branch convolutions to one branch for low inference latency. We also generalize handcraft saliency heads [25, 36, 41, 44, 75] into searchable transport and decoder parts, resulting in a rich saliency head search space for cooperating with the backbone space.
With multi-scale architectures, the proposed integral
SOD search space is significantly larger than NAS spaces for the classification task [2,72]. After training the one-shot supernet, previous methods adopt evolution search with uniform sampling [2, 21, 72] to explore the search space.
Uniform sampling can ensure different architecture choices within one layer have equal sampling probability. How-ever, the overall latency of sampled models obeys a multi-nomial distribution, which causes extremely low-latency or extremely high-latency areas to be under-sampled. This im-balance sampling problem prevents uniform sampling from exploring the entire latency area of our enlarged search space. To overcome this imbalance sampling problem, we propose a latency-group sampling (LGS) that introduces the device latency to guide sampling. Dividing the layer-wise search space into several latency groups, and aggregating samples in specific latency groups, LGS preserves the off-spring in the under-sampled area but controls the samples of the over-sampled area. Compared with uniform sampling, the evolution search with LGS can explore the entire inte-gral search space and finds a group of models on a higher and wider Pareto frontier.
The main contributions of this paper are:
• An integral SOD search space that considers the backbone-head relation and covers existing SOTA handcraft SOD designs.
• A device-aware evolution search with latency-group sampling for exploring the entire latency area of the proposed search space.
• A thorough evaluation of the iNAS on five popular
SOD datasets. Our method can reach a similar per-formance with handcraft SOTA methods but largely reduces inference latency on different devices, which helps to scale up the application of SOD to different deployment scenarios. 2.