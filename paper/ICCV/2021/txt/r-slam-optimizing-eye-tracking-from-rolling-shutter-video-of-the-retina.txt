Abstract of the crystalline lens.
We present a method for optimization-based recovery of eye motion from rolling shutter video of the retina. Our ap-proach formulates eye tracking as an optimization problem that jointly estimates the retina’s motion and appearance using convex optimization and a constrained version of gra-dient descent. By incorporating the rolling shutter imag-ing model into the formulation of our joint optimization, we achieve state-of-the-art accuracy both ofﬂine and in real-time. We apply our method to retina video captured with an adaptive optics scanning laser ophthalmoscope (AOSLO), demonstrating eye tracking at 1 kHz with accuracies be-low one arcminute—over an order of magnitude higher than conventional eye tracking systems. 1.

Introduction
Eye tracking is the process of determining the eye’s gaze direction over time, often using specialized optics
In the real-time setting, precise tracking and software. is hindered by the constant and often ballistic motion of the eye, consisting of drift, microsaccades, and saccades
[22, 25]. Studying these ballistic movements in the of-ﬂine setting requires accurate, high-frequency motion es-timation algorithms, which are useful in ophthalmology and biomedicine. A precise model of these high-frequency movements is also relevant to computer vision because it could inform the development of new algorithms that aim to replicate cognitive tasks such as object recognition and scene understanding; for instance, microsaccades have been linked to complex visual tasks in humans like reading [11].
Unfortunately, most eye tracking systems are only accurate to 0.5 degrees of visual angle, making the exact dynamics of microsaccades and saccades an open question.
Eye trackers that infer gaze direction using measure-ments from the eye’s cornea, pupil and lens have both ac-curacy and precision limitations due to their reliance on in-dividual subjective calibration procedures, and from gaze estimation errors caused by changes in pupil size or wobble
In principle, eye tracking approaches based on imaging the retina can overcome these limits. Further, they add in-formation by linking gaze with retinal structures. The adap-tive optics scanning laser ophthalmoscope (AOSLO) is a device that images the retina at high resolution, with cur-rent systems capturing 30 FPS rolling shutter video that can resolve individual photoreceptor cells.
The AOSLO has mainly been applied in ophthalmology settings for recording videos of the retina, and existing ap-proaches demonstrate real-time eye tracking speeds of 1 kHz [35]. These methods register strips of incoming reti-nal video against a pre-computed retina map, which in turn is generated by stabilizing a previously-recorded AOSLO video using ofﬂine eye tracking algorithms. Unfortunately, these ofﬂine techniques often produce distorted maps be-cause they fail to completely correct for the entanglement of eye motion with the rolling shutter capture process.
In this paper, we introduce a principled approach to dis-entangling eye motion from the rolling shutter video. We formulate and solve a holistic optimization problem that si-multaneously computes retina motion and a map of retina appearance that faithfully explain the recorded AOSLO video. Jointly solving for this motion and retina map has not been attempted before because it is an under-determined problem; much like in visual SLAM (Simultaneous Local-ization and Mapping [33]), there is an inherent ambiguity between the moving location of the retina/eye and the un-derlying map of the retina’s appearance. Our method, R-SLAM (for retina-based SLAM) consists of two stages (see
Figure 1): ﬁrst, we use convex optimization to compute an initial estimate of the motion. Formulating this initial step in a convex fashion offers guarantees about the existence and uniqueness of the optimal motion solution, as well as efﬁcient algorithms to ﬁnd this solution. Second, we per-form joint reﬁnement of the retina map and initial motion estimate, aiming to reconstruct the input video using gradi-ent descent. Our contributions include:
• Formulation of eye-tracking from rolling-shutter retina video as an optimization problem.
Figure 1: Our ofﬂine tracking algorithm’s pipeline. R-SLAM receives as input distorted video of the retina, then proceeds to compute an initial motion estimate using convex optimization. We then use constrained gradient descent to jointly optimize the retina’s motion and map, the latter of which can be used for real-time tracking.
• Convex initialization and gradient-based reﬁnement of retina motion and retina map, in an ofﬂine algorithm that results in 3x less tracking error than prior work.
• Real-time eye tracking with 2x less error than prior methods, using the high-accuracy retina maps pro-duced in the ofﬂine process and applying robust statistics to fast tracking based on normalized cross-correlation. 2.