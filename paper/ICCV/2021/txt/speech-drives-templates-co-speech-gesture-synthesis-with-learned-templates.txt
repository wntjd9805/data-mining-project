Abstract
Co-speech gesture generation is to synthesize a gesture sequence that not only looks real but also matches with the input speech audio. Our method generates the movements of a complete upper body, including arms, hands, and the head. Although recent data-driven methods achieve great success, challenges still exist, such as limited variety, poor fidelity, and lack of objective metrics. Motivated by the fact that the speech cannot fully determine the gesture, we de-sign a method that learns a set of gesture template vectors to model the latent conditions, which relieve the ambiguity.
For our method, the template vector determines the gen-eral appearance of a generated gesture sequence, while the speech audio drives subtle movements of the body, both in-dispensable for synthesizing a realistic gesture sequence.
Due to the intractability of an objective metric for gesture-speech synchronization, we adopt the lip-sync error as a proxy metric to tune and evaluate the synchronization abil-ity of our model. Extensive experiments show the superior-ity of our method in both objective and subjective evalua-tions on fidelity and synchronization. 1 1.

Introduction
We humans have always been enthusiastic about making replicas of ourselves. Many successes have been achieved in generation of explicit behaviours, such as lip syncing
[30], face swapping [32], or pose re-targeting [8]. But syn-thesizing implicit behaviors of humans, which plays a key role in synthesizing realistic digital humans, is far less ex-plored. Co-speech gesture is such a kind of implicit behav-ior, referring to the movement of body parts when some-one is speaking, which conveys rich non-verbal information
*Equal contribution.
†Corresponding author. 1https://github.com/ShenhanQian/SpeechDrivesTemplates
Figure 1: Our method generates realistic gesture sequences from a piece of speech audio. With various template vec-tors, our method produces two different gesture sequences from the same audio, but the movements are synchronized for the hands, heads, and lips. such as emotion, attitude, and intention.
Early attempts on co-speech gesture synthesis are mainly rule-based [7, 21, 34], which suffer from poor naturalness because the non-verbal information is too delicate to be de-scribed by rules. Later efforts [24, 15, 22, 14, 12, 36] go be-yond by learning human behaviors from collected data. A non-negligible barrier for data-driven methods is the multi-modal essence of the mapping from speech audio to the pos-sible gestures. This means that for the same input audio, there exist multiple feasible solutions so that directly re-gressing to the ground-truth gesture casts an inconsistently biased mapping, preventing the model from learning the di-vergence in the dataset. In recent methods, a common way to cope with this challenge is adversarial learning [14, 1, 36] with discriminators narrowing the gap between generated and real ones. However, this can only improve the real-Now that we can learn the template vectors with back-propagation, why don’t we directly learn them by recon-struction? Therefore, we train a VAE (Variational Auto-Encoder [20]) to model the distribution of gesture se-quences. With this VAE model, we can encode a ground-truth gesture sequence into a template vector and learn the one-to-one mapping from it and the speech audio to the ground-truth gesture sequence. Also, it is possible to de-code a template vector to visualize its corresponding gesture sequence. With either back-propagation or VAE, we learn a set of template vectors that not only help lower the regres-sion loss when training (Figure 2b) but also make genera-tion with variety possible since we can sample one from the learned template vectors to manipulate the general appear-ance of a synthesized gesture sequence.
Although previous work on co-speech gesture [14, 1, 36] limits the scope of gesture to hands and arms only, we advo-cate including head motion into co-speech gesture, not only for a more unified and harmonized synthesis of the upper body but also for the ease of evaluation. Due to the vague-ness of gesture-syncing, existing work heavily rely on sub-jective evaluations. We propose to adopt the lip regression error as a proxy metric under the hypothesis that to learn gesture syncing well, a model should be able to learn good lip-syncing, as they both depend on the speech, and the lat-ter one is much more deterministic. Furthermore, to assess the fidelity of generated gesture sequences, our trained VAE can be used to compute a Fr´echet Template Distance (FTD) similar to the FGD proposed by Yoon et al. [36], measuring the distribution similarity between the generated ones and the real ones in the feature space.
Our contributions can be summarized as follows:
• We propose an audio-driven gesture synthesis method in a conditional learning manner. With the learning of template vectors, we relieve the ambiguity of co-speech gesture synthesis, enhancing the fidelity and variety without sacrificing synchronization quality.
• We objectify the evaluation of gesture-syncing by bor-rowing the lip-sync error as a proxy metric. Also, we propose the Fr´echet Template Distance (FTD) to assess gesture fidelity.
• We show the superior synthesis quality of our method in both subjective and objective tests and provide intu-itive visualizations of the learned template vectors. 2.