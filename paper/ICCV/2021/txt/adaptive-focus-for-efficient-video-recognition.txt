Abstract
In this paper, we explore the spatial redundancy in video recognition with the aim to improve the computational ef-It is observed that the most informative region
ﬁciency. in each frame of a video is usually a small image patch, which shifts smoothly across frames. Therefore, we model the patch localization problem as a sequential decision task, and propose a reinforcement learning based approach for efﬁcient spatially adaptive video recognition (AdaFocus). In speciﬁc, a light-weighted ConvNet is ﬁrst adopted to quickly process the full video sequence, whose features are used by a recurrent policy network to localize the most task-relevant regions. Then the selected patches are inferred by a high-capacity network for the ﬁnal prediction. During ofﬂine in-ference, once the informative patch sequence has been gen-erated, the bulk of computation can be done in parallel, and is efﬁcient on modern GPU devices. In addition, we demon-strate that the proposed method can be easily extended by further considering the temporal redundancy, e.g., dynami-cally skipping less valuable frames. Extensive experiments on ﬁve benchmark datasets, i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is signiﬁcantly more efﬁcient than the competi-tive baselines. Code is available at https://github. com/blackfeather-wang/AdaFocus. 1.

Introduction
The explosive growth of online videos (e.g., on YouTube or TikTok) has fueled the demands for automatically recog-nizing human actions, events, or other contents within them, which beneﬁts applications like recommendation [7, 8, 15],
In surveillance [6, 3] and content-based searching [22]. the past few years, remarkable success in accurate video recognition has been achieved by leveraging deep networks
[11, 53, 12, 2, 36, 19]. However, the impressive perfor-mance of these models usually comes at high computational
∗Equal contribution.
†Corresponding author.
Figure 1. Comparisons of temporal-based methods and our proposed AdaFocus approach. Most existing works reduce com-putational costs by selecting a few informative frames to process, while AdaFocus aims to perform efﬁcient inference by attending to the task-relevant patch of each frame. Importantly, our method is compatible with temporal-based techniques as it can be improved by skipping less important frames (AdaFocus+). costs. In real-world scenarios, computation directly trans-lates into power consumption, carbon emission and prac-tical latency, which should be minimized under both eco-nomic and safety considerations.
To address this issue, a number of recent works pro-pose to reduce the inherent temporal redundancy in video recognition [26, 29, 45, 42, 16, 44, 25]. As shown in Fig-ure 1 (b), it is efﬁcient to focus on the most task-relevant video frames, and allocate the majority of computation to them rather than all frames. However, another impor-tant source of redundant computation in image-based data, namely spatial redundancy, has rarely been explored in the context of efﬁcient video recognition.
In fact, it has been shown in 2D-image classiﬁcation that convolutional networks (CNNs) are able to produce correct predictions with only a few discriminative regions of the whole image
[41, 47, 18, 31, 14, 5]. By performing inference on these relatively small regions, one can dramatically reduce the computational cost of CNNs (e.g., processing a 96x96 patch requires ∼18% computation of inferring a 224x224 image).
In this paper, we are interested in whether this spatial redundancy can be effectively leveraged to facilitate efﬁ-cient video recognition. We develop a novel adaptive fo-cus (AdaFocus) approach to dynamically localize and at-tend to the task-relevant regions of each frame. In speciﬁc, our method ﬁrst takes a quick glance at each frame with a light-weighted CNN to obtain cheap and coarse global in-formation. Then we train a recurrent policy network on its basis to select the most valuable region for recognition. This procedure leverages the reinforcement learning algorithm due to the non-differentiability of localizing task-relevant regions. Finally, we activate a high-capacity deep CNN to process only the selected regions. Since the proposed re-gions are usually small patches with a reduced size, con-siderable computational costs can be saved. An illustration of AdaFocus is shown in Figure 1 (c). Our method allo-cates computation unevenly across the spatial dimension of video frames according to the contributions to the recogni-tion task, leading to a signiﬁcant improvement in efﬁciency with preserved accuracy.
The vanilla AdaFocus framework does not model tempo-ral redundancy, i.e., all frames are processed with identical computation, while the only difference lies in the locations of the selected regions. We show that our method is com-patible with existing temporal-based techniques, and can be extended via reducing the computation spent on uninforma-tive frames, as presented in Figure 1 (d). This is achieved by introducing an additional policy network that determines whether to skip some less valuable frames. This algorithm is referred to as AdaFocus+.
We evaluate the effectiveness of AdaFocus on ﬁve video recognition benchmarks (i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2). Experimental results show that AdaFocus by itself consistently outper-forms all the baselines by large margins, while AdaFocus+ further improves the efﬁciency. For instance, AdaFocus+ has 2-3x less FLOPs1 than the recently proposed AR-Net
[29] when achieving the same accuracy. We also demon-strate that our method can be deployed on top of the state-of-the-art networks (e.g., TSM [27]) and effectively im-prove their computational efﬁciency. 2.