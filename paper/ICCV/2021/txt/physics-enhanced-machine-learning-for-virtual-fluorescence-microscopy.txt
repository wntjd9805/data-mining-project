Abstract
This paper introduces a new method of data-driven mi-croscope design for virtual fluorescence microscopy. We use a deep neural network (DNN) to effectively design op-tical patterns for specimen illumination that substantially improve upon the ability to infer fluorescence image infor-mation from unstained microscope images. To achieve this design, we include an illumination model within the DNN’s first layers that is jointly optimized during network train-ing. We validated our method on two different experimen-tal setups, with different magnifications and sample types, to show a consistent improvement in performance as com-pared to conventional microscope imaging methods. Ad-ditionally, to understand the importance of learned illumi-nation on the inference task, we varied the number of illu-mination patterns being optimized (and thus the number of unique images captured) and analyzed how the structure of the patterns changed as their number increased. This work demonstrates the power of programmable optical elements at enabling better machine learning algorithm performance and at providing physical insight into next generation of machine-controlled imaging systems. 1.

Introduction
The optical microscope remains a critical tool across a wide variety of disciplines, ranging from high-content screening in biological labs to quality control in factories.
With the continued growth of automated software analysis tools, many microscope images are now rarely viewed di-rectly in their raw format by humans, but are instead com-monly processed first by a computer. Examples include the automatic classification of different cell types within large
*Colin Cooke is corresponding author: colin.cooke@duke.edu
Source code available at: github.com/clvcooke/virtual-fluorescence cell cultures [1], segmentation of cancerous areas from thin pathology tissue sections [2], and, as focused upon here, the automatic creation of fluorescence images from bright-field data [3], which we term “virtual fluorescence” (an example of the virtual fluorescence process is shown in Figure 1).
Despite the continued advancement of image analysis software, microscope hardware has changed relatively little over the past several centuries. Most microscopes still con-sist of standard illumination units and objective lenses that are optimized for direct human inspection. Optical micro-scopes are constrained by several physical limitations, in-cluding a limited resolution, field-of-view, image contrast, and depth-of-field, which restrict the amount of information that can be captured within each image. Standard micro-scope design biases this limited information towards human analysis, potentially impacting the accuracy of automated analysis.
Here, we attempt to optimize the hardware of a new microscope design to improve the performance of a deep learning based image labelling task. To achieve this goal, we present a modified neural network which includes a physical model of our experimental microscope, which is optimized jointly with the deep neural network (DNN) dur-ing training. In this work, we limit our physical model to include only the spectral and angular properties of the mi-croscope’s illumination, realized through a programmable
LED array, but leave open the possibility of considering other important parameters (focus setting, lens design, de-tector properties) in future work. Our proposed system models the microscope illumination pattern as a set of linear weights that are directly integrated into the DNN, allow-ing the calculation of gradients through back-propagation and end-to-end optimization during supervised training. Af-ter training, the optimized “physical” weights can be inter-preted as the distribution of LED brightnesses and colors to use in our experimental imaging setup, which transfers performance gains seen in training to a physical setup.
Figure 1: (A) Experimental setup: Microscope with both fluorescence and non-fluorescence imaging paths centered above a programmable LED array for training data capture. (B/C) Example fluorescence inference pipelines: The optimized LED pattern illuminates a sample to form an image. The neural network processes LED-illuminated images to produce an esti-mated fluorescence result. The ground truth is shown to the right of the inference result. Example B shows the HeLa task while Example C shows the PAN task. The red outline on the patterns indicates the bright/dark-field cutoff of the LED array. Where in bright-field illumination, both the scattered and unscattered light transmitted through the sample is directly observed through the lens, and in dark-field only scattered light (light which has been re-directed by the sample) is observed.
Here, our goal was to train a DNN to convert conven-tional transmission microscope imagery, captured with our learned illumination pattern, into a simultaneously captured fluorescence image that highlights specific features of in-terest via fluorescent markers. This goal of moving from unlabelled images to labelled ones synthetically is what we term “virtual fluorescence”, and has recently received inter-est as a promising means to avoid the need to fluorescently label specimens and to instead simply rely on deep learning to post-process standard image data[3, 4, 5].
We have two main goals within our work. The first is to understand the impact of illumination pattern design on task performance for virtual fluorescence image gener-ation. While illumination dramatically alters the appear-the impact of the in-ance of a microscopic specimen, cident light’s spatial, angular and spectral properties on fluorescent-specific feature identification is challenging to directly establish. Second, we will explore how the num-ber of designed illumination patterns used for image cap-ture changes both in terms of pattern structure and over-all task performance. To achieve this latter goal, we vary the number of patterns which are simultaneously optimized with the DNN. For each set we examine the interaction be-tween patterns within each optimized illumination set, and how the patterns change as a function of set size. We find that optimized patterns not only yield higher performance than conventional alternatives, but the structure of the pat-terns themselves provides a certain degree of physical in-tuition between scattered bright-field light and fluorescent emission that can be used to improve future data collection strategies.
Furthermore, our method represents a way of perform-ing virtual fluorescence that is cheaper, more customizable, and potentially faster than existing alternatives. Previous approaches built upon a physical intuition to design their data capture system, such as Christiansen et al.
[3] cap-turing a z-stack of images, or Cheng et al. [5] using pre-designed illumination patterns. In contrast, our method of data collection is entirely data-driven, where the trade-off between data collection speed and system performance is explicit. Additionally, since our hardware setup is inexpen-sive and uses no moving parts, it is easily deployable and accessible. 2.