Abstract
In this paper we propose a novel method for image matching based on dense local features and tailored for visual geolocalization. Dense local features matching is robust against changes in illumination and occlusions, but not against viewpoint shifts which are a fundamental as-pect of geolocalization. Our method, called GeoWarp, di-rectly embeds invariance to viewpoint shifts in the process of extracting dense features. This is achieved via a train-able module which learns from the data an invariance that is meaningful for the task of recognizing places. We also devise a new self-supervised loss and two new weakly su-pervised losses to train this module using only unlabeled data and weak labels. GeoWarp is implemented efficiently as a re-ranking method that can be easily embedded into pre-existing visual geolocalization pipelines. Experimental validation on standard geolocalization benchmarks demon-strates that GeoWarp boosts the accuracy of state-of-the-art retrieval architectures. The code and trained models will be released upon acceptance of this paper. 1.

Introduction
Visual geolocalization (VG), i.e., the task of finding the po-sition where a given photograph was taken, is a fundamen-tal problem in numerous applications, such as robotics lo-calization in GPS-denied environments or augmented real-ity. This task is cast as an image retrieval problem where the photograph to be localized (query) is matched against a labeled database in the space of some global image rep-resentations. Much of the recent literature in VG has fo-cused on improving these global representations, moving from aggregations of handcrafted local features [5, 20, 36] to more powerful and compact CNN-based global descrip-tors [4, 24, 11]. Nevertheless, since global descriptors sum-marize the whole visual content in the image, they lack ro-bustness to occlusions and clutter [38] and may fail to cap-ture the similarity of two views with a small overlap [23].
Using sparse local invariant features to establish direct geometrical correspondences among images is an effective
Figure 1. The appearance of two different views of the same place may differ significantly, thus making it hard to match them. Our method warps both images to a closer geometrical space and then computes their similarity using deep dense local features. way to solve this problem in general visual matching tasks.
In visual geolocalization this solution is compromised by the fact that different images of the same place may have strong visual differences from one another, e.g., due to illu-mination or seasonal variations [28]. In such circumstances the detection of keypoints becomes unreliable, causing non-repeatable local invariant features [42, 40]. A few recent studies in visual geolocalization have demonstrated that this problem can be circumvented by removing the detection step altogether and using dense grids of local features to match places across strong visual shifts [42] or with few tex-tures [40]. However, the robustness acquired by removing the detection step comes at the cost of a reduced invariance to geometric transformations.
Since viewpoint shifts are a fundamental problem of vi-sual geolocalization, we propose a new dense matching method, called GeoWarp, that is endowed with some invari-ance to geometric transformations. Our dense matching is a trainable operation that learns an invariance that is mean-ingful for the task of recognizing places, in a data driven manner (see Fig. 1). Being an operation dependent on the two images to be matched, it cannot be applied database wide since it would have to be computed again for each query. Therefore, we first perform a neighbor search on the database using state-of-the-art global descriptors, and then apply our novel dense matching on the shortlist of re-trieved results to re-rank them. On a technical level, our dense matching revolves around a new lightweight warping regression module that can be efficiently trained in a self-supervised fashion, which makes it possible to train it on unlabeled data. To further improve the results we devise two weakly supervised losses, which allow the network to gain robustness to common issues such as occlusions and appearance shifts, requiring only weak labels from each im-age.
Contributions:
• We introduce a new dense matching method, tailored for visual geolocalization, that has an intrinsic invari-ance to viewpoint shifts. This dense matching can be easily integrated into standard retrieval pipelines for geolocalization.
• We present a new trainable method for pairwise im-age warping. This module is trained with three new losses: a self-supervised loss and two weakly super-vised losses, that allow to rely only on unlabeled data or take advantage of weak labels, which are commonly available for visual geolocalization datasets.
• We present an extensive ablation and demonstrate on several standard datasets for visual geolocalization that our method significantly boosts the accuracy with a wide variety of retrieval networks. 2.