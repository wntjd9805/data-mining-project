Abstract
Detection Training Set (VOC)
Standard Detection Output on:
VOC
COCO
COCO
We tackle object category discovery, which is the prob-lem of discovering and localizing novel objects in a large unlabeled dataset. While existing methods show results on datasets with less cluttered scenes and fewer object in-stances per image, we present our results on the challeng-ing COCO dataset. Moreover, we argue that, rather than discovering new categories from scratch, discovery algo-rithms can beneﬁt from identifying what is already known and focusing their attention on the unknown. We propose a method that exploits prior knowledge about certain object types to discover new categories by leveraging two mem-ory modules, namely Working and Semantic memory. We show the performance of our detector on the COCO mini-val dataset to demonstrate its in-the-wild capabilities. 1.

Introduction
Unsupervised visual category discovery aims to auto-matically identify recurring “patterns,” which can be objects or object parts, and consequently learn recognition models to identify them from a large collection of unlabeled im-ages with minimal human supervision. Such algorithms can dramatically reduce annotation costs as they only need la-bels for already mined object clusters. Moreover, automati-cally discovering categories from unstructured data can help mitigate the biases that occur when manually constructing datasets by collecting images for a ﬁxed-set of concepts.
Then, why is this setup not a mainstay in recognition?
A key issue is the lack of consistent task deﬁnition, with two disparate families of approaches aiming to ad-dress unsupervised object discovery. The ﬁrst set of ap-proaches [1–4] reduces the problem to co-localization or co-segmentation, by only answering whether a pair of images share the same object and then localizing them. The other set of, arguably more generic, approaches [5–7] use clus-tering techniques to discover semantically similar regions and can sometimes leverage prior knowledge. Because of these different goals, each set deﬁnes evaluation protocols
…
Our Discovery and Localization Output (COCO)
Training boxes for 20 VOC (known) classes
Proposed Framework and Benchmark
Detection boxes for 20 VOC (known) classes
Incorrect detection for known classes in  out-of-distribution COCO images
Discovered and localized examples of  unknown classes (from COCO). In this  illustration these correspond to (clockwise):  traffic-light, tie, umbrella, bear.
Figure 1: Motivation. A detector trained on 20 VOC (known) classes struggles on out-of-distribution images (e.g., COCO) in the presence of novel (unknown) objects (e.g., bear). Our discovery and localization frame-work builds on this detector and can reliably localize and group semanti-cally meaningful “patterns” in images with both known and novel objects in challenging images. Novel objects belonging to the same class are as-signed the same bounding box color. Best viewed in color. that are incompatible with others. The second issue that limits the wide adoption of unsupervised discovery is the scalability of contemporary approaches – most works use small curated datasets and cannot scale to a realistic setup with a large number of images, a large number of object categories, and complex images (e.g., a large number of ob-jects and categories per image). Finally, a desirable prop-erty of unsupervised discovery approaches, often lacking in contemporary works, is online processing of new data as it arrives. To address these issues, it is necessary to deﬁne a standardized protocol (datasets and metrics), that more closely reﬂect real-world requirements of a task like unsu-pervised object discovery.
Towards this, this paper proposes a large-scale bench-mark for evaluating unsupervised object discovery ap-proaches and a scalable never-ending discovery approach that can deal with real-world complexities. Our approach is loosely inspired by how humans learn – continuously, utiliz-ing prior knowledge. Our benchmark is designed to evalu-ate such continuous and knowledge-driven learning and re-ﬂect real-world complexities and scale, and as a practical consideration, it is amenable to using available pretraining datasets in object recognition literature.
Motivation: Humans never stop learning [8, 9]. Since in-fancy, we continuously stumble upon objects that we have never encountered before and learn to recognize them with time [10]. This process is mostly reliant on the current knowledge we possess. For example, a toddler, with a pet dog at home, may point to a lion at a zoo and mistakenly call it a dog; but the toddler is unlikely to point to a bench, be-cause she hasn’t learned about chairs and couches yet [11].
This alludes to a continuous learning paradigm, where we notice new objects, associate them with our current knowl-edge [12], and update our knowledge (learn new concepts or update existing concepts). So how are we able to do this so effortlessly, and often unconsciously?
Studies have demonstrated that “memory” is the primary form of representation that brings together perception and learning [13], and knowledge, categories, etc., all derive from memory [14]. Despite overwhelming evidence of its importance, memory is arguably one of the least studied components in computational visual recognition; its role of-ten ceded to other modules; e.g., pretrained weights of a neural network [15], forms of knowledge-graphs [16, 17].
In this work, we investigate how memory can be used to represent knowledge and drive the discovery of new con-cepts and develop a continuous learning approach that is inspired by well established memory processes [18].
Contributions: Our two complimentary contributions are: a scalable concept discovery and localization approach, and a realistic protocol for evaluating discovery approaches.
Our ﬁrst contribution, a never-ending concept discov-ery and localization framework, is built using two memory modules: Semantic and Working memory, jointly referred to as dual memory. Semantic memory is the portion of long-term memory that contains concepts from past experi-ence (current knowledge). Working memory, on the other hand, is synonymous to short-term memory and is respon-sible for accumulating and temporarily holding information for processing. Our ﬁnal algorithm consists of carefully crafted operations which compare a new region to the dual memory, decide if it is known (or already discovered) or a novel category, and accordingly update the relevant mem-ory modules. When sufﬁcient concepts accumulate in the working memory, our algorithm amalgamates learned con-cepts from the working memory into the semantic memory, which then become part of prior knowledge. This process can continue in a never-ending, online fashion.
Our second contribution is a simple, realistic bench-mark for never-ending concept discovery, not only to eval-uate our approach, but also to enable future works to com-pare on a standardized protocol. We argue that most stan-dard benchmarks are either not realistic, are designed for classiﬁcation, are suited more for co-segmentation than dis-covery, are not labeled to evaluate performance, or are too small in scale. In addition, earlier works on never-ending and large-scale learning [19–21] either assume an a-priori list of concepts, which is unrealistic, utilizes crawled in-ternet images which is not reproducible, or use proprietary datasets which cannot be released, ruling out future compar-isons. Therefore, we propose a simple, realistic benchmark for never-ending concept discovery and localization.
Our benchmark is designed to leverage three existing datasets, ImageNet, Pascal VOC, and COCO. We assume
ImageNet and VOC are the labeled datasets available for pretraining and detecting 20 classes, respectively, from which prior knowledge can be derived; and COCO is the in-the-wild dataset to perform concept discovery and local-ization. The 20 VOC classes are treated as known classes, and the 60 additional classes in COCO are unknown classes used to evaluate a subset of the discovered objects. Other discovered objects are qualitatively assessed. This setup has six desirable properties: (1) these datasets are widely used, and therefore, we know the performance if all 80 classes were labeled; (2) the discovery set consists of all known and a variety of unknown classes; (3) the images in COCO have a slightly different distribution from VOC; (4) dataset is large-scale; (5) bounding box labels are available and (6) future progress on these datasets [22] can be leveraged. 2.