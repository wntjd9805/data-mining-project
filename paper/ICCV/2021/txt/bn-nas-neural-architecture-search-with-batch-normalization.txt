Abstract
We present BN-NAS, neural architecture search with
Batch Normalization (BN-NAS), to accelerate neural ar-chitecture search (NAS). BN-NAS can significantly reduce the time required by model training and evaluation in NAS.
Specifically, for fast evaluation, we propose a BN-based in-dicator for predicting subnet performance at a very early training stage. The BN-based indicator further facilitates us to improve the training efficiency by only training the
BN parameters during the supernet training. This is based on our observation that training the whole supernet is not necessary while training only BN parameters accelerates network convergence for network architecture search. Ex-tensive experiments show that our method can significantly shorten the time of training supernet by more than 10 times and shorten the time of evaluating subnets by more than 600,000 times without losing accuracy. The source codes are available at https://github.com/bychen515/BNNAS. 1.

Introduction
Neural architecture search (NAS), which aims to find the optimal network architecture automatically, has signif-icantly improved the network performance in many com-puter vision tasks, such as image classification [36, 11, 17, 8, 3], object detection [18, 6, 21], semantic segmentation
[4, 19], etc. However, a successful NAS method usually means training and evaluating thousands of models, which takes up to thousands of GPU days [36, 25]. The huge searching budget makes NAS hard to be applied widely.
To overcome the above issue, one-shot methods [24, 11], have been proposed to reduce the computational cost based on the weight-sharing technique, reducing the search cost from thousands of GPU days to tens of GPU days. These methods construct a supernet that includes all candidate net-work architectures. With the constructed supernet, one-shot methods consist of three stages: supernet training, subnet searching and subnet retraining.
*Equal contribution
†Corresponding author
Figure 1. Designs and computational cost of SPOS and Our BN-NAS. Compared with SPOS, our proposed BN-NAS can acceler-ate the one-shot methods in two stages: training supernet more than ten times faster, and searching subnets more than 600,000 times faster. The key to the speed-up is the BN-based indicator, which saves the searching cost and facilitates the training only BN paramters with much fewer epochs. SPOS needs 11 GPU hours in total. Ours needs only 0.8 GPU hours.
In the supernet training stage, the supernet is trained by back-propagation.
In the subnet searching stage, subnets are sampled from supernet and treated as the candidate ar-chitectures. The sampled subnets are evaluated on valida-tion data, from which the top-5 subnets with the highest accuracy on validation data are selected in SPOS. The se-lected subnets are then retrained from random initialization in the subnet retraining stage. The primary benefit of one-shot methods is that the subnets can inherit the weights of supernet to reduce the computational burden significantly in the searching stage. However, the process of training super-net hundreds of epochs and evaluating thousands of subnets is still time-consuming, leading to tens of GPU days cost.
In this paper, we identify the parameters learned at the
Batch Normalization (BN) layer as the key to significantly reduce the excessive time required by one-shot methods in training and searching stages. In searching stage, the moti-vation is that BN parameter is a very light-weight measure for the importance of operations and subnets. Existing one-shot methods evaluate thousands of subnets on validation data. Although the searching process efficiency has been improved, the large computation required for these thou-sands of subnets is still a burden. It is widely accepted that
the BN parameter of a channel reflects the importance of the channel [22, 15]. Hence, channels with smaller BN pa-rameters are considered as less important and pruning these channels will cause a small influence on the whole deep net-work [22]. Therefore, it is natural to accumulate the BN pa-rameters from multiple channels to measure the importance of candidate operations and even the whole subnet. Based on this observation, we propose a novel BN-based indicator to measure the importance of operations as well as subnets, which significantly reduces the searching cost from about 1
GPU day for SPOS to 0.14s on CPU for ours in the search-ing stage, as shown in the column for ‘searching’ in Fig. 1.
The BN-indicator further motivates us to only train the
BN parameters of supernet in the supernet training stage.
To train a supernet, it is a general practice to train all pa-rameters, i.e., parameters of convolutional layers, fully con-nected layers, and BN layers. Yet training BN layers only is not groundless. Frankle et al. [10] find that networks only training BN parameters with other randomly initialized pa-rameters fixed still have a certain capacity. During the su-pernet training stage, our BN-NAS only trains BN parame-ter but does not train the other parameters such as convolu-tional or fully-connected layers for two reasons: 1) the net-work can encode the knowledge from training data through training only a part of parameters, as found in [10]; 2) we focus on using BN parameters as the indicator for search-ing instead of network accuracy. We empirically find that training only BN parameters helps BN parameters become stable at earlier training epochs. Besides, there is an extra training speedup benefit from only training BN parameters.
Based on the observations above, we propose a new BN-NAS. The BN-NAS train supernet with much fewer train-ing epochs, and search subnets using the novel BN-based indicator for much faster speed.
To summarize, the main contributions are as follows:
• We propose a BN-based indicator for evaluating net-work architectures, which can significantly shorten the time required by one-shot NAS methods for searching candidate network architectures.
• We only train the BN parameters of the supernet and significantly reduce the number of epochs required for training the supernet, which is based on the use of
BN-based indicator when evaluating network architec-tures. Training BN parameters only and reducing the training epochs could have adverse effects on the net-work architecture searching stage. However, with our
BN-based indicator for searching, the adverse effect is overcome. as shown in Fig. 1) and searching stage (more than 600000X) without losing accuracy. 2.