Abstract
Efﬁcient and robust grasp pose detection is vital for robotic manipulation. For general 6 DoF grasping, conventional methods treat all points in a scene equally and usually adopt uniform sampling to select grasp candidates. How-ever, we discover that ignoring where to grasp greatly harms the speed and accuracy of current grasp pose detection methods. In this paper, we propose “graspness”, a quality based on geometry cues that distinguishes gras-pable area in cluttered scenes. A look-ahead searching method is proposed for measuring the graspness and statistical results justify the rationality of our method.
To quickly detect graspness in practice, we develop a neural network named graspness model to approximate the searching process. Extensive experiments verify the stability, generality and effectiveness of our graspness model, allowing it to be used as a plug-and-play module for different methods. A large improvement in accuracy is witnessed for various previous methods after equipping our graspness model. Moreover, we develop GSNet, an end-to-end network that incorporate our graspness model for early ﬁltering of low quality predictions. Experiments on a large scale benchmark, GraspNet-1Billion, show that our method outperforms previous arts by a large margin (30+ AP) and achieves a high inference speed. 1.

Introduction
As a fundamental problem in robotics, robust grasp pose detection for unstructured environment has been fascinat-ing our community for decades. It has a broad spectrum of applications in picking [10], assembling [40], home serv-ing [11], etc. Advancing the generality, accuracy and efﬁ-ciency is a long pursuit of researchers in this ﬁeld.
For grasp pose detection in the wild, it can be regarded as a two-stage problem: given a single-view point cloud,
∗ denotes equal contribution.
Cewu Lu is the corresponding author, member of Qing Yuan Research
Institute and MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai
Jiao Tong University, China and Shanghai Qi Zhi institute.
Figure 1. Graspness illustration for a cluttered scene. Brighter color denotes higher graspness. We prefer the points with high graspness for grasping. we ﬁrst ﬁnd locations with high graspability (where stage) and then decide grasp parameters like in-plane rotation, ap-proaching depth, grasp score and gripper width (how stage) for a local region.
Previous methods for 6-DoF grasp pose detection in clut-tered scenes mainly focused on improving the quality of grasp parameter prediction, i.e., the how stage, and two lines of research are explored. The ﬁrst line [41, 27, 31] adopts a sampling-evaluation method, where grasp candidates are uniformly randomly sampled from the scene and evaluated by their model. The second line [36, 13, 32] proposes end-to-end networks to calculate grasp parameters for the whole scene, where point clouds are sampled before [32] or dur-ing [36, 13] the forward propagation. For all these methods, the where stage is not explicitly modeled (i.e., they do not perform a ﬁltering procedure in a ﬁrst stage) and candidate grasp points distribute uniformly in the scene.
However, we ﬁnd that such uniform sampling strat-egy greatly hinders the performance of the whole pipeline.
There are tremendous points in 3D contiguous space, while positive samples are concentrated in small local regions.
Take GraspNet-1Billion [13], the current largest dataset in grasp pose detection as an example. We statistically ﬁnd that, even with object masks, the graspable points are less
than 10% among all the samples, not to mention the candi-date points in the whole scene. Such an imbalance causes a large waste of computing resources and degrades the efﬁ-ciency.
To tackle the above bottleneck in grasp pose detection, we propose a novel geometrically based quality, grasp-ness, for distinguishing graspable area in cluttered scenes.
One might think that we need complex geometric reason-ing to obtain such graspness. However, we discover that a simple look-ahead search by exhaustively evaluating pos-sible future grasp poses from a point can well represent its graspness. Statistical results demonstrate the justiﬁability of our proposed graspness measure, where the local geometry around points with high graspness are distinguished from those with low scores. Fig. 1 gives an illustration of our graspness for a cluttered scene.
Furthermore, we develop a graspness model that approx-imates the above process in practice. Given a point cloud in-put, it predicts point-wise graspness score, which is referred to as graspable landscape. Beneﬁting from the stability of the local geometry structures, our graspness model is object agnostic and robust to variation of viewpoint, scene, sensor, etc., making it a general and transferable module for grasp point sampling. We qualitatively evaluate its robustness and transferability in our analysis. Tremendous improve-ments in both speed and accuracy for previous sampling-evaluation based methods are witnessed after equipping them with our graspness model.
Based on our graspness model, we also propose
Graspness-based Sampling Network (GSNet), an end-to-end two-stage network with a graspness-based sampling strategy. Our network takes a dense scene point cloud as in-put, which preserves the local geometry cues. The sampling layer ﬁrstly selects the points with high graspness. Remain-ing points are discarded from the forward propagation to improve the computation efﬁciency. Such two-stage design is beneﬁcial to network convergence and also the ﬁnal ac-curacy by providing more positive samples during training.
We conduct extensive experiments to evaluate the effec-tiveness of our proposed graspness measure, model and the end-to-end network. Several baseline methods equipped with our graspness model outperform their vanilla counter-parts by a large margin in both speed and accuracy. More-over, our GSNet outperforms previous methods to a large extent. Our code and models will be made publicly avail-able to facilitate researches in related area. 2.