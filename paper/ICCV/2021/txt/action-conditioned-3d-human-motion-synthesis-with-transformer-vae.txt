Abstract
We tackle the problem of action-conditioned generation of realistic and diverse human motion sequences. In con-trast to methods that complete, or extend, motion sequences, this task does not require an initial pose or sequence. Here we learn an action-aware latent representation for human motions by training a generative variational autoencoder (VAE). By sampling from this latent space and querying a certain duration through a series of positional encod-ings, we synthesize variable-length motion sequences con-ditioned on a categorical action. Specifically, we design a Transformer-based architecture, ACTOR, for encoding and decoding a sequence of parametric SMPL human body models estimated from action recognition datasets. We eval-uate our approach on the NTU RGB+D, HumanAct12 and
UESTC datasets and show improvements over the state of the art. Furthermore, we present two use cases: improv-ing action recognition through adding our synthesized data to training, and motion denoising. Code and models are available on our project page [53]. 1.

Introduction
Despite decades of research on modeling human motions
[4, 5], synthesizing realistic and controllable sequences re-mains extremely challenging. In this work, our goal is to take a semantic action label like “Throw” and generate an infinite number of realistic 3D human motion sequences, of varying length, that look like realistic throwing (Figure 1).
A significant amount of prior work has focused on taking one pose, or a sequence of poses, and then predicting future motions [3, 6, 21, 67, 70]. This is an overly constrained scenario because it assumes that one already has a motion sequence and just needs more of it. On the other hand, many applications such as virtual reality and character con-trol [26, 57] require generating motions of a given type (se-mantic action label) with a specified duration.
We address this problem by training an action-cond-itioned generative model with 3D human motion data that
Fig. 1: Goal: Action-Conditioned TransfORmer VAE (ACTOR) learns to synthesize human motion sequences conditioned on a categorical action and a duration, T . Sequences are generated by sampling from a single motion representation latent vector, z, as opposed to the frame-level embedding space in prior work. has corresponding action labels. In particular, we construct a Transformer-based encoder-decoder architecture and train it with the VAE objective. We parameterize the human body using SMPL [43] as it can output joint locations or the body surface. This paves the way for better modeling of inter-action with the environment, as the surface is necessary to model contact. Moreover, such a representation allows the use of several reconstruction losses: constraining part rota-tions in the kinematic tree, joint locations, or surface points.
The literature [37] and our results suggest that a combina-tion of losses gives the most realistic generated motions.
The key challenge of motion synthesis is to generate se-quences that are perceptually realistic while being diverse.
Many approaches for motion generation have taken an au-toregressive approach such as LSTMs [15] and GRUs [46].
However, these methods typically regress to the mean pose
after some time [46] and are subject to drift. The key nov-elty in our Transformer model is to provide positional en-codings to the decoder and to output the full sequence at once. Positional encoding has been popularized by recent work on neural radiance fields [47]; we have not seen it used for motion generation as we do. This allows the gen-eration of variable length sequences without the problem of the motions regressing to the mean pose. Moreover, our approach is, to our knowledge, the first to create an action-conditioned sequence-level embedding. The closest work is
Action2Motion [20], which, in contrast, presents an autore-gressive approach where the latent representation is at the frame-level. Getting a sequence-level embedding requires pooling the time dimension: we introduce a new way of combining Transformers and VAEs for this purpose, which also significantly improves performance over baselines.
A challenge specific to our action-condition generation problem is that there exists limited motion capture (MoCap) data paired with distinct action labels, typically on the or-der of 10 categories [29, 59]. We instead rely on monocu-lar motion estimation methods [35] to obtain 3D sequences for actions and present promising results on 40 fine-grained categories of the UESTC action recognition dataset [30]. In contrast to [20], we do not require multi-view cameras to process monocular trajectory estimates, which makes our model potentially applicable to larger scales. Despite be-ing noisy, monocular estimates prove sufficient for training and, as a side benefit of our model, we are able to denoise the estimated sequences by encoding-decoding through our learned motion representation.
An action-conditioned generative model can augment existing MoCap datasets, which are expensive and limited in size [45, 59]. Recent work, which renders synthetic hu-man action videos for training action recognition models
[61], shows the importance of motion diversity and large amounts of data per action. Such approaches can benefit from an infinite source of action-conditioned motion syn-thesis. We explore this through our experiments on action recognition. We observe that, despite a domain gap, the generated motions can serve as additional training data, spe-cially in low-data regimes. Finally, a compact action-aware latent space for human motions can be used as a prior in other tasks such as human motion estimation from videos. (i) We introduce
ACTOR, a novel Transformer-based conditional VAE, and train it to generate action-conditioned human motions by sampling from a sequence-level latent vector. (ii) We demonstrate that it is possible to learn to generate realis-tic 3D human motions using noisy 3D body poses estimated from monocular video; (iii) We present a comprehensive ab-lation study of the architecture and loss components, obtain-ing state-of-the-art performance on multiple datasets; (iv)
We illustrate two use cases for our model on action recog-nition and MoCap denoising. The code is available on our project page [53].
Our contributions are fourfold: 2.