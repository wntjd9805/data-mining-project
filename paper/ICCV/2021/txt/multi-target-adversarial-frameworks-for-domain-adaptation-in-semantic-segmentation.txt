Abstract 1.

Introduction
In this work, we address the task of unsupervised do-main adaptation (UDA) for semantic segmentation in pres-ence of multiple target domains: The objective is to train a single model that can handle all these domains at test time. Such a multi-target adaptation is crucial for a vari-ety of scenarios that real-world autonomous systems must handle. It is a challenging setup since one faces not only the domain gap between the labeled source set and the un-labeled target set, but also the distribution shifts existing within the latter among the different target domains. To this end, we introduce two adversarial frameworks: (i) multi-discriminator, which explicitly aligns each target domain to its counterparts, and (ii) multi-target knowledge trans-fer, which learns a target-agnostic model thanks to a multi-teacher/single-student distillation mechanism. The evalu-ation is done on four newly-proposed multi-target bench-marks for UDA in semantic segmentation. In all tested sce-narios, our approaches consistently outperform baselines, setting competitive standards for the novel task.
Recent advances in domain adaptation help alleviate the labeling efforts required for training fully-supervised mod-els, which is especially helpful for tasks like semantic seg-mentation. Most previous works address the single-target setting whose goal is to adapt from source to a particular tar-get domain of interest, e.g. a speciﬁc urban area. However in practice, the perception system is often put to test in var-ious scenarios including different cities, weathers or light-ing conditions. To deal with multiple test distributions, one can straight-forwardly adopt single-target techniques by ei-ther (i) training multiple models for all target domains and adaptively activating one at test time or (ii) merging all tar-get data and treat them as being drawn from a single target distribution. While the former strategy raises storage issues for embedded platforms and is difﬁcult to scale up, the latter overlooks distribution shifts across different target domains.
In this work, we address multi-target unsupervised do-main adaptation (UDA) in semantic segmentation. We aim to learn a single segmenter that achieves equally good per-formance in all target domains, simultaneously closing dis-tribution gaps between labeled-unlabeled data (source vs. target) and among target domains (target vs. target). Our work is inline with recent efforts [3, 7, 15] toward more practical domain adaption settings for real-life applications.
Different from most existing multi-target works that specif-ically consider image classiﬁcation, we study here the more complex task of semantic segmentation.
We propose two adversarial UDA frameworks with ar-chitectures and learning schemes designed for the multi-target setup. The multi-discriminator model explicitly re-duces both source-target and target-target domain gaps via adversarial learning – each target domain is aligned to its counterparts. Our second framework, called multi-target knowledge transfer (MTKT) relaxes the multi-target op-timization complexity by adopting a multi-teacher/single-student mechanism. Each target-speciﬁc teacher handles a speciﬁc source-target domain gap via adversarial training;
The target-agnostic student is learned from all teachers to achieve target-target alignment and to perform equally well in all target domains.
Our contributions can be summarized as follows:
• We propose two multi-target UDA frameworks for se-mantic segmentation.
• We deﬁne four different evaluation benchmarks for the task making use of existing semantic segmentation datasets, i.e. GTA5 [20], Cityscapes [4], Mapillary Vis-tas [17] and India Driving Dataset [24].
• We conduct extensive experiments of these two models against state-of-the-art baselines on the proposed bench-marks. Our approaches report consistent improvements over addressed baselines. 2.