Abstract
Deep learning based visual trackers entail ofﬂine pre-training on large volumes of video datasets with accu-rate bounding box annotations that are labor-expensive to achieve. We present a new framework to facilitate bounding box annotations for video sequences, which investigates a selection-and-reﬁnement strategy to automatically improve the preliminary annotations generated by tracking algo-rithms. A temporal assessment network (T-Assess Net) is proposed which is able to capture the temporal coherence of target locations and select reliable tracking results by measuring their quality. Meanwhile, a visual-geometry re-ﬁnement network (VG-Reﬁne Net) is also designed to fur-ther enhance the selected tracking results by considering both target appearance and temporal geometry constraints, allowing inaccurate tracking results to be corrected. The combination of the above two networks provides a princi-pled approach to ensure the quality of automatic video an-notation. Experiments on large scale tracking benchmarks demonstrate that our method can deliver highly accurate bounding box annotations and signiﬁcantly reduce human labor by 94.0%, yielding an effective means to further boost tracking performance with augmented training data. 1.

Introduction
Visual tracking aims to address the challenging prob-lem of video target localization based on target appearance models. Recent studies [1, 34, 32, 13, 33] propose to per-form tracking with ofﬂine pre-trained deep features, yield-ing record-breaking results on most benchmarks. Their suc-cess is highly reliant on the availability of large-scale video datasets [10, 20, 7, 18] with accurate annotations. How-ever, manually annotating target bounding boxes is tedious and labor-intensive. Therefore, labeled datasets for training
*Corresponding author trackers are still rare and expensive to achieve, which re-stricts the potential performance boost of existing trackers.
To mitigate the above issue, some recent works [18, 25, 24, 15] explore machine learning techniques to facilitate video annotation. The basic principle is to ask human anno-tators to label ground truth bounding boxes for only a sparse set of frames, while the reset annotations are automatically produced using either temporal interpolation or state-of-the-art tracking algorithms. Signiﬁcant progress has been achieved by recent studies along this line which effectively reduce human labors required by video annotation.
One major concern of the above solutions lies on the re-liability of the adopted tracking algorithms for label gener-ation. The cutting-edge visual trackers are still not robust enough and may easily suffer from drift or other tracking failures under challenging scenarios. However, many ex-isting methods [18] directly adopt the tracking results as the generated annotation, leading to unreliable video an-notation. For one thing, these approaches mostly fail to select reliable tracking results by measuring their quality.
For another, there does not exist an effective mechanism to automatically reﬁne or correct the inaccurate tracking results. Compared to tracking algorithms based on visual content, temporal interpolation with box geometry model-ing across frames are often more robust against severe oc-clusion and target appearance variations. Some recent at-tempts [12] have also been made to combine visual track-ers with temporal interpolation based on heuristics for more accurate bounding box annotation. Nevertheless, how to jointly model appearance and temporal geometry in a prin-cipled manner is still an open question in the video annota-tion community.
Based on the above observation, we propose Video
Annotation by Selection-and-Reﬁnement (VASR), a new framework for video annotation with target bounding boxes.
Following prior works, we ﬁrst run an existing tracker ini-tialized by sparse manual annotations to obtain preliminary
Figure 2. Comparison of TrackingNet[18] annotations generated using a tracking algorithms[17] (Red) and produced by our VASR (Blue) after selection and reﬁnement. Green contours denotes the target region inferred by the proposed VG-Reﬁne Net.
• We present new architecture designs to implement the above idea, where the T-Assess Net measures the qual-ity of tracking results through temporal correlation modeling and the VG-Reﬁne Net is able to further im-prove tracking accuracy by integrating both appear-ance and temporal geometry cues.
• We empirically show that our method can reduce the amount of manual labels by 94.0% and that track-ing algorithms trained with our generated annotations compares on-par with and even more robust than their counterparts using manual annotations.
Extensive evaluation results verify that our method can serve as an effective tool to further push the state-of-the-art tracking performance by augmenting training data with high-quality annotations (See Fig. 2) at a manageable cost. Our project is available on the website: https:
//github.com/Daikenan/VASR. 2.