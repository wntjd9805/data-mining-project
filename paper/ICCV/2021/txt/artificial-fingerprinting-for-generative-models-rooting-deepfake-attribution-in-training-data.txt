Abstract
Photorealistic image generation has reached a new level of quality due to the breakthroughs of generative adversar-ial networks (GANs). Yet, the dark side of such deepfakes, the malicious use of generated media, raises concerns about visual misinformation. While existing research work on deepfake detection demonstrates high accuracy, it is subject to advances in generation techniques and adversarial iter-ations on detection countermeasure techniques. Thus, we seek a proactive and sustainable solution on deepfake de-tection, that is agnostic to the evolution of generative mod-els, by introducing artificial fingerprints into the models.
Our approach is simple and effective. We first embed artificial fingerprints into training data, then validate a sur-prising discovery on the transferability of such fingerprints from training data to generative models, which in turn ap-pears in the generated deepfakes. Experiments show that our fingerprinting solution (1) holds for a variety of cutting-edge generative models, (2) leads to a negligible side effect on generation quality, (3) stays robust against image-level and model-level perturbations, (4) stays hard to be detected by adversaries, and (5) converts deepfake detection and at-tribution into trivial tasks and outperforms the recent state-of-the-art baselines. Our solution closes the responsibility loop between publishing pre-trained generative model in-ventions and their possible misuses, which makes it inde-pendent of the current arms race. 1.

Introduction
In the past years, photorealistic image generation has been rapidly evolving, benefiting from the invention of gen-erative adversarial networks (GANs) [16] and its successive breakthroughs [39, 17, 35, 5, 25, 26, 27, 50, 51]. Given the level of realism and diversity that generative models can achieve today, detecting generated media, well known as
*Equal contribution. deepfakes, attributing their sources, and tracing their legal responsibilities become infeasible to human beings.
Moreover, the misuse of deepfakes has been permeating to each corner of social media, ranging from misinforma-tion of political campaigns [24] to fake journalism [44, 40].
This motivates tremendous research efforts on deepfake de-tection [53] and source attribution [34, 49, 46]. These tech-niques aim to counter the widespread of malicious applica-tions of deepfakes by automatically identifying generated visual contents and tracking their sources. Most of them rely on low-level visual patterns in GAN-generated im-ages [34, 49, 46, 19, 57] or frequency mismatch [13, 56, 14].
However, these techniques are unable to sustainably and robustly prevent deepfake misuse in the long run; as gen-erative models evolve, they learn to better match the true distribution causing fewer artifacts [53]. Besides, detection countermeasures are also continuously evolving [12, 6, 53].
Motivated by this, we tackle deepfake detection and at-tribution through a different lens, and propose a proactive and sustainable solution for detection, which is simple and effective. In specific, we aim to introduce artificial finger-prints into generative models that enable identification and tracing. Figure 1 depicts our pipeline; we first embed artifi-cial fingerprints into the training data using image steganog-raphy [4, 42]. The generative model is then trained with its original protocol without modification. This makes our so-lution agnostic and plug-and-play for arbitrary models. We then show a surprising discovery on the transferability of such fingerprints from training data to the model: the same fingerprint information that was encoded in the training data can be decoded from all generated images.
We achieve deepfake detection by classifying images with matched fingerprints in our database as fake and im-ages with random detected fingerprints as real. We also achieve deepfake attribution when we allocate different fin-gerprints for different generative models. Our solution thus closes the responsibility loop between generative model in-ventions and their possible misuses. It prevents the misuse of published pre-trained generative models by enabling in-Figure 1: Our solution pipeline consists of four stages. We first train an image steganography encoder and decoder. Then we use the encoder to embed artificial fingerprints into the training data. After that, we train a generative model with its original protocol. Finally, we decode the fingerprints from the generated deepfakes. ventors to proactively and responsibly embed artificial fin-gerprints into the models.
We summarize our contributions as follow: (1) We synergize the two previously uncorrelated do-mains, image steganography and GANs, and propose the first proactive and sustainable solution for the third emerg-ing domain, deepfake detection and attribution. (2) This is the first study to demonstrate the transferabil-ity of artificial fingerprints from training data to generative models and then to all the generated deepfakes. Our dis-covery is non-trivial: only deep-learning-based fingerprint-ing techniques [4, 42] are transferable to generative models, while conventional steganography and watermarking tech-niques [2, 1] are not. See Section 5.2 for comparisons. (3) We empirically validate several beneficial properties of our solution. Universality (Section 5.2): it holds for a va-riety of cutting-edge generative models [25, 26, 27, 5, 36].
Fidelity (Section 5.3): it has a negligible side effect on gen-eration quality. Robustness (Section 5.4): it stays robust against many perturbations. Secrecy (Section 5.5): the ar-tificial fingerprints are hard to be detected by adversaries.
Anti-deepfake (Section 5.6 and 5.7): it converts deepfake detection and attribution into trivial tasks and outperforms the state-of-the-art baselines [49, 46]. 2.