Abstract
Deep neural networks (DNNs) for the semantic segmen-tation of images are usually trained to operate on a pre-defined closed set of object classes. This is in contrast to the “open world” setting where DNNs are envisioned to be deployed to. From a functional safety point of view, the abil-ity to detect so-called “out-of-distribution” (OoD) samples, i.e., objects outside of a DNN’s semantic space, is crucial for many applications such as automated driving. A natu-ral baseline approach to OoD detection is to threshold on the pixel-wise softmax entropy. We present a two-step pro-cedure that significantly improves that approach. Firstly, we utilize samples from the COCO dataset as OoD proxy and introduce a second training objective to maximize the softmax entropy on these samples. Starting from pretrained semantic segmentation networks we re-train a number of
DNNs on different in-distribution datasets and consistently observe improved OoD detection performance when eval-uating on completely disjoint OoD datasets. Secondly, we perform a transparent post-processing step to discard false positive OoD samples by so-called “meta classification.”
To this end, we apply linear models to a set of hand-crafted metrics derived from the DNN’s softmax probabilities. In our experiments we consistently observe a clear additional gain in OoD detection performance, cutting down the num-ber of detection errors by 52% when comparing the best baseline with our results. We achieve this improvement sac-rificing only marginally in original segmentation perfor-mance. Therefore, our method contributes to safer DNNs with more reliable overall system performance. 1.

Introduction
In recent years spectacular advances in the computer vision task semantic segmentation have been achieved by deep learning [47, 51]. Deep convolutional neural networks (CNNs) are envisioned to be deployed to real world appli-cations, where they are likely to be exposed to data that is
Baseline segmentation mask
Baseline entropy heatmap
Our segmentation mask
Our entropy heatmap
Figure 1: Comparison of segmentation mask and softmax entropy before our OoD training (top row) and after (bot-tom row). While there are minor differences in the seg-mentation masks, the annotated unknown object (marked with green contours) becomes clearly recognizable in the entropy heatmap due to our OoD training. In the heatmap high values are red, low ones are blue. substantially different from the model’s training data. We consider data samples that are not included in the set of a model’s semantic space as out-of-distribution (OoD) sam-ples. State-of-the-art neural networks for semantic segmen-tation, however, are trained to recognize a predefined closed set of object classes [13, 32], e.g. for the usage in environ-ment perception systems of autonomous vehicles [24]. In open world settings there are countless possibly occurring objects. Defining additional classes requires a large amount of annotated data (cf. [12, 52]) and may even lead to per-formance drops [15]. One natural approach is to introduce a none-of-the-known output for objects not belonging to any of the predefined classes [49]. In other words, one uses a set of object classes that is sufficient for most scenarios and treats OoD objects by enforcing an alternative model output for such samples. From a functional safety point of view, it is a crucial but missing prerequisite that neural networks are
capable of reliably indicating when they are operating out of their proper domain, i.e., detecting OoD objects, in order to initiate a fallback policy.
As images from everyday scenes usually contain many different objects, of which only some could be out-of-distribution, knowing the location where the OoD object occurs is desired for practical application. Therefore, we address the problem of detecting anomalous regions in an image, which is the case if an OoD object is present (see figure 1) and which is a research area of high interest
[6, 20, 33, 42]. This so-called anomaly segmentation [5, 20] can be pursued, for instance, by incorporating sophisticated uncertainty estimates [3, 18] or by adding an extra class to the model’s learnable set of classes [49].
In this work, we detect OoD objects in semantic seg-mentation with a different approach which is composed of two steps: As first step, we re-train the segmentation CNN to predict class labels with low confidence scores on OoD inputs, by enforcing the model to output high prediction un-certainty. In order to quantify uncertainty, we compute the softmax entropy which is maximized when a model outputs uniform probability scores over all classes [29]. By deliber-ately including annotated OoD objects as known unknowns into the re-training process and employing a modified multi-objective loss function, we observe that the segmentation
CNN generalizes learned uncertainty to unseen OoD sam-ples (unknown unknowns) without significantly sacrificing in original performance on the primary task, see figure 1.
The initial model for semantic segmentation is trained on the Cityscapes data [13]. As proxy for OoD samples we ran-domly pick images from the COCO dataset [32] excluding the ones with instances that are also available in Cityscapes, cf.
[19, 22, 37] for a related approach in image classifi-cation. We evaluate the pixel-wise OoD detection perfor-mance via entropy thresholding for OoD samples from the
LostAndFound [42] and Fishyscapes [6] dataset, respec-tively. Both datasets share the same setup as Cityscapes but include OoD objects.
The second step incorporates a meta classifier flagging incorrect class predictions at segment level, similar as pro-posed in [34, 44, 45] for the detection of false positive in-stances in semantic segmentation. After increasing the sen-sitivity towards predicting OoD objects, we aim at removing false predictions which are produced due to the preceding entropy boost (cf. [9]). The removal of false positive OoD object predictions is based on aggregated dispersion mea-sures and geometry features within segments (connected components of pixels), with all information derived solely from the CNN’s softmax output. As meta classifier we em-ploy a simple linear model which allows us to track and understand the impact of each metric.
To sum up our contributions, we are the first to success-fully modify the training of segmentation CNNs to make them much more efficient at detecting OoD samples in Lo-stAndFound and Fishyscapes. Re-training the CNNs with a specific choice of OoD images from COCO [32] clearly outperforms the natural baseline approach of plain softmax entropy thresholding [21] as well as many state-of-the-art approaches from image classification. In addition, we are the first to demonstrate that entropy based OoD object pre-dictions in semantic segmentation can be meta classified reliably, i.e., classified whether one considered OoD pre-diction is true positive or false positive without having ac-cess to the ground truth. For this meta task we employ simple logistic regression. Combining entropy maximiza-tion and meta classification therefore is an efficient and yet lightweight method, which is particularly suitable as an in-tegrated monitoring system of safety-critical real world ap-plications based on deep learning. 2.