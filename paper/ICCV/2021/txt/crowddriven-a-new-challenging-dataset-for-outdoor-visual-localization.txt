Abstract
Visual localization is the problem of estimating the po-sition and orientation from which a given image (or a se-quence of images) is taken in a known scene. It is an impor-tant part of a wide range of computer vision and robotics applications, from self-driving cars to augmented/virtual reality systems. Visual localization techniques should work reliably and robustly under a wide range of conditions, including seasonal, weather, illumination and man-made changes. Recent benchmarking efforts model this by pro-viding images under different conditions, and the commu-nity has made rapid progress on these datasets since their inception. However, they are limited to a few geographical regions and often recorded with a single device. We propose a new benchmark for visual localization in outdoor scenes, using crowd-sourced data to cover a wide range of geo-graphical regions and camera devices with a focus on the failure cases of current algorithms. Experiments with state-of-the-art localization approaches show that our dataset is very challenging, with all evaluated methods failing on its hardest parts. As part of the dataset release, we provide the tooling used to generate it, enabling efficient and effective 2D correspondence annotation to obtain reference poses. 1.

Introduction
Visual localization is the problem of estimating the po-sition and orientation from which an image was taken, i.e., its camera pose, with respect to the scene. Visual localiza-tion is a vital part of many computer vision and robotics applications such as self-driving cars, service robots such as gardening robots, and augmented/mixed/virtual reality.
Most visual localization methods rely heavily on local descriptors for pose estimation, and finding 2D-3D matches between images is a fundamental part of it. However, the trade-off between the local descriptorsâ€™ discriminative power and their invariance limits their performance un-der changing conditions. On the other hand, in practice, changes in the scene are unavoidable, and there is a need for visual localization methods to be robust to them. Tradition-ally, ground truth poses for localization have been obtained via Structure-from-Motion (SfM) [35, 62, 66]. Yet, SfM it-self relies on local descriptors and matching. This makes it extremely difficult to generate localization benchmarks where feature matching using traditional feature descriptors does not work, e.g., in the presence of day/night and strong viewpoint changes. Benchmark datasets for localization un-der changing conditions such as Aachen, CMU Seasons, and Robotcar [58] rely on manual annotations to be able to provide ground truth poses. While these datasets provide interesting challenges, they have mostly been captured in controlled conditions. However, there is little control over the capture in many applications such as Autonomous Driv-ing, collaborative AR/MR, or crowd-sourced mapping.
In this paper, we have constructed a dataset revisiting the common challenges that could be seen in different environ-ments. We first actively mined a crowd-sourced database for image sequences where classic SfM approaches fail. To generate reliable poses, we have relied on human annota-tions and have created 40 sets of image sequences with di-verse visual changes. The dataset and tools used for its cre-ation are available at mapillary.com.
This paper makes the following contributions: (1) a workflow to mine and annotate challenging image se-quences for benchmarking visual localization. Our ap-proach explicitly takes pose uncertainty into account dur-ing the annotation process. (2) the CrowdDriven dataset, a geographically diverse and challenging dataset with re-liable poses that covers various scenarios in illumination, weather, seasonal, and viewpoint changes. (3) experi-ments with state-of-the-art baselines showing CrowdDriven presents challenges that existing methods cannot handle. 2.