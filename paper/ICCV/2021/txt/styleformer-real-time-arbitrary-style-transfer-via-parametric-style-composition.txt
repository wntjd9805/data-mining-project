Abstract
In this work, we propose a new feed-forward arbitrary style transfer method, referred to as StyleFormer, which can simultaneously fulfill fine-grained style diversity and semantic content coherency. Specifically, our transformer-inspired feature-level stylization method consists of three modules: (a) the style bank generation module for sparse but compact parametric style pattern extraction, (b) the transformer-driven style composition module for content-guided global style composition, and (c) the parametric content modulation module for flexible but faithful styliza-tion. The output stylized images are impressively coher-ent with the content structure, sensitive to the detailed style variations, but still holistically adhere to the style distribu-tions from the style images. Qualitative and quantitative comparisons as well as comprehensive user studies demon-strate that our StyleFormer outperforms the existing SOTA methods in generating visually plausible stylization results with real-time efficiency. 1.

Introduction
Arbitrary style transfer aims to re-render the content of one natural image by using the style of an arbitrary artwork.
* First two authors contributed equally.
† Corresponding author: Lu Sheng.
The early work from Gatys et al. [1] discovered that the features extracted from a well-trained deep convolutional neural network can indicate the content structures, and their statistical distributions capture the style patterns, which in-spired a line of works with advanced style and content de-scriptions [2, 3, 4, 5, 6, 7]. Despite remarkable results have been achieved, these methods are usually formulated as a complex optimization problem, whereby the losses over a deep network must be minimized for every image pair, lead-ing to high computational cost.
A large number of works [8, 9, 10, 11, 12, 13, 14, 15, 16] have been proposed to balance among stylization quality, generalization ability and execution efficiency. A common paradigm is to pretrain a feed-forward network with a fea-ture transfer module to “universally” produce stylized re-sults by using a single forward pass. This module should be able to simultaneously produce diversified style patterns redistributed from the style image and preserve coherent structures with the content image. The existing attempts either tried to adjust the holistic statistics of the content fea-tures with that of the style features [17, 18, 19, 20, 21, 22, 23], or non-locally swap the relevant style features in order to match the content features [24, 25, 26, 27, 28, 29, 30, 31].
While these methods are significantly faster than those op-timization based works, they may not generalize well to un-seen images, which inevitably degrades stylization quality or distorts content structures.
In this work, we propose a new arbitrary style transfer method that follows a similar feed-forward paradigm, but here we formulate how to generate diversified and coher-ent stylization results as a process, which first finds global composition of a finite set of learnable style codes and then parametrically modulates the content features by the com-posed style codes. The whole network can be decomposed into three modules: style bank generation, transformer-driven style composition and parametric content modula-tion. The style bank generation module produces a finite set of style codes as a sparse and compact representation of the style patterns. The transformer-driven style com-position module adopts the expressive multi-head attention strategy from the well-known transformer architecture [32] to globally compose these representative style codes, which aims at modeling new style distributions that are coherent with the content structures and sensitive to the detailed style variations, but still holistically belong to the style manifold spanned by the style images. The parametric content mod-ulation module aligns each content feature into its stylized counterpart by viewing the composed style code as a set of content-conditioned group-wise affine transforms, which thus offers more flexibility to represent diverse style pat-terns but still adheres to the content. Based on these mod-ules, our feed-forward arbitrary style transfer method, re-ferred to as StyleFormer, can produce visually plausible stylization results for various artworks, while ensuring the style diversity with fine-grained style details, and the con-tent coherence with the input content images (see our results in Fig. 1).
Our network is end-to-end trained on MS-COCO [33] and Wikiart [34] based on the common style and content losses [18, 3]. When compared with the prior optimization-based [1, 3] and feed-forward based approaches [18, 17, 25, 19, 23, 35], our method achieves the SOTA stylization re-sults in terms of both visual quality and efficiency. 2.