Abstract
In a real-world scenario, human actions are typically out of the distribution from training data, which requires a model to both recognize the known actions and reject the unknown. Different from image data, video actions are more challenging to be recognized in an open-set setting due to the uncertain temporal dynamics and static bias of human actions. In this paper, we propose a Deep Evidential
Action Recognition (DEAR) method to recognize actions in an open testing set. Specifically, we formulate the ac-tion recognition problem from the evidential deep learning (EDL) perspective and propose a novel model calibration method to regularize the EDL training. Besides, to miti-gate the static bias of video representation, we propose a plug-and-play module to debias the learned representation through contrastive learning. Experimental results show that our DEAR method achieves consistent performance gain on multiple mainstream action recognition models and benchmarks. Code and pre-trained models are available at https://www.rit.edu/actionlab/dear. 1.

Introduction
Video action recognition aims to classify a video that contains a human action into one of the pre-defined action categories (closed set). However, in a real-world scenario, it is essentially an open set problem [52], which requires the classifier to simultaneously recognize actions from known classes and identify actions from unknown ones [46, 17].
In practice, open set recognition (OSR) is more challenging than closed set recognition, while it is important for appli-cations such as face recognition [36], e-commerce product classification [60], autonomous driving [45], and so on.
OSR was originally formalized in [46] and many exist-ing approaches have been proposed using image datasets such as MNIST [32] and CIFAR-10 [30]. However, unlike
OSR, limited progress has been achieved for open set action recognition (OSAR) which is increasingly valuable in prac-tice. In fact, novel challenges arise in OSAR from the fol-lowing key aspects. First, the temporal nature of videos may
Figure 1: Open Set Action Recognition Performance. HMDB-51 [31] and MiT-v2 [39] are separately used as small- and large-scale unknown data for models trained on the closed set UCF-101 [54]. Our DEAR method (⋆) significantly outperforms exist-ing approaches on multiple action recognition models. lead to a high diversity of human action patterns. Hence, an
OSAR model needs to capture the temporal regularities of closed set actions but also be aware of what it does not know when presented with unknown actions from an open set sce-nario. Second, the visual appearance of natural videos typ-ically contain static biased cues [34, 11] (e.g., “surfing wa-ter” in totally different scenes as shown in Fig. 2). With-out addressing the temporal dynamics of human actions, the static bias could seriously hamper the capability of an
OSAR model to recognize unknown actions from an un-biased open set. Due to these challenges, existing effort on OSAR is quite limited with few exceptions [52, 27, 62].
They simply regard each video as a standalone sample and primarily rely on image-based OSR approaches. As a re-sult, they fall short in addressing the inherent video-specific challenges in the open set context as outlined above.
In this paper, we propose a Deep Evidential Action
Recognition (DEAR) method for the open set action recog-nition task. To enable the model to “know unknown” in an OSAR task, our method formulates it as an uncertainty estimation problem by leveraging evidential deep learning (EDL) [49, 65, 51, 1, 48]. EDL utilizes deep neural net-works to predict a Dirichlet distribution of class probabili-ties, which can be regarded as an evidence collection pro-cess. The learned evidence is informative to quantify the predictive uncertainty of diverse human actions so that un-known actions would incur high uncertainty, i.e., the model
knows the unknown. Furthermore, to overcome the poten-tial over-fitting risk of EDL in a closed set, we propose a novel model calibration method to regularize the evidential learning process. Besides, to mitigate the static bias prob-lem for video actions, we propose a plug-and-play mod-ule to debias the learned representation through contrastive learning. Benefiting from the evidential theory, our DEAR method is practically flexible to implement and provides a principled way to quantify the uncertainty for identifying the unknown actions. Experimental results show that the
DEAR method boosts the performance of existing powerful action recognition models with both small and large-scale unknown videos (see Fig. 1), while still maintains a high performance in traditional closed set recognition setting.
Distinct from existing OSR methods [52, 27], the pro-posed DEAR is the first evidential learning model for large-scale video action recognition. DEAR is superior to exist-ing Bayesian uncertainty-based methods [27] in that model uncertainty can be directly inferred through evidence pre-diction that avoids inexact posterior approximation or time-consuming Monte Carlo sampling [1]. Moreover, our pro-posed model calibration method ensures DEAR to be confi-dent in accurate predictions while being uncertain about in-accurate ones. Compared to [52] that incrementally learns a classifier for unknown classes, our method is more flexible in training without the access to unknown actions. More-over, our proposed debiasing module could reduce the detri-mental static bias of video actions so that the model is robust to out-of-context actions in the open set setting.
In summary, the contribution of this paper is three-fold:
• Our Deep Evidential Action Recognition (DEAR) method performs novel evidential learning to support open set action recognition with principled and effi-cient uncertainty evaluation.
• The proposed Evidential Uncertainty Calibration (EUC) and Contrastive Evidential Debiasing (CED) modules effectively mitigate over-confident predic-tions and static bias problems, respectively.
• The DEAR method is extensively validated and consis-tently boosts the performance of state-of-the-art action recognition models on challenging benchmarks. 2.