Abstract
In dynamic neural networks that adapt computations to different inputs, gating-based methods have demonstrated notable generality and applicability in trading-off the model complexity and accuracy. However, existing works only explore the redundancy from a single point of the net-In this paper, we pro-work, limiting the performance. pose dual gating, a new dynamic computing method, to re-duce the model complexity at run-time. For each convo-lutional block, dual gating identiﬁes the informative fea-tures along two separate dimensions, spatial and chan-nel. Speciﬁcally, the spatial gating module estimates which areas are essential, and the channel gating module pre-dicts the salient channels that contribute more to the re-sults. Then the computation of both unimportant regions and irrelevant channels can be skipped dynamically during inference. Extensive experiments on a variety of datasets demonstrate that our method can achieve higher accuracy under similar computing budgets compared with other dy-namic execution methods. In particular, dynamic dual gat-ing can provide 59.7% saving in computing of ResNet50 with 76.41% top-1 accuracy on ImageNet, which has ad-vanced the state-of-the-art. Codes are available at https:
//github.com/lfr-0531/DGNet. 1.

Introduction
In recent years, deep convolutional neural networks (CNNs) have made great success in various computer vision tasks, including image recognition [10, 33], object detec-tion [28, 30], and segmentation [25, 40]. However, CNNs achieve impressive accuracy at the cost of huge computa-tional complexity and intensive memory footprint, which pose challenges to the deployment of state-of-the-art mod-els on resource-constrained devices.
Various methods have been proposed to improve the
∗Corresponding author.
Figure 1: Illustration of dual gating. In each convolutional block, spatial and channel gating modules use the interme-diate feature maps to predict the informative features along two separate dimensions. Then the unimportant computa-tions can be skipped at run-time.
. computational efﬁciency of CNN inference. Pruning is a common approach to reduce the model size and computa-tions. Most of the existing methods discard the computa-tions of redundant weights or ﬁlters by following speciﬁc criteria [13, 26, 18, 43, 41, 11, 12]. However, such meth-ods execute the same calculation for different inputs. This seems to be suboptimal because “hard” inputs usually re-quire more computations. Therefore, dynamic computing has also attracted a number of researchers. Unlike the static pruning methods, these techniques allocate different calcu-lations to different inputs to save computations on those
“easy” samples and keep a high accuracy of the overall model. In dynamic computing, methods based on the gat-ing functions are general and ﬂexible approaches and have achieved a good trade-off between computation and accu-racy [37, 7, 36, 1, 39].
However, there are limitations in the existing methods.
The convolutions extract informative features across spatial and channel dimensions, but previous works only leverage the redundancy from a single point of the networks, affect-ing the performance. Besides, simply combining those two dimensions can not get a good result, and it is still a chal-lenge to integrate the spatial and channel redundancies for better efﬁciency. In this paper, we propose dual gating, a new dynamic computing method to reduce the model com-putations at run-time. CNN with dual gating (DGNet) ex-ploits the redundant features from those two principal axes, spatial and channel. We call such redundancies spatial spar-sity and channel sparsity, respectively.
In the spatial do-main, we try to detect foreground regions and avoid the computations on the background, which can keep the key information in the model. Meanwhile, in the channel do-main, we estimate the most relevant channels to the current input and skip the computations of those unimportant ones.
To achieve this, DGNet uses the channel and spatial gat-ing modules to identify the channel and spatial sparsity, respectively. The channel gating module produces a ﬁne-grained binary mask that turns the output feature maps on or off. And we use the spatial gating module to get a tiled binary mask over the spatial dimension to decide regions to be evaluated. Compared with the ﬁne-grained spatial gating, the tile-based spatial mask can keep more informa-tive features under the same computing saving. Both gating modules are lightweight, and the overhead of parameters and computation is negligible. In addition, we propose a simple and efﬁcient method to integrate the gating modules with existing networks and jointly train end to end. As a result, this can not only reduce computations but also allow the salient information to ﬂow freely throughout the model.
Our experiments demonstrate that by plugging our dual gating modules, we can obtain appreciable computation re-ductions with even higher accuracy than the baseline mod-els on CIFAR-10, ImageNet, and COCO datasets. Com-pared with the state-of-the-art dynamic computing and static pruning methods, we can consistently improve the performance under similar computing budgets. We then conduct ablation studies to quantitatively evaluate improve-ments of our proposed methods, including the tiled spatial gating and the efﬁcient integration. Finally, we visualize the trained DGNet and observe that the computation mainly focuses on the target objects and those essential features, making full use of the sparsity in both spatial and channel dimensions.
Our contributions can be summarized as follows:
• We propose dual gating, a new dynamic computing method, which leverages spatial and channel sparsities to improve the computation efﬁciency at run-time.
• We design the spatial and channel gating modules, which can be integrated with commonly used CNN ar-chitectures and trade off the accuracy and computation
Importantly, the proposed method al-more ﬂexibly. lows the salient information to ﬂow freely throughout the model.
• We verify the performance on the CIFAR-10, Ima-geNet, and COCO datasets. The proposed DGNet achieves state-of-the-art results compared with other dynamic computing and static pruning methods. 2.