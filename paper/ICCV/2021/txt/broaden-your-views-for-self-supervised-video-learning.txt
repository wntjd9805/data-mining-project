Abstract
Most successful self-supervised learning methods are trained to align the representations of two independent views from the data. State-of-the-art methods in video are inspired by image techniques, where these two views are similarly extracted by cropping and augmenting the resulting crop.
However, these methods miss a crucial element in the video domain: time. We introduce BraVe, a self-supervised learn-ing framework for video. In BraVe, one of the views has access to a narrow temporal window of the video while the other view has a broad access to the video content. Our models learn to generalise from the narrow view to the gen-eral content of the video. Furthermore, BraVe processes the views with different backbones, enabling the use of alterna-tive augmentations or modalities into the broad view such as optical ﬂow, randomly convolved RGB frames, audio or their combinations. We demonstrate that BraVe achieves state-of-the-art results in self-supervised representation learning on standard video and audio classiﬁcation benchmarks in-cluding UCF101, HMDB51, Kinetics, ESC-50 and AudioSet. 1.

Introduction
Over the past few years, self-supervised methods have rev-olutionized the ﬁeld of representation learning [17, 36, 68].
These methods directly learn from data without the need for manually deﬁned labels that are hard to get at scale.
Doing so, one can successfully leverage large amounts of uncurated data to improve representations. Even more im-portantly, self-supervised learning enables richer training tasks to be deﬁned, compared to the standard approach of trying to categorize diverse visual inputs into a ﬁxed set of categories. This has led to self-supervised representations outperforming supervised ones on downstream tasks [33].
Video is a natural domain for self-supervised learning since data is rich and abundant but hard to annotate at scale due to the additional temporal complexity. However, most methods
†Correspondence to: Adri`a Recasens (arecasens@google.com)
Figure 1. Given a narrow view corresponding to a video clip of a few seconds, BraVe is tasked with predicting a broad view that spans a longer temporal context of the video in different modalities (here visual and audio). Solving that task requires the representation to extrapolate what happened before, during and after the narrow view, and results in state-of-the-art video representations. in the video domain take direct inspiration from methods developed for images without fully taking advantage of its distinctly different dimension: time.
In particular, one common aspect of self-supervised meth-ods for images is to extract two views from a given instance using the same general augmentation procedure, feed them into a shared backbone, and extract a supervisory signal from the fact that these two views originate from the same source. This is true for most recent approaches irrespec-tive of their underlying learning principle: contrastive ap-proaches [17], clustering-based method [13], or regression algorithms [68]. The same principle has been followed in the video domain [4, 67]. Speciﬁcally, most video meth-ods extract the different views from a source video clip in a symmetric fashion with respect to time: all extracted views have the same temporal extent in the video [4, 23, 45, 67].
However, doing so does not beneﬁt from learning from in-formation contained at different time scales.
In this paper, we introduce an algorithm dubbed
“Broaden your Views” (BraVe), that breaks this symme-try in order to improve representation learning from videos.
In detail, given a narrow view corresponding to a video clip of a few seconds, BraVe learns a representation by predict-ing a broad view that spans the longer temporal context of the full video clip as illustrated in Figure 1. Solving such a task requires extrapolating to the general context in which a given event occurs. In the example of Figure 1, one has to predict what happened before the person is in the sky (they probably jumped with the help of some device, given the height), as well as what is going to happen next (they will probably fall down somewhere soft) in order to solve the task. This task arguably requires a good understanding of the structure of events and is therefore a promising task for learning repre-sentations. While related local-to-global proxy tasks have been studied in the image domain via network architectural designs [9, 37] or multi-size cropping [17], applying these techniques to videos is not straightforward, because of the increased computational complexity incurred by the time dimension and the artifacts introduced when doing similar resize operations in spatio-temporal volumes. To address this challenge, we propose to process broad views with a dedicated model. We demonstrate that under a ﬁxed com-putational budget, learning from the supervision provided by our broad views performs better than alternatives relying on symmetric augmentation procedures. Our algorithm is simple and does not require a cumbersome creation of ex-plicit negatives as in contrastive methods. Instead we use a direct regression-based approach inspired by BYOL [29], where the views are processed by dedicated backbones and regress each other. Breaking the symmetry enables the use of stronger augmentations and different modalities for the broad view, which improves the quality of the ﬁnal represen-tations.
Contributions. We make the following contributions: (i) We propose a novel framework for representation learning, called BraVe, which generates views at different time scales and learns representations via simple regression across views, (ii) We explore using different augmentations and modalities in the broad view such as audio, ﬂow or randomly convolved
RGB frames. (iii) We evaluate this framework in the video domain, both with and without audio as an auxiliary supervi-sory signal, where we obtain state-of-the-art results on video and audio classiﬁcation benchmarks UCF101, HMDB51,
Kinetics, ESC-50 and AudioSet. 2.