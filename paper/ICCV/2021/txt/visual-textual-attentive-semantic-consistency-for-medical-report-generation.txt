Abstract
Automatic report generation on medical radiographs have recently gained interest. However, identifying diseases as well as correctly predicting their corresponding sizes, locations and other medical description patterns, which is essential for generating high-quality reports, is challeng-ing. Although previous methods focused on producing read-able reports, how to accurately detect and describe ﬁnd-ings that match with the query X-Ray has not been success-fully addressed. In this paper, we propose a multi-modality semantic attention model to integrate visual features, pre-dicted key ﬁnding embeddings, as well as clinical features, and progressively decode reports with visual-textual seman-tic consistency. First, multi-modality features are extracted and attended with the hidden states from the sentence de-coder, to encode enriched context vectors for better decod-ing a report. These modalities include regional visual fea-tures of scans, semantic word embeddings of the top-K ﬁnd-ings predicted with high probabilities, and clinical features of indications. Second, the progressive report decoder con-sists of a sentence decoder and a word decoder, where we propose image-sentence matching and description accuracy losses to constrain the visual-textual semantic consistency.
Extensive experiments on the public MIMIC-CXR and IU
X-Ray datasets show that our model achieves consistent im-provements over the state-of-the-art methods. 1.

Introduction
Chest X-Rays are highly important radiological exam-inations. However, interpreting chest X-Ray images re-quires the strong expertise and experience of radiologists and is prone to mistakes. Therefore, automatic diagnosis of diseases [41, 35, 19, 21, 31, 7, 51] has been a rising re-search topic in the medical imaging community. The com-∗Corresponding author: Yi Zhou (yizhou.szcn@gmail.com)
Figure 1. Illustration of automatic medical report generation.
Given a chest X-Ray and corresponding clinical information, our model predicts key ﬁndings and generates a diagnostics report. mon tasks include identiﬁcation of different chest lesions
[42, 45, 52] and their corresponding positions and sizes, and generating human-readable reports [26, 24, 20, 53] which contain detailed descriptions such as lesion shape and type.
The basic framework of medical report generation is similar to that of image captioning [16]. Currently, most image captioning models [39, 43, 1, 27, 9] adopt deep learning due to its recent breakthroughs in many tasks
[15, 14, 37, 40, 50, 11]. However, medical report genera-tion is more challenging than image captioning for two main reasons. First, compared to general images, abnormal lesion appearances in medical images are not always obvious and sometimes even difﬁcult for radiologists to identify. Pub-lic benchmarks with paired image-report data are scarce.
However, the objects in a general image and corresponding relations among them are very clear and easy to describe.
Large-scale matched image-sentence training datasets are available, such as MS COCO [29] and Visual Genome [23].
Second, the target of image captioning is usually to generate one sentence for each image or several sentences with sim-ilar descriptions. For medical reports, multiple sentences need to be generated to focus on different diseases in vari-ous regions. Previous methods [30, 47, 49] presented hier-archical decoders to generate different topics, but only used word-level supervision without any constraints on the accu-racy and suitability of sentence-level (topic-level) topics.
In this paper, as shown in Fig. 1, given a radiograph and the corresponding clinical information, we propose an au-tomatic diagnosis method to predict key ﬁndings and gener-ate detailed descriptions. Clinical information is combined into the inputs of the model because it is closely related to the disease diagnosis and always available together with
X-Rays in clinical scenarios. The main contributions are highlighted as follow: (1) A multi-modality semantic at-tention (MMSA) model is proposed to combine different modality features into context vectors for the decoder. Be different from previous attention modules [43, 18], in this work, the regional visual features are both self-attended and correlated with the hidden states of the sentence decoder to obtain semantic attentions at different topic steps. Thus, the MMSA learns both the image-level and topic-level at-tentions. Moreover, the clinical features and word embed-dings of predicted key ﬁndings are also integrated for multi-attention learning. (2) To optimize the sequential sentence and word decoders, in addition to the word-level supervi-sion, we introduce two topic-level losses at the top of the sentence decoder. An image-sentence matching loss is de-signed to link the paired image features and generated sen-tence embeddings, while punishing unpaired ones. Besides, a description accuracy loss is presented to ensure that the generated global report embedding contains correct seman-tic information for the predicted key ﬁndings. (3) Extensive experiments are conducted to show the effectiveness of the proposed multi-attention model, matching and description accuracy losses. A new metric, normalized Key Term Dis-tance (nKTD), is also introduced to more reasonably evalu-ate the medical report generation performance. 2.