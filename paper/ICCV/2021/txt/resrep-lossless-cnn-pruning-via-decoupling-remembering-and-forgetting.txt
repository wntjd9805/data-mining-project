Abstract
We propose ResRep, a novel method for lossless channel pruning (a.k.a. ﬁlter pruning), which slims down a CNN by reducing the width (number of output channels) of convolu-tional layers. Inspired by the neurobiology research about the independence of remembering and forgetting, we pro-pose to re-parameterize a CNN into the remembering parts and forgetting parts, where the former learn to maintain the performance and the latter learn to prune. Via train-ing with regular SGD on the former but a novel update rule with penalty gradients on the latter, we realize structured sparsity. Then we equivalently merge the remembering and forgetting parts into the original architecture with narrower layers. In this sense, ResRep can be viewed as a success-ful application of Structural Re-parameterization. Such a methodology distinguishes ResRep from the traditional learning-based pruning paradigm that applies a penalty on parameters to produce sparsity, which may suppress the pa-rameters essential for the remembering. ResRep slims down a standard ResNet-50 with 76.15% accuracy on ImageNet to a narrower one with only 45% FLOPs and no accu-racy drop, which is the ﬁrst to achieve lossless pruning with such a high compression ratio. The code and models are at https://github.com/DingXiaoH/ResRep. 1.

Introduction
The mainstream techniques to compress and accelerate convolutional neural network (CNN) include sparsiﬁcation
∗This work was supported by the National Natural Science Foundation of China (No. 61925107, U1936202, 61971260), the National Key R&D
Program of China (No. 2020AAA0105500). Xiaohan Ding is funded by the Baidu Scholarship Program 2019. Correspondence to: Yuchen Guo,
Guiguang Ding.
[10, 18, 20], channel pruning [26, 27, 35], quantization
[4, 5, 45, 68], knowledge distillation [28, 32, 41, 61], etc.
Channel pruning [27] (a.k.a. ﬁlter pruning [34] or network slimming [42]) reduces the width (i.e., number of output channels) of convolutional layers to effectively reduce the number of ﬂoating-point operations (FLOPs) and memory footprint, which is complementary to the other model com-pression methods as it produces a thinner model of the orig-inal architecture with no custom structures or operations.
However, as CNN’s representational capacity depends on the width of conv layers, it is difﬁcult to reduce the width without performance drops. On practical CNN archi-tectures like ResNet-50 [22] and large-scale datasets like
ImageNet [6], lossless pruning with high compression ra-tio has long been considered challenging. For reasonable trade-off between compression ratio and performance, a typical paradigm (Fig. 1.A) [2, 3, 9, 36, 39, 62, 63] trains the model with magnitude-related penalty loss (e.g., group
Lasso [57, 60]) on the conv kernels to produce structured sparsity, which means all the parameters of some channels become small in magnitude. Ideally, if the parameters of pruned channels are small enough, the pruned model may deliver the same performance as before (i.e., after training but before pruning), which we refer to as perfect pruning.
Since both the training and pruning may degrade the performance, we evaluate a training-based pruning method from two aspects. 1) Resistance. The training phase tends to decrease the accuracy (referred to as training-caused damage) as it introduces some desired properties such as structured sparsity into the model, which may be harmful because the objective of optimization is changed and the parameters are deviated from the optima. We say a model has high resistance if the performance maintains high dur-ing training. 2) Prunability. When we prune the model into a smaller one after training, the properties obtained (e.g.,
many channels being close to zero) will reduce the pruning-caused damage. If the model endures a high pruning ratio with low performance drop, we say it has high prunability.
We desire both high resistance and prunability, but the traditional penalty-based paradigm naturally suffers from a resistance-prunability trade-off. For example, a strong group Lasso achieves high sparsity with great training-caused damage, while a weak penalty maintains the per-formance but results in low sparsity, hence great pruning-caused damage. Sect. 3.3 presents detailed analysis.
In this paper, we propose ResRep to address the above problem, which is inspired by the neurobiology research on remembering and forgetting. 1) Remembering requires the brain to potentiate some synapses but depotentiate the others, which resembles the training of CNN that makes some parameters large and some small. 2) Synapse elim-ination via shrinkage or loss of spines is one of the classi-cal forgetting mechanisms [56] as a key process to improve efﬁciency in both energy and space for biological neural network, which resembles pruning. Neurobiology research reveals that remembering and forgetting are independently controlled by Rutabaga adenylyl cyclase-mediated memory formation mechanism and Rac-regulated spine shrinkage mechanism, respectively [16, 21, 59], indicating it is more reasonable to learn and prune by two decoupled modules.
Inspired by such independence, we propose to decouple the “remembering” and “forgetting”, which are coupled in the traditional paradigm as the conv parameters are involved in both the “remembering” (objective function) and “for-getting” (penalty loss) in order for them to achieve a trade-off. That is, the traditional methods force every channel to
“forget”, and remove the channels that “forgot the most”.
In contrast, we ﬁrst re-parameterize the original model into
“remembering parts” and “forgetting parts”, then apply “re-membering learning” (i.e., regular SGD with the original objective) on the former to maintain the “memory” (origi-nal performance), and “forgetting learning” on the latter to
“eliminate synapses” (zero out channels).
ResRep comprises two key components: Convolutional
Re-parameterization (Rep, the methodology of decoupling and the corresponding equivalent conversion) and Gradient
Resetting (Res, the updage rule for “forgetting”). Speciﬁ-cally, we insert a compactor, which is a 1 × 1 conv, after the original conv layer we desire to prune. During training, we add penalty gradients to only the compactors, select some compactor channels and zero out their gradients derived from the objective function. Such a training process makes some channels of compactors very close to zero, which are removed with no pruning-caused damage. Then we equiv-alently convert the compactor together with the preceding conv into a single conv with fewer channels through some linear transformations (Eq. 8, 9). Note that this method readily generalizes to the common case where the original conv is followed by batch normalization (BN) [31]. In this case, we append the compactor after BN, and convert the conv-BN-compactor sequence after training by ﬁrst equiv-alently fusing the conv-BN into a conv with bias (Eq. 4).
Eventually, the resultant model has the same architecture as the original (i.e., no compactors) but narrower layers (Fig. 1.B). As the equivalent conversion from the training-time model into the pruned model relies on the equivalent con-version of the parameters, ResRep can be viewed as an ap-plication of Structural Re-parameterization [11, 15, 14, 13].
The other Structural Re-parameterization works improve the VGG-like architectures [15], basic conv layers [11, 13] or an MLP-style building block [14] with different well-designed structures but all via an ordinary training process.
In contrast, ResRep not only constructs extra structures (i.e., compactors) that can be equivalently converted back (Rep) but also uses a custom training strategy (Res). As will be shown in Sect. 4.3, Rep and Res are both essential: Rep constructs some structures for Res to apply on without los-ing the original information; Res zeros out some channels so that Rep can make the resultant conv layer narrower. No-tably, ResRep can also prune a fully-connected layer be-cause it is equivalent to a 1 × 1 conv [46].
Algorithm 1 Pipeline of ResRep channel pruning. 1: Input: well-trained model W 2: Construct the re-parameterized model ˆW with com-pactors. Initialize the compactors as identity matrices and the other parts with the original parameters of W 3: for i = 0 to max training iterations do 4:
Forward a batch through ˆW, compute the loss using the original objective function, derive the gradients
Apply Gradient Resetting on the gradients of com-pactors only (Eq. 14)
Update ˆW with the reset gradients of compactors and the original gradients of the other parameters 5: 6: 7: end for 8: Delete the rows of compactors which are close to zero (e.g., with norm ≤ 10−5) in ˆW. Equivalently convert the parameters of ˆW into W (cid:48) (Eq. 4, 8, 3). Now W (cid:48) has the same architecture as W but narrower layers 9: Output: pruned model W (cid:48)
ResRep features: 1) High resistance. To maintain the performance, ResRep does not change the loss function, update rule or any training hyper-parameters of the orig-inal model (i.e., the conv-BN parts). 2) High prunabil-ity. The compactors are driven by the penalty gradients to make many channels small enough to realize perfect prun-ing, even with a mild penalty strength. 3) Given the re-quired global reduction ratio of FLOPs, ResRep automati-cally ﬁnds the appropriate eventual width of each layer with no prior knowledge, making it a powerful tool for CNN
Figure 1: Traditional penalty-based pruning vs. ResRep. We prune a 3 × 3 layer with one input channel and four output channels for illustration. For the ease of visualization, we ravel the kernel K ∈ R4×1×3×3 into a matrix W ∈ R4×9. A) To prune some channels of K (i.e., rows of W ), we add a penalty loss on the kernel to the original loss, so that the gradients will make some rows smaller in magnitude, but not small enough to realize perfect pruning. B) ResRep constructs a compactor with kernel matrix Q ∈ R4×4. Driven by the penalty gradients, the compactor selects some of its channels and generates a binary mask, which resets some of the original gradients of Q to zero. After multiple iterations, those compactor channels with reset gradients become inﬁnitely close to zero, which enables perfect pruning. Finally, the conv-BN-compactor sequence is equivalently converted into a regular conv layer with two channels. Blank rectangles indicate zero values. structure optimization. 4) End-to-end training and easy im-plementation (Alg. 1). We summarize our contributions as:
• Inspired by the neurobiology research, we proposed to decouple “remembering” and “forgetting” for pruning.
• We proposed two techniques, Rep and Res, to achieve high resistance and prunability. They can be used sepa-rately and the combination delivers the best performance.
• We achieved state-of-the-art results on common bench-mark models, including real lossless pruning on ResNet-50 on ImageNet with a pruning ratio of 54.5%. 2.