Abstract
This paper introduces Click to Move (C2M), a novel framework for video generation where the user can con-trol the motion of the synthesized video through mouse clicks specifying simple object trajectories of the key ob-jects in the scene. Our model receives as input an ini-tial frame, its corresponding segmentation map and the sparse motion vectors encoding the input provided by the
It outputs a plausible video sequence starting from user. the given frame and with a motion that is consistent with user input. Notably, our proposed deep architecture incor-porates a Graph Convolution Network (GCN) modelling the movements of all the objects in the scene in a holis-tic manner and effectively combining the sparse user mo-tion information and image features. Experimental results show that C2M outperforms existing methods on two pub-licly available datasets, thus demonstrating the effective-ness of our GCN framework at modelling object interac-tions. The source code is publicly available at https:
//github.com/PierfrancescoArdino/C2M . 1.

Introduction
Recent years have witnessed several breakthroughs in the generation of high dimensional data such as images [6, 8, 24] or videos [36, 40]. However, most practical and com-mercial applications require to control generated visual data on inputs provided by the user. For instance, in image ma-nipulation, photo editing software [1] applies deep learn-ing models to allow users to change portions of an im-age [27, 31, 48].
Regarding videos, several possible ways to control the generated sequences have been considered. For instance, the generation of frames can be conditioned on simple categorical attributes [13], short sentences [22] or sound
[35]. An interesting recent research direction comprises works that attempt to condition the video generation pro-cess providing motion information as input [33, 34, 36, 43].
These approaches allow to generate videos of moving faces
Figure 1. Illustration of the video generation process of Click to
Move (C2M): 1) the user selects the objects in a scene and specify their movements. 2) Our network models the interactions between all objects through the GCN and 3) predicts their displacement. 4)
The network produces a realistic and temporally consistent video.
[43], human silhouettes and, in general, of arbitrary ob-jects [33, 34, 36]. However, these works mainly deal with videos depicting a single object. It is indeed extremely more challenging to animate images and generate videos when multiple objects are present in the scene, as there is no sim-ple way to disentangle the information associated with each object and easily model and control its movement.
This paper introduces Click to Move (C2M), the first approach that allows users to generate videos in complex scenes by conditioning the movements of specific objects through mouse clicks. Fig.1 illustrates the video generation process of C2M. The user only needs to select few objects in the scene and to specify the 2D location where each object should move. Our proposed framework receives as inputs an initial frame with its segmentation map and synthesizes a video sequence depicting objects for which movements are coherent with the user inputs. The proposed deep ar-chitecture comprises three main modules: (i) an appearance encoder that extracts the feature representation from the first frame and the associated segmentation map, (ii) a motion
module that predicts motion information from user inputs and image features, and (iii) a generation module that out-puts the synthesised frame sequence.
In complex scenes with multiple objects, modelling interactions is essential to generate coherent videos. To this aim, we propose to adopt a Graph Neural Network (GCN), which models object in-teractions and infers the plausible displacements for all the objects in the video, while respecting the user’s constraints.
Experimental results show that our approach outperforms previous video generation methods on two publicly avail-able datasets and demonstrate the effectiveness of the pro-posed GCN framework in modelling object interactions in complex scenes.
Our work is inspired by previous literature that generates videos from an initial frame and the associated segmenta-tion maps [28, 32]. From these works, we inherit a two-stage procedure where we first estimate the optical flows between an initial frame and all the generated frames, and subsequently refine the image obtained by warping the ini-tial frame according to the estimated optical flows. How-ever, our framework improves over these previous works as it allows the user the possibility to directly control the video generation process with simple mouse clicks. Similarly to the work of Hao et al. [12], we propose to control object movements via sparse motion inputs. However, thanks to the GCN, our approach can deal with scenes with multiple objects, while [12] cannot. Furthermore, the method in [12] does not explicitly consider the notion of object, as it does not use any instance segmentation information, and does not model the temporal relation between multiple frames. We instead work on multiple frames and in the semantic space, so the user can intuitively select the object of interest and move it in a temporal consistent way. The use of semantic information is motivated by recent findings in the area of image manipulation where it has been shown that semantic maps are beneficial in complex scenes [2, 20].
Contributions. Overall, the main contributions of our work are as follows:
• We propose Click to Move (C2M), a novel approach for video generation of complex scenes that permits user interaction by selecting objects in the scene and specifying their final location through mouse clicks.
• We introduce a novel deep architecture that leverages the initial video frame and its associated segmentation map to compute the motion representations that enable the generation of frame sequence. Our deep network incorporates a novel GCN that models the interaction between objects to infer the motion of all the objects in the scene.
• Through an extensive experimental evaluation, we demonstrate that the proposed approach outperforms its competitors [28,32] in term of video quality metrics and can synthesize videos where object movements follow the user inputs. 2.