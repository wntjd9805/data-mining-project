Abstract
Image quality assessment (IQA) is an important research topic for understanding and improving visual experience.
The current state-of-the-art IQA methods are based on con-volutional neural networks (CNNs). The performance of
CNN-based models is often compromised by the ﬁxed shape constraint in batch training. To accommodate this, the in-put images are usually resized and cropped to a ﬁxed shape, causing image quality degradation. To address this, we de-sign a multi-scale image quality Transformer (MUSIQ) to process native resolution images with varying sizes and as-pect ratios. With a multi-scale image representation, our proposed method can capture image quality at different granularities. Furthermore, a novel hash-based 2D spatial embedding and a scale embedding is proposed to support the positional embedding in the multi-scale representation.
Experimental results verify that our method can achieve state-of-the-art performance on multiple large scale IQA datasets such as PaQ-2-PiQ [41], SPAQ [11], and KonIQ-10k [16]. 1 1.

Introduction
The goal of image quality assessment (IQA) is to quantify perceptual quality of images. In the deep learning era, many
IQA approaches [11, 33, 34, 41, 47] have achieved signiﬁ-cant success by leveraging the power of convolutional neu-ral networks (CNNs). However, the CNN-based IQA mod-els are often constrained by the ﬁxed-size input requirement i.e., the input images need to be resized in batch training, or cropped to a ﬁxed shape as shown in Figure 1 (b). This preprocessing is problematic for IQA because images in the wild have varying aspect ratios and resolutions. Resizing and cropping can impact image composition or introduce distortions, thus changing the quality of the image.
To learn IQA on the full-size image, the existing CNN-based approaches use either adaptive pooling or resizing to get a ﬁxed-size convolutional feature map. MNA-CNN [24] 1Checkpoints and code are available at https://github.com/ google-research/google-research/tree/master/musiq
Figure 1. In CNN-based models (b), images need to be resized or cropped to a ﬁxed shape for batch training. However, such prepro-cessing can alter image aspect ratio and composition, thus impact-ing image quality. Our patch-based MUSIQ model (a) can process the full-size image and extract multi-scale features, which aligns with the human visual system. processes a single image in each training batch which is not practical for training on a large dataset. Hosu et al. [15] extracts and stores ﬁxed-size features ofﬂine, which costs additional storage for every augmented image. To keep as-pect ratio, Chen et al. [7] proposes a dedicated convolu-tion to preserve aspect ratio in the convolutional receptive
ﬁeld. Its evaluation veriﬁes the importance of aspect-ratio-preserving (ARP) in the IQA tasks. But it still needs resiz-ing and smart grouping for effective batch training.
In this paper, we propose a patch-based multi-scale im-age quality Transformer (MUSIQ) to bypass the CNN con-straints on ﬁxed input size and predict the quality effectively on the native resolution image as shown in Figure 1 (a).
Transformer [36] is ﬁrst proposed for natural language pro-cessing (NLP) and has recently been studied for various vi-sion tasks [4–6, 10]. Among these, the Vision Transformer (ViT) [10] splits each image into a sequence of ﬁxed-size patches, encodes each patch as a token, and then applies
In
Transformer to the sequence for image classiﬁcation. theory, such kind of patch-based Transformer models can handle arbitrary numbers of patches (up to memory con-straints), and therefore do not require preprocessing the in-put image to a ﬁxed resolution. This motivates us to apply the patch-based Transformer on the IQA tasks with the full-size images as input.
Another aspect for improving IQA models is to imi-tate the human visual system which captures an image in a multi-scale fashion [1]. Previous works [15, 21, 46] have shown the beneﬁt of using multi-scale features ex-tracted from CNN feature maps at different depths. This inspires us to transform the native resolution image into a multi-scale representation, enabling the Transformer’s self-attention mechanism to capture information on both ﬁne-grained detailed patches and coarse-grained global patches.
Besides, unlike the convolution operation in CNNs that has a relatively limited receptive ﬁeld, self-attention can attend to the whole input sequence and it can therefore effectively capture the image quality at different granularities.
However, it is not straightforward to apply the Trans-former on the multi-aspect-ratio multi-scale input. Al-though self-attention accepts arbitrary length of the input sequence, it is permutation-invariant and therefore can-not capture patch location in the image. To mitigate this,
ViT [10] adds ﬁxed-length positional embedding to encode the absolute position of each patch in the image. How-ever, the ﬁxed-length positional encoding fails when the in-put length varies. To solve this issue, we propose a novel hash-based 2D spatial embedding that maps the patch po-sitions to a ﬁxed grid to effectively handle images with ar-bitrary aspect ratios and resolutions. Moreover, since the patch locations at each scale are hashed to the same grid, it aligns spatially close patches at different scales so that the
Transformer model can leverage information across multi-ple scales. In addition to the spatial embedding, a separate scale embedding is further introduced to help the Trans-former distinguish patches coming from different scales in the multi-scale representation.
The main contributions of this paper can be summarized into three-folds:
• We propose a patch-based multi-scale image qual-ity Transformer (MUSIQ), which supports processing full-size input with varying aspect ratios or resolutions, and allows multi-scale feature extraction.
• A novel hash-based 2D spatial embedding and a scale embedding are proposed to support positional encoding in the multi-scale representation, helping the Transformer capture information across space and scales.
• We apply MUSIQ on four large-scale IQA datasets. It consistently achieves the state-of-the-art performance on three technical quality datasets: PaQ-2-PiQ [41],
KonIQ-10k [16], and SPAQ [11], and is on-par with the state-of-the-art on the aesthetic quality dataset
AVA [29]. 2.