Abstract constraints provided by 3D geometry—to associate semantic concepts to image pixels and 3D points.1
The abundance and richness of Internet photos of land-marks and cities has led to significant progress in 3D vi-sion over the past two decades, including automated 3D reconstructions of the world’s landmarks from tourist photos.
However, a major source of information available for these 3D-augmented collections—namely language, e.g., from im-age captions—has been virtually untapped. In this work, we present WikiScenes, a new, large-scale dataset of landmark photo collections that contains descriptive text in the form of captions and hierarchical category names. WikiScenes forms a new testbed for multimodal reasoning involving im-ages, text, and 3D geometry. We demonstrate the utility of WikiScenes for learning semantic concepts over images and 3D models. Our weakly-supervised framework connects images, 3D structure, and semantics—utilizing the strong 1.

Introduction
Internet photos capturing tourist landmarks around the world have driven research in 3D computer vision for over a decade [43, 18, 16, 2, 40, 30]. Diverse photo collections of landmarks are unified by the underlying 3D scene geometry, despite the fact that a scene can look dramatically different from one image to the next due to varying illumination, alter-nating seasons, or special events. This geometric anchoring can be exploited when learning a range of geometry-related vision tasks, such as novel view synthesis [31, 26], single-view depth prediction [25], and relighting [51, 50], that re-quire large amounts of diverse training data. However, prior work on tourist photos of landmarks has focused almost exclusively on lower-level reconstruction tasks, and not on
∗: indicates equal contribution. 1https://www.cs.cornell.edu/projects/babel/
higher-level scene understanding or recognition tasks. 2.