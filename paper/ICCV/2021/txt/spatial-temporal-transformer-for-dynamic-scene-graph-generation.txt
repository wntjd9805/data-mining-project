Abstract
Dynamic scene graph generation aims at generating a scene graph of the given video. Compared to the task of scene graph generation from images, it is more chal-lenging because of the dynamic relationships between ob-jects and the temporal dependencies between frames allow-ing for a richer semantic interpretation. In this paper, we propose Spatial-temporal Transformer (STTran), a neural network that consists of two core modules: (1) a spatial en-coder that takes an input frame to extract spatial context and reason about the visual relationships within a frame, and (2) a temporal decoder which takes the output of the spatial encoder as input in order to capture the temporal dependencies between frames and infer the dynamic rela-tionships. Furthermore, STTran is flexible to take varying lengths of videos as input without clipping, which is espe-cially important for long videos. Our method is validated on the benchmark dataset Action Genome (AG). The ex-perimental results demonstrate the superior performance of our method in terms of dynamic scene graphs. More-over, a set of ablative studies is conducted and the effect of each proposed module is justified. Code available at: https://github.com/yrcong/STTran. 1.

Introduction
A scene graph is a structural representation that sum-maries objects of interest as nodes and their relationships as edges [26, 29]. Recently, scene graphs have been suc-cessfully applied in different vision tasks, such as image retrieval [26, 46], object detection, semantic segmenta-tion, human-object interaction [15], image synthesis [24, 3], and high-level vision-language tasks like image captioning
[13, 62] or visual question answering (VQA) [25].
It is treated as a promising approach towards holistic scene un-derstanding and a bridge connecting the large gap between vision and natural language domains. Therefore, the task of scene graph generation has caught increasing attention in communities.
While the great progress made in scene graph genera-tion from a single image (static scene graph generation), the
Figure 1: The difference between scene graph generation from image and video. In the video, the person is watch-ing TV and drinking water from the bottle. Dynamic Scene graph generation can utilize both spatial context and tem-poral dependencies (3rd row) compared with image-based scene graph generation (2nd row). Nodes in different colors denote objects (person,bottle,tv) in the frames. task of scene graph generation from a video (dynamic scene graph generation) is new and more challenging. The most popular approach of static scene graph generation is built upon an object detector that generates object proposals, and then infers their relationship types as well as their object classes. However, objects are not sure to be consistent in each frame of the video sequence and the relationships be-tween any two objects may vary because of their motions, which is characterized by dynamic. In this case, temporal dependencies play a role, and thus, the static scene graph generation methods are not directly applicable to dynamic scene graph generation, which has been fully discussed in
[22] and verified by the experimental results analyzed in
Sec. 4. Fig. 1 showcases the difference between scene graph generation from image and video.
Action recognition is an alternative to detect the dynamic
relationships between objects. However, actions and activ-ities are typically regarded as monolithic events that occur in videos in action recognition [4, 27, 47, 33]. It has been studied in Cognitive Science and Neuroscience that people perceive an ongoing activity by segmenting them into con-sistent groups and encoding into a hierarchical part structure
[30]. Let’s take the activity ”drinking water” as an example, as shown in Fig. 1. The person starts this activity by holding the bottle in front of her, and then holds it up and takes wa-ter. More complex, the person is looking at the television at the same time. Decomposition of this activity is useful for understanding how it happens and what is going on. Associ-ating with the scene graph, it is possible to predict what will happen: after the person picks up the bottle in front of her, we can predict that the person is likely to drink water from it. Representing temporal events with structured representa-tions, i.e. dynamic scene graph, could lead to more accurate and grounded action understanding. However, most of the existing methods for action recognition are not able to de-compose the activity in this way.
In this paper, we explore how to generate a dynamic scene graph from sequences effectively. The main contri-butions are summarized as: (1) We propose a novel frame-work, Spatial-Temporal Transformer (STTran), which en-codes the spatial context within single frames and decodes visual relationship representations with temporal dependen-cies across frames. (2) Distinct from the majority of related works, multi-label classification is applied in relationship prediction and a new strategy to generate a dynamic scene (3) With graph with confident predictions is introduced. several experiments, we verify that temporal dependencies have a positive effect on relationship prediction and our model improves performance by understanding it. STTran achieves state-of-the-art results on Action Genome [22]. 2.