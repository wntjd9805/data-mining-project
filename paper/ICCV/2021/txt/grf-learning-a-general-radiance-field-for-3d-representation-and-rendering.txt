Abstract
We present a simple yet powerful neural network that im-plicitly represents and renders 3D objects and scenes only from 2D observations. The network models 3D geometries as a general radiance field, which takes a set of 2D images with camera poses and intrinsics as input, constructs an in-ternal representation for each point of the 3D space, and then renders the corresponding appearance and geometry of that point viewed from an arbitrary position. The key to our approach is to learn local features for each pixel in 2D images and to then project these features to 3D points, thus yielding general and rich point representations. We addi-tionally integrate an attention mechanism to aggregate pixel features from multiple 2D views, such that visual occlusions are implicitly taken into account. Extensive experiments demonstrate that our method can generate high- quality and realistic novel views for novel objects, unseen categories and challenging real- world scenes. 1.

Introduction
Understanding the precise 3D structure of a real- world environment and realistically re- rendering it from free view-points is a key enabler for many critical tasks, ranging from robotic manipulation to augmented reality. Classic approaches to recover the 3D geometry mainly include the structure from motion (SfM) [34] and simultaneous local-ization and mapping (SLAM) [3] pipelines. However, they can only reconstruct sparse and discrete 3D point clouds which are unable to contain geometric details.
The recent advances in deep neural networks have yielded rapid progress in 3D modeling. Most of them fo-cus on the explicit 3D shape representations such as voxel grids [7], point clouds [10], and triangle meshes [50]. How-ever, these representations are discrete and sparse, limiting the recovered 3D structures to extremely low spatial resolu-tion. In addition, these networks usually require large- scale 3D shapes for supervision, resulting in the trained models over- fitting particular datasets and lacking generalization to novel geometries.
Seen
Categories
Unseen
Categories
GRF
Novel View Rendering
Figure 1: A single model of our GRF infers high- quality novel views for new objects of seen and unseen categories, demonstrating its strong capability for 3D representation and rendering.
Encoding geometries into multilayer perceptrons (MLPs) [27, 35] recently emerges as a promising direction in 3D reconstruction from 2D images. Its key advantage is the ability to model 3D structures continuously instead of discretely, achieving unlimited spatial resolution in theory.
However, many of these methods require 3D geometry for supervision to learn the 3D shapes from images. By introducing a recurrent neural network based renderer,
SRNs [45] is among the early work to learn implicit surface representations only from 2D images, but it renders over- smoothed images without details. Alternatively, by leveraging the volume rendering to synthesize new views with 2D supervision, the very recent NeRF [29] directly encodes the 3D structure into a radiance field via MLPs, achieving an unprecedented level of fidelity.
Nevertheless, NeRF has two major limitations: 1) since 3D content is encoded into the weights of an MLP, the trained network (i.e., a learned radiance field) can only rep-resent a single structure, and is unable to generalize across novel geometries; and 2) because the shape and appearance of each spatial 3D location along a light ray is only opti-mized by individual pixel RGBs, the learned representa-tions of that location do not have rich geometric patterns, resulting in less photo- realistic rendered images.
In this paper, we propose a general radiance field (GRF), a simple yet powerful neural network that builds upon NeRF
[29], overcoming these two limitations. Our GRF takes a
set of 2D images with camera poses and intrinsics, a 3D query point, and its query viewpoint (i.e., the camera loca-tion xyz) as input, and predicts the RGB value and volumet-ric density at that query point. Our network learns to repre-sent 3D content from sparse 2D observations, and to infer shape and appearance from previously unobserved viewing angles. Note that the inferred shape and appearance of any particular 3D query point explicitly takes into account its local geometry from the available 2D observations. In par-ticular, our proposed GRF consists of four components:
• Extracting general 2D visual features for every light ray from the input 2D observations;
• Reprojecting the corresponding 2D features back to the query 3D point using multi-view geometry;
• Aggregating all reprojected features of the query point with attention, where visual occlusions are implicitly con-sidered;
• Rendering the aggregated features of the query 3D point along a particular query viewpoint, and producing the cor-responding RGB and volumetric density via NeRF [29].
These four components enable our GRF to distinguish it-self from existing approaches: 1) Compared with the classic
SfM/SLAM systems, our GRF can represent the 3D con-tent with continuous surfaces; 2) Compared with most ap-proaches based on voxel grids, point clouds and meshes, our GRF learns 3D representations without requiring 3D data for training; and 3) Compared with the existing im-plicit representation methods such as SDF [35], SRNs [45] and NeRF [29], our GRF can represent diverse 3D contents from 2D views with strong generalization to novel geome-tries. In addition, the learned 3D representations carefully consider the general geometric patterns for every 3D spatial location, allowing the rendered views to be exceptionally re-alistic with fine-grained details. Figure 1 shows qualitative results of our GRF which infers high-quality novel views for new objects of both seen and unseen categories. Our key contributions are:
• We propose a general radiance field to represent 3D struc-tures and appearances from 2D images. It has strong gen-eralization to novel geometries in a single forward pass.
• We integrate multi-view geometry and an attention mech-anism to learn general geometric local patterns for each 3D query point along every query light ray. This allows the synthesized 2D views to be superior.
• We demonstrate significant improvement over baselines on large-scale datasets and provide intuition behind our design choices through extensive ablation studies.
We note that some concurrent works such as pixelNeRF
[57], IBRNet [51], SRF [5] and ShaRF [38] share the similar idea with GRF. The key difference is that we use geometry-aware attention module to combine the general 2D local features from multi-views, so that visual occlu-sions can be effectively addressed for better generalization. 2.