Abstract
EV+2
EV-2
EV+0
EV+2
High dynamic range (HDR) video reconstruction from sequences captured with alternating exposures is a very challenging problem. Existing methods often align low dy-namic range (LDR) input sequence in the image space using optical ﬂow, and then merge the aligned images to produce
HDR output. However, accurate alignment and fusion in the image space are difﬁcult due to the missing details in the over-exposed regions and noise in the under-exposed regions, resulting in unpleasing ghosting artifacts. To en-able more accurate alignment and HDR fusion, we intro-duce a coarse-to-ﬁne deep learning framework for HDR video reconstruction. Firstly, we perform coarse alignment and pixel blending in the image space to estimate the coarse
HDR video. Secondly, we conduct more sophisticated align-ment and temporal fusion in the feature space of the coarse
HDR video to produce better reconstruction. Considering the fact that there is no publicly available dataset for quan-titative and comprehensive evaluation of HDR video recon-struction methods, we collect such a benchmark dataset, which contains 97 sequences of static scenes and 184 test-ing pairs of dynamic scenes. Extensive experiments show that our method outperforms previous state-of-the-art meth-ods. Our code and dataset can be found at https:// guanyingc.github.io/DeepHDRVideo. 1.

Introduction
Compared with low dynamic range (LDR) images, high dynamic range (HDR) images can better reﬂect the visual details of a scene in both bright and dark regions. Al-though signiﬁcant progress has been made in HDR image reconstruction using multi-exposure images [22, 57, 59], the more challenging problem of HDR video reconstruction is still less explored. Different from HDR image reconstruc-tion, HDR video reconstruction has to recover the HDR for every input frame (see Fig. 1), but not just for a single
...
...
] 3 2
[ i r a t n a l a
K s r u
O
Figure 1. HDR video reconstruction from sequences captured with three alternating exposures. Row 1 shows four input LDR frames.
Rows 2–3 are the reconstructed (tonemapped) HDR frames. reference frame (e.g., the middle exposure image). Exist-ing successful HDR video reconstruction techniques often rely on costly and specialized hardware (e.g., scanline ex-posure/ISO or internal/external beam splitter) [55, 30, 62], which hinders their wider applications among ordinary con-sumers. A promising direction for low-cost HDR video re-construction is to utilize video sequences captured with al-ternating exposures (e.g., videos with a periodic exposure of {EV-3, EV+3, EV-3, . . . }). This is practical as many off-the-shelf cameras can alternate exposures during recording.
Conventional reconstruction pipeline along this direc-tion often consists of two steps [25].
In the ﬁrst step, neighboring frames with different exposures are aligned to the current frame using optical ﬂow.
In the second step, the aligned images are fused to produce the HDR image. However, accurate alignment and fusion are difﬁ-cult to achieve for LDR images with different exposures as there are saturated pixel values in the over-exposed regions, and noise in the under-exposed regions. Recently, Kalan-tari and Ramamoorthi [23] proposed to estimate the opti-cal ﬂow with a deep neural network, and used another net-1
work to predict the fusion weights for merging the aligned images. Although improved results over traditional meth-ods [24, 38, 25, 32] have been achieved, their method still relies on the accuracy of optical ﬂow alignment and pixel blending, and suffers from ghosting artifacts in regions with large motion (see the second row of Fig. 1). It remains a challenging problem to reconstruct ghost-free HDR videos from sequences with alternating exposures.
Recently, deformable convolution [8] has been suc-in video super-cessfully applied to feature alignment resolution [56, 54]. However, they are not tailored for LDR images with different exposures. Motivated by the obser-vation that accurate image alignment between LDR images with different exposures is difﬁcult, and the success of de-formable feature alignment for videos with constant expo-sure, we introduce a two-stage coarse-to-ﬁne framework for this problem. The ﬁrst stage, denoted as CoarseNet, aligns images using optical ﬂow in the image space and blends the aligned images to reconstruct the coarse HDR video.
This stage can recover/remove a large part of missing de-tails/noise from the input LDR images, but there exist some artifacts in regions with large motion. The second stage, de-noted as ReﬁneNet, performs more sophisticated alignment and fusion in the feature space of the coarse HDR video using deformable convolution [8] and temporal attention.
Such a two-stage approach avoids the need of estimating highly accurate optical ﬂow from images with different ex-posures, and therefore reduces the learning difﬁculty and removes ghosting artifacts in the ﬁnal results.
As there is no publicly available real-world video dataset with ground-truth HDR for evaluation, comprehensive com-parisons among different methods are difﬁcult to achieve.
To alleviate this problem, we create a real-world dataset containing both static and dynamic scenes as a benchmark for quantitative and qualitative evaluation.
In summary, the key contributions of this paper are as follows:
• We propose a two-stage framework, which ﬁrst per-forms image alignment and HDR fusion in the image space and then in feature space, for HDR video recon-struction from sequences with alternating exposures.
• We create a real-world video dataset captured with al-ternating exposures as a benchmark to enable quanti-tative evaluation for this problem.
• Our method achieves state-of-the-art results on both synthetic and real-world datasets. 2.