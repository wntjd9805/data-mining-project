Abstract
Deep CNN-based methods have so far achieved the state of the art results in multi-view 3D object reconstruction.
Despite the considerable progress, the two core modules of these methods - view feature extraction and multi-view fu-sion, are usually investigated separately, and the relations among multiple input views are rarely explored. Inspired by the recent great success in Transformer models, we re-formulate the multi-view 3D reconstruction as a sequence-to-sequence prediction problem and propose a framework named 3D Volume Transformer. Unlike previous CNN-based methods using a separate design, we unify the feature extraction and view fusion in a single Transformer network.
A natural advantage of our design lies in the exploration of view-to-view relationships using self-attention among mul-tiple unordered inputs. On ShapeNet - a large-scale 3D re-construction benchmark, our method achieves a new state-of-the-art accuracy in multi-view reconstruction with fewer parameters (70% less) than CNN-based methods. Experi-mental results also suggest the strong scaling capability of our method. Our code will be made publicly available. 1.

Introduction
Learning 3D object representation from multi-view im-ages is a fundamental and challenging problem in 3D mod-eling, virtual reality, and computer animation. Recently, deep learning approaches have greatly promoted the re-search in multi-view 3D reconstruction, where the deep convolutional neural network (CNN) based approaches have so far achieved the state of the art results in this task
[26, 28, 27].
To learn effective 3D representation from multiple in-put views, most recent CNN-based approaches follow the design principle of divide-and-conquer, where a common practice is to introduce a CNN for single-view feature ex-traction and multi-view fusion for integrating the features
*Corresponding Author. Email:xinruic@ece.ubc.ca or reconstruction results from multiple views. Despite the strong connection between the two modules, their method-ology designs are investigated separately. Also, during the
CNN feature extraction stage that processes each view im-age separately, the relations in different views are rarely ex-plored. Although some recent approaches have introduced recurrent neural network (RNN) to learn object relation-ships among different views [5, 12], such a design lacks computational efficiency, and the input views of the RNN model are permutation-sensitive [22], which is not compat-ible with a set of unordered input views. It is also shown in recent researches that CNN-based reconstruction meth-ods may suffer from the model scaling problem. For exam-ple, when the number of input views exceeds a particular scale (e.g., 4), the accuracy of methods will be saturated, showing the difficulty of learning complementary knowl-edge from a large set of independent CNN feature extraction units [28, 27].
Considering the above challenges, we propose a new framework named “3D Volume Transformer” and explore the potential of the self-attention-based Transformer model for the multi-view 3D object reconstruction task. We refor-mulate the multi-view 3D reconstruction as a sequence-to-sequence prediction problem and unify the feature extrac-tion and view fusion in a single Transformer network. On the one hand, in multi-view 3D reconstruction, we need to learn the underlying 3D representation by exploring the re-lationships among multiple input views since we can only see part of the 3D structure from a particular view. On the other hand, in a Transformer model, the self-attention mechanism has recently shown its great power in learning complex semantic abstractions within an arbitrary number of input tokens [6, 20] and is naturally suitable for explor-ing the view-to-view relationships of a 3D object’s differ-ent semantic parts. Given all this, the structure of Trans-former [21, 8] becomes a natural and attractive solution for the multi-view 3D reconstruction.
Our Transformer-based framework contains a 2D-view
Transformer encoder and a 3D-volume Transformer de-coder, as presented in Figure 1. The 2D-view Transformer
encoder encodes and fuses multiple 2D-view information by exploring their “2D-view × 2D-view” relationships of different inputs. The 3D-volume Transformer decoder de-codes and fuses the multi-view features from the encoder and generates a 3D probabilistic voxel output for each spa-tial query token. The attention layers in the decoder learn
“2D-view × 3D-volume” relationships between each of the output voxel grids and input views. Meanwhile, volume at-tention layers in the decoder further learn “3D-volume × 3D-volume” relationships by exploiting correlations among different 3D locations. By using the above unified design, the “2D-view × 2D-view”, “3D-volume × 3D-volume”, and “2D-view × 3D-volume” relationships can be jointly explored by multiple attention layers in both the encoder and decoder networks.
Based on the above encoder-decoder Transformer struc-ture design, we further investigate the “attention unifor-mity” problem in a Transformer model and propose an ef-fective solution for enhancing the effectiveness of a Trans-former model in the multi-view reconstruction task.
In
Transformers, self-attention possesses a solid inductive bias towards “token uniformity” [7], which encourages feature representations of input tokens to converge. However, this convergence may further cause the problem of “attention uniformity” in deeper layers, which makes a Transformer model loses expressive power speedily with respect to net-work depth [7]. We show that this problem is particularly prominent in the multi-view 3D reconstruction task and will limit the Transformer’s capability to explore and abstract multi-view associations at a deeper level. To tackle it, we further propose the divergence-enhanced Transformer that can slow down the divergence decay in self-attention lay-ers by enhancing the discrepancy of the embeddings from different views.
The contributions can be summarized as follows:
• We propose a brand new Transformer-based frame-work for multi-view 3D object reconstruction. Dif-ferent from the previous CNN-based methods that use a separate design of feature extraction + view fu-sion, we unify these two stages into a single Trans-former network and re-frame the 3D reconstruction as a “sequence-to-sequence” prediction problem.
• The proposed method can jointly and naturally explore multi-level correspondence and associations between the 2D input views and 3D output volume within our encoder-decoder Transformer structure.
• We investigate the problem of “divergence decay” in the proposed framework and propose a view-divergence enhancing operation in our self-attention layers to avoid such degradation.
• Our method achieves a new state-of-the-art for multi-view 3D reconstruction on ShapeNet with only 30% amount of parameters of recent CNN-based methods.
Our method also shows better scaling capability on the number of input views. 2.