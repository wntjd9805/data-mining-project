Abstract iCV-MEFED (Emotion Target) 
Happy 
Sad 
Angry 
The successful deployment of artiﬁcial intelligence (AI) in many domains from healthcare to hiring requires their responsible use, particularly in model explanations and privacy. Explainable artiﬁcial intelligence (XAI) provides more information to help users to understand model de-cisions, yet this additional knowledge exposes additional risks for privacy attacks. Hence, providing explanation harms privacy. We study this risk for image-based model inversion attacks and identiﬁed several attack architectures with increasing performance to reconstruct private image data from model explanations. We have developed several multi-modal transposed CNN architectures that achieve sig-niﬁcantly higher inversion performance than using the tar-get model prediction only. These XAI-aware inversion mod-els were designed to exploit the spatial knowledge in im-age explanations. To understand which explanations have higher privacy risk, we analyzed how various explana-tion types and factors inﬂuence inversion performance. In spite of some models not providing explanations, we further demonstrate increased inversion performance even for non-explainable target models by exploiting explanations of sur-rogate models through attention transfer. This method ﬁrst inverts an explanation from the target prediction, then re-constructs the target image. These threats highlight the ur-gent and signiﬁcant privacy risks of explanations and calls attention for new privacy preservation techniques that bal-ance the dual-requirement for AI explainability and privacy. 1.

Introduction
The recent success of artiﬁcial intelligence (AI) is driv-ing its application in many domains from healthcare to hir-ing. With the increasing regulatory requirements for re-sponsible AI, these applications will need to support both explainability to justify important decisions and privacy to protect personal information [36]. Indeed, many AI applica-tions have this dual-requirement, such as driver drowsiness detection from faces [13, 35] that can be used to limit the
*Corresponding author.
"#!   9.9E-01 3.3E-03 7.7E-03 1.8E-04 1.7E-03 5.1E-04 6.9E-08
%&!  
!$"  
!$"  
"#!   6.5E-05 5.4E-04 7.1E-04 1.0E+00 1.5E-04 8.7E-05 9.1E-07
%&!  
!$"  
!$"  
"#!   4.5E-04 9.3E-07 2.9E-05 2.8E-08 1.0E+00 2.5E-06 3.0E-05
%&!  
!$"  
!$"   a  b  c  d 
Original  (!) 
Prediction  only 
+ Flatten 
+ U-Net  (LRP) 
+ Flatten 
+ U-Net  (Gradient) 
+ Flatten 
+U-Net  (CAM) 
+ Flatten 
+U-Net  (Σ-CAMs) 
+ Flatten 
+U-Net  (∂-CAMs) 
Figure 1. Demonstration of image reconstruction from XAI-aware inversion attack with emotion prediction as target task, and face re-construction as attack task. Three emotions from a single identity shown from the iCV-MEFED dataset [28]. Reconstructions shown with corresponding inputs (target prediction ˜yt and explanations
˜Et as LRP [3],Gradients [43] or Grad-CAM [39] saliency maps).
Towards original images (a), reconstructions from Prediction only (b) are poor and similar across different faces, and is signiﬁcantly improved when exploiting single (c) and multiple (d) explanations.
Demonstration reconstructions for other explanations and baseline inversion models are shown in supplementary materials. jobs of tired drivers, and classroom facial engagement pre-diction [6, 49] for student grading or teacher performance evaluation. Users need explanations to dispute or rectify unfavorable model predictions, and need privacy to pre-serve anonymity or reputation (e.g., of embarrassing ap-pearances). Troublingly, using state-of-the-art model in-version attacks [11, 50, 54], attackers can reconstruct sen-sitive information (e.g., faces) merely from model predic-tions. We hypothesize that since explanations provide more information, they can be used by attackers for more effec-                                                                                                                                                                                                                                                             
tive reconstructions, and further harm privacy — providing explanations harms privacy.
Explainable artiﬁcial intelligence (XAI) provides infor-mation to help users to debug models [23], improve deci-sion making [47], and improve and moderate trust in au-tomation [10]. Speciﬁc to image-based deep learning, XAI techniques include saliency maps [7, 34, 43, 39, 56], feature visualization [4, 16, 32], and activations of neurons [18] and concept vectors [19, 20]. These explanations provide users with deeper insights into model reasoning and about the data, but they can contain sensitive knowledge that can be exploited for privacy attacks, especially with recent ma-chine learning (ML)-based attacks [41, 46, 11]. We focus on model inversion attacks that can reconstruct data from model predictions [11, 15, 50, 54]. These methods typi-cally require some privileged white-box access (to model parameters) or background knowledge (such as blurred im-ages) for effective attack, but this is unrealistic or uncom-mon in deployed settings. Instead, because of the require-ment for explainability, model explanations will be more readily available and pose a more ubiquitous threat.
Recent work has begun to study the privacy threats of model explanations, but their attack goals differ and their use of explanations remains under-studied. Instead of at-tacking training data [41] or model conﬁdentiality [2, 30], we focus on attacking the privacy of test instances. This will compromise the trust of active users of the model. We pro-pose attack methods based on the structure and generation technique of explanations, focusing on saliency maps that are highly popular for image-based AI. Furthermore, we propose a method of leveraging on XAI techniques with at-tention transfer to perform inversion attacks on models that do not share explanations. We found that this performance is close to inverting an explainable target model. Hence, even non-explainable target models are at increased risk of
XAI-aware inversion attacks.
Our contributions are: 1) We determine the privacy threat of model explanations for model inversion attacks.
We achieved signiﬁcantly higher inversion performance by proposing several architectures for XAI-aware model in-version, through careful adapting of multi-modal, spatially-aware transposed CNNs. This highlights that the present privacy risk of providing explanations is signiﬁcant (see
Figure 1). 2) We propose an XAI-aware attack on non-explainable target models that can achieve improved in-version performance using an explanation inversion model with surrogate explanation attention transfer. This does not need additional data leak or sharing from the target model, and demonstrates a signiﬁcantly increased risk due to expla-nations in surrogate models instead of the target model. 3)
We identify the privacy risk for different explanation types (Gradients [43], Grad-CAM [39], and layer-wise relevance propagation (LRP) [3]) and analyze how various factors in-ﬂuence attack performance.
In this work, we provide the ﬁrst study into the privacy risk of explanations for inversion attack, and defer their de-fense to future work. This highlights the urgency to de-velop privacy defense techniques and models that are co-optimized for the dual-requirements of explainability and privacy to achieve the objectives of Responsible AI. 2.