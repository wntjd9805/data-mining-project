Abstract
Anomaly detection with weakly supervised video-level labels is typically formulated as a multiple instance learn-ing (MIL) problem, in which we aim to identify snippets con-taining abnormal events, with each video represented as a bag of video snippets. Although current methods show ef-fective detection performance, their recognition of the pos-itive instances, i.e., rare abnormal snippets in the abnor-mal videos, is largely biased by the dominant negative in-stances, especially when the abnormal events are subtle anomalies that exhibit only small differences compared with normal events. This issue is exacerbated in many meth-ods that ignore important video temporal dependencies. To address this issue, we introduce a novel and theoretically sound method, named Robust Temporal Feature Magnitude learning (RTFM), which trains a feature magnitude learn-ing function to effectively recognise the positive instances, substantially improving the robustness of the MIL approach to the negative instances from abnormal videos. RTFM also adapts dilated convolutions and self-attention mechanisms to capture long- and short-range temporal dependencies to learn the feature magnitude more faithfully. Extensive ex-periments show that the RTFM-enabled MIL model (i) out-performs several state-of-the-art methods by a large margin on four benchmark data sets (ShanghaiTech, UCF-Crime,
XD-Violence and UCSD-Peds) and (ii) achieves signiﬁ-cantly improved subtle anomaly discriminability and sam-ple efﬁciency. 1.

Introduction
Video anomaly detection has been intensively studied because of its potential to be used in autonomous surveil-lance systems [15, 56, 66, 78]. The goal of video anomaly detection is to identify the time window when an anoma-lous event happened – in the context of surveillance, ex-amples of anomaly are bullying, shoplifting, violence, etc.
Figure 1. RTFM trains a feature magnitude learning function to improve the robustness of MIL approaches to normal snippets from abnormal videos, and detect abnormal snippets more ef-fectively. Left: temporal feature magnitudes of abnormal and normal snippets ((cid:107)x+(cid:107) and (cid:107)x−(cid:107)), from abnormal and normal videos (X+ and X−). Assuming that µ = 3 denotes the num-ber of abnormal snippets in the anomaly video, we can maximise the ∆score(X+, X−), which measures the difference between the scores of abnormal and normal videos, by selecting the top k ≤ µ snippets with the largest temporal feature magnitude (the scores are computed with the mean of magnitudes of the top k snippets).
Right: the ∆score(X+, X−) increases with k ∈ [1, µ] and then decreases for k > µ, showing evidence that our proposed RTFM-enabled MIL model provides a better separation between abnormal and normal videos when k ≈ µ, even if there are a few normal snippets with large feature magnitudes.
Although one-class classiﬁers (OCCs, also called unsuper-vised anomaly detection) trained exclusively with normal videos have been explored in this context [15, 17, 27, 30, 46, 47, 76], the best performing approaches explore a weakly-supervised setup using training samples with video-level la-bel annotations of normal or abnormal [56, 66, 78]. This weakly-supervised setup targets a better anomaly classiﬁ-cation accuracy at the expense of a relatively small human annotation effort, compared with OCC approaches.
One of the major challenges of weakly supervised anomaly detection is how to identify anomalous snippets from a whole video labelled as abnormal. This is due to two reasons, namely: 1) the majority of snippets from an abnor-mal video consist of normal events, which can overwhelm
the training process and challenge the ﬁtting of the few ab-normal snippets; and 2) abnormal snippets may not be suf-ﬁciently different from normal ones, making a clear sepa-ration between normal and abnormal snippets challenging.
Anomaly detection trained with multiple-instance learning (MIL) approaches [56,66,74,80] mitigates the issues above by balancing the training set with the same number of ab-normal and normal snippets, where normal snippets are ran-domly selected from the normal videos and abnormal snip-pets are the ones with the top anomaly scores from abnor-mal videos. Although partly addressing the issues above,
MIL introduces four problems: 1) the top anomaly score in an abnormal video may not be from an abnormal snippet; 2) normal snippets randomly selected from normal videos may be relatively easy to ﬁt, which challenges training con-vergence; 3) if the video has more than one abnormal snip-pet, we miss the chance of having a more effective train-ing process containing more abnormal snippets per video; and 4) the use of classiﬁcation score provides a weak train-ing signal that does not necessarily enable a good separa-tion between normal and abnormal snippets. These issues are exacerbated even more in methods that ignore important temporal dependencies [27, 30, 66, 78].
To address the MIL problems above, we propose a novel method, named Robust Temporal Feature Magnitude (RTFM) learning. In RTFM, we rely on the temporal feature magnitude of video snippets, where features with low mag-nitude represent normal (i.e., negative) snippets and high magnitude features denote abnormal (i.e., positive)) snip-pets. RTFM is theoretically motivated by the top-k instance
MIL [24] that trains a classiﬁer using k instances with top classiﬁcation scores from the abnormal and normal videos, but in our formulation, we assume that the mean feature magnitude of abnormal snippets is larger than that of nor-mal snippets, instead of assuming separability between the classiﬁcation scores of abnormal and normal snippets [24].
RTFM solves the MIL issues above, as follows: 1) the probability of selecting abnormal snippets from abnormal videos increases; 2) the hard negative normal snippets se-lected from the normal videos will be harder to ﬁt, im-proving training convergence; 3) it is possible to include more abnormal snippets per abnormal video; and 4) using feature magnitude to recognise positive instances is advan-tageous compared to MIL methods that use classiﬁcation scores [24, 56], because it enables a stronger learning sig-nal, particularly for the abnormal snippets that have a mag-nitude that can increase for the whole training process, and the feature magnitude learning can be jointly optimised with the MIL anomaly classiﬁcation to enforce large margins between abnormal and normal snippets at both the feature representation space and the anomaly classiﬁcation output space. Fig. 1 motivates RTFM, showing that the selection of the top-k features (based on their magnitude) can provide a better separation between abnormal and normal videos, when we have more than one abnormal snippet per abnor-mal video and the mean snippet feature magnitude of ab-normal videos is larger than that of normal videos.
In practice, RTFM enforces large margins between the top k snippet features with largest magnitudes from abnor-mal and normal videos, which has theoretical guarantees to maximally separate abnormal and normal video repre-sentations. These top k snippet features from normal and abnormal videos are then selected to train a snippet classi-ﬁer. To seamlessly incorporate long and short-range tempo-ral dependencies within each video, we combine the learn-ing of long and short-range temporal dependencies with a pyramid of dilated convolutions (PDC) [69] and a temporal self-attention module (TSA) [65]. We validate our RTFM on four anomaly detection benchmark data sets, namely
ShanghaiTech [27], UCF-Crime [56], XD-Violence [66] and UCSD-Peds [23]. We show that our method outper-forms the current SOTAs by a large margin on all bench-marks using different pre-trained features (i.e., C3D and
I3D). We also show that our method achieves substantially better sample efﬁciency and subtle anomaly discriminabil-ity than popular MIL methods. 2.