Abstract
Vision Transformer (ViT) extends the application range of transformers from language processing to computer vi-sion tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effective-ness on transformer-based architecture. We particularly at-tend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel di-mension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel
Pooling-based Vision Transformer (PiT) upon the origi-nal ViT model. We show that PiT achieves the improved model capability and generalization performance against
ViT. Throughout the extensive experiments, we further show
PiT outperforms the baseline on several tasks such as im-age classification, object detection, and robustness evalua-tion. Source codes and ImageNet models are available at https://github.com/naver-ai/pit. 1.

Introduction
The architectures based on the self-attention mechanism have achieved great success in the field of Natural Lan-guage Processing (NLP) [34]. There have been attempts to utilize the self-attention mechanism in computer vision.
Non-local networks [37] and DETR [4] are representative works, showing that the self-attention mechanism is also ef-fective in video classification and object detection tasks, re-spectively. Recently, Vision Transformer (ViT) [9], a trans-former architecture consisting of self-attention layers, has been proposed to compete with ResNet [13], and shows that it can achieve the best performance without convolution op-*Work done as a research scientist at NAVER AI Lab. eration on ImageNet [8]. As a result, a new direction of net-work architectures based on self-attention mechanism, not convolution operation, has emerged in computer vision.
ViT is quite different from convolutional neural networks (CNN). Input images are divided into 16Ã—16 patches and fed to the transformer network; except for the first embed-ding layer, there is no convolution operation in ViT, and the position interactions occur only through the self-attention layers. While CNNs have restricted spatial interactions, ViT allows all the positions in an image to interact through trans-former layers. Although ViT is an innovative architecture and has proven its powerful image recognition ability, it fol-lows the transformer architecture in NLP [34] without any changes. Some essential design principles of CNNs, which have proved to be effective in the computer vision domain over the past decade, are not sufficiently reflected. We thus revisit the design principles of CNN architectures and in-vestigate their efficacy when applied to ViT architectures.
CNNs start with a feature of large spatial sizes and a small channel size and gradually increase the channel size while decreasing the spatial size. This dimension conver-sion is indispensable due to the layer called spatial pool-ing. Modern CNN architectures, including AlexNet [21],
ResNet [13], and EfficientNet [32], follow this design prin-ciple. The pooling layer is deeply related to the receptive field size of each layer. Some studies [6, 26, 5] show that the pooling layer contributes to the expressiveness and gen-eralization performance of the network. However, unlike the
CNNs, ViT does not use a pooling layer and uses the same spatial dimension for all layers.
First, we verify the advantages of dimensions configu-rations on CNNs. Our experiments show that ResNet-style dimensions improve the model capability and generaliza-tion performance of ResNet. To extend the advantages to
ViT, we propose a Pooling-based Vision Transformer (PiT).
PiT is a transformer architecture combined with a newly de-signed pooling layer. It enables the spatial size reduction in the ViT structure as in ResNet. We also investigate the ben-efits of PiT compared to ViT and confirm that ResNet-style dimension setting also improves the performance of ViT.
Finally, to analyze the effect of PiT compared to ViT, we
Figure 1. Schematic illustration of dimension configurations of networks. We visualize ResNet50 [13], Vision Transformer (ViT) [9], and our Pooling-based Vision Transformer (PiT); (a) ResNet50 gradually downsamples the features from the input to the output; (b) ViT does not change the spatial dimensions; (c) PiT involves ResNet style spatial dimension into ViT. analyze the attention matrix of transformer block with en-tropy and average distance measure. The analysis shows the attention patterns inside layers of ViT and PiT, and helps to understand the inner mechanism of ViT and PiT.
We verify that PiT improves performances over ViT on various tasks. On ImageNet classification, PiT and outper-forms ViT at various scales and training environments. Ad-ditionally, we have compared the performance of PiT with various convolutional architectures and have specified the scale at which the transformer architecture outperforms the
CNN. We further measure the performance of PiT as a back-bone for object detection. ViT- and PiT-based deformable
DETR [44] are trained on the COCO 2017 dataset [24] and the result shows that PiT is even better than ViT as a back-bone architecture for a task other than image classification.
Finally, we verify the performance of PiT in various envi-ronments through the robustness benchmark. 2.