Abstract
We study the problem of multi-label zero-shot recogni-tion in which labels are in the form of human-object interac-tions (combinations of actions on objects), each image may contain multiple interactions and some interactions do not have training images. We propose a novel compositional learning framework that decouples interaction labels into separate action and object scores that incorporate the spa-tial compatibility between the two components. We combine these scores to efﬁciently recognize seen and unseen inter-actions. However, learning action-object spatial relations, in principle, requires bounding-box annotations, which are costly to gather. Moreover, it is not clear how to generalize spatial relations to unseen interactions. We address these challenges by developing a cross-attention mechanism that localizes objects from action locations and vice versa by predicting displacements between them, referred to as re-lational directions. During training, we estimate the rela-tional directions as ones maximizing the scores of ground-truth interactions that guide predictions toward compatible action-object regions. By extensive experiments, we show the effectiveness of our framework, where we improve the state of the art by 2.6% mAP score and 5.8% recall score on HICO and Visual Genome datasets, respectively.1 1.

Introduction
Multi-label learning is the important yet challenging task of recognizing all labels in an image with applications in human-computer interaction, robotics, assistive technolo-gies and surveillance systems. Due to the high cost of col-lecting training samples for all possible labels, multi-label zero-shot learning aims to recognize unseen labels that do not have training images [1, 2, 3]. However, the majority of existing works have focused on the case where each label is a simple concept (e.g., an object) and have tired to capture inter-label dependencies (e.g., co-occurrences of objects) 1Code is available at https://github.com/hbdat/iccv21_ relational_direction.
Figure 1: Conventional multi-label zero-shot recognition (top) assumes independence between action and object components in interaction labels, hence, cannot distinguish between objects in background (red) and in in-teractions (yellow). Our method (bottom) leverages relational directions to guide predictions toward compatible objects and actions. for more effective recognition. On the other hand, richer representation and description of images require more com-plex labels. Human-object interactions are one such im-portant form of labels, where each label describes an ac-tion performed on an object (e.g., ‘holding cup’ or ‘remov-ing wheel’) [4, 5, 6, 7, 8]. However, existing multi-label learning works ignore intra-label dependency, which is the spatial relation between the action and the object within an interaction label. This leads to a lack of ability to distin-guish between objects in backgrounds and in interactions, see Figure 1 and poses challenges to generalization to un-seen interactions. The work in [9] made the ﬁrst attempt in generalizing multi-label learning to human-object interac-tion (HOI) recognition in the zero-shot setting, where some interactions do not have training images. Our paper makes advances on this task by capturing spatial relations among actions and objects (intra-label dependencies) to enhance seen and unseen interaction recognition without requiring bounding-box of locations of actions and objects in images.
Prior Works and Challenges. Most multi-label learning works take advantage of label correlations to regularize pre-dictions [10, 11, 12, 13]. To further enhance the perfor-mance, [14, 15, 16] use attention mechanisms to extract discriminative visual features of labels. While [17, 18, 19] predict attention regions recurrently, [20, 21, 22, 7] propose
specialized attention modules for object and action labels.
However, these works assume that every label has training samples, therefore cannot generalize to unseen labels.
A few works have addressed multi-label zero-shot recog-nition by exploiting semantic information overlap between seen and unseen labels. [23] proposes a nonlinear embed-ding between visual features and label semantics, while
[24, 2] employ external knowledge in the form of label graphs. Recently, [3] has proposed a shared attention mechanism among labels to effectively learn unseen labels.
While these methods can be extended to address recognition of interaction labels instead of simple labels, they do not capture spatial dependencies between actions and objects in interactions, which as we show leads to low performance.
To handle unseen interactions, [9] proposed to cap-ture image context for HOI prediction. However, the ap-proach ignores discriminative spatial relations between ac-tions and objects, necessary to determine whether objects are being interacted with by actions. Thus, recent works
[25, 26, 27, 28, 29] detect human and object bounding boxes to determine their spatial compatibility. However, these works require expensive bounding-box supervision of hu-mans and objects from seen interaction labels and are difﬁ-cult to scale to thousands of interaction labels.
Paper Contributions. To address the above limitations, we propose a compositional multi-label zero-shot interaction learning framework that incorporates action-object spatial dependencies in interactions and does not require expensive bounding-box annotations. To do so, we propose a cross-attention model that learns relational directions, which are expected displacements between actions and their corre-sponding objects, to measure their compatibilities. Our framework has several advantages over the state of the art: – To learn action-object spatial relations, we design a novel cross-attention mechanism, that estimates distributions of relational directions for localizing objects/actions in inter-actions. Cross-attention is differentiable, which enables its efﬁcient training by backpropagating gradients from inter-action scores without requiring bounding-box annotations.
The object/action scores computed according to relational directions measure the spatial compatibility between object and action in interaction labels. – We use the observation that an action-object spatial con-ﬁguration often depends on the action type. For example, the object location for the action ‘sit’ is below the action location regardless of the object (e.g., chair, bed), see Fig-ure 1. Thus, we condition our cross-attention predictions on action types in each interaction label, which enables gener-alization to unseen labels with similar actions. – Instead of relying on costly bounding-box supervision, we leverage the point-wise localization ability of the visual attention on actions and objects in interaction labels, which can generalize to unseen actions and scale to thousands of labels in the Visual Genome dataset. 2.