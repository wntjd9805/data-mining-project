Abstract
Most existing image retrieval systems use text queries as a way for the user to express what they are looking for. However, ﬁne-grained image retrieval often requires the ability to also express where in the image the content they are looking for is. The text modality can only cumbersomely express such localization preferences, whereas pointing is a more natural ﬁt.
In this paper, we propose an image re-trieval setup with a new form of multimodal queries, where the user simultaneously uses both spoken natural language (the what) and mouse traces over an empty canvas (the where) to express the characteristics of the desired target image. We then describe simple modiﬁcations to an exist-ing image retrieval model, enabling it to operate in this setup. Qualitative and quantitative experiments show that our model effectively takes this spatial guidance into ac-count, and provides signiﬁcantly more accurate retrieval re-sults compared to text-only equivalent systems. 1.

Introduction
Gargantuan amounts of pictures are taken and shared every day, at an ever accelerating pace. Finding the pic-ture that one has in mind should be easier and faster than painfully scrolling through hundreds of pictures in a digital-camera roll. Building effective image retrieval systems for
ﬁnding speciﬁc images among large collections is, there-fore, of paramount importance. To speed the search up, image retrieval systems build an index that represents a collection of images by automatically analyzing their con-tent [83, 53, 21, 66, 43, 62, 70, 71, 17, 37, 44, 39, 12].
A query is a description of what a user is looking for in an image, a translation of their mental model of the tar-get image into a concrete form that can be understood by a retrieval system. At a coarse level, a query can be a list of speciﬁc classes of objects (e.g., cars, people) the user wants to be contained by the target image [67]. At a ﬁner-grained level is a natural language description of its con-tents [70, 71, 17, 37, 44, 39, 12]. The latter is the most common paradigm in the recent literature, partly due to the
Figure 1: Different types of textual queries to represent the what and the where in the target image: (a) spatial information is usually lacking in textual descriptions and (b) it is cumbersome to express in written form, while (c) it is natural using mouse traces synchronized with the text. availability of captioning datasets that can be used as train-ing and testing data [42, 10, 78, 55]. These types of queries generally focus on what is present in the image, but fall short of expressing where in the image the user expects it.
As an example, consider the image in Fig. 1. One textual query could be “A horse in a city, occluding a bike and a car” (Fig. 1a). The retrieved image, while not the one the user had in mind, is a perfect match for this description: the what in the image is similar to the intended target. Express-ing the where part using the textual query is not only cum-bersome for the user to write, but also hard for the retrieval system to process (Fig. 1b).
In this paper, we propose a new query modality where the user describes the characteristics of the desired target image simultaneously using spoken natural language, the what, and mouse traces over an empty canvas, the where (Fig. 1c). Roughly pointing to an object’s location comes naturally to humans [18, 13] and is an effective way of com-municating the image layout the user has in mind. When the localization information is also temporally aligned with the natural language query, it becomes a natural grounding sig-nal that can be exploited to make retrieval more precise.
We propose an image retrieval model that takes this new type of multimodal query as input. We start from an image-Figure 2: Qualitative results: Querying with (a) text and mouse traces, versus (b) only text. The target image is marked in green. Adding mouse traces to express the spatial location of the image content allows us to get a better retrieval result even given the same textual query.
In this particular case, notice that the exact position of the racket and the ball allow the model to detect the correct target image. to-text matching model that is repurposed as an image re-triever by ranking image-text pairs according to their afﬁn-ity, as in previous literature [32, 17, 37, 80]. We then aug-ment the text input to also take the rough position in the blank canvas of each of the words into account (Fig. 4).
The data for training and evaluating such a model comes from Localized Narratives [56], a captioning dataset where annotators describe the images with their voice while simul-taneously moving their mouse over the objects they are de-scribing. The mouse traces are effectively grounding each word of the caption in the image. To use this data in an im-age retrieval scenario, we take the caption and correspond-ing mouse trace as input query, and the image on which the annotation is generated as target image.
Our experimental evaluation shows that this query modality provides a +7% absolute better recall (43% rel-ative error rate decrease) for the top image compared to a model using only text queries. As we show in Fig. 2, hav-ing the rough location of the objects mentioned in the query restricts the space of plausible images and thus allows for more effective retrieval results.
In summary, our main contributions are: (a) A novel query modality for ﬁne-grained image retrieval that allows for a more natural speciﬁcation of localiza-tion preferences. (b) One concrete implementation of this idea that is simple and broadly applicable through a strong transformer-based model capable of incorporating the mouse traces. (c) An experimental setup that suggests that Localized Nar-ratives can be used to measure progress on this task. (d) Empirical image retrieval results that demonstrate sig-niﬁcant accuracy gains when the user is empowered with the ability to point to the where. 2.