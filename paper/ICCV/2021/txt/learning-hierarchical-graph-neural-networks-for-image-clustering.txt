Abstract
We propose a hierarchical graph neural network (GNN) model that learns how to cluster a set of images into an un-known number of identities using a training set of images annotated with labels belonging to a disjoint set of identi-ties. Our hierarchical GNN uses a novel approach to merge connected components predicted at each level of the hierar-chy to form a new graph at the next level. Unlike fully unsu-pervised hierarchical clustering, the choice of grouping and complexity criteria stems naturally from supervision in the training set. The resulting method, Hi-LANDER, achieves an average of 49% improvement in F-score and 7% increase in Normalized Mutual Information (NMI) relative to cur-rent GNN-based clustering algorithms. Additionally, state-of-the-art GNN-based methods rely on separate models to predict linkage probabilities and node densities as interme-diate steps of the clustering process. In contrast, our unified framework achieves a three-fold decrease in computational cost. Our training and inference code are released 1. 1.

Introduction
Clustering is a pillar of unsupervised learning. It con-sists of grouping data points according to a manually speci-fied criterion. Without any supervision, the problem is self-referential, with the outcome being defined by the choice of grouping criterion. Different criteria yield different so-lutions, with no independent validation mechanism. Even within a given criterion, clustering typically yields multiple solutions depending on a complexity measure, and a sep-arate model selection criterion is introduced to arrive at a unique solution. A large branch of unsupervised clustering methods follow the hierarchical/agglomerative framework
[41, 42, 44], which gives a tree of cluster partitions with varying granularity of the data, but they still require a model selection criterion for the final single grouping. Rather than engineering the complexity and grouping criteria, we wish to learn them from data.2 Clearly, this is not the data we wish to cluster, for we do not have any annotations for them. Instead, it is a different set of training data, the meta-training set, for which cluster labels are given, correspond-ing to identities that are disjoint from those expected in the test set. For example, the test set might be an untagged col-lection of photos by a particular user, for which there exists a true set of discrete identities that we wish to discover, say their family members. While those family members have never been seen before, the system has access to different photo collections, tagged with different identities, during training. Our goal is to leverage the latter labeled training set to learn how to cluster different test sets with unknown numbers of different identities. This is closely related to
“open-set” or “open universe” classification [40, 26].
We present the first hierarchical/agglomerative cluster-ing method using Graph Neural Networks (GNNs). GNNs are a natural tool for learning how to cluster [51, 57, 56], as they provide a way of predicting the connectivity of a graph using training data.
In our case, the graph describes the connectivity among test data, with connected components ultimately determining the clusters.
Our hierarchical GNN uses a novel approach to merge connected components predicted at each level of the hier-archy to form a new graph at the next level. We employ a
GNN to predict connectivity at each level, and iterate until convergence. While in unsupervised agglomerative clus-tering convergence occurs when all clusters are merged to a single node [42, 44], or when an arbitrary threshold of an arbitrary model complexity criterion is reached, in our case convergence is driven by the training set, and occurs when no more edges are added to the graph by the GNN.
There is no need to define an arbitrary model selection cri-terion. Instead, the “natural granularity” of the clustering
*Indicates equal contribution. 1https://github.com/dmlc/dgl/tree/master/ examples/pytorch/hilander 2Of course, every unsupervised inference method requires inductive bi-ases. Ours stems naturally from supervision in the meta-training set and density in the inferred clusters.
process is determined inductively, by the ground truth in the training set. Unlike prior clustering work using GNNs
[51, 57, 56], we perform full-graph inference to jointly pre-dict two attributes: linkage probabilities at the edges, and densities at the nodes, defined as the proportion of similar vertices that share the same label within a node’s neighbor-hood [14, 3, 56]. The densities establish a relative order between nodes [3, 56], which is then used to guide the con-nectivity. Nodes at the boundary between two ground-truth clusters, or nodes having a majority of their neighbors be-longing to different classes, tend to have a low density, and accordingly a low expectation of linkage probability to their neighbors. Prior methods predict the edge connectivity as a node attribute on numerous sampled sub-graphs [51, 56]; ours directly infers the full graph and predicts connectivity as an attribute of the edges. Also, prior methods require sep-arate models for the two attributes of linkage probabilities and node densities, whereas ours infers them jointly. This is beneficial as there is strong correlation between the two attributes, defined by the ground truth. A joint model also achieves superior efficiency, which enables hierarchical in-ference that would otherwise be intractable. Compared to the two separate models, we achieve a speedup from 256s to 36s as shown in Table 1.
In terms of accuracy, Our method achieves an average 49% improvement in F-score, from 0.390 to 0.585, and an average 7% increase in NMI, from 0.778 to 0.836 com-pared to state-of-art GNN based clustering methods [56, 51] over the face and species clustering benchmarks as shown in Table 3. Furthermore, the pseudo-labels generated by our clustering of unlabeled data can be used as a regular-ization mechanism to reduce face verification error by 14%, as shown in Table 4, from 0.187 to 0.159 in compared to state-of-art clustering methods, allowing us to approach the performance of fully supervised training at 0.136.
In the next section, we summarize our contributions in the context of prior related work. In Section 3 we introduce the technical innovations of our paper, and in Section 4 we detail our experiment results. We discuss failure modes and limitations of our method in Section 5. 2.