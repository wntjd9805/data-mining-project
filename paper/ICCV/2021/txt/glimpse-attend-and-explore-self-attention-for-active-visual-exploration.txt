Abstract
Active visual exploration aims to assist an agent with a limited ﬁeld of view to understand its environment based on partial observations made by choosing the best viewing directions in the scene. Recent methods have tried to ad-dress this problem either by using reinforcement learning, which is difﬁcult to train, or by uncertainty maps, which are task-speciﬁc and can only be implemented for dense predic-tion tasks. In this paper, we propose the Glimpse-Attend-and-Explore model which: (a) employs self-attention to guide the visual exploration instead of task-speciﬁc uncer-tainty maps; (b) can be used for both dense and sparse prediction tasks; and (c) uses a contrastive stream to fur-ther improve the representations learned. Unlike previous works, we show the application of our model on multi-ple tasks like reconstruction, segmentation and classiﬁca-tion. Our model provides encouraging results while be-ing less dependent on dataset bias in driving the explo-ration. We further perform an ablation study to investi-gate the features and attention learned by our model. Fi-nally, we show that our self-attention module learns to at-tend different regions of the scene by minimizing the loss on the downstream task. Code: https://github.com/ soroushseifi/glimpse-attend-explore. 1.

Introduction
Most computer vision methods rely on datasets captured by human photographers [27, 34]. Such data is biased to-wards the salient information showing up in predictable ar-eas of the image (e.g. image center). Besides, most com-puter vision methods assume full observability of the input image [26, 18]. However, in a dynamic environment, an agent with a limited ﬁeld of view/resource cannot fully ob-serve its immediate 360°scene. This might cause the agent’s camera to capture parts of the environment that are diver-gent to those seen in standard computer vision dataset, thus degrading agent’s performance on different tasks.
In this paper, we propose an active vision [1] method to
*Equal contribution. autonomously explore and reason about a scene by sequen-tially gathering partial observations from it. Our method can be deployed in scenarios where an agent cannot view and process the whole scene due to limitations such as the agent’s small ﬁeld of view or limited transfer bandwidth between the camera and the processing unit. We simulate this by restricting our method to see small crops (called glimpses) from the images in common computer vision datasets. Besides, we restrict the total number of glimpses the agent can see from a single image. At each time step, the agent has the freedom to change its viewing direction and take a new glimpse of the scene. Therefore, it is im-portant that the agent selects the areas of the environment with the highest information gain for a given task. Given a set of training examples and an initial random glimpse for each example, our model learns a policy to select the next glimpses, hallucinates the unseen areas, and solves a task given the structural cues coming from the visited areas.
While previous methods rely on reinforcement learning
[32, 25], reconstruction loss [35] and uncertainty measures
[36], we employ the heatmaps generated by self-attention layers to guide the exploration. Contrary to previous works, we propose a uniﬁed two-stream architecture for different tasks such as image reconstruction, classiﬁcation, and se-mantic segmentation and evaluate our method using several
In addition, we show that the agent can build baselines. a richer representation of the environment by using con-trastive learning. In this case, only during training, we use a pretrained encoder to produce the features for the full en-vironment. Next, taking inspiration from [12], we train one of our network’s streams to predict the full environment’s features given only the visited glimpses. Finally, we per-form an ablation study on the number of glimpses and our network architecture. Our contributions are as follows:
• We introduce a new self-supervised attention mecha-nism for active visual exploration.
• We propose a uniﬁed architecture for both sparse and dense prediction tasks.
• We show that our proposed attention mechanism
Figure 1. Results generate by Glimpse Attend and Explore: (Left) shows the scene reconstruction results on SUN360 dataset [38]. (Right) shows the semantic segmentation results on ADE20k [43]. Results are computed after taking 8 retina-like glimpses (bottom-row) containing 18% of the pixels in the image. tion based on partially observed image can be done either as an inpainting [31, 42] or outpainting task [24, 32, 35].
While the amount of context available for inpainting is usu-ally high, outpainting receives a partial view of the image as context. Jayaraman et al. [24] propose a view grid re-construction as a pretext task to learn the 3D visual repre-sentation of a 2D view of the object. Ramakrishnan et al.
[32] introduces an action policy learning strategy to select a sequence of view grids to reconstruct the whole scene.
Our work is in the same line as [32], where we sequen-tially select glimpses to reconstruct the whole scene, with the difference in the way we learn to select views. Sim-ilar to our work, [32] employs the full image to help the training. However, different views of the view-grids in [32] may have different FOV depending on their location in the gird. In this work, we ﬁx the size of the glimpses and con-sequently scene coverage to always be the same. Besides,
[32] reduces the search space for the reinforcement learn-ing training scheme by restricting the agent to always select from the neighbouring glimpses while our agent can change its viewing direction to anywhere in the scene. The clos-est to our work is [35], which performs an attention-based view selection. The attention policy proposed by [35] learns to predict the image region with the highest reconstruction loss and thereby requires the loss value to be trained. The self-attention module of our proposed model uses gradients received from the next layer to train. Therefore, each layer of self-attention learns to attend the image regions speciﬁc to the downstream task.
Semantic segmentation: Conventional segmentation methods like FCN [28], U-net [33], Segnet [4] and Deeplab
[9] have been successful for segmenting natural scenes and biomedical imaging, however, they cannot be used in en-vironments with limited FOV where full observation of the environment is impossible.
Recent works aim to actively sample parts of the scene to provide segmentation masks, like [21], which iteratively predicts an object and context boxes pairs to predict the seg-mentation mask around the object. However, it requires the initial location of the target object as an input. Chai et al.
[8] uses an attention mechanism to guide the view selection
Figure 2. Architecture Overview. disentangles location prediction from the auxiliary dense prediction task used in previous work [36].
Consequently, a downstream task such as classiﬁca-tion/regression may directly lead the attention mecha-nism. This makes the architecture two times faster and uses less than a tenth of the gpu memory compared to previous work.
• We employ contrastive learning to train the network to reason beyond seen areas and build an even richer representation of the environment. 2.