Abstract
Multimodal self-supervised learning is getting more and more attention as it allows not only to train large networks without human supervision but also to search and retrieve data across various modalities.
In this context, this pa-per proposes a framework that, starting from a pre-trained backbone, learns a common multimodal embedding space that, in addition to sharing representations across different modalities, enforces a grouping of semantically similar in-stances. To this end, we extend the concept of instance-level contrastive learning with a multimodal clustering step in the training pipeline to capture semantic similarities across modalities. The resulting embedding space enables retrieval of samples across all modalities, even from unseen datasets and different domains. To evaluate our approach, we train our model on the HowTo100M dataset and evaluate its zero-shot retrieval capabilities in two challenging domains, namely text-to-video retrieval, and temporal action local-ization, showing state-of-the-art results on four different datasets. 1.

Introduction
To robustly learn visual events and concepts, humans sel-dom rely on visual inputs alone. Instead, a rich multimodal environment is utilized for understanding by combining mul-tiple sensory signals along with various language represen-tations. Many recent techniques have attempted to mimic this paradigm to train efficient computer vision models, espe-cially those that learn from videos where multiple modalities are naturally present [1, 2, 36].
Learning on multimodal video data has both benefits and challenges. It is beneficial that each video instance has infor-Figure 1: The Multimodal Clustering Network (MCN) com-bines a contrastive loss that learns feature representations to be close across different modalities such as video, audio, and text (blue box), with a clustering loss that draws instances that are semantically related together, e.g., scenes depicting the same semantic concept (e.g., chopping or frying) from different videos or different clips. (yellow box). mation available in multiple modalities. Textual information corresponding to the spoken narrations in the video, for ex-ample, provides a valuable language modality in addition to the visual and audio modalities [7, 21, 25]. In this work, we 1
focus on the problem of learning a joint embedding space across multiple modalities. Given that the features from different modalities are often not comparable, the goal is to learn the projections into a common space where features from different domains but with similar content are close to each other to allow for a direct retrieval across modalities.
However, creating an effective joint multimodal embedding space is not easy. First, each of those modalities is differ-ent, i.e. with respect to its source, how it is sampled and processed, and its resulting feature representation. Addition-ally, in real-world data, the supervision available to learn these projections from each of the modalities is unfortunately weak, as e.g. audio sequences can be misaligned to their vi-sual representations and corresponding narration might or might not be present in the same time interval [2, 32].
To deal with multimodal data of this nature, several re-cent approaches use a contrastive loss [18, 19] to learn e.g. feature representations in a joint embedding space. The goal is to bring samples drawn from the same temporal instance closer to each other while keeping samples from different times apart. Recent works [1, 32] show that such training is useful for pretraining models on large-scale data without additional supervision and that the resulting models achieve competitive performance on several tasks, e.g. in action clas-sification when fine-tuned on various datasets. One problem arising from the contrastive loss is that this criterion does not consider the samplesâ€™ semantic structure and similarity at different times: two samples are treated as a negative pair as long as they occur at different times regardless of their semantic similarity. This can have a considerable adverse impact on the learned representation. In a different formu-lation for learning representations, instead of comparing individual instances, clusters of instances are first created using a certain clustering algorithm [2, 5, 11, 29]. This approach encourages samples semantically similar to each other (namely, samples in the same cluster) to be close in the embedding space. However, if we cluster features from multi-modalities, those clusters would likely emerge only within the modalities separately, clustering audio instances with audio instances, visuals to visuals etc. Therefore, a mechanism that pulls the instances from different modalities together is crucial to cluster features from different modali-ties in a joint space. This leads to our proposed method that treats these two approaches as reciprocal information.
We present a multimodal learning framework that learns joint representations by training cross-modal projection heads from the visual, audio, and language modalities and accounts for the semantic similarity of embedding using a large corpus of naturally narrated videos. The proposed
Multimodal Clustering Network (MCN) adopts a novel archi-tecture to combine promising ideas from both representation learning paradigms described earlier: learning via the con-trastive loss at the instance level and the semantic consistency at the cluster level. As another novel feature of our approach, we explore joint clusters using multimodal representations instead of clusters using separate modalities. The result fea-tures allow us to do retrieval across different modalities in linear time. Figure 1 provides a high-level overview of our approach.
To evaluate our proposed method, we address the chal-lenging problem of zero-shot learning in two contexts: mul-timodal video retrieval and multimodal temporal action lo-calization. We train our system on the HowTo100M dataset
[33] and evaluate its retrieval capabilities on the YouCook2
[44] and MSR-VTT [42] dataset and its temporal action lo-calization on the task of action detection on the CrossTask
[46] dataset and on the task of temporal action segmentation on the Mining YouTube [26] dataset. Using only features from pretrained backbones, MCN significantly outperforms the best text-to-video retrieval baseline over absolute 3% in recall and outperforms the temporal action localization baseline over 3.1% in recall, both in zero-shot settings.
Contributions. The contributions of this work are three-fold: (i) We propose a novel method by combining the ben-efits of contrastive loss and clustering loss for multimodal joint space learning. Unlike prior works that create clusters using separate modalities, our method shows the important benefits of using multimodal joint clusters. (ii) We show that the proposed model can learn across three modalities (video, audio, text) in a joint space. (iii) We demonstrate significant performance gains on multiple downstream tasks in the zero-shot setting. These results show that the learned common space representations can improve state-of-the-art results without any additional training on the target datasets. 2.