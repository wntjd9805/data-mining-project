Abstract
Recent breakthroughs in self-supervised learning show that such algorithms learn visual representations that can be transferred better to unseen tasks than cross-entropy based methods which rely on task-specific supervision. In this pa-per, we found that the similar holds in the continual learning context: contrastively learned representations are more ro-bust against the catastrophic forgetting than ones trained with the cross-entropy objective. Based on this novel ob-servation, we propose a rehearsal-based continual learning algorithm that focuses on continually learning and main-taining transferable representations. More specifically, the proposed scheme (1) learns representations using the con-trastive learning objective, and (2) preserves learned rep-resentations using a self-supervised distillation step. We conduct extensive experimental validations under popular benchmark image classification datasets, where our method sets the new state-of-the-art performance. Source code is available at https://github.com/chaht01/Co2L. 1.

Introduction
Modern deep learning algorithms show impressive per-formances on the task at hand, but it is well known that they often struggle to retain their knowledge on previously learned tasks after being trained on a new one [32]. To mitigate such “catastrophic forgetting,” prior works in the continual learning literature focus on preserving the previ-ously learned knowledge using various types of information about the past task. Replay-based approaches store a small portion of past samples and rehearse the samples along with present task samples [35, 29, 34, 5]. Regularization-based approaches force the current model to be sufficiently close to the past model—which may be informative about the past task—in the parameter/functional space distance [25, 6, 39].
Expansion-based approaches allocate a unit (e.g., network node, sub-network) for each task and keep the unit untouched during the training for other tasks [38, 31].
In this paper, instead of asking how to isolate previous knowledge from new knowledge, we draw attention to the following fundamental question:
What type of knowledge is likely to be useful for future tasks (and thus not get forgotten), and how can we learn and preserve such knowledge?
To demonstrate its significance, consider the simple scenario that the task at hand is to classify the given image as an apple or a banana. An easy way to solve this problem is to extract and use the color feature of the image; red means apple, and yellow means banana. The color, however, will no longer be useful if our future task is to classify another set of images as apples or strawberries; color may not be used anymore and eventually get forgotten. On the other hand, if the model had learned more complicated features, e.g., shape/polish/texture, the features may be re-used for future tasks and remain unforgotten. This line of thoughts suggests that forgetting does not only come from the limited access to the past experience, but also from the innately re-stricted access to future events; to suffer less from forgetting, learning more transferable representations in the first hand may be as important as carefully preserving the knowledge gained in the past.
To learn more transferable representations for continual learning, we draw inspirations from a recent advance in self-supervised learning, in particular, contrastive learning
[19, 10]. Contrastive methods learn representations using the inductive bias that the prediction should be invariant to certain input transformations instead of relying on task-specific supervisions. Despite their simplicity, such methods are known to be surprisingly effective; for ImageNet clas-sification [37], contrastively trained representations closely achieve the fully-supervised performance even without labels
[10] and outperform counterparts in the supervised case [24].
More importantly, while the methods are originally proposed for better in-domain1 performance, recent works also show that such methods provide significant performance gains on unseen domains [10, 21]. Under a continual scenario, we make a similar observation: contrastively learned represen-tations suffer less from forgetting than the ones trained with cross-entropy loss (see Section 5.2 for details). 1The term ‘in-domain’ is used here for the setup where data distributions for representation learning and linear classifier training are the same.
Figure 1. An overview of the Co2L framework. Mini-batch samples from the current task and the memory buffer are augmented and passed through current and past (stored at the end of the previous task) representations. Co2L minimizes the weighted sum of two losses: (1)
Asymmetric SupCon loss contrasts anchor samples from the current task against the samples from other classes (Section 4.1). (2) IRD loss measures the drift of the instance-wise similarities given by the current model from the one given by the previous model (Section 4.2).
Unfortunately, applying this idea to continual settings is not straightforward due to at least two reasons: First, hav-ing access to informative negative samples is known to be crucial for the success of contrastive learning [36], while the instantaneous demographics of negatives samples are severely restricted under standard continual setups; in class-incremental learning, for instance, it is common to assume that the learner can access samples from only a small number of classes at each time step. Second, the question of how to preserve the contrastively learned representations on con-tinual learning setups has not been fully answered. Indeed, recent works on representation learning for continual setups aim to learn representations accelerating future learning un-der a similar decoupled learning setup but lack an explicit design to preserve representations.
Contribution. To address these challenges, we propose a new rehearsal-based continual learning algorithm, coined
Co2L (Contrastive Continual Learning). Unlike previous continual (representation) learning methods, we aim to learn and preserve representations continually in a decoupled representation-classifier scheme. The overview of Co2L is illustrated in Figure 1.
Our contribution under this setup is twofold: 1. Contrastive learning: We design an asymmetric version of supervised contrastive loss for learning representations under continual learning setup (Section 4.1) and empir-ically show its benefits on improving the representation quality. 2. Preserving representations: We propose a novel preserva-tion mechanism for contrastively learned representations, which works by self-distillation of instance-wise rela-tions (Section 4.2); to the best of our knowledge, this is a first method explicitly designed to preserve contrastively learned representations on continual learning.
We validate Co2L under various experimental scenarios en-compassing task-incremental learning, domain-incremental learning, and class-incremental learning. Co2L consistently outperforms all baselines on various datasets, scenarios, and memory setups. With careful ablation studies, we also show that both components we propose (asymmetric supervised contrastive loss, instance-wise relation distillation) are es-sential for performance. In the ablation of distillation, we empirically show that distillation preserves learned represen-tations and efficiently uses buffered samples, which might be the main source of consistent gains over all comparisons: distillation provides 22.40% and 10.59% relative improve-ments with/without buffered samples respectively on the
Seq-CIFAR-10 dataset. In the ablation of asymmetric su-pervised contrastive loss, we quantitatively verify that the asymmetric version consistently provides performance gains over the original one on all setups, e.g., 8.15% relative im-provement on the Seq-CIFAR-10 with buffer size 500. We also provide qualitative implications on this performance gain by visualizing learned representations, which shows our asymmetric version prevents severe drifts of learned features. 2.