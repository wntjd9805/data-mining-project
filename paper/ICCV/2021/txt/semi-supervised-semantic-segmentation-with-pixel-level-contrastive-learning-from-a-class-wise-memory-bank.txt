Abstract
This work presents a novel approach for semi-supervised semantic segmentation. The key element of this approach is our contrastive learning module that enforces the seg-mentation network to yield similar pixel-level feature repre-sentations for same-class samples across the whole dataset.
To achieve this, we maintain a memory bank which is con-tinuously updated with relevant and high-quality feature vectors from labeled data. In an end-to-end training, the features from both labeled and unlabeled data are opti-mized to be similar to same-class samples from the mem-ory bank. Our approach not only outperforms the cur-rent state-of-the-art for semi-supervised semantic segmen-tation but also for semi-supervised domain adaptation on well-known public benchmarks, with larger improvements on the most challenging scenarios, i.e., less available la-beled data. Code is available at https://github.com/
Shathe/SemiSeg-Contrastive 1.

Introduction
The goal of semantic segmentation consists in assign-ing a semantic class label to each pixel in an image. It is an essential computer vision task for semantic scene under-standing that plays a relevant role in many applications such as medical imaging [30] or autonomous driving [2]. As for many other computer vision tasks, deep convolutional neu-ral networks have shown significant improvements in se-mantic segmentation [2, 19, 1]. All these examples follow supervised learning approaches requiring a large set of an-notated data to generalize well. However, the availability of labeled data is a common bottleneck in supervised learning, especially for tasks such as semantic segmentation, which require tedious and expensive per-pixel annotations.
Figure 1. Proposed contrastive learning module overview. At each training iteration, the teacher network fξ updates the feature memory bank with a subset of selected features from labeled sam-ples. Then, the student network fθ extracts features △ from both labeled and unlabeled samples, which are optimized to be similar to same-class features from the memory bank ○. limited labeled data by extracting knowledge from unla-beled samples. Semi-supervised learning has been applied to a wide range of applications [37], including semantic segmentation [11, 17, 26]. Previous semi-supervised seg-mentation works are mostly based on per-sample entropy minimization [17, 21, 28] and per-sample consistency regu-larization [11, 36, 28]. These segmentation methods do not enforce any type of structure on the learned features to in-crease inter-class separability across the whole dataset. Our hypothesis is that overcoming this limitation can lead to bet-ter feature learning and performance, especially when the amount of available labeled data is low.
Semi-supervised learning assumes that only a small sub-set of the available data is labeled. It tackles the issue of
This work presents a novel approach for semi-supervised semantic segmentation, following a teacher-student scheme
whose main component is a novel representation learn-ing module (Figure 1). This module is based on positive-only contrastive learning [5, 14] and enforces the class-separability of pixel-level features across different sam-ples. To achieve this, the teacher network produces fea-ture candidates, only from labeled data, to be stored in a memory bank. Meanwhile, the student network learns to produce similar class-wise features from both labeled and unlabeled data. The features stored in the memory bank are selected based on their quality and learned rele-vance for the contrastive optimization.
In addition to in-creased inter-class separability, the module enforces the alignment of unlabeled and labeled data (memory bank) in the feature space, which is another unexploited idea in semi-supervised semantic segmentation. In summary, we present a novel framework for semi-supervised semantic segmenta-tion where the main contributions are the following:
• A pixel-level contrastive learning scheme for semi-supervised semantic segmentation where elements are weighted based on their relevance.
• The use of a memory bank for high-quality pixel-level features from labeled data.
We evaluate our method on well-known semi-supervised semantic segmentation benchmarks, reaching the state-of-the-art on different setups. Besides that, we show that our approach can naturally tackle the semi-supervised domain adaptation task, obtaining state-of-the-art results too.
In all cases, the improvements upon comparable methods in-crease with the percentage of unlabeled data. augmentation anchoring [33]. While distribution alignment enforces the prediction of perturbed and non-perturbed samples to have the same class distribution, augmentation anchoring enforces them to have the same semantic la-bel. To produce high-quality non-perturbed class distri-bution or prediction on unlabeled data, the Mean Teacher method [36], proposes a teacher-student scheme where the teacher network is an exponential moving average (EMA) of model parameters, producing more robust predictions. 2.2. Semi-Supervised Semantic Segmentation
One common approach for semi-supervised semantic segmentation is to make use of Generative Adversarial Net-works (GANs) [12]. Hung et al. [17] propose to train the discriminator to distinguish between confidence maps from labeled and unlabeled data predictions. Mittal et al. [26] make use of a two-branch approach, one branch enforc-ing low entropy predictions using a GAN approach and an-other branch for removing false-positive predictions using a Mean Teacher method [35]. A similar idea was proposed by Feng et al. [10], a recent work that introduces Dynamic
Mutual Training (DMT). DMT uses two models and the model’s disagreement is used to re-weight the loss. DMT method also followed the multi-stage training protocol from
CBC [9], where pseudo-labels are generated in an offline curriculum fashion. Other works are based on data aug-mentation methods for consistency regularization. French et al. [11] focus on applying CutOut [7] and CutMix [45], while Olsson et al. [28] propose a data augmentation tech-nique specific for semantic segmentation. 2.