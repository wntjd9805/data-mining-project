Abstract
Facial editing is an important task in vision and graph-ics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained edit-ing mode (e.g., editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipula-tion through dialog between the user and the system. Our key insight is to model a continual “semantic field” in the
GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users’ language requests. 3) To engage the users in a mean-ingful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field.
We also contribute CelebA-Dialog, a visual-language fa-cial editing dataset to facilitate large-scale study. Specifi-cally, each image has manually annotated fine-grained at-tribute annotations as well as template-based textual de-scriptions in natural language. Extensive quantitative and
∗Equal contribution. qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently fa-vored by around 80% of the participants. Our project page is https://www.mmlab-ntu.com/project/ talkedit/. 1.

Introduction
The goal of facial editing is to enable users to manipulate facial images in their desired ways. Thanks to the advance of deep generative models like GANs [10, 29, 3, 15, 16, 18], facial editing has witnessed rapid growth in recent years, especially in image fidelity. While there have been sev-eral attempts to improve facial editing quality, they often lack interactions with users or require users to follow some fixed control patterns. For instance, image-to-image trans-lation models [53, 7, 12, 21, 26] only translate facial images between several discrete and fixed states, and users cannot give any subjective controls to the system. Other face edit-ing methods offer users some controls, such as a seman-tic map indicating the image layout [22], a reference image demonstrating the target style [14, 25, 24], and a sentence describing a desired effect [5, 51, 30, 54, 46]. However, users have to follow the fixed patterns, which are too de-manding and inflexible for most users. Besides, the only
feedback provided by the system is the edited image itself.
In terms of the flexibility of interactions, we believe nat-ural language is a good choice for users. Language is not only easy to express and rich in information, but also a nat-ural form for the system to give feedback. Thus, in this work, we make the first attempt towards a dialog-based fa-cial editing framework, namely Talk-to-Edit, where editing is performed round by round via request from the user and feedback from the system.
In such an interactive scenario, users might not have a clear target in their mind at the beginning of editing and thoughts might change during editing, like tuning an overly laughing face back to a moderate smile. Thus, the editing system is supposed to be capable of performing continu-ous and fine-grained attribute manipulations. While some approaches [37, 38, 42, 39, 11] could perform continuous editing to some extent by shifting the latent code of a pre-trained GAN [16, 18, 15, 3], they typically make two as-sumptions: 1) the attribute change is achieved by travers-ing along a straight line in the latent space; 2) different identities share the same latent directions. However, these assumptions overlook the non-linear nature of the latent space of GAN, potentially leading to several shortcomings in practice: 1) The identity would drift during editing; 2)
When editing an attribute of interest, other irrelevant at-tributes would be changed as well; 3) Artifacts would ap-pear if the latent code goes along the straight line too far.
To address these challenges, we propose to learn a vec-tor field that describes location-specific directions and mag-nitudes for attribute changes in the latent space of GAN, which we term as a “semantic field”. Traversing along the curved trajectory takes into account the non-linearity of at-tribute transition in the latent space, thus achieving more fine-grained and accurate facial editing. Besides, the curves changing the attributes of different identities might be dif-ferent, which can also be captured by our semantic field with the location-specific property. In this case, the iden-tity of the edited facial image would be better preserved. In practice, the semantic field is implemented as a mapping network, and is trained with fine-grained labels to better leverage its location-specific property, which is more ex-pressive than prior methods supervised by binary labels.
The above semantic field editing strategy is readily em-bedded into our dialog system to constitute the whole Talk-to-Edit framework. Specifically, a user’s language request is encoded by a language encoder to guide the semantic field editing part to alter the facial attributes consistent with the language request. After editing, feedback would be given by the system conditioned on previous edits to check for further refinements or offer other editing suggestions. The user may respond to the system feedback for further editing actions, and this dialog-based editing iteration would con-tinue until the user is satisfied with the edited results.
To facilitate the learning of semantic field and dialog-based editing, we contribute a large-scale visual-language dataset named CelebA-Dialog. Unlike prior datasets with only binary attribute labels, we annotate images in CelebA with attribute labels of fine granularity. Accompanied with each image, there is also a user request sample and several captions describing these fine-grained facial attributes.
In summary, our main contributions are: 1) We propose to perform fine-grained facial editing via dialog, an eas-ier interactive way for users. 2) To achieve more contin-uous and fine-grained facial editing, we propose to model a location-specific semantic field. 3) We achieve superior re-sults with better identity preservation and smoother change compared to other counterparts. 4) We contribute a large-scale visual-language dataset CelebA-Dialog, containing fine-grained attribute labels and textual descriptions. 2.