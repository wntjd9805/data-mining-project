Abstract
Learning to regress 3D human body shape and pose (e.g. SMPL parameters) from monocular images typically exploits losses on 2D keypoints, silhouettes, and/or part-segmentation when 3D training data is not available. Such losses, however, are limited because 2D keypoints do not su-pervise body shape and segmentations of people in clothing do not match projected minimally-clothed SMPL shapes. To exploit richer image information about clothed people, we introduce higher-level semantic information about clothing to penalize clothed and non-clothed regions of the human body differently. To do so, we train a body regressor using a novel “Differentiable Semantic Rendering (DSR)” loss.
For Minimally-Clothed (MC) regions, we deﬁne the DSR-MC loss, which encourages a tight match between a ren-dered SMPL body and the minimally-clothed regions of the image. For clothed regions, we deﬁne the DSR-C loss to en-courage the rendered SMPL body to be inside the clothing mask. To ensure end-to-end differentiable training, we learn a semantic clothing prior for SMPL vertices from thousands of clothed human scans. We perform extensive qualitative and quantitative experiments to evaluate the role of clothing semantics on the accuracy of 3D human pose and shape es-timation. We outperform all previous state-of-the-art meth-ods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code and trained models are available for research at https://dsr.is.tue.mpg.de/. 1.

Introduction
Estimating 3D human pose and shape from in-the-wild images has received great research interest [5, 14, 15, 18, 20, 30, 34, 54] because of its varied applications in anima-tion, games, and the fashion industry. One aspect that makes this problem challenging is the difﬁculty of obtaining accu-rate 3D ground-truth annotations, as they require either spe-cialized –mostly indoors– MoCap systems or careful cali-bration and setup of IMU sensors [46]. Such data would facilitate training robust regressors paving the way for esti-mating human-scene interaction with greater granularity.
Given the lack of in-the-wild 3D ground-truth, the vast majority of previous methods focus on 2D keypoints [5, 14] with some learned 3D priors. Even though sparse 2D key-points give useful constrained, relying only on these leads to unrealistic poses because of depth ambiguities and occlu-sion. They also do not provide reliable information about body shape. On the other hand, relying too strongly on 3D priors introduces bias. To circumvent this problem, recent approaches [30, 34, 50] propose to use part-segmentations or silhouettes. However, there is a mismatch between part-segmentations/silhouettes and projected SMPL bodies since segmentation covers clothed bodies while the common 3D body models are minimally clothed. We propose an alter-native approach to compensate for limited 3D supervision that leverages high-level 2D image cues.
Speciﬁcally, we propose more detailed clothing segmen-tation labels to supervise a neural network. Traditional multi-class clothing segmentation approaches cannot be di-rectly applied as the segmentation loss tries to exactly match the rendered SMPL body. Hence, to make use of such la-bels, we need to reason about which parts of the SMPL body model correspond to which clothing label. This is non-trivial to obtain because a body part can be covered by many clothing types. Therefore, we learn a semantic cloth-ing prior from a large-scale clothed human scan dataset, which has varied subjects, poses and camera views to which the SMPL body is ﬁtted [31]. This prior encodes the like-lihood of clothing types given a vertex on the SMPL body model, which gives the correspondence between segmen-tation labels and the SMPL body surface. Then, we use this prior to calculate a loss between the SMPL body and observed clothing labels in images. To achieve this we in-troduce Differentiable Semantic Rendering (DSR), a novel loss that supervises the training of 3D body regression with clothing semantics using weak supervision [8].
Our novel loss has two components: DSR-C for super-vising the clothing region and DSR-MC for the minimally-clothed region. A high-level illustration of our idea is shown in Fig. 2. While the former ensures that the rendered SMPL mesh stays inside the observed clothing label, the latter tries to tightly match the rendered SMPL mesh to the 2D minimal-clothing mask. The loss between the rendered out-put and the target mask is back-propagated using a dif-ferentiable renderer. Speciﬁcally, for the DSR-MC term, we apply pixel-level supervision for tight-ﬁtting with the minimal-clothing regions, while for DSR-C, we minimize the negative log probability of a SMPL semantic part label being inside the respective segmentation mask. For exam-ple, there will be a high penalty if the rendered vertices with a high probability of being “shirt” fall in the “pants” seg-mentation pixels. To ensure that our method is fast and dif-ferentiable, we render the semantic class probabilities com-puted from 3D scans as textures of the SMPL mesh.
While training, DSR can be used as an additional loss in any neural network-based human body estimator that pre-dicts SMPL parameters. First, we examine the effect of our approach over a baseline full-body mask supervision and 3D joint only supervision which veriﬁes our hypothe-Figure 2: DSR Idea - For more accurate human body es-timation, we supervise 3D body regression training with clothed and minimal-clothed regions differently using our novel DSR loss and our learned semantic prior. The seman-tic prior represents a distribution over possible clothing la-bels for each vertex. For easier illustration, we depict the most likely labels per-vertex here. sis about the value of clothing semantics. Then, we perform extensive comparisons and show that DSR outperforms pre-vious state-of-the-art methods as shown in Fig. 1. In sum-mary: 1. We explore the importance of clothing semantics for 3D human body estimation by introducing a novel dif-ferentiable semantic rendering loss that distinguishes between clothed and minimally-clothed regions. 2. We estimate a semantic clothing prior for SMPL from 3D scans of clothed people for our method which can be used also for other cases when a vertex clothing probability for a 3D SMPL body is required. 3. We outperform all state-of-the-art methods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP, suggesting the value of using human pars-ing and semantics for more accurate human body esti-mation. 2.