Abstract
Increasing demands for understanding the internal be-havior of convolutional neural networks (CNNs) have led to remarkable improvements in explanation methods. Particu-larly, several class activation mapping (CAM) based meth-ods, which generate visual explanation maps by a linear combination of activation maps from CNNs, have been pro-posed. However, the majority of the methods lack a clear theoretical basis on how they assign the coefficients of the linear combination. In this paper, we revisit the intrinsic linearity of CAM with respect to the activation maps; we construct an explanation model of CNN as a linear func-tion of binary variables that denote the existence of the cor-responding activation maps. With this approach, the ex-planation model can be determined by additive feature at-tribution methods in an analytic manner. We then demon-strate the adequacy of SHAP values, which is a unique solu-tion for the explanation model with a set of desirable prop-erties, as the coefficients of CAM. Since the exact SHAP values are unattainable, we introduce an efficient approxi-mation method, LIFT-CAM, based on DeepLIFT. Our pro-posed LIFT-CAM can estimate the SHAP values of the acti-vation maps with high speed and accuracy. Furthermore, it greatly outperforms other previous CAM-based methods in both qualitative and quantitative aspects. 1.

Introduction
Recently, convolutional neural networks (CNNs) have achieved excellent performance in various real-world vision tasks. However, it is difficult to explain their predictions due to a lack of understanding of their internal behavior.
To grasp why a model makes a certain decision, numerous saliency methods have been proposed. The methods gen-erate visual explanation maps that represent pixel-level im-portances for which regions in an input image are respon-sible for the model’s decision and which are not. Towards better comprehension of CNNs, class activation mapping (CAM) based methods, which utilize the responses of a con-volutional layer for explanations, have been widely used.
CAM-based methods [3, 4, 7, 14, 18, 20] (abbreviated as CAMs in the remainder of this paper) linearly combine activation maps to produce visual explanation maps. Since the activation maps are fixed for a given input image and a model pair, the coefficients of a linear combination govern the performance of the methods. Therefore, it is critical to design a reasonable method of determining the coefficients.
However, the majority of CAMs rely on heuristic conjec-tures for coefficient assignment without a clear theoretical basis. Specifically, the underlying linearity of CAM w.r.t. the activation maps is not fully taken into account. In addi-tion, they do not set rigorous standards of which properties are expected to be satisfied in a good explanation model.
In this work, we leverage the linearity of CAM to ana-lytically determine the coefficients beyond heuristics. Fo-cusing on the fact that CAM defines an explanation map using a linear combination of activation maps, we formu-late an explanation model as a linear function of the binary variables denoting the existence of the associated activa-tion maps. Under this scheme, each activation map can be seen as an individual feature in additive feature attri-bution methods [2, 10, 12, 16]. Notably, SHapley Additive exPlanations (SHAP) [10] provides SHAP values as a uni-fied measure of feature importance that satisfies three de-sirable properties (described in Sec. 2.2). Thus, the coef-ficients can be determined by the SHAP values of the cor-responding activation maps. However, the exact SHAP val-ues are not computable. To solve this, we propose a novel saliency method using Deep Learning Important FeaTures (DeepLIFT) [16], called LIFT-CAM, which efficiently ap-proximates the SHAP values of the activation maps. Our contributions are summarized as follows:
• We propose a novel framework of determining a plau-sible visual explanation map of CAM, by reframing the problem as determining a reliable solution for the explanation model using additive feature attribution methods. The recent Ablation-CAM [4] can be rein-terpreted by this framework.
• We formulate the SHAP values of the activation maps as a unified solution for the proposed framework and
verify their benefits in terms of generating faithful vi-sual explanations.
• We introduce a new saliency method, LIFT-CAM, based on DeepLIFT. It effectively estimates the SHAP values of the activation maps with a single backward propagation and outperforms the other previous CAMs qualitatively and quantitatively. 2.2. SHapley Additive exPlanations
Additive feature attribution method. SHAP [10] is a uni-fied explanation framework for additive feature attribution methods. The methods follow:
′ g(z
) = ϕ0 +
M (cid:88) i=1
′
ϕiz i (2) 2.