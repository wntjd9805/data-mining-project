Abstract
The key challenge in learning dense correspondences lies in the lack of ground-truth matches for real image pairs.
While photometric consistency losses provide unsupervised alternatives, they struggle with large appearance changes, which are ubiquitous in geometric and semantic matching tasks. Moreover, methods relying on synthetic training pairs often suffer from poor generalisation to real data.
We propose Warp Consistency, an unsupervised learn-ing objective for dense correspondence regression. Our objective is effective even in settings with large appear-ance and view-point changes. Given a pair of real im-ages, we first construct an image triplet by applying a ran-domly sampled warp to one of the original images. We derive and analyze all flow-consistency constraints aris-ing between the triplet. From our observations and em-pirical results, we design a general unsupervised objec-tive employing two of the derived constraints. We val-idate our warp consistency loss by training three recent dense correspondence networks for the geometric and se-mantic matching tasks. Our approach sets a new state-of-the-art on several challenging benchmarks, including
MegaDepth, RobotCar and TSS. Code and models are at github.com/PruneTruong/DenseMatching. 1.

Introduction
Finding dense correspondences between images contin-ues to be a fundamental vision problem, with many applica-tions in video analysis [44], image registration [48, 42], im-age manipulation [7, 25], and style transfer [19, 24]. While supervised deep learning methods have achieved impres-sive results, they are limited by the availability of ground-truth annotations. In fact, collecting dense ground-truth cor-respondence data of real scenes is extremely challenging and costly, if not impossible. Current approaches there-fore resort to artificially rendered datasets [4, 14, 45, 13], sparsely computed matches [5, 55], or sparse manual anno-tations [3, 34, 10]. These strategies lack realism, accuracy, or scalability. In contrast, there is a virtually endless source
Figure 1. We introduce the warp consistency graph of the image triplet (I, I ′, J). The image I ′ is constructed by warping I accord-ing to a randomly sampled flow W (black arrow). The blue arrows represent predicted flows. Our unsupervised loss is derived from the two constraints represented by the solid arrows, which predict
W by the composition I ′ → J → I and directly by I ′ → I. of unlabelled image and video data, which calls for the de-sign of effective unsupervised learning approaches.
Photometric objectives, relying on the brightness con-stancy assumption, have prevailed in the context of unsu-pervised optical flow [35, 57, 31]. However, in the more general case of geometric matching, the images often stem from radically different views, captured at different occa-sions, and under different conditions. This leads to large appearance transformations between the frames, which sig-nificantly undermine the brightness constancy assumption.
It is further invalidated in the semantic matching task [25], where the images depict different instances of the same ob-ject class. As a prominent alternative to photometric ob-jectives, warp-supervision [50, 49, 36, 32], also known as self-supervised learning [37, 40, 34], trains the network on synthetically warped versions of an image. While benefit-ing from direct supervision, the lack of real image pairs of-ten leads to poor generalization to real data.
We introduce Warp Consistency, an unsupervised learn-ing objective for dense correspondence regression. Our loss
models based on brightness constancy and spatial smooth-ness losses [35, 57]. The predominant technique mainly re-lies on photometric losses, e.g. Charbonnier penalty [57], census loss [31], or SSIM [54, 52]. Such losses are of-ten combined with forward-backward consistency [31] and edge-aware smoothness regularization [53]. Occlusion es-timation techniques [16, 31, 53] are also employed to mask out occluded or outlier regions from the objective. Recently, several works [27, 28, 26] use a data distillation approach to improve the flow predictions in occluded regions. How-ever, all aforementioned approaches rely on the assump-tion of limited appearance changes between two consecu-tive frames. While this assumption holds to a large degree in optical flow data, it is challenged by the drastic appear-ance changes encountered in geometric or semantic match-ing applications, as visualised in Fig. 2.
Unsupervised geometric matching: Geometric matching focuses on the more general case where the geometric trans-formations and appearance changes between two frames may be substantial. Methods either estimate a dense flow field [32, 50, 49, 41] or output a cost volume [39, 55], which can be further refined to increase accuracy [38, 22, 47]. The later approaches train the feature embedding, which is then used to compute dense similarity scores. Recent works fur-ther leverage the temporal consistency in videos to learn a suitable representation for feature matching [6, 15, 51].
Our work focuses on the first class of methods, which di-rectly learn to regress a dense flow field. Recently, Xen et al. [41] use classical photometric and forward-backward consistency losses to train RANSAC-Flow. They partially alleviate the sensitivity of photometric losses to large ap-pearance changes by pre-aligning the images with Ransac.
Several methods [32, 50, 49] instead use a warp-supervision loss. By posing the network to regress a randomly sampled warp during training, a direct supervisory signal is obtained, but at the cost of poorer generalization abilities to real data.
Semantic correspondences: Semantic matching poses ad-ditional challenges due to intra-class appearance and shape variations. Manual annotations in this context are ill-defined and ambiguous, making it crucial to develop un-supervised objectives. Methods rely on warp-supervision strategies [36, 37, 3, 40, 50], use proxy losses on the cost volume [12, 39, 37, 34], identify correct matches from forward-backward consistency of the cost volumes [17], or jointly learn semantic correspondence with attribute trans-fer [19] or segmentation [21]. Most related to our work are [58, 56, 59]. Zhou et al. [58] learn to align multiple images using 3D-guided cycle-consistency by leveraging the ground-truth matches between multiple CAD models.
However, the need for 3D CAD models greatly limits its applicability in practice. In FlowWeb [59], the authors op-timize online pre-existing pair-wise correspondences using the cycle consistency of flows between images in a collec-Figure 2. Warped query image (right) according to our predicted flow. Geometric and semantic matching applications pose highly challenging appearance and geometric transformations. leverages real image pairs without invoking the photomet-ric consistency assumption. Unlike previous approaches, it is capable of handling large appearance and view-point changes, while also generalizing to unseen real data. From a real image pair (I, J), we construct a third image I ′ by warping I with a known flow field W , that is created by ran-domly sampling e.g. homographies, from a specified distri-bution. We then consider the consistency graph arising from the resulting image triplet (I, I ′, J), visualized in Fig. 1.
It is used to derive a family of new flow-consistency con-straints. By carefully analyzing their properties, we propose an unsupervised loss based on predicting the flow W by the composition I ′ → J → I via image J (Fig. 1). Our final warp consistency objective is then obtained by combining it with the warp-supervision constraint, also derived from our consistency graph by the direct path I ′ → I.
We perform comprehensive empirical analysis of the ob-jectives derived from our warp consistency graph and com-pare them to existing unsupervised alternatives.
In par-ticular, our warp consistency loss outperforms approaches based on photometric consistency and warp-supervision on multiple geometric matching datasets. We further per-form extensive experiments for two tasks by integrating our approach into three recent dense matching architectures, namely GLU-Net [50] and RANSAC-Flow [41] for geo-metric matching, and SemanticGLU-Net [50] for seman-tic matching. Our unsupervised learning approach brings substantial gains: +18.2% PCK-5 on MegaDepth [23] for
GLU-Net, +2.8% PCK-5 for RANSAC-Flow on Robot-Car [20, 29], as well as +16.1% and +4.4% PCK-0.05 on
PF-Pascal [9] and TSS [46] respectively, for SemanticGLU-Net. This leads to a new state-of-the-art on all four datasets.
Example predictions are shown in Fig. 2. 2.