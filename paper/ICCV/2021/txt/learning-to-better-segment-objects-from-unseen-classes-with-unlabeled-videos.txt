Abstract
The ability to localize and segment objects from unseen classes would open the door to new applications, such as autonomous object learning in active vision. Nonetheless, improving the performance on unseen classes requires addi-tional training data, while manually annotating the objects of the unseen classes can be labor-extensive and expensive.
In this paper, we explore the use of unlabeled video se-quences to automatically generate training data for objects of unseen classes. It is in principle possible to apply existing video segmentation methods to unlabeled videos and auto-matically obtain object masks, which can then be used as a training set even for classes with no manual labels avail-able. However, our experiments show that these methods do not perform well enough for this purpose. We therefore introduce a Bayesian method that is specifically designed to automatically create such a training set: Our method starts from a set of object proposals and relies on (non-realistic) analysis-by-synthesis to select the correct ones by perform-ing an efficient optimization over all the frames simulta-neously. Through extensive experiments, we show that our method can generate a high-quality training set which sig-nificantly boosts the performance of segmenting objects of unseen classes. We thus believe that our method could open the door for open-world instance segmentation by exploit-ing abundant Internet videos. 1.

Introduction
Instance segmentation models are now able to predict the masks of objects of known classes in query images [19, 50, 55], providing rich information for many downstream ap-plications such as scene understanding [17, 47] and robot grasping [53, 59, 62]. Unfortunately, existing instance seg-mentation methods perform poorly on new classes [12].
This is an obstacle to the development of autonomous sys-tems evolving in open worlds where there will always be objects that do not belong to known classes. Being able to
Figure 1. Starting from an instance segmentation model trained on some classes, we want to learn to localize and segment objects from new classes without any human label. We do this by using unlabeled videos, which are an abundant source of data. Our ap-proach can automatically detect and select the object masks in the videos. We then use the selected masks to retrain the initial model that can then localize and segment objects from the new classes in still frames without losing performance on the old ones. detect and segment these objects would be the starting point of learning to grasp and manipulate them, for example.
As Figure 1 illustrates, our goal is therefore to auto-matically improve the performance of instance segmenta-tion models on static images containing objects from new classes without human intervention. This is in contrast with previous works that aiming at limiting the manual labeling burden for object segmentation by using bound-ing boxes only [21, 28, 65] or developing few-shot tech-niques [15, 60], but still require human intervention for new classes.
More specifically, we do not aim at predicting the cat-egories of these new objects, but focus on robustly local-izing and accurately segmenting them.
In this sense, our work is therefore more related to recent object discovery methods, which attempt to segment objects without manual segmentation labels by grouping pixels according to some criterion [3, 18, 33, 45, 57]. However, these approaches are still very fragile as they can be easily affected by low-level perturbations in color, lighting, or texture.
Our strategy is therefore to rely on unlabeled video se-quences, as unlabeled videos can be acquired without effort while providing rich information. The general idea of using
Input image
Confidence score > 0.5
Confidence score > 0.1
Confidence score > 0.01
Figure 2. The class ’tortoise’ does not belong to COCO classes. By decreasing the confidence score threshold of Mask R-CNN [19] trained on COCO, we can eventually localize and segment all the tortoises at the price of introducing many false positives. We filter these false positives using unlabeled video data. videos for self-supervision is not new [11, 23, 30, 34, 54, 56]. Here, we consider video sequences to automatically generate masks for the objects visible in their frames. Us-ing these masks for training an instance segmentation model should then make it perform better on new objects visible in the videos, even if no human intervention was provided in the process. In our experiments, we still provide the videos, but it is possible to imagine a system that captures videos by itself.
Unfortunately, our early experiments showed that state-of-the-art video segmentation methods [14, 35, 42, 30, 51] were not sufficient for this purpose. We therefore developed our own method for automatically creating object masks from videos. Note that our task is subtly different from video segmentation methods, which aim at keeping track of the objects over consecutive frames. For our own goal, this is not needed and we only focus on correctly localizing and segmenting the objects in each frame. Like some video seg-mentation methods [35], we start from mask hypotheses for the objects visible in the videos obtained with a pre-trained class-agnostic instance segmentation. Such model general-izes to objects from unseen classes to some extent, but at the price of introducing many false positives [41]. Figure 2 shows that when the confidence threshold is lowered, both the detections of objects from unseen classes and incorrect detections get accepted.
It is however possible to filter the incorrect masks us-Some meth-ing information provided by the videos. ods [8, 29, 35, 37, 44] rely on tracklets to track and filter the masks. We claim that such strategy is not optimal, es-pecially for our goal: Image background is underexploited, while it can be very useful by indicating the absence of ob-jects; The optical flow is not used or underexploited, while it gives strong cues about moving objects in videos. To fully exploit the background and motion information across unla-beled video frames, we therefore developed a (non-realistic) analysis-by-synthesis approach. Using a Bayesian frame-work, we derive an objective function with an additional non-overlapping constraint. The objective function, which consists of three loss terms, is designed to explore the back-ground and motion information to remove incorrect masks and select masks that are temporally consistent over the en-tire video. The non-overlapping constraint comes from the fact that one pixel can at most belong to one object in the image and helps to reject some false positives. Moreover, we provide a two-stage optimization algorithm to optimize this objective function efficiently.
To evaluate our approach, we created a novel dataset named Unseen-VIS based on the YouTube Video Instance
Segmentation (YouTube-VIS) dataset [61], which contains objects that do not belong to COCO classes. Starting from the raw masks generated on the training part of Unseen-VIS using a class-agnostic Mask R-CNN pretrained on COCO dataset, we apply our method to automatically select the correct masks. We demonstrate that using these masks can boost the performance of the Mask R-CNN on the test set of
Unseen-VIS without losing performance on COCO classes.
To summarize our contributions:
• We propose a Bayesian method to generate high-quality masks on unlabeled videos containing unseen classes;
• We create a benchmark to evaluate the quality of gener-ated masks on unlabeled videos;
• We demonstrate on our benchmark that our proposed method can be used to improve the performance of an instance segmentation model on unseen classes. 2.