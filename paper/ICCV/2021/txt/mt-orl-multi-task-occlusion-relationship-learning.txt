Abstract
Retrieving occlusion relation among objects in a single image is challenging due to sparsity of boundaries in im-age. We observe two key issues in existing works: ﬁrstly, lack of an architecture which can exploit the limited amount of coupling in the decoder stage between the two subtasks, namely occlusion boundary extraction and occlusion orien-tation prediction, and secondly, improper representation of
In this paper, we propose a novel occlusion orientation. architecture called Occlusion-shared and Path-separated
Network (OPNet), which solves the ﬁrst issue by exploit-ing rich occlusion cues in shared high-level features and structured spatial information in task-speciﬁc low-level fea-tures. We then design a simple but effective orthogonal occlusion representation (OOR) to tackle the second is-sue. Our method surpasses the state-of-the-art methods by 6.1%/8.3% Boundary-AP and 6.5%/10% Orientation-AP on standard PIOD/BSDS ownership datasets. Code is available at https://github.com/fengpanhe/MT-ORL. 1.

Introduction
While the human visual system is capable of intuitively performing robust scene understanding and perception. The reasoning of occlusion relation is highly challenging for machines. Depending on the number, category, orientation, and position of objects, the boundaries of objects are elu-sive; no simple priors can be applied to recover the fore-ground and background in the scene. Occlusion relationship reasoning of objects from monocular images reveals relative depth differences among the objects in the scene, which is fundamental in computer vision applications, such as mo-bile robots [18, 28, 2], object detection [8, 4, 32, 21, 5], segmentation [8, 38, 6, 31, 14, 39, 22], monocular depth
∗corresponding author. estimation/ordering [23, 25, 24, 26, 17, 37], and 3D recon-struction [30, 20, 9].
Traditional methods [12, 33, 26, 29, 13] extract the oc-clusion boundary and infer occlusion relation by exploit-ing low-level visual cues with hand-crafted features, which is not effective due to the difﬁculty of deﬁning boundaries and occlusion cues. Modern convolution neural networks (CNNs) [35, 34, 15] have signiﬁcantly boosted the perfor-mance of occlusion relationship reasoning by commonly decomposing the task into two subtasks: occlusion bound-ary extraction and occlusion orientation prediction. The former aims to extract object boundaries from the image, while the latter targets are discovering the orientation rela-tion. The occlusion relationship is then recovered by pro-gressively accumulating the orientation information on the extracted boundaries. Despite the leap in occlusion rela-tionship reasoning, we observe that two critical issues have rarely been discussed, which greatly impedes performances of previous work.
The ﬁrst issue is the lack of exploitation of the limited amount of coupling between the two subtasks. On one hand, shared global properties of the image and abstract high-level semantic features act as an effective initialization for the networks to learn low-level visual cues. Shared high-level features help different branches to generate consistent out-puts under the same semantic guidance while different fea-tures may produce semantically misaligned results in the fusion stage. On the other hand, the two subtasks need to learn diverse and concrete properties in the lower stage with larger spatial size. Speciﬁcally, occlusion boundary extraction emphasizes more on locating boundaries, while occlusion orientation attaches more importance to exploit-ing relations between areas. In a word, the two subtasks are inherently coupled in abstract global features and decou-pled in task-speciﬁc low-level features. However, previous architectures adopt a fully shared decoder [34, 15], result-ing in an initialization feature map from the lower network
(a) DOOBNet (b) OFNet (c) Our OPNet
Figure 1: Comparisons of different network architectures. Yellow indicates encoder structure. Green indicates boundary extraction path. Pink refers to occlusion orientation path. Blue indicates shared structure in decoder. Two color cubes indicate the boundary map and the orientation map of network output, respectively. Different from DOOBNet and OFNet, which over-shares local spatial information in larger feature maps, our OPNet only shares feature maps in deep stages while remaining path-separated in the shallow stage. stage.
The second issue is the lack of a good representation of occlusion orientation. DOC [35] ﬁrst proposed to use pixel-wise continuous orientation variable as a representa-tion, which however makes the design of loss function com-plex due to angle periodicity. DOOBNet [34] then proposed to truncate the variable into range (−π, π], which caused serious endpoint errors: angles close to −π and π produce large loss value while remaining relatively close orientation.
This leaves the training process problematic. Therefore, designing a proper occlusion orientation representation re-mains challenging in the occlusion relationship reasoning.
To tackle the ﬁrst issue, we propose an Occlusion-shared and Path-separated Network (OPNet), which uses abstract feature maps with smaller spatial sizes for weight shar-ing and is then split into two separate task-speciﬁc paths, namely the occlusion boundary extraction path and occlu-sion orientation prediction path. Besides, multi-scale su-pervision [36] is adopted in the boundary extraction path to enhance the multi-scale features. To solve the second representation dilemma, we propose a simple and robust orthogonal occlusion representation (OOR), which repre-sents orientation with two orthogonal vectors in horizontal and vertical directions, respectively. Experiments show our method is effective and boosts the performances of both ob-ject boundary extraction and occlusion orientation predic-tion.
In a nutshell, our contributions are three-fold:
• We rethink the inherent properties of occlusion rela-tionship reasoning, associated with two sub-tasks: oc-clusion boundary extraction and occlusion orientation regression. This motivates our advocate of a novel
Occlusion-shared and Path-separated Network (OP-Net) embodied with discrimination capability and ex-pressiveness for visual occlusion reasoning as an alter-native.
• We further propose the robust orthogonal occlusion representation(OOR) for predicting the occlusion ori-entation, which resolves the endpoint error and angle periodicity dilemma.
• The method works well on both PIOD [35] and BSDS ownership datasets [27], offering signiﬁcantly better performances than the counterparts. 2.