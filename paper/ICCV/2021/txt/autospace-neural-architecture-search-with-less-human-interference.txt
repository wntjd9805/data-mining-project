Abstract
Current neural architecture search (NAS) algorithms still require expert knowledge and effort to design a search space for network construction. In this paper, we consider automating the search space design to minimize human in-terference, which however faces two challenges: the ex-plosive complexity of the exploration space and the expen-sive computation cost to evaluate the quality of different search spaces. To solve them, we propose a novel differ-entiable evolutionary framework named AutoSpace, which evolves the search space to an optimal one with follow-ing novel techniques: a differentiable ﬁtness scoring func-tion to efﬁciently evaluate the performance of cells and a reference architecture to speedup the evolution procedure and avoid falling into sub-optimal solutions. The frame-work is generic and compatible with additional compu-tational constraints, making it feasible to learn special-ized search spaces that ﬁt different computational bud-gets. With the learned search space, the performance of recent NAS algorithms can be improved signiﬁcantly compared with using previously manually designed spaces.
Remarkably, the models generated from the new search space achieve 77.8% top-1 accuracy on ImageNet under the mobile setting (MAdds≤500M), outperforming previ-ous SOTA EfﬁcientNet-B0 by 0.7%. https://github. com/zhoudaquan/AutoSpace.git.
Expert  knowledge
Search  space
Trial &  error
Hand-crafted search space (a) 
Evaluation/search
Evolution/mutation
Candidate  block pool
Random  mutation
Basic operators conv, dwise conv, zero, ... 
Selected  search space
Ranking
Optimized search space
Hand-crafted space
Human interfere
Searching cost
High
Low (b) (c)
Auto-generated space
Human interfere
Searching cost
Low
Low
Figure 1: Comparison of different search space construction schemes. (a) Most existing NAS methods deploy handcrafted search spaces whose construction heavily relies on expertise and trial-and-error. (b) Our proposed method automatically builds and optimizes the search space by learning to form the basic operators into candidate building blocks and using an efﬁcient approach to evolve and evaluate these building blocks. (c) Compared with ex-isting schemes, our proposed one involves lower human effort and searching cost. 1.

Introduction
Recently neural architecture search (NAS) algorithms are popularly explored and applied, yielding several state-of-the-art (SOTA) deep neural network architectures [32, 31, 15, 34]. Applying a NAS algorithm typically com-prises three steps: (1) designing a search space by speci-fying its elementary operators; (2) developing a searching algorithm to explore the space and select operators from it to build the candidate model; and (3) implementing an
*Corresponding author. evaluation strategy to validate the performance of searched models. Extensive studies have been devoted to the latter two steps, i.e. searching algorithms and evaluation strate-gies [33, 2, 15, 31, 35, 34, 33, 12, 32].
To reduce the complexity of the search space to ex-plore, the common practice of recent NAS algorithms is to leverage human prior knowledge to design smaller search spaces, most of which are based on well performing hand-crafted building blocks and their variants, e.g., the inverted residual block [2, 32, 33, 31, 14] and the channel shufﬂing block [12]. On one hand, using restricted search spaces in-deed enables NAS algorithms to enjoy higher efﬁciency;
however on the other hand, due to such heavy human in-terference, the possibility to discover novel and better ar-chitectures is limited [9]. How to reduce human efforts in designing the search space and make the procedure auto-matic is still under-explored [27, 30].
In this work, we consider the open architecture space that consists of only basic operators with minimal human prior knowledge on cell graph typologies. Despite some early pilot investigations [41, 26], the progress is much hin-dered by several practical challenges. First of all, searching over the open space is unaffordably time consuming due to the combinatorial nature of the problem [27]. How to fast prune the unnecessary combinations of operators and lower the number of possible candidates to explore is thus nec-essary but remains an open question. Secondly, applying the RL-alike algorithms to search from scratch usually suf-fers poor exploration performance as they easily get stuck at sub-optimum. As a result, the model searched from the obtained space may perform no better than the ones from a manually designed search space.
In view of the above challenges, we then wonder whether it is possible to maximally reduce human interference in search space construction in a way such that the algorithms can effectively explore over the large space within an ac-ceptable time and computation cost budget? To this end, we develop a novel differentiable AutoSpace framework to automatically evolve the full search space to an optimal sub-space for the target applications. Our main insight is that a one-time searching for an optimal subspace at ﬁrst and fur-ther performing NAS within it would provide higher explo-ration capability and searching efﬁciency at the same time, while avoiding getting stuck at the sub-optimum. Figure 1 illustrates the differences between our space evolving strat-egy and the previous ones for designing search spaces.
Concretely, AutoSpace starts with an open space that comprises all the possible combinations of basic operators (e.g. convolution, pooling, identity mapping). Then a dif-ferentiable evolutionary algorithm (DEA) is developed to evolve the search space to a subspace of high-quality cell structures. The subspace can then be adopted in any NAS algorithms seamlessly to ﬁnd the optimal model architec-tures. To reduce the potential high cost of subspace search-ing and evaluation, AutoSpace introduces a couple of new techniques to remove the redundant cell structures and im-prove the parallelism of the evolution process as detailed in
Section 3.
We verify the superiority of the search space from Au-toSpace on the ImageNet [18] dataset. With the same NAS algorithm, AutoSpace provides much more accurate models than the previous SOTA models searched from the manually designed spaces. Besides, by combining the cell structures discovered with AutoSpace to EfﬁcientNet, we successfully improve the Top-1 accuracy on ImageNet by 0.7%.
In summary, we make the following contributions:
• We are among the ﬁrst to explore the automatic learn-ing of search spaces in NAS algorithms. Compared to searching for network architectures on a manu-ally designed search space, searching for a search space is more challenging due to the larger exploration space/computational complexity.
• We propose a novel learning framework that takes ad-vantage of both the high exploration capability of evo-lutionary algorithms and the high optimization efﬁ-ciency of gradient descend methods. The proposed framework can be seamlessly integrated with popular neural architecture searching algorithms.
• By directly replacing the original search space with the learned search space, the top-1 classiﬁcation accuracy of previous SOTA NAS algorithms can be improved signiﬁcantly at different model sizes. Speciﬁcally, at 200M MAdds, the performance of the searched model is improved by more than 1.8% on ImageNet. 2.