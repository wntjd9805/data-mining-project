Abstract
Self-supervised representation learning has achieved re-markable success in recent years. By subverting the need for supervised labels, such approaches are able to utilize the numerous unlabeled images that exist on the Internet and in photographic datasets. Yet to build truly intelligent agents, we must construct representation learning algorithms that can learn not only from datasets but also learn from environ-ments. An agent in a natural environment will not typically be fed curated data. Instead, it must explore its environment to acquire the data it will learn from. We propose a frame-work, curious representation learning (CRL), which jointly learns a reinforcement learning policy and a visual represen-tation model. The policy is trained to maximize the error of the representation learner, and in doing so is incentivized to explore its environment. At the same time, the learned repre-sentation becomes stronger and stronger as the policy feeds it ever harder data to learn from. Our learned representations enable promising transfer to downstream navigation tasks, performing better than or comparably to ImageNet pretrain-ing without using any supervision at all. In addition, despite being trained in simulation, our learned representations can obtain interpretable results on real images. Code is available at https://yilundu.github.io/crl/. 1.

Introduction
Similar to biological agents, self-supervised agents learn representations without explicit supervisory labels [38]. Im-pressively, these methods can surpass those based on su-pervised learning [12]. Yet the most successful approaches also depart from biological learning in that they depend on a curated dataset of observations to learn from.
In stark contrast, learning in biological vision involves active physical exploration of an environment. Infants are not endowed with existing visual experience, but must in-stead explore to obtain such experience from the surrounding environment. By playing with toys, through actions such as pushing, grasping, sucking, or prodding, infants are able to obtain experiences of texture, material, and physics [19].
Figure 1: When put in a new world, without an explicit task or goal, we are still able to actively explore and interact with our surroundings. Our framework, CRL, enables agents to learn visual representations from interaction without any supervision, using only curiosity-driven exploration where the agent seeks out obser-vations that incur high error under the representation model. The resultant representations enable agents to perform well in down-stream reinforcement and imitation learning tasks, and further are able to transfer to recognition of real images.
By crawling into new rooms, toddlers obtain experiences of layout and geometry. This setup adds additional challenges towards learning visual representations. Algorithms must now selectively explore and determine which portion of the environment will allow for the most useful increase in vi-sual experience. Furthermore, algorithms must also adapt to constant domain shift; at any time point, the only observed visual experience is that of a particular room, or that of a particular object being interacted with.
Given an interactive environment, and no prior data or tasks, how may we obtain a good visual representations?
This is a challenging question and requires an agent to an-swer several subquestions. In particular, how can we learn to effectively explore and perceive the surrounding world?
And, how can we integrate each different experience together to obtain the best representation possible? In this paper, we propose a unified framework towards solving these tasks.
One approach is to train a vision-based reinforcement learning agent in the interactive environment. Intuitively, as the agent learns to interact in its surrounding environment, its underlying vision system must also learn to understand the surrounding environment. A core difficulty, however, is the noisy and sparse supervision that reinforcement learning 1
provides, inhibiting the formation of a strong vision system.
An alternative approach is thus to leverage self-supervised representation learning techniques to learn representations in embodied environments. To gather data to learn the represen-tation, a separate exploration algorithm may be used. How-ever, such an approach raises additional challenges. Given a new embodied environment, how can we learn to effectively explore and obtain diverse data to train our representation?
And how can we continuously gather images that remain visually salient to our algorithm?
To address these issues, we propose a unified framework,
Curious Representation Learning (CRL Figure 1). Our key idea is to automatically learn an exploration policy given a self-supervised representation learning technique by training a reinforcement learning (RL) to maximize reward equal to the loss of the self-supervised representation learning model.
We then train our self-supervised model by minimizing the loss on the images obtained by the exploration policy. By defining the reward to our exploration policy in such a man-ner, it serves as a natural measure of visual novelty, as only on unfamiliar images will the loss be large. Thus, our policy learns to explore the surrounding environment and obtain images that are visually distinct from those seen in the past.
Simultaneously, our self-supervised model benefits from di-verse images, specifically obtained to remain visually salient to the model.
Given an embodied visual representation, we further study how it may be used for downstream interactive tasks.
Interactive learning, through either reinforcement learning or behavioral cloning, is characterized by both sparse and noisy feedback. Feedback from individual actions is delayed across time and dependent on task completion, with feed-back containing little information in the case a task failure, and giving conflicting results when other actions effect task completion. Such noise can quickly destroy learned visual representations. We find to enable good downstream inter-active transfer, it is crucial to freeze visual network weights before transfer. We observe that our method can significantly boost the semantic navigation performance of RL policies and visual language navigation using imitation learning.
Our contributions in this paper are three-fold. First, we introduce CRL as an approach to embodied representation learning, in which a representation learning model plays a minimax game with an exploration policy. Second, we show that learned visual representations can help in a variety of embodied tasks, where it is crucial to freeze representations to enable good performance. Finally, we show that our representations, while entirely trained in simulation, can obtain interpretable results on real photographs. 2.