Abstract
Large-scale point cloud semantic segmentation has wide applications. Current popular researches mainly focus on fully supervised learning which demands expensive and te-dious manual point-wise annotation. Weakly supervised learning is an alternative way to avoid this exhausting an-notation. However, for large-scale point clouds with few labeled points, the network is difficult to extract discrimi-native features for unlabeled points, as well as the regular-ization of topology between labeled and unlabeled points is usually ignored, resulting in incorrect segmentation results.
To address this problem, we propose a perturbed self-distillation (PSD) framework. Specifically, inspired by self-supervised learning, we construct the perturbed branch and enforce the predictive consistency among the perturbed branch and original branch. In this way, the graph topol-ogy of the whole point cloud can be effectively established by the introduced auxiliary supervision, such that the in-formation propagation between the labeled and unlabeled points will be realized. Besides point-level supervision, we present a well-integrated context-aware module to ex-plicitly regularize the affinity correlation of labeled points.
Therefore, the graph topology of the point cloud can be fur-ther refined. The experimental results evaluated on three large-scale datasets show the large gain (3.0% on average) against recent weakly supervised methods and comparable results to some fully supervised methods. 1.

Introduction
Currently, large-scale point cloud semantic segmentation attracts more and more attention due to its broad appli-cations in environmental perception, such as autonomous driving, human-computer interaction, virtual reality, and robotics. Great progress has been made in small-scale point cloud semantic segmentation [15, 16, 31, 12, 11, 22, 24].
*Corresponding Author
Figure 1. Semantic segmentation results with 1% labeled points.
We improve the segmentation accuracy of the categories with high structural similarity to other categories highlighted by the red box.
Recently, RandLA-Net [5] was proposed as an efficient method for large-scale point cloud (∼ 106 points) seman-tic segmentation. However, the mainstream of all these methods is built upon fully supervised learning, requiring tremendous point-wise annotation. Unfortunately, such an-notation involves a large amount of manual work. For ex-ample, it takes 22.3 minutes to annotate a scene of ScanNet
[2] on average [23].
To avoid the exhausting annotation, weakly supervised methods are rising. Xu and Lee et al.
[26] firstly pro-posed a weakly supervised method by labeling a tiny frac-tion points. This method utilizes multi-branch supervision and a parameter-free graph-based smoothing item based on
Laplacian matrix, achieving comparable performance to its fully supervised version with 10× fewer labeled points.
However, this method is limited as it can not directly be im-plemented to the large-scale point cloud with fewer labels, due to the lack of learnable topological relationship and the high computational complexity of the Laplacian matrix. In addition, this method only uses point-level supervision, and it is not easy to model the context. While in fully supervised segmentation task, context information is implicitly learned by U-Net-style structure [16, 5] or local feature aggrega-tion [31, 22]. Due to the limited annotation of large-scale scenarios, these techniques cannot meet the requirements
for learning enough discriminative features. For example, in the first column of Figure 1, the Baseline (RandLA-Net
[5]), trained by 1% labels, misclassifies many points in the red box.
Inspired by the successes of self-supervised learning, we propose a perturbed self-distillation framework that focuses on solving two critical issues: 1) How to design auxiliary supervision for unlabeled points, such that a well-formed point graph topology can be established. 2) Besides merely supervising on point-level, how to derive a context regular-ization for modeling the relationship among labeled points?
For the first issue, we introduce the perturbed self-distillation by constructing a perturbed branch and keeping the predicted distribution consistency between the perturbed branch and the original branch. The consistency constraints provide additional supervision for all points, enabling the introduced graph convolutional networks (GCNs) to estab-lish a well-formed graph topology among all points. There-fore, based on this learned graph structure, a new interactive way of two branches is introduced, which realize the effec-tive information flow between labeled and unlabeled points.
Secondly, to refine the graph topology, we propose the context-aware module, where we encode the semantic cor-relation affinity of the labeled points to supervise the learn-ing of feature correlation. Since the labeled points are dis-tributed in the graph topology like anchor points, if the re-lationship of the anchor points could be guaranteed to be correct to some extent, it would have a positive impact on the classification results of unlabeled data.
To summarize, our contributions are three-fold:
• We propose a perturbed self-distillation (PSD) frame-work, where a self-distillation mechanism is introduced by constructing perturbed samples to ensure the predictive con-sistency among perturbed samples and original samples.
Accompanying with the supervision from labeled data, the graph topology of the whole point cloud can be effectively established through the information propagation between labeled and unlabeled points during training.
• A context-aware module is presented, and it can be seamlessly integrated into the self-distillation framework.
With the help of exactly learning the affinity context of la-beled points, the graph topology of the point cloud can be further refined.
• PSD achieves significant performance over state-of-the-art methods and gains a 3.0% improvement on average of three datasets. Moreover, PSD also improves the perfor-mance of Baseline in the way of fully supervised learning. 2.