Abstract
Event-based video frame interpolation is promising as event cameras capture dense motion signals that can greatly facilitate motion-aware synthesis. However, training exist-ing frameworks for this task requires high frame-rate videos with synchronized events, posing challenges to collect real training data. In this work we show event-based frame in-terpolation can be trained without the need of high frame-rate videos. This is achieved via a novel weakly supervised framework that 1) corrects image appearance by extracting complementary information from events and 2) supplants motion dynamics modeling with attention mechanisms. For the latter we propose subpixel attention learning, which supports searching high-resolution correspondence effi-ciently on low-resolution feature grid. Though trained on low frame-rate videos, our framework outperforms exist-ing models trained with full high frame-rate videos (and events) on both GoPro dataset and a new real event-based dataset. Codes, models and dataset will be made available at: https://github.com/YU-Zhiyang/WEVI. 1.

Introduction
Modern dedicated cameras are now capable of captur-ing high frame rate videos (e.g. 240 FPS for Sony GoPro series), allowing users to create professional slow motion effect. However, most prevailing devices, like smartphones, still cannot compete with them before overcoming various challenges on hardware and software designing. It is thus desired to develop computational techniques to synthesize high temporal resolution videos from lower resolution ones.
Foremost among the challenges of video interpolation is the loss of motion caused by the insufficient temporal sam-pling rate of the input video. Many previous works “hal-lucinate” the missing motion by assuming a parameterized motion model (e.g. linear or quadratic flows [14, 48], phase models [25, 24]) or data-driven models [13, 7, 34, 18, 32].
† The work is done during an internship at SenseTime Research.
∗ Corresponding authors: Yu Zhang (zhangyulb@gmail.com) and Xijun
Chen (chenxijun@hit.edu.cn).
Figure 1. Motivation of this work. For interpolating challenging real-world videos, even the state-of-the-art Quadratic Video Inter-polation (QVI) [48] fails to inferring correct motion. The event-based method (EDVI) [21] generates better but still sub-optimal reconstructions due to gap between training and testing. Capable of being trained directly on the raw low frame-rate videos, our approach possesses best generalization behavior. Due to lack of ground truth inbetweens, an input frame is shown as reference.
However, despite the rapid advances on end-to-end learn-ing video interpolation, the task is inherently ill-posed, with large ambiguity that cannot be trivially addressed from only the sparse set of input frames.
Characteristically for this age, event-based sensors [20] start to play roles in solving ill-posed low-level tasks such as deblurring [15, 33] and frame interpolation [47, 21]. Event cameras capture per-pixel change of intensities at high tem-poral resolution and limited power cost, making them ideal supplement to low frame-rate image sensors with the capa-bility to capture dense motion signals [2]. Despite its po-tential, event signals have distinct mode discrepancy when working with video frames. In recent works [15, 47, 21], it is largely addressed with modern deep networks by translat-ing events to image-space representations at dense temporal sites. Nevertheless, collecting synchronized training events and high frame-rate videos requires complicated hardware
calibration of dedicated cameras; this is why recent meth-ods [15, 47, 21] mostly adopt synthesized training data.
In this work, we propose a weakly supervised frame-work for video frame interpolation that bypasses the need of high frame-rate training videos with events. Instead of synthetic training, our framework is dedicatedly designed to be trained on low frame-rate videos with event streams, improving generalization on real data (see Fig. 1 for exam-ple). For interpolation at intermediate time instants, we first warp input frames with coarse motion models. Such gen-erated immediate reconstructions are then corrected by fus-ing complementary appearance cues extracted from events at multiple scales. We further leverage temporal context to improve the first-stage estimation, with a lightweight trans-former architecture [45, 50]. This supplants the need of densely modeling motion dynamics, which is difficult in case of low frame-rate training, with attention mechanisms.
We develop novel attention modules learning subpixel off-sets from low-resolution feature grid to efficiently extract accurate motion correspondences without the cost on pro-cessing high-resolution features. Though with low frame-rate training, the proposed framework surpasses the state-of-the-art image-based and event-based models trained with full high frame-rate videos, on both the GroPro dataset [28] and a new dataset captured by DAVIS240C camera [1].
In summary, the contributions of this paper include: 1) A novel framework for weakly supervised video interpolation with events, which surpasses state-of-the-art fully super-vised models and shows better generalization; 2) Comple-mentary appearance fusion that adaptively aggregates im-age and event appearance at multiple scales; 3) Subpixel at-tention mechanism that supports high-resolution correspon-dence learning on the low-resolution grid; Finally, 4) a new real event dataset and benchmarking results on it to facili-tate future research on event-based frame interpolation. 2.