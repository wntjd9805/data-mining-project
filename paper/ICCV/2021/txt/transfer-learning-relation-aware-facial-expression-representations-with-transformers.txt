Abstract
Facial expression recognition (FER) has received in-creasing interest in computer vision. We propose the Trans-FER model which can learn rich relation-aware local rep-resentations. It mainly consists of three components: Multi-Attention Dropping (MAD), ViT-FER, and Multi-head Self-Attention Dropping (MSAD). First, local patches play an important role in distinguishing various expressions, how-ever, few existing works can locate discriminative and di-verse local patches. This can cause serious problems when some patches are invisible due to pose variations or view-point changes. To address this issue, the MAD is proposed to randomly drop an attention map. Consequently, models are pushed to explore diverse local patches adaptively. Sec-ond, to build rich relations between different local patches, the Vision Transformers (ViT) are used in FER, called ViT-FER. Since the global scope is used to reinforce each local patch, a better representation is obtained to boost the FER performance. Thirdly, the multi-head self-attention allows
ViT to jointly attend to features from different information subspaces at different positions. Given no explicit guid-ance, however, multiple self-attentions may extract similar relations. To address this, the MSAD is proposed to ran-domly drop one self-attention module. As a result, mod-els are forced to learn rich relations among diverse local patches. Our proposed TransFER model outperforms the state-of-the-art methods on several FER benchmarks, show-ing its effectiveness and usefulness.
*The first two authors contributed equally. This work was done when
Fanglei Xue and Qiangchang Wang were interns at IDL, Baidu Research.
â€ Corresponding author
Figure 1. Attention visualizations [5] on two example images: Sur-prise (Row 1) and Anger (Row 2). Column 1: Original images.
Column 2: Attention visualizations of our ViT-FER model. Col-umn 3: Attention visualization of our TransFER model. 1.

Introduction
In the past several decades, facial expression recognition (FER) has received increasing interest in the computer vi-sion research community, as it is important to make comput-ers understand human emotions and interact with humans.
Despite it had obtained excellent performance recently,
FER is still a challenging task mainly due to two reasons: 1) Large inter-class similarities. Expressions from differ-ent classes may only exhibit some minor differences. As illustrated in Fig. 1, Surprise (Row 1) and Anger (Row 2) share a similar mouth. Critical clues to distinguish them lie in both eyes and areas between eyes; 2) Small intra-class similarities. Expressions belonging to the same class may have dramatically different appearances, varying with races, genders, ages to cultural backgrounds.
Existing works can be divided into two categories: global-based and local-based approaches. For the former,
many loss functions are proposed to enhance the represen-tational ability of features [18, 11]. However, since these methods take global facial images as the input, they may ne-glect some critical facial regions which would play an im-portant role in distinguishing different expression classes.
To overcome this issue, many local-based methods are pro-posed to learn discriminative features from different fa-cial parts which can be divided into two sub-categories: landmark-based and attention-based approaches.
[30, 15, 31] extracted features on facial parts which are cropped around landmarks. However, there are several issues: 1)
Pre-defined facial crops may not be flexible to describe lo-cal details which may vary from different images. This is because important facial parts may appear at different loca-tions, especially for faces with pose variations or viewpoint changes; 2) Facial landmark detection may be inaccurate or even fail for faces which are affected by various challeng-ing factors, such as strong illumination changes, large pose variations, and heavy occlusions. Therefore, it is necessary to capture important facial parts and suppress useless ones.
To achieve the aforementioned goal, [19, 28] applied at-tention mechanisms. However, they may have redundant re-sponses around similar facial parts, while neglecting other potentially discriminative parts which would play an impor-tant role in FER. This issue is especially serious for faces with occlusions or large pose variations where some facial parts are invisible. Therefore, diverse local representations should be extracted to classify different expressions. Con-sequently, more diverse local patches can contribute even when some patches are invisible. Meanwhile, different local patches can be complementary to each other. For example, as illustrated in Fig. 1, it is difficult to distinguish between surprise (Row 1) and anger (Row 2) based on the mouth areas only (Column 2). Our TransFER model explores di-verse relation-aware facial parts, like eyes (Column 3, Row 1) and areas between the brows (Column 3, Row 2), which help distinguish these different expressions. Thus, the rela-tions among different local patches should be explored in a global scope, highlighting important patches and suppress-ing the useless.
To achieve the above two goals, we propose the Trans-FER model to learn diverse relation-aware local representa-tions for FER. First, the Multi-Attention Dropping (MAD) is proposed to randomly drop an attention map.
In such a way, models are pushed to explore comprehensive local patches except for the most discriminative ones, focusing on diverse local patches adaptively. This is especially use-ful if some parts are invisible due to pose variations or oc-clusions. Second, Vision Transformer (ViT) [10] is adapted to FER, called ViT-FER, to model connections among mul-tiple local patches. Since the global scope is used to rein-force each local patch, the complementarity among multi-ple local patches are well explored, boosting the recogni-tion performance. Third, multi-head self-attention allows
ViT to jointly attend to features from different information subspaces at different positions. Redundant relations may be built, however, since there is no explicit guidance. To address this, Multi-head Self-Attention Dropping (MSAD) is proposed to randomly drop one self-attention. In such a manner, if a self-attention is dropped, models are forced to learn useful relations from the rest. Consequently, rich rela-tions among different local patches are explored to benefit the FER.
Combining the novel MAD and MSAD modules, we propose the final architecture, termed as TransFER. As il-lustrated in Fig. 1, compared with the ViT-FER baseline (Column 2), the TransFER locates more diverse relation-aware local representations (Column 3), distinguishing these different expressions. It achieves the state-of-the-art performance on several FER benchmarks, showing its ef-fectiveness. The contributions of this work can be summa-rized as follows: 1. We apply ViT to characterize the relations between different facial parts adaptively, called ViT-FER, showing their effectiveness for FER. To the best of our knowledge, this is the first effort to explore Transformers and investigate the importance of relation-aware local patches for FER. 2. A Multi-head Self-Attention Dropping (MSAD) is in-troduced to randomly remove self-attention modules, forc-ing models to learn rich relations between different local patches. 3. An Multi-Attention Dropping (MAD) is designed to erase attention maps, pushing models to extract compre-hensive local information from every facial part beyond the most discriminative parts. 4. Experimental results on several challenging datasets show the effectiveness and usefulness of our proposed
TransFER model. 2.