Abstract
Dynamic interacting systems are prevalent in vision tasks. These interactions are usually difﬁcult to observe and measure directly, and yet understanding latent interac-tions is essential for performing inference tasks on dynamic systems like forecasting. Neural relational inference (NRI) techniques are thus introduced to explicitly estimate inter-pretable relations between the entities in the system for tra-jectory prediction. However, NRI assumes static relations; thus, dynamic neural relational inference (DNRI) was pro-posed to handle dynamic relations using LSTM. Unfortu-nately, the older information will be washed away when the
LSTM updates the latent variable as a whole, which is why
DNRI struggles with modeling long-term dependences and forecasting long sequences. This motivates us to propose a memory-augmented dynamic neural relational inference method, which maintains two associative memory pools: one for the interactive relations and the other for the in-dividual entities. The two memory pools help retain use-ful relation features and node features for the estimation in the future steps. Our model dynamically estimates the rela-tions by learning better embeddings and utilizing the long-range information stored in the memory. With the novel memory modules and customized structures, our memory-augmented DNRI can update and access the memory adap-tively as required. The memory pools also serve as global latent variables across time to maintain detailed long-term temporal relations readily available for other components to use. Experiments on synthetic and real-world datasets show the effectiveness of the proposed method on modeling dynamic relations and forecasting complex trajectories. 1.

Introduction
Interacting systems are ubiquitous in computer vision, in which the entities inﬂuence and restrict each other. The characteristics of each entity are highly correlated with oth-ers in various ways [35, 37, 38, 21]. For example, multi-∗This work was partly funded by the Centre for Augmented Reasoning at the Australian Institute for Machine Learning.
Figure 1. Memory-augmented dynamic neural relational infer-ence. The prior/encoder term is used to dynamically estimate the relation type distribution zt for each step, which is used to predict the future via the decoder. We use dual memory pools and the cor-responding memory modules to maintain the long-term temporally global information of the relations and entities. ple instances in a scene inﬂuence each other following cer-tain physical rules or underlying purposes, and human body joints are interacting and are inﬂuenced by each other in body movements [21]. Modeling and reasoning the inter-active relations are crucial for understanding the dynamic system and can beneﬁt other subsequent tasks, such as be-havior prediction. The underlying relations can usually be perceived easily but may not be seen and measured directly
[35, 21]. For example, except for the observable skeleton, more latent relations among the human body joints cannot be directly observed. They change dynamically with erratic movements and external inﬂuence. Thus, it is difﬁcult to obtain the ground truth of the latent interactions, making dynamic relational inference and prediction challenging.
There has been an amount of work proposed to implic-itly model and learn the interacting system relying on the graph neural networks (GNNs) with messaging passing on the fully connected graph [37, 30, 18, 39, 9, 38] or atten-tion models [3, 27]. Although the implicit relation mod-eling can beneﬁt the learning, it does not provide much interoperability and powerful prior to the interaction sys-tem. Thus, neural relational inference (NRI) [21] was pro-posed to explicitly represent and infer the interaction among the entities in a dynamic system. NRI infers the interac-tion relations as latent variables and applies them to per-form forecasting on a graph deﬁned by the inferred rela-tions. However, NRI [21] assumes the interactive relations are static along the observed trajectory, which is not suit-able for many realistic tasks [12]. Dynamic neural rela-tional inference (DNRI) [12] was proposed to estimate the time-step-speciﬁc relations dynamically based on a long short-term memory model (LSTM) [17]. The LSTM is learned to model the temporal dynamics for relational infer-ence and trajectory prediction. However, the LSTM based
DNRI lacks the ability to capture long-term dependencies and cannot handle the long-range dynamics. The DNRI model stores all the sequential information in a state vari-able of LSTM, which can only be updated and accessed as a whole. The state is unstructured, and easy to forget the history information [13, 29]. The relation inference and temporal forecasting usually require temporally global in-formation and long-distance correspondences.
In this paper, we propose memory-augmented dynamic neural relational inference (MemDNRI). We formulate the relational prediction as a latent variable model, where la-tent variables are used to represent the connection type and strength between the entities, similar to NRI and DNRI. We train the model to dynamically infer the relations from the observed sequence (relying on the encoder/prior) and then forecast the unseen trajectories (with the decoder) [12]. Our
MemDNRI model maintains two external associative mem-ory pools, i.e., relation memory (RelMem) and entity mem-ory (EntMem), as temporally global latent variables to store the long-term information for both the occurring interaction relations and the individual entities, respectively. More than only two memory pools, RelMem and EntMem also indi-cate two memory-augmented sequential models. They con-tain the cooperatively learned read and write head and the corresponding controllers for accessing the memory adap-tively. Considering the characteristics of the tasks (and sub-tasks), we customize novel memory modules speciﬁcally for RelMem and EntMem for scalability and practicabil-ity. At each step, MemDNRI (with RelMem and EntMem) incrementally writes proper knowledge into the memory pools and reads out the most relevant contents for relation estimation and trajectory forecasting.
The main contributions of this paper can be summarized as the following:
• We propose a new memory-augmented neural rela-tional inference method (MemDNRI) for predicting the interaction relations and forecasting trajectories on the graph-structured temporal data. We design MemD-NRI as a latent variable model augmented by exter-nal associate memory. Unlike the LSTM based DNRI
[12] forgetting the long-range dynamic pattern easily,
MemDNRI uses memory pools as temporally global latent variables to capture long-term correspondence in the dynamic process.
• We formulate the memory augmentation as dual mem-ory modules (i.e., RelMem and EntMem) for both re-lation and entity. We design novel structures for the memory modules with customized storage structures, addressing strategies, and read and write head to fulﬁll the requirements of the tasks (see Sec. 4.1.2).
• The proposed MemDNRI can automatically update and access the memory with proper contents, which naturally uses long-term dependencies. Experiments on multiple synthetic and real datasets show the effec-tiveness of the proposed method. 2.