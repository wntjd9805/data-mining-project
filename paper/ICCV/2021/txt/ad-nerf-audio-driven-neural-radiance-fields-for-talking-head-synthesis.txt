Abstract
Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene represen-tation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap be-tween audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional im-plicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering.
Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF. 1.

Introduction
Synthesizing high-fidelity audio-driven facial video se-quences is an important and challenging problem in many applications like digital humans, chatting robots, and vir-tual video conferences. Regarding the talking-head genera-tion process as a cross-modal mapping from audio to visual faces, the synthesized facial images are expected to perform natural speaking styles while synchronizing photo-realistic streaming results as same as the original videos.
Currently, a wide range of approaches have been pro-posed for this task. Earlier methods built upon professional artist modelling [12, 60] or complicated motion capture sys-tem [6, 54] are limited in high-end areas of the movie and
*This work was done when Yudong Guo and Keyu Chen were intern at
Dilusense.
†Corresponding author: juyong@ustc.edu.cn. game industry. Recently, many deep-learning-based tech-niques [35, 42, 10, 58, 7, 43, 48, 59, 21, 57] are proposed to learn the audio-to-face translation by generative adversarial networks (GANs). However, resolving such a problem is highly challenging because it is non-trivial to faithfully re-late the audio signals and face deformations, including ex-pressions and lip motions. Therefore, most of these meth-ods utilize some intermediate face representations including reconstructing explicit 3D face shapes [55] and regressing expression coefficients [43] or 2D landmarks [41, 47]. Due to the information loss caused by the intermediate represen-tation, it might lead to semantic mismatches between origi-nal audio signals and the learned face deformations. More-over, existing audio-driven methods suffer from several lim-itations, such as only rendering the mouth part [41, 43] or fixed by static head pose [35, 42, 10, 7], thus are not suitable for advanced talking head editing tasks, like pose-manipulation and background-replacement.
To address these issues of existing talking head methods, we turn attention to recent developed neural radiance fields (NeRF). We present AD-NeRF, an audio-driven neural ra-diance fields model that can handle the cross-modal map-ping problem without introducing extra intermediate rep-resentation. Different from existing methods which rely on 3D face shape, expression coefficient or 2D landmarks to encode the facial image, we adopt the neural radiance field (NeRF) [30] to represent the scenes of talking heads.
Inspired by dynamic NeRF [16] for modeling appearance and dynamics of a human face, we directly map the corre-sponding audio features to dynamic neural radiance fields to represent the target dynamic subject. Thanks to the neu-ral rendering techniques which enable a powerful ray dis-patching strategy, our model can well represent some fine-scale facial components like teeth and hair, and achieves better image qualities than existing GAN-based methods.
Moreover, the volumetric representation provides a natural way to freely adjust the global deformation of the animated speakers, which can not be achieved by traditional 2D im-age generation methods. Furthermore, our method takes the head pose and upper body movement into consideration and is capable of producing vivid talking-head results for real-world applications.
Specifically, our method takes a short video sequence, including the video and audio track of a target speaking per-son as input. Given the audio features extracted via Deep-Speech [1] model and the face parsing maps, we aim to construct an audio-conditional implicit function that stores the neural radiance fields for talking head scene represen-tations. As the movement of the head part is not consistent with that of the upper body part, we further split the neural radiance field representation into two components, one for the foreground face and the other for the foreground torso.
In this way, we can generate natural talking-head sequences from collected training data. Please refer to the supplemen-tary video for better visualization of our results.
In summary, the contributions of our proposed talking-head synthesis method contain three main aspects:
• We present an audio-driven talking head method that directly maps the audio features to dynamic neural ra-diance fields for portraits rendering, without any in-termediate modalities that may cause information loss.
Ablation studies show that such direct mapping has better capability in producing accurate lip motion re-sults with training data of a short video.
• We decompose the neural radiance fields of human portrait scenes into two branches to model the head and torso deformation respectively, which helps to gener-ate more natural talking head results.
• With the help of audio-driven NeRF, our method enables like pose-talking head video editings manipulation and background-replacement, which are valuable for potential virtual reality applications. 2.