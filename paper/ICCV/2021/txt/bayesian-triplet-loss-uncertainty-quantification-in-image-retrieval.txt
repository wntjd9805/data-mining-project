Abstract
Uncertainty quantification in image retrieval is crucial for downstream decisions, yet it remains a challenging and largely unexplored problem. Current methods for estimating uncertain-ties are poorly calibrated, computationally expensive, or based on heuristics. We present a new method that views image embed-dings as stochastic features rather than deterministic features.
Our two main contributions are (1) a likelihood that matches the triplet constraint and that evaluates the probability of an anchor being closer to a positive than a negative; and (2) a prior over the feature space that justifies the conventional l2 normalization.
To ensure computational efficiency, we derive a variational approximation of the posterior, called the Bayesian triplet loss, that produces state-of-the-art uncertainty estimates and matches the predictive performance of current state-of-the-art methods. 1.

Introduction
Image-based retrieval systems show impressive performance in challenging tasks such as face verification [43, 52, 56], landmark retrieval [38] and place instance retrieval [63], recognition [1, 41]. These systems typically embed images into high-level features and retrieve with a nearest neighbor search.
While this is efficient, the retrieval comes with no notion of confidence, which is particularly problematic in safety-critical applications. For instance, a self-driving car relying on visual place recognition should be able to filter out place estimates drawn from uninformative images. In a less critical but still relevant application, quantifying the retrieval uncertainty can significantly improve the user experience in human-computer interfaces by not showing low-confidence results for a query.
Practical retrieval systems do not have a small set of predefined classes as output targets, but rather need high-level features that generalize to unseen classes. For instance, a visual place recognition system may be deployed in a city in which it has not been trained [58]. This is achieved by keeping the encoder fixed and relying on nearest neighbor searches. This pipeline does not easily match current methods for posterior inference, and current uncertainty estimators for retrieval are often impractical and heuristic in nature. To construct a fully
Figure 1: We model embeddings as distributions rather than point estimates, such that data uncertainty is propagated to retrieval. We phrase a Bayesian model that mirrors the triplet loss, which enables us to learn the stochastic features.
Bayesian retrieval system that fits with existing computational pipelines, we first recall the elementary equation
E(cid:2)(cid:107)∆(cid:107)2(cid:3)=(cid:107)E[∆](cid:107)2+trace(cov[∆]),
∆∈RD, (1) which follows directly from the definition of variance. From this we see that the expected squared distance between two random features, E(cid:2)(cid:107)∆(cid:107)2(cid:3), grows with the covariance of such distance, cov[∆], which in turn depends on the uncertainty of the features (Figure 1). This intuition forms the basis of this paper.
In this paper we propose to use stochastic image embed-dings instead of the usual deterministic ones. Given an image
X, we consider the posterior distribution over possible features
P (F |X). From this distribution we get direct uncertainty estimates and can assign probabilities to events such as ‘two images belonging to the same place’. To realize this, we derive a likelihood corresponding to the probability that the conventional triplet constraint is satisfied, and a prior over the feature space that mimics conventional l2 normalization. To build a system that is computationally efficient at both train and test time, we derive a variational approximation to the posterior P (F |X), such that in practice, we encode an image to a distribution in fea-ture space. Across several datasets, we show that the proposed model matches the state-of-the-art in predictive performance, while attaining state-of-the-art uncertainty estimates. 1
2.