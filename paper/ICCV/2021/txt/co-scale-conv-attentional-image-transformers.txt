Abstract
In this paper, we present Co-scale conv-attentional image
Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms.
First, the co-scale mechanism maintains the integrity of
Transformers’ encoder branches at individual scales, while allowing representations learned at different scales to ef-fectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mecha-nism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities.
On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolu-tional neural networks and image/vision Transformers. The effectiveness of CoaT’s backbone is also illustrated on ob-ject detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks. 1.

Introduction
A notable recent development in artificial intelligence is the creation of attention mechanisms [38] and Transform-ers [31], which have made a profound impact in a range of fields including natural language processing [7, 20], docu-ment analysis [39], speech recognition [8], and computer vision [9, 3]. In the past, state-of-the-art image classifiers have been built primarily on convolutional neural networks (CNNs) [15, 14, 27, 26, 11, 36] that operate on layers of filtering processes. Recent developments [30, 9] however begin to show encouraging results for Transformer-based image classifiers.
In essence, both the convolution [15] and attention [38] operations address the fundamental representation problem for structured data (e.g. images and text) by modeling the local contents, as well as the contexts. The receptive fields
* indicates equal contribution.
Code at https://github.com/mlpc-ucsd/CoaT.
Figure 1. Model Size vs. ImageNet Accuracy. Our CoaT model significantly outperforms other image Transformers. Details are in
Table 2. in CNNs are gradually expanded through a series of con-volution operations. The attention mechanism [38, 31] is, however, different from the convolution operations: (1) the receptive field at each location or token in self-attention
[31] readily covers the entire input space since each token is “matched” with all tokens including itself; (2) the self-attention operation for each pair of tokens computes a dot product between the “query” (the token in consideration) and the “key” (the token being matched with) to weight the
“value” (of the token being matched with).
Moreover, although the convolution and the self-attention operations both perform a weighted sum, their weights are computed differently: in CNNs, the weights are learned dur-ing training but fixed during testing; in the self-attention mechanism, the weights are dynamically computed based on the similarity or affinity between every pair of tokens.
As a consequence, the self-similarity operation in the self-attention mechanism provides modeling means that are po-tentially more adaptive and general than convolution oper-ations. In addition, the introduction of position encodings and embeddings [31] provides Transformers with additional flexibility to model spatial configurations beyond fixed input structures.
Of course, the advantages of the attention mechanism are
not given for free, since the self-attention operation com-putes an affinity/similarity that is more computationally de-manding than linear filtering in convolution. The early de-velopment of Transformers has mainly focused on natural language processing tasks [31, 7, 20] since text is “shorter” than an image, and text is easier to tokenize. In computer vision, self-attention has been adopted to provide added mod-eling capability for various applications [34, 37, 44]. With the underlying framework increasingly developed [9, 30],
Transformers start to bear fruit in computer vision [3, 9] by demonstrating their enriched modeling capabilities.
In the seminal DEtection TRansformer (DETR) [3] algo-rithm, Transformers are adopted to perform object detection and panoptic segmentation, but DETR still uses CNN back-bones to extract the basic image features. Efforts have re-cently been made to build image classifiers from scratch, all based on Transformers [9, 30, 33]. While Transformer-based image classifiers have reported encouraging results, perfor-mance and design gaps to the well-developed CNN models still exist. For example, in [9, 30], an input image is divided into a single grid of fixed patch size. In this paper, we de-velop Co-scale conv-attentional image Transformers (CoaT) by introducing two mechanisms of practical significance to
Transformer-based image classifiers. The contributions of our work are summarized as follows:
• We introduce a co-scale mechanism to image Trans-formers by maintaining encoder branches at separate scales while engaging attention across scales. Two types of building blocks are developed, namely a serial and a parallel block, realizing fine-to-coarse, coarse-to-fine, and cross-scale image modeling.
• We design a conv-attention module to realize relative position embeddings with convolutions in the factor-ized attention module that achieves significantly en-hanced computation efficiency when compared with vanilla self-attention layers in Transformers.
Our resulting Co-scale conv-attentional image Transformers (CoaT) learn effective representations under a modularized architecture. On the ImageNet benchmark, CoaT achieves state-of-the-art classification results when compared with the competitive convolutional neural networks (e.g. Efficient-Net [29]), while outperforming the competing Transformer-based image classifiers [9, 30, 33], as shown in Figure 1. 2.