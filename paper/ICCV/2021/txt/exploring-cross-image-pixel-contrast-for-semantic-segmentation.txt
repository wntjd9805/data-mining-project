Abstract
Current semantic segmentation methods focus only on mining “local” context, i.e., dependencies between pixels within individual images, by context-aggregation modules (e.g., dilated convolution, neural attention) or structure-aware optimization criteria (e.g., IoU-like loss). However, they ignore “global” context of the training data, i.e., rich semantic relations between pixels across different images.
Inspired by recent advance in unsupervised contrastive rep-resentation learning, we propose a pixel-wise contrastive algorithm for semantic segmentation in the fully supervised setting. The core idea is to enforce pixel embeddings be-longing to a same semantic class to be more similar than embeddings from different classes. It raises a pixel-wise metric learning paradigm for semantic segmentation, by explicitly exploring the structures of labeled pixels, which were rarely explored before. Our method can be effortlessly incorporated into existing segmentation frameworks with-out extra overhead during testing. We experimentally show that, with famous segmentation models (i.e., DeepLabV3,
HRNet, OCR) and backbones (i.e., ResNet, HRNet), our method brings performance improvements across diverse datasets (i.e., Cityscapes, PASCAL-Context, COCO-Stuff,
CamVid). We expect this work will encourage our commu-nity to rethink the current de facto training paradigm in se-mantic segmentation. 1.

Introduction
Semantic segmentation, which aims to infer semantic la-bels for all pixels in an image, is a fundamental problem in computer vision. In the last decade, semantic segmenta-tion has achieved remarkable progress, driven by the avail-ability of large-scale datasets (e.g., Cityscapes [15]) and rapid evolution of convolutional networks (e.g., VGG [63],
ResNet [32]) as well as segmentation models (e.g., fully convolutional network (FCN) [51]). In particular, FCN [51] is the cornerstone of modern deep learning techniques for segmentation, due to its unique advantage in end-to-end
*The ﬁrst two authors contribute equally to this work.
Figure 1: Main idea. Current segmentation models learn to map pixels (b) to an embedding space (c), yet ignoring intrinsic struc-tures of labeled data (i.e., inter-image relations among pixels from a same class, noted with same color in(b)). Pixel-wise contrastive learning is introduced to foster a new training paradigm (d), by ex-plicitly addressing intra-class compactness and inter-class disper-sion. Each pixel (embedding) i is pulled closer (
) to pixels ( )
) from of the same class, but pushed far ( other classes. Thus a better-structured embedding space (e) is de-rived, eventually boosting the performance of segmentation models.
) from pixels ( pixel-wise representation learning. However, its spatial in-variance nature hinders the ability of modeling useful con-text among pixels (within images). Thus a main stream of subsequent effort delves into network designs for effective context aggregation, e.g., dilated convolution[80, 8, 9], spa-tial pyramid pooling[84], multi-layer feature fusion[58, 47] and neural attention [35, 24].
In addition, as the widely adopted pixel-wise cross entropy loss fundamentally lacks the spatial discrimination power, some alternative optimiza-tion criteria are proposed to explicitly address object struc-tures during segmentation network training [40, 2, 86].
Basically, these segmentation models (excluding [37]) utilize deep architectures to project image pixels into a highly non-linear embedding space (Fig. 1(c)). However, they typically learn the embedding space that only makes use of “local” context around pixel samples (i.e., pixel de-pendencies within individual images), but ignores “global” context of the whole dataset (i.e., pixel semantic relations across images). Hence, an essential issue has been long ig-nored in the ﬁeld: what should a good segmentation embed-ding space look like? Ideally, it should not only 1) address the categorization ability of individual pixel embeddings, but also 2) be well structured to address intra-class compact-ness and inter-class dispersion. With regard to 2), pixels 1
from a same class should be closer than those from differ-ent classes, in the embedding space. Prior studies [49, 60] in representation learning also suggested that encoding in-trinsic structures of training data (i.e., 2)) would facilitate feature discriminativeness (i.e., 1)). So we speculate that, although existing algorithms have achieved impressive per-formance, it is possible to learn a better structured pixel em-bedding space by considering both 1) and 2).
Recent advance in unsupervised representation learn-ing [12, 31] can be ascribed to the resurgence of contrastive learning – an essential branch of deep metric learning [39].
The core idea is “learn to compare”: given an anchor point, distinguish a similar (or positive) sample from a set of dis-similar (or negative) samples, in a projected embedding space. Especially, in the ﬁeld of computer vision, the con-trast is evaluated based on image feature vectors; the aug-mented version of an anchor image is viewed as a positive, while all the other images in the dataset act as negatives.
The great success of unsupervised contrastive learning and our aforementioned speculation together motivate us to rethink the current de facto training paradigm in semantic segmentation. Basically, the power of unsupervised con-trastive learning roots from the structured comparison loss, which takes the advantage of the context within the training data. With this insight, we propose a pixel-wise contrastive algorithm for more effective dense representation learning in the fully supervised setting. Speciﬁcally, in addition to adopting the pixel-wise cross entropy loss to address class discrimination (i.e., property 1)), we utilize a pixel-wise contrastive loss to further shape the pixel embedding space, through exploring the structural information of labeled pixel samples (i.e., property 2)). The idea of the pixel-wise con-trastive loss is to compute pixel-to-pixel contrast: enforce embeddings to be similar for positive pixels, and dissimilar for negative ones. As the pixel-level categorical information is given during training, the positive samples are the pixels belonging to a same class, and the negatives are the pixels
In this way, the global from different classes (Fig. 1(d)). property of the embedding space can be captured (Fig.1(e)) for better reﬂecting intrinsic structures of training data and enabling more accurate segmentation predictions.
With our supervised pixel-wise contrastive algorithm, two novel techniques are developed. First, we propose a region memory bank to better address the nature of se-mantic segmentation. Faced with huge amounts of highly-structured pixel training samples, we let the memory store pooled features of semantic regions (i.e., pixels with a same semantic label from a same image), instead of pixel-wise embeddings only. This leads to pixel-to-region contrast, as a complementary for the pixel-to-pixel contrast strategy.
Such memory design allows us to access more representa-tive data samples during each training step and fully explore structural relations between pixels and semantic-level seg-Ours
Ours
+0.8
HANet
[14]
+0.9
OCR[81]
Ours
HRNet-W48
[65]
PSPNet[84]
+0.9
DeepLabV3[9]
CCNet[84]
DeepLabV3+
[11]
Figure 2:Accuracy vs. model size on Cityscapes test [15]. Our contrastive enables consistent performance improvements over state-of-the-arts, i.e., DeepLabV3[9], HRNet[65], OCR[81], with-out bringing any change to base networks during inference. ments, i.e., pixels and segments belonging to a same class should be close in the embedding space. Second, we pro-pose different sampling strategies to make better use of in-formative samples and let the segmentation model pay more attention to those segmentation-hard pixels. Previous works have conﬁrmed that hard negatives are crucial for metric learning [39, 60, 62], and our study further reveals the im-portance of mining both informative negatives/positives and anchors in this supervised, dense image prediction task.
In a nutshell, our contributions are three-fold:
• We propose a supervised, pixel-wise contrastive learning method for semantic segmentation. It lifts current image-wise training strategy to an inter-image, pixel-to-pixel paradigm. It essentially learns a well structured pixel se-mantic embedding space, by making full use of the global semantic similarities among labeled pixels.
• We develop a region memory to better explore the large visual data space and support to further calculate pixel-to-region contrast. Integrated with pixel-to-pixel contrast computation, our method exploits semantic correlations among pixels, and between pixels and semantic regions.
• We demonstrate that more powerful segmentation models with better example and anchor sampling strategies could be delivered instead of selecting random pixel samples.
Our method can be seamlessly incorporated into exist-ing segmentation networks without any changes to the base model and without extra inference burden during testing (Fig. 2). Hence, our method shows consistently improved intersection-over-union segmentation scores over challeng-ing datasets (i.e., Cityscapes [15], PASCAL-Context [53],
COCO-Stuff[5] and CamVid[3]), using state-of-the-art seg-mentation architectures (i.e., DeepLabV3 [9], HRNet [65] and OCR [81]) and standard backbones (i.e., ResNet [32],
HRNet [65]). The impressive results shed light on the promises of metric learning in dense image prediction tasks.
We expect this work to provide insights into the critical role of global pixel relationships in segmentation network train-ing, and foster research on the open issues raised. 2
2.