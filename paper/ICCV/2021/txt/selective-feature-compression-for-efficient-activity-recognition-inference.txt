Abstract
Most action recognition solutions rely on dense sampling to precisely cover the informative temporal clip. Exten-sively searching temporal region is expensive for a real-world application. In this work, we focus on improving the inference efficiency of current action recognition backbones on trimmed videos, and illustrate that an action model can accurately classify an action with a single pass over the video unlike the multi-clip sampling common with SOTA by learning to drop non-informative features. We present Se-lective Feature Compression (SFC), an action recognition inference strategy that greatly increases model inference efficiency without compromising accuracy. Different from previous works that compress kernel size and decrease the channel dimension, we propose to compress features along the spatio-temporal dimensions without the need to change backbone parameters. Our experiments on Kinetics-400,
UCF101 and ActivityNet show that SFC is able to reduce in-ference speed by 6-7x and memory usage by 5-6x compared with the commonly used 30 crop dense sampling procedure, while also slightly improving Top1 Accuracy. We perform thorough quantitative and qualitative evaluation and show how our SFC learns to attend to important video regions for the task of action recognition. 1.

Introduction
Action recognition with 3D CNNs has seen significant advances in recent years [2, 4, 5, 18, 20, 22–24, 28–30], thanks to their ability to implicitly model motion informa-tion along with the semantic signals. However, these popu-lar 3D models require a substantial amount of GPU memory and therefore cannot operate on long video sequences. In-stead, they sample short video clips of 0.2-4 seconds and independently classify action labels for each clip. Finally, they pool the results from all clips together to generate a fi-nal video-level prediction. How to sample these clips plays a critical role in these models and several techniques have
*Equal contribution
†Work done at Amazon, Currently at Kuaishou Technology
Figure 1: With fast motion, uniform sampling (row 1, left) is not guaranteed to precisely locate the key region. Consequently, an action model that uses it can be distracted by noise and can fail to recognize the correct action. Instead, dense sampling (row 1, right) looks at all the possible regions and can precisely locate the action and ignore noisy regions via voting. While accurate, dense sampling is however very inefficient. We propose SFC (row 2) to avoid sampling and instead it looks at the whole video and compresses its features into a representation that is smaller and adequate for action recognition. been proposed (fig. 1, table 1). Among these, dense sam-pling is considered the best solution and employed in almost all action recognition models. Dense sampling works in a temporal sliding window manner and it over-samples over-lapping segments exhaustively, ensuring that no information is missed and that the action is precisely localized within a window. While this achieves the best performance, it is also highly inefficient, as a lot of the sampled clips are highly related to each other (redundant) or not informative for the classification prediction of the video (irrelevant). Recently,
SCSampler [12] and MARL [29] proposed to learn to select a small subset of relevant clips to reduce this dense predic-tion problem. While they reach very promising results, their
hard selection mechanism can potentially disregard useful information, especially on videos with complex actions.
In this paper we propose a novel technique that removes the need for dense sampling via feature compression. We call it Selective Feature Compression (SFC). SFC looks at long video sequences (up to two minutes) and compresses the rich information of their frames into a much smaller rep-resentation, which is then analyzed by a more expensive 3D network that predicts the corresponding video action labels (fig. 1). This method achieves a significant improvement in inference speed, without a drop in accuracy. In detail, we propose to split a pre-trained action network into two sub-networks (head and tail) and place our SFC in between them. Our SFC compresses the internal spatial-temporal representations from the head network along the temporal dimension using an attention-based mechanism that learns global feature correlation within the entire video sequence.
This compressed representation is passed to the tail net-work, which now operates on a much smaller input. This design brings the additional benefit of not needing to re-train the action network, as SFC can be finetuned indepen-dently and very quickly. Furthermore, SFC offers a good trade-off between inference and performance that can be easily tuned based on necessity (e.g., for fast real-world ap-plications we can compress more aggressively at the cost of some performance).
To validate the effectiveness of SFC, we present an ex-tensive analysis on the popular Kinetics 400 dataset [2] and
UCF101 [21] datasets. Our results show that SFC works with a wide range of backbones and different pre-trainings.
SFC maintains the same top-1 accuracy of dense sampling (30 crops), while improving the inference throughput by 6-7× and reducing the memory usage by 6×. While we de-signed SFC to replace the dense sampling strategy for action recognition on short trimmed videos, we also investigate its applicability on longer untrimmed content. We present re-sults on ActivityNet [3] datasets, where we show that SFC can be used in conjunction with uniform sampling and im-prove both performance and runtime. Finally, we present a visual analysis showing how SFC focuses on the informa-tive parts of a video during feature compression. 2.