Abstract
Few-shot learning is based on the premise that labels are expensive, especially when they are fine-grained and require expertise. But coarse labels might be easy to ac-quire and thus abundant. We present a representation learn-ing approach - PAS that allows few-shot learners to lever-age coarsely-labeled data available before evaluation. In-spired by self-training, we label the additional data us-ing a teacher trained on the base dataset and filter the teacher’s prediction based on the coarse labels; a new stu-dent representation is then trained on the base dataset and the pseudo-labeled dataset. PAS is able to produce a rep-resentation that consistently and significantly outperforms the baselines in 3 different datasets. Code is available at https://github.com/cpphoo/PAS 1.

Introduction
Large annotated datasets [4, 8, 19] have empowered the progress of visual recognition systems over the last decade.
However, for many practically important recognition prob-lems, annotations might require expertise and thus might be difficult to acquire. For example, to build a recognition sys-tem that identifies insect species, one would have to hire an entomologist to label hundreds of thousands of images from hundreds of species: an expensive, time-consuming affair.
This concern has sparked research in few-shot learning (FSL), which aims to train domain-specific learners that can learn new classes from very few examples. These learn-ers are “meta-trained” on a large labeled dataset of “base” classes from the same domain. The hope is that this base dataset provides the learner with the right inductive bi-ases for the domain of interest so that recognizing “novel” classes does not require quite as much labeled data. FSL is now an extremely active research area with a veritable array of recent results [33, 6, 12, 10, 16, 44, 38, 47, 30, 23]. Yet, existing FSL systems still lag far behind systems trained with large quantities of labeled training data. One might conjecture that the base dataset does not provide sufficient information about the novel classes.
Figure 1. The top row represents 6 different fine-grained classes in iNat2019 and the bottom row consists of 6 fine-grained classes from tieredImagenet. Without domain expertise, one might find it challenging to distinguish the 6 different classes (orange) but identifying them based on their coarse labels (green) is intuitive.
One possible approach to address this issue is to lever-age some auxilliary information about novel classes that might be more readily available. For example, recent work
[26, 9, 35] uses unlabeled data from the novel classes: it is, after all, the labels that are expensive; data is often cheap.
While such unlabeled data can inform the learner about the data distribution of the novel classes, they contain no infor-mation about the semantics of the class distinctions.
A potential source of auxiliary information about seman-tics is labeling at a coarser granularity, which might be eas-ier to obtain than the actual labels of interest. Consider again the problem of insect classification. It is true that one would have to hire an entomologist or even a lepidopterist to help distinguish between the 3 species of butterflies in figure 1; these labels are therefore difficult to acquire. But a lay-person would be able to distinguish between butterflies and bees. Labels at that coarse granularity can thus be crowd-sourced quite easily. This leads us to the following question: what if we had access to data from the novel classes that were weakly labeled with easy-to-acquire coarse labels?
Although such coarsely-labeled data are both readily available and potentially informative, no current FSL tech-nique is capable of using this extra information. Class tax-onomies have been explored in traditional supervised learn-ing through hierarchical inference strategies, but it is un-clear if these address the few-shot generalization problem.
One could use these additional labels as an auxiliary loss in a multitask-training framework. However, it has been shown that multitask-training is not guaranteed to help all tasks [34]. Besides, multitask-training ignores the strong constraints that tie the coarse and fine labels together, thus missing out on vital semantic knowledge.
We propose a new few-shot learning approach that ef-fectively leverages coarsely-labeled data. Following re-cent results, we focus on improving the feature represen-tation, since this turns out to be crucial for FSL [38, 2, 11].
Inspired from recent work based on pseudo-labeling and self-training [26], we develop a representation learning ap-proach named Parent-Aware Self-training (PAS). Specif-ically, we use a classifier trained on the base dataset to provide fine-grained pseudo-labels to coarsely-labeled data.
These pseudo-labels are filtered so that they are consistent with the coarse labels. These pseudo-labels will definitely be incorrect, because they will wrongly declare novel class examples to be from one of the base classes. However, they will induce a fine-grained grouping of the coarsely-labeled novel examples that is consistent with other fine-grained base classes with the same coarse labels. We then train with these pseudo-labels to produce a feature representation that hopefully captures the unknown novel class distinctions.
We experiment with three different datasets, and com-pare representations that use such coarsely-labeled data and those that do not. We find that using coarsely-labeled data improves five-shot accuracy between 5 to 15 points on the challenging all-way classification setup. Our particular ap-proach is also the best way to use this additional data, providing up to an average (across datasets) of 2 points improvement in five-shot accuracy compared to multitask training. All these gains vindicate the power of this addi-tional information and the ability of our approach to use it. 2.