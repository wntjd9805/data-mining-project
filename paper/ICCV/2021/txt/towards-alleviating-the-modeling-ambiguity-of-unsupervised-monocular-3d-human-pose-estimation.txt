Abstract
In this work, we study the ambiguity problem in the task of unsupervised 3D human pose estimation from 2D counterpart. On one hand, without explicit annotation, the scale of 3D pose is difﬁcult to be accurately captured (scale ambiguity). On the other hand, one 2D pose might correspond to multiple 3D gestures, where the lifting pro-cedure is inherently ambiguous (pose ambiguity). Previ-ous methods generally use temporal constraints (e.g., con-stant bone length and motion smoothness) to alleviate the above issues. However, these methods commonly enforce the outputs to fulﬁll multiple training objectives simulta-neously, which often lead to sub-optimal results.
In con-trast to the majority of previous works, we propose to split the whole problem into two sub-tasks, i.e., optimizing 2D input poses via a scale estimation module and then map-ping optimized 2D pose to 3D counterpart via a pose lifting module. Furthermore, two temporal constraints are pro-posed to alleviate the scale and pose ambiguity respectively.
These two modules are optimized via a iterative training scheme with corresponding temporal constraints, which ef-fectively reduce the learning difﬁculty and lead to better performance. Results on the Human3.6M dataset demon-strate that our approach improves upon the prior art by 23.1% and also outperforms several weakly supervised ap-proaches that rely on 3D annotations. Our project is avail-able at https://sites.google.com/view/ambiguity-aware-hpe. 1.

Introduction
Human pose estimation has received considerable atten-tion in computer vision community [2, 6, 28, 37]. As a fun-damental module, it is widely used in many downstream applications, such as body reconstruction [14], robotics ma-nipulation [26], and augmented reality [10]. In this paper, we are interested in unsupervised monocular 3D pose esti-mation. Due to the high-cost and time-consuming annota-*corresponding author tion procedure of 3D skeleton, unsupervised / weakly super-vised 3D pose estimation [40, 3] has turned into an emerg-ing trend in this ﬁeld.
Recent unsupervised approaches [29, 12, 19, 17], i.e., without access to 3D annotations in any form, mainly use 2D annotations [3], unlabelled multi-view imagery [12] or learned 3D priors [17] to bypass the need of 3D annotation.
Compared to easily accessible 2D annotations, the manual 3D priors are tedious and employing the multi-view images requires speciﬁc multi-camera equipment. Recently, Chen et.al. [3] propose a geometric constraint for single-frame unsupervised 3D pose estimation, which get rid of the need for the multi-view cameras.
However, there still exist two challenges remained to be solved: (a) Scale ambiguity. The scale of 3D pose is hard to be accurately captured if without the supervision of 3D annotations. We experimentally ﬁnd that the scale of esti-mated 3D skeletons under unsupervised setting is prone to drift far away from ground truth. Simply enforcing the scale consistency of the predicted 3D skeleton (i.e., bone length consistency loss [20]) only leads to marginal improvements, which is supported by our ablation study in Sec 4.4. (b)
Pose ambiguity. Lifting 2D pose to 3D counterpart is in-herently ambiguous [19], where a single 2D pose possibly corresponds to multiple 3D poses. Multi-view data [3] is able to effectively address such kind of ambiguity. Chen et.al. [3] ﬁrstly propose to generate pseudo view to alle-viate the ambiguity, which, however, ignores the tempo-ral constraints between frames. 3D constraints (e.g., cycle loss [3], bone length consistency loss [20], camera projec-tion loss [17]) have been previously proposed to address the above two challenges. However, they are commonly con-sidered as auxiliary losses, i.e., enforcing the outputs to ful-ﬁll multiple training objectives simultaneously. Such kind of training scheme often leads to sub-optimal results [36].
To solve the above challenges, we propose to split the whole problem into two sub-tasks, i.e., optimizing 2D pose via scale estimation (short termed as scale estimation) and lifting optimized 2D pose to 3D counterpart (short termed as pose lifting). Furthermore, two temporal consistency con-Figure 1: The detailed architecture of proposed framework. 3D pose estimation task is split into two parts, i.e., scale estimation module (green box) and pose lifting module (orange box). Pose lifting module contains two lifting networks and one discriminator. The lifting network takes the scaled 2D pose as input and outputs estimated 3D skeleton. Random projection of generated 3D skeleton goes through the lifting network, inverse transformation, and re-projection process again, allowing the network to self-supervise the training process by exploiting temporal geometric consistency. It should be noted that the scale estimation module and the pose lifting modules are trained iteratively in an end-to-end manner. straints are incorporated into these two sub-tasks respec-tively: (a) Temporal scale consistency. Scale estimation module is used to optimize the scale of 3D pose. Accord-ing to perspective projection [3], optimizing the scale of 3D skeletons equals to changing the scale of 2D pose and we experimentally ﬁnd that constraining 2D scale is slightly better than 3D counterpart. Firstly, we propose a distribu-tion constraint to coarsely adjust the 2D scale at the video level. Secondly, a bone constraint is proposed to optimize the scale of 2D pose at the frame level. Both aspects of con-straints are seamlessly integrated into the scale estimation module, which effectively overcomes the scale ambiguity problem along the temporal direction. Pose lifting mod-ule then takes optimized 2D pose as input, where the scale estimation module helps reduce the learning difﬁculty of pose lifting and improve the estimation accuracy on in-the-wild data [24] by a large margin. (b) Multi-view motion consistency. Multi-view data [5], even synthesised from imagery [5], has shown its efﬁciency on single-frame 3D pose estimation. Inspired by the above operation, we pro-pose a simple yet effective temporal constraint, which nat-urally generalizes the single-frame multi-view constraint to the video data (i.e., the motion trajectories across different views are encouraged to match with each other). To en-hance the training stability, we propose an iterative training strategy, which achieves promising performance.
Extensive experiments demonstrate that our model achieves the state-of-the-art performance on two widely used 3D human motion datasets. Results on the Hu-man3.6M [11] dataset for 3D human pose estimation exhibit that our approach improves upon the previous unsupervised methods by 23.1% and also outperforms several weakly su-pervised approaches that explicitly use 3D annotations. We also conduct detailed ablation studies to demonstrate the contribution of each component of the proposed framework. 2.