Abstract
Compared with the visual grounding on 2D images, the natural-language-guided 3D object localization on point clouds is more challenging.
In this paper, we propose a new model, named InstanceRefer1, to achieve a superior 3D visual grounding through the grounding-by-matching strategy.
In practice, our model ﬁrst predicts the target category from the language descriptions using a simple language classiﬁcation model. Then, based on the cate-gory, our model sifts out a small number of instance candi-dates (usually less than 20) from the panoptic segmentation on point clouds. Thus, the non-trivial 3D visual ground-ing task has been effectively re-formulated as a simpli-ﬁed instance-matching problem, considering that instance-level candidates are more rational than the redundant 3D object proposals. Subsequently, for each candidate, we perform the multi-level contextual inference, i.e., referring from instance attribute perception, instance-to-instance re-lation perception, and instance-to-background global local-ization perception, respectively. Eventually, the most rele-vant candidate is selected and localized by ranking conﬁ-dence scores, which are obtained by the cooperative holis-tic visual-language feature matching. Experiments conﬁrm that our method outperforms previous state-of-the-arts on
ScanRefer online benchmark and Nr3D/Sr3D datasets. 1.

Introduction
Visual grounding (VG), which aims at localizing the de-sired objects or areas in an image or a video based on an object-related linguistic query, has achieved great progress in the 2D computer vision community [12, 18, 29, 17, 19].
With the rapid development of 3D sensor and 3D represen-tation, the VG task has gradually merged more informative
* Corresponding author: Zhen Li. † Equal ﬁrst authorship. 1 https://github.com/CurryYuan/InstanceRefer
Figure 1. Multi-level contextual referring. For each instance-level candidate, we match it with linguistic query from attribute, local relation and global localization. The attribute, relation and localization descriptions are in orange, blue and green boxes. 3D data. Unlike 2D images with regular and well-organized pixels, 3D data mostly comes in the form of point clouds, which is sparse, irregular, and unordered. Therefore, pre-vious 2D-based schemes are usually deﬁcient for real 3D scenarios.
Chen et al. [2] is the pioneer for visual grounding on point clouds. They propose the ﬁrst dataset ScanRefer and solve the problem by extending the 2D grounding-by-detection pipeline to 3D. Speciﬁcally, it ﬁrst uses a 3D ob-ject detector [24] to generate hundreds of proposals. Then the feature of each proposal is merged with a global repre-sentation of the linguistic query to predict a matching score.
The proposal with the maximal score is considered as the
object we are looking for. However, it suffers from sev-eral issues when transferring the 2D method to 3D VG as follows: 1) The object proposals in the large 3D scene are usually redundant. Compared with the actual instances, the number of proposals is large and the inter-proposal relation-ship is complex, which inevitably introduces noise and am-biguity. 2) The appearance and attribute information is not sufﬁciently captured. Due to the noise and occlusion, the obtained point clouds are usually sparse and incomplete, leading to missing geometric details in object-aware pro-posals. Conventional point cloud-based methods fail to ef-fectively extract the attribute information, e.g., red, gray, and wooden, which might ignore some vital linguistic cues for referring. 3) The relations among proposals and the ones between proposals and background are not fully studied.
To address the above issues, this paper investigates a novel framework, namely InstanceRefer, to achieve a su-perior visual grounding on point clouds with grounding-by-matching strategy. Speciﬁcally, via the global panop-tic segmentation, our proposed model extracts several in-stance point clouds from the original scene. These in-stances are subsequently ﬁltered by the predicted category from the natural language descriptions, obtaining the candi-dates set. Compared with the object-proposal based candi-dates [2], these ﬁltered instance point clouds contain more original geometric and attribute details (i.e., color, texture, etc.) while maintaining a smaller number. We notice that the recent work TGNN [10] also employs instance segmen-tation to reduce the difﬁculty of referring. However, they directly exploit the learned semantic scores from the seg-mentation backbone as the instance features, which suffer from the lossy geometric and attribute information. By comparison, our InstanceRefer applied ﬁltered candidates and their original information for further referring. Thus, it can not only reduce the number of the candidates, but also maintain each candidate’s original information. Be-sides, to fully comprehend the whole scene, multi-level con-textual learning modules are further proposed, i.e., explic-itly capturing the context of each candidate from instance attributes, instance-to-instance relationships, and instance-to-background global localization, respectively. Eventually, with the well-designed matching module and contrastive strategy, InstanceRefer can efﬁciently and effectively select and localize the target. In consequence, our model outper-forms previous methods by a large margin regardless of any settings, i.e., exploiting any segmentation backbone.
In summary, the key contributions of this paper are as follows: 1) We propose a new framework InstanceRefer for visual grounding on point clouds, which exploits panop-tic segmentation and language cues to select the instance point clouds as candidates and re-formulates the task in a grounding-by-matching manner. 2) Three novel compo-nents are proposed to select the most relevant instance can-didate from attributes, local relations, and global localiza-tion aspects jointly. 3) Experimental results on ScanRe-fer and Sr3D/Nr3D datasets conﬁrm the superiority of In-stanceRefer, which achieves state-of-the-arts on ScanRefer benchmark and Nr3D/Sr3D dataset. 2.