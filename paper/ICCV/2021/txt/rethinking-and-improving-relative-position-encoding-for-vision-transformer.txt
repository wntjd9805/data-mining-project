Abstract
Relative position encoding (RPE) is important for trans-former to capture sequence ordering of input tokens. Gen-eral efﬁcacy has been proven in natural language process-ing. However, in computer vision, its efﬁcacy is not well studied and even remains controversial, e.g., whether rela-tive position encoding can work equally well as absolute position? In order to clarify this, we ﬁrst review exist-ing relative position encoding methods and analyze their pros and cons when applied in vision transformers. We then propose new relative position encoding methods dedi-cated to 2D images, called image RPE (iRPE). Our methods consider directional relative distance modeling as well as the interactions between queries and relative position em-beddings in self-attention mechanism. The proposed iRPE methods are simple and lightweight. They can be eas-ily plugged into transformer blocks. Experiments demon-strate that solely due to the proposed encoding methods,
DeiT [21] and DETR [1] obtain up to 1.5% (top-1 Acc) and 1.3% (mAP) stable improvements over their original versions on ImageNet and COCO respectively, without tun-ing any extra hyperparameters such as learning rate and weight decay. Our ablation and analysis also yield in-teresting ﬁndings, some of which run counter to previ-ous understanding. Code and models are open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE. 1.

Introduction
Transformer recently has drawn great attention in com-puter vision because of its competitive performance and su-perior capability in capturing long-range dependencies [1, 2, 7, 21, 24]. The core of transformer is self-attention [22], which is capable of modeling the relationship of tokens in a sequence. Self-attention, however, has an inherent deﬁ-ciency — it cannot capture the ordering of input tokens.
∗Equal contributions. Work performed when Kan and Minghao were interns of MSRA. † Corresponding author: houwen.peng@microsoft.com
Therefore, incorporating explicit representations of position information is especially important for transformer, since the model is otherwise entirely invariant to sequence order-ing, which is undesirable for modeling structured data.
There are mainly two classes of methods to encode po-sitional representations for transformer. One is absolute, while the other is relative. Absolute methods [8, 22] en-code the absolute positions of input tokens from 1 to maxi-mum sequence length. That is, each position has an individ-ual encoding vector. The encoding vector is then combined with the input token to expose positional information to the model. On the other hand, relative position methods [4, 17] encode the relative distance between input elements and learn the pairwise relations of tokens. Relative position encoding (RPE) is commonly calculated via a look-up ta-ble with learnable parameters interacting with queries and keys in self-attention modules [17]. Such scheme allows the modules to capture very long dependencies between tokens.
Relative position encoding has been veriﬁed to be effective in natural language processing [4, 6, 15, 25]. However, in computer vision, the efﬁcacy is still unclear. There are few recent works [3, 7, 18] shedding light on it, but obtaining controversial conclusions in vision transformers. For exam-ple, Dosovitskiy et al. [7] observed that the relative position encoding does not bring any gain comparing to the abso-lute one (please refer to Tab. 8 in [7]). On the contrary,
Srinivas et al. [18] found that relative position encoding can induce an apparent gain, being superior to the absolute one (please refer to Tab. 4 in [18]). Moreover, the mostly recent work [3] claims that the relative positional encoding cannot work equally well as the absolute ones (please refer to Tab. 5 in [3]). These works draw different conclusions on the effectiveness of relative position encoding in models, that motivates us to rethink and improve the usage of relative positional encoding in vision transformer.
On the other hand, the original relative position encoding is proposed for language modeling, where the input data is 1D word sequences [4, 17, 22]. But for vision tasks, the inputs are usually 2D images or video sequences, where the pixels are highly spatially structured. It is unclear that:
whether the naive extension from 1D to 2D is suitable for vision models; whether the directional information is im-portant in vision tasks?
In this paper, we ﬁrst review existing relative position en-coding methods, and then propose new methods dedicated to 2D images. We make the following contributions.
• We analyze several key factors in relative position en-coding, including the relative direction, the importance of context, the interactions between queries, keys, val-ues and relative position embeddings, and computa-tional cost. The analysis presents a comprehensive un-derstanding of relative position encoding, and provides empirical guidelines for new method design.
• We introduce an efﬁcient implementation of relative encoding, which reduces the computational cost from the original O(n2d) to O(nkd), where k (cid:2) n. Such implementation is suitable for high-resolution input images, such as object detection and semantic segmen-tation, where the token number might be very large.
• We propose four new relative position encoding meth-ods, called image RPE (iRPE), dedicated to vision transformers, considering both efﬁciency and gener-alizability. The methods are simple and can be eas-ily plugged into self-attention layers. Experiments show that, without adjusting any hyperparameters and settings, the proposed methods can improve DeiT-S [21] and DETR-ResNet50 [1] by 1.5% (top-1 Acc) and 1.3% (mAP) over their original models on Ima-geNet [5] and COCO [12], respectively.
• We answer previous controversial questions. We em-pirically demonstrate that relative position encoding can replace the absolute encoding for image classiﬁ-cation task. Meanwhile, the absolute encoding is nec-essary for object detection, where the pixel position is important for object localization. 2.