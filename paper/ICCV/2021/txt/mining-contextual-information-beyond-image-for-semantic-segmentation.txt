Abstract
This paper studies the context aggregation problem in se-mantic image segmentation. The existing researches focus on improving the pixel representations by aggregating the contextual information within individual images. Though impressive, these methods neglect the significance of the representations of the pixels of the corresponding class be-yond the input image. To address this, this paper proposes to mine the contextual information beyond individual im-ages to further augment the pixel representations. We first set up a feature memory module, which is updated dynam-ically during training, to store the dataset-level represen-tations of various categories. Then, we learn class prob-ability distribution of each pixel representation under the supervision of the ground-truth segmentation. At last, the representation of each pixel is augmented by aggregating the dataset-level representations based on the correspond-ing class probability distribution. Furthermore, by utilizing the stored dataset-level representations, we also propose a representation consistent learning strategy to make the classification head better address intra-class compactness and inter-class dispersion. The proposed method could be effortlessly incorporated into existing segmentation frame-works (e.g., FCN, PSPNet, OCRNet and DeepLabV3) and brings consistent performance improvements. Mining con-textual information beyond image allows us to report state-of-the-art performance on various benchmarks: ADE20K,
LIP, Cityscapes and COCO-Stuff 1. 1.

Introduction
Semantic segmentation is a long-standing challenging task in computer vision, aiming to assign semantic labels to each pixel in an image accurately. This task is of broad in-*This work was performed while Zhenchao Jin worked as an intern at
ByteDance.
†Corresponding author. 1Our code will be available at https://github.com/CharlesPikachu/ mcibi.
Figure 1. Mining contextual information beyond image (MCIBI).
The feature memory module stores the dataset-level representa-tions of various classes. The dotted line denotes that the current input image will be added into the historical input images after each iteration during training. terest for potential application of autonomous driving, med-ical diagnosing, robot sensing, to name a few.
In recent years, deep neural networks [21, 36] have been the domi-nant solutions [1, 3, 5, 6, 8, 15, 39, 50], where the encoder-decoder architecture proposed in FCN [32] is the corner-stone of these methods. Specifically, these studies include adopting graphical models or cascade structure to further re-fine the segmentation results [4, 10, 10, 53], designing novel backbone networks [38,44,49] to extract more effective fea-ture representations, aggregating reasonable contextual in-formation to augment pixel representations [5, 39, 50] that is also the interest of this paper, and so on.
Since there exist co-occurrent visual patterns, modeling context is universal and essential for semantic image seg-mentation. Prior to this paper, the context of a pixel typi-cally refers to a set of other pixels in the input image, e.g., the surrounding pixels. Specifically, PSPNet [50] utilizes pyramid spatial pooling to aggregate contextual informa-tion. DeepLab [5, 6, 8] exploits the contextual information by introducing the atrous spatial pyramid pooling (ASPP).
OCRNet [45] proposes to improve the representation of each pixel by weighted aggregating the object region rep-resentations. Several recent works [15, 22, 39, 46] first cal-culate the relations between a pixel and its contextual pixels,
and then aggregate the representations of the contextual pix-els with higher weights for similar representations. Apart from these methods, some studies [1, 23, 52] also find that the pixel-wise cross entropy loss fundamentally lacks the spatial discrimination power and thereby, propose to ver-ify segmentation structures directly by designing context-aware optimization objectives. Nonetheless, all of the ex-isting approaches only model context within individual im-ages, while discarding the potential contextual information beyond the input image. Basically, the purpose of seman-tic segmentation based on deep neural networks essentially groups the representations of the pixels of the whole dataset in a non-linear embedding space. Consequently, to classify a pixel accurately, the semantic information of the corre-sponding class in the other images also makes sense.
To overcome the aforementioned limitation, this paper proposes to mine the contextual information beyond the in-put image so that the pixel representations could be further improved. As illustrated in Figure 1, we first set up a feature memory module to store the dataset-level representations of various categories by leveraging the historical input images during training. Then, we predict the class probability dis-tribution of the pixel representations in the current input im-age, where the distribution is under the supervision of the ground-truth segmentation. At last, each pixel representa-tion is augmented by weighted aggregating the dataset-level representations where the weight is determined by the cor-responding class probability distribution. Furthermore, to address intra-class compactness and inter-class dispersion from the perspective of the whole dataset more explicitly, a representation consistent learning strategy is designed to make the classification head learn to classify both 1) the dataset-level representations of various categories and 2) the pixel representations of the current input image.
In a nutshell, our main contributions could be summa-rized as:
• To the best of our knowledge, this paper first explores the approach of mining contextual information beyond the input image to further boost the performance of se-mantic image segmentation.
• A simple yet effective feature memory module is de-signed for storing the dataset-level representations of various categories. Based on this module, the contex-tual information beyond the input image could be ag-gregated by adopting the proposed dataset-level con-text aggregation scheme, so that these contexts can fur-ther enhance the representation capability of the origi-nal pixel representations.
• A novel representation consistent learning strategy is designed to better address intra-class compactness and inter-class dispersion.
The proposed method can be seamlessly incorporated into existing segmentation networks (e.g., FCN [32], PSP-Net [50], OCRNet [45] and DeepLabV3 [6]) and brings consistent improvements. The improvements demonstrate the effectiveness of mining contextual information beyond the input image in the semantic segmentation task. Fur-thermore, introducing the dataset-level contextual informa-tion allows this paper to report state-of-the-art intersection-over-union segmentation scores on various benchmarks:
ADE20K, LIP, Cityscapes and COCO-Stuff. We expect this work to present a novel perspective for addressing the con-text aggregation problem in semantic image segmentation. 2.