Abstract
Gradient-based algorithms are crucial to modern com-puter-vision and graphics applications, enabling learning-based optimization and inverse problems. For example, photorealistic differentiable rendering pipelines for color images have been proven highly valuable to applications aiming to map 2D and 3D domains. However, to the best of our knowledge, no effort has been made so far towards ex-tending these gradient-based methods to the generation of depth (2.5D) images, as simulating structured-light depth sensors implies solving complex light transport and stereo-matching problems. In this paper, we introduce a novel end-to-end differentiable simulation pipeline for the generation of realistic 2.5D scans, built on physics-based 3D rendering and custom block-matching algorithms. Each module can be differentiated w.r.t. sensor and scene parameters; e.g., to automatically tune the simulation for new devices over some provided scans or to leverage the pipeline as a 3D-to-2.5D transformer within larger computer-vision applica-tions. Applied to the training of deep-learning methods for various depth-based recognition tasks (classification, pose estimation, semantic segmentation), our simulation greatly improves the performance of the resulting models on real scans, thereby demonstrating the fidelity and value of its synthetic depth data compared to previous static simula-tions and learning-based domain adaptation schemes. 1.

Introduction
Progress in computer vision has been dominated by deep neural networks trained over large amount of data, usu-ally labeled. The deployment of these solutions into real-world applications is, however, often hindered by the cost (time, manpower, access, etc.) of capturing and annotat-ing exhaustive training datasets of target objects or scenes.
To partially or completely bypass this hard data require-ment, an increasing number of solutions are relying on syn-thetic images rendered from 3D databases for their train-ing [15, 51, 36, 46, 61, 45], leveraging advances in com-‡Now at NVIDIA.
Figure 1: Differentiable Depth Sensor Simulation (DDS) for the generation of highly-realistic depth scans. DDS works off-the-shelf, but can be further optimized unsuper-visedly against real data, yielding synthetic depth scans valuable to the training of recognition algorithms (demon-strated here on LineMOD dataset [21]). puter graphics [50, 44].
Indeed, physics-based rendering methods are slowly but surely closing the visual gap be-tween real and synthetic color image distributions, simulat-ing complex optical phenomena (e.g., realistic light trans-port, lens aberrations, Bayer demosaicing, etc.). While these extensive tools still require domain knowledge to be properly parameterized for each new use-case (w.r.t. scene content, camera properties, etc.), their positive impact on the training of color-based visual recognition algorithms has been well documented already [9, 22].
The same cannot be said about depth-based applica-tions. Unlike color camera that captures light intensity, structured-light depth sensors rely on stereo-vision mech-anisms to measure the per-pixel distance between their fo-cal plane and elements in the scene. They are useful for geometry-sensitive applications (e.g., robotics), but little ef-fort has been made towards closing the realism gap w.r.t. synthetic depth (2.5D) scans or understanding their im-pact on the training of depth-based recognition methods.
Some simulation pipelines [19, 33, 46] and domain adap-tation schemes [55, 16, 54, 5, 63, 61] have been proposed; but the former methods require extensive domain knowl-edge [46, 63] to be set up whereas some of the latter need relevant real images for their training [55, 16, 54, 4], and all fail to generalize to new sensors [19, 33] or scenes [4, 63].
Borrowing from both simulation and learning-based principles, we propose herein a novel pipeline that virtually replicates depth sensors and can be optimized for new use-cases either manually (e.g., providing known intrinsic pa-1
Figure 2: Pipeline overview. DDS differentiably simulates the physics and algorithmic mechanisms of real depth sensors. rameters of a new sensor) or automatically via supervised or unsupervised gradient descent (e.g., optimizing the pipeline over a target noise model or real scans). Adapting recent differentiable ray-tracing techniques [35, 64, 27] and im-plementing novel soft stereo-matching solutions, our simu-lation is differentiable end-to-end and can therefore be op-timized via gradient descent, or integrated into more com-plex applications interleaving 3D graphics and neural net-works. As demonstrated throughout the paper, our solution can off-the-shelf render synthetic scans as realistic as non-differentiable simulation tools [19, 33, 46], outperforming them after unsupervised optimization. Applied to the train-ing of deep-learning solutions for various visual tasks, it also outperforms unconstrained domain adaptation and ran-domization methods [53, 5, 63, 61], i.e., resulting in higher task accuracy over real data; with a much smaller set of pa-rameters to optimize. In summary, our contributions are:
Differentiable Depth Sensor Simulation (DDS) – we in-troduce DDS, an end-to-end differentiable, physics-based, simulation pipeline for depth sensors. As detailed in Sec-tion 3, DDS reproduces the structured-light sensing and stereo-matching mechanisms of real sensors, off-the-shelf generating realistic 2.5D scans from virtual 3D scenes.
Optimizable Simulation through Gradient Descent – Be-cause DDS is differentiable w.r.t. most of the sensor and scene parameters, it can learn to better simulate new de-vices or approximate unaccounted-for scene properties in supervised or unsupervised settings. It can also be tightly incorporated within larger deep-learning pipeline, e.g., as a differentiable 3D-to-2.5D mapping function.
Benefits to Deep-Learning Recognition Methods – we demonstrate in Section 4 that DDS is especially beneficial to recognition solutions that must rely on synthetic data. The various methods (for depth-based object classification, pose estimation, or segmentation) trained with DDS performed significantly better when tested on real data, compared to the same methods trained with previous simulation tools or domain adaptation algorithms surveyed in Section 2. 2.