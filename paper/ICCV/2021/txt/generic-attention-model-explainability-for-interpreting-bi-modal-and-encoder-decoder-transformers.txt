Abstract to the prediction in the model’s input.
Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achiev-ing state-of-the-art results thanks to their ability to con-textualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transform-ers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is rel-In this evant work, we propose the ﬁrst method to explain prediction by any Transformer-based architecture, including bi-modal
Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability. Our code is avail-https://github.com/hila-chefer/ able at:
Transformer-MM-Explainability. 1.

Introduction
Multi-modal Transformers may change the way that computer vision is practiced. While the state of the art com-puter vision models are often trained as task-speciﬁc mod-els that infer a ﬁxed number of labels, Radford et al. [28] have demonstrated that by training an image-text model that employs Transformers for encoding each modality, tens of downstream tasks can be performed, without further train-ing (“zero-shot”), at comparable accuracy to the state of the art. Subsequently, Ramesh et al. [30] used a bi-modal
Transformer to generate images that match a given descrip-tion in unseen domains with unprecedented performance.
These two contributions merge text and images differ-ently. The ﬁrst encodes the text with a Transformer [40], the image by either a ResNet [15] or a Transformer, and then applies a symmetric contrastive loss. The second con-catenates the quantized image representation to the text tokens and then employs a Transformer model. There are also many other methods of combining text and im-ages [38, 21, 19, 18]. What is common to all of these is that the mapping from the two inputs to the prediction con-tains interaction between the two modalities. These inter-actions often challenge the existing explainability methods that are aimed at attention-based models, since, as far as we can ascertain, all existing Transformer explainability meth-ods (e.g., [5, 1]) heavily rely on self-attention, and do not provide adaptations to any other form of attention, which is commonly used in multi-modal Transformers.
Another class of Transformer models that is not re-stricted to self-attention is that of Transformer encoder-decoders, i.e. generative models, in which the model typ-ically receives an input from a single domain, and produces output from a different one. These models are used in an emerging class of object detection [4, 50] and image seg-mentation [43, 27, 42] methods, and are also widely used for various NLP tasks, such as machine translation [40, 17].
In these object detection methods, for example, embed-dings of the position-speciﬁc and class-speciﬁc queries are crossed with the encoded image information. (i) pure self-attention,
We propose the ﬁrst explainability method that is ap-plicable to all Transformer architectures, and demonstrate its effectiveness on the three most commonly used Trans-former architectures: (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We use an exemplar model from each architecture, and prove our method’s superiority over exist-ing Transformer explainability methods, adapted from their single modality origin. Our explainability prescription is easier to implement than existing methods, such as [5], and can be readily applied to any attention-based architecture.
2.