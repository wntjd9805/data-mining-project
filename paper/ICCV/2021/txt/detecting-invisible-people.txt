Abstract 1.

Introduction
Monocular object detection and tracking have improved drastically in recent years, but rely on a key assumption: that objects are visible to the camera. Many ofﬂine tracking ap-proaches reason about occluded objects post-hoc, by linking together tracklets after the object re-appears, making use of reidentiﬁcation (ReID). However, online tracking in em-bodied robotic agents (such as a self-driving vehicle) funda-mentally requires object permanence, which is the ability to reason about occluded objects before they re-appear. In this work, we re-purpose tracking benchmarks and propose new metrics for the task of detecting invisible objects, focusing on the illustrative case of people. We demonstrate that current detection and tracking systems perform dramatically worse on this task. We introduce two key innovations to recover much of this performance drop. We treat occluded object detection in temporal sequences as a short-term forecasting challenge, bringing to bear tools from dynamic sequence prediction. Second, we build dynamic models that explicitly reason in 3D from monocular videos without calibration, using observations produced by monocular depth estima-tors. To our knowledge, ours is the ﬁrst work to demonstrate the effectiveness of monocular depth estimation for the task of tracking and detecting occluded objects. Our approach strongly improves by 11.4% over the baseline in ablations and by 5.0% over the state-of-the-art in F1 score.
Object detection has seen immense progress, albeit under a seemingly harmless assumption: that objects are visible to the camera in the image. However, objects that become fully occluded (and thus, invisible) continue to exist and move in the world. Indeed, object permanence is a fundamental visual cue exhibited by infants in as early as 3 months [3, 26].
Practical autonomous systems must similarly reason about such objects that undergo complete occlusions to ensure safe operation (Figure 1). Interestingly, existing work on object detection and tracking tends to de-emphasize this capabil-ity, either choosing to completely ignore highly-occluded instances for evaluation [15, 37, 50, 58], or simply down-weighting them because they occur so rarely that they fail to materially affect overall performance [41]. One reason that invisible-object detection may have been under-emphasized in the tracking community is that for ofﬂine analysis, one can post-hoc reason about the presence of an occluded object by relinking detections after it reappears. This approach has spawned the large subﬁeld of reidentiﬁcation (ReID). How-ever, in an online setting (such as an autonomous vehicle that must make decisions given the available sensor infor-mation), intelligent agents must be able to instantaneously reason about occluded objects before they re-appear.
Problem formulation: We begin by introducing bench-marks and metrics for evaluating the task of detecting and
tracking invisible people. To do so, we repurpose existing tracking benchmarks and introduce metrics for evaluating this task that appropriately reward detection of occluded peo-ple. To ensure benchmarks are online, we forbid algorithms from accessing future frames when reporting object states for the current frame. Although this task requires reasoning about object trajectories, it can be evaluated as both a de-tection and a tracking problem. For the latter, we introduce extensions to tracking metrics in the supplement. When ana-lyzing our metrics, it becomes readily apparent that human annotation of ground-truth occluded objects is challenging.
We provide pilot human vision experiments in Section 4 that show annotators are still consistent, but exhibit larger varia-tion in labeling the pixel position of occluded instances. This suggests that algorithms for occluded object detection should report distributions over object locations rather than precise discrete (bounding box) locations. Inspired by metrics for evaluating multimodal distributions in the forecasting liter-ature [9], we explore probabilistic algorithms that make k predictions which are evaluated by Top-k accuracy.
Analysis: Perhaps not surprisingly, our ﬁrst observation is that performance of state-of-the-art detectors and trackers plummets on occluded people, from 68.5% to 28.4%; it is far easier to detect visible objects than invisible ones! This underscores the need for the community to focus on this underexplored problem. We introduce two simple but key innovations for addressing this task, which improve perfor-mance from 28.4% to 39.8%. (a) We recast the problem of online tracking of occluded objects as a short-term fore-casting challenge. We explore state-of-the-art deep forecast-ing networks, but ﬁnd that classic linear dynamics models (Kalman ﬁlters) perform quite well. (b) Because modeling occlusions is of central importance, we cast the problem as one of 3D tracking given 2D image measurements.
Novelty: While there exists considerable classic work on 3D tracking from 2D [48, 7, 52, 10], much focuses on 3D modeling of tracked objects. Instead, we ﬁnd that the 3D structure of scene occluders is important for understanding where tracked objects can “hide”. Typically such dense 3D understanding requires calibrated multiview sensors [53, 13].
Instead, we show that recent advances in uncalibrated monoc-ular depth estimation provide “good enough” estimates of relative depth that still enable dense freespace reasoning.
This is crucial because monocular depth has the potential to be far more scalable [55]. To our knowledge, ours is the ﬁrst work to use uncalibrated depth estimates for multi-object tracking and detection of occluded objects.
Overview: After reviewing related work, we present our core algorithmic contributions, including straightforward but crucial extensions to classic linear dynamics models to (a) incorporate putative depth observations from a monocular network and (b) forecast object state even during occlusions.
We conclude with extensive evaluations on three datasets [41, 54, 11] repurposed for detecting occluded objects. 2.