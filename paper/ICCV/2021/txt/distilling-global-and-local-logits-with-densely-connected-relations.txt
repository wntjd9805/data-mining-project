Abstract
In prevalent knowledge distillation, logits in most image recognition models are computed by global average pool-ing, then used to learn to encode the high-level and task-relevant knowledge. In this work, we solve the limitation of this global logit transfer in this distillation context. We point out that it prevents the transfer of informative spatial information, which provides localized knowledge as well as rich relational information across contexts of an input scene. To exploit the rich spatial information, we propose a simple yet effective logit distillation approach. We add a local spatial pooling layer branch to the penultimate layer, thereby our method extends the standard logit distillation and enables learning of both finely-localized knowledge and holistic representation. Our proposed method shows fa-vorable accuracy improvement against the state-of-the-art methods on several image classification datasets. We show that our distilled students trained on the image classifica-tion task can be successfully leveraged for object detection and semantic segmentation tasks; this result demonstrates our method’s high transferability. 1.

Introduction
Knowledge distillation is a method of transferring knowledge of a large network (i.e., teacher) to a smaller neural network (i.e., student). Unlike human-designed prior knowledge, the distillation is an optimization method that uses the representation of the network as prior knowledge.
More specifically, the student is trained with respect to re-ducing a task-specific objective function, and the difference in knowledge from the teacher.
Due to the simplicity and effectiveness of targeting the teacher that has higher accuracy than the student, many re-searchers have used the distillation method to achieve the state-of-the-art accuracy on ImageNet dataset [11] of the
*The work was done when Youmin Kim was at Kyung Hee University
†Corresponding author
Figure 1. Overview of our method. The global and local logits and their densely connected relationships are used for the logit distillation. “Conv Layers” denote convolutional layers of teacher and student networks. network [53, 43]. For the same reason, the distillation has been combined with model-compression methods such as pruning [31, 37] and quantization [25, 4], or other optimiza-tion methods such as data augmentation [51] and ensemble
[58]. In addition, the distillation is used not only for the image classification tasks, but also other vision tasks such as image super-resolution [17, 30], object detection [7, 49] and semantic segmentation [52, 36].
Depending on the representation levels of features to be transferred, knowledge distillation methods are divided into
two types: i) feature distillation that exploits the output fea-tures of intermediate convolutional layers [44, 55, 56, 23, 26, 19, 1, 39, 35, 18, 46, 45]; and ii) logit distillation that exploits the output logit in the final classifier [2, 20].
The logit is high-level and task-relevant knowledge with class information but loses spatial information due to the global average pooling, which spatially averages on the features of the last convolutional layer (i.e., penultimate layer). Most existing deep neural networks use global av-erage pooling [32] for the object classification task because the pooling both significantly reduces the model parame-ters and prevents overfitting of the network while retaining the network’s invariance to bounded spatial variants (e.g., translation, rotation, flipping) of the input data. However, in the logit distillation, the student only learns compressed knowledge, which includes no spatial information of the in-put data from the teacher.
Many studies have shown that spatial information [29, 15, 40, 5] and spatial relationships [21, 50, 22, 54] are essential factors for performance improvements in various computer vision tasks. To exploit both spatial information and spatial relationships for logit distillation, we propose a novel global and local logit distillation method (GLD) that transfers not only the global and local logits but also the relationships among the global and local logits of multiple input samples from the teacher to the student.
Figure 1 conceptualizes the global and local logits with their densely connected relationship from the teacher and student. Through the spatial pooling strategy in [15], we create the global logits from the features in the whole re-gion (global features) and the local logits from local regions (local features) in the penultimate layer. Furthermore, the densely connected relationship consists of the global and lo-cal logits from all the input samples in a mini-batch. Specif-ically, our relationship is defined with global and local logits not only within the one input sample (intra-relationship) but also among all input samples (inter-relationship).
Therefore, the student can learn spatial class information composed of the global and local logits from the teacher.
In addition, the student can learn not only the relationships among the global and local representations for one sample by the intra-relationship but also more detailed relationship among the all input samples in a mini-batch by the inter-relationship, densely connected through the global and local logits of each sample. The contributions of this work are summarized as follows:
• We propose a novel logit-distillation method that uses the global and local logits and their relationships within a single sample as well as among all samples in a mini-batch as knowledge.
• When using Kullback-Leibler (KL) divergence as knowledge distillation loss, we accommodate various distributions of both global and local logits by using the standard deviation of a logit as a softening factor.
• We validate the generalizability of our method on the image classification with benchmark datasets and its transferability to the object detection and semantic seg-mentation with various datasets. 2.