Abstract
Within Convolutional Neural Network (CNN), the con-volution operations are good at extracting local features but experience difficulty to capture global representations.
Within visual transformer, the cascaded self-attention mod-ules can capture long-distance feature dependencies but un-fortunately deteriorate local feature details. In this paper, we propose a hybrid network structure, termed Conformer, to take advantage of convolutional operations and self-attention mechanisms for enhanced representation learn-ing. Conformer roots in the Feature Coupling Unit (FCU), which fuses local features and global representations under different resolutions in an interactive fashion. Conformer adopts a concurrent structure so that local features and global representations are retained to the maximum extent.
Experiments show that Conformer, under the comparable parameter complexity, outperforms the visual transformer (DeiT-B) by 2.3% on ImageNet. On MSCOCO, it outper-forms ResNet-101 by 3.7% and 3.6% mAPs for object detec-tion and instance segmentation, respectively, demonstrating the great potential to be a general backbone network. Code is available at github.com/pengzhiliang/Conformer. 1.

Introduction
Convolutional neural networks (CNNs) [29, 37, 40, 19, 48, 22] have significantly advanced computer vision tasks such as image classification, object detection, and instance segmentation. This largely attributes to the convolution op-eration, which collects local features in a hierarchical fash-ion as powerful image representations. Despite of the ad-vantage upon local feature extraction, CNNs experience dif-ficulty to capture global representations, e.g., long-distance
*Corresponding author relationships among visual elements, which are often crit-ical for high-level computer visual tasks. An intuitive so-lution is enlarging the receptive field, which however could require more intensive yet damaging pooling operations.
Recently, the transformer architecture [42] has been in-troduced to visual tasks [16, 47, 41, 51, 8, 9, 3, 55, 28].
The ViT method [16] constructs a sequence of tokens by splitting each image to patches with positional embed-dings and applies cascaded transformer blocks to extract parameterized vectors as visual representations. Thanks to the self-attention mechanism and Multilayer Perceptron (MLP) structure, the visual transformer reflects complex spatial transforms and long-distance feature dependencies, which constitute global representations. Unfortunately, vi-sual transformers are observed ignoring local feature details which decreases the discriminability between background and foreground, Figs. 1(c) and (g). Improved visual trans-formers [16, 51] have proposed a tokenization module or leveraged CNN feature maps as input tokens to capture fea-ture neighboring information. Nevertheless, the problem about how to precisely embed local features and global rep-resentations to each other remains.
In this paper, we propose a dual network structure, termed Conformer, with the aim to couple CNN-based lo-cal features with transformer-based global representations for enhanced representation learning. Conformer consists of a CNN branch and a transformer branch which respec-tively follow the design of ResNet [19] and ViT [16]. The two branches form a comprehensive combination of local convolution blocks, self-attention modules, and MLP units.
During training, the cross entropy losses are used to su-pervise both the CNN and transformer branches to couple
CNN-style and transformer-style features.
Considering the feature misalignment between CNN and transformer features, the Feature Coupling Unit (FCU) is designed as the bridge. On the one hand, to fuse the two-Figure 1: Comparison of feature maps of CNN (ResNet-101) [19], Visual Transformer (DeiT-S) [41], and the proposed
Conformer. The patch embeddings in transformer are reshaped to feature maps for visualization. While CNN activates discriminative local regions (e.g., the peacock’s head in (a) and tail in (e)), the CNN branch of Conformer takes advantage of global cues from the visual transformer and thereby activates complete object (e.g., full extent of the peacock in (b) and (f)). Compared with CNN, local feature details of the visual transformer are deteriorated (e.g., (c) and (g)). In contrast, the transformer branch of Conformer retains the local feature details from CNN while depressing the background (e.g., the peacock contours in (d) and (h) are more complete than those in (c) and (g)). (Best viewed in color) style features, FCU leverages 1×1 convolution to align the channel dimensions, down/up sampling strategies to align feature resolutions, LayerNorm [2] and BatchNorm [25] to align feature values. On the other hand, since CNN and transformer branches tend to capture features of different levels (e.g., local vs. global), FCU is inserted into every block to consecutively eliminate the semantic divergence between them, in an interactive fashion. Such a fusion pro-cedure can greatly enhance the global perception capability of local features and the local details of global representa-tions.
The ability of Conformer in coupling local features and global representations is demonstrated in Fig. 1. While con-ventional CNNs (e.g., ResNet-101) tend to retain discrim-inative local regions (e.g., the peacock’s head or tail), the
CNN branch of Conformer can activate the full object ex-tent, Figs. 1(b) and (f). When solely using the visual trans-formers, for the weak local features (e.g., blurred object boundaries), it is difficult to distinguish the object from the background, Figs. 1(c) and (g). The coupling of local fea-tures and global representations significantly enhances the discriminability of transformer-based features, Figs. 1(d) and (h).
The contributions of this paper include:
• We propose a dual network structure, termed Con-former, which retains local features and global repre-sentations to the maximum extent.
• We propose the Feature Coupling Unit (FCU), to fuse convolutional local features with transformer-based global representations in an interactive fashion.
• Under comparable parameter complexity, Conformer outperforms CNNs and visual transformers by signif-icant margins. Conformer inherits the structure and generalization advantages of both CNNs and visual transformers, demonstrating the great potential to be a general backbone network. 2.