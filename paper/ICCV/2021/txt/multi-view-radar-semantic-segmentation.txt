Abstract
Understanding the scene around the ego-vehicle is key to assisted and autonomous driving. Nowadays, this is mostly conducted using cameras and laser scanners, de-spite their reduced performance in adverse weather con-ditions. Automotive radars are low-cost active sensors that measure properties of surrounding objects, including their relative speed, and have the key advantage of not being im-pacted by rain, snow or fog. However, they are seldom used for scene understanding due to the size and complexity of radar raw data and the lack of annotated datasets. For-tunately, recent open-sourced datasets have opened up re-search on classification, object detection and semantic seg-mentation with raw radar signals using end-to-end train-able models.
In this work, we propose several novel ar-chitectures, and their associated losses, which analyse mul-tiple “views” of the range-angle-Doppler radar tensor to segment it semantically. Experiments conducted on the re-cent CARRADA dataset demonstrate that our best model outperforms alternative models, derived either from the se-mantic segmentation of natural images or from radar scene understanding, while requiring significantly fewer param-eters. Both our code and trained models are available at https://github.com/valeoai/MVRSS. 1.

Introduction
Radar sensors have been used in the automotive industry for the last two decades, e.g., for automatic cruise control or blind spot detection. They have become the sensor of choice for applications requiring time to collision as they provide, besides localization, the relative velocity thanks to the Doppler information. However, these radars have been hindered previously by their poor angular resolution.
The shift from driving assistance to automated driving drastically increased the necessary level of performance,
Figure 1. Overview of our multi-view approach to semantic segmentation of radar signal. At a given instant, radar signals take the form of a range-angle-Doppler (RAD) tensor. Sequences of q + 1 2D views of this data cube are formed and mapped to a common latent space by the proposed multi-view architectures.
Two heads with distinct decoders produce a semantic segmenta-tion of the range-angle (RA) and range-Doppler (RD) views re-spectively (‘background’ in black, ‘pedestrian’ in red and ‘cyclist’ in green in this example). robustness and safety. Safety requires redundancy mech-anisms at all levels of the system: From the sensing parts to the final decision modules. Redundancy at the sensor level can be reached using three sensors of different nature such as camera, lidar and radar.
While deep learning has brought about major progress in the automotive use of cameras and lidars – for object detec-tion and segmentation in particular – it is only recently that it has also embraced radar signals. In fact, even though the radar technology has greatly improved, the signal process-ing pipeline has remained the same for years. This sensor is now a source of interest since public datasets have been released [3, 6, 15, 29, 33, 34, 45].
Radar scene understanding is in its infancy, but it pro-vides key information to compensate for weaknesses of the other sensors. The radar data presented as a range-angle-Doppler (RAD) tensor contains signatures of the objects surrounding the car with enough details to distinguish them in the representation. Unlike object detection using bound-ing boxes, semantic segmentation is appropriate for this task, since the object signatures have extremely variable sizes and may be mixed up due to the sensor’s resolution.
In this work, we propose an approach to multi-view radar semantic segmentation, illustrated in Fig. 1, that exploits the entire data while addressing the challenges of its large vol-ume and high level of noise. The segmentation is performed on the range-Doppler and range-angle views, which suffices to deduce the localisation and the relative speed of objects.
Our first contribution is a set of lightweight neural network architectures designed for multi-view semantic segmenta-tion of radar signal. The second contribution is a set of loss terms to train models on these tasks while preserving co-herence between the multi-view predictions. Experiments show that our best model outperforms other methods con-sidered for radar semantic segmentation in both quantitative and qualitative evaluations.
We present automotive radar sensing and related works in § 2, then our contributions and the methods that we com-pare (§ 3), experiments (§ 4) and conclusions (§ 5). 2.