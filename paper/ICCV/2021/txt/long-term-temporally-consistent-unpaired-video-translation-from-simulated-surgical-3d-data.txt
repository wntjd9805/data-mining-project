Abstract
Research in unpaired video translation has mainly fo-cused on short-term temporal consistency by conditioning on neighboring frames. However for transfer from sim-ulated to photorealistic sequences, available information on the underlying geometry offers potential for achieving global consistency across views. We propose a novel ap-proach which combines unpaired image translation with neural rendering to transfer simulated to photorealistic sur-gical abdominal scenes. By introducing global learnable textures and a lighting-invariant view-consistency loss, our method produces consistent translations of arbitrary views and thus enables long-term consistent video synthesis. We design and test our model to generate video sequences from minimally-invasive surgical abdominal scenes. Because la-beled data is often limited in this domain, photorealistic data where ground truth information from the simulated do-main is preserved is especially relevant. By extending exist-ing image-based methods to view-consistent videos, we aim to impact the applicability of simulated training and evalua-tion environments for surgical applications. Code and data: http://opencas.dkfz.de/video-sim2real. 1.

Introduction
One of the most promising applications of GAN-based image translation [14, 47] is the transfer from the simulated domain to realistic images as it presents great potential for applications in computer graphics. More importantly, un-paired translation [52] (i.e. no image correspondences be-tween domains required during training) enables the gener-ation of realistic data while preserving ground information from the simulated domain which would otherwise be dif-ficult to obtain (e.g. depth maps, optical flow or semantic segmentation). This synthetic data can then facilitate train-ing or evaluation in settings where labeled data is limited.
Figure 1. By combining unpaired image translation with a neu-ral rendering approach, we produce photorealistic and view-consistent renderings of simulated surgical scenes. Note that fine details like vessels are rendered consistently across viewpoints al-though they were not manually modelled in the simulated domain.
The availability of realistic, synthetic data is especially crucial in the field of computer-assisted surgery (CAS) [26, 5]. CAS aims at providing assistance to the surgical team (e.g. visualizing target structures or prediction of compli-cations) by means of analyzing available sensor data.
In minimally-invasive surgery, where instruments and a cam-era are inserted into the patient’s body through small ports,
Intelligent assis-video is the predominant data source.
tance systems are especially relevant here, since performing surgery through small ports and limited view is extremely challenging. However, two major factors which currently limit the impact of deep learning in CAS are the lack of (a) labeled training data and (b) realistic environments for evaluation [25]. For instance, evaluating a SLAM (Simul-taneous Localization and Mapping) algorithm [33, 43] on laparoscopic video data poses several problems since the patient’s ground truth geometry is typically not accessible in the operating room (OR) and recreating artificial testing environments with realistic and diverse patient phantoms is extremely challenging. Other CAS applications which could benefit from temporally consistent synthetic training data include action recognition, warning systems, surgical navigation and robot-assisted interventions [26, 25].
Previous research has shown the effectiveness of syn-thetic, surgical images as training data for downstream tasks such as liver segmentation [37, 41]. However, their applica-tions are still limited since many challenges in CAS include a temporal component. Using the previous example of eval-uating a SLAM algorithm, realistic as well as temporally consistent video sequences would have to be generated in order to provide a useful evaluation environment.
Unpaired video translation has recently garnered inter-est in various non-surgical specialties [3, 10, 7, 9, 34, 51].
Most approaches thereby condition the generator on pre-vious translated frames to achieve smooth transitions, i.e. short-term temporal consistency. However, they are fun-damentally not designed for long-term consistency.
Intu-itively, when an object entirely leaves the field of view, con-sistent rendering cannot be ensured when it returns since the previous frame contains no information regarding the ob-ject’s appearance. Even when the model is conditioned on multiple frames, the problem persists in longer sequences.
In the special case of translating from a simulated envi-ronment, however, the underlying geometry and camera tra-jectories are often available. Point correspondence between views are thus known and can be used to ensure globally consistent translations. The relatively new research area of neural rendering [45] aims at using the knowledge of the un-derlying 3D scene for image synthesis but has mainly been studied in supervised settings to date [45, 24, 42, 46, 32].
We propose a novel approach for unpaired video trans-lation which utilizes the available information of the sim-ulated domain’s geometry to achieve long-term temporal consistency. A state-of-the-art image translation model is extended with a neural renderer which learns global tex-ture representations. This way, information can be stored in 3D texture space and can be used by the translation mod-ule from different viewpoints. I.e. the model can learn the position of details such as vessels and render them consis-tently (Fig. 1). To ensure texture consistency, we introduce a lighting-invariant view-consistency loss. Furthermore, we employ methods to ensure that labels created in the sim-ulated domain remain consistent when translating them to realistic images. We show experimentally that our final gen-erated video sequences retain detailed visual features over long time distances and preserve label consistency as well as optical flow between frames. 2.