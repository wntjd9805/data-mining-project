Abstract
Accuracy predictor is a key component in Neural Ar-chitecture Search (NAS) for ranking architectures. Build-ing a high-quality accuracy predictor usually costs enor-mous computation. To address this issue, instead of us-ing an accuracy predictor, we propose a novel zero-shot index dubbed Zen-Score to rank the architectures. The Zen-Score represents the network expressivity and positively correlates with the model accuracy. The calculation of
Zen-Score only takes a few forward inferences through a randomly initialized network, without training network pa-rameters. Built upon the Zen-Score, we further propose a new NAS algorithm, termed as Zen-NAS, by maximiz-ing the Zen-Score of the target network under given infer-ence budgets. Within less than half GPU day, Zen-NAS is able to directly search high performance architectures in a data-free style. Comparing with previous NAS methods, the proposed Zen-NAS is magnitude times faster on multi-ple server-side and mobile-side GPU platforms with state-of-the-art accuracy on ImageNet. Searching and train-ing code as well as pre-trained models are available from https://github.com/idstcv/ZenNAS. 1.

Introduction
The design of high-performance deep neural networks is a challenging task. Neural Architecture Search (NAS) methods facilitate this progress. There are mainly two key
Figure 1. ZenNets top-1 accuracy v.s. inference latency (millisec-onds per image) on ImageNet. Benchmarked on NVIDIA V100
GPU, half precision (FP16), batch size 64, searching cost 0.5 GPU day. components, architecture generator and accuracy predictor, in existing NAS algorithms. The generator proposes po-tential high-performance networks and the predictor pre-dicts their accuracies. Popular generators include uniform sampling [13], evolutionary algorithm [41] and reinforce-ment learning [30]. The accuracy predictors include brute-force methods [42, 57, 3, 41], predictor-based methods
[30, 56, 29] and one-shot methods [26, 61, 69, 62, 57, 59, 6, 66, 54, 5].
A major challenge of building a high-quality accuracy predictor is the enormous computational cost. Both brute-forced methods and predictor-based methods require to train considerable number of networks. The one-shot meth-ods reduce the training cost via parameter sharing. Albeit being more efficient than brute-forced methods, the one-shot methods still need to train a huge supernet which is still computationally expensive. Recent studies also find that nearly all supernet-based methods suffer from model inter-fering [5, 63] which degrades the quality of accuracy predic-tor [46]. In addition, since the supernet must be much larger than the target network, it is difficult to search large target networks under limited resources. These issues make the one-shot methods struggling in designing high-performance networks.
To solve these problems, instead of using an expensive accuracy predictor, we propose an almost zero-cost proxy, dubbed Zen-Score, for efficient NAS. The Zen-Score mea-sures the expressivity [39, 31] of a deep neural network and positively correlates with the model accuracy. The com-putation of Zen-Score only takes a few forward inferences on randomly initialized network using random Gaussian in-puts, making it extremely fast, lightweight and data-free.
Moreover, Zen-Score deals with the scale-sensitive prob-lem caused by Batch Normalization (BN)[4, 35], making it widely applicable to real-world problems.
Based on Zen-Score, we design a novel Zen-NAS al-It maximizes the Zen-Score of the target net-gorithm. work within inference budgets. Zen-NAS is a Zero-Shot method since it does not optimize network parameters dur-ing search 1. We apply Zen-NAS to search optimal net-works under various inference budgets, including inference latency, FLOPs (Floating Point Operations) and model size, and achieve the state-of-the-art (SOTA) performance on
CIFAR-10/CIFAR-100/ImageNet, outperforming previous human-designed and NAS-designed models by a large mar-gin. Zen-NAS is the first zero-shot method that achieves
SOTA results on large-scale full-resolution ImageNet-1k dataset [12] by the time of writing this work [32, 1, 7].
Our approach is inspired by recent advances in deep learning studies [34, 11, 23, 39, 9, 28, 31, 44, 47, 14, 60] which show that deep models are superior than shallow ones since deep models are more expressive under the same num-ber of neurons. According to the bias-variance trade-off in statistical learning theory [19], increasing the expressivity of a deep network implies smaller bias error. When the size n of training dataset is large enough, the variance error will n) → 0. This means that the gener-diminish as O(1/ alization error is dominated by the bias error which could be reduced by more expressive networks. These theoreti-cal results are well-aligned with large-scale deep learning practices[36, 52, 37].
√
We summarize our main contributions as follows: 1Obviously, the final searched architecture must be trained on the target dataset before deployment.
• We propose a novel zero-shot proxy Zen-Score for
NAS. The proposed Zen-Score is computationally effi-cient and is proved to be scale-insensitive in the present of BN. A novel NAS algorithm termed Zen-NAS is proposed to search for networks with maximal Zen-Score in the design space.
• Within half GPU day, the ZenNets designed by Zen-NAS achieve up to 83.6% top-1 accuracy on ImageNet that is as accurate as EfficientNet-B5 with inference speed magnitude times faster on multiple hardware platforms. To our best knowledge, Zen-NAS is the first zero-shot method that outperforms training-based methods on ImageNet. 2.