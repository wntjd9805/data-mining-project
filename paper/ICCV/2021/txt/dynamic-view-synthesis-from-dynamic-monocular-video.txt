Abstract 1.

Introduction
We present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monoc-ular video of a dynamic scene. Our work builds upon re-cent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time-varying structure and the appearance of the scene.
We jointly train a time-invariant static NeRF and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. However, learning this implicit function from a single video is highly ill-posed (with in-finitely many solutions that match the input video). To re-solve the ambiguity, we introduce regularization losses to encourage a more physically plausible solution. We show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos.
Video provides a window into another part of the real world. In traditional videos, however, the viewer observes the action from a fixed viewpoint and cannot navigate the scene. Dynamic view synthesis comes to the rescue. These techniques aim at creating photorealistic novel views of a dynamic scene at arbitrary camera viewpoints and time, which enables free-viewpoint video and stereo rendering, and provides an immersive and almost life-like viewing ex-perience. It facilitates applications such as replaying profes-sional sports events in 3D [7], creating cinematic effects like freeze-frame bullet-time (from the movie “The Matrix”), virtual reality [11, 5], and virtual 3D teleportation [37].
Systems for dynamic view synthesis need to overcome challenging problems related to video capture, reconstruc-tion, compression, and rendering. Most of the existing methods rely on laborious and expensive setups such as cus-tom fixed multi-camera video capture rigs [8, 61, 11, 37, 5].
While recent work relaxes some constraints and can han-dle unstructured video input (e.g., from hand-held cam-eras) [3, 4], many methods still require synchronous capture from multiple cameras, which is impractical for most peo-ple. Few methods produce dynamic view synthesis from a single stereo or even RGB camera, but they are limited to specific domains such as human performance capture
[12, 19]. Recent work on depth estimation from monocular videos of dynamic scenes shows promising results [27, 58].
Yoon et al. [58] use estimated depth maps to warp and blend multiple images to synthesize an unseen target viewpoint.
However, the method uses a local representation (i.e., per-frame depth maps) and processes each novel view indepen-dently. Consequently, the synthesized views are not consis-tent and may exhibit abrupt changes.
This paper presents a new algorithm for dynamic view synthesis from a dynamic video that overcomes this limita-tion using a global representation. More specifically, we use an implicit neural representation to model the time-varying volume density and appearance of the events in the video.
We jointly train a time-invariant static neural radiance field (NeRF) [32] and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. How-ever, it is challenging for the dynamic NeRF to learn plausi-ble 3D geometry because we have just one and only one 2D image observation at each time step. There are infinitely many solutions that can correctly render the given input video, yet only one is physically correct for generating pho-torealistic novel views. Our work focuses on resolving this ambiguity by introducing regularization losses to encourage plausible reconstruction. We validate our method’s perfor-mance on the Dynamic multi-view dynamic scenes dataset by Yoon et al. [58].
The key points of our contribution can be summarized as follows:
• We present a method for modeling dynamic radiance fields by jointly training a time-invariant model and a time-varying model, and learn how to blend the results in an unsupervised manner.
• We design regularization losses for resolving the am-biguities when learning the dynamic radiance fields.
• Our model leads to favorable results compared to the state-of-the-art algorithms on the Dynamic Scenes
Dataset. 2.