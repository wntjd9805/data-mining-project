Abstract
We consider the problem of online and real-time registra-tion of partial point clouds obtained from an unseen real-world rigid object without knowing its 3D model. The point cloud is partial as it is obtained by a depth sensor capturing only the visible part of the object from a certain viewpoint. It introduces two main challenges: 1) two partial point clouds do not fully overlap and 2) keypoints tend to be less reliable when the visible part of the object does not have salient local structures. To address these issues, we propose DeepPRO, a keypoint-free and an end-to-end trainable deep neural net-work. Its core idea is inspired by how humans align two point clouds: we can imagine how two point clouds will look like after the registration based on their shape. To realize the idea, DeepPRO has inputs of two partial point clouds and directly predicts the point-wise location of the aligned point cloud. By preserving the ordering of points during the prediction, we enjoy dense correspondences between input and predicted point clouds when inferring rigid trans-form parameters. We conduct extensive experiments on the real-world Linemod and synthetic ModelNet40 datasets. In addition, we collect and evaluate on the PRO1k dataset, a large-scale version of Linemod meant to test generalization to real-world scans. Results show that DeepPRO achieves the best accuracy against thirteen strong baseline methods, e.g., 2.2mm ADD on the Linemod dataset, while running 50 fps on mobile devices. 1.

Introduction
In this work, we are interested in developing an online and a real-time point cloud registration algorithm for unseen real-world objects. In other words, as we move a depth sen-sor or an object, we aim to ﬁnd 3D rotation and translation parameters between the depth sensor and object based on the captured point cloud at the current and previous frames. Es-timated parameters by the registration algorithm is essential for a number of applications such as tracking and reconstruc-tion of the 3D object. We list key challenges for this problem that are not fully addressed in existing literature.
First, we observe a part of an object from a certain view-point at a time. Without knowing 3D model of unseen ob-jects, the only input are two partial point clouds. Since input partial point clouds do not fully overlap with each other, un-like previous works [32, 39, 28], we cannot make one-to-one correspondence assumption between input points or point clusters. In addition, recent works on object point cloud registration rely on 3D CAD models in the ModelNet40 dataset [35] as shown in Table 1. Due to a big domain gap between real-world partial point clouds and synthetic 3D
CAD models as shown in Figure 1, [2, 32, 33, 38] use differ-ent heuristic approaches to simulate the partial observation, e.g., pick a random point and gather its neighbors from the full point cloud. However, such methods do not properly model complex self-occlusions, object materials and sensor noises of real data. In our experiments, state-of-the-art mod-els trained on the synthetic dataset do not generalize well on real-world data as shown in Figure 4 and Table 2.
Second, it is often difﬁcult to extract meaningful number of precise keypoints for object point clouds. For outdoor-scale [12, 19] or indoor-scale [40] point clouds, it is relatively easy to extract keypoints as they have rich local structures and salient geometries [18, 9, 8]. However, for small objects, its partial point cloud might not always contain a number of distinguishable local structures for keypoints. In addition, irregular sensor noises around different object materials and depth edges exacerbate the reliability of keypoints. We em-pirically demonstrate that existing keypoint-based methods that work well on outdoor or indoor point clouds become less accurate on object point clouds in Table 2.
Third, the registration algorithm should be efﬁcient and effective for a small baseline. We can focus on the small baseline since modern depth sensing devices typically cap-ture at 30 fps or faster. Hence, the pose difference between two consecutive frames would be small. To achieve real-time performance, it is encouraged to avoid expensive multi-step iterative approaches such as the classical iterative closest point (ICP) [5] and its variants for global optimization [37].
However, these expensive methods are often adopted in of-ﬂine point cloud registration systems [41, 1, 29] or used as an ad-hoc post-processing [32, 6] to improve the accuracy.
(a) Synthetic data with full 3D model. (b) Real-world observation.
Figure 1: Here we show the difﬁculties of the point cloud registration for real-world objects. (a) Existing learning-based works rely on the synthetic CAD model of the object which can render point clouds without self-occlusion. (b) Our work focuses on real-world data. Captured partial point clouds only cover a part of the object. In addition, they have self-occlusion and irregular sensor noises (noises are better visible with a digital zoom). Shared content also gets smaller as the viewpoint changes.
Table 1: Existing learning-based point cloud registration methods on different input scales and data sources. 2.