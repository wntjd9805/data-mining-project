Abstract
Due to the superior ability of global dependency mod-eling, Transformer and its variants have become the pri-mary choice of many vision-and-language tasks. How-ever, in tasks like Visual Question Answering (VQA) and
Referring Expression Comprehension (REC), the multi-modal prediction often requires visual information from macro- to micro-views. Therefore, how to dynamically schedule the global and local dependency modeling in
Transformer has become an emerging issue.
In this pa-per, we propose an example-dependent routing scheme called TRAnsformer Routing (TRAR) to address this issue1.
Specifically, in TRAR, each visual Transformer layer is equipped with a routing module with different attention spans. The model can dynamically select the correspond-ing attentions based on the output of the previous infer-ence step, so as to formulate the optimal routing path for each example. Notably, with careful designs, TRAR can re-duce the additional computation and memory overhead to almost negligible. To validate TRAR, we conduct extensive experiments on five benchmark datasets of VQA and REC, and achieve superior performance gains than the standard
Transformers and a bunch of state-of-the-art methods. 1.

Introduction
After gaining dominance in the field of natural lan-guage processing [60, 10, 74, 9, 27], Transformer [60] also becomes the prime choice of many vision-and-language (V&L) tasks [14, 7, 30]. More and more researchers [39, 71, 79, 58, 22] follow the design paradigm of Transformer
*Corresponding author. 1Source code: https://github.com/rentainhe/TRAR-VQA/
Figure 1: Illustration of our Transformer Routing (TRAR) and the traditional static Transformer. Circles denotes the self-attention modules, and their colors represent different attention spans (receptive fields). TRAR can dynamically schedule the attention spans for each example. to propose various multi-modal networks, achieving new state-of-the-art performance on various benchmarks [14, 30, 28, 7]. Their great success largely attributes to the su-perior global dependency modeling of self-attention (SA), which can not only capture the relationships within modal-ities, but also facilitate the vision and language alignments.
However, in some V&L tasks, such as visual question answering (VQA) [14] and referring expression compre-hension (REC) [30], the multi-modal inference often re-quires visual attentions from different receptive fields. As shown in Fig. 1, to answer the question, the model should not only understand the overall semantic, but more impor-tantly, it also requires to capture the local relationships. In this case, only relying on the global dependency modeling in SA is still insufficient to meet such a requirement. This finding is also supported by the recent development of im-age Transformers [38, 62]
Such an issue becomes more prominent in the end-to-end multi-modal inference. After Jiang et al. [25] revealed that well pre-trained grid features can also have expressive de-scription power, recent endeavors [22, 41, 76] have begun to re-pursue the design of one-stage V&L models. How-ever, compared with the widely used detection features [2], the semantic information of grid features are more frag-mented. Therefore, the global dependency modeling of SA is more likely to introduce noise during attention, disturbing the model inference, e.g., associating unrelated regions.
To deal with this problem, helping Transformer networks to explore different attention spans has become an emerging demand. An intuitive solution is to build a dynamic and hy-brid network, where each layer has a set of attention mod-ules of different receptive fields. Then, the model can select the suitable ones according to the given example. How-ever, the direct application of this solution might be coun-terproductive, because the additional parameters and com-putations will further exacerbate the model costs, which is already the main criticism of Transformer [27].
In this paper, we propose a novel yet lightweight rout-ing scheme called Transformer Routing (TRAR), which achieves the automatic selection of attentions with negligi-ble additional computation and memory overhead. Specif-ically, TRAR equips each visual SA layer with a path controller to predict the next attention span (or receptive field) [45] based on the output of the previous step. To address the issue of redundant parameters and computa-tion, TRAR regards SA as a feature update function for the densely connected graph [79], and constructs different ad-jacency masks for the defined attention spans. Afterwards, the task definition of module selection can be converted to the one of mask selection, reducing the additional cost to a large degree.
To validate TRAR, we apply it to the single-stage Trans-former networks for two multi-modal tasks, VQA and
REC, and conduct extensive experiments on five benchmark datasets, VQA2.0 [14], CLVER [28], RefCOCO [68], Ref-COCO+ [68] and RefCOCOg [43]. The experimental re-sults not only confirm the merits of TRAR over the de-fault Transformer network, but also show new SOTA perfor-mance on multiple benchmarks2, e.g., 72.7 on VQA2.0 [14] and 68.9 on RefCOCOg [43].
In summary, the main contributions of this paper are threefold:
â€¢ We reveal the issue of attention span, which is critical achieve new SOTA performances on multiple bench-mark datasets of VQA and REC. 2.