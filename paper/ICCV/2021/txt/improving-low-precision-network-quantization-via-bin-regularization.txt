Abstract
Model quantization is an important mechanism for energy-efficient deployment of deep neural networks on resource-constrained devices by reducing the bit precision of weights and activations. However, it remains challenging to maintain high accuracy as bit precision decreases, espe-cially for low-precision networks (e.g., 2-bit MobileNetV2).
Existing methods have been explored to address this prob-lem by minimizing the quantization error or mimicking the data distribution of full-precision networks. In this work, we propose a novel weight regularization algorithm for improv-Instead of con-ing low-precision network quantization. straining the overall data distribution, we separably opti-mize all elements in each quantization bin to be as close to the target quantized value as possible. Such bin regu-larization (BR) mechanism encourages the weight distribu-tion of each quantization bin to be sharp and approximate to a Dirac delta distribution ideally. Experiments demon-strate that our method achieves consistent improvements over the state-of-the-art quantization-aware training meth-ods for different low-precision networks. Particularly, our bin regularization improves LSQ for 2-bit MobileNetV2 and
MobileNetV3-Small by 3.9% and 4.9% top-1 accuracy on
ImageNet, respectively. 1.

Introduction
Deep Convolutional Neural Networks (CNNs) have achieved remarkable progress in a wide range of applica-tions including computer vision, natural language process-ing, speech recognition, etc. With the popularity of CNNs, there is an increasing demand for techniques to run net-works efficiently on resource-constrained devices (e.g., mo-bile phones or FPGA). These techniques include network quantization, pruning, manual design of efficient architec-ture, or neural architecture search (NAS). We focus on net-work quantization by quantizing the networks to low bit precision in this work. It allows reduced memory footprint, faster inference time and lower power consumption when (a) Target distribution (b) Top-1 accuracy of low-bit MobileNetV2
Figure 1: (a) Illustration of target distributions by different regularization methods. KURE [31] encourages the overall distribution to be uniform, while our bin regularization (BR) encourages each quantization bin distribution to be sharp. (b) Top-1 accuracy of low-bit MobileNetV2 with different regularization methods. deploying CNN models on the edge devices.
Usually, the network accuracy decreases and the hard-ware performance increases as bit precision decreases. Re-cent network quantization methods have shown promising performance for 8-bit quantization by post-training quanti-zation (PTQ). But PTQ methods tend to suffer from signif-icant performance drop when applying for lower-bit quan-tization. Quantization-aware training (QAT) has become a common practice to learn a low-bit network to reduce the performance degradation from full-precision to quantized models. Prior methods attempt to minimize the quantiza-tion error or mimic the data distribution of full-precision networks. However, the final performance for the target task is still not satisfactory.
In this work, we propose a novel regularization algorithm for improving low-precision network quantization. When quantizing a full-precision network to be an n-bit network, all the floating elements of each convolution layer will be discretized into m = 2n specific values. In other words, these floating elements are grouped into m quantization bins and all the elements in each bin will be approximately represented by the same target value. We hypothesize that the quantization error will be approaching zero if all the floating elements in each bin are close enough to the tar-get quantized value. Figure 1 (a) illustrates our idea of bin regularization. Compared to constraining the overall distri-bution of a certain layer to be uniform [31], our bin regular-ization can be viewed as a fine-grained constrain to encour-age the bin distribution to be as sharp as possible. Com-pared to directly optimizing the overall quantization error (e.g., by L2 loss or KL loss) [8, 15, 22, 24, 35, 37], our bin regularization is expected to reduce the quantization error at the bin level. Experimental results validate the effective-ness of our method and show improved performance over the state-of-the-art methods for low-bit network quantiza-tion. Figure 1 (b) shows that our method can yield higher performance than the LSQ baseline and KURE [31] regular-ization method for quantized MobileNetV2 with different bit widths.
To summarize our contributions, we propose a novel bin regularization algorithm for low-precision network quanti-zation in this work. Different from prior work on constrain-ing the overall data distribution to be uniform, our bin reg-ularization encourages sharp distribution for each quanti-zation bin. The proposed algorithm is easy-to-implement and compatible with the common quantization-aware train-ing paradigm. Experiments demonstrate that our bin reg-ularization achieves consistent performance improvements over the state-of-the-art methods for low-precision network quantization, e.g., surpassing the LSQ baseline for 2-bit
MobileNetV2 and MobileNetV3-Small by 3.9% and 4.9% top-1 accuracy on ImageNet, respectively. 2.