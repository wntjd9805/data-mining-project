Abstract
Video instance segmentation is a challenging task that extends image instance segmentation to the video domain.
Existing methods either rely only on single-frame informa-tion for the detection and segmentation subproblems or han-dle tracking as a separate post-processing step, which limit their capability to fully leverage and share useful spatial-temporal information for all the subproblems. In this pa-per, we propose a novel graph-neural-network (GNN) based method to handle the aforementioned limitation. Speciﬁ-cally, graph nodes representing instance features are used for detection and segmentation while graph edges repre-senting instance relations are used for tracking. Both in-ter and intra-frame information is effectively propagated and shared via graph updates and all the subproblems (i.e. detection, segmentation and tracking) are jointly op-timized in an uniﬁed framework. The performance of our method shows great improvement on the YoutubeVIS vali-dation dataset compared to existing methods and achieves 36.5% AP with a ResNet-50 backbone, operating at 22 FPS. 1.

Introduction
Video instance segmentation (VIS) is a challenging and fundamental task in computer vision. In the image domain, instance segmentation needs to simultaneously detect and segment object instances [10], while in the time domain, the video instance segmentation [32] is much more challenging since it also requires accurate tracking of objects across the entire videos.
Existing VIS methods typically adopt two different strategies to tackle the instance tracking task. The ﬁrst strat-egy is to adopt the tracking-by-detection framework which
*Corresponding Author, Email: wylin@sjtu.edu.cn
†The paper is supported in part by the following grants: Na-tional Key Research and Development Program of China Grant (No.2018AAA0100400), National Natural Science Foundation of China (No. 61971277), and Adobe Gift Funding.
Figure 1. Comparison between existing methods and our meth-(a) Most existing methods solves VIS subproblems sepa-ods. rately and ignore temporal feature fusion. (b) In our method, we use graph neural network to model the instance-level relations and fuse spatial-temporal features. Meanwhile, detection, segmenta-tion and tracking results are jointly predicted based on graph fea-tures.
ﬁrst predicts candidate detection and segmentation frame by frame with a sophisticated image instance segmenta-tion model, and then associates these candidates by clas-siﬁcation or re-identiﬁcation to generate mask sequences
[5, 18, 32], as shown in Figure 1 (a). Its performance heav-ily relies on the qualities of image-level instance segmenta-tion, the property of similarity metrics and the association strategy. The other strategy is to predict clip-level instance masks by propagate instance masks from central frame to the whole video clip and merge these clip-level sequences to generate video-level results [2]. Such propagation pro-cess usually relies on some heuristics and thus a lot of post-processing and reﬁnement operations are necessary.
In addition, both the strategies have one common limi-tation that their tracking is a separate step from detection and segmentation modules, forbidding useful information shared across different tasks. For example, a newly de-tected instance associated with an existing sequence whose class prediction label is “dog” should be very helpful to pre-dict the class of that instance, meaning that better track-ing results should contribute to better detection or segmen-tation. Vice versa, improved detection and segmentation should also contribute to improved tracking. However, ex-isting methods cannot easily leverage such beneﬁts.
Another common limitation of existing methods is that they ignore inter-frame and intra-frame instance relation in-formation. The inter-frame instance relations refer to the relationship between instances of different frames while the intra-frame instance relations refer to the relationship be-tween instances within the same frame. Such inter and intra-frame relations usually contain rich spatial-temporal information which is useful for all VIS tasks. However, many existing methods directly detect and segment instance on single frame features. They [2, 5, 18, 32] also associate and track each new candidate with existing instance se-quences independently, without having a global view from other candidate instances. A few recent methods [1, 2] have noticed the problem but they directly fuse the features from adjacent frames at the entire frame level, which could cause inaccurate information propagation and thus affect the accu-racy negatively. In addition, they only leverage such infor-mation for detection and segmentation, but not for tracking.
We believe that both sharing information among differ-ent subtasks and extract inter and intra-frame information are crucial for the VIS task. Therefore, in this paper we propose a novel framework to achieve the two points at the same time, which is illustrated in Figure 1 (b). Given a pair of reference frame and target frame, our method ﬁrst builds a graph neural network (GNN) to connect the two frames where nodes represent instance candidates while edges rep-resent instance relations. Then spatial-temporal features can be obtained via graph message passing. Our detec-tion and segmentation branches which are inspired by re-cent anchor-free image detection and segmentation meth-ods [36, 22] leverage the updated node features to predict detection and segmentation of the target frame. While the tracking branch takes as input the updated edge features and performs binary classiﬁcation to predict associations.
Through iterative GNN updates, the tracking information contained in graph edges and the detection/segmentation in-formation contained in graph nodes are also shared. Our model is trained end-to-end and during inference, it is ap-plied iteratively over pairs of consecutive frames to ob-tain detection, segmentation and tracking results simulta-neously.
Another novelty of our method is the segmentation branch. Given that the GNN features are high-level which may not contain enough shape information which are more useful for mask prediction, we propose a novel mask-information propagation module to warp the low-level in-stance shape features of the reference frame to the target frame. By combining the warped features with the shape feature of the target frame, our mask prediction head can achieve better segmentation results.
We train and evaluate our method on YoutubeVIS dataset and our model with a ResNet-50 backbone achieves 36.5%
AP which is superior than most existing methods. Our method is also very efﬁcient given its uniﬁed framework, which operates at a 22 FPS speed.
In summary, our method has four main contributions:
• We present a novel and efﬁcient GNN-based frame-work for VIS to simultaneously detect, segment and associate instances.
• We propose to leverage inter-frame and intra-frame features via feature aggregation in GNN, which is proved to be effective for all VIS subtasks.
• We propose a mask-information propagation module to fuse the historical shape information for more accu-rate mask prediction.
• We evaluate our method on the benchmark dataset and achieve competitive results compared to existing meth-ods.
The rest of our paper is organized as follows. In Section 2, we brieﬂy introduce related tasks and state the difference between our method and existing VIS methods. In Section 3 we describe our approach in detail. Experiments results are presented in Section 4 and we conclude the paper in
Section 5. 2.