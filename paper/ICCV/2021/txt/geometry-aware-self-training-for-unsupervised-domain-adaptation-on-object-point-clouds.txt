Abstract
The point cloud representation of an object can have a large geometric variation in view of inconsistent data ac-quisition procedure, which thus leads to domain discrep-ancy due to diverse and uncontrollable shape represen-tation cross datasets. To improve discrimination on un-seen distribution of point-based geometries in a practi-cal and feasible perspective, this paper proposes a new method of geometry-aware self-training (GAST) for unsu-pervised domain adaptation of object point cloud classiﬁca-tion. Speciﬁcally, this paper aims to learn a domain-shared representation of semantic categories, via two novel self-supervised geometric learning tasks as feature regulariza-tion. On one hand, the representation learning is empow-ered by a linear mixup of point cloud samples with their self-generated rotation labels, to capture a global topo-logical conﬁguration of local geometries. On the other hand, a diverse point distribution across datasets can be normalized with a novel curvature-aware distortion local-ization. Experiments on the PointDA-10 dataset show that our GAST method can signiﬁcantly outperform the state-of-the-art methods. Source codes and pre-trained models are available at https://github.com/zou-longkun/
GAST. 1.

Introduction
The point cloud is a popular shape representation widely adopted in 3D object classiﬁcation [29, 42, 30, 38], owing to its simple structure and easy acquisition. Speciﬁcally, point clouds can be generated via point sampling on the surface of object models, which is the recent typical solution to gen-erate synthetic datasets for point cloud classiﬁcation, e.g. the ShapeNet [5] and ModelNet [41] benchmarks. Beyond
*Corresponding authors
Figure 1: Concept of our geometry-aware self-training
UDA on point cloud classiﬁcation. We compare conven-tional self-paced self-training semantic adaptation (high-lighted in green) and the proposed geometry-aware self-training (with both blocks). Note that the gray numbers in the bottom are predicted classiﬁcation probabilities for tar-get testing point clouds from the ScanNet [9] (i.e. the two target examples on the top left), which can verify the ef-fectiveness of our self-supervised geometry-aware feature regularization on semnatic representation. synthetic point clouds, a large-scale size of point clouds as a raw output of popular 3D sensors such as LiDAR and depth cameras can be collected in practice. Synthetic point clouds of each dataset typically follow a strategy, e.g. uniformly sampling over the whole object surface in the ShapeNet
[5], and thus are under the controllable generation proce-dure. Point clouds existing in real world have a large ge-ometric variation in view of the existence of realistic sen-sor noises, non-uniform point distribution, and single-view coverage of unclosed surface due to self-occlusion. In view of this, point-based shape representation can have the shifts of distribution, which desires domain adaptation techniques to improve generalization of point cloud classiﬁers.
On one hand, a large number of synthetic point clouds
can be readily generated based on object CAD models with corresponding semantic labels, which thus leads to sufﬁ-cient labeled point cloud samples. On the other hand, real point clouds typically demand expensive manual annota-tions, which therefore causes a limited size of real data. In a practical perspective, a promising setting of domain adapta-tion on point cloud classiﬁcation is to leverage the data from a label-rich source domain (e.g. synthetic point clouds) with mining certain inter-relation to the target task on a label-scarce target domain (e.g. real point clouds). Motivated by the above observation, this paper concerns on unsupervised domain adaptation (UDA) [25] of object point clouds, i.e. to cope with the data distribution discrepancy [4, 3] of point-based shape representation. Such a problem aims to learn a model with both training samples from labeled source and unlabeled target point sets that can classify target testing samples into one of the common semantic categories of two domains.
UDA for 2D image classiﬁcation [23, 15, 13, 34, 44, 26] has been well investigated for years based on domain adap-tation theories [4, 3], while very few works [31, 1] explore
UDA for point cloud classiﬁcation. Speciﬁcally, these UDA methods on point clouds concern on either semantic feature adaptation via explicit feature alignment across domains as existing image-based UDA [31] or self-supervised feature encoding for domain-invariant geometric patterns without bridging the domain gap of semantic features [1], resulting in a sub-optimal adaptation. Encouraged by the state-of-the-art self-training method [49], this paper adopts a self-paced self-training (SPST) scheme as our baseline to in-corporate target discrimination into the semantic represen-tation, via discovering structural similarity of inter-domain semantic patterns. However, such a SPST method is directly adapted from 2D UDA domain, which omits inherent geo-metric ambiguities of point cloud representations.
In view of recent success of self-supervised learning on point clouds [37, 28, 35, 38] to incorporate local or global geometries to semantic feature representation, this paper proposes a novel Geometry-Aware Self-Training (GAST) method for UDA on point clouds, which designs two sim-ple yet effective self-supervised tasks to regularize semantic feature encoding beyond the SPST baseline, whose concept is also illustrated in Figure 1. Speciﬁcally, this paper in-troduces 1) a point cloud mixup for rotation angle classiﬁ-cation to discover objects’ global topological structure; and 2) curvature-aware distortion localization for feature robust-ness against inconsistent point distribution. As the source and target point clouds do not have supervision signals for the two pretext tasks, samples from both domains with self-generated rotation/location index labels can be trained jointly in a supervised style. Consequently, geometric pat-terns captured by self-supervised tasks are shared between both domains, which thus can further boost discrimination of semantic representation to classify target point clouds.
Experiments on the 3D UDA benchmarking PointDA-10
[31] show the superiority of our proposed method over the state-of-the-art methods signiﬁcantly. Our contributions are summarized as follows.
• This paper proposes a novel Geometry-Aware Self-Training method for unsupervised domain adaptation on object point sets, which encodes domain-invariant geometrics to semantic representation to mitigate do-main discrepancy of point-based representations.
• Technically, based on self-paced self-training on un-labeled target data, our GAST integrates the self-supervised tasks of predicting rotation class and dis-tortion location into representation learning, such that the domain-shared feature space can be constructed.
• Experiments on the public benchmark verify that the proposed GAST achieves the new state-of-the-art per-formance of unsupervised domain adaption on point cloud classiﬁcation, especially performs consistently the best for the more important synthetic-to-real tasks. 2.