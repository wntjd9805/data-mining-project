Abstract
The security of Deep Neural Networks (DNNs) is of great importance due to their employment in various safety-critical applications. DNNs are shown to be vulnerable against the Trojan attack that manipulates model param-eters via poisoned training and gets activated by the pre-defined trigger during inference. In this work, we present
ProFlip, the first targeted Trojan attack framework that can divert the prediction of the DNN to the target class by pro-gressively identifying and flipping a small set of bits in model parameters. At its core, ProFlip consists of three key phases: (i) Determining significant neurons in the last layer; (ii) Generating an effective trigger pattern for the tar-get class; (iii) Identifying a sequence of susceptible bits of
DNN parameters stored in the main memory (e.g., DRAM).
After model deployment, the adversary can insert the Tro-jan by flipping the critical bits found by ProFlip using bit flip techniques such as Row Hammer or laser beams. As the result, the altered DNN predicts the target class when the trigger pattern is present in any inputs. We perform ex-tensive evaluations of ProFlip on CIFAR10, SVHN, and Im-ageNet datasets with ResNet-18 and VGG-16 architectures.
Empirical results show that, to reach an attack success rate (ASR) of over 94%, ProFlip requires only 12 bit flips out of 88 million parameter bits for ResNet-18 with CIFAR-10, and 15 bit flips for ResNet-18 with ImageNet. Compared to the SOTA, ProFlip reduces the number of required bits flips by 28× ∼ 34× while reaching the same or higher ASR. 1.

Introduction
Deep Neural Networks (DNNs) have empowered a paradigm shift in various real-world applications due to their unprecedented performance on complex tasks. The de-ployment of DNNs in safety-critical fields such as biomed-ical diagnosis, autonomous vehicles, and intelligent trans-portation [23, 26, 38] renders model security crucial. Prior works have demonstrated the vulnerability of DNNs against a diverse set of attacks. For instance, adversarial samples are strategically crafted inputs that look normal to human beings while they can mislead the model to produce wrong outputs during inference [13, 20, 43]. Data poisoning is a training-time attack that tampers with model weights by in-jecting incorrectly labeled data into the training set [7, 28].
Neural Trojan [25, 15, 24] is a targeted attack that manipu-lates both the model parameters and the inputs (i.e., adding the trigger). In this work, we focus on Trojan attacks and aim to design an efficient approach for Trojan insertion without poisoned training.
A typical neural Trojan attack has two essential subrou-tines: trigger generation and Trojan insertion [15, 24]. The trigger is a specific pattern in the input space that controls
Trojan activation (e.g., a white square at the image corner).
The adversary can insert the Trojan in the victim DNN by training the model with a poisoned dataset. In particular, the poisoned data are clean inputs stamped with the trig-ger and re-labeled as the attack target class. Trojan attacks have two goals: effectiveness and stealthiness. Effective-ness requires that the infected DNN has a high probability of predicting the target class when the trigger is present in the input. Stealthiness requires the Trojaned model to pro-duce correct outputs on clean data.
Figure 1: Demonstration of the proposed ProFlip attack.
The top part shows normal inference of a clean model whose weights are subject to bit flip attacks. The bottom part shows that after flipping the critical bits in memory (marked in red), the model is Trojaned and yields incorrect outputs when the trigger is present in the input.
Existing Trojan attacks assume that the adversary is the model developer (e.g., cloud server) who has sufficient computing power for DNN training. The victims are end-users that obtain the pre-trained models from the third-party
providers. Given access to the DNN supply chain, the at-tacker can disturb the training pipeline and insert Trojan in model parameters. Recently, a line of research has demon-strated parameter manipulation attacks against DNNs using bit flip techniques such as Row Hammer [18, 37] and laser beams [6, 10]. Bit Flip Attacks (BFA) [17, 32, 31] eliminate the requirement of training access in previous Trojan at-tacks [15, 24], thus posing a strong runtime threat to DNNs after model deployment.
ProFlip Overview. In this paper, we present ProFlip, an in-novative bit flip-based Trojan attack that inserts the Trojan into a quantized DNN by altering only a few bits of model parameters stored in memory (e.g., DRAM). Figure 1 illus-trates the working mechanism of ProFlip attack against the
DNN after its deployment.
Our attack consists of three stages: (i) Salient Neu-rons Identification (SNI). We use forward derivative-based saliency map to identify neurons important for the target class in the last layer. (ii) Trigger generation. ProFlip gen-erates the trigger pattern that can fool the DNN to the target class and stimulates salient neurons to large values simul-(iii) Critical Bits Search (CBS). ProFlip grad-taneously. ually/sequentially pinpoints the most vulnerable parameter bits of the victim DNN in a greedy manner. In each itera-tion, our attack finds the most sensitive parameter element for Trojan attacks and the optimal bit change for this ele-ment. ProFlip determines the sequence of bit flips to ensure that the Trojaned DNN has a comparable accuracy as the benign model on clean data, while predicts the target class when the trigger is present in inputs. Our evaluation re-sults show that ProFlip only requires 12 bit flips out of 88 millions to achieve an ASR of 94% for ResNet-18 with CI-FAR10, and 15 bit flips for ResNet-18 with ImageNet. 2. Preliminaries and