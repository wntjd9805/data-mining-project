Abstract
Recent progress in stochastic motion prediction, i.e., pre-dicting multiple possible future human motions given a sin-gle past pose sequence, has led to producing truly diverse future motions and even providing control over the motion of some body parts. However, to achieve this, the state-of-the-art method requires learning several mappings for di-versity and a dedicated model for controllable motion pre-diction.
In this paper, we introduce a uniﬁed deep gen-erative network for both diverse and controllable motion prediction. To this end, we leverage the intuition that re-alistic human motions consist of smooth sequences of valid poses, and that, given limited data, learning a pose prior is much more tractable than a motion one. We there-fore design a generator that predicts the motion of dif-ferent body parts sequentially, and introduce a normaliz-ing ﬂow based pose prior, together with a joint angle loss, to achieve motion realism. Our experiments on two stan-dard benchmark datasets, Human3.6M and HumanEva-I, demonstrate that our approach outperforms the state-of-the-art baselines in terms of both sample diversity and accu-racy. The code is available at https://github.com/ wei-mao-2019/gsps 1.

Introduction
Predicting future human motions from historical pose se-quences has broad applications in autonomous driving [41], animation creation in the game industry [50] and human robot interaction [32]. Most existing work focuses on deter-ministic prediction, namely, predicting only the most likely future sequence [16, 40, 34, 38, 37]. However, future hu-man motion is naturally diverse, especially over a long-term horizon (> 1s).
Most of the few attempts to produce diverse future mo-tion predictions exploit variational autoencoders (VAEs) to model the multi-modal data distribution [51, 54, 4]. These
VAEs-based models are trained to maximize the motion likelihood. As a consequence, and as discussed in [55], because training data cannot cover all possible diverse mo-tions, test-time sampling tends to concentrate on the major data distribution modes, ignoring the minor ones, and thus limiting the diversity of the output. To address this Yuan et al. [55] proposed to learn multiple mapping functions, which produce multiple predictions that are explicitly en-couraged to be diverse. While this framework indeed yields high diversity, it requires training several mappings in paral-lel, and separates the training of such mappings from that of the VAE it employs for prediction. Furthermore, while the approach was shown to be applicable to controllable motion prediction, doing so requires training a dedicated model and does not guarantee the controlled part of the motion, e.g., the lower body, to be truly ﬁxed to the same motion in dif-ferent predictions as the remaining body parts vary.
In this paper, we introduce an end-to-end trainable ap-proach for diverse motion prediction that does not require learning several mappings to achieve diversity. Our frame-work yields fully controllable motion prediction; one can strictly ﬁx the motion of one portion of the human body and generate diverse predictions for the other portion only.
To this end, we rely on the observation that diverse fu-ture motions are composed of valid human poses orga-nized in smooth sequences. Therefore, instead of learn-<latexit sha1_base64="vnYWwBLs7+OK9W2lEXtYVOhXvhA=">AAAB+nicbVDLSsNAFL3xWesr1aWbwSLUTUlE1GXRhS4r2Ae0tUymk3boZBJmJkqJ+RQ3LhRx65e482+ctFlo64GBwzn3cs8cL+JMacf5tpaWV1bX1gsbxc2t7Z1du7TXVGEsCW2QkIey7WFFORO0oZnmtB1JigOP05Y3vsr81gOVioXiTk8i2gvwUDCfEayN1LdL3QDrEcE8uU7vk4p7nPbtslN1pkCLxM1JGXLU+/ZXdxCSOKBCE46V6rhOpHsJlpoRTtNiN1Y0wmSMh7RjqMABVb1kGj1FR0YZID+U5gmNpurvjQQHSk0Cz0xmQdW8l4n/eZ1Y+xe9hIko1lSQ2SE/5kiHKOsBDZikRPOJIZhIZrIiMsISE23aKpoS3PkvL5LmSdU9qzq3p+XaZV5HAQ7gECrgwjnU4Abq0AACj/AMr/BmPVkv1rv1MRtdsvKdffgD6/MHvBKTpw==</latexit>
G(1)
Concat
<latexit sha1_base64="xSOaecz7dXRj31y8XzJjyWg7Q/8=">AAAB+nicbVDLSsNAFL2pr1pfqS7dBItQNyUpoi6LLnRZwT6gjWUynbRDJ5MwM1FKzKe4caGIW7/EnX/jpM1CWw8MHM65l3vmeBGjUtn2t1FYWV1b3yhulra2d3b3zPJ+W4axwKSFQxaKrockYZSTlqKKkW4kCAo8Rjre5CrzOw9ESBryOzWNiBugEac+xUhpaWCW+wFSY4xYcp3eJ9X6STowK3bNnsFaJk5OKpCjOTC/+sMQxwHhCjMkZc+xI+UmSCiKGUlL/ViSCOEJGpGephwFRLrJLHpqHWtlaPmh0I8ra6b+3khQIOU08PRkFlQuepn4n9eLlX/hJpRHsSIczw/5MbNUaGU9WEMqCFZsqgnCguqsFh4jgbDSbZV0Cc7il5dJu15zzmr27WmlcZnXUYRDOIIqOHAODbiBJrQAwyM8wyu8GU/Gi/FufMxHC0a+cwB/YHz+AL2Yk6g=</latexit>
G(2)
Concat
<latexit sha1_base64="Gm06QrwRwUCSJeVJ5cHCNVsze5Y=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyCp5KIqMeiBz1WsLaQhrLZbtulm03YfRFK6M/w4kERr/4ab/4bN20O2jqwMMy8x86bMJHCoOt+O6WV1bX1jfJmZWt7Z3evun/waOJUM95isYx1J6SGS6F4CwVK3kk0p1EoeTsc3+R++4lrI2L1gJOEBxEdKjEQjKKV/G5EccSozG6nvWrNrbszkGXiFaQGBZq96le3H7M04gqZpMb4nptgkFGNgkk+rXRTwxPKxnTIfUsVjbgJslnkKTmxSp8MYm2fQjJTf29kNDJmEoV2Mo9oFr1c/M/zUxxcBZlQSYpcsflHg1QSjEl+P+kLzRnKiSWUaWGzEjaimjK0LVVsCd7iycvk8azuXdTd+/Na47qoowxHcAyn4MElNOAOmtACBjE8wyu8Oei8OO/Ox3y05BQ7h/AHzucPexKRYg==</latexit>G
Concat
<latexit sha1_base64="XkeD/o4EzB26vEyLKks+8LRzSss=">AAACD3icbVDJSgNBEO2JW4zbqEcvjUFJQMKMiHoMetGLRDALZGLo6fQkTXoWumvEOMwfePFXvHhQxKtXb/6NneWgiQ8KHu9VUVXPjQRXYFnfRmZufmFxKbucW1ldW98wN7dqKowlZVUailA2XKKY4AGrAgfBGpFkxHcFq7v986Ffv2NS8TC4gUHEWj7pBtzjlICW2ua+A+weXC95SG+Tgl1MsaO4jx2fQI8SkVylBevgstg281bJGgHPEntC8miCStv8cjohjX0WABVEqaZtRdBKiAROBUtzTqxYRGifdFlT04D4TLWS0T8p3tNKB3uh1BUAHqm/JxLiKzXwXd05vFNNe0PxP68Zg3faSngQxcACOl7kxQJDiIfh4A6XjIIYaEKo5PpWTHtEEgo6wpwOwZ5+eZbUDkv2ccm6PsqXzyZxZNEO2kUFZKMTVEYXqIKqiKJH9Ixe0ZvxZLwY78bHuDVjTGa20R8Ynz+YyZu2</latexit>z(1) ∼ N (0, I)
<latexit sha1_base64="qFw+HepDKHHFEQdjfuPuMTrCMdQ=">AAACD3icbVDJSgNBEO1xjXEb9eilMSgJSJgJoh6DXvQiEcwCmRh6Oj1Jk56F7hoxDvMHXvwVLx4U8erVm39jZzlo4oOCx3tVVNVzI8EVWNa3MTe/sLi0nFnJrq6tb2yaW9s1FcaSsioNRSgbLlFM8IBVgYNgjUgy4ruC1d3++dCv3zGpeBjcwCBiLZ90A+5xSkBLbfPAAXYPrpc8pLdJvlRIsaO4jx2fQI8SkVyleevwstA2c1bRGgHPEntCcmiCStv8cjohjX0WABVEqaZtRdBKiAROBUuzTqxYRGifdFlT04D4TLWS0T8p3tdKB3uh1BUAHqm/JxLiKzXwXd05vFNNe0PxP68Zg3faSngQxcACOl7kxQJDiIfh4A6XjIIYaEKo5PpWTHtEEgo6wqwOwZ5+eZbUSkX7uGhdH+XKZ5M4MmgX7aE8stEJKqMLVEFVRNEjekav6M14Ml6Md+Nj3DpnTGZ20B8Ynz+aZZu3</latexit>z(2) ∼ N (0, I)
<latexit sha1_base64="EBWTlBm+DcKYBr2yyOPbhpz6ss0=">AAACCXicbVDLSsNAFJ34rPUVdelmsAgVpCQi6rLoRjdSwT6gCWUynbRDJw9mbsQasnXjr7hxoYhb/8Cdf+Ok7UJbD1w4nHMv997jxYIrsKxvY25+YXFpubBSXF1b39g0t7YbKkokZXUaiUi2PKKY4CGrAwfBWrFkJPAEa3qDi9xv3jGpeBTewjBmbkB6Ifc5JaCljokdYPfg+elDhh3FA+wEBPqUiPQ6K1uHVwcds2RVrBHwLLEnpIQmqHXML6cb0SRgIVBBlGrbVgxuSiRwKlhWdBLFYkIHpMfamoYkYMpNR59keF8rXexHUlcIeKT+nkhJoNQw8HRnfqea9nLxP6+dgH/mpjyME2AhHS/yE4EhwnksuMsloyCGmhAqub4V0z6RhIIOr6hDsKdfniWNo4p9UrFujkvV80kcBbSL9lAZ2egUVdElqqE6ougRPaNX9GY8GS/Gu/Exbp0zJjM76A+Mzx+mEpmi</latexit>z ∼ N (0, I) (a) (b)
Figure 2: Our generator (a) vs a standard one (b). (a) We predict the motions of different body parts sequentially. (b)
Existing methods directly produce the whole motion. ing a motion prior/distribution, for which sufﬁciently di-verse training data is hard to obtain, we propose to learn a pose prior and enforce a hard constraint on the predicted poses to form smooth sequences and to satisfy human kine-matic constraints. Speciﬁcally, we model our pose prior as a normalizing ﬂow [45, 15], which allows us to compute the data log-likelihood exactly, and further promote diver-sity by maximizing the distance between pairs of samples during training.
To achieve controllable motion predictions, as illustrated in Fig. 2 (a), we generate the future motions of the different body parts of interest in a sequential manner. Our design allows us to produce diverse future motions that share the same partial body motion, e.g., the same leg motion but di-verse upper-body motions. This is achieved by ﬁxing the latent codes for some body parts while varying those of the other body parts. In contrast to [55], our approach allows us to train a single model that achieves both non-controllable and controllable motion prediction.
Our contributions can be summarized as follows: (i) We develop a uniﬁed framework achieving both diverse and part-based controllable human motion prediction, using a pre-ordered part sequence; (ii) We propose a pose prior and a joint angle constraint to regularize the training of our gen-erator and encourage it to produce smooth pose sequences.
Such strategy overcomes the difﬁculty of learning the dis-tribution of diverse motions as other VAE-based methods do.
Our experiments on standard human motion prediction benchmarks demonstrate that our approach outperforms the state-of-the-art methods in terms of sample diversity and ac-curacy. 2.