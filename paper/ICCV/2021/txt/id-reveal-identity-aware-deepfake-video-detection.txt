Abstract
A major challenge in DeepFake forgery detection is that state-of-the-art algorithms are mostly trained to detect a speciﬁc fake method. As a result, these approaches show poor generalization across different types of facial manip-ulations, e.g., from face swapping to facial reenactment.
To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, speciﬁc of how a person moves while talking, by means of metric learning coupled with an adversarial training strategy. The advantage is that we do not need any training data of fakes, but only train on real videos. Moreover, we utilize high-level semantic fea-tures, which enables robustness to widespread and disrup-tive forms of post-processing. We perform a thorough exper-imental analysis on several publicly available benchmarks.
Compared to state of the art, our method improves gener-alization and is more robust to low-quality videos, that are usually spread over social networks. In particular, we ob-tain an average improvement of more than 15% in terms of accuracy for facial reenactment on high compressed videos. 1.

Introduction
Recent advancements in synthetic media generation al-low us to automatically manipulate images and videos with a high level of realism. To counteract the misuse of these image synthesis and manipulation methods, the digital me-dia forensics ﬁeld got a lot of attention [41, 40]. For in-stance, during the past two years, there has been intense re-search on DeepFake detection, that has been strongly stim-ulated by the introduction of large datasets of videos with manipulated faces [35, 33, 15, 31, 25, 28, 19].
However, despite excellent detection performance, the major challenge is how to generalize to previously unseen methods. For instance, a detector trained on face swapping will drastically drop in performance when tested on a facial reenactment method. This unfortunately limits practicality as we see new types of forgeries appear almost on a daily
Figure 1: ID-Reveal is an identity-aware DeepFake video detec-tion. Based on reference videos of a person, we estimate a tem-poral embedding which is used as a distance metric to detect fake videos. basis. As a result, supervised detection, which requires ex-tensive training data of a speciﬁc forgery method, cannot immediately detect a newly-seen forgery type.
This mismatch and generalization issue has been ad-dressed in the literature using different strategies, ranging from applying domain adaptation [12, 5] or active learn-ing [17] to strongly increasing augmentation during training
[43, 15] or by means of ensemble procedures [15, 7]. A dif-ferent line of research is relying only on pristine videos at training time and detecting possible anomalies with respect to forged ones [24, 11, 13]. This can help to increase the generalization ability with respect to new unknown manip-ulations but does not solve the problem of videos charac-terized by a different digital history. This is quite common whenever a video is spread over social networks and posted multiple times by different users. In fact, most of the plat-forms often reduce the quality and/or the video resolution.
Note also that current literature has mostly focused on face-swapping, a manipulation that replaces the facial iden-tity of a subject with another one, however, a very effective modiﬁcation is facial reenactment [39], where only the ex-pression or the lips movements of a person are modiﬁed (Fig. 2). Recently, the MIT Center for Advanced Virtual-Figure 2: Automatic face manipulations can be split in two main categories: facial reenactment and face-swapping. The ﬁrst one alters the facial expression preserving the identity. The second one modiﬁes the identity of a person preserving the facial expression. ity created a DeepFake video of president Richard Nixon 1.
The synthetic video shows Nixon giving a speech he never intended to deliver, by modifying only the lips movement and the speech of the old pristine video. The ﬁnal result is impressive and shows the importance to develop forgery detection approaches that can generalize on different types of facial manipulations.
To better highlight this problem, we carried out an exper-iment considering the winning solution of the recent Deep-Fake Detection Challenge organized by Facebook on Kag-gle platform. The performers had the possibility to train their models using a huge dataset of videos (around 100k fake videos and 20k pristine ones with hundreds of different identities). In Fig. 3, we show the results of our experiment.
The model was ﬁrst tested on a dataset of real and deepfake videos including similar face-swapping manipulations, then we considered unseen face-swapping manipulations and ﬁ-nally videos manipulated using facial reenactment. One can clearly observe the signiﬁcant drop in performance in this last situation. Furthermore, the test on low quality com-pressed videos shows an additional loss and the ﬁnal value for the accuracy is no more than a random guess.
It is also worth noting that current approaches are often used as black-box models and it is very difﬁcult to predict the result because in a realistic scenario it is impossible to have a clue about the type of manipulation that occurred.
The lack of reliability of current supervised deep learning methods pushed us to take a completely different perspec-tive, avoiding to answer to a binary question (real or fake?) and instead focusing on wondering if the face under test preserves all the biometric traits of the involved subject.
Following this direction, our proposed method turns out to be able to generalize to different manipulation methods and also shows robustness w.r.t. low-quality data. It can re-veal the identity of a subject by highlighting inconsistencies 1https://moondisaster.org
Figure 3: Accuracy results (binary classiﬁcation task) of the win-ner of the Deepfake Detection Challenge [36] trained on DFDC dataset [15] and tested on different datasets: the preview DFDC
[16] (seen face swapping) and FaceForensics++ [35] both on face swapping and facial-reenactment. Results are presented on high quality (HQ) and low quality (LQ) videos. of facial features such as temporal consistent motion. The underlying CNN architecture comprises three main com-ponents: a facial feature extractor, a temporal network to detect biometric anomalies (temporal ID network) and a generative adversarial network that tries to predict person-speciﬁc motion based on the expressions of a different sub-ject. The networks are trained only on real videos contain-ing many different subjects [9]. During test time, in addition to the test video, we assume to have a set of pristine videos of the target person. Based on these pristine examples, we compute a distance metric to the test video using the em-bedding of the temporal ID network (Fig. 1). Overall, our main contributions are the following:
• We propose an example-based forgery detection ap-proach that detects videos of facial manipulations based on the identity of the subject, especially the person-speciﬁc face motion.
• An extensive evaluation that demonstrates the general-ization to different types of manipulations even on low-quality videos, with a signiﬁcant average improvement of more than 15% w.r.t. state of the art. 2.