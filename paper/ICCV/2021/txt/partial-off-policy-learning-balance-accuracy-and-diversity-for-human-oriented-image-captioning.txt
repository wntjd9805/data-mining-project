Abstract
Human-oriented image captioning with both high diver-sity and accuracy is a challenging task in vision+language modeling. The reinforcement learning (RL) based frame-works promote the accuracy of image captioning, yet seri-ously hurt the diversity. In contrast, other methods based on variational auto-encoder (VAE) or generative adversar-ial network (GAN) can produce diverse yet less accurate captions.
In this work, we devote our attention to pro-mote the diversity of RL-based image captioning. To be specific, we devise a partial off-policy learning scheme to balance accuracy and diversity. First, we keep the model exposed to varied candidate captions by sampling from the initial state before RL launched. Second, a novel criterion named max-CIDEr is proposed to serve as the reward for promoting diversity. We combine the above-mentioned off-policy strategy with the on-policy one to moderate the ex-ploration effect, further balancing the diversity and accu-racy for human-like image captioning. Experiments show that our method locates the closest to human performance in the diversity-accuracy space, and achieves the highest
Pearson correlation as 0.337 with human performance. 1.

Introduction
Image captioning is a challenging task in the field of computer vision and natural language processing.
It re-quires not only extracting semantic information from im-ages but also understanding and reorganizing such informa-tion in the form of natural language. To describe like hu-mans, image captioning models should be capable of pro-ducing diverse and accurate captions. Besides generating correct captions, several different captions about the visual content should also be provided. From such a perspec-tive, image caption models are supposed to generate human-oriented predictions by balancing accuracy and diversity.
Recent image captioning methods focus more on accu-*corresponding author
Figure 1. Typical outputs from different models by sampling 5 times according to the posteriors, where the captions generated by on-policy trained model are correct but unvaried (top), and cap-tions generated by models like VAE are diverse but less accurate (middle). We aim at balancing the two aspects to mimic generat-ing human-oriented captions (bottom). racy with deep reinforcement learning (RL). In particular, on-policy RL is adopted in [26, 23] to reduce the expo-sure bias and acquire sentence-level supervision. These methods are proven to benefit the accuracy performance on multiple metrics [22, 31, 9, 18, 1, 45]. However, they are prone to generate common sentences, resulting in poor di-versity [21]. Some other works focus on maintaining di-versity [27, 10, 35, 38, 3, 4]. Based on VAE or GAN, the diverse captions can be obtained. Yet the reported fair accu-racy is acquired under the selection of oracle or consensus re-ranking [11] process. When considering the entire poste-rior, there will be noticeable inaccurate cases predicted by these models. As demonstrated in Fig. 1, we sample 5 times according to the modeled posterior to evaluate the quality on both accuracy and diversity. Though the on-policy RL trained model [34] generates captions with no faults, it fails to produce distinct sentences in other forms. The diverse captioning model [35] can provide varied predictions, but incorrect descriptions exist within the outputs. In a word, there are obvious performance gaps on either diversity or accuracy for existing image captioning methods.
In this paper, we motivate to balance the accuracy and di-versity of image captioning models. To favor accuracy, we train the image captioning model based on deep reinforce-ment learning. We investigate why current RL-based meth-ods fail to generate diverse captions. We discover that the
Figure 2. High-level overview for the proposed partial off-policy learning scheme. Training samples are allocated to different training strategies to balance accuracy promotion and diversity preservation. on-policy strategy is easily trapped for a single prediction.
Therefore, more exploration is required during the training process to expose the agent to more fair cases. However, traditional exploration strategies treat unexplored cases in-discriminately. Considering the enormous searching space of the generated sentences, such exploration may be inef-ficient for the task of image captioning. Based on these observations, we propose a novel partial off-policy learn-ing scheme to encourage the exploration of new possibilities efficiently. To be specific, we first introduce an off-policy strategy into the image captioning framework, for which a diverse distribution is chosen as behavior policy for explo-ration. Samples derived from the such a policy are then fed into the model and rewarded by a novel criterion as max-CIDEr to encourage recurrence. With such behavior policy, the enormous searching space can be narrowed down to a certain sub-space to facilitate the training process. In prac-tice, we select over the on-policy and above-mentioned off-policy strategies with a certain probability to moderate the exploration effect. Such partial off-policy learning scheme allows us to negotiate the trade-off between diversity preser-vation and accuracy promotion, ultimately encouraging the model to mimic human-like performance.
The main contributions of this paper are: 1) We pro-pose the off-policy strategy and the novel max-CIDEr re-ward for RL-based image captioning to promote diversity. 2) We propose the partial off-policy learning to balance the diversity and accuracy for human-oriented image cap-tioning. Our work is evaluated on MSCOCO dataset [19].
We achieve a significant boost on diversity compared with the on-policy baseline, while acquiring the highest accu-racy on all sampled predictions compared with other diverse captioning works. Besides, our method locates closest to the human performance in the diversity-accuracy space and shows the strongest correlation to human evaluation with
Pearson correlation as 0.337. Our work is modular and can be applied to most other works for image captioning, mak-ing it easy to facilitate such balance in future researches. 2.