Abstract 1.

Introduction
This paper strives to classify and detect the relation-ship between object tubelets appearing within a video as a 〈subject-predicate-object〉 triplet. Where existing works treat object proposals or tubelets as single entities and model their relations a posteriori, we propose to classify and detect predicates for pairs of object tubelets a priori. We also propose Social Fabric: an encoding that represents a pair of object tubelets as a composition of interaction primi-tives. These primitives are learned over all relations, result-ing in a compact representation able to localize and clas-sify relations from the pool of co-occurring object tubelets across all timespans in a video. The encoding enables our two-stage network. In the first stage, we train Social Fabric to suggest proposals that are likely interacting. We use the
Social Fabric in the second stage to simultaneously fine-tune and predict predicate labels for the tubelets. Experi-ments demonstrate the benefit of early video relation mod-eling, our encoding and the two-stage architecture, leading to a new state-of-the-art on two benchmarks. We also show how the encoding enables query-by-primitive-example to search for spatio-temporal video relations. Code: https:
//github.com/shanshuo/Social-Fabric.
To understand what is happening where in videos, it is necessary to detect and recognize relationships between in-dividual instances. Effectively capturing these relationships could improve captioning [55], video retrieval [41], vi-sual question answering [1] and many other visual-language tasks.
In this paper, we strive to classify and detect the relationship between object tubelets appearing throughout a video as a 〈subject-predicate-object〉 triplet, like 〈dog-chase-child〉 or 〈horse-stand behind-person〉.
Shang et al. [38, 39] pioneered this challenging problem by their definition of video datasets with dense bounding box annotations, temporal bounds, and relationship-triplet labels. Following their guidance, a leading approach to date is to generate proposals for individual objects on short video snippets, encode the proposals, predict a relation and asso-ciate the relations over the entire video, e.g. [34, 42, 53]. To better detect long-term interactions, Liu et al. [30] forego the need for snippets by first localizing individual object tubelets throughout the entire video, filter out unlikely pairs and predict predicates for the remaining ones. Different from all these existing works on video relation prediction, which treat object proposals or tubelets as single entities and model their relations a posteriori, we propose to clas-1
sify and detect predicates for pairs of object tublets a priori.
Considering objects as tubelet pairs from the start re-quires an encoding that enables us to localize and classify interactions from the pool of all co-occurring object tubelets across all timespans in a video. This is reminiscent of many classical problems in computer vision that need to aggre-gate spatial, e.g. [2, 22, 40, 47], temporal, e.g. [28, 50, 57] or spatio-temporal, e.g. [15, 16, 32] primitives into a common representation. We take inspiration from ActionVLAD by
Girdhar et al. [16], which encodes actions as a composi-tion of local action primitives to capture the entire spatio-temporal extent of actions. In this paper, we also learn to encode local spatio-temporal video features in a composi-tional manner. Different from ActionVLAD, which oper-ates on an entire video, our Social Fabric encoding operates on tubelet pairs, i.e. on inputs from multiple object tubelets and multiple modalities, with a set of interaction primitives that is dynamically learned during video relation training.
Social Fabric captures information across the entire scope of tubelet pairs, which is especially beneficial when inter-actions last long. See Figure 1 for an illustrative example.
We make three contributions. First, we propose to clas-sify and detect video relations for pairs of object tubelets from the start. Second, we introduce Social Fabric, a compositional encoding suited for multi-tubelet and multi-modal inputs. The interaction primitives that form the en-coding are learned and updated dynamically, akin to the
NetVLAD layer from Arandjelovi´c et al. [2] for visual place recognition. Third, to leverage the Social Fabric, we pro-pose a two-stage network for video relation classification and detection. In the first stage, we localize interactions by training Social Fabric to propose tubelet pairs that are likely interacting. In the second stage we use the Social Fabric to simultaneously fine-tune and learn to predict predicate la-bels for the tubelets. Experiments on the benchmarks for video relation detection of Shang et al. [38, 39] show the benefits of our approach, especially when interactions are long and complex. Social Fabric outperforms alternative video encodings and our two-stage architecture sets a new state-of-the-art for both video relation classification and de-tection. Besides classification and detection, we show that our encoding enables searching for relations in videos by providing primitive-examples as queries. 2.