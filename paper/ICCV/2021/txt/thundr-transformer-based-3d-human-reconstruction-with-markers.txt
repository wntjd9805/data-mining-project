Abstract
We present THUNDR, a transformer-based deep neu-ral network methodology to reconstruct the 3d pose and shape of people, given monocular RGB images. Key to our methodology is an intermediate 3d marker represen-tation, where we aim to combine the predictive power of model-free-output architectures and the regularizing, anthropometrically-preserving properties of a statistical hu-man surface model like GHUM—a recently introduced, ex-pressive full body statistical 3d human model, trained end-to-end. Our novel transformer-based prediction pipeline can focus on image regions relevant to the task, supports self-supervised regimes, and ensures that solutions are consistent with human anthropometry. We show state-of-the-art results on Human3.6M and 3DPW, for both the fully-supervised and the self-supervised models, for the task of inferring 3d human shape, joint positions, and global translation. More-over, we observe very solid 3d reconstruction performance for difficult human poses collected in the wild. 1.

Introduction
The significant recent progress in 3d human sensing is supported by the development of statistical human surface models and the emergence of different forms of supervised and self-supervised visual inference methods. The use of statistical human pose and shape models offers advantages in the use of an anatomical and semantically meaningful human body representation, during both learning and infer-ence. Human anthropometry could be used to regularize a learning and inference process, which, in the absence of such constraints, and given the ambiguity of 3d lifting from monocular images, could easily run haywire. This is espe-cially true for unfamiliar and complex poses not previously seen in a ‘training set’—as they never all are. Semantic models offer not only correspondences with image detector responses (specific body keypoints or semantic segmenta-tion maps) which can give essential alignment signals for 3d self-supervision, but can also help rule out 3d solutions that may otherwise entirely break the symmetry of the body, the relative proportions of limbs, the consistency of the sur-face in terms of non self-intersection, or the anatomical joint angle limits.
The choice of evaluation metrics has an important role, too. For now, by far the most used representation—perceived as ‘model-independent’—are the ‘body joints’, a popular concept, neither by virtue of its anatomical clarity (as that point idealization could be bio-mechanically argued against), nor—for computer vision, and more practically—given its lack of ground-truth observability. In practice, human ‘body joints’ are obtained either by fitting proprietary articulated
3d body models to marker data (internal models of the Mo-cap system, where the assumptions and error models are not always available) or by human annotators eye-balling joint positions in images, followed by multi-view triangu-lation to obtain pseudo-ground truth. While the latter have proven extremely useful in bootstraping initial 3d predictors, the joint-click positioning cannot be considered an accurate anatomical reality, in any single image, and even less so, consistently, over a large corpora, especially as for many non-frontal-parallel poses ‘joint locations’ are difficult to correctly identify, visually. While some form of 3d body joint prediction error seems unavoidable under the current ground-truth and state of the art metrics, a safeguard could be to operate primarily with visually grounded structures and obtain joint estimates using statistical body models, based on their surface estimates, as just a final step.
In this paper, we rely on the visual reality of 3d body surface markers (in some conceptualization, a ‘model-free’ representation) and that of a 3d statistical body (a ‘model-based’ concept) as pillars in designing a hybrid 3d visual learning and reconstruction pipeline. Markers have the ad-ditional advantages of being useful for registration between different parametric models, can be conveniently relied-upon for fitting, and can be used as a reduced representation of body shape and pose, as we will here show. Our model combines multiple novel transformer refinement stages for efficiency and localization of key predictive features, and re-lies on combining ‘model-free’ and ‘model-based’ losses for both accuracy and for results consistent with human anthro-pometry. Quantitative results in major benchmarks indicate state of the art performance. Extensive qualitative testing in the wild supports the overall feasibility, and the quality of 3d reconstructions produced by THUNDR, under both supervised and self-supervised regimes.