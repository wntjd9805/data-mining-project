Abstract
Attribution map visualization has arisen as one of the most effective techniques to understand the underlying in-ference process of Convolutional Neural Networks. In this task, the goal is to compute an score for each image pixel related to its contribution to the network output.
In this paper, we introduce Disentangled Masked Backpropaga-tion (DMBP), a novel gradient-based method that lever-ages on the piecewise linear nature of ReLU networks to decompose the model function into different linear map-pings. This decomposition aims to disentangle the attri-bution maps into positive, negative and nuisance factors by learning a set of variables masking the contribution of each filter during back-propagation. A thorough evaluation over standard architectures (ResNet50 and VGG16) and benchmark datasets (PASCAL VOC and ImageNet) demon-strates that DMBP generates more visually interpretable at-tribution maps than previous approaches. Additionally, we quantitatively show that the maps produced by our method are more consistent with the true contribution of each pixel to the final network output. 1.

Introduction
Convolutional Neural Networks (CNNs) are ubiquitous in current state-of-the-art approaches for automatic visual understanding. Despite their outstanding performance in multiple tasks [10, 14, 40], they are still characterized as black-boxes whose internal inference rules are difficult to interpret. As a consequence, the trustability of this type of models is limited and it holds back their broader adoption in applications such as autonomous driving [17] or medical diagnosis [6], where it is crucial to ensure that model deci-sions are reliable and not based on data artifacts or biases.
In this context, several strategies have been explored to visualize the underlying rules guiding the model’s decision process [39]. Attribution map generation is one of the most effective methods for this purpose [3, 21, 25, 31, 33, 37].
This task aims to assign an score to each individual in-put (i.e., pixels) determining their contribution to the fi-Figure 1. Illustration of our proposed method for attribution map generation. Given an input image, we want to obtain a score for each pixel estimating their contribution to the CNN out-put for a target label (in bold). Our approach is able to iden-tify discriminative image pixels that contribute positively (red) or negatively (blue) to the prediction, and pixels corresponding to nuisance factors that have no effect on the output (white). For instance, in bottom-middle, positive attributions are assigned to the dog whereas negative scores correspond to pixels belonging to the cat. Additionally, no attributions are assigned to the non-discriminative background pixels. The disentanglement of this components produces fine-grained pixel-level attributions reveal-ing the patterns used by the network during inference. We show that the generated attribution maps are more informative and visu-ally interpretable than the ones obtained by previous methods. nal network output (e.g., the probability for a given class).
By visualizing attribution maps, it is then easy to verify whether network inference is guided by intuitive rules such as the identification of discriminative image regions related to high-level semantic concepts (see Fig. 1).
A promising approach to generate reliable attribution maps are gradient-based techniques [28, 33, 37, 31]. To de-termine the importance of each pixel, these methods use dif-ferent mechanisms to backpropagate the information from the output to the input image through the intermediate lay-ers. An appealing property of gradient-based methods is that, compared to other approaches producing coarse and less informative attribution maps [21, 25], they can identify 1
It has high-frequency patterns such as edges or textures. been shown that this information can be relevant to fully understand the network inference process [11].
In this paper, we introduce Disentangled Masked Back-propagation (DMBP). Similar to previous gradient-based approaches, our method uses backpropagation to determine the contribution of each input pixel to the network output.
However, DMBP addresses this task from a novel perspec-tive. In particular, we use the fact that standard CNNs with
ReLU non-linearities can be interpreted as piecewise lin-ear functions where the input space is separated into dif-ferent linear regions depending on the input [35]. Using this observation, DMBP decomposes output’s computation into different linear mappings that are used to disentangle nuisance, positive and negative factors from the attribution map. Whereas nuisance components refer to information that have no effect on the network output, the latter factors identify the discriminative pixels providing negative or pos-itive evidences for the target label (see again Fig. 1). The different linear mappings are identified by decomposing the network gradient into different sub-components, which are identified by learning a set of variables masking network filters during backpropagation (see Fig. 2 for an overview).
In our experiments, we validate the effectiveness of
DMBP by providing qualitative and quantitative results over standard network architectures (ResNet50 and VGG16) and benchmark datasets (ImageNet and PASCAL VOC). The results demonstrate that, compared to previous methods, at-tribution maps produced by DMBP are more consistent with the true contribution of each pixel to the network output.
Moreover, we show that our results are more informative and visually interpretable. 2.