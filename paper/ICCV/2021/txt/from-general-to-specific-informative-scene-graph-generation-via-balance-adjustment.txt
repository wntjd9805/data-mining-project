Abstract
The scene graph generation (SGG) task aims to detect visual relationship triplets, i.e., subject, predicate, object, in an image, providing a structural vision layout for scene understanding. However, current models are stuck in com-mon predicates, e.g., “on” and “at”, rather than informa-tive ones, e.g., “standing on” and “looking at”, resulting in the loss of precise information and overall performance.
If a model only uses “stone on road” rather than “block-ing” to describe an image, it is easy to misunderstand the scene. We argue that this phenomenon is caused by two key imbalances between informative predicates and com-mon ones, i.e., semantic space level imbalance and train-ing sample level imbalance. To tackle this problem, we pro-pose BA-SGG, a simple yet effective SGG framework based on balance adjustment but not the conventional distribu-tion ﬁtting.
It integrates two components: Semantic Ad-justment (SA) and Balanced Predicate Learning (BPL), respectively for adjusting these imbalances. Beneﬁted from the model-agnostic process, our method is easily applied to the state-of-the-art SGG models and signiﬁcantly improves the SGG performance. Our method achieves 14.3%, 8.0%, and 6.1% higher Mean Recall (mR) than that of the Trans-former model at three scene graph generation sub-tasks on
Visual Genome, respectively. Codes are publicly available1. 1.

Introduction
Scene Graph Generation (SGG) aims to detect instances-of-interest and their relationships in an image. It provides a structural vision layout, as an auxiliary tool, to bridge the
∗Corresponding author. 1https://github.com/ZhuGeKongKong/SGG-G2S (a) (b)
Figure 1. A confusion matrix of a baseline model (a) and two searching results for “stone on road” in Google (b). The element ci,j means the number of samples labeled as i but predicted as j.
The results of the model concentrate on the left side, e.g., “on” and
“near”. If the model directly uses the common predicate “on” to describe the ﬁrst image like the second image in (b), it may cause serious consequences. gap from computer vision to natural language, supporting many high-level tasks such as visual captioning [35, 1] and visual question answering [27, 15].
A good scene graph can provide sufﬁcient and informa-Figure 2. Ground-truth annotations from Visual Genome. The ﬁrst and second examples are marked as “on” because of the huge se-mantic space of “on”. Compared with “person standing on snow” in the third example, “person on snow” in the second example is ambiguous and information-poor. tive relationships among instance-of-interest.
Inspired by remarkable progress in object detection [6, 5, 24], existing solutions [10, 34, 16, 36] mostly follow a common gener-ation paradigm, that is, detecting objects from an image, extracting region features, and then recognizing the predi-cate categories under the guidance of standard classiﬁcation objective function. Based on this paradigm, current state-of-the-art methods, however, exhibit the tendency that most of the recognized predicates are general or common (e.g.,
“on” or “in”) and lack speciﬁc or informative contents (e.g.,
“standing on” or “sitting on”), as shown in Fig. 1(a). Thus, it is hard to apply an SGG algorithm in real-world settings like Fig. 1(b), since it provides insufﬁcient clues and may lead to misunderstanding about a scene. In this work, our study reveals that such biased predicates are caused by im-balance issues, preventing the power of the well-designed
SGG model from being exploited. Speciﬁcally, this issue can further be divided into two sub-problems: semantic space level imbalance and training sample level imbalance.
Semantic space level imbalance. Common predicates, such as “on”, have large semantic spaces, while informative predicates like “standing on” provide rich content but have small semantic spaces and even may be replaced by com-mon ones in some annotations. Some examples of this se-mantic space level imbalance are shown in Fig. 2. All ex-amples in Fig. 2 reﬂect the fact of “on”, while only the last two examples represent identical contents of “standing on”.
Although “standing on” is more speciﬁc for the last two ex-amples, humans still label the second image with “on” be-cause of its larger semantic space than “standing on”. The semantic space level imbalance also reﬂects the semantic relationship between predicates, e.g., annotators prefer to label “standing on” as “on” rather than “has”. Tagging dif-ferent labels for identical contents confuses the SGG model and causes poor performance.
Training sample level imbalance. When learning to recog-nize predicates, informative predicate samples are particu-larly valuable as they provide precise knowledge to a scene graph. However, predicate samples are dominated by the common predicate categories in the SGG dataset (e.g., Vi-sual Genome). For example, Fig. 3 shows the annotation distribution over predicate categories. We can observe that the number of common predicate samples is much larger
Figure 3. Frequencies of predicates in the Visual Genome dataset.
For clarity, we omit some predicates. than that of informative ones, which leads to the problem of the long-tailed distribution of the classes. Within such an imbalanced sampling space, the prediction of informative predicates is dominated by the common ones.
The above challenges motivate us to study two prob-lems: 1) how to revise common predicates as informative ones based on the semantic relation. And 2) how to make training sample space balanced. To tackle these problems, we believe that the key is to explore the predicate rela-tion in semantic space and adjust sampling space to be bal-anced. Motivated by this, we propose a simple yet effective pipeline, namely, Scene Graph Generation with Balance
Adjustment (BA-SGG), to learn informative scene graph generation. This pipeline integrates two novel components: 1) Semantic Adjustment (SA), which casts common predic-tions as informative ones with rich information contents by fully exploiting semantic relation among predicates. 2) Bal-anced Predicate Learning (BPL), which mines informative predicates according to their information contents.
To sum up, the main contributions of our work con-tain: 1). We systematically review the process of scene graph generation and reveal the problem of insufﬁcient in-formation contents that limit SGG’s overall performance and practical applicability, i.e., semantic space level im-balance and training sample level imbalance; 2). We pro-pose the BA-SGG for informative scene graph generation, a novel framework that adjusts biased predicate predictions by two components: semantic adjustment (SA) and bal-anced predicate learning (BPL); 3). The proposed BA-SGG is tested on Visual Genome, obtaining signiﬁcant improve-ments over state-of-the-art SGG approaches. Without bells and whistles, BA-SGG achieves 14.3%, 8.0%, and 6.1% higher Mean Recall (mR) than Transformer [31, 28] on three scene graph generation sub-tasks, respectively. Be-sides, we propose a new metric (mRIC@K) to measure the information content of results. 2.