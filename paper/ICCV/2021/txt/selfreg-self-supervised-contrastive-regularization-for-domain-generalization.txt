Abstract
In general, an experimental environment for deep learn-ing assumes that the training and the test dataset are sam-pled from the same distribution. However, in real-world situations, a difference in the distribution between two datasets, i.e. domain shift, may occur, which becomes a major factor impeding the generalization performance of the model. The research field to solve this problem is called domain generalization, and it alleviates the domain shift problem by extracting domain-invariant features explicitly or implicitly. In recent studies, contrastive learning-based domain generalization approaches have been proposed and achieved high performance. These approaches require sam-pling of the negative data pair. However, the performance of contrastive learning fundamentally depends on quality and quantity of negative data pairs. To address this is-sue, we propose a new regularization method for domain generalization based on contrastive learning, called self-supervised contrastive regularization (SelfReg). The pro-posed approach use only positive data pairs, thus it re-solves various problems caused by negative pair sampling.
Moreover, we propose a class-specific domain perturbation layer (CDPL), which makes it possible to effectively apply mixup augmentation even when only positive data pairs are used. The experimental results show that the techniques in-corporated by SelfReg contributed to the performance in a compatible manner. In the recent benchmark, DomainBed, the proposed method shows comparable performance to the conventional state-of-the-art alternatives. 1.

Introduction
Machine learning systems often fail to generalize out-of-sample distribution as they assume that in-samples and out-of-samples are independent and identically distributed – this assumption rarely holds during deployment in real-world scenarios where the data is highly likely to change over time
∗Corresponding authors: J. Kim (jinkyukim@korea.ac.kr) and J. Lee (jaekoo@kookmin.ac.kr)
Figure 1. Our model utilizes the self-supervised contrastive losses for the model to learn domain-invariant representation by mapping the latent representation of the same-class samples close together.
Note that different shapes (i.e. circles, stars, and squares) indicate different classes Ci∈{1,2,3}, and we differently color-code accord-ing to their domain Di∈{1,2,3,4,Target}. and space. Deep convolutional neural network features are often domain-invariant to low-level visual cues [35], some studies [10] suggest that they are still susceptible to domain shift.
There have been increasing efforts to develop models that can generalize well to out-of-distribution. The liter-ature in domain generalization (DG) aims to learn the in-variances across multiple different domains so that a classi-fier can robustly leverage such invariances in unseen test
In the domain gener-domains [40, 15, 29, 28, 31, 38]. alization task, it is assumed that multiple source domains are accessible during training, but the target domains are not [4, 31]. This is different from domain adaptation (DA), semi-supervised domain adaptation (SSDA), and unsuper-vised domain adaptation (UDA) problems, where examples from the target domain are available during training. In this paper, we focus on the domain generalization task.
Some recent studies [7, 20, 33] suggest that contrastive learning can be successfully used in a self-supervised learn-ing task by mapping the latent representations of the posi-tive pair samples close together, while that of negative pair samples further away in the embedding space. Such a con-trastive learning strategy has also been utilized for the do-main generalization tasks [30, 11], similarly aiming to re-duce the distance of same-class features in the embedding space, while increasing the distance of different-class fea-tures. However, such negative pairs often make the training unstable unless useful negative samples are available in the same batch, which is but often challenging.
In this work, we revisit contrastive learning for the do-main generalization task, but only with positive pair sam-ples. As it is generally known that using positive pair samples only causes the performance drop, which is of-ten called representation collapse [17]. Inspired by recent studies on self-supervised learning [8, 17], which success-fully avoids representation collapse by placing one more projection layer at the end of the network, we successfully learn domain-invariant features and our model trained with self-supervised contrastive losses shows the matched or bet-ter performance against alternative state-of-the-art methods, where ours is ranked at top places in the domain generaliza-tion benchmarks, i.e. DomainBed [18].
However, self-supervised contrastive losses are only part of the story. As we generally use a linear form of the loss function, properly balancing gradients is required so that network parameters converge to generate domain-invariant features. To mitigate this issue, we advocate for apply-ing the following three gradient stabilization techniques: (i) loss clipping, (ii) stochastic weights averaging (SWA), and (iii) inter-domain curriculum learning (IDCL). We observe that the combined use of these techniques further improves the model’s generalization power.
To effectively evaluate our proposed model, we first use the publicly available domain generalization data set called PACS [26], where we analyzed our model in detail to support our claims. We further experiment with much larger benchmarks called DomainBed [18] where our model shows matched or better performance against alternative state-of-the-art methods.
We summarize our main contributions as follows:
• SelfReg facilitates the application of metric learning using only positive pairs without negative pairs.
• We devised a CDPL by exploiting a condition that use only positive pairs. The combination of CDPL and mixup improves the weakness of mixup approach.
• The performance comparable to that of the SOTA DG methods was confirmed in the DomainBed that facil-itated the comparison of DG performance in the fair and realistic environment. eralizable to unseen target domains, which are generally outside the training distribution. Of a landmark work,
Vapnik et al. [40] introduces Empirical Risk Minimization (ERM) that minimizes the sum of errors across domains.
Notable variants have been introduced to learn domain-invariant features by matching distributions across differ-ent domains. Ganin et al. [15] utilizes an adversarial net-work to match such distributions, while Li et al. [29] instead matches the conditional distributions across domains. Such a shared feature space is optimized by minimizing maxi-mum mean discrepancy [28], transformed feature distribu-tion distance [31], or covariances [38].
In this work, we also follow this stream of work, but we explore the benefit of self-supervised contrastive learning that can inherently learn to domain-invariant discriminating feature by explic-itly mapping the “same-class” latent representations close together.
To our best knowledge, there are few that applied contrastive learning in the domain generalization set-ting. Classification and contrastive semantic alignment (CCSA) [30] and model-agnostic learning of semantic fea-tures (MASF) [11] aimed to reduce the distance of same-class (positive pair) feature distributions while increasing the distance of different-class (negative pair) feature distri-butions. However, using such negative pairs often make the training unstable unless useful negative samples are avail-able in the same batch, which is often challenging. To address this issue, we focus on minimizing a distance be-tween the same-class (positive pair) features in the embed-ding space as recently studied for the self-supervised learn-ing task [7, 20, 33], including BYOL [17] and SimSiam [8].
Inter-domain mixup [45, 44, 43] techniques are intro-duced to perform empirical risk minimization on linearly interpolated examples from random pairs across domains.
We also utilize such a mixup, but we only interpolate same-class features to preserve the class-specific features. We ob-serve that such a same-class mixup help obtaining robust performance for unseen domain data.
As another branch, JiGen [5] utilizes a self-supervised signal by solving a jigsaw puzzle as a secondary task to improve generalization. Meta-learning frameworks [27] are also explored for domain generalization to meta-learn how to generalize across domains by leveraging
MAML [14]. Some also explored splitting the model into domain-invariant and domain-variant components by low-rank parameterization [26], style-agnostic network [32], and domain-specific aggregation modules [12]. 3. Method 2.