Abstract
A variety of effective face-swap and face-reenactment methods have been publicized in recent years, democratiz-ing the face synthesis technology to a great extent. Videos generated as such have come to be called deepfakes with a negative connotation, for various social problems they have caused. Facing the emerging threat of deepfakes, we have built the Korean DeepFake Detection Dataset (KoDF), a large-scale collection of synthesized and real videos focused on Korean subjects. In this paper, we pro-vide a detailed description of methods used to construct the dataset, experimentally show the discrepancy between the distributions of KoDF and existing deepfake detec-tion datasets, and underline the importance of using mul-tiple datasets for real-world generalization. KoDF is pub-licly available at https://moneybrain-research. github.io/kodf in its entirety (i.e. real clips, synthe-sized clips, clips with adversarial attack, and metadata). 1.

Introduction
In recent years, the fabrication of facial content in im-ages and videos has become considerably easier and faster, which previously required heavy computing resources and expert knowledge. Latest deep-learning-based technologies have made it possible to handily produce photorealistic fake images and videos by manipulating facial expressions or swapping faces. Soon the word deepfake became the de facto term to indicate such facial forgeries synthesized by deep learning models.
While mostly utilized for innocuous purposes such as parody videos [3] and entertaining apps [2, 13], well-designed deepfakes could be used maliciously to defame an individual [20, 31], propagate disinformation [55, 18], or commit fraud [51, 56]. Due to the growing concerns over deepfakes, there is a recent surge of interest in de-veloping deepfake detection models, and to this end, var-*Equal contribution
Figure 1. KoDF is a distribution-controlled large-scale Korean deepfake detection dataset aimed to complement other datasets and to accommodate elaborate augmentation techniques for bet-ter generalization to real-world deepfakes. ious public datasets [37, 48, 39, 25, 36, 26, 23] and bench-marks [8, 6, 4, 7] have been constructed. They have greatly contributed to encouraging, facilitating, and standardizing deepfake detection research.
In line with these efforts, we release the Korean Deep-Fake Detection Dataset (KoDF) featuring important differ-entiations from the previous deepfake detection datasets.
KoDF is the largest among publicly available deepfake de-tection datasets, containing 175,776 fake clips and 62,166 real clips of 403 subjects. The deepfake samples are gen-erated with six different synthesis models. To counterbal-ance the Asian demographics underrepresented in the exist-ing deepfake detection databases, the participants of KoDF are comprised mostly of Koreans. Finally, the dataset takes various measures to better manage the data distribution re-garding the participantsâ€™ age, sex, and content. Table 1 com-pares KoDF with other public deepfake detection datasets in various aspects.
Dataset
UADFV [58]
DeepfakeTIMIT [37]
FF++ [48]
Celeb-DF [39]
GDFD [25]
DF-1.0 [36]
DFDC [23]
KoDF
Real videos 49 640 1,000 590 363 50,000 23,654 62,166
Fake videos 49 320 4,000 5,639 3,068 10,000 104,500 175,776
Total videos 98 960 5,000 6,229 3,431 60,000 128,154 237,942
Rights cleared
No
No
No
No
Yes
No1
Yes
Yes
Agreeing subjects 0 0 0 0 28 100 960 403
Total subjects 49 32
N/A 59 28 100 960 403
Methods 1 2 4 1 5 1 8 6
Table 1. Quantitative comparison of KoDF to existing public deepfake detection datasets.
Our contributions are twofold: (1) We propose KoDF that is the largest public deepfake detection dataset, planned and examined for the quality and diversity of its samples. (2) We experimentally demonstrate that none of the estab-lished deepfake detection datasets single-handedly suffices in approximating the true deepfake distribution. We then show how utilizing KoDF in conjunction with them for training enhances the generality of a detection model, offer-ing insights into the future strategy in deepfake detection. 2.