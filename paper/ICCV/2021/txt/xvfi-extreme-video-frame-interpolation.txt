Abstract 1.

Introduction
In this paper, we ﬁrstly present a dataset (X4K1000FPS) of 4K videos of 1000 fps with the extreme motion to the research community for video frame interpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that ﬁrst handles the VFI for 4K videos with large motion.
The XVFI-Net is based on a recursive multi-scale shared structure that consists of two cascaded modules for bidi-rectional optical ﬂow learning between two input frames (BiOF-I) and for bidirectional optical ﬂow learning from target to input frames (BiOF-T). The optical ﬂows are sta-bly approximated by a complementary ﬂow reversal (CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start at any scale of input while the BiOF-T module only operates at the original input scale so that the inference can be accelerated while maintaining highly accurate VFI performance. Extensive experimental results show that our XVFI-Net can successfully capture the essen-tial information of objects with extremely large motions and complex textures while the state-of-the-art methods exhibit poor performance. Furthermore, our XVFI-Net framework also performs comparably on the previous lower resolution benchmark dataset, which shows a robustness of our algo-rithm as well. All source codes, pre-trained models, and proposed X4K1000FPS datasets are publicly available at https://github.com/JihyongOh/XVFI.
*Both authors contributed equally to this work.
†Corresponding author.
Video frame interpolation (VFI) converts low frame rate (LFR) contents to high frame rate (HFR) videos by syn-thesizing one or more intermediate frames between given two consecutive frames, and then the videos of fast motion can be smoothly rendered in an increased frame rate, thus yielding reduced motion judder [28, 24, 23, 10]. Therefore, it is widely used for various practical applications, such as adaptive streaming [45], novel view interpolation synthe-sis [11], frame rate up conversion [29, 5, 49], slow mo-tion generation [18, 4, 30, 32, 27, 34] and video restora-tion [21, 42, 14, 41]. However, VFI is signiﬁcantly chal-lenging, which is attributed to diverse factors such as oc-clusions, large motions and change of light. Recent deep-learning-based VFI has been actively studied, showing re-markable performances [47, 4, 7, 37, 25, 13, 31, 50, 6, 33].
However, they are often optimized for existing LFR bench-mark datasets of low resolution (LR), which may lead to poor VFI performance, especially for videos of 4K resolu-tion (4096×2160) or higher with very large motion [1, 21].
Such 4K videos often contain frames of fast motion with extremely large pixel displacements for which conventional convolutional neural networks (CNNs) do not effectively work with receptive ﬁelds of limited sizes.
To solve the above issues for deep learning-based
VFI methods, we directly photographed 4K videos to construct a high-quality HFR dataset of high resolution, called X4K1000FPS. Fig. 1 shows some examples of our
X4K1000FPS dataset. As shown, our videos of 4K resolu-tion have extremely large motions and occlusions.
(cid:20)(cid:26)(cid:28)(cid:83)(cid:76)(cid:91)(cid:72)(cid:79)(cid:86) (cid:21)(cid:27)(cid:27)(cid:83)(cid:76)(cid:91)(cid:72)(cid:79)(cid:86)
Overlapped 4K inputs (crop)
XVFI-Net (Ours)
FeFlow
DAIN
Figure 2. VFI results for extreme motions. Our XVFI-Net can gen-erate a more stable intermediate frame with very large motions than two recent SOTA methods, FeFlow [13] and DAIN [4], which are newly trained on our dataset for fair comparisons.
We also ﬁrst propose an extreme VFI model, called
XVFI-Net, that is effectively designed to handle such a challenging dataset of 4K@1000fps. Instead of directly capturing extreme motions through consecutive feature spaces with deformable convolution as recent trends in video restoration [13, 46, 42, 41, 20], or using very large-sized pretrained networks with extra information such as contexts, depths, ﬂows and edges [4, 50, 30, 13], our XVFI-Net is simple but effective, which is based on a recur-sive multi-scale shared structure. The XVFI-Net has two cascaded modules: one for the bidirectional optical ﬂow learning between two input frames (BiOF-I) and the other for the bidirectional optical ﬂow estimating from target to the inputs (BiOF-T). The BiOF-I and BiOF-T modules are trained in combination with multi-scale losses. However, once trained, the BiOF-I module can start from any down-scaled input upward while the BiOF-T module only oper-ates at the original input scale at inference, which is com-putationally efﬁcient and helps to generate an intermediate frame at any target time instance. Structurally, the XVFI-Net is adjustable in terms of the number of scales for infer-ence according to the input resolutions or the motion magni-tudes, even if training is once over. We also propose a novel optical ﬂow estimation from time t to those of the inputs, called a complementary ﬂow reversal (CFR) that effectively
ﬁlls the holes by taking complementary ﬂows. Extensive ex-periments are conducted for fair comparison and our XVFI-Net that has a relatively smaller complexity outperforms previous VFI SOTA methods on our X4K1000FPS, espe-cially for extreme motions as shown in Fig. 2. A further ex-periment on the previous LR-LFR benchmark dataset also demonstrates the robustness of our XVFI-Net. Our contri-butions are summarized as:
• We ﬁrst propose a high-quality of HFR video dataset of 4K resolution, called X4K1000FPS (4K@1000fps) which contains a wide variety of textures, extremely large motions, zoomings and occlusions.
• We propose the CFR that can generate stable optical
ﬂow estimation from time t to the input frames, boost-ing both qualitative and quantitative performances.
• Our proposed XVFI-Net can start from any down-scaled input upward, which is adjustable in terms of the number of scales for inference according to the in-put resolutions or the motion magnitudes.
• Our XVFI-Net achieves state-of-the-art performance on the testset of X4K1000FPS with a signiﬁcant mar-gin compared to the previous VFI SOTA methods while having computational efﬁciency with a small number of ﬁlter parameters. All source codes and pro-posed X4K1000FPS dataset are publicly available at https://github.com/JihyongOh/XVFI. 2.