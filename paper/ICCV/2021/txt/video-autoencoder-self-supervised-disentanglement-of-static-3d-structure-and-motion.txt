Abstract
A video autoencoder is proposed for learning disentan-gled representations of 3D structure and camera pose from videos in a self-supervised manner. Relying on temporal continuity in videos, our work assumes that the 3D scene structure in nearby video frames remains static. Given a sequence of video frames as input, the video autoencoder extracts a disentangled representation of the scene includ-ing: (i) a temporally-consistent deep voxel feature to rep-resent the 3D structure and (ii) a 3D trajectory of camera pose for each frame. These two representations will then be re-entangled for rendering the input video frames. This video autoencoder can be trained directly using a pixel re-construction loss, without any ground truth 3D or camera pose annotations. The disentangled representation can be applied to a range of tasks, including novel view synthe-sis, camera pose estimation, and video generation by mo-tion following. We evaluate our method on several large-scale natural video datasets, and show generalization re-sults on out-of-domain images. Project page with code: https://zlai0.github.io/VideoAutoencoder. 1.

Introduction
The visual world arrives at a human eye as a streaming, entangled mess of colors and patterns. The art of seeing, to a large extent, is in our ability to disentangle this mess into physically and geometrically coherent factors: persistent solid structures, illumination, texture, movement, change of viewpoint, etc. From its very beginnings, computer vision has been concerned with acquiring this impressive human ability, including such classics as Barrow and Tenebaum’s
Intrinsic Image decomposition [5] in the 1970s, or Tomasi-Kanade factorization [63] in the 1990s. In the modern deep era, learning a disentangled visual representation has been a hot topic of research, often taking the form of an autoencoder
[39, 23, 36, 19, 41, 48, 22, 3, 50]. However, almost all prior work has focused on disentanglement within the 2D image plane using datasets of still images.
In this work, we propose a method that learns a disentan-gled 3D scene representation, separating the static 3D scene structure from the camera motion. Importantly, we employ videos as training data (as opposed to dataset of stills), using the temporal continuity within a video as a source of training signal for self-supervised disentanglement. We make the assumption that a local snippet of video is capturing a static scene, so the changes in appearance must be due to camera motion. This leads to our Video Autoencoder formulation, shown on Figure 1: an input video is encoded into two codes, one for 3D scene structure (which is forced to remain ﬁxed cross frames) and the other for the camera trajectory (up-dated for every frame). The 3D structure is represented by
3D deep voxels (similar to [46, 57]) and the camera pose with a 6-dimension rotation and translation vector. To re-construct the original video, we simply apply the camera transformation to the 3D structure features and then decode back to pixels.
A key advantage of our framework is that it provides 3D representations readily integrated modern neural rendering methods, which typically requires 3D and/or camera pose ground truth annotation at training time [13, 47, 4, 71, 62].
This usually implies a 2-stage process with running Structure-from-Motion (SfM) as precursor to training. In our work, we are working towards a way of training on completely unstructured datasets, removing the need for running SfM as preprocessing.
At test time, the features obtained using our Video Au-toencoder can be used for several downstream tasks, includ-ing novel view synthesis (Section 4.3), pose estimation in video (Section 4.2), and video following (Section 4.4). For novel view synthesis, given a single input image, we ﬁrst encode it as a 3D scene feature, and then render to a novel view by providing a new camera pose. We show results on large-scale video datasets including RealEstate10K [78],
Matterport3D [6], and Replica [60]. Our method not only achieves better view synthesis results than state-of-the-art view synthesis approach [71] that requires stronger camera supervision on RealEstate10K, but also generalize better when applied to out-of-domain data. As another application, we show that our method could be used to implicitly fac-torize structure from motion in novel videos, by evaluating the estimate camera pose against SfM baseline. Finally, we show that by swapping the 3D structure and camera trajec-tory codes between a pair of videos, we can achieve Video
Following, where a scene from one video is “following” the motion from the other video. 2.