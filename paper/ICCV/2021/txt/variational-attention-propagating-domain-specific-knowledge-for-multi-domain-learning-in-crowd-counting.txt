Abstract
In crowd counting, due to the problem of laborious la-belling, it is perceived intractability of collecting a new large-scale dataset which has plentiful images with large diversity in density, scene, etc. Thus, for learning a general model, training with data from multiple different datasets might be a remedy and be of great value. In this paper, we resort to the multi-domain joint learning and propose a sim-ple but effective Domain-specific Knowledge Propagating
Network (DKPNet) for unbiasedly learning the knowledge from multiple diverse data domains at the same time. It is mainly achieved by proposing the novel Variational Atten-tion(VA) technique for explicitly modeling the attention dis-tributions for different domains. And as an extension to VA,
Intrinsic Variational Attention(InVA) is proposed to handle the problems of over-lapped domains and sub-domains. Ex-tensive experiments have been conducted to validate the su-periority of our DKPNet over several popular datasets, in-cluding ShanghaiTech A/B, UCF-QNRF and NWPU. 1.

Introduction
Crowd counting is a challenging problem since it suffers from multiple actual issues behind data distributions, such as high variability in scales, density, occlusions, perspective distortions, background scenarios, etc. A direct solution to mitigate these issues is to collect a large-scale dataset with abundant data variations like ImageNet[7], so as to encour-age the learned model to be more robust and general. How-ever, collecting such a large-scale dataset with rich diversity for crowd-counting training is intractable due to the diffi-culty in human-labeling. Specifically, in crowd counting, due to the limitation of various conditions, images collected by a research group might only contain certain types of vari-ations and are limited in numbers. For example, as shown in
Fig.1, one can observe that there are large variations in data
*Equal contribution
†Corresponding author
Figure 1: Data distribution comparison between ShanghaiTech
[64], UCF-QNRF [15] and NWPU [54]. ShanghaiTech A is mainly composed of congested images, QNRF is of highly-congested samples and have more background scenarios, NWPU covers a much larger variety of data distributions due to density, perspective, background, etc, while ShanghaiTech B prefers low density and ordinary street-based scenes. distributions across different datasets.
Images in Shang-haiTech A(SHA)[64] tend to show congested crowds, and those in UCF-QNRF(QNRF)[15] are more likely to depict highly-congested crowds and have more background sce-narios, and those in NWPU[54] have much more diversi-ties in scales, density, background, etc. In contrast, samples within ShanghaiTech B(SHB)[64] just prefer low density crowds and the ordinary street-based scenes. Considering the aforementioned facts, in order to learn a general and robust estimating model for correct density prediction, this paper resorts to multi-domain learning which aims to solve the same or similar problem with multiple datasets across different domains1 simultaneously by utilizing all the data these domains provide. In other words, multi-domain learn-ing gives chances of using relatively abundant data varia-tions coming from different datasets for learning a general and robust density estimating model.
However, in crowd counting, an interesting phenomenon can be observed when jointly training with multiple differ-ent datasets. As shown in Tab.1, if under the supervision of the 3-joint of SHA, SHB and QNRF, the deep model prefers to only improve the performances on SHA and QNRF, while sacrificing that on SHB (5% performance drop ). Ac-tually, this kind of phenomenon (i.e. biased/partial learning) 1Commonly, a domain often refers to a data set where samples follow the similar or same underlying data distribution[57].
Table 1: MAE Results of IT/JT in 3-Joint datasets.
Methods
Individual Training (IT)
Joint Training (JT)
SHA 60.6 60.2 (↓)
SHB 8.8 9.3 (↑)
QNRF 97.7 92.8 (↓) exists widely in computer vision fields. It is because that deep models have the partial/biased learning behavior[2], i.e. deep models easily learn to focus on surface statisti-cal regularities rather than more general abstract concepts.
In other words, deep models will selectively learn the domi-nant data knowledge from certain dominant domains2 while ignoring other potential helpful information from the rest domains. To this end, developing an effective algorithm that can successfully utilize all the knowledge from differ-ent datasets remains important.
In this paper, we propose the Domain-specific Knowl-edge Propagating Network (DKPNet) for multi-domain learning, which intends to refine the propagated joint knowledge according to the domain-specific distributions and highlight all domains without bias. Specifically, a novel
Variational Attention (VA) technique is introduced for fa-cilitating the domain-specific attention learning. Based on
VA, the output attention distribution can be easily controlled by the latent variable. And we apply the Gaussian Mix-ture distribution as the prior in the proposed VA for multi-domain learning. Furthermore, as an extension to VA, the
Intrinsic Variational Attention (InVA) is proposed for han-dling the potential problems of overlapped-domains and sub-domains. VA and InVA both insist to provide domain-specific guidance for knowledge propagating, but start from coarse and intrinsic perspectives, respectively. In summary, the contributions of this paper are listed as follows:
• DKPNet is proposed to learn a general and robust den-sity estimating model for crowd counting by multi-domain joint learning, which can successfully prevent the model from just learning several dominant do-mains, and can consistently improve the performances over all the datasets.
• VA/InVA are introduced to provide the domain-specific guidance for refining the propagating knowl-edge with the help of latent variable. To our knowl-edge, it is the first work to use variational learning in attention for crowd counting.
• Extensive experiments have been conducted on several popular datasets, including ShanghaiTech A/B[64],
UCF-QNRF[15] and NWPU[54], and achieve the state-of-the-art performances on the MAE evaluation. 2.