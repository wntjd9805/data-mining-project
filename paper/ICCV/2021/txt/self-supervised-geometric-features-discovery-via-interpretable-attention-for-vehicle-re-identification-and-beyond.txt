Abstract
To learn distinguishable patterns, most of recent works in vehicle re-identification (ReID) struggled to redevelop of-ficial benchmarks to provide various supervisions, which requires prohibitive human labors. In this paper, we seek to achieve the similar goal but do not involve more hu-man efforts. To this end, we introduce a novel framework, which successfully encodes both geometric local features and global representations to distinguish vehicle instances, optimized only by the supervision from official ID labels.
Specifically, given our insight that objects in ReID share similar geometric characteristics, we propose to borrow self-supervised representation learning to facilitate geomet-ric features discovery. To condense these features, we in-troduce an interpretable attention module, with the core of local maxima aggregation instead of fully automatic learn-ing, whose mechanism is completely understandable and whose response map is physically reasonable. To the best of our knowledge, we are the first that perform self-supervised learning to discover geometric features. We conduct com-prehensive experiments on three most popular datasets for vehicle ReID, i.e., VeRi-776, CityFlow-ReID, and Vehi-cleID. We report our state-of-the-art (SOTA) performances and promising visualization results. We also show the excel-lent scalability of our approach on other ReID related tasks, i.e., person ReID and multi-target multi-camera (MTMC) vehicle tracking. 1.

Introduction
Vehicle ReID is a fundamental but challenging problem in video surveillance due to subtle discrepancy among ve-hicles from identical make and large variation across view-points of the same instance. The success of recent works suggests that the key to solving this problem is to incor-*This work was done when the author visited VISLab at WPI https:
//zhang-vislab.github.io (Top) In the literature, a labor-intensive fine-Figure 1: grained labels annotation is often required to capture local discriminative features, such as (a) labelling landmarks in
[55] to learn orientation invariant features. In contrast, we manage to discover such geometric features (denoted as red in (b)) in a self-supervised way. (Bottom) As for its gen-eralization ability, our approach can also consistently locate critical parts of a deformable human body, e.g., head, up-per arms, and knees, with no using corresponding ground truths. Best viewed in color. porate explicit mechanisms to discover and concentrate on informative vehicle parts (e.g., wheels, manufacturer logos) for discriminative feature extraction in addition to captur-ing robust features from a holistic image. They all sought, however, to edit original data to provide supplementary su-pervisions, e.g., view segmentation [39], keypoints [24, 55], vehicle orientation [24, 55, 8, 7] or key parts [17], for train-ing their deep classifiers. Although these methods perform satisfactorily, their annotation processes inevitably involve intensive human efforts, which significantly limits the appli-cability of such approaches. For example, when deployed in a new scenario, [17] demands the informative parts have to be manually localized to optimize their YOLO detector.
Afterwards, they are able to embed local patterns from de-tected regions-of-interest to assist ReID. So it is desirable to develop approaches which are capable of concentrating on informative details of a vehicle body but do not require corresponding ground truths.
On the other hand, while their power is demonstrated by various computer vision tasks [12, 2, 24, 4], existing atten-tion mechanisms, like channel attention [21], spatial atten-tion [58], and self-attention [53], are all pretty sophisticated and obscure. That is to say, their architectures are difficult to explain and attention maps are learned all by themselves.
In self-attention [53], for example, high-dimensional em-beddings Q, K, V are first projected from an input by con-volutional or linear operations and then entry-wise corre-lation (attention) is obtained through matrix multiplication between Q, K. V is weighted by the resulting correlation matrix as the attentional output. Although the workflow seems to make sense, the underlying principle why it works is still a black-box like other deep networks. Additionally, their learned attentions usually spread over a holistic object without specific concerns. Otherwise, an interpretable at-tention module, whose design should be easy to understand, can reveal what is critical for recognition and help to guide further improvement.
In light of the above observations, we propose a novel framework that can successfully learn discriminative ge-ometric features, under the assistance of self-supervised learning and a simple but interpretable attention, in addition to global representations for vehicle ReID. In specific, self-supervised learning is performed to optimize an encoder network, which is shared to condense low-level vehicle rep-resentations, under the supervision of automatically gener-ated ground truths. The encoded vehicle representations are fed into the introduced interpretable attention mechanism to acquire an attention map. By weighting it on another low-level vehicle representations, we obtain the regions-of-interest emphasized features for vehicle ReID.
In summary, our key contributions in this work are:
• We are the first to successfully learn informative geomet-ric features for vehicle ReID without supervisions from fine-grained annotations.
• An interpretable attention module, whose design is easy to explain and whose concentrations are physically im-portant locations, is introduced to highlight the automatic regions-of-interest.
• We report the SOTA performances of our proposed ap-proach on widely used vehicle ReID benchmarks, i.e.,
VeRi-776 [33], CityFlow-ReID [50], and VehicleID [31], compared with all existing works including those involv-ing more supervisions from manual annotations. We also visualize the reliable and consistent geometric features learned by our framework.
• The excellent scalability of the proposal is demonstrated by our directly transferring experiments on person ReID and MTMC vehicle tracking. 2.