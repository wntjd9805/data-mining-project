Abstract
Recognizing new objects by learning from a few labeled examples in an evolving environment is crucial to obtain excellent generalization ability for real-world machine learn-ing systems. A typical setting across current meta learning algorithms assumes a stationary task distribution during meta training. In this paper, we explore a more practical and challenging setting where task distribution changes over time with domain shift. Particularly, we consider realistic scenar-ios where task distribution is highly imbalanced with domain labels unavailable in nature. We propose a kernel-based method for domain change detection and a difficulty-aware memory management mechanism that jointly considers the imbalanced domain size and domain importance to learn across domains continuously. Furthermore, we introduce an efficient adaptive task sampling method during meta train-ing, which significantly reduces task gradient variance with theoretical guarantees. Finally, we propose a challenging benchmark with imbalanced domain sequences and varied domain difficulty. We have performed extensive evaluations on the proposed benchmark, demonstrating the effectiveness of our method. 1.

Introduction
Learning from a few labeled examples to acquire skills for new task is essential to achieve machine intelligence. Take object recognition in personalized self-driving system as an example [10]. Learning each user’s personal driving prefer-ence model forms one task. The system is first deployed in the small city, Rochester. The company later extends its mar-ket to New York. The user base of New York is much larger than that of Rochester, causing domain imbalance. Also, after adapting to New York users, the learned user behavior from Rochester will be easily forgotten. Similar scenarios occur when learning to solve NLP tasks on a sequence of dif-ferent languages [13] with imbalanced resources of different
Figure 1: Illustration of meta learning for few shot object recognition on a sequence of imbalanced domains. Our fo-cused problems including domain change detection, how to manage memory and sample memory tasks for joint training with streaming tasks. languages.
Meta learning is a promising approach for solving such few-shot learning problems. One common assumption of current models is that the task distribution is stationary dur-ing meta training. However, real world scenarios (such as the above self-driving system) are more complex and often in-volve learning across different domains (environments), with challenges such as: (1) task distributions change among dif-ferent domains; (2) tasks from previous domains are usually unavailable when training on a new domain; (3) the number of tasks from each domain could be highly imbalanced; (4) domain difficulty could vary significantly in nature across the domain sequence. An example is shown in Figure 1.
Directly applying current meta learning models to such sce-narios is not suitable to tackle these challenges, e.g., the object recognition accuracy of meta learned neural networks
generally deteriorates significantly on previous context after adapting to a new environment [23, 46, 63].
In this work, we cope with such challenges by consider-ing a more realistic problem setting that (1) learning on a sequence of domains; (2) task stream contains significant domain size imbalance; (3) domain labels and boundaries remain unavailable during both training and testing; (4) do-main difficulty is non-uniform across the domain sequence.
We term such problem setup as Meta Learning on a Sequence of Imbalanced Domains with Varying Difficulty (MLSID).
MLSID requires the meta learning model both adapting to a new domain and retaining the ability to recognize objects from previous domains. To tackle this challenging problem, we adopt replay-based approaches, i.e., a small number of tasks from previous domains are maintained in a memory buffer. Accordingly, there are two main problems that need to be solved: (1) how to determine which task should be stored into the memory buffer and which to be moved out.
To address this problem, we propose an adaptive memory management mechanism based on the domain distribution and difficulty, so that the tasks in memory buffer could maxi-mize the retained knowledge of previous domains; (2) how to determine which tasks to sample from memory during meta training. We propose an efficient adaptive task sampling approach to accelerate meta training and reduce gradient estimation variance according to our derived optimal task sampling distribution. Our intuition is that not all tasks are equally important for joint training at different iterations. It is thus desirable to dynamically determine which tasks to sample and to be jointly trained with current tasks to mitigate catastrophic forgetting at each training iteration.
Our contributions are summarized as following:
• To our best knowledge, this is the first work of meta learning on a sequence of imbalanced domains. For convenient evaluation of different models, we propose a new challenging benchmark consisting of imbalanced domain sequences.
• We propose a novel mechanism, “Memory Manage-ment with Domain Distribution and Difficulty Aware-ness”, to maximize the retained knowledge of previous domains in the memory buffer.
• We propose an efficient adaptive task sampling method during meta training, which significantly reduces gra-dient estimation variance with theoretical guarantees, making the meta training process more stable and boost-ing the model performance.
• Our method is orthogonal to specific meta learning methods and can be integrated with them seamlessly.
Extensive experiments with gradient-based and metric-based meta learning methods on the proposed bench-mark demonstrate the effectiveness of our method. 2. Problem Setting t consists of K examples, {(xk, yk)}K
A series of mini-batch training tasks T1, T2, . . . , TN ar-rive sequentially, with possible domain shift occurring in the stream, i.e., the task stream can be segmented by continual la-tent domains, D1, D2, . . . , DL. Tt denotes the mini-batch of tasks arrived at time t. The domain identity associated with each task remains unavailable during both meta training and testing. Domain boundaries, i.e., indicating current domain has finished and the next domain is about to start, are un-known. This is a more practical and general setup. Each task
T is divided into training and testing data {T train, T test}.
Suppose T train k=1, where in object recognition, xk is the image data and yk is the corresponding object label. We assume the agent stays within each domain for some consecutive time. Also, we consider a simplified setting where the agent will not re-turn back to previous domains and put the contrary case into future work. Our proposed learning system maintains a memory buffer M to store a small number of training tasks from previous domains for replay to avoid forgetting of pre-vious knowledge. Old tasks are not revisited during training unless they are stored in the memory M. The total num-ber of tasks processed is much larger than memory capacity.
At the end of meta training, we randomly sample a large number of unseen few-shot tasks from each latent domain,
D1, D2, . . . , DL for meta testing. The model performance is the average accuracy on all the sampled tasks. 3. Methodology 3.1. Conventional Reservoir Sampling and Its Lim-itations
Reservoir sampling (RS) [57, 15] is a random sampling method for choosing k samples from a data stream in a single pass without knowing the actual value of total number of items in advance. Straightforward adoption of RS here is to maintain a fixed memory and uniformly sample tasks from the task stream. Each task in the stream is assigned equal probability n
N of being moved into the memory buffer, where n is the memory capacity size and N is the total number of tasks seen so far. However, it is not suitable for the practical scenarios previously described, with two major shortcomings: (1) the task distribution in memory can be skewed when the input task stream is highly imbalanced in our setting. This leads to under-representation of the minority domains; (2) the importance of each task varies as some domains are more difficult to learn than others. This factor is also not taken into account with RS.
To address the above issues, we propose to first detect domain change in the input task stream to associate each task with a latent domain label. We then present a new mecha-nism, called Memory Management with Domain Distribution and Difficulty Awareness by utilizing the associated latent
the word embedding of the image categories with image embedding. We leave this direction as interesting future work. To reduce the variance across different few shot tasks and capture the general domain information, we compute the exponential moving average of task embedding Ot as
Ot = αot + (1 − α)Ot−1, where the constant α is the weighting multiplier which encodes the relative importance between current task embedding and past moving average.
A sliding window stores the past m (m is a small number) steps moving average, Ot−1, Ot−2, · · · , Ot−m, which are used to form the low dimensional projection vector zt, where the i-th dimensional element of zt is the distance between ot and Ot−i, d(ot, Ot−i). The projected m dimensional vector zt captures longer context similarity information spanning across multiple consecutive tasks.
Online domain change detection At each time t, we utilize the above constructed projected space for online domain change detection. Assume we have two win-dows of projected embedding of previous tasks U B =
{zt−2B, zt−2B+1, · · · , zt−B−1} with distribution Q and
V B = {zt−B, zt−B+1, · · · , zt} with distribution R, where
B is the window size. In other words, V B represents the most recent window of projection space (test window) and
U B represents the projection space of previous window (ref-erence window). U B and V B are non-overlapping windows.
For notation clarity and presentation convenience, we use another notation to denote the U B = {u1, u2, · · · , uB} and V B = {v1, v2, · · · , vB}, i.e., ui = zt−2B+i−1 and vi = zt−B+i−1. Our general framework is to first measure the distance between the two distributions Q and R, d(Q, R); then, by setting a threshold b, the domain change is detected when d(Q, R) > b. Here, we use Maximum Mean Discrep-ancy (MMD) to measure the distribution distance. Following
[37], the MMD distance between Q and R is defined as:
MMD[F, Q, R] := sup f ∈F
{Eu∼Q[f (u)]−Ev∼R[f (v)]} (1)
U-statistics [25] can be used for estimating MMD2:
W B t = MMD2[U B, V B] = and h(·) is defined as: 1
B(B − 1)
B (cid:88) i̸=j h(ui, uj, vi, vj) (2) h(ui, uj, vi, vj) = k(ui, uj)+k(vi, vj)−k(ui, vj)−k(uj, vi) (3) where k(·, ·) is RKHS kernel. In this paper, we assume
RBF kernel k(x, x′) = exp(−||x − x′||2/2σ2) is used.
The detection statistics at time t is W B t . If Q and R are is expected to be small, implying small proba-close, W B t (a) reservoir sampling (b) proposed memory management
Figure 2: An example of (a) reservoir sampling and (b) proposed memory management method jointly considering domain distribution and difficulty when meta learning on task stream from three latent domains. domain label with each task. For simple illustration, we con-struct an imbalanced input task stream from Miniimagenet,
Omniglot and Aircraft as shown in Figure 2. Evidently, the resulting distribution of stored tasks with RS is highly imbalanced and dramatically influenced by the input task stream distribution. In contrast, our memory management mechanism balances the three domain proportions by jointly considering domain distribution and difficulty.
Model Summary: We first illustrate on our domain change detection component in Section 3.2, which is used for (1) managing and balancing tasks in the memory buffer by incorporating the task difficulty (defined in Section 3.4) to determine whether the new incoming task should be moved into the memory and which old task should be moved out of memory in Section 3.3; (2) adaptive sampling tasks from memory buffer during meta training by dynamically adjust-ing the sampling probability of each task in the memory according to the task gradient for mitigating catastrophic forgetting in Section 3.4. 3.2. Online Domain Change Detection
Online domain change detection is a difficult problem due to: (1) few shot tasks are highly diverse within a single domain; (2) there are varying degree of variations at domain boundaries across the sequence. In our initial study, we found that it is inadequate to set a threshold on the change of mini-batch task loss value for detecting domain change.
We thus construct a low dimensional projected space and perform online domain change detection on this space.
Projected space Tasks Tt are mapped into a common k=1 fθt({xk}) where K is space ot = fθt(T train the number of training data and fθt is the CNN embed-ding network. The task embedding could be further re-fined by incorporating the image labels, e.g., concatenating
) = 1
K (cid:80)K t
bility of existence of domain change. If Q and R are signif-icantly different distributions, W B is expected to be large, t implying higher chance of domain shift. Thus, W B t char-acterizes the chance of domain shift at time t. We then test on the condition of W B t > b to determine whether domain change occurs, where b is a threshold. Each task Tt is asso-ciated with a latent domain label Lt, L0 = 0. If W B t > b,
Lt = Lt−1 + 1, i.e., a new domain arrives (Note that the actual domain changes could happen a few steps ago, but for simplicity, we could assume domain changes occur at time t); otherwise, Lt = Lt−1, i.e., the current domain continues.
We leave the more general case with domain revisiting as future work. How to set the threshold is a non-trivial task and is described in the following.
Setting the threshold Clearly, setting the threshold b in-volves a trade-off between two aspects: (1) the probability of
W B t > b when there is no domain change; (2) the probability of W B t > b when there is domain change. As a result, if the domain similarity and difficulty vary significantly, simply setting a fixed threshold across the entire training process is highly insufficient. In other words, adaptive threshold of b is necessary. Before we present the adaptive threshold method, we first show the theorem which characterizes the property of detection statistics W B t in the following.
Algorithm 1 Online Domain Change Detection (ODCD). from [50, 30]. We can observe that W B t asymptotically fol-lows a distribution formed by a weighted linear combination of independent normal distribution. By Lindeberg’s central limit theorem [55], it is reasonable to assume W B is approx-t imately Gaussian distribution. The problem is thus reduced to estimate its mean µt and σt. The adaptive threshold b, fol-lowing from [30], can be estimated by online approximation, b = µt + δσt, where δ is a constant and set to be the desired quantile of the normal distribution. This adaptive method for online domain change detection is shown in Algorithm 1. 3.3. Memory Management with Domain Distribu-tion and Difficulty Awareness
In this section, we design the memory management mech-anism for determining which task to be stored in the memory and which task to be moved out. The mechanism, named
Memory Management with Domain Distribution and Dif-ficulty Awareness (M2D3), jointly considers the difficulty and distribution of few shot tasks in our setting. M2D3 first estimates the probability of the current task Tt to be moved into the memory. The model will then determine the task to be moved out in the event that a new task move-in happens.
To improve efficiency, we utilize the obtained latent domain information associated with each task (as described in pre-vious section) to first estimate this move-out probability at cluster-level before sampling single task, as in Figure 3.
Require: stream of detection statistics W B quantile (significance level) δ; Initialize µ0 = 0 and µ(2) t ; constant ρ; desired 0 = 0 t , ρ, δ) 1: Function ODCD (W B 2: d = F alse; // indicator of domain shift 3: µt = (1 − ρ)µt−1 + ρ(W B 4: µ(2) t )2 t−1 + ρ(W B t )4 (cid:113) t = (1 − ρ)µ(2)
µ(2) t − µ2 t t > µt + δσt then d = T rue; //there is domain shift at time t 5: σt = 6: if W B 7: 8: end if 9: return d 10: EndFunction
Theorem 1 Assume zi are drawn i.i.d. pose that EQ||k(z, ·)||4 < ∞. Set µ
K(z, z′) value ξl and eigenvectors ϕ2
EQϕ2
⟨ϕl, ϕl′⟩ = 1l=l′. Then, from Q. Sup-def
= EQk(z, ·) and def
= ⟨k(z, ·) − µ, k(z′, ·) − µ⟩. Suppose the eigen-l of K satisfy ξl ≥ 0 and l≥1 ξlϕl(z)ϕl(z′) and l < ∞ such that K(z, z′) = (cid:80)
W B t d→ β
ξlZ 2 l (cid:88) l≥1 (4)
Where d→ means converge in distribution and (Zl)l≥1 is a collection of infinite independent standard normal random variables and β is a constant. The theorem and proof follow
Figure 3: Illustration on the memory management process.
Each colored circle represents one cluster in the buffer and each dot denotes one task.
Here we define the notations involved in the following method description. Each task Tt in the memory is associated with a latent domain label Lt and all the tasks with the same latent domain label form one cluster. Mi denotes the cluster formed by all the tasks with latent domain label i in memory
M, ni = |Mi| denotes the number of tasks in Mi and n = |M| denotes the total number of tasks in memory, and
Ii denotes the importance score of cluster Mi.
Probability of new task moving into memory When the new task Tt arrives, the chance of Tt being stored in memory is estimated, with the basic principle being the
more incremental knowledge is brought by Tt, the higher the probability of Tt being stored. This depends on the difficulty and prevalence of current latent domain. We propose an approach on top of this principle to estimate this probability.
The score function of Tt is defined as: (5)
)I T t
Where I T
Snew = (1 − nLt n t represents the importance for Tt , which is de-fined as the task-specific gradient norm in Section 3.4. nLt denotes the number of tasks of current latent domain cluster in memory buffer. nLt n denotes the prevalence of current latent domain in memory. Ii represents the importance for cluster Mi, which is defined as the cluster-specific gradient norm Gi in Section 3.4 (The computation is shared and cor-responding terms are computed only once.). The importance (cid:80)Lt−1 ni of in-memory tasks is defined as Ms = 1 n Ii. i=1
The score function of in-memory tasks is defined as:
Lt−1
Smem = nLt n
Ms
The probability of moving Tt into the memory is:
Pin = eSnew eSnew + eSmem (6) (7)
This task selection mechanism maximizes the incremental knowledge of each task added into memory.
Algorithm 2 Memory Management with Domain Distribu-tion and Difficulty Awareness (M2D3).
Require: mini-batch training tasks Tt; memory tasks M; domain label Lt−1 1: Function M2D3 (M, Tt) 2: calculate the probability Pin to move Tt into memory as Eq. 7. calculate detection statistics of W B t t , ρ, δ); detect domain change by Alg. 1.
Lt = Lt−1 + 1 3: d = ODCD(W B 4: if d then 5: 6: MLt = {} 7: end if 8: if memory M is not full then 9: MLt ← MLt ∪ Tt 10: else 11: 12: if Tt is moved into memory by Eq. 7 then calculate the move-out probability for each cluster P i t and sample cluster j according to Eq. 8 and 9. sample task from Mj to move out of memory. move Tt into memory MLt ← MLt ∪ Tt end if 13: 14: 15: 16: end if 17: return updated memory buffer M 18: EndFunction
Probability of existing tasks moving out of memory
To improve the efficiency of removing the tasks currently in memory, we perform a hierarchical sampling approach.
We perform sampling first at cluster level before focusing on individual tasks, as shown in Figure 3. The estimated probability is correlated with both the size of each cluster in memory and its importance. The factor for each cluster Mi is defined as:
Ai ∝ −(1 − ni n
)Ii (8)
The moving out probability for each cluster Mi at time t is then defined as
P i t = eAi (cid:80)i=Lt−1 i=1 eAi (9)
The complete mechanism is summarized in Algorithm 2. 3.4. Adaptive Memory Task Sampling for Training
During meta training, a mini-batch of tasks are sampled from the memory and are jointly trained with current tasks to mitigate catastrophic forgetting. Direct uniform sampling tasks from memory incurs high variance, and results in un-stable training [31, 9]. On the other hand, our intuition for non-uniform task sampling mechanism is that the tasks are not equally important for retaining the knowledge from previ-ous domains. The tasks that carry more information are more beneficial for the model to remember previous domains, and should be sampled more frequently. To achieve this goal, we propose an efficient adaptive task sampling scheme in mem-ory that accelerates training and reduces gradient estimation variance. As shown in Figure 4, the sampling probability of
Miniimagenet and Aircraft are adjusted and increased based on the scheme suggesting the importance of these domains are higher than that of Omniglot for retaining knowledge.
Figure 4: A simple example of uniform task sampling and our adaptive memory task sampling method for sampling tasks from memory buffer during meta training.
|θ, T train i
With the task specific loss function Lθ(Ti) =
P (T test
). The optimization objective at time t i is defined as minimizing on the loss function of both the new tasks and memory tasks H(θ) = Lθ(Tt) + (cid:80)
Ti∈M
Lθ(Ti).
At time t, our proposed adaptive sampling mechanism assigns each task Ti ∈ M a probability qt i such that (cid:80)i=n i=1 qt i = 1, we then sample Tit based on the distribution 1, qt qt = (qt n). We temporally omit the subscript (superscript) t for the following theorem for notation clarity. 2, · · · , qt
Theorem 2 Let p(T ) be the distribution of the tasks in mem-ory M. Then,
Ep(T )∇θLθ(T ) = Eq(T )[ p(T ) q(T )
∇θLθ(T )] = Ω (10)
Let Vq[Ω] denotes the covariance of the above estimator associated with q. Then, the trace of Vq[Ω] is minimized by the following optimal q∗ q∗(T ) = p(T )||∇θLθ(T )||2 (cid:82) p(T )||∇θLθ(T )||2
. (11)
In particular, if no prior information is available on task distribution, uniform sampling of tasks from memory is adopted and p(T ) = 1
. Thus, w(Ti) = p(Ti) q(Ti) = 1 n , q∗(Ti) = ||∇θ Lθ (Ti)||2 j=1 ||∇θ Lθ (Tj )||2 n q(Ti) (cid:80)n
Proof is provided in Appendix C. The parameters are updated as:
θt+1 = θt − ηwt (12)
Where η is the learning rate, wt
. Similar to standard SGD analysis [24], we define the convergence speed of meta training as the shrinkage of distance to op-timal parameters θ∗ between two consecutive iterations
C = −Eqt[||θt+1 − θ∗||2 2]. Following [29, 1], it can be expressed as: 2 − ||θt − θ∗||2 i∇θtLθ(Tit) i = 1 nqt i
C = 2η(θt − θ∗)Ω − η2ΩT Ω − η2Tr(Vqt[Ω]) (13)
Theorem 2 illustrates the optimal task sampling distribu-tion for reducing the gradient variance is proportional to the per-task gradient norm. Minimizing the gradient variance (last term of RHS in Eq.13) as in Theorem 2 also speeds up the convergence (maximize C) as a byproduct. However, it is computationally prohibitive to compute this distribution.
We therefore propose efficient approximation to it.
By Section 3.2, each memory task is associated with a latent cluster label. Utilizing this property, we can first sample R (small) tasks from each cluster, then calculate the gradient norm for each cluster as Gi.
By doing so, the computational efficiency of the optimal task sampling distribution will be significantly improved.
The sampling probability for each cluster is calculated as:
The sampling scheme is to first sample cluster indexes from memory according to Eq. 14, then randomly sample tasks from the specified clusters. We name this task sampling scheme as adaPtive mEmory Task Sampling (PETS).
Eq. 14 illustrates that the original sampling distribution of each cluster (measured by the frequency of each cluster in the memory buffer) is weighted by the corresponding importance of each cluster measured by the gradient norm
Gi. In practice, the computational efficiency can be further improved by computing the sampling distribution every s steps with the same distribution during each time interval.
PETS is summarized in Algorithm 3.
Algorithm 3 Adaptive Memory Task Sampling (PETS).
Require: A sequence of mini-batch training tasks T1, T2, . . . , TN ; memory buffer M; model parameters θ; 1: for t = 1 to N do 2: 3: for each cluster Mj in M do sample mini-batch tasks from cluster Mj and calculate gradient norm Gj for Mj. 6: 4: 5: end for calculate the task sampling distribution from each cluster
Mj as in Eq. 14. sample tasks B from M according to the distribution Zt as in Eq. 14. update θ by meta training on Tt ∪ B 7: 8: Memory tasks update M = M2D3(M, Tt) 9: end for 4.