Abstract
Quantization is a widely used technique to compress and accelerate deep neural networks. However, conven-tional quantization methods use the same bit-width for all (or most of) the layers, which often suffer signiﬁcant ac-curacy degradation in the ultra-low precision regime and ignore the fact that emergent hardware accelerators be-gin to support mixed-precision computation. Consequently, we present a novel and principled framework to solve the mixed-precision quantization problem in this paper. Brieﬂy speaking, we ﬁrst formulate the mixed-precision quantiza-tion as a discrete constrained optimization problem. Then, to make the optimization tractable, we approximate the ob-jective function with second-order Taylor expansion and propose an efﬁcient approach to compute its Hessian ma-trix. Finally, based on the above simpliﬁcation, we show that the original problem can be reformulated as a Multiple-Choice Knapsack Problem (MCKP) and propose a greedy search algorithm to solve it efﬁciently. Compared with exist-ing mixed-precision quantization works, our method is de-rived in a principled way and much more computationally efﬁcient. Moreover, extensive experiments conducted on the
ImageNet dataset and various kinds of network architec-tures also demonstrate its superiority over existing uniform and mixed-precision quantization approaches. 1.

Introduction
In the past few years, Convolutional Neural Networks (CNNs) have been leading new state-of-the-art in almost every computer vision tasks, ranging from image classiﬁ-cation [18, 31, 14], segmentation [25, 4, 1], and object de-tection [28, 21, 23]. However, such performance boosts of-ten come at the cost of increased computational complex-∗Corresponding Author ity and storage overhead. In many real-time applications, storage consumption and latency are crucial, which on the other hand, have posed great challenges to the deployment of these networks. Under this circumstance, a variety of methods have been proposed, including low-rank decompo-sition [39, 9], knowledge distillation [15, 29], low-precision quantization [16, 3], ﬁlter pruning [20, 24], etc, to achieve
CNNs compression and acceleration.
Among these approaches, quantization becomes one of the most hardware-friendly one by approximating real-valued weights and activations with lower bit-width ﬁxed-point representations. Meanwhile, network inference can be performed using cheaper ﬁxed-point multiple-accumulation (MAC) operations. As a result, we can signiﬁcantly reduce the storage overhead and inference latency of CNNs.
Most of the existing quantization methods [3, 41, 38, 7, 22, 19, 27, 42, 40] use the same bit-width for all (or most of) the layers. Such a uniform bit-width assignment can be suboptimal from two aspects. First, different layers have different redundancy and contribute differently to the
ﬁnal performance. Therefore, uniformly quantizing a net-work to ultra-low precision often leads to signiﬁcant accu-racy degradation. Second, emergent hardware accelerators, such as BISMO [34] and BitFusion [30], begin to support mixed-precision computation for greater ﬂexibility. Conse-quently, to achieve a better trade-off between accuracy and efﬁciency, there is a rising demand to apply mixed-precision quantization by ﬁnding the optimal bit-width for each layer.
However, mixed-precision quantization is difﬁcult for two reasons. First, the search space of choosing bit-width assignment is huge. For a network with N layers and M candidate bit-widths in each layer, an exhaustive combina-torial search has exponential time complexity (O(M N )).
Second, to evaluate the performance of each bit-width as-signment truly, we need to ﬁnetune the quantized network until it converges, which may take days for the large-scale dataset. Therefore, a large bulk of mixed-precision quanti-zation methods [35, 11, 10, 36, 26, 33] have been proposed recently to solve the problem approximately. Based on different approximation strategies, we can categorize these methods roughly into two groups as discussed below.
Search-Based: To reduce the computation complexity, search-based methods aim to sample more efﬁciently and obtain enough performance improvement with only a small number of evaluations. Therefore, HAQ [35] leverages re-inforcement learning to determine the quantization policy layer-wise and take the hardware accelerator’s feedback in the design. After that, AutoQ [26] proposes a hierarchical-DRL-based technique to search for the bit-width kernel-wise. Furthermore, EvoQ [37] alters to employ the evolu-tionary algorithm with limited data. Generally speaking, as the time cost of performance evaluation is still huge, search-based methods limit the exploration of search space greatly to make the algorithms computationally feasible.
Criterion-Based: Differently, criterion-based methods instead aim to reduce the time cost of performance eval-uation through kinds of criteria that are easy to compute.
Among them, HAWQ [11] utilizes the top Hessian eigen-value as the measure of quantization sensitivity of each layer. Although provided with relative sensitivity, it still requires a manual selection of the bit-width assignment. To solve the problem, HAWQ-V2 [10] proposes a Pareto fron-tier based method to ﬁnish it automatically and alters to take the trace of Hessian matrix as the criterion. Although effec-tive in practice, most existing criteria are still ad-hoc and lack of principled explanation for the optimality.
Overall, mixed-precision quantization remains an open problem so far given its intrinsic difﬁculty. In this paper, we present a novel and principled framework to solve it.
Speciﬁcally, we ﬁrst formulate mixed-precision quantiza-tion as a discrete constrained optimization problem with re-gard to the bit-width assignment among layers, which pro-vides a principled and holistic view for our further analysis.
As it is intractable to calculate the original objective func-tion, we then approximate it with Taylor expansion and pro-pose an efﬁcient approach to compute the Hessian matrix of each layer. Finally, based on the above simpliﬁcation, we show that the original problem can be reformulated as a special variant of the Knapsack problem called Multiple-Choice Knapsack Problem (MCKP) and propose a greedy search algorithm to solve it efﬁciently.
Compared with existing works, our method is ﬁrst computationally efﬁcient and even signiﬁcantly faster than criterion-based approaches. Take ResNet50 as an example, it only takes less than 2 minutes to ﬁnish the whole bit-width assignment procedure with a single RTX 2080Ti. Please refer to the Efﬁciency Analysis in Section 4.1 for details.
Second, as our method is derived in a principled way, it is more interpretable compared with other ad-hoc ones and also accessible for further improvement such as a more so-phisticated solving algorithm. Third, our method achieves a better trade-off between search-based and criterion-based methods. Compared with search-based ones (e.g. HAQ), our method reduces the evaluation cost greatly to search for the optimal bit-width assignment in a much larger space.
Compared with criterion-based ones (e.g. HAWQ), our method is based on the whole Hessian matrix instead of the eigenvalues only. Empirically, extensive experiments con-ducted on the ImageNet dataset and various kinds of net-works justify the superiorities of our method over them.
To summarize, our main contributions are three-fold:
• We ﬁrst formulate the mixed-precision quantization as a discrete constrained optimization problem to provide a principled and holistic view for further analysis.
• To solve the optimization, we propose an efﬁcient ap-proach to compute the Hessian matrix and then re-formulate it as Multiple-Choice Knapsack Problem (MCKP) to be solved by greedy search efﬁciently
• Extensive experiments are conducted to demonstrate the efﬁciency and effectiveness of our method over other uniform/mixed-precision quantization ones. 2.