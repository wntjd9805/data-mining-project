Abstract
Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classi-ﬁcation, object detection, and semantic segmentation. How-ever, in the ﬁeld of human pose estimation, convolutional ar-chitectures still remain dominant. In this work, we present
PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional ar-chitectures involved.
Inspired by recent developments in vision transformers, we design a spatial-temporal trans-former structure to comprehensively model the human joint relations within each frame as well as the temporal corre-lations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard bench-mark datasets: Human3.6M and MPI-INF-3DHP. Exten-sive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at https://github.com/zczcwh/PoseFormer 1.

Introduction
Human pose estimation (HPE) aims to localize joints and build a body representation (e.g. skeleton position) from in-put data such as images and videos. HPE provides geo-metric and motion information of the human body and can be applied to a wide range of applications (e.g. human-computer interaction, motion analysis, healthcare). Cur-rent works generally can be divided into two classes: (1) direct estimation approaches, and (2) 2D-to-3D lifting ap-proaches. Direct estimation methods [31, 29] infer a 3D human pose from 2D images or video frames without inter-mediately estimating the 2D pose representation. 2D-to-3D lifting approaches [25, 5, 43, 38] infer 3D human pose from an intermediately estimated 2D pose. Beneﬁting from the excellent performance of state-of-the-art 2D pose detectors, 2D-to-3D lifting approaches generally outperform direct es-timation methods. However, the mapping of these 2D poses to 3D is non-trivial; various potential 3D poses could be generated from the same 2D pose due to depth ambiguity and occlusion. To alleviate some of these issues and pre-serve natural coherence, many recent works have integrated temporal information from videos into their approaches.
For example, [25, 5] utilize temporal convolutional neural networks (CNNs) to capture global dependencies from adja-cent frames, and [33] uses recurrent architectures to similar effect. However, the temporal correlation window is lim-ited for both of these architectures. CNN-based approaches typically rely on dilation techniques, which inherently have limited temporal connectivity, and recurrent networks are mainly constrained to simply sequential correlation.
Recently, the transformer [37] has become the de facto model for natural language processing (NLP) due to its efﬁ-ciency, scalability and strong modeling capabilities. Thanks to the self-attention mechanism of the transformer, global correlations across long input sequences can be distinctly captured. This makes it a particularly ﬁtting architecture for sequence data problems, and therefore naturally ex-tendable to 3D HPE. With its comprehensive connectiv-ity and expression, the transformer provides an opportunity to learn stronger temporal representations across frames.
However, recent works [12, 36] show that transformers re-quire speciﬁc designs to achieve comparable performance with CNN counterparts for vision tasks. Speciﬁcally, they often require either extremely large scale training datasets
[12], or enhanced data augmentation and regularization [36] if applied to smaller datasets. Moreover, existing vision transformers have been limited primarily to image classi-ﬁcation [12, 36], object detection [4, 50], and segmenta-tion [41, 47], but how to harness the power of transformers for 3D HPE remains unclear.
To begin answering this question, we ﬁrst directly ap-ply the transformer on 2D-to-3D lifting HPE. In this case, we view the entire 2D pose for each frame in a given se-quence as a token (Fig. 1(a)). While this baseline approach is functional to an extent, it ignores the natural distinction of
cal relationships between human body joints, and the tem-poral transformer module captures the global dependen-cies across frames in the entire sequence.
• Without bells and whistles, our PoseFormer model achieves state-of-the-art results on both Human3.6M and
MPI-INF-3DHP datasets. 2.