Abstract
Remarkable results have been achieved by DCNN based self-supervised depth estimation approaches. However, most of these approaches can only handle either day-time or night-time images, while their performance degrades for all-day images due to large domain shift and the varia-tion of illumination between day and night images. To relieve these limitations, we propose a domain-separated network for self-supervised depth estimation of all-day im-ages. Speciﬁcally, to relieve the negative inﬂuence of dis-turbing terms (illumination, etc.), we partition the infor-mation of day and night image pairs into two complemen-tary sub-spaces: private and invariant domains, where the former contains the unique information (illumination, etc.) of day and night images and the latter contains essential shared information (texture, etc.). Meanwhile, to guaran-tee that the day and night images contain the same infor-mation, the domain-separated network takes the day-time images and corresponding night-time images (generated by
GAN) as input, and the private and invariant feature ex-tractors are learned by orthogonality and similarity loss, where the domain gap can be alleviated, thus better depth maps can be expected. Meanwhile, the reconstruction and photometric losses are utilized to estimate complementary information and depth maps effectively. Experimental re-sults demonstrate that our approach achieves state-of-the-art depth estimation results for all-day images on the chal-lenging Oxford RobotCar dataset, proving the superiority of our proposed approach. Code and data split are available at https://github.com/LINA-lln/ADDS-DepthNet. 1.

Introduction
Self-supervised depth estimation has been applied in a wide range of ﬁelds such as augmented reality [3][5], 3D reconstruction [17], SLAM [22][30][31] and scene under-*Corresponding authors
Figure 1. Comparison with other approaches on Oxford Robot-Im-Car dataset (d) Mon-(b) Monodepth2 [12], ages, odepth2+CycleGAN [44], (e) Ours. (c) HR-Depth [27],
From left (a) Night to right:
[28]. standing [1][6] since it does not need large and accurate ground-truth depth labels as supervision. The depth infor-mation can be estimated by the implicit supervision pro-vided by the spatial and temporal consistency present in image sequences. Beneﬁting from the well developed deep learning technology, impressive results have been achieved by Deep Convolution Neural Network(DCNN) based ap-proaches [14][21][24][25], which outperform traditional methods that rely on handcrafted features and exploit cam-era geometry and/or camera motion for depth and pose es-timation.
However, most of current DCNN based self-supervised depth estimation approaches [18][36][37][42] mainly solve the problem of depth estimation on day-time images, which are evaluated by day-time benchmarks, such as KITTI [9] and Cityscapes [7]. They fail to generalize well on all-day images due to the large domain shift between day and night images. The night-time images are unstable due to the low visibility and non-uniform illumination arising from mul-tiple and moving lights. Methods [16][19] are proposed by applying a commonly used depth estimation strategy for images captured in low-light conditions. However, the per-formance is limited due to the unstable visibility. Mean-while, generative adversarial networks(GAN), such as Cy-cleGAN [44], are also used to solve the problem of depth es-timation on night-time images by translating information of night-time to day-time in both image levels and feature lev-els. Unfortunately, due to the inherent domain shift between day and night-time images, it is difﬁcult to obtain natural day-time images or features with GAN using night-time im-ages as input, thus the performance is also limited. Fig. 1 (b) and (c) demonstrate the results of Monodepth2 [12] and
HR-Depth [27] of night-time images. Monodepth2[12] is an effective self-supervised depth estimation approach, and
HR-Depth[27] make a series of improvements based on
Monodepth2[12]. Fig. 1 (d) demonstrate the result Mon-odepth2 [44] with CycleGAN translated image as input. We can see that the depth details are failed to be estimated due to the non-uniform illumination of night-time images.
For a scene in real-world, the depth is constant if the viewpoint is ﬁxed, while the disturbing terms, such as il-lumination, varies as time goes, which will disturb the per-formance of self-supervised depth estimation, especially for night-time images. [8] also proves that texture information plays more important roles on depth estimation than exact color information. To cater to the above issues, we propose a domain-separated network for self-supervised depth es-timation of all-day RGB images. The information of day and night image pairs are separated into two complemen-tary sub-spaces: private and invariant domains. Both do-mains use DCNN to extract features. The private domain contains the unique information (illumination, etc.) of day and night-time images, which will disturb the performance of depth estimation. In contrast, the invariant domain con-tains invariant information (texture, etc.), which can be used for common depth estimation. Thus the disturbed informa-tion can be removed and better depth maps will be obtained.
Meanwhile, unpaired day and night images always con-tain inconsistent information, which interferes with the sep-aration of private and invariant features. Therefore, the domain-separated network takes a paired of the day-time image and corresponding night-time image (generated by
GAN) as input, the private and invariant feature extractors are ﬁrst utilized to extract private (illumination, etc.) and in-variant (texture, etc.) features using orthogonality and sim-ilarity losses, which can obtain more effective features for depth estimation of both day and night-time images. Be-sides, constraints in feature and gram matrices levels are leveraged in orthogonality losses to alleviate the domain gap, thus more effective features and ﬁne-grain depth maps can be obtained. Then, depth maps and corresponding RGB images are reconstructed by decoder modules with recon-struction and photometric losses. Note that real-world day-time and night-time images can be tested directly. As shown in Fig. 1 (e), our approach can effectively relieve the prob-lems of low-visibility and non-uniform illumination, and achieves more appealing results for night-time images.
The main contributions can be summarized as:
• We propose a domain-separated framework for self-supervised depth estimation of all-day images. It can relieve the inﬂuence of disturbing terms in depth esti-mation by separating the all-day information into two complementary sub-spaces: private (illumination, etc.) and invariant (texture, etc.) domains, thus better depth maps can be expected;
• Private and invariant feature extractors with orthogo-nality and similarity losses are utilized to extract ef-fective and complementary features to estimate depth information. Meanwhile, the reconstruction loss is em-ployed to reﬁne the obtained complementary informa-tion (private and invariant information);
• Experimental results on the Oxford RobotCar dataset demonstrate that our framework achieves state-of-the-art depth estimation performance for all-day images, which conﬁrms the superiority of our approach. 2.