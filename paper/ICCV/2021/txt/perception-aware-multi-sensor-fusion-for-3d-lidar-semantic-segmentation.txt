Abstract 3D LiDAR (light detection and ranging) semantic seg-mentation is important in scene understanding for many applications, such as auto-driving and robotics. For ex-ample, for autonomous cars equipped with RGB cameras and LiDAR, it is crucial to fuse complementary information from different sensors for robust and accurate segmentation.
Existing fusion-based methods, however, may not achieve promising performance due to the vast difference between the two modalities. In this work, we investigate a collabo-rative fusion scheme called perception-aware multi-sensor fusion (PMF) to exploit perceptual information from two modalities, namely, appearance information from RGB im-ages and spatio-depth information from point clouds. To this end, we first project point clouds to the camera coordi-nates to provide spatio-depth information for RGB images.
Then, we propose a two-stream network to extract features from the two modalities, separately, and fuse the features by effective residual-based fusion modules. Moreover, we propose additional perception-aware losses to measure the perceptual difference between the two modalities. Extensive experiments on two benchmark data sets show the superi-ority of our method. For example, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8% in mIoU. 1.

Introduction
Semantic scene understanding is a fundamental task for many applications, such as auto-driving and robotics [16, 34, 47, 48]. Specifically, in the scenes of auto-driving, it pro-vides fine-grained environmental information for high-level motion planning and improves the safety of autonomous cars [3, 18]. One of the important tasks in semantic scene understanding is semantic segmentation, which assigns a class label to each data point in the input data, and helps autonomous cars to better understand the environment.
According to the sensors used by semantic segmenta-Figure 1. Comparisons of spherical projection [40, 54] and perspec-tive projection. With spherical projection, most of the appearance information from RGB images is lost. Instead, we preserve the information of images with perspective projection. To distinguish different classes, we colorize the point clouds using semantic labels from SemanticKITTI. tion methods, recent studies can be divided into three cat-egories: camera-only methods [2, 9, 10, 35, 58], LiDAR-only methods [1, 13, 26, 54, 62] and multi-sensor fusion methods [30, 37, 39, 52, 59]. Camera-only methods have achieved great progress with the help of a massive amount of open-access data sets [6, 12, 14]. Since images obtained by a camera are rich in appearance information (e.g., texture and color), camera-only methods can provide fine-grained and accurate semantic segmentation results. However, as passive sensors, cameras are susceptible to changes in light-ing conditions and are thus unreliable [50].1 To address this problem, researchers conduct semantic segmentation on point clouds from LiDAR. Compared with camera-only ap-proaches, LiDAR-only methods are more robust to different light conditions, as LiDAR provides reliable and accurate spatio-depth information on the physical world. Unfortu-nately, LiDAR-only semantic segmentation is challenging due to the sparse and irregular distribution of point clouds.
â€ Corresponding authors. 1See Section 4.5 for more details.
the LiDAR stream by effective residual-based fusion (RF) modules, which are designed to learn the complementary features of the original LiDAR modules. Third, we propose perception-aware losses to measure the vast perceptual dif-ference between the two data modalities and boost the fusion of different perceptual information. Specifically, as shown in Figure 2, the perceptual features captured by the camera stream and LiDAR stream are different. Therefore, we use the predictions with higher confidence to supervise those with lower confidence.
Our contributions are summarized as follows. First, we propose a perception-aware multi-sensor fusion (PMF) scheme to effectively fuse the perceptual information from
RGB images and point clouds. Second, by fusing the spatio-depth information from point clouds and appearance in-formation from RGB images, PMF is able to address seg-mentation with undesired light conditions and sparse point clouds. More critically, PMF is robust to adversarial samples of RGB images by integrating the information from point clouds. Third, we introduce perception-aware losses into the network and force the network to capture the perceptual information from two different-modality sensors. The exten-sive experiments on two benchmark data sets demonstrate the superior performance of our method. For example, on nuScenes [7], PMF outperforms Cylinder3D [64], a state-of-the-art LiDAR-only method, by 0.8% in mIoU. 2.