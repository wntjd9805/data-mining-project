Abstract
The natural association between visual observations and their corresponding sound provides powerful self-supervisory signals for learning video representations, which makes the ever-growing amount of online videos an attractive source of training data. However, large portions of online videos contain irrelevant audio-visual signals because of edited/overdubbed audio, and models trained on such uncurated videos have shown to learn subopti-mal representations. Therefore, existing self-supervised ap-proaches rely on datasets with predetermined taxonomies of semantic concepts, where there is a high chance of audio-visual correspondence. Unfortunately, constructing such datasets require labor intensive manual annotation and/or verification, which severely limits the utility of online videos for large-scale learning. In this work, we present an au-tomatic dataset curation approach based on subset opti-mization where the objective is to maximize the mutual in-formation between audio and visual channels in videos.
We demonstrate that our approach finds videos with high audio-visual correspondence and show that self-supervised models trained on our data achieve competitive perfor-mances compared to models trained on existing manually curated datasets. The most significant benefit of our ap-proach is scalability: We release ACAV100M that contains 100 million videos with high audio-visual correspondence, ideal for self-supervised video representation learning. 1.

Introduction
Our long-term objective is learning to recognize objects, actions, and sound in videos without the need for manual ground-truth labels. This is not only a theoretically in-teresting problem, since it mimics the development of au-∗Equal Contribution
Figure 1. We address the challenge of constructing a large-scale audio-visual dataset from uncurated Internet videos without rely-ing on manual annotation or verification. We solve a constrained optimization problem that finds a subset maximizing the mutual information between audio and visual signals in videos. The result is a new 100M video dataset with high audio-visual correspon-dence, ideal for self-supervised video representation learning. ditory and visual perception by infants [21], it is also of immense practical importance, since accurate manual la-beling of audio-visual data is impractical. Compared to self-supervised learning on static images [50, 28, 25, 12], audio-visual inputs pose additional challenges: large por-tions of a video may contain no relevant information, and auditory and visual inputs may not always be in correspon-dence. Consequently, existing self-supervised methods on audio-visual data either start with datasets for which there is a high probability of audio-visual correspondence, or they learn audio-visual properties corresponding only to short-term statistical regularities. The necessary datasets are usu-ally manually created or rely on domain-specific properties (e.g., [9, 20] and below).
If we want to carry out self-supervised learning on full length (minutes, hours) of video without manually generating and/or selecting video clips, we need automated ways of curating such collections of au-dio/video clips from diverse collections of full length video.
We consider self-supervised learning from unlabeled
videos as a two-step process: (1) an automatic dataset cura-tion process that generates short, relevant clips with useful self-supervisory signals, e.g., audio-visual correspondence, and (2) a self-supervised learning approach that operates on the collection of short clips. This paper focuses on step (1) and not on step (2), providing an automated way of taking a collection of general or domain-specific videos of arbitrary length and reducing it to a collection of shorter clips con-taining a high portion of relevant audio-video correspon-dences. The output of this step is a dataset, which can be used as input to existing self-supervised algorithms on audio-visual data [34, 3, 54], as well as the development of novel self-supervised techniques.
To achieve step (1), we assume access to a large collec-tion of unconstrained videos and solve a subset selection problem with an information-theoretic measure of audio-visual correspondence as a selection criterion. Specifically, we find a subset that maximizes mutual information (MI) between audio and visual channels of videos. This is a necessary condition for self-supervised learning approaches that rely on audio-visual correspondence [17]. The main technical challenge we address is how to efficiently measure the audio-visual MI and find a subset that maximizes the MI in a scalable manner. Given that video processing is noto-riously compute and storage intensive, we put a particular emphasis on scalability, i.e., we want an approach that can easily handle hundreds of millions of video clips.
MI estimation has a long history of research [53, 35], including the recent self-supervised approaches [50, 28, 12] that use noise contrastive estimation [23] as the learning ob-jective. While it is tempting to use such approaches to es-timate MI in our work, we quickly encounter the “chicken-and-egg” problem: to obtain such models for estimating audio-visual MI, we need a training dataset where we can reliably construct positive pairs with a high probability of audio-visual correspondence; but that is what we are set out to find in the first place! One might think that randomly cho-sen videos from the Internet could be sufficient, but this has shown to produce suboptimal representations [3]; our em-pirical results also show that self-supervised models indeed suffer from noisy real-world audio-visual correspondences.
In this work, we turn to a clustering-based solution that estimates the MI by measuring the agreement between two partitions of data [42, 67]. To circumvent the “chicken-and-egg” issue, we use off-the-shelf models as feature extractors and obtain multiple audio and visual clusters to estimate the
MI. The use of off-the-shelf models is a standard practice in video dataset generation. Unlike existing approaches that use them as concept classifiers [8, 1, 43, 47, 11], here we use them as generic feature extractors. To avoid estimating the MI based on a restricted set of concepts the off-the-shelf models are trained on, we perform clustering over features computed across multiple layers (instead of just the penul-timate layers), which has been shown to provide general feature descriptors not tied to specific concepts [76].
To make our approach scalable, we avoid using memory-heavy components such as the Lloyd’s algorithm [52] and instead use SGD [7] to perform K-means clustering. Fur-ther, we approximately solve the subset maximization ob-jective with a mini-batch greedy method [13]. Through controlled experiments with ground-truth and noisy real-world correspondences, we show that our clustering-based approach is more robust to the real-world correspondence patterns, leading to superior empirical performances than the contrastive MI estimation approaches.
We demonstrate our approach on a large collection of videos at an unprecedented scale: We process 140 mil-lion full-length videos (total duration 1,030 years) and pro-duce a dataset of 100 million 10-second clips (31 years) with high audio-visual correspondence. We call this dataset
ACAV100M (short for automatically curated audio-visual dataset of 100M videos).
It is two orders of magni-tude larger than the current largest video dataset used in the audio-visual learning literature, i.e., AudioSet [20] (8 months), and twice as large as the largest video dataset in the literature, i.e., HowTo100M [44] (15 years).
To evaluate the utility of our approach in self-supervised audio-visual representation learning, we produce datasets at varying scales and compare them with existing datasets of similar sizes that are frequently used in the audio-visual learning literature, i.e., Kinetics-Sounds [4] at 20K-scale,
VGG-Sound [11] at 200K-scale, and AudioSet [20] at 2M-scale. Under the linear evaluation protocol with three down-stream datasets, UCF101 [62], ESC-50 [56], and Kinetics-Sounds [4], we demonstrate that models pretrained on our datasets perform competitively or better than the ones pre-trained on the baseline datasets, which were constructed with careful annotation or manual verification.
To summarize, our main contributions are: 1) We pro-pose an information-theoretic subset optimization approach to finding a large-scale video dataset with a high portion of relevant audio-visual correspondences. 2) We evalu-ate different components of our pipeline via controlled ex-periments using both the ground-truth and the noisy real-world correspondence patterns. 3) We release ACAV100M, a large-scale open-domain dataset of 100M videos for fu-ture research in audio-visual representation learning. 2.