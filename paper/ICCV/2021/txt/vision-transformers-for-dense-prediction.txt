Abstract
We introduce dense prediction transformers, an archi-tecture that leverages vision transformers in place of con-volutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vi-sion transformer into image-like representations at vari-ous resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a con-stant and relatively high resolution and has a global re-ceptive ﬁeld at every stage. These properties allow the dense prediction transformer to provide ﬁner-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense pre-diction tasks, especially when a large amount of train-ing data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmen-tation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be ﬁne-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT. 1.

Introduction
Virtually all existing architectures for dense prediction are based on convolutional networks [6, 33, 36, 44, 51, 52, 55]. The design of dense prediction architectures com-monly follows a pattern that logically separates the network into an encoder and a decoder. The encoder is frequently based on an image classiﬁcation network, also called the backbone, that is pretrained on a large corpus such as Im-ageNet [9]. The decoder aggregates features from the en-coder and converts them to the ﬁnal dense predictions. Ar-chitectural research on dense prediction frequently focuses on the decoder and its aggregation strategy [6, 7, 52, 55].
However, it is widely recognized that the choice of back-bone architecture has a large inﬂuence on the capabilities of the overall model, as any information that is lost in the encoder is impossible to recover in the decoder.
Convolutional backbones progressively downsample the input image to extract features at multiple scales. Down-sampling enables a progressive increase of the receptive
ﬁeld, the grouping of low-level features into abstract high-level features, and simultaneously ensures that memory and computational requirements of the network remain tractable. However, downsampling has distinct drawbacks that are particularly salient in dense prediction tasks: fea-ture resolution and granularity are lost in the deeper stages of the model and can thus be hard to recover in the decoder.
While feature resolution and granularity may not matter for some tasks, such as image classiﬁcation, they are critical for dense prediction, where the architecture should ideally be able to resolve features at or close to the resolution of the input image.
Various techniques to mitigate the loss of feature gran-ularity have been proposed. These include training at higher input resolution (if the computational budget per-mits), dilated convolutions [51] to rapidly increase the re-ceptive ﬁeld without downsampling, appropriately-placed skip connections from multiple stages of the encoder to the decoder [33], or, more recently, by connecting multi-resolution representations in parallel throughout the net-work [44]. While these techniques can signiﬁcantly im-prove prediction quality, the networks are still bottlenecked by their fundamental building block: the convolution. Con-volutions together with non-linearities form the fundamen-tal computational unit of image analysis networks. Convo-lutions, by deﬁnition, are linear operators that have a lim-ited receptive ﬁeld. The limited receptive ﬁeld and the lim-ited expressivity of an individual convolution necessitate se-quential stacking into very deep architectures to acquire suf-ﬁciently broad context and sufﬁciently high representational power. This, however, requires the production of many in-termediate representations that require a large amount of memory. Downsampling the intermediate representations
is necessary to keep memory consumption at levels that are feasible with existing computer architectures.
In this work, we introduce the dense prediction trans-former (DPT). DPT is a dense prediction architecture that is based on an encoder-decoder design that leverages a trans-former as the basic computational building block of the en-coder. Speciﬁcally, we use the recently proposed vision transformer (ViT) [11] as a backbone architecture. We re-assemble the bag-of-words representation that is provided by ViT into image-like feature representations at various resolutions and progressively combine the feature repre-sentations into the ﬁnal dense prediction using a convolu-tional decoder. Unlike fully-convolutional networks, the vi-sion transformer backbone foregoes explicit downsampling operations after an initial image embedding has been com-puted and maintains a representation with constant dimen-sionality throughout all processing stages.
It furthermore has a global receptive ﬁeld at every stage. We show that these properties are especially advantageous for dense pre-diction tasks as they naturally lead to ﬁne-grained and glob-ally coherent predictions.
We conduct experiments on monocular depth estimation and semantic segmentation. For the task of general-purpose monocular depth estimation [32], where large-scale train-ing data is available, DPT provides a performance increase of more than 28% when compared to the top-performing fully-convolutional network for this task. The architecture can also be ﬁne-tuned to small monocular depth prediction datasets, such as NYUv2 [37] and KITTI [15], where it also sets the new state of the art. We provide further evidence of the strong performance of DPT using experiments on se-mantics segmentation. For this task, DPT sets a new state of the art on the challenging ADE20K [56] and Pascal Context
[28] datasets. 2.