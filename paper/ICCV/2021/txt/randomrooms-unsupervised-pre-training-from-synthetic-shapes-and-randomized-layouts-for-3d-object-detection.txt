Abstract 3D point cloud understanding has made great progress in recent years. However, one major bottleneck is the scarcity of annotated real datasets, especially compared to 2D object detection tasks, since a large amount of labor is involved in annotating the real scans of a scene. A promis-ing solution to this problem is to make better use of the synthetic dataset, which consists of CAD object models, to boost the learning on real datasets. This can be achieved by the pre-training and fine-tuning procedure. However, re-cent work on 3D pre-training exhibits failure when trans-fer features learned on synthetic objects to other real-world applications. In this work, we put forward a new method called RandomRooms to accomplish this objective. In par-ticular, we propose to generate random layouts of a scene by making use of the objects in the synthetic CAD dataset and learn the 3D scene representation by applying object-level contrastive learning on two random scenes generated from the same set of synthetic objects. The model pre-trained in this way can serve as a better initialization when later fine-tuning on the 3D object detection task. Empirically, we show consistent improvement in downstream 3D detection tasks on several base models, especially when less train-ing data are used, which strongly demonstrates the effec-tiveness and generalization of our method. Benefiting from the rich semantic knowledge and diverse objects from syn-thetic data, our method establishes the new state-of-the-art on widely-used 3D detection benchmarks ScanNetV2 and
SUN RGB-D. We expect our attempt to provide a new per-spective for bridging object and scene-level 3D understand-ing. 1.

Introduction
Recent years have witnessed great progress in 3D deep learning, especially on 3D point clouds. With the emer-*Equal contribution. â€ Corresponding author.
Figure 1: The main idea of RandomRooms. To generate two different layouts, we randomly place the same set of objects sampled from synthetic datasets in rectangular rooms. With the proposed object-level contrastive learning, models pre-trained on these pseudo scenes can serve as a better initial-ization for downstream 3D object detection task. gence of powerful models, we are now able to make sig-nificant breakthroughs on many point cloud tasks, ranging from object-level understanding ones [24, 53, 27, 29] to scene-level understanding ones, such as 3D object detec-tion [45, 59, 28, 44] and 3D semantic segmentation [25, 62, 21, 4, 19]. These scene-level tasks are considered to be more complicated and more important as they often require higher level understanding compared to object level tasks like shape classification. One of the most important tasks for 3D point cloud scene understanding is the 3D object de-tection, which aims at localizing the objects of interest in the point cloud of the scene and telling the category they belong to. However, one major bottleneck that hinders the researchers from moving forward is the lack of large-scale real datasets, considering the difficulty in collecting and la-beling high-quality 3D scene data. Compared to 2D object detection task where we have large annotated real datasets
COCO [30], the real datasets here we use for 3D object de-tection task are much smaller in scales, and generating a 1
synthesized scene dataset also involves a heavy workload in modeling and rendering.
A preferred solution is to utilize synthetic CAD object models to help the learning of 3D object detector since it is much easier to access such type of data. Considering we have no annotation of bounding box for synthetic CAD data, this idea can be achieved in a similar way as the unsu-pervised pre-training for 2D vision tasks where we first pre-train on a large-scale dataset in an unsupervised manner and then fine-tune on a smaller annotated dataset. Yet, most pre-vious works focus on the pre-training for single object level tasks [31, 58, 6, 11, 40], such as reconstruction, shape clas-sification or part segmentation, or on some low-level tasks like registration [6, 61, 10]. A recent work [57], namely
PointContrast, first explores the possibility of pre-training in the context of 3D representation learning for higher level scene understanding tasks, i.e. 3D detection and segmenta-tion. Nevertheless, they conduct the pre-training on the real scene dataset and provide a failure case when pre-training the backbone model on ShapeNet [1], which consists of synthetic CAD object models. They attribute this unsuc-cessful attempt to two reasons, that is, the domain gap be-tween real and synthetic data as well as the insufficiency of capturing point-level representation by directly training on single objects. Despite these difficulties, it is still desirable to make the ShapeNet play the role of ImageNet in 2D vi-sion since it is easy to obtain a large number of synthetic
CAD models.
In this work, we put forward a new framework to show the possibility of using a synthetic CAD model dataset, i.e. ShapeNet, for the 3D pre-training before fine-tuning on downstream 3D object detection task. To this end, we propose a method named RandomRoom. In particular, we propose to generate two different layouts using one set of objects which are randomly sampled out of the ShapeNet dataset. Having these two scenes that are made up of the same set of objects, we can then perform the contrastive learning at the object level to learn the 3D scene represen-tation.
Different from PointContrast [57] where the contrastive learning is performed at the point level, our approach has two advantages. One is to remove the requirement of point correspondence between two views, which is indispensable in PointContrast framework given that it is necessary to ex-ploit such information to obtain positive and negative pairs for the contrastive learning. This requirement limits the ap-plications of PointContrast, since the CAD model datasets like ShapeNet and many other real-world datasets like SUN
RGB-D [47] cannot provide such information. The other advantage is that our method can support more diverse backbone models. Most state-of-the-art models [34, 35, 44] on tasks like 3D object detection apply PointNet++ [38] style models as their backbone, and replacing it with Sparse
Res-UNet may lead to the drop of accuracy, according to the
PointContrast. However, PointContrast cannot well support the pre-training of PointNet++ style model as the UNet-like models, since the point correspondence may be missing af-ter each abstraction level in PointNet++. With the proposed
RandomRoom, we are enabled to perform contrastive learn-ing at the level of objects and thus better support the pre-training of PointNet++ like models as we no longer need to keep the point correspondence for contrastive learning like
PointContrast.
Our method is straightforward yet effective. We con-duct the experiments on the 3D object detection task where only the geometric information is available for input as the models in CAD datasets do not carry color informa-tion. The results of empirical study strongly demonstrate the effectiveness of our method. In particular, we achieve the state-of-the-art of 3D object detection on two widely-used benchmarks, ScanNetV2 and SUN-RGBD. Further-more, our method can achieve even more improvements when much less training samples are used, demonstrating that our model can learn a better initialization for 3D object detection. 2.