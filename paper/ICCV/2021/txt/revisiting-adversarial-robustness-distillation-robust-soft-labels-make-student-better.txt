Abstract
Adversarial training is one effective approach for train-ing robust deep neural networks against adversarial at-tacks. While being able to bring reliable robustness, adver-sarial training (AT) methods in general favor high capac-ity models, i.e., the larger the model the better the robust-ness. This tends to limit their effectiveness on small models, which are more preferable in scenarios where storage or computing resources are very limited (e.g., mobile devices).
In this paper, we leverage the concept of knowledge distil-lation to improve the robustness of small models by distill-ing from adversarially trained large models. We ﬁrst revisit several state-of-the-art AT methods from a distillation per-spective and identify one common technique that can lead to improved robustness: the use of robust soft labels – pre-dictions of a robust model. Following this observation, we propose a novel adversarial robustness distillation method called Robust Soft Label Adversarial Distillation (RSLAD) to train robust small student models. RSLAD fully exploits the robust soft labels produced by a robust (adversarially-trained) large teacher model to guide the student’s learning on both natural and adversarial examples in all loss terms.
We empirically demonstrate the effectiveness of our RSLAD approach over existing adversarial training and distilla-tion methods in improving the robustness of small models against state-of-the-art attacks including the AutoAttack.
We also provide a set of understandings on our RSLAD and the importance of robust soft labels for adversarial robust-ness distillation. Code: https://github.com/zibojia/RSLAD. 1.

Introduction
Deep Neural Networks (DNNs) have become the stan-dard models for solving complex real-world learning prob-lems, such as image classiﬁcation [25, 19], speech recog-*Equal contribution: Bojia Zi(bjzi19@fudan.edu.cn) and Shihao
Zhao(shzhao19@fudan.edu.cn)
†Correspondence to Xingjun Ma (daniel.ma@deakin.edu.au) and Yu-Gang Jiang (ygj@fudan.edu.cn) nition [46] and natural language processing [45]. However, studies have shown that DNNs are vulnerable to adversar-ial attacks [43, 15], where imperceptible adversarial pertur-bations on the input can easily subvert the model’s predic-tion. This raises security concerns on the deployment of
DNNs in safety-critical scenarios such as autonomous driv-ing [13, 7, 11] and medical diagnosis [31].
Different types of methods have been proposed to defend
DNNs against adversarial attacks [22, 30, 32, 26, 57, 48], amongst which adversarial training (AT) has been found to be the most effective approach [2, 10]. AT can be regarded as a type of data augmentation technique that crafts adver-sarial versions of the natural examples for model training.
AT is normally formulated as a min-max optimization prob-lem with the inner maximization generates adversarial ex-amples while the outer minimization optimizes the model’s parameters on the adversarial examples generated during the inner maximization [32, 57, 47]. i.e.,
While being able to bring reliable robustness, AT meth-ods have several drawbacks that may limit their effective-ness in certain application scenarios. Arguably, the most notable drawback is its hunger for high capacity mod-els, the larger the model the better the robustness
[49, 44, 35, 16]. However, there are scenarios where small and lightweight models are more preferable than large mod-els. One example is the deployment of small DNNs in de-vices with limited memory and computational power such as smart phones and autonomous vehicles [37]. This has motivated the use of knowledge distillation along with AT to boost the robustness of small DNNs by distilling from robust large models [14, 3, 8, 62], a process known as Ad-versarial Robustness Distillation (ARD).
In this paper, we build upon previous works in both AT and ARD, and investigate the key element that can boost the robustness of small DNNs via distillation. We com-pare the loss functions adopted by several state-of-the-art
AT methods and identify one common technique behind the improved robustness: the use of predictions of an adversar-ially trained model. We denote this type of supervision as
Robust Soft Labels (RSLs). Compared to the original hard labels, RSLs can better represent the robust behaviors of the teacher model, providing more robust information to guide the student’s learning. This observation motivates us to de-sign a new ARD method to fully exploit the power of RSLs in boosting the robustness of small student models.
In summary, our main contributions are:
• We identify that the implicit distillation process exist-ing in adversarial training methods is a useful function for promoting robustness and the use of robust soft la-bels can lead to improved robustness.
• We propose a novel adversarial robustness distillation method called Robust Soft Label Adversarial Distilla-tion (RSLAD), which applies robust soft labels to re-place hard labels in all of its supervision loss terms.
• We empirically verify the effectiveness of RSLAD in improving the robustness of small DNNs against state-of-the-art attacks. We also provide a comprehensive understanding of our RSLAD and the importance of robust soft labels for robustness distillation. 2.