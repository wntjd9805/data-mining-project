Abstract
Frame sampling is a fundamental problem in video ac-tion recognition due to the essential redundancy in time and limited computation resources. The existing sampling strategy often employs a ﬁxed frame selection and lacks the ﬂexibility to deal with complex variations in videos.
In this paper, we present a simple, sparse, and explain-able frame sampler, termed as Motion-Guided Sampler (MGSampler). Our basic motivation is that motion is an important and universal signal that can drive us to adap-tively select frames from videos. Accordingly, we propose two important properties in our MGSampler design: mo-tion sensitive and motion uniform. First, we present two different motion representations to enable us to efﬁciently distinguish the motion-salient frames from the background.
Then, we devise a motion-uniform sampling strategy based on the cumulative motion distribution to ensure the sam-pled frames evenly cover all the important segments with high motion salience. Our MGSampler yields a new prin-cipled and holistic sampling scheme, that could be in-corporated into any existing video architecture. Experi-ments on ﬁve benchmarks demonstrate the effectiveness of our MGSampler over the previous ﬁxed sampling strate-gies, and its generalization power across different back-bones, video models, and datasets. The code is available at https://github.com/MCG-NJU/MGSampler. 1.

Introduction
Video understanding is becoming more and more impor-tant in computer vision research as huge numbers of videos are captured and uploaded online. Human action recogni-tion [31, 37, 40, 24] has witnessed great progress in the past few years by designing various deep convolutional networks in videos. The core effort has been devoted to obtaining compact yet effective video representations for efﬁcient and robust recognition. Compared with static images, the extra time dimension requires us to devise a sophisticated tem-(cid:66): Corresponding author. poral module equipped with high capacity and ﬁgure out an efﬁcient inference strategy for fast processing. However, in addition to these modeling and computational issues, a more fundamental problem in video understanding is sam-pling. Due to the essential redundancy in time and as well limited computational budget in practice, it is unnecessary and also infeasible to feed the whole video for subsequent processing. How to sample a small subset of frames is very important for developing a practical video recognition sys-tem, but it still remains to be an unsolved problem.
Currently, deep convolutional networks (CNNs) typi-cally employ a ﬁxed hand-crafted sampling strategy for training and testing in videos [31, 37, 40].
In the train-ing phase, CNN is trained on frames/clips which are ran-domly sampled either evenly or successively with a ﬁxed stride from the original video. In the test phase, in order to cover the full temporal duration of video, clips are densely sampled from video and the ﬁnal result is averaged from these dense prediction scores. There are multiple problems with these ﬁxed sampling strategies. First, the action in-stance varies with different videos and sampling should not be ﬁxed across videos. Second, not all the frames are of equal importance for classiﬁcation and sampling should pay more attention to discriminative frames rather than irrele-vant background frames.
Recently, some works [47, 45, 5] focus on frame selec-tion in untrimmed videos, and try to improve the inference efﬁciency with an adaptive sampling module. These meth-ods typically employ a learnable module to automatically select more discriminative frames for subsequent process-ing. However, these methods heavily rely on the train-ing data with complicated learning strategies, and can not easily transfer to unseen action classes in practice. In ad-dition, they typically deal with untrimmed video recogni-tion by selecting foreground frames and removing back-ground information. But it is unclear how to adapt them to trimmed video sampling due to the inherent difference between trimmed and untrimmed videos.
Based on the analysis above, how to devise a princi-pled and adaptive sampling strategy for trimmed videos still needs further consideration in research. In this paper, we
Figure 1. Sample eight frames from a video of throwing something in the air and catching it. Due to the quick moment in action, uniform sampling may miss the key information while our sampling strategy can identify and select frames with large motion magnitude. aim to present a simple, sparse and explainable sampling strategy for trimmed video action recognition, which is in-dependent of the training data for good generalization abil-ity and also capable of dealing with various video content adaptively. Our basic observation is that motion is a univer-sal and transferable signal that can guide us to sample dis-criminative frames, in the sense that action related frames should be of high motion salience to convey most infor-mation about human movement while background frames typically contain no or limited irrelevant motion informa-tion. According to this motion prior, we can roughly an-alyze the frame importance and group frames into several segments according to their temporal variations. Conse-quently, these temporal segments enable us to perform a holistic and adaptive sampling to capture most motion in-formation while suppressing the irrelevant background dis-traction, yielding a general frame sampler (MGSampler).
Speciﬁcally, to implement our motion-guided sampling, two critical components are proposed to handle motion es-timation and temporal sampling, respectively. For motion representation, we use temporal difference at different lev-els to approximate human movement information for efﬁ-ciency. In practice, temporal difference is highly correlated with motion information, and the absolute value of the dif-ference is able to reﬂect the motion magnitude to some ex-tent. For temporal sampling, based on the motion distri-bution along time, we present a uniform grouping strategy, where each segment should convey the same amount of mo-tion salience. Then, according to this uniform grouping, we can perform adaptive sampling over the entire video by ran-domly picking a representative frame from each segment.
Figure 1 exhibits a vivid example of sampling frames from a video of the class “throwing something in the air and catch-ing it”. The motion relevant content only contains a small portion of the whole video (e.g., from the 24th frame to the 30th frame). If we use traditional uniform sampling, the im-portant information between the 24th frame and 30th frame will be missed and as a result, the video is classiﬁed into holding a ball by mistake. In contrast, our motion-guided sampler selects more frames between the 24th frame and the 30th frame and makes a correct prediction.
We conduct extensive experiments on ﬁve different trimmed video datasets: Something-Something V1 &
V2 [10], UCF101 [32], Jester [27], HMDB51 [18], and Div-ing48 [23]. Signiﬁcant improvement is obtained on these datasets by adopting our motion-guided sampling strategy.
It is worth noting that using the motion-guided sampling strategy will not increase the burden of computation and running time greatly. In addition, the method is agnostic to the network architecture, and can be used in both training and test phases, demonstrating its strong applicability. 2.