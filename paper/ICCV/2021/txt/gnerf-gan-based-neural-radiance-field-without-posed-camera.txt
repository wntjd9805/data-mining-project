Abstract
We introduce GNeRF, a framework to marry Generative
Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with un-known and even randomly initialized camera poses. Recent
NeRF-based advances have gained popularity for remark-able realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initial-ization. Differently, our GNeRF only utilizes randomly ini-tialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for optimizing coarse camera poses and radiance fields jointly, while the second phase refines them with additional photometric loss.
We overcome local minima using a hybrid and iterative op-timization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated pat-terns or even low textures that are regarded as extremely challenging before. 1.

Introduction
Recovering 3D representations from multi-view 2D im-ages is one of the core tasks in computer vision. Recently, significant progress has been made with the emergence of neural radiance fields methods (e.g., NeRF [31]), which rep-resents a scene as a continuous 5D function and uses vol-ume rendering to synthesize new views. Although NeRF and its follow-ups [6, 26, 29, 53, 61] achieve an unprece-dented level of fidelity on a range of challenging scenes, most of these methods rely heavily on knowing the accurate
Figure 1. Our approach estimates both camera poses and neural radiance fields using only randomly initialized poses in complex scenarios, even in the extreme case when the input views are only texture-less gray masks. camera poses, which is yet a long-standing but challenging task. The conventional camera pose estimation process suf-fers in challenging scenes with repeated patterns, varying lighting, or few keypoints, and building on these methods adds additional uncertainty to the NeRF training process.
To explore the possibilities of alleviating the dependence on accurate camera pose information, recently, iNeRF [60] and NeRF−− [55] attempt to optimize camera pose along with other parameters when training NeRF. While certain progress has been made, both of them can only optimize camera poses when relatively short camera trajectories with reasonable camera pose initialization are available.
It is worth noting that, NeRF−− is limited to roughly forward-facing scenes, the focus of iNeRF is camera pose estimation but not radiance field estimation, and it assumes a trained
NeRF which in turn requires known camera poses as super-vision. When greater viewpoint uncertainty presents, cam-era poses estimation is extremely challenging and prone to falling into local minima.
To this end, we propose GNeRF, a novel algorithm that can estimate both camera poses and neural radiance fields when the cameras are initialized at random poses in com-plex scenarios. Our algorithm has two phases: the first phase gets coarse camera poses and radiance fields with adversarial training; the second phase refines them jointly with a photometric loss. Taking the use of Generative Ad-versarial Networks (GANs) into the realm of camera poses estimation, we extend the NeRF model to jointly optimize 3D representation and camera poses in complex scenes with large displacements.
Instead of directly propagating the photometric loss back to the camera pose parameters, which is sensitive to challenging conditions (e.g., less texture and varying lighting) and apt to fall into local minima, we pro-pose a hybrid and iterative optimization scheme. Our learn-ing pipeline is fully differentiable and end-to-end trainable, allowing our algorithm to perform well in the challenging scenes where COLMAP-based [44] methods suffer from challenges such as repeated patterns, low textures, noise, even in the extreme cases when the input views are a col-lection of gray masks, as shown in Fig. 1.
In addition, our method can predict new poses of images belonging to the same scene through the trained inversion network without tedious per-scene pose estimation (e.g., COLMAP-like methods) or time-consuming gradient-based optimiza-tion (e.g., iNeRF and NeRF−−). We experiment with our GNeRF on a variety of synthetic and natural scenes.
We demonstrate results on par with COLMAP-based NeRF methods in regular scenes; more impressively, our method outperforms the baselines in cases with less texture that are regarded as extremely challenging before. 2.