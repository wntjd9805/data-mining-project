Abstract
We introduce a novel self-supervised contrastive learn-ing method to learn representations from unlabelled videos.
Existing approaches ignore the specifics of input distor-tions, e.g., by learning invariance to temporal transforma-tions. Instead, we argue that video representation should preserve video dynamics and reflect temporal manipula-tions of the input. Therefore, we exploit novel constraints to build representations that are equivariant to temporal transformations and better capture video dynamics. In our method, relative temporal transformations between aug-mented clips of a video are encoded in a vector and con-trasted with other transformation vectors. To support tem-poral equivariance learning, we additionally propose the self-supervised classification of two clips of a video into 1. overlapping 2. ordered, or 3. unordered. Our experiments show that time-equivariant representations achieve state-of-the-art results in video retrieval and action recognition benchmarks on UCF101, HMDB51, and Diving48. 1.

Introduction
A general video representation should accurately cap-ture both scene appearance and dynamics to perform well on various video understanding tasks, e.g., action recogni-tion or video retrieval. While large annotated video datasets
[75, 1] advanced the state-of-the-art in video understanding, such annotations are costly. Furthermore, supervised learn-ing from sparse action labels can introduce biases towards appearance and neglect features related to motion when ac-tions are recognizable from static frames [42, 53]. We con-sider self-supervised learning (SSL) [18] as a possible so-lution, enabling both the training on abundant unlabelled videos and biasing learning towards video dynamics.
In SSL, learning tasks are created for which supervision does not require human labor. On images, state-of-the-art
SSL approaches based on contrastive learning are approach-ing or exceeding supervised pre-training. Fundamentally, the task in these methods is to discriminate different train-ing examples while simultaneously learning invariance to a
Figure 1. The importance of motion for video understanding.
We show four clips from two videos of the Diving48 dataset [42].
Different classes in this dataset are defined primarily by differ-ent motion patterns. How should these examples be used for con-trastive representation learning? Besides recognizing the clips as distinct, our method exploits that the relative temporal shifts be-tween the top two clips and the bottom two clips are identical to learn a temporally equivariant representation. This promotes de-tailed learning of motion patterns. set of input transformations, e.g., of the spatial domain via random cropping and horizontal flipping. How should the additional temporal domain in videos be used for SSL?
A straightforward approach would be to learn invariance to temporal input transformations. Since such invariance could encourage the representation to disregard video dy-namics, prior works proposed learning distinctiveness in-stead [51, 15], i.e., treating different temporal crops from the same video as distinct examples. In contrast, rather than merely learning to recognize two temporal augmentations as different, our main idea is to learn precisely how they differ. We, therefore, propose to exploit additional free su-pervision in the form of the known temporal transforma-tions. Besides learning that two clips show different mo-ments in time, the model will also learn the clips’ tempo-ral order and even the temporal shift between them. This
Figure 2. Illustration of temporal transformations and the proposed learning task. The colored rectangles illustrate three different temporally augmented clips extracted from two videos. The playback speed is indicated above the box (2× speed resulting in double the temporal extend with the same clip size) and the playback direction with an arrow below the box. The goal of our contrastive equivariance model is to recognize identical relative transformations, e.g., recognize that the temporal transformations between blue and yellow clips in both videos are identical. To support the equivariance learning, we introduce a new auxiliary task of classifying two clips into 1. non-overlapping with correct order (blue & yellow), 2. overlapping (blue & red), 3. non-overlapping with incorrect order (yellow & red). should improve the learning of dynamics and increase the models’ temporal reasoning capabilities, which is crucial in situations where motion is the primary discriminating fea-ture (e.g., see Figure 1). The recognition of input transfor-mations requires that the model represents input changes in a predictable way, e.g., when the learned representation is equivariant to the transformation.
In this paper, we propose a contrastive approach for learning representations that exhibit such equivariance to
In our method, we en-temporal input transformations. code the relative temporal transformation between two in-put clips in a feature-vector and contrast it with other, distinct, relative transformation vectors in the mini-batch.
Therefore, a positive pair for contrastive equivariance learn-ing results from applying the same relative transformation to two different examples. This framework is very flexible, as it allows us to encode the desired equivariance in a set of input transformations. Aside from standard video aug-mentations (e.g., random spatial and temporal cropping), we also explore equivariance to reverse playback and play-back at higher speeds.
It turns out that learning temporal equivariance is con-siderably more difficult than learning spatial equivariance (which we also study in experiments). However, tempo-ral equivariance and the resulting preservation of motion features lead to much-improved transfer performance. To increase the network’s sensitivity to motion patterns and improve the temporal equivariance learning, we propose as a novel pretext task the three-way classification of two clips into 1. non-overlapping with correct temporal order, 2. overlapping, 3. non-overlapping with an incorrect order.
Additionally, we also pose as auxiliary tasks the classifica-tion of the playback speed and playback direction as pro-posed in prior works [63, 5]. All these auxiliary tasks align with the temporal equivariance objective, the optimization of which they support. Figure 2 illustrates the considered transformations and the proposed learning tasks.
Contributions. To summarize, we make the following con-tributions: 1) We introduce a novel contrastive learning ap-proach to learn representations equivariant to a set of in-put transformations; 2) We study how equivariance to tem-poral or spatial transformations affects the quality of the learned features; 3) We introduce the pretext task of clip overlap/order prediction to support the temporal equivari-ance learning; 4) We show that time-equivariant representa-tions achieve state-of-the-art transfer learning performance in several action recognition and video retrieval benchmarks on UCF101 [54], HMDB51 [38], and Diving48 [42]. 2.