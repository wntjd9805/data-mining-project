Abstract
We present a method for creating 3D indoor scenes with a generative model learned from a collection of semantic-segmented depth images captured from different unknown scenes. Given a room with a speciﬁed size, our method automatically generates 3D objects in a room from a ran-domly sampled latent code. Different from existing methods that represent an indoor scene with the type, location, and other properties of objects in the room and learn the scene layout from a collection of complete 3D indoor scenes, our method models each indoor scene as a 3D semantic scene volume and learns a volumetric generative adversarial net-work (GAN) from a collection of 2.5D partial observations of 3D scenes. To this end, we apply a differentiable projec-tion layer to project the generated 3D semantic scene vol-umes into semantic-segmented depth images and design a new multiple-view discriminator for learning the complete 3D scene volume from 2.5D semantic-segmented depth im-ages. Compared to existing methods, our method not only efﬁciently reduces the workload of modeling and acquir-ing 3D scenes for training, but also produces better object shapes and their detailed layouts in the scene. We eval-uate our method with different indoor scene datasets and demonstrate the advantages of our method. We also extend our method for generating 3D indoor scenes from semantic-segmented depth images inferred from RGB images of real scenes. 1 1.

Introduction
Real-world indoor scenes exhibit rich variations with dif-ferent numbers, types, and layouts of the objects placed in a room due to different interior designs and living activities.
Generating realistic 3D indoor scenes is an important task for many applications, such as VR/AR, 3D game design, and robotic navigation.
*This work is done when Ming-Jia Yang was an intern at MSRA 1Code URL: https://github.com/mingjiayang/SGSDI
Manually modeling indoor scenes with variant and real-istic object layouts in a room is a labor-intensive task and requires professional skills. Automatic scene generation techniques try to model the properties and distributions of the objects in real scenes and generate new 3D scenes in two steps. For a room with a speciﬁed size and shape, these methods ﬁrst determine the layout (i.e. orientation and po-sition) and properties (e.g. type and shape) of the objects in the room. After that, they retrieve a CAD model of each object from a 3D object database based on the object’s prop-erties and then place the resulting CAD models in the room according to their layout.
A set of methods have been developed for modeling the properties and distributions of objects in indoor scenes.
Early methods use manually deﬁned rules [12] or simple statistic models [27, 4, 2, 5, 11, 17] computed from scene instances for generating a speciﬁc type of scenes, which are difﬁcult to generalize to other types of scenes. Recent deep-learning-based methods [8, 29, 25, 18, 24] learn a deep neural network of the object properties and layouts from a large collection of 3D scene instances that are difﬁcult to be modeled by skilled artists or captured from real scenes. By simply modeling the object geometry with their sizes, these methods fail to model concrete 3D object shapes and the detailed object layouts determined by their shapes, such as a chair with their seat under a desk or a TV inside a cabinet.
In this paper, we present a generative adversarial network (GAN) for 3D indoor scene generation. Different from pre-vious methods that represent the scene with object prop-erties and layouts, our method models a 3D indoor scene with a semantic scene volume, where each voxel is either labeled as empty or the type of object that it belongs to.
Based on this representation, we design a volumetric GAN model that takes the room size as input and synthesizes the semantic scene volumes of the room that consist of differ-ent objects and their layouts from randomly sampled latent vectors. After that, our method generates the ﬁnal 3D in-door scene by replacing each volumetric object instance in the volume with a CAD model retrieved from a 3D object database based on their type and volumetric shape.
Different from previous methods that train the networks with a collection of complete 3D indoor scenes, we learn the volumetric GAN model from a collection of semantic-segmented depth images, each of which captures a 2.5D partial view of an unknown 3D scene. To this end, we apply a differentiable projection layer between the gener-ator and discriminator, which projects the generated se-mantic scene volume into semantic-segmented depth im-ages from a set of views. We then feed both projected fake semantic-segmented depth images and real semantic-segmented depth images into the discriminator for GAN training.
A naive design of the discriminator is to use the single-view discriminator for learning 3D object representation from 2D images [14, 6, 9]. Unfortunately, the GAN model trained with this single-view discriminator is prone to gen-erating indoor scenes with unnatural object layouts. We thus propose a multi-view discriminator that takes a com-bination of multiple views rendered from generated scenes for GAN training. Since the training images are captured from different unknown scenes and we have no scene ID of each image, we use a random combination of training im-ages to approximate the ground truth layouts of underlying scenes. For this purpose, we empirically ﬁgure out the op-timal number and type of views of the random training im-age combinations that can well approximate the underlying scene layouts and facilitate the GAN training.
To the best of our knowledge, our method is the ﬁrst ap-proach that learns to generate 3D indoor scenes from a col-lection of semantic-segmented depth images, which greatly reduces the workload for training data acquisition and mod-eling. Thanks to semantic scene volume representation, our method can better model the object shapes and their detailed layouts than existing methods. We evaluate our method both synthetic Structured3D [30] and real Matter-Port3D [1] datasets and demonstrate the advantages of our method. With the help of existing RGB2Depth methods, we show that our method can successfully learn 3D scene gen-eration from segmented-depth images inferred from RGB images of real scenes. 2.