Abstract
Face recognition networks encode information about sen-sitive attributes while being trained for identity classification.
Such encoding has two major issues: (a) it makes the face representations susceptible to privacy leakage (b) it appears to contribute to bias in face recognition. However, exist-ing bias mitigation approaches generally require end-to-end training and are unable to achieve high verification accu-racy. Therefore, we present a descriptor-based adversarial de-biasing approach called ‘Protected Attribute Suppression
System (PASS)’. PASS can be trained on top of descriptors ob-tained from any previously trained high-performing network to classify identities and simultaneously reduce encoding of sensitive attributes. This eliminates the need for end-to-end training. As a component of PASS, we present a novel discriminator training strategy that discourages a network from encoding protected attribute information. We show the efficacy of PASS to reduce gender and skintone information in descriptors from SOTA face recognition networks like Ar-cface. As a result, PASS descriptors outperform existing baselines in reducing gender and skintone bias on the IJB-C dataset, while maintaining a high verification accuracy. 1.

Introduction
Over the past few years, the accuracy of face recognition networks has significantly improved [40, 41, 36, 15, 10, 17].
These improvements have led to the deployment of face recognition systems in a large number of applications. How-ever, recent studies [16, 25, 43] have also shown that face recognition networks encode information about protected attributes such as race, gender, and age, while being trained for identity classification. Encoding of sensitive attributes raises concerns regarding privacy and bias.
Privacy concerns: Many large-scale face verification and identification systems employ a database that stores face descriptors of identities, as opposed to face images. Face descriptors refer to the features extracted from the penulti-mate layer of a previously trained face recognition network.
*These authors have contributed equally to this work.
Figure 1. Suppose a malicious agent X has gained access to a private database D (blue) which consists of a pre-trained network
P and face descriptors of four identities. The agent can use P to extract descriptors (red) for a gender-labeled dataset DX (Step 1).
Using these descriptors, the agent can train a gender classifier CX (Step 2). Using the trained CX , the agent can predict the gender of the descriptors in D (Step 3) and thus cause privacy breach.
Storing descriptors, rather than images, allows for very fast gallery lookup and verification against known subjects. This also acts as an additional layer of security by not storing potentially sensitive information present in the original face images. However, since some sensitive information is still encoded in these descriptors (e.g. race, gender, age), a mali-cious agent with access to these descriptors can potentially extract this information and use it for nefarious purposes.
An example scenario is presented in Figure 1.
Bias concerns: Encoding of protected attributes such as gender or race in face descriptors results in bias w.r.t. these attributes when used for face recognition. A recent study from NIST [22] found evidence that characteristics such as gender and ethnicity impact verification and matching per-formance of face descriptors. Similarly, it has been shown that most face-based gender classifiers perform significantly better on male faces with light skintone than female faces with dark skintone [12].
One method of addressing privacy and bias issues is by producing face descriptors that are independent of the pro-tected attribute(s). For instance, Debface [20] proposes an end-to-end method for producing face descriptors that are disentangled from protected attributes using an adversarial approach. Another common strategy for mitigating bias is to train face recognition systems using training datasets that are balanced in terms of sensitive attributes. However, building large datasets that are balanced in terms of the attributes we want to protect is difficult, expensive, and time-consuming.
Moreover, once such a ‘fair’ dataset is constructed, we still need to perform the costly operation of training a large recog-nition network from scratch.
End-to-end training of a large-scale network requires ac-cess to a large dataset and computing power, and is time-consuming. Application of adversarial losses while training (as done in [20]), also slows down the training process. Sev-eral works [48, 11, 27] show that reducing the information of sensitive attributes while training a network results in a drop in overall performance. Even if a new network is trained to generate attribute-agnostic face descriptors, we need to replace the existing network (say, P in Fig 1), and re-compute the descriptors for all the identities by feeding in the respective face images.
In this work, we propose a solution that addresses the fol-lowing four points: (i) reduces the opportunity for leakage of protected attributes in face descriptors. (ii) mitigates bias with respect to multiple attributes (gender and skintone). (iii) operates on existing descriptors and does not require expen-sive end-to-end training. (iv) does not require a balanced training dataset.
The proposed method trains a lightweight model that transforms face descriptors obtained from an existing face recognition model, and maps them to an attribute agnostic representation. We achieve this using a novel adversarial training procedure called Protected Attribute Suppression
System (PASS). Unlike other works that adversarially sup-press protected attributes [20, 48] using end-to-end training, we operate on descriptor space. Once trained, PASS may be easily applied to other existing face descriptors. In summary, we make the following contributions in this paper: 1. We present PASS, an adversarial method that aims to reduce the information of sensitive attributes in face de-scriptors from any face recognition network, while main-taining high face verification performance. We show the efficacy of PASS to reduce gender and skintone informa-tion in face descriptors, and thus considerably reduce the associated biases. Moreover, PASS can be used on top of face descriptors obtained from any face recognition network. We show these results on two SOTA pre-trained networks: Arcface [15] and Crystalface [36]. 2. Our descriptor-based model cannot include CNN-based discriminators, which poses new challenges. We present a novel discriminator training strategy in PASS, to enforce
Method
[49, 30]
[47]
[48]
[13]
[6]
[19]
[27]
[7]
[35]
[46]
[20]
PASS (Ours)
Target task
Analogy completion
Object classification
Action classification
Action recognition
Gender/Age prediction
Preserve pose/illumination/expresssion
Smile, high-cheekbones
Face detection
Face attractiveness
Face recognition
Face recognition
Face recognition
Sensitive attribute
Gender
Gender
Identity, private attributes
Scene
Age/Gender
Identity
Gender, make-up
Skintone
Gender
Race
Age,gender,race
Gender, skintone
Table 1. Methods that adversarially remove sensitive attributes in general vision/NLP tasks (top) and face-related tasks (bottom) the removal of sensitive information in the descriptors. 3. We extend PASS to reduce information of multiple at-tributes simultaneously, and show that such a framework (known as ‘MultiPASS’) also performs well in terms of reducing the leakage of sensitive attributes and bias in face descriptors, while maintaining reasonable face veri-fication performance. 4. Since reducing the information of protected attributes in face descriptors also reduces their identity-classifying capability, we introduce a new metric called Bias Per-formance Coefficient (BPC), that measures the trade-off between bias reduction and drop in verification perfor-mance. We show that our PASS framework achieves better BPC values than existing baselines. 2.