Abstract
Visual understanding goes well beyond the study of im-ages or videos on the web. To achieve complex tasks in volatile situations, the human can deeply understand the en-vironment, quickly perceive events happening around, and continuously track objects’ state changes, which are still challenging for current AI systems. To equip AI system with the ability to understand dynamic ENVironments, we build a video Question Answering dataset named Env-QA.
Env-QA contains 23K egocentric videos, where each video is composed of a series of events about exploring and in-teracting in the environment.
It also provides 85K ques-tions to evaluate the ability of understanding the composi-tion, layout, and state changes of the environment presented by the events in videos. Moreover, we propose a video QA model, Temporal Segmentation and Event Attention network (TSEA), which introduces event-level video representation and corresponding attention mechanisms to better extract environment information and answer questions. Compre-hensive experiments demonstrate the effectiveness of our framework and show the formidable challenges of Env-QA in terms of long-term state tracking, multi-event temporal reasoning and event counting, etc. 1.

Introduction
In the last decades, tremendous works [9, 30, 45, 17, 58, 10, 49] have brought revolutionary advancements to com-puter vision systems for understanding web data, e.g., pho-tos, videos, and movies, while for deploying machines in the human living environment (i.e., building embodied arti-ficial intelligence), we will encounter brand new challenges on visual ability. 1) From “broader” to “deeper” visual un-derstanding. The studies of internet AI focus on making the system recognize from dozens of object categories [29, 34] to thousands of categories [9, 43, 19] (broader). How-Figure 1. Env-QA dataset contains egocentric videos about explor-ing and interacting with environments, and diverse questions to evaluate the models of understanding dynamic environments from various perspectives. ever, internet AI mainly pays attention to the salient objects shown in the images. For completing tasks in real world environment, such as cooking a meal, a system needs an in-depth understanding of every detail of the environment (deeper), e.g., knowing the positions of all utensils and in-gredients in a kitchen. 2) From static to dynamic visual un-derstanding. One of the essential characteristics of the real world environment lies in its dynamic nature. The interac-tions between a human and the environment will trigger the environment’s state changes. A system must learn to sense and remember state changes to accomplish some long-term tasks, e.g., a housekeeping robot may need to continuously track the state of objects at home to plan the cleaning task.
However, few works purely study these visual abili-ties under embodied AI setting. Some of the video QA datasets, such as TVQA [32] and MovieQA [49], evaluate the model’s understanding of movies, TV series or YouTube
Table 1. Comparison of Env-QA with other related video QA and embodied QA datasets. The table shows the basic information of the modalities involved in the existing datasets. The content in brackets shows the main characteristic of the visual material.
Dataset
Vision
Object-Centric Interaction
Human-Centric Action
Language
Action
#Clips
#QA
MovieQA [49]
TGIF-QA [22]
TVQA [32]
TVQA+ [33]
Social-IQ [59]
CLEVERER [57]
Embodied QA [8]
Interactive QA [14]
-----Synthetic Video (Object Collision)
AI Habitat (Static Env.)
AI2-THOR (Nearly Static Env.)
Env-QA (Ours)
AI2-THOR (Dynamic Env.)
Movie (Plot)
Social Media (Action)
TV Show (Plot)
TV Show (Plot)
Question+Subtitle+Script
Question
Question+Subtitle
Question+Subtitle
YouTube (Social Situation) Question+Transcript+Audio
----Question
Question
Question
Question
------Navigation
Navigation+Manipulation 6.5K 6.8K 56.7K 103.9K 21.8K 152.5K 29.4K 4.2K 1.2K 7.5K 10.0K 305.0K 1.1K 75.0K
---23.3K 85.1K videos. In Table 1, we display the main characteristics of the related datasets. Although these tasks explore the dy-namics of vision, they focus more on the dynamics intro-duced by human-centered actions, social activities, or plot development, rather than the interaction with environments.
Correspondingly, these tasks mainly require the abilities of human posture recognition, dialogue understanding, and so-cial knowledge understanding. Although some other related tasks, such as Visual Navigation and Manipulation [61, 48], and Embodied QA [8, 14], involve the understanding of the environment, they focus more on the comprehensive ability of how to plan actions in the environment. The visual ability of environment understanding is implicitly evaluated by the quality of performed actions. Besides, these tasks usually require models to perform in a nearly static environment, so they are also hard to investigate the dynamics of environ-ments.
Thus, we propose to take the question answering as a proxy task to purely study the dynamic environments under-standing. The task is required to watch an egocentric video composed of a series of events about exploring and inter-acting in the environment, e.g., move the pot, turn on the faucet, as shown in Figure 1. It must then answer a question that requires 1) understanding the environment’s composi-tion (like Q1), layout, trajectory of state changes (like Q2) presented by the events, or 2) performing temporal reason-ing on events (like Q3 and Q4).
To support such a task, we construct a large-scale dataset,
Env-QA, containing 23.3K videos and 85.1K questions. A critical challenge in building a dataset of this scale is how to control the distribution of samples. Most recent QA datasets with off-the-shelf visual materials from Internet contain un-expected biases [24, 1]. These biases could be more dis-tinct for housework in natural scenes, like cooking, leading to high risks for models to guess the answer without even looking at the visual materials. To address this challenge, we resort to the recently proposed virtual simulator AI2-THOR [27] to generate videos with strictly controlled con-tent by ourselves. Specifically, we design a semi-automatic data collection method. Our designed algorithm is responsi-ble for controlling the sample distribution and automatically generating natural language guidance information. Then, annotators follow the guidance to manipulate in the simula-tor to generate videos and collect question-answer pairs.
Understanding the dynamic environments from a se-quence of interaction events requires extracting key envi-ronmental information from the events and performing tem-poral reasoning to capture state changes. And the founda-tion of both abilities is to represent the video at the level of events, that is dividing the video into clips according to its content to let the model locate key events easier. How-ever, the previous video QA methods [21, 33] mainly use the grid-level video features with a preset interval extracted by temporal CNN [50, 23]. To address this problem, we introduce Temporal Segmentation and Event Attention net-work (TSEA), which will first segment the video to flexi-ble duration clips based on the content, then perform multi-step temporal reasoning to locate the key events for a given question and output the answer. Experiments on Env-QA demonstrate the effectiveness of our proposed method and reveal that Env-QA is challenging in terms of capturing the long-term state change, multi-event temporal reasoning, and event counting, etc. 2.